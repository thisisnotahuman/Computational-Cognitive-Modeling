PopSpikeActor(
  (encoder): PopSpikeEncoderRegularSpike()
  (snn): SpikeMLP(
    (hidden_layers): ModuleList(
      (0): Linear(in_features=340, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=64, bias=True)
    )
    (out_pop_layer): Linear(in_features=64, out_features=90, bias=True)
  )
  (decoder): PopSpikeDecoder(
    (decoder): Conv1d(9, 9, kernel_size=(10,), stride=(1,), groups=9)
    (output_activation): ELU(alpha=1.0)
  )
)
Sequential(
  (0): Linear(in_features=34, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=1, bias=True)
)
################################################################################
                      [1m Learning iteration 0/4000 [0m

                       Computation: 1648 steps/s (collection: 0.774s, learning 4.195s)
               Value function loss: 3.1011
                    Surrogate loss: 0.0084
             Mean action noise std: 1.00
                       Mean reward: 4.56
               Mean episode length: 14.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 20.84
--------------------------------------------------------------------------------
                   Total timesteps: 8192
                    Iteration time: 4.97s
                        Total time: 4.97s
                               ETA: 19874.9s

################################################################################
                      [1m Learning iteration 1/4000 [0m

                       Computation: 1830 steps/s (collection: 0.576s, learning 3.899s)
               Value function loss: 3.8525
                    Surrogate loss: 0.0072
             Mean action noise std: 1.00
                       Mean reward: 6.55
               Mean episode length: 23.50
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 13.21
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 4.47s
                        Total time: 9.44s
                               ETA: 18882.3s

################################################################################
                      [1m Learning iteration 2/4000 [0m

                       Computation: 1829 steps/s (collection: 0.590s, learning 3.888s)
               Value function loss: 3.8105
                    Surrogate loss: 0.0089
             Mean action noise std: 1.00
                       Mean reward: 6.58
               Mean episode length: 22.84
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 14.17
--------------------------------------------------------------------------------
                   Total timesteps: 24576
                    Iteration time: 4.48s
                        Total time: 13.92s
                               ETA: 18552.3s

################################################################################
                      [1m Learning iteration 3/4000 [0m

                       Computation: 1857 steps/s (collection: 0.511s, learning 3.899s)
               Value function loss: 3.6199
                    Surrogate loss: 0.0090
             Mean action noise std: 1.00
                       Mean reward: 6.84
               Mean episode length: 24.45
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 14.87
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 4.41s
                        Total time: 18.33s
                               ETA: 18317.4s

################################################################################
                      [1m Learning iteration 4/4000 [0m

                       Computation: 1826 steps/s (collection: 0.558s, learning 3.928s)
               Value function loss: 3.3500
                    Surrogate loss: 0.0084
             Mean action noise std: 1.00
                       Mean reward: 6.97
               Mean episode length: 26.64
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 15.54
--------------------------------------------------------------------------------
                   Total timesteps: 40960
                    Iteration time: 4.49s
                        Total time: 22.82s
                               ETA: 18235.2s

################################################################################
                      [1m Learning iteration 5/4000 [0m

                       Computation: 1839 steps/s (collection: 0.565s, learning 3.887s)
               Value function loss: 3.1236
                    Surrogate loss: 0.0073
             Mean action noise std: 1.00
                       Mean reward: 7.70
               Mean episode length: 27.09
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 15.94
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 4.45s
                        Total time: 27.27s
                               ETA: 18156.9s

################################################################################
                      [1m Learning iteration 6/4000 [0m

                       Computation: 1848 steps/s (collection: 0.520s, learning 3.910s)
               Value function loss: 2.5990
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 7.83
               Mean episode length: 27.59
                 Mean success rate: 0.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 18.16
--------------------------------------------------------------------------------
                   Total timesteps: 57344
                    Iteration time: 4.43s
                        Total time: 31.70s
                               ETA: 18087.2s

################################################################################
                      [1m Learning iteration 7/4000 [0m

                       Computation: 1838 steps/s (collection: 0.572s, learning 3.883s)
               Value function loss: 2.7052
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 9.42
               Mean episode length: 35.23
                 Mean success rate: 0.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 19.19
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 4.46s
                        Total time: 36.16s
                               ETA: 18046.0s

################################################################################
                      [1m Learning iteration 8/4000 [0m

                       Computation: 1857 steps/s (collection: 0.531s, learning 3.880s)
               Value function loss: 2.9124
                    Surrogate loss: 0.0064
             Mean action noise std: 1.00
                       Mean reward: 9.32
               Mean episode length: 34.53
                 Mean success rate: 0.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 21.79
--------------------------------------------------------------------------------
                   Total timesteps: 73728
                    Iteration time: 4.41s
                        Total time: 40.57s
                               ETA: 17993.2s

################################################################################
                      [1m Learning iteration 9/4000 [0m

                       Computation: 1825 steps/s (collection: 0.560s, learning 3.929s)
               Value function loss: 2.6389
                    Surrogate loss: 0.0064
             Mean action noise std: 1.00
                       Mean reward: 9.31
               Mean episode length: 34.52
                 Mean success rate: 0.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 23.08
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 4.49s
                        Total time: 45.05s
                               ETA: 17981.1s

################################################################################
                      [1m Learning iteration 10/4000 [0m

                       Computation: 1828 steps/s (collection: 0.543s, learning 3.936s)
               Value function loss: 2.0992
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 9.72
               Mean episode length: 37.99
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 90112
                    Iteration time: 4.48s
                        Total time: 49.53s
                               ETA: 17967.0s

################################################################################
                      [1m Learning iteration 11/4000 [0m

                       Computation: 1815 steps/s (collection: 0.600s, learning 3.911s)
               Value function loss: 2.0777
                    Surrogate loss: 0.0037
             Mean action noise std: 1.00
                       Mean reward: 11.33
               Mean episode length: 46.66
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 4.51s
                        Total time: 54.04s
                               ETA: 17965.3s

################################################################################
                      [1m Learning iteration 12/4000 [0m

                       Computation: 1836 steps/s (collection: 0.573s, learning 3.889s)
               Value function loss: 2.9096
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 13.77
               Mean episode length: 63.35
                 Mean success rate: 0.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 106496
                    Iteration time: 4.46s
                        Total time: 58.51s
                               ETA: 17947.9s

################################################################################
                      [1m Learning iteration 13/4000 [0m

                       Computation: 1858 steps/s (collection: 0.530s, learning 3.878s)
               Value function loss: 3.2453
                    Surrogate loss: 0.0036
             Mean action noise std: 1.00
                       Mean reward: 17.99
               Mean episode length: 89.55
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 24.67
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 4.41s
                        Total time: 62.91s
                               ETA: 17917.2s

################################################################################
                      [1m Learning iteration 14/4000 [0m

                       Computation: 1844 steps/s (collection: 0.589s, learning 3.853s)
               Value function loss: 2.8338
                    Surrogate loss: 0.0040
             Mean action noise std: 1.00
                       Mean reward: 20.04
               Mean episode length: 100.89
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 25.52
--------------------------------------------------------------------------------
                   Total timesteps: 122880
                    Iteration time: 4.44s
                        Total time: 67.36s
                               ETA: 17898.7s

################################################################################
                      [1m Learning iteration 15/4000 [0m

                       Computation: 1876 steps/s (collection: 0.491s, learning 3.875s)
               Value function loss: 2.8064
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 21.34
               Mean episode length: 105.92
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 4.37s
                        Total time: 71.72s
                               ETA: 17863.3s

################################################################################
                      [1m Learning iteration 16/4000 [0m

                       Computation: 1847 steps/s (collection: 0.536s, learning 3.897s)
               Value function loss: 3.0478
                    Surrogate loss: 0.0035
             Mean action noise std: 1.00
                       Mean reward: 20.88
               Mean episode length: 102.52
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 139264
                    Iteration time: 4.43s
                        Total time: 76.16s
                               ETA: 17847.3s

################################################################################
                      [1m Learning iteration 17/4000 [0m

                       Computation: 1822 steps/s (collection: 0.591s, learning 3.904s)
               Value function loss: 2.6879
                    Surrogate loss: 0.0023
             Mean action noise std: 1.00
                       Mean reward: 22.03
               Mean episode length: 106.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 4.50s
                        Total time: 80.65s
                               ETA: 17846.3s

################################################################################
                      [1m Learning iteration 18/4000 [0m

                       Computation: 1841 steps/s (collection: 0.548s, learning 3.901s)
               Value function loss: 2.6546
                    Surrogate loss: 0.0030
             Mean action noise std: 1.00
                       Mean reward: 23.45
               Mean episode length: 115.64
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 155648
                    Iteration time: 4.45s
                        Total time: 85.10s
                               ETA: 17835.2s

################################################################################
                      [1m Learning iteration 19/4000 [0m

                       Computation: 1849 steps/s (collection: 0.547s, learning 3.883s)
               Value function loss: 2.4400
                    Surrogate loss: 0.0037
             Mean action noise std: 1.00
                       Mean reward: 23.63
               Mean episode length: 117.11
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 4.43s
                        Total time: 89.53s
                               ETA: 17820.8s

################################################################################
                      [1m Learning iteration 20/4000 [0m

                       Computation: 1827 steps/s (collection: 0.531s, learning 3.951s)
               Value function loss: 3.1342
                    Surrogate loss: 0.0029
             Mean action noise std: 1.00
                       Mean reward: 25.89
               Mean episode length: 131.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 172032
                    Iteration time: 4.48s
                        Total time: 94.01s
                               ETA: 17817.5s

################################################################################
                      [1m Learning iteration 21/4000 [0m

                       Computation: 1846 steps/s (collection: 0.560s, learning 3.878s)
               Value function loss: 2.5058
                    Surrogate loss: 0.0021
             Mean action noise std: 1.00
                       Mean reward: 28.75
               Mean episode length: 147.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 4.44s
                        Total time: 98.45s
                               ETA: 17805.9s

################################################################################
                      [1m Learning iteration 22/4000 [0m

                       Computation: 1877 steps/s (collection: 0.478s, learning 3.885s)
               Value function loss: 4.6801
                    Surrogate loss: 0.0036
             Mean action noise std: 1.00
                       Mean reward: 33.33
               Mean episode length: 177.06
                 Mean success rate: 0.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 188416
                    Iteration time: 4.36s
                        Total time: 102.81s
                               ETA: 17782.2s

################################################################################
                      [1m Learning iteration 23/4000 [0m

                       Computation: 1849 steps/s (collection: 0.496s, learning 3.934s)
               Value function loss: 4.8355
                    Surrogate loss: 0.0023
             Mean action noise std: 1.00
                       Mean reward: 35.64
               Mean episode length: 189.76
                 Mean success rate: 0.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 4.43s
                        Total time: 107.24s
                               ETA: 17771.1s

################################################################################
                      [1m Learning iteration 24/4000 [0m

                       Computation: 1861 steps/s (collection: 0.509s, learning 3.892s)
               Value function loss: 7.4226
                    Surrogate loss: 0.0034
             Mean action noise std: 1.00
                       Mean reward: 35.34
               Mean episode length: 187.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 204800
                    Iteration time: 4.40s
                        Total time: 111.64s
                               ETA: 17755.9s

################################################################################
                      [1m Learning iteration 25/4000 [0m

                       Computation: 1868 steps/s (collection: 0.522s, learning 3.862s)
               Value function loss: 8.3492
                    Surrogate loss: 0.0034
             Mean action noise std: 1.00
                       Mean reward: 39.83
               Mean episode length: 207.22
                 Mean success rate: 0.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 4.38s
                        Total time: 116.03s
                               ETA: 17739.0s

################################################################################
                      [1m Learning iteration 26/4000 [0m

                       Computation: 1864 steps/s (collection: 0.529s, learning 3.864s)
               Value function loss: 11.5521
                    Surrogate loss: 0.0038
             Mean action noise std: 1.00
                       Mean reward: 38.16
               Mean episode length: 193.16
                 Mean success rate: 0.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 221184
                    Iteration time: 4.39s
                        Total time: 120.42s
                               ETA: 17724.4s

################################################################################
                      [1m Learning iteration 27/4000 [0m

                       Computation: 1874 steps/s (collection: 0.493s, learning 3.877s)
               Value function loss: 10.1541
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 37.63
               Mean episode length: 180.33
                 Mean success rate: 0.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 4.37s
                        Total time: 124.79s
                               ETA: 17707.2s

################################################################################
                      [1m Learning iteration 28/4000 [0m

                       Computation: 1795 steps/s (collection: 0.645s, learning 3.918s)
               Value function loss: 15.9489
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 42.76
               Mean episode length: 192.19
                 Mean success rate: 0.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 25.36
--------------------------------------------------------------------------------
                   Total timesteps: 237568
                    Iteration time: 4.56s
                        Total time: 129.36s
                               ETA: 17717.2s

################################################################################
                      [1m Learning iteration 29/4000 [0m

                       Computation: 1843 steps/s (collection: 0.565s, learning 3.880s)
               Value function loss: 12.0364
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 46.88
               Mean episode length: 198.47
                 Mean success rate: 0.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 4.44s
                        Total time: 133.80s
                               ETA: 17710.6s

################################################################################
                      [1m Learning iteration 30/4000 [0m

                       Computation: 1798 steps/s (collection: 0.640s, learning 3.914s)
               Value function loss: 11.2348
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 54.60
               Mean episode length: 223.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 253952
                    Iteration time: 4.55s
                        Total time: 138.35s
                               ETA: 17718.3s

################################################################################
                      [1m Learning iteration 31/4000 [0m

                       Computation: 1820 steps/s (collection: 0.599s, learning 3.901s)
               Value function loss: 11.5062
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 58.86
               Mean episode length: 231.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 4.50s
                        Total time: 142.85s
                               ETA: 17718.4s

################################################################################
                      [1m Learning iteration 32/4000 [0m

                       Computation: 1836 steps/s (collection: 0.571s, learning 3.890s)
               Value function loss: 9.9279
                    Surrogate loss: 0.0069
             Mean action noise std: 1.00
                       Mean reward: 64.75
               Mean episode length: 251.43
                 Mean success rate: 0.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 270336
                    Iteration time: 4.46s
                        Total time: 147.31s
                               ETA: 17713.4s

################################################################################
                      [1m Learning iteration 33/4000 [0m

                       Computation: 1822 steps/s (collection: 0.580s, learning 3.916s)
               Value function loss: 11.2291
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 67.32
               Mean episode length: 253.51
                 Mean success rate: 0.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 4.50s
                        Total time: 151.81s
                               ETA: 17712.7s

################################################################################
                      [1m Learning iteration 34/4000 [0m

                       Computation: 1844 steps/s (collection: 0.529s, learning 3.912s)
               Value function loss: 6.3601
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 68.09
               Mean episode length: 254.38
                 Mean success rate: 0.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 286720
                    Iteration time: 4.44s
                        Total time: 156.25s
                               ETA: 17705.4s

################################################################################
                      [1m Learning iteration 35/4000 [0m

                       Computation: 1825 steps/s (collection: 0.632s, learning 3.855s)
               Value function loss: 12.7191
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 69.11
               Mean episode length: 246.63
                 Mean success rate: 0.00
                  Mean reward/step: 0.37
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 4.49s
                        Total time: 160.74s
                               ETA: 17703.5s

################################################################################
                      [1m Learning iteration 36/4000 [0m

                       Computation: 1864 steps/s (collection: 0.546s, learning 3.847s)
               Value function loss: 10.2849
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 69.62
               Mean episode length: 237.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.38
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 303104
                    Iteration time: 4.39s
                        Total time: 165.13s
                               ETA: 17691.4s

################################################################################
                      [1m Learning iteration 37/4000 [0m

                       Computation: 1848 steps/s (collection: 0.562s, learning 3.870s)
               Value function loss: 12.8352
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 69.34
               Mean episode length: 227.73
                 Mean success rate: 0.00
                  Mean reward/step: 0.38
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 4.43s
                        Total time: 169.56s
                               ETA: 17683.8s

################################################################################
                      [1m Learning iteration 38/4000 [0m

                       Computation: 1840 steps/s (collection: 0.529s, learning 3.923s)
               Value function loss: 12.4687
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 76.53
               Mean episode length: 238.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.38
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 319488
                    Iteration time: 4.45s
                        Total time: 174.02s
                               ETA: 17678.2s

################################################################################
                      [1m Learning iteration 39/4000 [0m

                       Computation: 1848 steps/s (collection: 0.541s, learning 3.890s)
               Value function loss: 8.3645
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 80.61
               Mean episode length: 244.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.38
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 4.43s
                        Total time: 178.45s
                               ETA: 17670.7s

################################################################################
                      [1m Learning iteration 40/4000 [0m

                       Computation: 1833 steps/s (collection: 0.514s, learning 3.954s)
               Value function loss: 9.5163
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 83.88
               Mean episode length: 247.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 335872
                    Iteration time: 4.47s
                        Total time: 182.92s
                               ETA: 17666.9s

################################################################################
                      [1m Learning iteration 41/4000 [0m

                       Computation: 1792 steps/s (collection: 0.571s, learning 3.999s)
               Value function loss: 11.3706
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 87.17
               Mean episode length: 249.38
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 4.57s
                        Total time: 187.48s
                               ETA: 17672.7s

################################################################################
                      [1m Learning iteration 42/4000 [0m

                       Computation: 1789 steps/s (collection: 0.621s, learning 3.958s)
               Value function loss: 9.4808
                    Surrogate loss: 0.0041
             Mean action noise std: 1.00
                       Mean reward: 93.00
               Mean episode length: 261.81
                 Mean success rate: 0.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 352256
                    Iteration time: 4.58s
                        Total time: 192.06s
                               ETA: 17678.8s

################################################################################
                      [1m Learning iteration 43/4000 [0m

                       Computation: 1819 steps/s (collection: 0.537s, learning 3.965s)
               Value function loss: 16.4545
                    Surrogate loss: 0.0063
             Mean action noise std: 1.00
                       Mean reward: 102.88
               Mean episode length: 284.73
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 4.50s
                        Total time: 196.57s
                               ETA: 17677.5s

################################################################################
                      [1m Learning iteration 44/4000 [0m

                       Computation: 1766 steps/s (collection: 0.711s, learning 3.927s)
               Value function loss: 20.6193
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 114.67
               Mean episode length: 314.93
                 Mean success rate: 0.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 368640
                    Iteration time: 4.64s
                        Total time: 201.20s
                               ETA: 17687.9s

################################################################################
                      [1m Learning iteration 45/4000 [0m

                       Computation: 1772 steps/s (collection: 0.641s, learning 3.981s)
               Value function loss: 23.6201
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 124.78
               Mean episode length: 339.25
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 4.62s
                        Total time: 205.82s
                               ETA: 17696.4s

################################################################################
                      [1m Learning iteration 46/4000 [0m

                       Computation: 1789 steps/s (collection: 0.655s, learning 3.924s)
               Value function loss: 18.9892
                    Surrogate loss: 0.0041
             Mean action noise std: 1.00
                       Mean reward: 129.78
               Mean episode length: 349.51
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 385024
                    Iteration time: 4.58s
                        Total time: 210.40s
                               ETA: 17700.7s

################################################################################
                      [1m Learning iteration 47/4000 [0m

                       Computation: 1818 steps/s (collection: 0.640s, learning 3.866s)
               Value function loss: 16.6134
                    Surrogate loss: 0.0053
             Mean action noise std: 0.99
                       Mean reward: 131.51
               Mean episode length: 353.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 4.51s
                        Total time: 214.91s
                               ETA: 17698.6s

################################################################################
                      [1m Learning iteration 48/4000 [0m

                       Computation: 1783 steps/s (collection: 0.628s, learning 3.966s)
               Value function loss: 12.0271
                    Surrogate loss: 0.0041
             Mean action noise std: 0.99
                       Mean reward: 131.18
               Mean episode length: 350.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 401408
                    Iteration time: 4.59s
                        Total time: 219.50s
                               ETA: 17703.6s

################################################################################
                      [1m Learning iteration 49/4000 [0m

                       Computation: 1783 steps/s (collection: 0.580s, learning 4.014s)
               Value function loss: 17.8795
                    Surrogate loss: 0.0046
             Mean action noise std: 0.99
                       Mean reward: 132.73
               Mean episode length: 354.22
                 Mean success rate: 0.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 4.59s
                        Total time: 224.10s
                               ETA: 17708.1s

################################################################################
                      [1m Learning iteration 50/4000 [0m

                       Computation: 1623 steps/s (collection: 0.996s, learning 4.050s)
               Value function loss: 12.7166
                    Surrogate loss: 0.0028
             Mean action noise std: 0.99
                       Mean reward: 131.68
               Mean episode length: 350.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 417792
                    Iteration time: 5.05s
                        Total time: 229.14s
                               ETA: 17747.3s

################################################################################
                      [1m Learning iteration 51/4000 [0m

                       Computation: 1787 steps/s (collection: 0.628s, learning 3.954s)
               Value function loss: 22.8633
                    Surrogate loss: 0.0029
             Mean action noise std: 0.99
                       Mean reward: 123.61
               Mean episode length: 323.29
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 4.58s
                        Total time: 233.72s
                               ETA: 17749.5s

################################################################################
                      [1m Learning iteration 52/4000 [0m

                       Computation: 1817 steps/s (collection: 0.553s, learning 3.954s)
               Value function loss: 14.6419
                    Surrogate loss: 0.0054
             Mean action noise std: 0.99
                       Mean reward: 120.43
               Mean episode length: 313.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 434176
                    Iteration time: 4.51s
                        Total time: 238.23s
                               ETA: 17745.9s

################################################################################
                      [1m Learning iteration 53/4000 [0m

                       Computation: 1831 steps/s (collection: 0.556s, learning 3.918s)
               Value function loss: 17.3626
                    Surrogate loss: 0.0053
             Mean action noise std: 0.99
                       Mean reward: 123.67
               Mean episode length: 323.39
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 4.47s
                        Total time: 242.70s
                               ETA: 17739.9s

################################################################################
                      [1m Learning iteration 54/4000 [0m

                       Computation: 1824 steps/s (collection: 0.570s, learning 3.920s)
               Value function loss: 20.6992
                    Surrogate loss: 0.0063
             Mean action noise std: 0.99
                       Mean reward: 116.89
               Mean episode length: 302.01
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 450560
                    Iteration time: 4.49s
                        Total time: 247.19s
                               ETA: 17735.0s

################################################################################
                      [1m Learning iteration 55/4000 [0m

                       Computation: 1802 steps/s (collection: 0.550s, learning 3.995s)
               Value function loss: 24.9451
                    Surrogate loss: 0.0048
             Mean action noise std: 0.99
                       Mean reward: 105.91
               Mean episode length: 273.92
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 4.55s
                        Total time: 251.74s
                               ETA: 17734.1s

################################################################################
                      [1m Learning iteration 56/4000 [0m

                       Computation: 1788 steps/s (collection: 0.598s, learning 3.982s)
               Value function loss: 20.1128
                    Surrogate loss: 0.0057
             Mean action noise std: 0.99
                       Mean reward: 102.41
               Mean episode length: 260.57
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 466944
                    Iteration time: 4.58s
                        Total time: 256.32s
                               ETA: 17735.5s

################################################################################
                      [1m Learning iteration 57/4000 [0m

                       Computation: 1750 steps/s (collection: 0.689s, learning 3.990s)
               Value function loss: 18.5577
                    Surrogate loss: 0.0035
             Mean action noise std: 0.99
                       Mean reward: 92.97
               Mean episode length: 238.72
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 4.68s
                        Total time: 261.00s
                               ETA: 17743.3s

################################################################################
                      [1m Learning iteration 58/4000 [0m

                       Computation: 1669 steps/s (collection: 0.963s, learning 3.943s)
               Value function loss: 15.2259
                    Surrogate loss: 0.0049
             Mean action noise std: 0.99
                       Mean reward: 91.58
               Mean episode length: 235.85
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 483328
                    Iteration time: 4.91s
                        Total time: 265.90s
                               ETA: 17766.0s

################################################################################
                      [1m Learning iteration 59/4000 [0m

                       Computation: 1838 steps/s (collection: 0.575s, learning 3.882s)
               Value function loss: 24.4678
                    Surrogate loss: 0.0043
             Mean action noise std: 0.99
                       Mean reward: 89.61
               Mean episode length: 230.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 4.46s
                        Total time: 270.36s
                               ETA: 17758.2s

################################################################################
                      [1m Learning iteration 60/4000 [0m

                       Computation: 1831 steps/s (collection: 0.566s, learning 3.907s)
               Value function loss: 32.2478
                    Surrogate loss: 0.0037
             Mean action noise std: 0.99
                       Mean reward: 89.37
               Mean episode length: 227.60
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 499712
                    Iteration time: 4.47s
                        Total time: 274.83s
                               ETA: 17751.5s

################################################################################
                      [1m Learning iteration 61/4000 [0m

                       Computation: 1855 steps/s (collection: 0.554s, learning 3.861s)
               Value function loss: 30.9345
                    Surrogate loss: 0.0038
             Mean action noise std: 0.99
                       Mean reward: 100.18
               Mean episode length: 254.26
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 4.42s
                        Total time: 279.25s
                               ETA: 17741.3s

################################################################################
                      [1m Learning iteration 62/4000 [0m

                       Computation: 1834 steps/s (collection: 0.587s, learning 3.878s)
               Value function loss: 17.6025
                    Surrogate loss: 0.0036
             Mean action noise std: 0.99
                       Mean reward: 103.44
               Mean episode length: 263.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 516096
                    Iteration time: 4.46s
                        Total time: 283.71s
                               ETA: 17734.3s

################################################################################
                      [1m Learning iteration 63/4000 [0m

                       Computation: 1848 steps/s (collection: 0.572s, learning 3.859s)
               Value function loss: 19.2921
                    Surrogate loss: 0.0036
             Mean action noise std: 0.99
                       Mean reward: 109.49
               Mean episode length: 277.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 4.43s
                        Total time: 288.15s
                               ETA: 17725.4s

################################################################################
                      [1m Learning iteration 64/4000 [0m

                       Computation: 1832 steps/s (collection: 0.601s, learning 3.871s)
               Value function loss: 19.2484
                    Surrogate loss: 0.0039
             Mean action noise std: 0.99
                       Mean reward: 116.39
               Mean episode length: 294.01
                 Mean success rate: 0.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 532480
                    Iteration time: 4.47s
                        Total time: 292.62s
                               ETA: 17719.1s

################################################################################
                      [1m Learning iteration 65/4000 [0m

                       Computation: 1851 steps/s (collection: 0.568s, learning 3.855s)
               Value function loss: 15.1552
                    Surrogate loss: 0.0040
             Mean action noise std: 0.99
                       Mean reward: 118.36
               Mean episode length: 296.41
                 Mean success rate: 0.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 4.42s
                        Total time: 297.04s
                               ETA: 17709.9s

################################################################################
                      [1m Learning iteration 66/4000 [0m

                       Computation: 1837 steps/s (collection: 0.601s, learning 3.857s)
               Value function loss: 14.2758
                    Surrogate loss: 0.0042
             Mean action noise std: 0.99
                       Mean reward: 119.31
               Mean episode length: 298.45
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 548864
                    Iteration time: 4.46s
                        Total time: 301.50s
                               ETA: 17702.9s

################################################################################
                      [1m Learning iteration 67/4000 [0m

                       Computation: 1850 steps/s (collection: 0.563s, learning 3.863s)
               Value function loss: 21.1505
                    Surrogate loss: 0.0049
             Mean action noise std: 0.99
                       Mean reward: 118.01
               Mean episode length: 294.74
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 4.43s
                        Total time: 305.93s
                               ETA: 17694.2s

################################################################################
                      [1m Learning iteration 68/4000 [0m

                       Computation: 1827 steps/s (collection: 0.599s, learning 3.885s)
               Value function loss: 15.0745
                    Surrogate loss: 0.0043
             Mean action noise std: 0.99
                       Mean reward: 118.42
               Mean episode length: 296.77
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 565248
                    Iteration time: 4.48s
                        Total time: 310.41s
                               ETA: 17688.8s

################################################################################
                      [1m Learning iteration 69/4000 [0m

                       Computation: 1826 steps/s (collection: 0.578s, learning 3.908s)
               Value function loss: 19.5803
                    Surrogate loss: 0.0045
             Mean action noise std: 0.99
                       Mean reward: 109.29
               Mean episode length: 273.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 4.49s
                        Total time: 314.90s
                               ETA: 17683.6s

################################################################################
                      [1m Learning iteration 70/4000 [0m

                       Computation: 1852 steps/s (collection: 0.542s, learning 3.879s)
               Value function loss: 11.0383
                    Surrogate loss: 0.0039
             Mean action noise std: 0.99
                       Mean reward: 108.33
               Mean episode length: 271.11
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 581632
                    Iteration time: 4.42s
                        Total time: 319.32s
                               ETA: 17674.9s

################################################################################
                      [1m Learning iteration 71/4000 [0m

                       Computation: 1852 steps/s (collection: 0.551s, learning 3.871s)
               Value function loss: 27.9483
                    Surrogate loss: 0.0055
             Mean action noise std: 0.99
                       Mean reward: 99.91
               Mean episode length: 251.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 4.42s
                        Total time: 323.74s
                               ETA: 17666.2s

################################################################################
                      [1m Learning iteration 72/4000 [0m

                       Computation: 1834 steps/s (collection: 0.541s, learning 3.924s)
               Value function loss: 12.0926
                    Surrogate loss: 0.0047
             Mean action noise std: 0.99
                       Mean reward: 102.75
               Mean episode length: 254.82
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 598016
                    Iteration time: 4.47s
                        Total time: 328.20s
                               ETA: 17660.0s

################################################################################
                      [1m Learning iteration 73/4000 [0m

                       Computation: 1826 steps/s (collection: 0.588s, learning 3.897s)
               Value function loss: 23.2673
                    Surrogate loss: 0.0041
             Mean action noise std: 0.99
                       Mean reward: 101.85
               Mean episode length: 252.00
                 Mean success rate: 0.00
                  Mean reward/step: 0.44
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 4.49s
                        Total time: 332.69s
                               ETA: 17655.0s

################################################################################
                      [1m Learning iteration 74/4000 [0m

                       Computation: 1823 steps/s (collection: 0.604s, learning 3.888s)
               Value function loss: 17.0673
                    Surrogate loss: 0.0031
             Mean action noise std: 0.99
                       Mean reward: 102.20
               Mean episode length: 248.99
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 614400
                    Iteration time: 4.49s
                        Total time: 337.18s
                               ETA: 17650.2s

################################################################################
                      [1m Learning iteration 75/4000 [0m

                       Computation: 1865 steps/s (collection: 0.524s, learning 3.867s)
               Value function loss: 22.2182
                    Surrogate loss: 0.0033
             Mean action noise std: 0.99
                       Mean reward: 106.12
               Mean episode length: 253.59
                 Mean success rate: 0.00
                  Mean reward/step: 0.44
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 4.39s
                        Total time: 341.57s
                               ETA: 17640.4s

################################################################################
                      [1m Learning iteration 76/4000 [0m

                       Computation: 1820 steps/s (collection: 0.598s, learning 3.903s)
               Value function loss: 23.3900
                    Surrogate loss: 0.0030
             Mean action noise std: 0.99
                       Mean reward: 113.72
               Mean episode length: 271.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 630784
                    Iteration time: 4.50s
                        Total time: 346.07s
                               ETA: 17636.2s

################################################################################
                      [1m Learning iteration 77/4000 [0m

                       Computation: 1805 steps/s (collection: 0.599s, learning 3.937s)
               Value function loss: 40.4086
                    Surrogate loss: 0.0055
             Mean action noise std: 0.99
                       Mean reward: 121.17
               Mean episode length: 285.23
                 Mean success rate: 0.00
                  Mean reward/step: 0.46
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 4.54s
                        Total time: 350.61s
                               ETA: 17633.8s

################################################################################
                      [1m Learning iteration 78/4000 [0m

                       Computation: 1850 steps/s (collection: 0.529s, learning 3.897s)
               Value function loss: 29.7565
                    Surrogate loss: 0.0044
             Mean action noise std: 0.99
                       Mean reward: 123.19
               Mean episode length: 291.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 647168
                    Iteration time: 4.43s
                        Total time: 355.03s
                               ETA: 17625.9s

################################################################################
                      [1m Learning iteration 79/4000 [0m

                       Computation: 1846 steps/s (collection: 0.570s, learning 3.868s)
               Value function loss: 32.1904
                    Surrogate loss: 0.0033
             Mean action noise std: 0.99
                       Mean reward: 124.53
               Mean episode length: 294.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 4.44s
                        Total time: 359.47s
                               ETA: 17618.6s

################################################################################
                      [1m Learning iteration 80/4000 [0m

                       Computation: 1832 steps/s (collection: 0.583s, learning 3.887s)
               Value function loss: 16.4067
                    Surrogate loss: 0.0041
             Mean action noise std: 0.99
                       Mean reward: 127.89
               Mean episode length: 299.26
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 663552
                    Iteration time: 4.47s
                        Total time: 363.94s
                               ETA: 17613.0s

################################################################################
                      [1m Learning iteration 81/4000 [0m

                       Computation: 1856 steps/s (collection: 0.561s, learning 3.851s)
               Value function loss: 20.7226
                    Surrogate loss: 0.0032
             Mean action noise std: 0.99
                       Mean reward: 132.14
               Mean episode length: 308.76
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 4.41s
                        Total time: 368.35s
                               ETA: 17604.6s

################################################################################
                      [1m Learning iteration 82/4000 [0m

                       Computation: 1837 steps/s (collection: 0.571s, learning 3.887s)
               Value function loss: 37.7945
                    Surrogate loss: 0.0047
             Mean action noise std: 0.99
                       Mean reward: 138.56
               Mean episode length: 320.06
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 679936
                    Iteration time: 4.46s
                        Total time: 372.81s
                               ETA: 17598.5s

################################################################################
                      [1m Learning iteration 83/4000 [0m

                       Computation: 1850 steps/s (collection: 0.562s, learning 3.866s)
               Value function loss: 34.7581
                    Surrogate loss: 0.0050
             Mean action noise std: 0.99
                       Mean reward: 136.70
               Mean episode length: 310.24
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 4.43s
                        Total time: 377.24s
                               ETA: 17591.1s

################################################################################
                      [1m Learning iteration 84/4000 [0m

                       Computation: 1846 steps/s (collection: 0.578s, learning 3.859s)
               Value function loss: 33.5350
                    Surrogate loss: 0.0048
             Mean action noise std: 0.99
                       Mean reward: 143.13
               Mean episode length: 320.08
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 696320
                    Iteration time: 4.44s
                        Total time: 381.68s
                               ETA: 17584.1s

################################################################################
                      [1m Learning iteration 85/4000 [0m

                       Computation: 1848 steps/s (collection: 0.564s, learning 3.868s)
               Value function loss: 33.5545
                    Surrogate loss: 0.0038
             Mean action noise std: 0.99
                       Mean reward: 145.12
               Mean episode length: 320.83
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 4.43s
                        Total time: 386.11s
                               ETA: 17577.0s

################################################################################
                      [1m Learning iteration 86/4000 [0m

                       Computation: 1831 steps/s (collection: 0.570s, learning 3.902s)
               Value function loss: 20.1667
                    Surrogate loss: 0.0026
             Mean action noise std: 0.99
                       Mean reward: 150.09
               Mean episode length: 328.33
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 712704
                    Iteration time: 4.47s
                        Total time: 390.58s
                               ETA: 17571.7s

################################################################################
                      [1m Learning iteration 87/4000 [0m

                       Computation: 1799 steps/s (collection: 0.672s, learning 3.881s)
               Value function loss: 27.2470
                    Surrogate loss: 0.0042
             Mean action noise std: 0.99
                       Mean reward: 155.66
               Mean episode length: 333.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 4.55s
                        Total time: 395.13s
                               ETA: 17570.0s

################################################################################
                      [1m Learning iteration 88/4000 [0m

                       Computation: 1832 steps/s (collection: 0.572s, learning 3.899s)
               Value function loss: 30.8169
                    Surrogate loss: 0.0031
             Mean action noise std: 1.00
                       Mean reward: 149.93
               Mean episode length: 321.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 729088
                    Iteration time: 4.47s
                        Total time: 399.60s
                               ETA: 17564.7s

################################################################################
                      [1m Learning iteration 89/4000 [0m

                       Computation: 1844 steps/s (collection: 0.577s, learning 3.865s)
               Value function loss: 27.9726
                    Surrogate loss: 0.0026
             Mean action noise std: 1.00
                       Mean reward: 147.57
               Mean episode length: 314.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 4.44s
                        Total time: 404.05s
                               ETA: 17558.1s

################################################################################
                      [1m Learning iteration 90/4000 [0m

                       Computation: 1817 steps/s (collection: 0.640s, learning 3.866s)
               Value function loss: 34.8305
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 130.64
               Mean episode length: 279.16
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 745472
                    Iteration time: 4.51s
                        Total time: 408.55s
                               ETA: 17554.3s

################################################################################
                      [1m Learning iteration 91/4000 [0m

                       Computation: 1827 steps/s (collection: 0.594s, learning 3.888s)
               Value function loss: 19.9737
                    Surrogate loss: 0.0023
             Mean action noise std: 1.00
                       Mean reward: 131.80
               Mean episode length: 281.19
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 4.48s
                        Total time: 413.03s
                               ETA: 17549.5s

################################################################################
                      [1m Learning iteration 92/4000 [0m

                       Computation: 1840 steps/s (collection: 0.565s, learning 3.886s)
               Value function loss: 32.7612
                    Surrogate loss: 0.0040
             Mean action noise std: 1.00
                       Mean reward: 128.29
               Mean episode length: 272.14
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 761856
                    Iteration time: 4.45s
                        Total time: 417.49s
                               ETA: 17543.4s

################################################################################
                      [1m Learning iteration 93/4000 [0m

                       Computation: 1826 steps/s (collection: 0.616s, learning 3.868s)
               Value function loss: 43.6844
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 129.24
               Mean episode length: 271.35
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 4.48s
                        Total time: 421.97s
                               ETA: 17538.7s

################################################################################
                      [1m Learning iteration 94/4000 [0m

                       Computation: 1833 steps/s (collection: 0.585s, learning 3.882s)
               Value function loss: 44.6472
                    Surrogate loss: 0.0035
             Mean action noise std: 1.00
                       Mean reward: 128.18
               Mean episode length: 266.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 778240
                    Iteration time: 4.47s
                        Total time: 426.44s
                               ETA: 17533.3s

################################################################################
                      [1m Learning iteration 95/4000 [0m

                       Computation: 1819 steps/s (collection: 0.628s, learning 3.874s)
               Value function loss: 47.3784
                    Surrogate loss: 0.0033
             Mean action noise std: 1.00
                       Mean reward: 131.25
               Mean episode length: 269.58
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 4.50s
                        Total time: 430.94s
                               ETA: 17529.3s

################################################################################
                      [1m Learning iteration 96/4000 [0m

                       Computation: 1836 steps/s (collection: 0.569s, learning 3.893s)
               Value function loss: 37.8203
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 129.16
               Mean episode length: 261.38
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 794624
                    Iteration time: 4.46s
                        Total time: 435.40s
                               ETA: 17523.8s

################################################################################
                      [1m Learning iteration 97/4000 [0m

                       Computation: 1821 steps/s (collection: 0.624s, learning 3.873s)
               Value function loss: 46.6734
                    Surrogate loss: 0.0039
             Mean action noise std: 1.00
                       Mean reward: 120.05
               Mean episode length: 238.40
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 4.50s
                        Total time: 439.90s
                               ETA: 17519.6s

################################################################################
                      [1m Learning iteration 98/4000 [0m

                       Computation: 1822 steps/s (collection: 0.582s, learning 3.914s)
               Value function loss: 45.0862
                    Surrogate loss: 0.0033
             Mean action noise std: 1.00
                       Mean reward: 115.28
               Mean episode length: 223.80
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 811008
                    Iteration time: 4.50s
                        Total time: 444.39s
                               ETA: 17515.3s

################################################################################
                      [1m Learning iteration 99/4000 [0m

                       Computation: 1824 steps/s (collection: 0.630s, learning 3.860s)
               Value function loss: 30.8594
                    Surrogate loss: 0.0025
             Mean action noise std: 1.00
                       Mean reward: 113.22
               Mean episode length: 220.72
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 4.49s
                        Total time: 448.88s
                               ETA: 17510.9s

################################################################################
                     [1m Learning iteration 100/4000 [0m

                       Computation: 1853 steps/s (collection: 0.556s, learning 3.863s)
               Value function loss: 38.9183
                    Surrogate loss: 0.0039
             Mean action noise std: 1.00
                       Mean reward: 115.07
               Mean episode length: 220.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 827392
                    Iteration time: 4.42s
                        Total time: 453.30s
                               ETA: 17503.7s

################################################################################
                     [1m Learning iteration 101/4000 [0m

                       Computation: 1858 steps/s (collection: 0.562s, learning 3.847s)
               Value function loss: 22.9143
                    Surrogate loss: 0.0042
             Mean action noise std: 1.00
                       Mean reward: 117.25
               Mean episode length: 221.04
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 4.41s
                        Total time: 457.71s
                               ETA: 17496.2s

################################################################################
                     [1m Learning iteration 102/4000 [0m

                       Computation: 1833 steps/s (collection: 0.543s, learning 3.924s)
               Value function loss: 35.0982
                    Surrogate loss: 0.0038
             Mean action noise std: 1.00
                       Mean reward: 118.23
               Mean episode length: 222.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 843776
                    Iteration time: 4.47s
                        Total time: 462.18s
                               ETA: 17490.9s

################################################################################
                     [1m Learning iteration 103/4000 [0m

                       Computation: 1818 steps/s (collection: 0.645s, learning 3.859s)
               Value function loss: 37.1772
                    Surrogate loss: 0.0027
             Mean action noise std: 1.00
                       Mean reward: 125.61
               Mean episode length: 235.85
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 4.50s
                        Total time: 466.68s
                               ETA: 17487.1s

################################################################################
                     [1m Learning iteration 104/4000 [0m

                       Computation: 1862 steps/s (collection: 0.552s, learning 3.846s)
               Value function loss: 44.2714
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 132.29
               Mean episode length: 247.48
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 860160
                    Iteration time: 4.40s
                        Total time: 471.08s
                               ETA: 17479.3s

################################################################################
                     [1m Learning iteration 105/4000 [0m

                       Computation: 1814 steps/s (collection: 0.615s, learning 3.899s)
               Value function loss: 50.5416
                    Surrogate loss: 0.0038
             Mean action noise std: 1.00
                       Mean reward: 124.16
               Mean episode length: 228.98
                 Mean success rate: 0.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 4.51s
                        Total time: 475.59s
                               ETA: 17475.8s

################################################################################
                     [1m Learning iteration 106/4000 [0m

                       Computation: 1805 steps/s (collection: 0.630s, learning 3.907s)
               Value function loss: 51.0165
                    Surrogate loss: 0.0034
             Mean action noise std: 1.00
                       Mean reward: 121.13
               Mean episode length: 219.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 876544
                    Iteration time: 4.54s
                        Total time: 480.13s
                               ETA: 17473.1s

################################################################################
                     [1m Learning iteration 107/4000 [0m

                       Computation: 1802 steps/s (collection: 0.654s, learning 3.890s)
               Value function loss: 47.7720
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 121.11
               Mean episode length: 215.31
                 Mean success rate: 0.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 4.54s
                        Total time: 484.67s
                               ETA: 17470.7s

################################################################################
                     [1m Learning iteration 108/4000 [0m

                       Computation: 1771 steps/s (collection: 0.689s, learning 3.935s)
               Value function loss: 55.8526
                    Surrogate loss: 0.0031
             Mean action noise std: 1.00
                       Mean reward: 120.17
               Mean episode length: 205.77
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 892928
                    Iteration time: 4.62s
                        Total time: 489.30s
                               ETA: 17471.1s

################################################################################
                     [1m Learning iteration 109/4000 [0m

                       Computation: 1794 steps/s (collection: 0.670s, learning 3.894s)
               Value function loss: 32.1892
                    Surrogate loss: 0.0039
             Mean action noise std: 1.00
                       Mean reward: 121.19
               Mean episode length: 208.06
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 4.56s
                        Total time: 493.86s
                               ETA: 17469.2s

################################################################################
                     [1m Learning iteration 110/4000 [0m

                       Computation: 1793 steps/s (collection: 0.644s, learning 3.924s)
               Value function loss: 52.8814
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 130.76
               Mean episode length: 222.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 909312
                    Iteration time: 4.57s
                        Total time: 498.43s
                               ETA: 17467.5s

################################################################################
                     [1m Learning iteration 111/4000 [0m

                       Computation: 1847 steps/s (collection: 0.560s, learning 3.875s)
               Value function loss: 37.5059
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 135.51
               Mean episode length: 231.38
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 4.44s
                        Total time: 502.87s
                               ETA: 17461.1s

################################################################################
                     [1m Learning iteration 112/4000 [0m

                       Computation: 1817 steps/s (collection: 0.588s, learning 3.920s)
               Value function loss: 32.5295
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 142.63
               Mean episode length: 244.54
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 925696
                    Iteration time: 4.51s
                        Total time: 507.37s
                               ETA: 17457.2s

################################################################################
                     [1m Learning iteration 113/4000 [0m

                       Computation: 1790 steps/s (collection: 0.636s, learning 3.939s)
               Value function loss: 37.5056
                    Surrogate loss: 0.0034
             Mean action noise std: 1.00
                       Mean reward: 145.72
               Mean episode length: 250.41
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 4.57s
                        Total time: 511.95s
                               ETA: 17455.6s

################################################################################
                     [1m Learning iteration 114/4000 [0m

                       Computation: 1781 steps/s (collection: 0.663s, learning 3.936s)
               Value function loss: 29.6578
                    Surrogate loss: 0.0039
             Mean action noise std: 1.00
                       Mean reward: 143.93
               Mean episode length: 251.76
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 942080
                    Iteration time: 4.60s
                        Total time: 516.55s
                               ETA: 17454.8s

################################################################################
                     [1m Learning iteration 115/4000 [0m

                       Computation: 1808 steps/s (collection: 0.597s, learning 3.931s)
               Value function loss: 34.8382
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 132.73
               Mean episode length: 234.85
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 4.53s
                        Total time: 521.08s
                               ETA: 17451.6s

################################################################################
                     [1m Learning iteration 116/4000 [0m

                       Computation: 1804 steps/s (collection: 0.590s, learning 3.951s)
               Value function loss: 29.3270
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 128.60
               Mean episode length: 228.70
                 Mean success rate: 0.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 958464
                    Iteration time: 4.54s
                        Total time: 525.62s
                               ETA: 17448.7s

################################################################################
                     [1m Learning iteration 117/4000 [0m

                       Computation: 1830 steps/s (collection: 0.570s, learning 3.906s)
               Value function loss: 39.1947
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 123.50
               Mean episode length: 221.60
                 Mean success rate: 0.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 4.48s
                        Total time: 530.09s
                               ETA: 17443.6s

################################################################################
                     [1m Learning iteration 118/4000 [0m

                       Computation: 1780 steps/s (collection: 0.637s, learning 3.964s)
               Value function loss: 37.1956
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 122.03
               Mean episode length: 213.42
                 Mean success rate: 0.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 974848
                    Iteration time: 4.60s
                        Total time: 534.69s
                               ETA: 17442.7s

################################################################################
                     [1m Learning iteration 119/4000 [0m

                       Computation: 1790 steps/s (collection: 0.628s, learning 3.948s)
               Value function loss: 27.4851
                    Surrogate loss: 0.0040
             Mean action noise std: 1.00
                       Mean reward: 121.86
               Mean episode length: 212.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 4.58s
                        Total time: 539.27s
                               ETA: 17440.9s

################################################################################
                     [1m Learning iteration 120/4000 [0m

                       Computation: 1811 steps/s (collection: 0.607s, learning 3.917s)
               Value function loss: 35.3329
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 130.56
               Mean episode length: 227.31
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 991232
                    Iteration time: 4.52s
                        Total time: 543.79s
                               ETA: 17437.4s

################################################################################
                     [1m Learning iteration 121/4000 [0m

                       Computation: 1788 steps/s (collection: 0.626s, learning 3.954s)
               Value function loss: 42.5387
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 136.56
               Mean episode length: 237.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 4.58s
                        Total time: 548.37s
                               ETA: 17435.6s

################################################################################
                     [1m Learning iteration 122/4000 [0m

                       Computation: 1813 steps/s (collection: 0.545s, learning 3.972s)
               Value function loss: 28.1719
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 145.49
               Mean episode length: 250.36
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 1007616
                    Iteration time: 4.52s
                        Total time: 552.89s
                               ETA: 17431.8s

################################################################################
                     [1m Learning iteration 123/4000 [0m

                       Computation: 1779 steps/s (collection: 0.647s, learning 3.956s)
               Value function loss: 34.9319
                    Surrogate loss: 0.0064
             Mean action noise std: 1.00
                       Mean reward: 149.49
               Mean episode length: 253.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 4.60s
                        Total time: 557.49s
                               ETA: 17430.7s

################################################################################
                     [1m Learning iteration 124/4000 [0m

                       Computation: 1809 steps/s (collection: 0.602s, learning 3.926s)
               Value function loss: 37.6287
                    Surrogate loss: 0.0021
             Mean action noise std: 1.00
                       Mean reward: 160.02
               Mean episode length: 273.25
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1024000
                    Iteration time: 4.53s
                        Total time: 562.02s
                               ETA: 17427.2s

################################################################################
                     [1m Learning iteration 125/4000 [0m

                       Computation: 1753 steps/s (collection: 0.704s, learning 3.968s)
               Value function loss: 39.8609
                    Surrogate loss: 0.0038
             Mean action noise std: 1.00
                       Mean reward: 169.54
               Mean episode length: 285.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 4.67s
                        Total time: 566.69s
                               ETA: 17428.1s

################################################################################
                     [1m Learning iteration 126/4000 [0m

                       Computation: 1832 steps/s (collection: 0.583s, learning 3.886s)
               Value function loss: 61.0716
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 185.36
               Mean episode length: 312.81
                 Mean success rate: 0.00
                  Mean reward/step: 0.60
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1040384
                    Iteration time: 4.47s
                        Total time: 571.16s
                               ETA: 17422.8s

################################################################################
                     [1m Learning iteration 127/4000 [0m

                       Computation: 1854 steps/s (collection: 0.548s, learning 3.868s)
               Value function loss: 71.2957
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 171.62
               Mean episode length: 290.38
                 Mean success rate: 0.00
                  Mean reward/step: 0.60
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 4.42s
                        Total time: 575.58s
                               ETA: 17415.8s

################################################################################
                     [1m Learning iteration 128/4000 [0m

                       Computation: 1812 steps/s (collection: 0.624s, learning 3.895s)
               Value function loss: 69.1488
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 159.15
               Mean episode length: 273.41
                 Mean success rate: 0.00
                  Mean reward/step: 0.62
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 1056768
                    Iteration time: 4.52s
                        Total time: 580.10s
                               ETA: 17412.0s

################################################################################
                     [1m Learning iteration 129/4000 [0m

                       Computation: 1845 steps/s (collection: 0.559s, learning 3.881s)
               Value function loss: 57.3520
                    Surrogate loss: 0.0040
             Mean action noise std: 1.00
                       Mean reward: 132.99
               Mean episode length: 234.22
                 Mean success rate: 0.00
                  Mean reward/step: 0.61
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 4.44s
                        Total time: 584.54s
                               ETA: 17405.8s

################################################################################
                     [1m Learning iteration 130/4000 [0m

                       Computation: 1832 steps/s (collection: 0.599s, learning 3.870s)
               Value function loss: 68.1240
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 102.43
               Mean episode length: 184.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.61
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 1073152
                    Iteration time: 4.47s
                        Total time: 589.01s
                               ETA: 17400.5s

################################################################################
                     [1m Learning iteration 131/4000 [0m

                       Computation: 1831 steps/s (collection: 0.589s, learning 3.885s)
               Value function loss: 36.1042
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 109.14
               Mean episode length: 192.46
                 Mean success rate: 0.00
                  Mean reward/step: 0.61
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 4.47s
                        Total time: 593.48s
                               ETA: 17395.3s

################################################################################
                     [1m Learning iteration 132/4000 [0m

                       Computation: 1836 steps/s (collection: 0.596s, learning 3.864s)
               Value function loss: 34.1123
                    Surrogate loss: 0.0042
             Mean action noise std: 1.00
                       Mean reward: 113.74
               Mean episode length: 199.66
                 Mean success rate: 0.00
                  Mean reward/step: 0.61
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1089536
                    Iteration time: 4.46s
                        Total time: 597.94s
                               ETA: 17389.8s

################################################################################
                     [1m Learning iteration 133/4000 [0m

                       Computation: 1815 steps/s (collection: 0.613s, learning 3.899s)
               Value function loss: 27.3650
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 109.64
               Mean episode length: 193.06
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 4.51s
                        Total time: 602.46s
                               ETA: 17385.8s

################################################################################
                     [1m Learning iteration 134/4000 [0m

                       Computation: 1819 steps/s (collection: 0.589s, learning 3.914s)
               Value function loss: 35.8433
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 119.00
               Mean episode length: 212.71
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1105920
                    Iteration time: 4.50s
                        Total time: 606.96s
                               ETA: 17381.5s

################################################################################
                     [1m Learning iteration 135/4000 [0m

                       Computation: 1821 steps/s (collection: 0.588s, learning 3.910s)
               Value function loss: 19.1369
                    Surrogate loss: 0.0032
             Mean action noise std: 1.00
                       Mean reward: 124.84
               Mean episode length: 220.19
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 4.50s
                        Total time: 611.46s
                               ETA: 17377.0s

################################################################################
                     [1m Learning iteration 136/4000 [0m

                       Computation: 1823 steps/s (collection: 0.605s, learning 3.886s)
               Value function loss: 30.6555
                    Surrogate loss: 0.0020
             Mean action noise std: 1.00
                       Mean reward: 134.34
               Mean episode length: 233.96
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1122304
                    Iteration time: 4.49s
                        Total time: 615.95s
                               ETA: 17372.4s

################################################################################
                     [1m Learning iteration 137/4000 [0m

                       Computation: 1811 steps/s (collection: 0.618s, learning 3.905s)
               Value function loss: 49.2233
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 145.27
               Mean episode length: 250.26
                 Mean success rate: 0.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 4.52s
                        Total time: 620.47s
                               ETA: 17368.7s

################################################################################
                     [1m Learning iteration 138/4000 [0m

                       Computation: 1811 steps/s (collection: 0.626s, learning 3.896s)
               Value function loss: 40.3800
                    Surrogate loss: 0.0041
             Mean action noise std: 1.00
                       Mean reward: 156.35
               Mean episode length: 266.14
                 Mean success rate: 0.00
                  Mean reward/step: 0.61
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1138688
                    Iteration time: 4.52s
                        Total time: 624.99s
                               ETA: 17364.9s

################################################################################
                     [1m Learning iteration 139/4000 [0m

                       Computation: 1806 steps/s (collection: 0.623s, learning 3.912s)
               Value function loss: 51.2998
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 165.37
               Mean episode length: 284.45
                 Mean success rate: 0.00
                  Mean reward/step: 0.61
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 4.53s
                        Total time: 629.53s
                               ETA: 17361.4s

################################################################################
                     [1m Learning iteration 140/4000 [0m

                       Computation: 1777 steps/s (collection: 0.709s, learning 3.899s)
               Value function loss: 50.2896
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 157.42
               Mean episode length: 273.72
                 Mean success rate: 0.00
                  Mean reward/step: 0.62
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1155072
                    Iteration time: 4.61s
                        Total time: 634.13s
                               ETA: 17360.0s

################################################################################
                     [1m Learning iteration 141/4000 [0m

                       Computation: 1790 steps/s (collection: 0.664s, learning 3.911s)
               Value function loss: 47.4160
                    Surrogate loss: 0.0037
             Mean action noise std: 0.99
                       Mean reward: 160.42
               Mean episode length: 276.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.64
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 4.57s
                        Total time: 638.71s
                               ETA: 17357.6s

################################################################################
                     [1m Learning iteration 142/4000 [0m

                       Computation: 1800 steps/s (collection: 0.650s, learning 3.900s)
               Value function loss: 54.3179
                    Surrogate loss: 0.0029
             Mean action noise std: 0.99
                       Mean reward: 156.18
               Mean episode length: 265.07
                 Mean success rate: 0.00
                  Mean reward/step: 0.66
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1171456
                    Iteration time: 4.55s
                        Total time: 643.26s
                               ETA: 17354.5s

################################################################################
                     [1m Learning iteration 143/4000 [0m

                       Computation: 1818 steps/s (collection: 0.635s, learning 3.869s)
               Value function loss: 63.1920
                    Surrogate loss: 0.0062
             Mean action noise std: 0.99
                       Mean reward: 152.52
               Mean episode length: 258.36
                 Mean success rate: 0.00
                  Mean reward/step: 0.69
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 4.50s
                        Total time: 647.76s
                               ETA: 17350.2s

################################################################################
                     [1m Learning iteration 144/4000 [0m

                       Computation: 1799 steps/s (collection: 0.651s, learning 3.902s)
               Value function loss: 69.4868
                    Surrogate loss: 0.0050
             Mean action noise std: 0.99
                       Mean reward: 153.87
               Mean episode length: 258.65
                 Mean success rate: 0.00
                  Mean reward/step: 0.71
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1187840
                    Iteration time: 4.55s
                        Total time: 652.32s
                               ETA: 17347.1s

################################################################################
                     [1m Learning iteration 145/4000 [0m

                       Computation: 1773 steps/s (collection: 0.703s, learning 3.917s)
               Value function loss: 81.1355
                    Surrogate loss: 0.0055
             Mean action noise std: 0.99
                       Mean reward: 167.25
               Mean episode length: 267.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.70
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 4.62s
                        Total time: 656.94s
                               ETA: 17345.8s

################################################################################
                     [1m Learning iteration 146/4000 [0m

                       Computation: 1806 steps/s (collection: 0.620s, learning 3.916s)
               Value function loss: 67.9888
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 176.78
               Mean episode length: 277.81
                 Mean success rate: 0.00
                  Mean reward/step: 0.69
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1204224
                    Iteration time: 4.54s
                        Total time: 661.47s
                               ETA: 17342.3s

################################################################################
                     [1m Learning iteration 147/4000 [0m

                       Computation: 1794 steps/s (collection: 0.660s, learning 3.905s)
               Value function loss: 42.3299
                    Surrogate loss: 0.0041
             Mean action noise std: 1.00
                       Mean reward: 179.97
               Mean episode length: 281.17
                 Mean success rate: 0.00
                  Mean reward/step: 0.70
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 4.57s
                        Total time: 666.04s
                               ETA: 17339.5s

################################################################################
                     [1m Learning iteration 148/4000 [0m

                       Computation: 1806 steps/s (collection: 0.644s, learning 3.890s)
               Value function loss: 50.9685
                    Surrogate loss: 0.0036
             Mean action noise std: 0.99
                       Mean reward: 174.28
               Mean episode length: 269.96
                 Mean success rate: 0.00
                  Mean reward/step: 0.70
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1220608
                    Iteration time: 4.53s
                        Total time: 670.57s
                               ETA: 17335.9s

################################################################################
                     [1m Learning iteration 149/4000 [0m

                       Computation: 1806 steps/s (collection: 0.635s, learning 3.899s)
               Value function loss: 52.6201
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 152.51
               Mean episode length: 238.24
                 Mean success rate: 0.00
                  Mean reward/step: 0.71
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 4.53s
                        Total time: 675.11s
                               ETA: 17332.2s

################################################################################
                     [1m Learning iteration 150/4000 [0m

                       Computation: 1795 steps/s (collection: 0.663s, learning 3.900s)
               Value function loss: 48.6580
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 150.83
               Mean episode length: 232.82
                 Mean success rate: 0.00
                  Mean reward/step: 0.71
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1236992
                    Iteration time: 4.56s
                        Total time: 679.67s
                               ETA: 17329.3s

################################################################################
                     [1m Learning iteration 151/4000 [0m

                       Computation: 1794 steps/s (collection: 0.670s, learning 3.895s)
               Value function loss: 70.2452
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 130.86
               Mean episode length: 201.16
                 Mean success rate: 0.00
                  Mean reward/step: 0.74
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 4.57s
                        Total time: 684.23s
                               ETA: 17326.4s

################################################################################
                     [1m Learning iteration 152/4000 [0m

                       Computation: 1808 steps/s (collection: 0.635s, learning 3.895s)
               Value function loss: 50.0792
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 124.29
               Mean episode length: 186.65
                 Mean success rate: 0.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1253376
                    Iteration time: 4.53s
                        Total time: 688.76s
                               ETA: 17322.6s

################################################################################
                     [1m Learning iteration 153/4000 [0m

                       Computation: 1791 steps/s (collection: 0.646s, learning 3.927s)
               Value function loss: 57.6382
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 131.64
               Mean episode length: 189.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.72
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 4.57s
                        Total time: 693.34s
                               ETA: 17319.9s

################################################################################
                     [1m Learning iteration 154/4000 [0m

                       Computation: 1777 steps/s (collection: 0.701s, learning 3.909s)
               Value function loss: 74.5084
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 134.75
               Mean episode length: 191.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.72
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1269760
                    Iteration time: 4.61s
                        Total time: 697.95s
                               ETA: 17318.1s

################################################################################
                     [1m Learning iteration 155/4000 [0m

                       Computation: 1792 steps/s (collection: 0.644s, learning 3.925s)
               Value function loss: 84.2772
                    Surrogate loss: 0.0089
             Mean action noise std: 1.00
                       Mean reward: 135.77
               Mean episode length: 191.50
                 Mean success rate: 0.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 4.57s
                        Total time: 702.52s
                               ETA: 17315.2s

################################################################################
                     [1m Learning iteration 156/4000 [0m

                       Computation: 1783 steps/s (collection: 0.653s, learning 3.940s)
               Value function loss: 69.6014
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 141.23
               Mean episode length: 198.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.72
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 1286144
                    Iteration time: 4.59s
                        Total time: 707.11s
                               ETA: 17312.9s

################################################################################
                     [1m Learning iteration 157/4000 [0m

                       Computation: 1730 steps/s (collection: 0.721s, learning 4.012s)
               Value function loss: 48.3235
                    Surrogate loss: 0.0061
             Mean action noise std: 1.00
                       Mean reward: 145.02
               Mean episode length: 204.61
                 Mean success rate: 0.00
                  Mean reward/step: 0.71
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 4.73s
                        Total time: 711.84s
                               ETA: 17314.0s

################################################################################
                     [1m Learning iteration 158/4000 [0m

                       Computation: 1803 steps/s (collection: 0.649s, learning 3.892s)
               Value function loss: 46.7175
                    Surrogate loss: 0.0068
             Mean action noise std: 1.00
                       Mean reward: 141.81
               Mean episode length: 205.16
                 Mean success rate: 0.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1302528
                    Iteration time: 4.54s
                        Total time: 716.38s
                               ETA: 17310.4s

################################################################################
                     [1m Learning iteration 159/4000 [0m

                       Computation: 1814 steps/s (collection: 0.632s, learning 3.882s)
               Value function loss: 73.8207
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 154.19
               Mean episode length: 221.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 4.51s
                        Total time: 720.90s
                               ETA: 17306.1s

################################################################################
                     [1m Learning iteration 160/4000 [0m

                       Computation: 1788 steps/s (collection: 0.689s, learning 3.891s)
               Value function loss: 85.9534
                    Surrogate loss: 0.0036
             Mean action noise std: 1.00
                       Mean reward: 165.78
               Mean episode length: 232.24
                 Mean success rate: 0.00
                  Mean reward/step: 0.74
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1318912
                    Iteration time: 4.58s
                        Total time: 725.48s
                               ETA: 17303.4s

################################################################################
                     [1m Learning iteration 161/4000 [0m

                       Computation: 1810 steps/s (collection: 0.601s, learning 3.924s)
               Value function loss: 65.2310
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 165.34
               Mean episode length: 231.42
                 Mean success rate: 0.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 4.53s
                        Total time: 730.00s
                               ETA: 17299.3s

################################################################################
                     [1m Learning iteration 162/4000 [0m

                       Computation: 1822 steps/s (collection: 0.630s, learning 3.865s)
               Value function loss: 58.8203
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 174.01
               Mean episode length: 244.07
                 Mean success rate: 0.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1335296
                    Iteration time: 4.50s
                        Total time: 734.50s
                               ETA: 17294.5s

################################################################################
                     [1m Learning iteration 163/4000 [0m

                       Computation: 1829 steps/s (collection: 0.603s, learning 3.874s)
               Value function loss: 38.2040
                    Surrogate loss: 0.0087
             Mean action noise std: 1.00
                       Mean reward: 175.41
               Mean episode length: 245.09
                 Mean success rate: 0.00
                  Mean reward/step: 0.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 4.48s
                        Total time: 738.98s
                               ETA: 17289.3s

################################################################################
                     [1m Learning iteration 164/4000 [0m

                       Computation: 1824 steps/s (collection: 0.624s, learning 3.865s)
               Value function loss: 72.0139
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 181.41
               Mean episode length: 248.24
                 Mean success rate: 0.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1351680
                    Iteration time: 4.49s
                        Total time: 743.47s
                               ETA: 17284.4s

################################################################################
                     [1m Learning iteration 165/4000 [0m

                       Computation: 1808 steps/s (collection: 0.627s, learning 3.903s)
               Value function loss: 52.6702
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 178.34
               Mean episode length: 243.54
                 Mean success rate: 0.00
                  Mean reward/step: 0.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 4.53s
                        Total time: 748.00s
                               ETA: 17280.5s

################################################################################
                     [1m Learning iteration 166/4000 [0m

                       Computation: 1784 steps/s (collection: 0.645s, learning 3.945s)
               Value function loss: 39.1591
                    Surrogate loss: 0.0063
             Mean action noise std: 1.00
                       Mean reward: 174.60
               Mean episode length: 238.81
                 Mean success rate: 0.00
                  Mean reward/step: 0.77
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 1368064
                    Iteration time: 4.59s
                        Total time: 752.59s
                               ETA: 17277.9s

################################################################################
                     [1m Learning iteration 167/4000 [0m

                       Computation: 1806 steps/s (collection: 0.641s, learning 3.895s)
               Value function loss: 74.7368
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 191.88
               Mean episode length: 258.68
                 Mean success rate: 0.00
                  Mean reward/step: 0.77
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 4.54s
                        Total time: 757.12s
                               ETA: 17274.1s

################################################################################
                     [1m Learning iteration 168/4000 [0m

                       Computation: 1809 steps/s (collection: 0.640s, learning 3.887s)
               Value function loss: 55.4243
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 190.36
               Mean episode length: 257.04
                 Mean success rate: 0.00
                  Mean reward/step: 0.77
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1384448
                    Iteration time: 4.53s
                        Total time: 761.65s
                               ETA: 17270.1s

################################################################################
                     [1m Learning iteration 169/4000 [0m

                       Computation: 1833 steps/s (collection: 0.590s, learning 3.877s)
               Value function loss: 67.0831
                    Surrogate loss: 0.0040
             Mean action noise std: 1.00
                       Mean reward: 193.55
               Mean episode length: 259.86
                 Mean success rate: 0.00
                  Mean reward/step: 0.78
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 4.47s
                        Total time: 766.12s
                               ETA: 17264.7s

################################################################################
                     [1m Learning iteration 170/4000 [0m

                       Computation: 1772 steps/s (collection: 0.680s, learning 3.943s)
               Value function loss: 90.6063
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 202.26
               Mean episode length: 269.07
                 Mean success rate: 0.00
                  Mean reward/step: 0.76
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1400832
                    Iteration time: 4.62s
                        Total time: 770.74s
                               ETA: 17262.7s

################################################################################
                     [1m Learning iteration 171/4000 [0m

                       Computation: 1753 steps/s (collection: 0.622s, learning 4.049s)
               Value function loss: 83.7794
                    Surrogate loss: 0.0076
             Mean action noise std: 1.00
                       Mean reward: 206.61
               Mean episode length: 276.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.76
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 4.67s
                        Total time: 775.41s
                               ETA: 17261.9s

################################################################################
                     [1m Learning iteration 172/4000 [0m

                       Computation: 1765 steps/s (collection: 0.710s, learning 3.930s)
               Value function loss: 101.5731
                    Surrogate loss: 0.0053
             Mean action noise std: 1.00
                       Mean reward: 205.77
               Mean episode length: 273.08
                 Mean success rate: 0.00
                  Mean reward/step: 0.76
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1417216
                    Iteration time: 4.64s
                        Total time: 780.05s
                               ETA: 17260.3s

################################################################################
                     [1m Learning iteration 173/4000 [0m

                       Computation: 1721 steps/s (collection: 0.758s, learning 4.002s)
               Value function loss: 61.9318
                    Surrogate loss: 0.0072
             Mean action noise std: 1.00
                       Mean reward: 192.61
               Mean episode length: 255.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 4.76s
                        Total time: 784.81s
                               ETA: 17261.3s

################################################################################
                     [1m Learning iteration 174/4000 [0m

                       Computation: 1778 steps/s (collection: 0.625s, learning 3.982s)
               Value function loss: 80.5244
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 185.52
               Mean episode length: 246.81
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1433600
                    Iteration time: 4.61s
                        Total time: 789.42s
                               ETA: 17258.9s

################################################################################
                     [1m Learning iteration 175/4000 [0m

                       Computation: 1741 steps/s (collection: 0.789s, learning 3.916s)
               Value function loss: 72.9193
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 185.38
               Mean episode length: 249.58
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 4.70s
                        Total time: 794.12s
                               ETA: 17258.6s

################################################################################
                     [1m Learning iteration 176/4000 [0m

                       Computation: 1803 steps/s (collection: 0.617s, learning 3.925s)
               Value function loss: 48.6993
                    Surrogate loss: 0.0076
             Mean action noise std: 1.00
                       Mean reward: 189.40
               Mean episode length: 252.87
                 Mean success rate: 0.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1449984
                    Iteration time: 4.54s
                        Total time: 798.66s
                               ETA: 17254.8s

################################################################################
                     [1m Learning iteration 177/4000 [0m

                       Computation: 1804 steps/s (collection: 0.631s, learning 3.910s)
               Value function loss: 73.1992
                    Surrogate loss: 0.0064
             Mean action noise std: 1.00
                       Mean reward: 186.00
               Mean episode length: 245.71
                 Mean success rate: 0.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 4.54s
                        Total time: 803.21s
                               ETA: 17250.9s

################################################################################
                     [1m Learning iteration 178/4000 [0m

                       Computation: 1813 steps/s (collection: 0.617s, learning 3.899s)
               Value function loss: 76.4487
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 178.81
               Mean episode length: 238.53
                 Mean success rate: 0.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1466368
                    Iteration time: 4.52s
                        Total time: 807.72s
                               ETA: 17246.4s

################################################################################
                     [1m Learning iteration 179/4000 [0m

                       Computation: 1840 steps/s (collection: 0.566s, learning 3.884s)
               Value function loss: 67.3739
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 185.74
               Mean episode length: 246.10
                 Mean success rate: 0.00
                  Mean reward/step: 0.83
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 4.45s
                        Total time: 812.17s
                               ETA: 17240.6s

################################################################################
                     [1m Learning iteration 180/4000 [0m

                       Computation: 1854 steps/s (collection: 0.549s, learning 3.869s)
               Value function loss: 80.9702
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 195.87
               Mean episode length: 255.57
                 Mean success rate: 0.00
                  Mean reward/step: 0.82
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1482752
                    Iteration time: 4.42s
                        Total time: 816.59s
                               ETA: 17234.1s

################################################################################
                     [1m Learning iteration 181/4000 [0m

                       Computation: 1845 steps/s (collection: 0.557s, learning 3.883s)
               Value function loss: 75.9643
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 194.56
               Mean episode length: 253.10
                 Mean success rate: 0.00
                  Mean reward/step: 0.83
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 4.44s
                        Total time: 821.03s
                               ETA: 17228.1s

################################################################################
                     [1m Learning iteration 182/4000 [0m

                       Computation: 1840 steps/s (collection: 0.567s, learning 3.884s)
               Value function loss: 57.4119
                    Surrogate loss: 0.0060
             Mean action noise std: 0.99
                       Mean reward: 198.70
               Mean episode length: 255.53
                 Mean success rate: 0.00
                  Mean reward/step: 0.81
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1499136
                    Iteration time: 4.45s
                        Total time: 825.48s
                               ETA: 17222.3s

################################################################################
                     [1m Learning iteration 183/4000 [0m

                       Computation: 1828 steps/s (collection: 0.638s, learning 3.843s)
               Value function loss: 66.4214
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 204.14
               Mean episode length: 257.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 4.48s
                        Total time: 829.96s
                               ETA: 17217.2s

################################################################################
                     [1m Learning iteration 184/4000 [0m

                       Computation: 1831 steps/s (collection: 0.563s, learning 3.908s)
               Value function loss: 71.0261
                    Surrogate loss: 0.0108
             Mean action noise std: 1.00
                       Mean reward: 213.81
               Mean episode length: 269.40
                 Mean success rate: 0.00
                  Mean reward/step: 0.78
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1515520
                    Iteration time: 4.47s
                        Total time: 834.43s
                               ETA: 17211.9s

################################################################################
                     [1m Learning iteration 185/4000 [0m

                       Computation: 1817 steps/s (collection: 0.593s, learning 3.915s)
               Value function loss: 56.4040
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 217.88
               Mean episode length: 272.11
                 Mean success rate: 0.00
                  Mean reward/step: 0.78
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 4.51s
                        Total time: 838.94s
                               ETA: 17207.3s

################################################################################
                     [1m Learning iteration 186/4000 [0m

                       Computation: 1804 steps/s (collection: 0.622s, learning 3.918s)
               Value function loss: 83.8487
                    Surrogate loss: 0.0052
             Mean action noise std: 0.99
                       Mean reward: 234.74
               Mean episode length: 292.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.78
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1531904
                    Iteration time: 4.54s
                        Total time: 843.48s
                               ETA: 17203.4s

################################################################################
                     [1m Learning iteration 187/4000 [0m

                       Computation: 1807 steps/s (collection: 0.645s, learning 3.888s)
               Value function loss: 86.2520
                    Surrogate loss: 0.0070
             Mean action noise std: 0.99
                       Mean reward: 239.20
               Mean episode length: 296.99
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 4.53s
                        Total time: 848.01s
                               ETA: 17199.3s

################################################################################
                     [1m Learning iteration 188/4000 [0m

                       Computation: 1808 steps/s (collection: 0.650s, learning 3.879s)
               Value function loss: 84.1697
                    Surrogate loss: 0.0060
             Mean action noise std: 0.99
                       Mean reward: 231.95
               Mean episode length: 287.35
                 Mean success rate: 0.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1548288
                    Iteration time: 4.53s
                        Total time: 852.54s
                               ETA: 17195.2s

################################################################################
                     [1m Learning iteration 189/4000 [0m

                       Computation: 1862 steps/s (collection: 0.549s, learning 3.849s)
               Value function loss: 51.1270
                    Surrogate loss: 0.0064
             Mean action noise std: 0.99
                       Mean reward: 242.45
               Mean episode length: 299.85
                 Mean success rate: 0.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 4.40s
                        Total time: 856.94s
                               ETA: 17188.4s

################################################################################
                     [1m Learning iteration 190/4000 [0m

                       Computation: 1862 steps/s (collection: 0.530s, learning 3.869s)
               Value function loss: 103.8310
                    Surrogate loss: 0.0072
             Mean action noise std: 0.99
                       Mean reward: 252.59
               Mean episode length: 313.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1564672
                    Iteration time: 4.40s
                        Total time: 861.34s
                               ETA: 17181.7s

################################################################################
                     [1m Learning iteration 191/4000 [0m

                       Computation: 1815 steps/s (collection: 0.655s, learning 3.857s)
               Value function loss: 59.5980
                    Surrogate loss: 0.0084
             Mean action noise std: 0.99
                       Mean reward: 253.03
               Mean episode length: 311.15
                 Mean success rate: 0.00
                  Mean reward/step: 0.83
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 4.51s
                        Total time: 865.85s
                               ETA: 17177.2s

################################################################################
                     [1m Learning iteration 192/4000 [0m

                       Computation: 1816 steps/s (collection: 0.580s, learning 3.929s)
               Value function loss: 75.0908
                    Surrogate loss: 0.0073
             Mean action noise std: 0.99
                       Mean reward: 260.84
               Mean episode length: 323.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1581056
                    Iteration time: 4.51s
                        Total time: 870.36s
                               ETA: 17172.7s

################################################################################
                     [1m Learning iteration 193/4000 [0m

                       Computation: 1819 steps/s (collection: 0.580s, learning 3.922s)
               Value function loss: 101.4889
                    Surrogate loss: 0.0060
             Mean action noise std: 0.99
                       Mean reward: 253.86
               Mean episode length: 315.61
                 Mean success rate: 0.00
                  Mean reward/step: 0.87
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 4.50s
                        Total time: 874.86s
                               ETA: 17168.1s

################################################################################
                     [1m Learning iteration 194/4000 [0m

                       Computation: 1816 steps/s (collection: 0.592s, learning 3.918s)
               Value function loss: 100.0036
                    Surrogate loss: 0.0075
             Mean action noise std: 0.99
                       Mean reward: 258.49
               Mean episode length: 315.95
                 Mean success rate: 0.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1597440
                    Iteration time: 4.51s
                        Total time: 879.37s
                               ETA: 17163.6s

################################################################################
                     [1m Learning iteration 195/4000 [0m

                       Computation: 1809 steps/s (collection: 0.615s, learning 3.911s)
               Value function loss: 97.0928
                    Surrogate loss: 0.0064
             Mean action noise std: 0.99
                       Mean reward: 270.37
               Mean episode length: 330.51
                 Mean success rate: 0.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 4.53s
                        Total time: 883.90s
                               ETA: 17159.4s

################################################################################
                     [1m Learning iteration 196/4000 [0m

                       Computation: 1818 steps/s (collection: 0.599s, learning 3.905s)
               Value function loss: 104.0122
                    Surrogate loss: 0.0067
             Mean action noise std: 0.99
                       Mean reward: 272.98
               Mean episode length: 331.06
                 Mean success rate: 0.00
                  Mean reward/step: 0.87
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1613824
                    Iteration time: 4.50s
                        Total time: 888.40s
                               ETA: 17154.7s

################################################################################
                     [1m Learning iteration 197/4000 [0m

                       Computation: 1828 steps/s (collection: 0.577s, learning 3.902s)
               Value function loss: 90.1797
                    Surrogate loss: 0.0060
             Mean action noise std: 0.99
                       Mean reward: 248.33
               Mean episode length: 301.51
                 Mean success rate: 0.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 4.48s
                        Total time: 892.88s
                               ETA: 17149.7s

################################################################################
                     [1m Learning iteration 198/4000 [0m

                       Computation: 1804 steps/s (collection: 0.609s, learning 3.930s)
               Value function loss: 114.4655
                    Surrogate loss: 0.0047
             Mean action noise std: 0.99
                       Mean reward: 234.75
               Mean episode length: 277.82
                 Mean success rate: 0.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1630208
                    Iteration time: 4.54s
                        Total time: 897.42s
                               ETA: 17145.7s

################################################################################
                     [1m Learning iteration 199/4000 [0m

                       Computation: 1795 steps/s (collection: 0.613s, learning 3.949s)
               Value function loss: 67.0554
                    Surrogate loss: 0.0074
             Mean action noise std: 0.99
                       Mean reward: 227.78
               Mean episode length: 265.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.87
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 4.56s
                        Total time: 901.98s
                               ETA: 17142.2s

################################################################################
                     [1m Learning iteration 200/4000 [0m

                       Computation: 1764 steps/s (collection: 0.692s, learning 3.951s)
               Value function loss: 57.4672
                    Surrogate loss: 0.0066
             Mean action noise std: 0.99
                       Mean reward: 230.71
               Mean episode length: 271.51
                 Mean success rate: 0.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1646592
                    Iteration time: 4.64s
                        Total time: 906.63s
                               ETA: 17140.2s

################################################################################
                     [1m Learning iteration 201/4000 [0m

                       Computation: 1812 steps/s (collection: 0.626s, learning 3.894s)
               Value function loss: 107.5557
                    Surrogate loss: 0.0073
             Mean action noise std: 0.99
                       Mean reward: 210.87
               Mean episode length: 248.59
                 Mean success rate: 0.00
                  Mean reward/step: 0.89
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 4.52s
                        Total time: 911.15s
                               ETA: 17135.9s

################################################################################
                     [1m Learning iteration 202/4000 [0m

                       Computation: 1786 steps/s (collection: 0.652s, learning 3.934s)
               Value function loss: 95.3809
                    Surrogate loss: 0.0061
             Mean action noise std: 0.99
                       Mean reward: 208.47
               Mean episode length: 244.23
                 Mean success rate: 0.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1662976
                    Iteration time: 4.59s
                        Total time: 915.73s
                               ETA: 17132.8s

################################################################################
                     [1m Learning iteration 203/4000 [0m

                       Computation: 1755 steps/s (collection: 0.728s, learning 3.939s)
               Value function loss: 129.5817
                    Surrogate loss: 0.0090
             Mean action noise std: 0.99
                       Mean reward: 227.22
               Mean episode length: 262.25
                 Mean success rate: 0.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 4.67s
                        Total time: 920.40s
                               ETA: 17131.2s

################################################################################
                     [1m Learning iteration 204/4000 [0m

                       Computation: 1793 steps/s (collection: 0.622s, learning 3.945s)
               Value function loss: 72.4488
                    Surrogate loss: 0.0070
             Mean action noise std: 0.99
                       Mean reward: 233.20
               Mean episode length: 267.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1679360
                    Iteration time: 4.57s
                        Total time: 924.97s
                               ETA: 17127.7s

################################################################################
                     [1m Learning iteration 205/4000 [0m

                       Computation: 1781 steps/s (collection: 0.669s, learning 3.929s)
               Value function loss: 72.6324
                    Surrogate loss: 0.0085
             Mean action noise std: 0.99
                       Mean reward: 234.45
               Mean episode length: 276.35
                 Mean success rate: 0.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 4.60s
                        Total time: 929.56s
                               ETA: 17124.7s

################################################################################
                     [1m Learning iteration 206/4000 [0m

                       Computation: 1788 steps/s (collection: 0.659s, learning 3.921s)
               Value function loss: 74.1861
                    Surrogate loss: 0.0063
             Mean action noise std: 0.99
                       Mean reward: 240.22
               Mean episode length: 282.46
                 Mean success rate: 0.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1695744
                    Iteration time: 4.58s
                        Total time: 934.14s
                               ETA: 17121.5s

################################################################################
                     [1m Learning iteration 207/4000 [0m

                       Computation: 1781 steps/s (collection: 0.678s, learning 3.920s)
               Value function loss: 54.2617
                    Surrogate loss: 0.0083
             Mean action noise std: 0.99
                       Mean reward: 232.71
               Mean episode length: 268.65
                 Mean success rate: 0.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 4.60s
                        Total time: 938.74s
                               ETA: 17118.5s

################################################################################
                     [1m Learning iteration 208/4000 [0m

                       Computation: 1814 steps/s (collection: 0.617s, learning 3.897s)
               Value function loss: 88.8696
                    Surrogate loss: 0.0084
             Mean action noise std: 0.99
                       Mean reward: 242.01
               Mean episode length: 277.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.96
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1712128
                    Iteration time: 4.51s
                        Total time: 943.26s
                               ETA: 17114.0s

################################################################################
                     [1m Learning iteration 209/4000 [0m

                       Computation: 1775 steps/s (collection: 0.715s, learning 3.899s)
               Value function loss: 131.9177
                    Surrogate loss: 0.0074
             Mean action noise std: 0.99
                       Mean reward: 256.30
               Mean episode length: 293.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.98
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 4.61s
                        Total time: 947.87s
                               ETA: 17111.3s

################################################################################
                     [1m Learning iteration 210/4000 [0m

                       Computation: 1783 steps/s (collection: 0.681s, learning 3.912s)
               Value function loss: 136.6425
                    Surrogate loss: 0.0086
             Mean action noise std: 0.99
                       Mean reward: 262.60
               Mean episode length: 301.00
                 Mean success rate: 0.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1728512
                    Iteration time: 4.59s
                        Total time: 952.46s
                               ETA: 17108.2s

################################################################################
                     [1m Learning iteration 211/4000 [0m

                       Computation: 1777 steps/s (collection: 0.676s, learning 3.932s)
               Value function loss: 92.1159
                    Surrogate loss: 0.0086
             Mean action noise std: 0.99
                       Mean reward: 249.26
               Mean episode length: 285.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 4.61s
                        Total time: 957.07s
                               ETA: 17105.4s

################################################################################
                     [1m Learning iteration 212/4000 [0m

                       Computation: 1810 steps/s (collection: 0.637s, learning 3.888s)
               Value function loss: 110.7388
                    Surrogate loss: 0.0081
             Mean action noise std: 0.99
                       Mean reward: 257.37
               Mean episode length: 290.39
                 Mean success rate: 0.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1744896
                    Iteration time: 4.53s
                        Total time: 961.60s
                               ETA: 17101.1s

################################################################################
                     [1m Learning iteration 213/4000 [0m

                       Computation: 1806 steps/s (collection: 0.624s, learning 3.911s)
               Value function loss: 101.5355
                    Surrogate loss: 0.0077
             Mean action noise std: 0.99
                       Mean reward: 261.04
               Mean episode length: 290.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 4.54s
                        Total time: 966.13s
                               ETA: 17096.9s

################################################################################
                     [1m Learning iteration 214/4000 [0m

                       Computation: 1798 steps/s (collection: 0.656s, learning 3.899s)
               Value function loss: 136.1250
                    Surrogate loss: 0.0075
             Mean action noise std: 0.99
                       Mean reward: 266.02
               Mean episode length: 293.61
                 Mean success rate: 0.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1761280
                    Iteration time: 4.56s
                        Total time: 970.69s
                               ETA: 17093.1s

################################################################################
                     [1m Learning iteration 215/4000 [0m

                       Computation: 1790 steps/s (collection: 0.676s, learning 3.901s)
               Value function loss: 83.0329
                    Surrogate loss: 0.0084
             Mean action noise std: 0.99
                       Mean reward: 265.41
               Mean episode length: 293.42
                 Mean success rate: 0.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 4.58s
                        Total time: 975.26s
                               ETA: 17089.7s

################################################################################
                     [1m Learning iteration 216/4000 [0m

                       Computation: 1773 steps/s (collection: 0.699s, learning 3.922s)
               Value function loss: 100.6933
                    Surrogate loss: 0.0062
             Mean action noise std: 0.99
                       Mean reward: 255.74
               Mean episode length: 279.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1777664
                    Iteration time: 4.62s
                        Total time: 979.88s
                               ETA: 17087.0s

################################################################################
                     [1m Learning iteration 217/4000 [0m

                       Computation: 1807 steps/s (collection: 0.629s, learning 3.903s)
               Value function loss: 159.9399
                    Surrogate loss: 0.0084
             Mean action noise std: 0.99
                       Mean reward: 271.60
               Mean episode length: 291.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 4.53s
                        Total time: 984.42s
                               ETA: 17082.8s

################################################################################
                     [1m Learning iteration 218/4000 [0m

                       Computation: 1783 steps/s (collection: 0.694s, learning 3.898s)
               Value function loss: 132.2468
                    Surrogate loss: 0.0072
             Mean action noise std: 0.99
                       Mean reward: 263.02
               Mean episode length: 280.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1794048
                    Iteration time: 4.59s
                        Total time: 989.01s
                               ETA: 17079.6s

################################################################################
                     [1m Learning iteration 219/4000 [0m

                       Computation: 1774 steps/s (collection: 0.723s, learning 3.894s)
               Value function loss: 139.0956
                    Surrogate loss: 0.0090
             Mean action noise std: 0.99
                       Mean reward: 251.30
               Mean episode length: 267.63
                 Mean success rate: 0.00
                  Mean reward/step: 0.98
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 4.62s
                        Total time: 993.63s
                               ETA: 17076.8s

################################################################################
                     [1m Learning iteration 220/4000 [0m

                       Computation: 1788 steps/s (collection: 0.672s, learning 3.908s)
               Value function loss: 127.9837
                    Surrogate loss: 0.0073
             Mean action noise std: 0.99
                       Mean reward: 240.69
               Mean episode length: 253.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1810432
                    Iteration time: 4.58s
                        Total time: 998.20s
                               ETA: 17073.4s

################################################################################
                     [1m Learning iteration 221/4000 [0m

                       Computation: 1794 steps/s (collection: 0.662s, learning 3.905s)
               Value function loss: 151.1237
                    Surrogate loss: 0.0074
             Mean action noise std: 0.99
                       Mean reward: 222.06
               Mean episode length: 231.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 4.57s
                        Total time: 1002.77s
                               ETA: 17069.7s

################################################################################
                     [1m Learning iteration 222/4000 [0m

                       Computation: 1806 steps/s (collection: 0.647s, learning 3.888s)
               Value function loss: 135.7666
                    Surrogate loss: 0.0068
             Mean action noise std: 0.99
                       Mean reward: 211.06
               Mean episode length: 219.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1826816
                    Iteration time: 4.53s
                        Total time: 1007.31s
                               ETA: 17065.5s

################################################################################
                     [1m Learning iteration 223/4000 [0m

                       Computation: 1782 steps/s (collection: 0.684s, learning 3.911s)
               Value function loss: 60.5423
                    Surrogate loss: 0.0088
             Mean action noise std: 0.99
                       Mean reward: 210.37
               Mean episode length: 217.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 4.60s
                        Total time: 1011.90s
                               ETA: 17062.3s

################################################################################
                     [1m Learning iteration 224/4000 [0m

                       Computation: 1796 steps/s (collection: 0.670s, learning 3.890s)
               Value function loss: 108.1998
                    Surrogate loss: 0.0128
             Mean action noise std: 0.99
                       Mean reward: 209.31
               Mean episode length: 214.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1843200
                    Iteration time: 4.56s
                        Total time: 1016.46s
                               ETA: 17058.5s

################################################################################
                     [1m Learning iteration 225/4000 [0m

                       Computation: 1784 steps/s (collection: 0.658s, learning 3.932s)
               Value function loss: 134.9781
                    Surrogate loss: 0.0055
             Mean action noise std: 0.99
                       Mean reward: 219.83
               Mean episode length: 220.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 4.59s
                        Total time: 1021.05s
                               ETA: 17055.2s

################################################################################
                     [1m Learning iteration 226/4000 [0m

                       Computation: 1796 steps/s (collection: 0.653s, learning 3.907s)
               Value function loss: 124.0448
                    Surrogate loss: 0.0054
             Mean action noise std: 0.99
                       Mean reward: 219.02
               Mean episode length: 218.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1859584
                    Iteration time: 4.56s
                        Total time: 1025.61s
                               ETA: 17051.4s

################################################################################
                     [1m Learning iteration 227/4000 [0m

                       Computation: 1780 steps/s (collection: 0.724s, learning 3.877s)
               Value function loss: 129.8120
                    Surrogate loss: 0.0087
             Mean action noise std: 0.99
                       Mean reward: 239.72
               Mean episode length: 239.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 4.60s
                        Total time: 1030.21s
                               ETA: 17048.2s

################################################################################
                     [1m Learning iteration 228/4000 [0m

                       Computation: 1768 steps/s (collection: 0.697s, learning 3.935s)
               Value function loss: 133.3897
                    Surrogate loss: 0.0094
             Mean action noise std: 0.99
                       Mean reward: 241.03
               Mean episode length: 239.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1875968
                    Iteration time: 4.63s
                        Total time: 1034.84s
                               ETA: 17045.6s

################################################################################
                     [1m Learning iteration 229/4000 [0m

                       Computation: 1798 steps/s (collection: 0.645s, learning 3.910s)
               Value function loss: 122.2751
                    Surrogate loss: 0.0110
             Mean action noise std: 0.99
                       Mean reward: 234.60
               Mean episode length: 233.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.96
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 4.56s
                        Total time: 1039.40s
                               ETA: 17041.6s

################################################################################
                     [1m Learning iteration 230/4000 [0m

                       Computation: 1787 steps/s (collection: 0.667s, learning 3.915s)
               Value function loss: 114.4037
                    Surrogate loss: 0.0067
             Mean action noise std: 0.99
                       Mean reward: 238.98
               Mean episode length: 238.40
                 Mean success rate: 0.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1892352
                    Iteration time: 4.58s
                        Total time: 1043.98s
                               ETA: 17038.1s

################################################################################
                     [1m Learning iteration 231/4000 [0m

                       Computation: 1736 steps/s (collection: 0.704s, learning 4.014s)
               Value function loss: 122.9521
                    Surrogate loss: 0.0086
             Mean action noise std: 0.99
                       Mean reward: 239.67
               Mean episode length: 242.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 4.72s
                        Total time: 1048.70s
                               ETA: 17036.9s

################################################################################
                     [1m Learning iteration 232/4000 [0m

                       Computation: 1751 steps/s (collection: 0.725s, learning 3.954s)
               Value function loss: 125.8358
                    Surrogate loss: 0.0084
             Mean action noise std: 0.99
                       Mean reward: 237.22
               Mean episode length: 238.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1908736
                    Iteration time: 4.68s
                        Total time: 1053.38s
                               ETA: 17034.9s

################################################################################
                     [1m Learning iteration 233/4000 [0m

                       Computation: 1803 steps/s (collection: 0.615s, learning 3.927s)
               Value function loss: 127.7229
                    Surrogate loss: 0.0060
             Mean action noise std: 0.99
                       Mean reward: 239.53
               Mean episode length: 238.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 4.54s
                        Total time: 1057.92s
                               ETA: 17030.7s

################################################################################
                     [1m Learning iteration 234/4000 [0m

                       Computation: 1772 steps/s (collection: 0.710s, learning 3.911s)
               Value function loss: 116.5765
                    Surrogate loss: 0.0093
             Mean action noise std: 0.99
                       Mean reward: 243.04
               Mean episode length: 239.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1925120
                    Iteration time: 4.62s
                        Total time: 1062.54s
                               ETA: 17027.8s

################################################################################
                     [1m Learning iteration 235/4000 [0m

                       Computation: 1786 steps/s (collection: 0.650s, learning 3.934s)
               Value function loss: 103.6058
                    Surrogate loss: 0.0146
             Mean action noise std: 0.99
                       Mean reward: 252.94
               Mean episode length: 249.04
                 Mean success rate: 0.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 4.58s
                        Total time: 1067.13s
                               ETA: 17024.3s

################################################################################
                     [1m Learning iteration 236/4000 [0m

                       Computation: 1739 steps/s (collection: 0.775s, learning 3.934s)
               Value function loss: 59.9628
                    Surrogate loss: 0.0105
             Mean action noise std: 0.99
                       Mean reward: 260.97
               Mean episode length: 259.50
                 Mean success rate: 0.00
                  Mean reward/step: 0.97
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 1941504
                    Iteration time: 4.71s
                        Total time: 1071.84s
                               ETA: 17022.7s

################################################################################
                     [1m Learning iteration 237/4000 [0m

                       Computation: 1800 steps/s (collection: 0.611s, learning 3.939s)
               Value function loss: 124.6529
                    Surrogate loss: 0.0074
             Mean action noise std: 0.99
                       Mean reward: 255.14
               Mean episode length: 255.79
                 Mean success rate: 0.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 4.55s
                        Total time: 1076.39s
                               ETA: 17018.6s

################################################################################
                     [1m Learning iteration 238/4000 [0m

                       Computation: 1812 steps/s (collection: 0.599s, learning 3.922s)
               Value function loss: 90.1751
                    Surrogate loss: 0.0069
             Mean action noise std: 0.99
                       Mean reward: 265.57
               Mean episode length: 262.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1957888
                    Iteration time: 4.52s
                        Total time: 1080.91s
                               ETA: 17014.1s

################################################################################
                     [1m Learning iteration 239/4000 [0m

                       Computation: 1785 steps/s (collection: 0.678s, learning 3.909s)
               Value function loss: 101.2585
                    Surrogate loss: 0.0088
             Mean action noise std: 0.99
                       Mean reward: 266.26
               Mean episode length: 264.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 4.59s
                        Total time: 1085.49s
                               ETA: 17010.6s

################################################################################
                     [1m Learning iteration 240/4000 [0m

                       Computation: 1827 steps/s (collection: 0.596s, learning 3.886s)
               Value function loss: 124.7848
                    Surrogate loss: 0.0060
             Mean action noise std: 0.99
                       Mean reward: 282.20
               Mean episode length: 279.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1974272
                    Iteration time: 4.48s
                        Total time: 1089.97s
                               ETA: 17005.4s

################################################################################
                     [1m Learning iteration 241/4000 [0m

                       Computation: 1825 steps/s (collection: 0.602s, learning 3.886s)
               Value function loss: 79.4436
                    Surrogate loss: 0.0093
             Mean action noise std: 0.99
                       Mean reward: 285.86
               Mean episode length: 284.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 4.49s
                        Total time: 1094.46s
                               ETA: 17000.3s

################################################################################
                     [1m Learning iteration 242/4000 [0m

                       Computation: 1757 steps/s (collection: 0.682s, learning 3.980s)
               Value function loss: 145.0659
                    Surrogate loss: 0.0080
             Mean action noise std: 0.99
                       Mean reward: 285.29
               Mean episode length: 286.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1990656
                    Iteration time: 4.66s
                        Total time: 1099.12s
                               ETA: 16998.0s

################################################################################
                     [1m Learning iteration 243/4000 [0m

                       Computation: 1785 steps/s (collection: 0.675s, learning 3.913s)
               Value function loss: 213.8384
                    Surrogate loss: 0.0093
             Mean action noise std: 0.99
                       Mean reward: 304.91
               Mean episode length: 302.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 4.59s
                        Total time: 1103.71s
                               ETA: 16994.4s

################################################################################
                     [1m Learning iteration 244/4000 [0m

                       Computation: 1785 steps/s (collection: 0.689s, learning 3.899s)
               Value function loss: 192.0876
                    Surrogate loss: 0.0071
             Mean action noise std: 0.99
                       Mean reward: 317.08
               Mean episode length: 313.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2007040
                    Iteration time: 4.59s
                        Total time: 1108.30s
                               ETA: 16990.9s

################################################################################
                     [1m Learning iteration 245/4000 [0m

                       Computation: 1803 steps/s (collection: 0.639s, learning 3.903s)
               Value function loss: 134.8130
                    Surrogate loss: 0.0086
             Mean action noise std: 0.99
                       Mean reward: 316.89
               Mean episode length: 313.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 4.54s
                        Total time: 1112.84s
                               ETA: 16986.7s

################################################################################
                     [1m Learning iteration 246/4000 [0m

                       Computation: 1813 steps/s (collection: 0.610s, learning 3.908s)
               Value function loss: 169.3244
                    Surrogate loss: 0.0099
             Mean action noise std: 0.99
                       Mean reward: 314.00
               Mean episode length: 307.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2023424
                    Iteration time: 4.52s
                        Total time: 1117.36s
                               ETA: 16982.1s

################################################################################
                     [1m Learning iteration 247/4000 [0m

                       Computation: 1812 steps/s (collection: 0.602s, learning 3.917s)
               Value function loss: 96.5738
                    Surrogate loss: 0.0084
             Mean action noise std: 0.99
                       Mean reward: 317.68
               Mean episode length: 310.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 4.52s
                        Total time: 1121.88s
                               ETA: 16977.5s

################################################################################
                     [1m Learning iteration 248/4000 [0m

                       Computation: 1799 steps/s (collection: 0.685s, learning 3.868s)
               Value function loss: 137.2919
                    Surrogate loss: 0.0098
             Mean action noise std: 0.99
                       Mean reward: 313.35
               Mean episode length: 303.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2039808
                    Iteration time: 4.55s
                        Total time: 1126.43s
                               ETA: 16973.4s

################################################################################
                     [1m Learning iteration 249/4000 [0m

                       Computation: 1789 steps/s (collection: 0.645s, learning 3.932s)
               Value function loss: 160.3725
                    Surrogate loss: 0.0065
             Mean action noise std: 0.99
                       Mean reward: 317.96
               Mean episode length: 307.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 4.58s
                        Total time: 1131.01s
                               ETA: 16969.7s

################################################################################
                     [1m Learning iteration 250/4000 [0m

                       Computation: 1768 steps/s (collection: 0.640s, learning 3.994s)
               Value function loss: 77.0615
                    Surrogate loss: 0.0096
             Mean action noise std: 0.99
                       Mean reward: 307.13
               Mean episode length: 294.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2056192
                    Iteration time: 4.63s
                        Total time: 1135.64s
                               ETA: 16966.8s

################################################################################
                     [1m Learning iteration 251/4000 [0m

                       Computation: 1789 steps/s (collection: 0.683s, learning 3.896s)
               Value function loss: 125.6688
                    Surrogate loss: 0.0103
             Mean action noise std: 0.99
                       Mean reward: 304.22
               Mean episode length: 289.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 4.58s
                        Total time: 1140.22s
                               ETA: 16963.0s

################################################################################
                     [1m Learning iteration 252/4000 [0m

                       Computation: 1802 steps/s (collection: 0.621s, learning 3.923s)
               Value function loss: 99.8022
                    Surrogate loss: 0.0072
             Mean action noise std: 0.99
                       Mean reward: 293.93
               Mean episode length: 278.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2072576
                    Iteration time: 4.54s
                        Total time: 1144.76s
                               ETA: 16958.8s

################################################################################
                     [1m Learning iteration 253/4000 [0m

                       Computation: 1798 steps/s (collection: 0.608s, learning 3.946s)
               Value function loss: 167.3308
                    Surrogate loss: 0.0064
             Mean action noise std: 0.99
                       Mean reward: 302.44
               Mean episode length: 284.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 4.55s
                        Total time: 1149.32s
                               ETA: 16954.7s

################################################################################
                     [1m Learning iteration 254/4000 [0m

                       Computation: 1770 steps/s (collection: 0.694s, learning 3.934s)
               Value function loss: 165.7415
                    Surrogate loss: 0.0075
             Mean action noise std: 0.99
                       Mean reward: 277.41
               Mean episode length: 260.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2088960
                    Iteration time: 4.63s
                        Total time: 1153.95s
                               ETA: 16951.7s

################################################################################
                     [1m Learning iteration 255/4000 [0m

                       Computation: 1820 steps/s (collection: 0.616s, learning 3.884s)
               Value function loss: 79.9581
                    Surrogate loss: 0.0079
             Mean action noise std: 0.99
                       Mean reward: 278.79
               Mean episode length: 261.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 4.50s
                        Total time: 1158.45s
                               ETA: 16946.8s

################################################################################
                     [1m Learning iteration 256/4000 [0m

                       Computation: 1791 steps/s (collection: 0.662s, learning 3.909s)
               Value function loss: 121.2763
                    Surrogate loss: 0.0094
             Mean action noise std: 0.99
                       Mean reward: 267.77
               Mean episode length: 251.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2105344
                    Iteration time: 4.57s
                        Total time: 1163.02s
                               ETA: 16943.0s

################################################################################
                     [1m Learning iteration 257/4000 [0m

                       Computation: 1775 steps/s (collection: 0.707s, learning 3.906s)
               Value function loss: 171.4310
                    Surrogate loss: 0.0102
             Mean action noise std: 0.99
                       Mean reward: 262.37
               Mean episode length: 247.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 4.61s
                        Total time: 1167.63s
                               ETA: 16939.7s

################################################################################
                     [1m Learning iteration 258/4000 [0m

                       Computation: 1779 steps/s (collection: 0.717s, learning 3.887s)
               Value function loss: 133.4332
                    Surrogate loss: 0.0098
             Mean action noise std: 0.99
                       Mean reward: 283.25
               Mean episode length: 264.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2121728
                    Iteration time: 4.60s
                        Total time: 1172.24s
                               ETA: 16936.3s

################################################################################
                     [1m Learning iteration 259/4000 [0m

                       Computation: 1830 steps/s (collection: 0.628s, learning 3.847s)
               Value function loss: 155.7522
                    Surrogate loss: 0.0129
             Mean action noise std: 0.99
                       Mean reward: 286.52
               Mean episode length: 268.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 4.48s
                        Total time: 1176.71s
                               ETA: 16931.1s

################################################################################
                     [1m Learning iteration 260/4000 [0m

                       Computation: 1791 steps/s (collection: 0.633s, learning 3.940s)
               Value function loss: 132.8827
                    Surrogate loss: 0.0108
             Mean action noise std: 0.99
                       Mean reward: 295.90
               Mean episode length: 274.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2138112
                    Iteration time: 4.57s
                        Total time: 1181.28s
                               ETA: 16927.2s

################################################################################
                     [1m Learning iteration 261/4000 [0m

                       Computation: 1797 steps/s (collection: 0.650s, learning 3.908s)
               Value function loss: 117.2251
                    Surrogate loss: 0.0099
             Mean action noise std: 0.99
                       Mean reward: 305.89
               Mean episode length: 283.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 4.56s
                        Total time: 1185.84s
                               ETA: 16923.1s

################################################################################
                     [1m Learning iteration 262/4000 [0m

                       Computation: 1770 steps/s (collection: 0.597s, learning 4.031s)
               Value function loss: 108.1447
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 310.65
               Mean episode length: 284.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2154496
                    Iteration time: 4.63s
                        Total time: 1190.47s
                               ETA: 16920.1s

################################################################################
                     [1m Learning iteration 263/4000 [0m

                       Computation: 1790 steps/s (collection: 0.672s, learning 3.903s)
               Value function loss: 103.8060
                    Surrogate loss: 0.0153
             Mean action noise std: 0.99
                       Mean reward: 305.87
               Mean episode length: 279.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 4.57s
                        Total time: 1195.04s
                               ETA: 16916.2s

################################################################################
                     [1m Learning iteration 264/4000 [0m

                       Computation: 1763 steps/s (collection: 0.676s, learning 3.970s)
               Value function loss: 192.3117
                    Surrogate loss: 0.0113
             Mean action noise std: 0.99
                       Mean reward: 282.92
               Mean episode length: 258.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 2170880
                    Iteration time: 4.65s
                        Total time: 1199.69s
                               ETA: 16913.4s

################################################################################
                     [1m Learning iteration 265/4000 [0m

                       Computation: 1787 steps/s (collection: 0.661s, learning 3.923s)
               Value function loss: 138.4244
                    Surrogate loss: 0.0081
             Mean action noise std: 0.99
                       Mean reward: 267.15
               Mean episode length: 243.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 4.58s
                        Total time: 1204.27s
                               ETA: 16909.6s

################################################################################
                     [1m Learning iteration 266/4000 [0m

                       Computation: 1819 steps/s (collection: 0.616s, learning 3.886s)
               Value function loss: 114.0061
                    Surrogate loss: 0.0090
             Mean action noise std: 0.99
                       Mean reward: 257.84
               Mean episode length: 233.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2187264
                    Iteration time: 4.50s
                        Total time: 1208.78s
                               ETA: 16904.8s

################################################################################
                     [1m Learning iteration 267/4000 [0m

                       Computation: 1787 steps/s (collection: 0.658s, learning 3.925s)
               Value function loss: 132.4505
                    Surrogate loss: 0.0083
             Mean action noise std: 0.99
                       Mean reward: 254.78
               Mean episode length: 230.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 4.58s
                        Total time: 1213.36s
                               ETA: 16901.0s

################################################################################
                     [1m Learning iteration 268/4000 [0m

                       Computation: 1796 steps/s (collection: 0.662s, learning 3.897s)
               Value function loss: 172.4055
                    Surrogate loss: 0.0097
             Mean action noise std: 0.99
                       Mean reward: 246.43
               Mean episode length: 223.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2203648
                    Iteration time: 4.56s
                        Total time: 1217.92s
                               ETA: 16896.9s

################################################################################
                     [1m Learning iteration 269/4000 [0m

                       Computation: 1731 steps/s (collection: 0.830s, learning 3.900s)
               Value function loss: 148.8593
                    Surrogate loss: 0.0077
             Mean action noise std: 0.99
                       Mean reward: 248.42
               Mean episode length: 226.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 4.73s
                        Total time: 1222.65s
                               ETA: 16895.2s

################################################################################
                     [1m Learning iteration 270/4000 [0m

                       Computation: 1801 steps/s (collection: 0.628s, learning 3.920s)
               Value function loss: 142.5003
                    Surrogate loss: 0.0088
             Mean action noise std: 0.99
                       Mean reward: 268.97
               Mean episode length: 242.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2220032
                    Iteration time: 4.55s
                        Total time: 1227.20s
                               ETA: 16890.9s

################################################################################
                     [1m Learning iteration 271/4000 [0m

                       Computation: 1809 steps/s (collection: 0.595s, learning 3.933s)
               Value function loss: 126.7980
                    Surrogate loss: 0.0102
             Mean action noise std: 0.99
                       Mean reward: 270.23
               Mean episode length: 243.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 4.53s
                        Total time: 1231.73s
                               ETA: 16886.4s

################################################################################
                     [1m Learning iteration 272/4000 [0m

                       Computation: 1782 steps/s (collection: 0.678s, learning 3.917s)
               Value function loss: 172.8993
                    Surrogate loss: 0.0110
             Mean action noise std: 0.99
                       Mean reward: 279.19
               Mean episode length: 250.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2236416
                    Iteration time: 4.60s
                        Total time: 1236.32s
                               ETA: 16882.8s

################################################################################
                     [1m Learning iteration 273/4000 [0m

                       Computation: 1811 steps/s (collection: 0.636s, learning 3.885s)
               Value function loss: 165.5702
                    Surrogate loss: 0.0100
             Mean action noise std: 0.99
                       Mean reward: 281.98
               Mean episode length: 255.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 4.52s
                        Total time: 1240.84s
                               ETA: 16878.2s

################################################################################
                     [1m Learning iteration 274/4000 [0m

                       Computation: 1787 steps/s (collection: 0.656s, learning 3.926s)
               Value function loss: 149.7841
                    Surrogate loss: 0.0099
             Mean action noise std: 0.99
                       Mean reward: 298.86
               Mean episode length: 270.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2252800
                    Iteration time: 4.58s
                        Total time: 1245.42s
                               ETA: 16874.4s

################################################################################
                     [1m Learning iteration 275/4000 [0m

                       Computation: 1776 steps/s (collection: 0.651s, learning 3.961s)
               Value function loss: 214.9572
                    Surrogate loss: 0.0136
             Mean action noise std: 0.99
                       Mean reward: 317.33
               Mean episode length: 288.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 4.61s
                        Total time: 1250.04s
                               ETA: 16871.0s

################################################################################
                     [1m Learning iteration 276/4000 [0m

                       Computation: 1813 steps/s (collection: 0.612s, learning 3.905s)
               Value function loss: 171.9238
                    Surrogate loss: 0.0083
             Mean action noise std: 0.99
                       Mean reward: 319.68
               Mean episode length: 288.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2269184
                    Iteration time: 4.52s
                        Total time: 1254.55s
                               ETA: 16866.3s

################################################################################
                     [1m Learning iteration 277/4000 [0m

                       Computation: 1830 steps/s (collection: 0.538s, learning 3.936s)
               Value function loss: 93.8069
                    Surrogate loss: 0.0118
             Mean action noise std: 0.99
                       Mean reward: 317.57
               Mean episode length: 288.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 4.47s
                        Total time: 1259.03s
                               ETA: 16861.0s

################################################################################
                     [1m Learning iteration 278/4000 [0m

                       Computation: 1801 steps/s (collection: 0.624s, learning 3.924s)
               Value function loss: 110.2474
                    Surrogate loss: 0.0110
             Mean action noise std: 0.99
                       Mean reward: 313.92
               Mean episode length: 284.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2285568
                    Iteration time: 4.55s
                        Total time: 1263.58s
                               ETA: 16856.7s

################################################################################
                     [1m Learning iteration 279/4000 [0m

                       Computation: 1751 steps/s (collection: 0.682s, learning 3.995s)
               Value function loss: 195.9734
                    Surrogate loss: 0.0096
             Mean action noise std: 0.99
                       Mean reward: 316.17
               Mean episode length: 283.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 4.68s
                        Total time: 1268.25s
                               ETA: 16854.2s

################################################################################
                     [1m Learning iteration 280/4000 [0m

                       Computation: 1746 steps/s (collection: 0.693s, learning 3.998s)
               Value function loss: 183.9468
                    Surrogate loss: 0.0101
             Mean action noise std: 0.99
                       Mean reward: 300.33
               Mean episode length: 265.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 2301952
                    Iteration time: 4.69s
                        Total time: 1272.94s
                               ETA: 16851.8s

################################################################################
                     [1m Learning iteration 281/4000 [0m

                       Computation: 1759 steps/s (collection: 0.678s, learning 3.978s)
               Value function loss: 141.3506
                    Surrogate loss: 0.0109
             Mean action noise std: 0.99
                       Mean reward: 298.64
               Mean episode length: 261.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 4.66s
                        Total time: 1277.60s
                               ETA: 16848.9s

################################################################################
                     [1m Learning iteration 282/4000 [0m

                       Computation: 1806 steps/s (collection: 0.616s, learning 3.920s)
               Value function loss: 129.6578
                    Surrogate loss: 0.0094
             Mean action noise std: 0.99
                       Mean reward: 293.94
               Mean episode length: 254.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2318336
                    Iteration time: 4.54s
                        Total time: 1282.14s
                               ETA: 16844.5s

################################################################################
                     [1m Learning iteration 283/4000 [0m

                       Computation: 1827 steps/s (collection: 0.592s, learning 3.892s)
               Value function loss: 120.5273
                    Surrogate loss: 0.0116
             Mean action noise std: 0.99
                       Mean reward: 305.96
               Mean episode length: 265.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 4.48s
                        Total time: 1286.62s
                               ETA: 16839.3s

################################################################################
                     [1m Learning iteration 284/4000 [0m

                       Computation: 1762 steps/s (collection: 0.680s, learning 3.968s)
               Value function loss: 189.6381
                    Surrogate loss: 0.0093
             Mean action noise std: 0.99
                       Mean reward: 299.35
               Mean episode length: 257.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2334720
                    Iteration time: 4.65s
                        Total time: 1291.27s
                               ETA: 16836.3s

################################################################################
                     [1m Learning iteration 285/4000 [0m

                       Computation: 1803 steps/s (collection: 0.663s, learning 3.879s)
               Value function loss: 153.4936
                    Surrogate loss: 0.0103
             Mean action noise std: 0.99
                       Mean reward: 305.52
               Mean episode length: 261.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 4.54s
                        Total time: 1295.81s
                               ETA: 16831.9s

################################################################################
                     [1m Learning iteration 286/4000 [0m

                       Computation: 1782 steps/s (collection: 0.670s, learning 3.925s)
               Value function loss: 155.5072
                    Surrogate loss: 0.0105
             Mean action noise std: 0.99
                       Mean reward: 310.59
               Mean episode length: 263.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2351104
                    Iteration time: 4.60s
                        Total time: 1300.41s
                               ETA: 16828.2s

################################################################################
                     [1m Learning iteration 287/4000 [0m

                       Computation: 1806 steps/s (collection: 0.634s, learning 3.900s)
               Value function loss: 124.3003
                    Surrogate loss: 0.0110
             Mean action noise std: 0.99
                       Mean reward: 299.56
               Mean episode length: 254.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 4.53s
                        Total time: 1304.94s
                               ETA: 16823.7s

################################################################################
                     [1m Learning iteration 288/4000 [0m

                       Computation: 1786 steps/s (collection: 0.660s, learning 3.926s)
               Value function loss: 185.3194
                    Surrogate loss: 0.0089
             Mean action noise std: 0.99
                       Mean reward: 290.56
               Mean episode length: 243.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2367488
                    Iteration time: 4.59s
                        Total time: 1309.53s
                               ETA: 16819.9s

################################################################################
                     [1m Learning iteration 289/4000 [0m

                       Computation: 1793 steps/s (collection: 0.657s, learning 3.912s)
               Value function loss: 119.4110
                    Surrogate loss: 0.0091
             Mean action noise std: 0.99
                       Mean reward: 294.88
               Mean episode length: 247.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 4.57s
                        Total time: 1314.09s
                               ETA: 16815.9s

################################################################################
                     [1m Learning iteration 290/4000 [0m

                       Computation: 1784 steps/s (collection: 0.659s, learning 3.932s)
               Value function loss: 195.1521
                    Surrogate loss: 0.0101
             Mean action noise std: 0.99
                       Mean reward: 294.47
               Mean episode length: 242.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 2383872
                    Iteration time: 4.59s
                        Total time: 1318.68s
                               ETA: 16812.1s

################################################################################
                     [1m Learning iteration 291/4000 [0m

                       Computation: 1800 steps/s (collection: 0.662s, learning 3.888s)
               Value function loss: 143.4492
                    Surrogate loss: 0.0093
             Mean action noise std: 0.99
                       Mean reward: 284.53
               Mean episode length: 233.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 4.55s
                        Total time: 1323.23s
                               ETA: 16807.8s

################################################################################
                     [1m Learning iteration 292/4000 [0m

                       Computation: 1783 steps/s (collection: 0.682s, learning 3.912s)
               Value function loss: 129.3335
                    Surrogate loss: 0.0097
             Mean action noise std: 0.99
                       Mean reward: 291.44
               Mean episode length: 236.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2400256
                    Iteration time: 4.59s
                        Total time: 1327.83s
                               ETA: 16804.0s

################################################################################
                     [1m Learning iteration 293/4000 [0m

                       Computation: 1801 steps/s (collection: 0.666s, learning 3.882s)
               Value function loss: 124.4876
                    Surrogate loss: 0.0101
             Mean action noise std: 0.99
                       Mean reward: 288.54
               Mean episode length: 230.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 4.55s
                        Total time: 1332.38s
                               ETA: 16799.7s

################################################################################
                     [1m Learning iteration 294/4000 [0m

                       Computation: 1751 steps/s (collection: 0.717s, learning 3.960s)
               Value function loss: 155.3014
                    Surrogate loss: 0.0093
             Mean action noise std: 0.99
                       Mean reward: 276.64
               Mean episode length: 221.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2416640
                    Iteration time: 4.68s
                        Total time: 1337.05s
                               ETA: 16797.0s

################################################################################
                     [1m Learning iteration 295/4000 [0m

                       Computation: 1769 steps/s (collection: 0.703s, learning 3.927s)
               Value function loss: 224.3912
                    Surrogate loss: 0.0099
             Mean action noise std: 0.98
                       Mean reward: 284.33
               Mean episode length: 227.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 4.63s
                        Total time: 1341.68s
                               ETA: 16793.7s

################################################################################
                     [1m Learning iteration 296/4000 [0m

                       Computation: 1760 steps/s (collection: 0.708s, learning 3.945s)
               Value function loss: 206.4138
                    Surrogate loss: 0.0066
             Mean action noise std: 0.98
                       Mean reward: 285.52
               Mean episode length: 229.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 2433024
                    Iteration time: 4.65s
                        Total time: 1346.34s
                               ETA: 16790.7s

################################################################################
                     [1m Learning iteration 297/4000 [0m

                       Computation: 1832 steps/s (collection: 0.581s, learning 3.889s)
               Value function loss: 117.0836
                    Surrogate loss: 0.0085
             Mean action noise std: 0.98
                       Mean reward: 278.43
               Mean episode length: 223.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 4.47s
                        Total time: 1350.81s
                               ETA: 16785.4s

################################################################################
                     [1m Learning iteration 298/4000 [0m

                       Computation: 1812 steps/s (collection: 0.629s, learning 3.891s)
               Value function loss: 182.7551
                    Surrogate loss: 0.0134
             Mean action noise std: 0.98
                       Mean reward: 281.64
               Mean episode length: 225.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2449408
                    Iteration time: 4.52s
                        Total time: 1355.33s
                               ETA: 16780.7s

################################################################################
                     [1m Learning iteration 299/4000 [0m

                       Computation: 1818 steps/s (collection: 0.617s, learning 3.888s)
               Value function loss: 186.7096
                    Surrogate loss: 0.0115
             Mean action noise std: 0.98
                       Mean reward: 292.57
               Mean episode length: 232.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 4.51s
                        Total time: 1359.83s
                               ETA: 16775.8s

################################################################################
                     [1m Learning iteration 300/4000 [0m

                       Computation: 1802 steps/s (collection: 0.608s, learning 3.938s)
               Value function loss: 134.2945
                    Surrogate loss: 0.0127
             Mean action noise std: 0.98
                       Mean reward: 287.88
               Mean episode length: 228.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2465792
                    Iteration time: 4.55s
                        Total time: 1364.38s
                               ETA: 16771.4s

################################################################################
                     [1m Learning iteration 301/4000 [0m

                       Computation: 1796 steps/s (collection: 0.662s, learning 3.898s)
               Value function loss: 165.9037
                    Surrogate loss: 0.0093
             Mean action noise std: 0.98
                       Mean reward: 286.86
               Mean episode length: 226.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 4.56s
                        Total time: 1368.94s
                               ETA: 16767.2s

################################################################################
                     [1m Learning iteration 302/4000 [0m

                       Computation: 1819 steps/s (collection: 0.640s, learning 3.861s)
               Value function loss: 108.8410
                    Surrogate loss: 0.0117
             Mean action noise std: 0.98
                       Mean reward: 284.47
               Mean episode length: 223.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2482176
                    Iteration time: 4.50s
                        Total time: 1373.44s
                               ETA: 16762.3s

################################################################################
                     [1m Learning iteration 303/4000 [0m

                       Computation: 1793 steps/s (collection: 0.670s, learning 3.898s)
               Value function loss: 99.1381
                    Surrogate loss: 0.0133
             Mean action noise std: 0.98
                       Mean reward: 288.46
               Mean episode length: 227.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 4.57s
                        Total time: 1378.01s
                               ETA: 16758.2s

################################################################################
                     [1m Learning iteration 304/4000 [0m

                       Computation: 1828 steps/s (collection: 0.610s, learning 3.869s)
               Value function loss: 158.3180
                    Surrogate loss: 0.0170
             Mean action noise std: 0.98
                       Mean reward: 288.00
               Mean episode length: 227.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2498560
                    Iteration time: 4.48s
                        Total time: 1382.49s
                               ETA: 16753.0s

################################################################################
                     [1m Learning iteration 305/4000 [0m

                       Computation: 1796 steps/s (collection: 0.653s, learning 3.908s)
               Value function loss: 132.7027
                    Surrogate loss: 0.0123
             Mean action noise std: 0.98
                       Mean reward: 299.89
               Mean episode length: 238.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 4.56s
                        Total time: 1387.05s
                               ETA: 16748.8s

################################################################################
                     [1m Learning iteration 306/4000 [0m

                       Computation: 1814 steps/s (collection: 0.640s, learning 3.874s)
               Value function loss: 164.4707
                    Surrogate loss: 0.0100
             Mean action noise std: 0.98
                       Mean reward: 309.09
               Mean episode length: 243.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2514944
                    Iteration time: 4.51s
                        Total time: 1391.56s
                               ETA: 16744.1s

################################################################################
                     [1m Learning iteration 307/4000 [0m

                       Computation: 1784 steps/s (collection: 0.641s, learning 3.949s)
               Value function loss: 130.2675
                    Surrogate loss: 0.0099
             Mean action noise std: 0.98
                       Mean reward: 298.15
               Mean episode length: 233.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 4.59s
                        Total time: 1396.15s
                               ETA: 16740.2s

################################################################################
                     [1m Learning iteration 308/4000 [0m

                       Computation: 1826 steps/s (collection: 0.611s, learning 3.874s)
               Value function loss: 136.6570
                    Surrogate loss: 0.0114
             Mean action noise std: 0.98
                       Mean reward: 299.39
               Mean episode length: 235.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2531328
                    Iteration time: 4.48s
                        Total time: 1400.64s
                               ETA: 16735.1s

################################################################################
                     [1m Learning iteration 309/4000 [0m

                       Computation: 1789 steps/s (collection: 0.658s, learning 3.921s)
               Value function loss: 125.3604
                    Surrogate loss: 0.0102
             Mean action noise std: 0.98
                       Mean reward: 297.77
               Mean episode length: 232.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 4.58s
                        Total time: 1405.21s
                               ETA: 16731.1s

################################################################################
                     [1m Learning iteration 310/4000 [0m

                       Computation: 1780 steps/s (collection: 0.628s, learning 3.973s)
               Value function loss: 190.0876
                    Surrogate loss: 0.0093
             Mean action noise std: 0.98
                       Mean reward: 305.34
               Mean episode length: 237.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2547712
                    Iteration time: 4.60s
                        Total time: 1409.81s
                               ETA: 16727.4s

################################################################################
                     [1m Learning iteration 311/4000 [0m

                       Computation: 1817 steps/s (collection: 0.622s, learning 3.884s)
               Value function loss: 144.1703
                    Surrogate loss: 0.0114
             Mean action noise std: 0.98
                       Mean reward: 307.76
               Mean episode length: 238.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 4.51s
                        Total time: 1414.32s
                               ETA: 16722.5s

################################################################################
                     [1m Learning iteration 312/4000 [0m

                       Computation: 1806 steps/s (collection: 0.593s, learning 3.941s)
               Value function loss: 187.6705
                    Surrogate loss: 0.0088
             Mean action noise std: 0.98
                       Mean reward: 300.54
               Mean episode length: 232.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2564096
                    Iteration time: 4.53s
                        Total time: 1418.86s
                               ETA: 16718.0s

################################################################################
                     [1m Learning iteration 313/4000 [0m

                       Computation: 1807 steps/s (collection: 0.642s, learning 3.890s)
               Value function loss: 100.2929
                    Surrogate loss: 0.0112
             Mean action noise std: 0.98
                       Mean reward: 305.72
               Mean episode length: 236.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 4.53s
                        Total time: 1423.39s
                               ETA: 16713.5s

################################################################################
                     [1m Learning iteration 314/4000 [0m

                       Computation: 1818 steps/s (collection: 0.612s, learning 3.892s)
               Value function loss: 164.4337
                    Surrogate loss: 0.0102
             Mean action noise std: 0.98
                       Mean reward: 297.29
               Mean episode length: 230.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2580480
                    Iteration time: 4.50s
                        Total time: 1427.89s
                               ETA: 16708.6s

################################################################################
                     [1m Learning iteration 315/4000 [0m

                       Computation: 1812 steps/s (collection: 0.645s, learning 3.875s)
               Value function loss: 150.7670
                    Surrogate loss: 0.0091
             Mean action noise std: 0.98
                       Mean reward: 313.82
               Mean episode length: 242.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 4.52s
                        Total time: 1432.41s
                               ETA: 16703.9s

################################################################################
                     [1m Learning iteration 316/4000 [0m

                       Computation: 1821 steps/s (collection: 0.625s, learning 3.871s)
               Value function loss: 147.6146
                    Surrogate loss: 0.0092
             Mean action noise std: 0.98
                       Mean reward: 308.12
               Mean episode length: 236.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2596864
                    Iteration time: 4.50s
                        Total time: 1436.91s
                               ETA: 16699.0s

################################################################################
                     [1m Learning iteration 317/4000 [0m

                       Computation: 1791 steps/s (collection: 0.658s, learning 3.915s)
               Value function loss: 169.4756
                    Surrogate loss: 0.0104
             Mean action noise std: 0.98
                       Mean reward: 303.02
               Mean episode length: 232.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 4.57s
                        Total time: 1441.48s
                               ETA: 16694.9s

################################################################################
                     [1m Learning iteration 318/4000 [0m

                       Computation: 1813 steps/s (collection: 0.606s, learning 3.912s)
               Value function loss: 128.3609
                    Surrogate loss: 0.0092
             Mean action noise std: 0.98
                       Mean reward: 309.31
               Mean episode length: 237.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2613248
                    Iteration time: 4.52s
                        Total time: 1446.00s
                               ETA: 16690.2s

################################################################################
                     [1m Learning iteration 319/4000 [0m

                       Computation: 1789 steps/s (collection: 0.674s, learning 3.904s)
               Value function loss: 153.8730
                    Surrogate loss: 0.0081
             Mean action noise std: 0.98
                       Mean reward: 303.23
               Mean episode length: 234.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 4.58s
                        Total time: 1450.58s
                               ETA: 16686.2s

################################################################################
                     [1m Learning iteration 320/4000 [0m

                       Computation: 1813 steps/s (collection: 0.638s, learning 3.879s)
               Value function loss: 140.6429
                    Surrogate loss: 0.0086
             Mean action noise std: 0.98
                       Mean reward: 323.08
               Mean episode length: 249.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2629632
                    Iteration time: 4.52s
                        Total time: 1455.09s
                               ETA: 16681.5s

################################################################################
                     [1m Learning iteration 321/4000 [0m

                       Computation: 1816 steps/s (collection: 0.649s, learning 3.861s)
               Value function loss: 128.2955
                    Surrogate loss: 0.0104
             Mean action noise std: 0.98
                       Mean reward: 328.59
               Mean episode length: 253.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 4.51s
                        Total time: 1459.60s
                               ETA: 16676.7s

################################################################################
                     [1m Learning iteration 322/4000 [0m

                       Computation: 1821 steps/s (collection: 0.614s, learning 3.883s)
               Value function loss: 102.0853
                    Surrogate loss: 0.0124
             Mean action noise std: 0.98
                       Mean reward: 323.39
               Mean episode length: 250.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2646016
                    Iteration time: 4.50s
                        Total time: 1464.10s
                               ETA: 16671.7s

################################################################################
                     [1m Learning iteration 323/4000 [0m

                       Computation: 1813 steps/s (collection: 0.623s, learning 3.893s)
               Value function loss: 181.5061
                    Surrogate loss: 0.0115
             Mean action noise std: 0.98
                       Mean reward: 312.38
               Mean episode length: 241.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 4.52s
                        Total time: 1468.62s
                               ETA: 16667.0s

################################################################################
                     [1m Learning iteration 324/4000 [0m

                       Computation: 1778 steps/s (collection: 0.677s, learning 3.930s)
               Value function loss: 160.7426
                    Surrogate loss: 0.0096
             Mean action noise std: 0.98
                       Mean reward: 326.64
               Mean episode length: 250.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2662400
                    Iteration time: 4.61s
                        Total time: 1473.22s
                               ETA: 16663.3s

################################################################################
                     [1m Learning iteration 325/4000 [0m

                       Computation: 1750 steps/s (collection: 0.673s, learning 4.005s)
               Value function loss: 128.5351
                    Surrogate loss: 0.0110
             Mean action noise std: 0.98
                       Mean reward: 333.55
               Mean episode length: 257.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 4.68s
                        Total time: 1477.90s
                               ETA: 16660.4s

################################################################################
                     [1m Learning iteration 326/4000 [0m

                       Computation: 1762 steps/s (collection: 0.706s, learning 3.942s)
               Value function loss: 159.0747
                    Surrogate loss: 0.0116
             Mean action noise std: 0.98
                       Mean reward: 330.11
               Mean episode length: 254.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2678784
                    Iteration time: 4.65s
                        Total time: 1482.55s
                               ETA: 16657.2s

################################################################################
                     [1m Learning iteration 327/4000 [0m

                       Computation: 1753 steps/s (collection: 0.702s, learning 3.969s)
               Value function loss: 172.3858
                    Surrogate loss: 0.0117
             Mean action noise std: 0.98
                       Mean reward: 337.13
               Mean episode length: 259.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 4.67s
                        Total time: 1487.22s
                               ETA: 16654.2s

################################################################################
                     [1m Learning iteration 328/4000 [0m

                       Computation: 1766 steps/s (collection: 0.720s, learning 3.918s)
               Value function loss: 169.8852
                    Surrogate loss: 0.0099
             Mean action noise std: 0.98
                       Mean reward: 338.31
               Mean episode length: 260.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2695168
                    Iteration time: 4.64s
                        Total time: 1491.86s
                               ETA: 16650.8s

################################################################################
                     [1m Learning iteration 329/4000 [0m

                       Computation: 1810 steps/s (collection: 0.603s, learning 3.922s)
               Value function loss: 219.7595
                    Surrogate loss: 0.0098
             Mean action noise std: 0.98
                       Mean reward: 337.39
               Mean episode length: 260.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 4.53s
                        Total time: 1496.39s
                               ETA: 16646.2s

################################################################################
                     [1m Learning iteration 330/4000 [0m

                       Computation: 1766 steps/s (collection: 0.786s, learning 3.852s)
               Value function loss: 159.0889
                    Surrogate loss: 0.0122
             Mean action noise std: 0.98
                       Mean reward: 351.29
               Mean episode length: 271.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2711552
                    Iteration time: 4.64s
                        Total time: 1501.02s
                               ETA: 16642.8s

################################################################################
                     [1m Learning iteration 331/4000 [0m

                       Computation: 1812 steps/s (collection: 0.632s, learning 3.888s)
               Value function loss: 123.6038
                    Surrogate loss: 0.0113
             Mean action noise std: 0.98
                       Mean reward: 337.29
               Mean episode length: 260.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 4.52s
                        Total time: 1505.54s
                               ETA: 16638.1s

################################################################################
                     [1m Learning iteration 332/4000 [0m

                       Computation: 1825 steps/s (collection: 0.613s, learning 3.874s)
               Value function loss: 126.1501
                    Surrogate loss: 0.0102
             Mean action noise std: 0.98
                       Mean reward: 343.03
               Mean episode length: 264.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2727936
                    Iteration time: 4.49s
                        Total time: 1510.03s
                               ETA: 16633.0s

################################################################################
                     [1m Learning iteration 333/4000 [0m

                       Computation: 1793 steps/s (collection: 0.673s, learning 3.893s)
               Value function loss: 107.9259
                    Surrogate loss: 0.0123
             Mean action noise std: 0.98
                       Mean reward: 340.47
               Mean episode length: 263.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 4.57s
                        Total time: 1514.60s
                               ETA: 16628.8s

################################################################################
                     [1m Learning iteration 334/4000 [0m

                       Computation: 1827 steps/s (collection: 0.598s, learning 3.884s)
               Value function loss: 89.9975
                    Surrogate loss: 0.0123
             Mean action noise std: 0.98
                       Mean reward: 346.06
               Mean episode length: 268.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 2744320
                    Iteration time: 4.48s
                        Total time: 1519.08s
                               ETA: 16623.7s

################################################################################
                     [1m Learning iteration 335/4000 [0m

                       Computation: 1837 steps/s (collection: 0.568s, learning 3.892s)
               Value function loss: 99.9846
                    Surrogate loss: 0.0124
             Mean action noise std: 0.98
                       Mean reward: 348.96
               Mean episode length: 270.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 4.46s
                        Total time: 1523.54s
                               ETA: 16618.3s

################################################################################
                     [1m Learning iteration 336/4000 [0m

                       Computation: 1767 steps/s (collection: 0.704s, learning 3.930s)
               Value function loss: 122.0900
                    Surrogate loss: 0.0104
             Mean action noise std: 0.98
                       Mean reward: 343.87
               Mean episode length: 266.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2760704
                    Iteration time: 4.63s
                        Total time: 1528.17s
                               ETA: 16614.9s

################################################################################
                     [1m Learning iteration 337/4000 [0m

                       Computation: 1822 steps/s (collection: 0.563s, learning 3.932s)
               Value function loss: 156.1394
                    Surrogate loss: 0.0108
             Mean action noise std: 0.98
                       Mean reward: 364.33
               Mean episode length: 283.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 4.49s
                        Total time: 1532.67s
                               ETA: 16609.9s

################################################################################
                     [1m Learning iteration 338/4000 [0m

                       Computation: 1804 steps/s (collection: 0.661s, learning 3.880s)
               Value function loss: 203.6315
                    Surrogate loss: 0.0079
             Mean action noise std: 0.98
                       Mean reward: 345.07
               Mean episode length: 266.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2777088
                    Iteration time: 4.54s
                        Total time: 1537.21s
                               ETA: 16605.5s

################################################################################
                     [1m Learning iteration 339/4000 [0m

                       Computation: 1843 steps/s (collection: 0.568s, learning 3.876s)
               Value function loss: 191.8484
                    Surrogate loss: 0.0080
             Mean action noise std: 0.98
                       Mean reward: 374.78
               Mean episode length: 290.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 4.44s
                        Total time: 1541.65s
                               ETA: 16599.9s

################################################################################
                     [1m Learning iteration 340/4000 [0m

                       Computation: 1826 steps/s (collection: 0.628s, learning 3.857s)
               Value function loss: 153.9642
                    Surrogate loss: 0.0134
             Mean action noise std: 0.98
                       Mean reward: 391.13
               Mean episode length: 302.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2793472
                    Iteration time: 4.48s
                        Total time: 1546.14s
                               ETA: 16594.9s

################################################################################
                     [1m Learning iteration 341/4000 [0m

                       Computation: 1835 steps/s (collection: 0.585s, learning 3.879s)
               Value function loss: 199.2297
                    Surrogate loss: 0.0097
             Mean action noise std: 0.98
                       Mean reward: 410.72
               Mean episode length: 317.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 4.46s
                        Total time: 1550.60s
                               ETA: 16589.6s

################################################################################
                     [1m Learning iteration 342/4000 [0m

                       Computation: 1849 steps/s (collection: 0.570s, learning 3.860s)
               Value function loss: 243.1650
                    Surrogate loss: 0.0103
             Mean action noise std: 0.98
                       Mean reward: 396.13
               Mean episode length: 305.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2809856
                    Iteration time: 4.43s
                        Total time: 1555.03s
                               ETA: 16584.0s

################################################################################
                     [1m Learning iteration 343/4000 [0m

                       Computation: 1794 steps/s (collection: 0.634s, learning 3.931s)
               Value function loss: 163.9728
                    Surrogate loss: 0.0097
             Mean action noise std: 0.98
                       Mean reward: 411.31
               Mean episode length: 317.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 4.57s
                        Total time: 1559.59s
                               ETA: 16579.8s

################################################################################
                     [1m Learning iteration 344/4000 [0m

                       Computation: 1811 steps/s (collection: 0.596s, learning 3.926s)
               Value function loss: 152.3398
                    Surrogate loss: 0.0096
             Mean action noise std: 0.98
                       Mean reward: 401.53
               Mean episode length: 310.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2826240
                    Iteration time: 4.52s
                        Total time: 1564.12s
                               ETA: 16575.1s

################################################################################
                     [1m Learning iteration 345/4000 [0m

                       Computation: 1793 steps/s (collection: 0.656s, learning 3.912s)
               Value function loss: 165.8541
                    Surrogate loss: 0.0115
             Mean action noise std: 0.98
                       Mean reward: 429.22
               Mean episode length: 333.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 4.57s
                        Total time: 1568.69s
                               ETA: 16571.0s

################################################################################
                     [1m Learning iteration 346/4000 [0m

                       Computation: 1734 steps/s (collection: 0.817s, learning 3.906s)
               Value function loss: 185.8963
                    Surrogate loss: 0.0109
             Mean action noise std: 0.98
                       Mean reward: 418.79
               Mean episode length: 324.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2842624
                    Iteration time: 4.72s
                        Total time: 1573.41s
                               ETA: 16568.4s

################################################################################
                     [1m Learning iteration 347/4000 [0m

                       Computation: 1823 steps/s (collection: 0.612s, learning 3.881s)
               Value function loss: 174.4245
                    Surrogate loss: 0.0091
             Mean action noise std: 0.98
                       Mean reward: 398.31
               Mean episode length: 308.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 4.49s
                        Total time: 1577.90s
                               ETA: 16563.4s

################################################################################
                     [1m Learning iteration 348/4000 [0m

                       Computation: 1840 steps/s (collection: 0.552s, learning 3.898s)
               Value function loss: 128.0309
                    Surrogate loss: 0.0109
             Mean action noise std: 0.98
                       Mean reward: 391.91
               Mean episode length: 304.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2859008
                    Iteration time: 4.45s
                        Total time: 1582.35s
                               ETA: 16558.0s

################################################################################
                     [1m Learning iteration 349/4000 [0m

                       Computation: 1821 steps/s (collection: 0.606s, learning 3.891s)
               Value function loss: 140.5652
                    Surrogate loss: 0.0103
             Mean action noise std: 0.98
                       Mean reward: 399.92
               Mean episode length: 310.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 4.50s
                        Total time: 1586.85s
                               ETA: 16553.1s

################################################################################
                     [1m Learning iteration 350/4000 [0m

                       Computation: 1795 steps/s (collection: 0.565s, learning 3.998s)
               Value function loss: 150.3956
                    Surrogate loss: 0.0103
             Mean action noise std: 0.98
                       Mean reward: 373.75
               Mean episode length: 288.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2875392
                    Iteration time: 4.56s
                        Total time: 1591.41s
                               ETA: 16548.9s

################################################################################
                     [1m Learning iteration 351/4000 [0m

                       Computation: 1767 steps/s (collection: 0.702s, learning 3.933s)
               Value function loss: 122.9012
                    Surrogate loss: 0.0104
             Mean action noise std: 0.98
                       Mean reward: 355.25
               Mean episode length: 273.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 4.63s
                        Total time: 1596.05s
                               ETA: 16545.4s

################################################################################
                     [1m Learning iteration 352/4000 [0m

                       Computation: 1792 steps/s (collection: 0.594s, learning 3.977s)
               Value function loss: 86.5938
                    Surrogate loss: 0.0112
             Mean action noise std: 0.98
                       Mean reward: 340.47
               Mean episode length: 262.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2891776
                    Iteration time: 4.57s
                        Total time: 1600.62s
                               ETA: 16541.2s

################################################################################
                     [1m Learning iteration 353/4000 [0m

                       Computation: 1797 steps/s (collection: 0.614s, learning 3.944s)
               Value function loss: 171.2722
                    Surrogate loss: 0.0110
             Mean action noise std: 0.98
                       Mean reward: 349.47
               Mean episode length: 268.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 4.56s
                        Total time: 1605.18s
                               ETA: 16536.9s

################################################################################
                     [1m Learning iteration 354/4000 [0m

                       Computation: 1775 steps/s (collection: 0.636s, learning 3.977s)
               Value function loss: 159.6229
                    Surrogate loss: 0.0099
             Mean action noise std: 0.98
                       Mean reward: 351.46
               Mean episode length: 268.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2908160
                    Iteration time: 4.61s
                        Total time: 1609.79s
                               ETA: 16533.2s

################################################################################
                     [1m Learning iteration 355/4000 [0m

                       Computation: 1780 steps/s (collection: 0.632s, learning 3.968s)
               Value function loss: 223.1692
                    Surrogate loss: 0.0105
             Mean action noise std: 0.98
                       Mean reward: 361.25
               Mean episode length: 277.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 4.60s
                        Total time: 1614.39s
                               ETA: 16529.3s

################################################################################
                     [1m Learning iteration 356/4000 [0m

                       Computation: 1819 steps/s (collection: 0.579s, learning 3.924s)
               Value function loss: 168.4197
                    Surrogate loss: 0.0124
             Mean action noise std: 0.98
                       Mean reward: 355.13
               Mean episode length: 271.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2924544
                    Iteration time: 4.50s
                        Total time: 1618.89s
                               ETA: 16524.5s

################################################################################
                     [1m Learning iteration 357/4000 [0m

                       Computation: 1784 steps/s (collection: 0.614s, learning 3.976s)
               Value function loss: 161.5660
                    Surrogate loss: 0.0093
             Mean action noise std: 0.98
                       Mean reward: 368.22
               Mean episode length: 282.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 4.59s
                        Total time: 1623.48s
                               ETA: 16520.5s

################################################################################
                     [1m Learning iteration 358/4000 [0m

                       Computation: 1781 steps/s (collection: 0.632s, learning 3.968s)
               Value function loss: 198.7027
                    Surrogate loss: 0.0099
             Mean action noise std: 0.98
                       Mean reward: 383.05
               Mean episode length: 293.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2940928
                    Iteration time: 4.60s
                        Total time: 1628.08s
                               ETA: 16516.6s

################################################################################
                     [1m Learning iteration 359/4000 [0m

                       Computation: 1793 steps/s (collection: 0.623s, learning 3.944s)
               Value function loss: 99.3032
                    Surrogate loss: 0.0108
             Mean action noise std: 0.98
                       Mean reward: 377.53
               Mean episode length: 288.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 4.57s
                        Total time: 1632.65s
                               ETA: 16512.4s

################################################################################
                     [1m Learning iteration 360/4000 [0m

                       Computation: 1815 steps/s (collection: 0.574s, learning 3.939s)
               Value function loss: 161.6125
                    Surrogate loss: 0.0109
             Mean action noise std: 0.98
                       Mean reward: 382.09
               Mean episode length: 294.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2957312
                    Iteration time: 4.51s
                        Total time: 1637.16s
                               ETA: 16507.7s

################################################################################
                     [1m Learning iteration 361/4000 [0m

                       Computation: 1804 steps/s (collection: 0.602s, learning 3.936s)
               Value function loss: 130.5050
                    Surrogate loss: 0.0084
             Mean action noise std: 0.98
                       Mean reward: 390.03
               Mean episode length: 299.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 4.54s
                        Total time: 1641.70s
                               ETA: 16503.2s

################################################################################
                     [1m Learning iteration 362/4000 [0m

                       Computation: 1783 steps/s (collection: 0.660s, learning 3.934s)
               Value function loss: 148.4865
                    Surrogate loss: 0.0086
             Mean action noise std: 0.98
                       Mean reward: 381.43
               Mean episode length: 291.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2973696
                    Iteration time: 4.59s
                        Total time: 1646.29s
                               ETA: 16499.2s

################################################################################
                     [1m Learning iteration 363/4000 [0m

                       Computation: 1833 steps/s (collection: 0.595s, learning 3.874s)
               Value function loss: 145.7239
                    Surrogate loss: 0.0106
             Mean action noise std: 0.98
                       Mean reward: 359.97
               Mean episode length: 277.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 4.47s
                        Total time: 1650.76s
                               ETA: 16494.0s

################################################################################
                     [1m Learning iteration 364/4000 [0m

                       Computation: 1792 steps/s (collection: 0.667s, learning 3.903s)
               Value function loss: 165.9209
                    Surrogate loss: 0.0098
             Mean action noise std: 0.98
                       Mean reward: 368.63
               Mean episode length: 284.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2990080
                    Iteration time: 4.57s
                        Total time: 1655.33s
                               ETA: 16489.8s

################################################################################
                     [1m Learning iteration 365/4000 [0m

                       Computation: 1836 steps/s (collection: 0.560s, learning 3.900s)
               Value function loss: 84.0242
                    Surrogate loss: 0.0155
             Mean action noise std: 0.98
                       Mean reward: 364.38
               Mean episode length: 282.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 4.46s
                        Total time: 1659.79s
                               ETA: 16484.6s

################################################################################
                     [1m Learning iteration 366/4000 [0m

                       Computation: 1835 steps/s (collection: 0.576s, learning 3.886s)
               Value function loss: 147.8547
                    Surrogate loss: 0.0143
             Mean action noise std: 0.98
                       Mean reward: 370.74
               Mean episode length: 286.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3006464
                    Iteration time: 4.46s
                        Total time: 1664.26s
                               ETA: 16479.3s

################################################################################
                     [1m Learning iteration 367/4000 [0m

                       Computation: 1814 steps/s (collection: 0.583s, learning 3.931s)
               Value function loss: 101.9390
                    Surrogate loss: 0.0111
             Mean action noise std: 0.98
                       Mean reward: 363.79
               Mean episode length: 277.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 4.51s
                        Total time: 1668.77s
                               ETA: 16474.6s

################################################################################
                     [1m Learning iteration 368/4000 [0m

                       Computation: 1837 steps/s (collection: 0.551s, learning 3.907s)
               Value function loss: 94.2460
                    Surrogate loss: 0.0113
             Mean action noise std: 0.98
                       Mean reward: 355.57
               Mean episode length: 271.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3022848
                    Iteration time: 4.46s
                        Total time: 1673.23s
                               ETA: 16469.3s

################################################################################
                     [1m Learning iteration 369/4000 [0m

                       Computation: 1791 steps/s (collection: 0.624s, learning 3.948s)
               Value function loss: 196.8226
                    Surrogate loss: 0.0108
             Mean action noise std: 0.98
                       Mean reward: 357.71
               Mean episode length: 272.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 4.57s
                        Total time: 1677.80s
                               ETA: 16465.1s

################################################################################
                     [1m Learning iteration 370/4000 [0m

                       Computation: 1825 steps/s (collection: 0.594s, learning 3.893s)
               Value function loss: 123.8446
                    Surrogate loss: 0.0111
             Mean action noise std: 0.98
                       Mean reward: 365.47
               Mean episode length: 278.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3039232
                    Iteration time: 4.49s
                        Total time: 1682.29s
                               ETA: 16460.1s

################################################################################
                     [1m Learning iteration 371/4000 [0m

                       Computation: 1783 steps/s (collection: 0.639s, learning 3.953s)
               Value function loss: 202.6996
                    Surrogate loss: 0.0092
             Mean action noise std: 0.98
                       Mean reward: 375.52
               Mean episode length: 286.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 4.59s
                        Total time: 1686.88s
                               ETA: 16456.1s

################################################################################
                     [1m Learning iteration 372/4000 [0m

                       Computation: 1818 steps/s (collection: 0.563s, learning 3.942s)
               Value function loss: 132.6480
                    Surrogate loss: 0.0089
             Mean action noise std: 0.97
                       Mean reward: 358.85
               Mean episode length: 273.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3055616
                    Iteration time: 4.50s
                        Total time: 1691.38s
                               ETA: 16451.3s

################################################################################
                     [1m Learning iteration 373/4000 [0m

                       Computation: 1773 steps/s (collection: 0.703s, learning 3.917s)
               Value function loss: 155.8394
                    Surrogate loss: 0.0115
             Mean action noise std: 0.97
                       Mean reward: 356.17
               Mean episode length: 274.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 4.62s
                        Total time: 1696.00s
                               ETA: 16447.6s

################################################################################
                     [1m Learning iteration 374/4000 [0m

                       Computation: 1779 steps/s (collection: 0.639s, learning 3.966s)
               Value function loss: 170.9452
                    Surrogate loss: 0.0123
             Mean action noise std: 0.97
                       Mean reward: 366.75
               Mean episode length: 284.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3072000
                    Iteration time: 4.60s
                        Total time: 1700.61s
                               ETA: 16443.7s

################################################################################
                     [1m Learning iteration 375/4000 [0m

                       Computation: 1772 steps/s (collection: 0.680s, learning 3.942s)
               Value function loss: 154.9935
                    Surrogate loss: 0.0102
             Mean action noise std: 0.98
                       Mean reward: 349.70
               Mean episode length: 273.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 4.62s
                        Total time: 1705.23s
                               ETA: 16440.0s

################################################################################
                     [1m Learning iteration 376/4000 [0m

                       Computation: 1832 steps/s (collection: 0.564s, learning 3.906s)
               Value function loss: 185.9636
                    Surrogate loss: 0.0091
             Mean action noise std: 0.97
                       Mean reward: 356.11
               Mean episode length: 277.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3088384
                    Iteration time: 4.47s
                        Total time: 1709.70s
                               ETA: 16434.9s

################################################################################
                     [1m Learning iteration 377/4000 [0m

                       Computation: 1788 steps/s (collection: 0.646s, learning 3.934s)
               Value function loss: 168.8582
                    Surrogate loss: 0.0105
             Mean action noise std: 0.97
                       Mean reward: 359.52
               Mean episode length: 278.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 4.58s
                        Total time: 1714.28s
                               ETA: 16430.8s

################################################################################
                     [1m Learning iteration 378/4000 [0m

                       Computation: 1809 steps/s (collection: 0.596s, learning 3.930s)
               Value function loss: 173.6473
                    Surrogate loss: 0.0098
             Mean action noise std: 0.97
                       Mean reward: 364.41
               Mean episode length: 281.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3104768
                    Iteration time: 4.53s
                        Total time: 1718.81s
                               ETA: 16426.2s

################################################################################
                     [1m Learning iteration 379/4000 [0m

                       Computation: 1789 steps/s (collection: 0.671s, learning 3.905s)
               Value function loss: 157.2453
                    Surrogate loss: 0.0113
             Mean action noise std: 0.97
                       Mean reward: 362.56
               Mean episode length: 278.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 4.58s
                        Total time: 1723.38s
                               ETA: 16422.0s

################################################################################
                     [1m Learning iteration 380/4000 [0m

                       Computation: 1847 steps/s (collection: 0.558s, learning 3.878s)
               Value function loss: 130.8753
                    Surrogate loss: 0.0132
             Mean action noise std: 0.97
                       Mean reward: 355.61
               Mean episode length: 272.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3121152
                    Iteration time: 4.44s
                        Total time: 1727.82s
                               ETA: 16416.5s

################################################################################
                     [1m Learning iteration 381/4000 [0m

                       Computation: 1814 steps/s (collection: 0.582s, learning 3.934s)
               Value function loss: 77.4768
                    Surrogate loss: 0.0102
             Mean action noise std: 0.97
                       Mean reward: 353.71
               Mean episode length: 269.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 4.52s
                        Total time: 1732.33s
                               ETA: 16411.8s

################################################################################
                     [1m Learning iteration 382/4000 [0m

                       Computation: 1812 steps/s (collection: 0.557s, learning 3.961s)
               Value function loss: 146.5326
                    Surrogate loss: 0.0111
             Mean action noise std: 0.97
                       Mean reward: 357.35
               Mean episode length: 271.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3137536
                    Iteration time: 4.52s
                        Total time: 1736.85s
                               ETA: 16407.1s

################################################################################
                     [1m Learning iteration 383/4000 [0m

                       Computation: 1792 steps/s (collection: 0.647s, learning 3.924s)
               Value function loss: 116.1457
                    Surrogate loss: 0.0133
             Mean action noise std: 0.97
                       Mean reward: 347.89
               Mean episode length: 264.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 4.57s
                        Total time: 1741.42s
                               ETA: 16402.9s

################################################################################
                     [1m Learning iteration 384/4000 [0m

                       Computation: 1833 steps/s (collection: 0.569s, learning 3.899s)
               Value function loss: 127.5256
                    Surrogate loss: 0.0084
             Mean action noise std: 0.97
                       Mean reward: 351.03
               Mean episode length: 267.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3153920
                    Iteration time: 4.47s
                        Total time: 1745.89s
                               ETA: 16397.8s

################################################################################
                     [1m Learning iteration 385/4000 [0m

                       Computation: 1811 steps/s (collection: 0.616s, learning 3.907s)
               Value function loss: 185.0331
                    Surrogate loss: 0.0079
             Mean action noise std: 0.97
                       Mean reward: 353.29
               Mean episode length: 266.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 4.52s
                        Total time: 1750.42s
                               ETA: 16393.1s

################################################################################
                     [1m Learning iteration 386/4000 [0m

                       Computation: 1816 steps/s (collection: 0.550s, learning 3.960s)
               Value function loss: 117.2410
                    Surrogate loss: 0.0122
             Mean action noise std: 0.97
                       Mean reward: 345.51
               Mean episode length: 260.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3170304
                    Iteration time: 4.51s
                        Total time: 1754.92s
                               ETA: 16388.4s

################################################################################
                     [1m Learning iteration 387/4000 [0m

                       Computation: 1791 steps/s (collection: 0.598s, learning 3.976s)
               Value function loss: 174.8873
                    Surrogate loss: 0.0107
             Mean action noise std: 0.97
                       Mean reward: 349.13
               Mean episode length: 262.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 4.57s
                        Total time: 1759.50s
                               ETA: 16384.2s

################################################################################
                     [1m Learning iteration 388/4000 [0m

                       Computation: 1825 steps/s (collection: 0.587s, learning 3.901s)
               Value function loss: 135.8707
                    Surrogate loss: 0.0177
             Mean action noise std: 0.97
                       Mean reward: 357.51
               Mean episode length: 267.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3186688
                    Iteration time: 4.49s
                        Total time: 1763.99s
                               ETA: 16379.2s

################################################################################
                     [1m Learning iteration 389/4000 [0m

                       Computation: 1817 steps/s (collection: 0.569s, learning 3.938s)
               Value function loss: 172.1912
                    Surrogate loss: 0.0142
             Mean action noise std: 0.97
                       Mean reward: 362.11
               Mean episode length: 270.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 4.51s
                        Total time: 1768.49s
                               ETA: 16374.4s

################################################################################
                     [1m Learning iteration 390/4000 [0m

                       Computation: 1808 steps/s (collection: 0.605s, learning 3.925s)
               Value function loss: 139.3187
                    Surrogate loss: 0.0112
             Mean action noise std: 0.97
                       Mean reward: 379.61
               Mean episode length: 283.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3203072
                    Iteration time: 4.53s
                        Total time: 1773.03s
                               ETA: 16369.9s

################################################################################
                     [1m Learning iteration 391/4000 [0m

                       Computation: 1808 steps/s (collection: 0.627s, learning 3.902s)
               Value function loss: 146.5322
                    Surrogate loss: 0.0137
             Mean action noise std: 0.97
                       Mean reward: 394.04
               Mean episode length: 292.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 4.53s
                        Total time: 1777.55s
                               ETA: 16365.3s

################################################################################
                     [1m Learning iteration 392/4000 [0m

                       Computation: 1806 steps/s (collection: 0.587s, learning 3.949s)
               Value function loss: 93.6942
                    Surrogate loss: 0.0118
             Mean action noise std: 0.97
                       Mean reward: 387.21
               Mean episode length: 290.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3219456
                    Iteration time: 4.54s
                        Total time: 1782.09s
                               ETA: 16360.8s

################################################################################
                     [1m Learning iteration 393/4000 [0m

                       Computation: 1822 steps/s (collection: 0.566s, learning 3.929s)
               Value function loss: 141.9861
                    Surrogate loss: 0.0085
             Mean action noise std: 0.97
                       Mean reward: 396.70
               Mean episode length: 297.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 4.50s
                        Total time: 1786.58s
                               ETA: 16355.9s

################################################################################
                     [1m Learning iteration 394/4000 [0m

                       Computation: 1838 steps/s (collection: 0.556s, learning 3.900s)
               Value function loss: 160.6939
                    Surrogate loss: 0.0102
             Mean action noise std: 0.97
                       Mean reward: 408.07
               Mean episode length: 308.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3235840
                    Iteration time: 4.46s
                        Total time: 1791.04s
                               ETA: 16350.6s

################################################################################
                     [1m Learning iteration 395/4000 [0m

                       Computation: 1794 steps/s (collection: 0.616s, learning 3.950s)
               Value function loss: 160.0981
                    Surrogate loss: 0.0079
             Mean action noise std: 0.97
                       Mean reward: 423.72
               Mean episode length: 320.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 4.57s
                        Total time: 1795.61s
                               ETA: 16346.4s

################################################################################
                     [1m Learning iteration 396/4000 [0m

                       Computation: 1818 steps/s (collection: 0.598s, learning 3.906s)
               Value function loss: 107.8940
                    Surrogate loss: 0.0127
             Mean action noise std: 0.97
                       Mean reward: 422.17
               Mean episode length: 320.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3252224
                    Iteration time: 4.50s
                        Total time: 1800.11s
                               ETA: 16341.6s

################################################################################
                     [1m Learning iteration 397/4000 [0m

                       Computation: 1830 steps/s (collection: 0.534s, learning 3.942s)
               Value function loss: 160.4404
                    Surrogate loss: 0.0142
             Mean action noise std: 0.97
                       Mean reward: 415.35
               Mean episode length: 315.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 4.48s
                        Total time: 1804.59s
                               ETA: 16336.5s

################################################################################
                     [1m Learning iteration 398/4000 [0m

                       Computation: 1790 steps/s (collection: 0.620s, learning 3.954s)
               Value function loss: 165.4303
                    Surrogate loss: 0.0112
             Mean action noise std: 0.97
                       Mean reward: 413.51
               Mean episode length: 313.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3268608
                    Iteration time: 4.57s
                        Total time: 1809.16s
                               ETA: 16332.3s

################################################################################
                     [1m Learning iteration 399/4000 [0m

                       Computation: 1828 steps/s (collection: 0.587s, learning 3.893s)
               Value function loss: 118.5172
                    Surrogate loss: 0.0101
             Mean action noise std: 0.97
                       Mean reward: 407.32
               Mean episode length: 311.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 4.48s
                        Total time: 1813.64s
                               ETA: 16327.3s

################################################################################
                     [1m Learning iteration 400/4000 [0m

                       Computation: 1816 steps/s (collection: 0.586s, learning 3.923s)
               Value function loss: 159.2970
                    Surrogate loss: 0.0115
             Mean action noise std: 0.97
                       Mean reward: 403.01
               Mean episode length: 307.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3284992
                    Iteration time: 4.51s
                        Total time: 1818.15s
                               ETA: 16322.5s

################################################################################
                     [1m Learning iteration 401/4000 [0m

                       Computation: 1816 steps/s (collection: 0.589s, learning 3.921s)
               Value function loss: 177.7613
                    Surrogate loss: 0.0123
             Mean action noise std: 0.97
                       Mean reward: 404.46
               Mean episode length: 304.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 4.51s
                        Total time: 1822.66s
                               ETA: 16317.8s

################################################################################
                     [1m Learning iteration 402/4000 [0m

                       Computation: 1807 steps/s (collection: 0.613s, learning 3.919s)
               Value function loss: 185.9741
                    Surrogate loss: 0.0175
             Mean action noise std: 0.97
                       Mean reward: 399.23
               Mean episode length: 298.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3301376
                    Iteration time: 4.53s
                        Total time: 1827.19s
                               ETA: 16313.2s

################################################################################
                     [1m Learning iteration 403/4000 [0m

                       Computation: 1801 steps/s (collection: 0.582s, learning 3.965s)
               Value function loss: 199.8640
                    Surrogate loss: 0.0122
             Mean action noise std: 0.97
                       Mean reward: 413.24
               Mean episode length: 310.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 4.55s
                        Total time: 1831.74s
                               ETA: 16308.8s

################################################################################
                     [1m Learning iteration 404/4000 [0m

                       Computation: 1805 steps/s (collection: 0.553s, learning 3.984s)
               Value function loss: 152.9163
                    Surrogate loss: 0.0118
             Mean action noise std: 0.97
                       Mean reward: 415.97
               Mean episode length: 313.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3317760
                    Iteration time: 4.54s
                        Total time: 1836.28s
                               ETA: 16304.3s

################################################################################
                     [1m Learning iteration 405/4000 [0m

                       Computation: 1802 steps/s (collection: 0.603s, learning 3.942s)
               Value function loss: 197.6968
                    Surrogate loss: 0.0112
             Mean action noise std: 0.97
                       Mean reward: 430.13
               Mean episode length: 326.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 4.54s
                        Total time: 1840.82s
                               ETA: 16299.9s

################################################################################
                     [1m Learning iteration 406/4000 [0m

                       Computation: 1825 steps/s (collection: 0.552s, learning 3.935s)
               Value function loss: 112.9463
                    Surrogate loss: 0.0136
             Mean action noise std: 0.97
                       Mean reward: 434.37
               Mean episode length: 330.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3334144
                    Iteration time: 4.49s
                        Total time: 1845.31s
                               ETA: 16294.9s

################################################################################
                     [1m Learning iteration 407/4000 [0m

                       Computation: 1809 steps/s (collection: 0.570s, learning 3.956s)
               Value function loss: 174.1491
                    Surrogate loss: 0.0123
             Mean action noise std: 0.97
                       Mean reward: 445.97
               Mean episode length: 339.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 4.53s
                        Total time: 1849.83s
                               ETA: 16290.3s

################################################################################
                     [1m Learning iteration 408/4000 [0m

                       Computation: 1796 steps/s (collection: 0.575s, learning 3.986s)
               Value function loss: 124.3486
                    Surrogate loss: 0.0109
             Mean action noise std: 0.97
                       Mean reward: 455.85
               Mean episode length: 346.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3350528
                    Iteration time: 4.56s
                        Total time: 1854.40s
                               ETA: 16286.0s

################################################################################
                     [1m Learning iteration 409/4000 [0m

                       Computation: 1819 steps/s (collection: 0.559s, learning 3.942s)
               Value function loss: 148.9820
                    Surrogate loss: 0.0112
             Mean action noise std: 0.97
                       Mean reward: 456.70
               Mean episode length: 349.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 4.50s
                        Total time: 1858.90s
                               ETA: 16281.2s

################################################################################
                     [1m Learning iteration 410/4000 [0m

                       Computation: 1816 steps/s (collection: 0.576s, learning 3.933s)
               Value function loss: 186.8978
                    Surrogate loss: 0.0119
             Mean action noise std: 0.97
                       Mean reward: 453.69
               Mean episode length: 346.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3366912
                    Iteration time: 4.51s
                        Total time: 1863.41s
                               ETA: 16276.5s

################################################################################
                     [1m Learning iteration 411/4000 [0m

                       Computation: 1814 steps/s (collection: 0.612s, learning 3.903s)
               Value function loss: 164.0028
                    Surrogate loss: 0.0127
             Mean action noise std: 0.97
                       Mean reward: 452.52
               Mean episode length: 345.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 4.51s
                        Total time: 1867.92s
                               ETA: 16271.8s

################################################################################
                     [1m Learning iteration 412/4000 [0m

                       Computation: 1825 steps/s (collection: 0.575s, learning 3.914s)
               Value function loss: 82.5272
                    Surrogate loss: 0.0144
             Mean action noise std: 0.97
                       Mean reward: 438.32
               Mean episode length: 333.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3383296
                    Iteration time: 4.49s
                        Total time: 1872.41s
                               ETA: 16266.8s

################################################################################
                     [1m Learning iteration 413/4000 [0m

                       Computation: 1834 steps/s (collection: 0.584s, learning 3.882s)
               Value function loss: 179.9250
                    Surrogate loss: 0.0113
             Mean action noise std: 0.97
                       Mean reward: 439.87
               Mean episode length: 334.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 4.47s
                        Total time: 1876.88s
                               ETA: 16261.7s

################################################################################
                     [1m Learning iteration 414/4000 [0m

                       Computation: 1801 steps/s (collection: 0.628s, learning 3.920s)
               Value function loss: 118.0194
                    Surrogate loss: 0.0105
             Mean action noise std: 0.97
                       Mean reward: 431.07
               Mean episode length: 326.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3399680
                    Iteration time: 4.55s
                        Total time: 1881.42s
                               ETA: 16257.3s

################################################################################
                     [1m Learning iteration 415/4000 [0m

                       Computation: 1820 steps/s (collection: 0.569s, learning 3.930s)
               Value function loss: 114.0605
                    Surrogate loss: 0.0136
             Mean action noise std: 0.97
                       Mean reward: 444.20
               Mean episode length: 336.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 4.50s
                        Total time: 1885.92s
                               ETA: 16252.5s

################################################################################
                     [1m Learning iteration 416/4000 [0m

                       Computation: 1840 steps/s (collection: 0.539s, learning 3.913s)
               Value function loss: 183.0779
                    Surrogate loss: 0.0132
             Mean action noise std: 0.97
                       Mean reward: 450.48
               Mean episode length: 339.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3416064
                    Iteration time: 4.45s
                        Total time: 1890.37s
                               ETA: 16247.3s

################################################################################
                     [1m Learning iteration 417/4000 [0m

                       Computation: 1829 steps/s (collection: 0.559s, learning 3.919s)
               Value function loss: 163.0147
                    Surrogate loss: 0.0126
             Mean action noise std: 0.97
                       Mean reward: 441.34
               Mean episode length: 332.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 4.48s
                        Total time: 1894.85s
                               ETA: 16242.2s

################################################################################
                     [1m Learning iteration 418/4000 [0m

                       Computation: 1832 steps/s (collection: 0.591s, learning 3.880s)
               Value function loss: 243.5319
                    Surrogate loss: 0.0100
             Mean action noise std: 0.97
                       Mean reward: 442.04
               Mean episode length: 332.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3432448
                    Iteration time: 4.47s
                        Total time: 1899.32s
                               ETA: 16237.2s

################################################################################
                     [1m Learning iteration 419/4000 [0m

                       Computation: 1846 steps/s (collection: 0.514s, learning 3.922s)
               Value function loss: 171.2747
                    Surrogate loss: 0.0101
             Mean action noise std: 0.97
                       Mean reward: 438.79
               Mean episode length: 330.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 4.44s
                        Total time: 1903.76s
                               ETA: 16231.8s

################################################################################
                     [1m Learning iteration 420/4000 [0m

                       Computation: 1854 steps/s (collection: 0.511s, learning 3.906s)
               Value function loss: 170.1393
                    Surrogate loss: 0.0096
             Mean action noise std: 0.97
                       Mean reward: 430.33
               Mean episode length: 322.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3448832
                    Iteration time: 4.42s
                        Total time: 1908.18s
                               ETA: 16226.3s

################################################################################
                     [1m Learning iteration 421/4000 [0m

                       Computation: 1851 steps/s (collection: 0.545s, learning 3.880s)
               Value function loss: 226.0546
                    Surrogate loss: 0.0110
             Mean action noise std: 0.97
                       Mean reward: 408.80
               Mean episode length: 306.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 4.43s
                        Total time: 1912.60s
                               ETA: 16220.9s

################################################################################
                     [1m Learning iteration 422/4000 [0m

                       Computation: 1838 steps/s (collection: 0.542s, learning 3.914s)
               Value function loss: 174.6530
                    Surrogate loss: 0.0075
             Mean action noise std: 0.97
                       Mean reward: 386.88
               Mean episode length: 289.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3465216
                    Iteration time: 4.46s
                        Total time: 1917.06s
                               ETA: 16215.7s

################################################################################
                     [1m Learning iteration 423/4000 [0m

                       Computation: 1836 steps/s (collection: 0.562s, learning 3.900s)
               Value function loss: 131.0544
                    Surrogate loss: 0.0118
             Mean action noise std: 0.97
                       Mean reward: 374.90
               Mean episode length: 278.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 4.46s
                        Total time: 1921.52s
                               ETA: 16210.6s

################################################################################
                     [1m Learning iteration 424/4000 [0m

                       Computation: 1832 steps/s (collection: 0.560s, learning 3.912s)
               Value function loss: 137.8186
                    Surrogate loss: 0.0113
             Mean action noise std: 0.97
                       Mean reward: 355.44
               Mean episode length: 264.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3481600
                    Iteration time: 4.47s
                        Total time: 1925.99s
                               ETA: 16205.5s

################################################################################
                     [1m Learning iteration 425/4000 [0m

                       Computation: 1841 steps/s (collection: 0.570s, learning 3.879s)
               Value function loss: 141.0952
                    Surrogate loss: 0.0127
             Mean action noise std: 0.97
                       Mean reward: 351.31
               Mean episode length: 261.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 4.45s
                        Total time: 1930.44s
                               ETA: 16200.3s

################################################################################
                     [1m Learning iteration 426/4000 [0m

                       Computation: 1846 steps/s (collection: 0.536s, learning 3.900s)
               Value function loss: 189.3331
                    Surrogate loss: 0.0123
             Mean action noise std: 0.97
                       Mean reward: 335.81
               Mean episode length: 247.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3497984
                    Iteration time: 4.44s
                        Total time: 1934.88s
                               ETA: 16195.0s

################################################################################
                     [1m Learning iteration 427/4000 [0m

                       Computation: 1798 steps/s (collection: 0.581s, learning 3.974s)
               Value function loss: 222.6057
                    Surrogate loss: 0.0145
             Mean action noise std: 0.97
                       Mean reward: 317.05
               Mean episode length: 230.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 4.55s
                        Total time: 1939.43s
                               ETA: 16190.6s

################################################################################
                     [1m Learning iteration 428/4000 [0m

                       Computation: 1834 steps/s (collection: 0.562s, learning 3.904s)
               Value function loss: 149.1423
                    Surrogate loss: 0.0100
             Mean action noise std: 0.97
                       Mean reward: 311.99
               Mean episode length: 226.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3514368
                    Iteration time: 4.47s
                        Total time: 1943.90s
                               ETA: 16185.6s

################################################################################
                     [1m Learning iteration 429/4000 [0m

                       Computation: 1803 steps/s (collection: 0.590s, learning 3.953s)
               Value function loss: 151.5382
                    Surrogate loss: 0.0108
             Mean action noise std: 0.97
                       Mean reward: 311.36
               Mean episode length: 226.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 4.54s
                        Total time: 1948.44s
                               ETA: 16181.1s

################################################################################
                     [1m Learning iteration 430/4000 [0m

                       Computation: 1794 steps/s (collection: 0.548s, learning 4.016s)
               Value function loss: 133.9830
                    Surrogate loss: 0.0106
             Mean action noise std: 0.97
                       Mean reward: 312.96
               Mean episode length: 225.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3530752
                    Iteration time: 4.56s
                        Total time: 1953.00s
                               ETA: 16176.9s

################################################################################
                     [1m Learning iteration 431/4000 [0m

                       Computation: 1820 steps/s (collection: 0.533s, learning 3.967s)
               Value function loss: 175.5175
                    Surrogate loss: 0.0103
             Mean action noise std: 0.97
                       Mean reward: 310.23
               Mean episode length: 223.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 4.50s
                        Total time: 1957.50s
                               ETA: 16172.1s

################################################################################
                     [1m Learning iteration 432/4000 [0m

                       Computation: 1804 steps/s (collection: 0.524s, learning 4.016s)
               Value function loss: 128.5993
                    Surrogate loss: 0.0130
             Mean action noise std: 0.97
                       Mean reward: 312.22
               Mean episode length: 223.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3547136
                    Iteration time: 4.54s
                        Total time: 1962.04s
                               ETA: 16167.6s

################################################################################
                     [1m Learning iteration 433/4000 [0m

                       Computation: 1804 steps/s (collection: 0.555s, learning 3.984s)
               Value function loss: 136.3124
                    Surrogate loss: 0.0122
             Mean action noise std: 0.97
                       Mean reward: 329.48
               Mean episode length: 237.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 4.54s
                        Total time: 1966.58s
                               ETA: 16163.1s

################################################################################
                     [1m Learning iteration 434/4000 [0m

                       Computation: 1818 steps/s (collection: 0.554s, learning 3.952s)
               Value function loss: 212.0382
                    Surrogate loss: 0.0111
             Mean action noise std: 0.97
                       Mean reward: 338.34
               Mean episode length: 245.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3563520
                    Iteration time: 4.51s
                        Total time: 1971.09s
                               ETA: 16158.4s

################################################################################
                     [1m Learning iteration 435/4000 [0m

                       Computation: 1823 steps/s (collection: 0.534s, learning 3.959s)
               Value function loss: 196.0325
                    Surrogate loss: 0.0143
             Mean action noise std: 0.97
                       Mean reward: 345.74
               Mean episode length: 250.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 4.49s
                        Total time: 1975.58s
                               ETA: 16153.6s

################################################################################
                     [1m Learning iteration 436/4000 [0m

                       Computation: 1815 steps/s (collection: 0.586s, learning 3.925s)
               Value function loss: 206.1363
                    Surrogate loss: 0.0116
             Mean action noise std: 0.97
                       Mean reward: 326.47
               Mean episode length: 239.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 3579904
                    Iteration time: 4.51s
                        Total time: 1980.09s
                               ETA: 16148.9s

################################################################################
                     [1m Learning iteration 437/4000 [0m

                       Computation: 1823 steps/s (collection: 0.578s, learning 3.914s)
               Value function loss: 113.8293
                    Surrogate loss: 0.0119
             Mean action noise std: 0.97
                       Mean reward: 329.18
               Mean episode length: 240.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 4.49s
                        Total time: 1984.59s
                               ETA: 16144.0s

################################################################################
                     [1m Learning iteration 438/4000 [0m

                       Computation: 1799 steps/s (collection: 0.642s, learning 3.910s)
               Value function loss: 152.8855
                    Surrogate loss: 0.0135
             Mean action noise std: 0.97
                       Mean reward: 329.20
               Mean episode length: 244.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3596288
                    Iteration time: 4.55s
                        Total time: 1989.14s
                               ETA: 16139.6s

################################################################################
                     [1m Learning iteration 439/4000 [0m

                       Computation: 1773 steps/s (collection: 0.660s, learning 3.959s)
               Value function loss: 162.8929
                    Surrogate loss: 0.0149
             Mean action noise std: 0.97
                       Mean reward: 300.53
               Mean episode length: 223.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 4.62s
                        Total time: 1993.76s
                               ETA: 16135.8s

################################################################################
                     [1m Learning iteration 440/4000 [0m

                       Computation: 1792 steps/s (collection: 0.634s, learning 3.936s)
               Value function loss: 176.1058
                    Surrogate loss: 0.0121
             Mean action noise std: 0.97
                       Mean reward: 297.29
               Mean episode length: 222.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3612672
                    Iteration time: 4.57s
                        Total time: 1998.33s
                               ETA: 16131.6s

################################################################################
                     [1m Learning iteration 441/4000 [0m

                       Computation: 1789 steps/s (collection: 0.633s, learning 3.945s)
               Value function loss: 164.0613
                    Surrogate loss: 0.0119
             Mean action noise std: 0.97
                       Mean reward: 306.21
               Mean episode length: 228.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 4.58s
                        Total time: 2002.90s
                               ETA: 16127.5s

################################################################################
                     [1m Learning iteration 442/4000 [0m

                       Computation: 1816 steps/s (collection: 0.608s, learning 3.902s)
               Value function loss: 182.3394
                    Surrogate loss: 0.0114
             Mean action noise std: 0.97
                       Mean reward: 315.30
               Mean episode length: 234.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3629056
                    Iteration time: 4.51s
                        Total time: 2007.41s
                               ETA: 16122.8s

################################################################################
                     [1m Learning iteration 443/4000 [0m

                       Computation: 1851 steps/s (collection: 0.506s, learning 3.918s)
               Value function loss: 177.2491
                    Surrogate loss: 0.0122
             Mean action noise std: 0.97
                       Mean reward: 328.13
               Mean episode length: 243.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 4.42s
                        Total time: 2011.84s
                               ETA: 16117.4s

################################################################################
                     [1m Learning iteration 444/4000 [0m

                       Computation: 1823 steps/s (collection: 0.583s, learning 3.911s)
               Value function loss: 175.4253
                    Surrogate loss: 0.0121
             Mean action noise std: 0.97
                       Mean reward: 325.57
               Mean episode length: 241.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3645440
                    Iteration time: 4.49s
                        Total time: 2016.33s
                               ETA: 16112.5s

################################################################################
                     [1m Learning iteration 445/4000 [0m

                       Computation: 1843 steps/s (collection: 0.556s, learning 3.887s)
               Value function loss: 128.6147
                    Surrogate loss: 0.0139
             Mean action noise std: 0.97
                       Mean reward: 333.10
               Mean episode length: 245.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 4.44s
                        Total time: 2020.78s
                               ETA: 16107.3s

################################################################################
                     [1m Learning iteration 446/4000 [0m

                       Computation: 1827 steps/s (collection: 0.554s, learning 3.929s)
               Value function loss: 113.3779
                    Surrogate loss: 0.0149
             Mean action noise std: 0.97
                       Mean reward: 331.32
               Mean episode length: 244.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3661824
                    Iteration time: 4.48s
                        Total time: 2025.26s
                               ETA: 16102.4s

################################################################################
                     [1m Learning iteration 447/4000 [0m

                       Computation: 1828 steps/s (collection: 0.548s, learning 3.931s)
               Value function loss: 192.5872
                    Surrogate loss: 0.0115
             Mean action noise std: 0.97
                       Mean reward: 327.81
               Mean episode length: 241.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 4.48s
                        Total time: 2029.74s
                               ETA: 16097.4s

################################################################################
                     [1m Learning iteration 448/4000 [0m

                       Computation: 1825 steps/s (collection: 0.595s, learning 3.893s)
               Value function loss: 165.4128
                    Surrogate loss: 0.0144
             Mean action noise std: 0.97
                       Mean reward: 325.53
               Mean episode length: 237.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3678208
                    Iteration time: 4.49s
                        Total time: 2034.23s
                               ETA: 16092.6s

################################################################################
                     [1m Learning iteration 449/4000 [0m

                       Computation: 1831 steps/s (collection: 0.534s, learning 3.939s)
               Value function loss: 146.2303
                    Surrogate loss: 0.0121
             Mean action noise std: 0.97
                       Mean reward: 328.08
               Mean episode length: 239.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 4.47s
                        Total time: 2038.70s
                               ETA: 16087.6s

################################################################################
                     [1m Learning iteration 450/4000 [0m

                       Computation: 1789 steps/s (collection: 0.645s, learning 3.932s)
               Value function loss: 171.3792
                    Surrogate loss: 0.0117
             Mean action noise std: 0.97
                       Mean reward: 334.02
               Mean episode length: 241.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3694592
                    Iteration time: 4.58s
                        Total time: 2043.28s
                               ETA: 16083.4s

################################################################################
                     [1m Learning iteration 451/4000 [0m

                       Computation: 1804 steps/s (collection: 0.563s, learning 3.976s)
               Value function loss: 164.9481
                    Surrogate loss: 0.0103
             Mean action noise std: 0.97
                       Mean reward: 350.77
               Mean episode length: 251.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 4.54s
                        Total time: 2047.82s
                               ETA: 16079.0s

################################################################################
                     [1m Learning iteration 452/4000 [0m

                       Computation: 1826 steps/s (collection: 0.562s, learning 3.922s)
               Value function loss: 203.4443
                    Surrogate loss: 0.0117
             Mean action noise std: 0.97
                       Mean reward: 361.08
               Mean episode length: 260.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3710976
                    Iteration time: 4.48s
                        Total time: 2052.30s
                               ETA: 16074.1s

################################################################################
                     [1m Learning iteration 453/4000 [0m

                       Computation: 1803 steps/s (collection: 0.580s, learning 3.962s)
               Value function loss: 190.6590
                    Surrogate loss: 0.0123
             Mean action noise std: 0.97
                       Mean reward: 370.60
               Mean episode length: 267.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 4.54s
                        Total time: 2056.84s
                               ETA: 16069.6s

################################################################################
                     [1m Learning iteration 454/4000 [0m

                       Computation: 1743 steps/s (collection: 0.578s, learning 4.119s)
               Value function loss: 157.7522
                    Surrogate loss: 0.0101
             Mean action noise std: 0.97
                       Mean reward: 395.00
               Mean episode length: 287.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3727360
                    Iteration time: 4.70s
                        Total time: 2061.54s
                               ETA: 16066.4s

################################################################################
                     [1m Learning iteration 455/4000 [0m

                       Computation: 1781 steps/s (collection: 0.594s, learning 4.004s)
               Value function loss: 169.3379
                    Surrogate loss: 0.0114
             Mean action noise std: 0.97
                       Mean reward: 404.90
               Mean episode length: 296.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 4.60s
                        Total time: 2066.14s
                               ETA: 16062.4s

################################################################################
                     [1m Learning iteration 456/4000 [0m

                       Computation: 1806 steps/s (collection: 0.593s, learning 3.943s)
               Value function loss: 96.3681
                    Surrogate loss: 0.0124
             Mean action noise std: 0.97
                       Mean reward: 407.56
               Mean episode length: 300.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3743744
                    Iteration time: 4.54s
                        Total time: 2070.67s
                               ETA: 16057.9s

################################################################################
                     [1m Learning iteration 457/4000 [0m

                       Computation: 1823 steps/s (collection: 0.551s, learning 3.942s)
               Value function loss: 215.1440
                    Surrogate loss: 0.0133
             Mean action noise std: 0.97
                       Mean reward: 413.61
               Mean episode length: 306.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 4.49s
                        Total time: 2075.17s
                               ETA: 16053.1s

################################################################################
                     [1m Learning iteration 458/4000 [0m

                       Computation: 1807 steps/s (collection: 0.586s, learning 3.947s)
               Value function loss: 144.6666
                    Surrogate loss: 0.0119
             Mean action noise std: 0.97
                       Mean reward: 396.56
               Mean episode length: 294.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3760128
                    Iteration time: 4.53s
                        Total time: 2079.70s
                               ETA: 16048.6s

################################################################################
                     [1m Learning iteration 459/4000 [0m

                       Computation: 1819 steps/s (collection: 0.548s, learning 3.955s)
               Value function loss: 150.1147
                    Surrogate loss: 0.0109
             Mean action noise std: 0.97
                       Mean reward: 402.53
               Mean episode length: 299.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 4.50s
                        Total time: 2084.20s
                               ETA: 16043.8s

################################################################################
                     [1m Learning iteration 460/4000 [0m

                       Computation: 1797 steps/s (collection: 0.568s, learning 3.989s)
               Value function loss: 162.7644
                    Surrogate loss: 0.0132
             Mean action noise std: 0.97
                       Mean reward: 422.05
               Mean episode length: 311.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3776512
                    Iteration time: 4.56s
                        Total time: 2088.76s
                               ETA: 16039.5s

################################################################################
                     [1m Learning iteration 461/4000 [0m

                       Computation: 1801 steps/s (collection: 0.559s, learning 3.990s)
               Value function loss: 132.2092
                    Surrogate loss: 0.0151
             Mean action noise std: 0.97
                       Mean reward: 417.15
               Mean episode length: 308.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 4.55s
                        Total time: 2093.31s
                               ETA: 16035.1s

################################################################################
                     [1m Learning iteration 462/4000 [0m

                       Computation: 1812 steps/s (collection: 0.571s, learning 3.949s)
               Value function loss: 164.2146
                    Surrogate loss: 0.0129
             Mean action noise std: 0.97
                       Mean reward: 399.20
               Mean episode length: 293.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3792896
                    Iteration time: 4.52s
                        Total time: 2097.83s
                               ETA: 16030.5s

################################################################################
                     [1m Learning iteration 463/4000 [0m

                       Computation: 1823 steps/s (collection: 0.573s, learning 3.921s)
               Value function loss: 133.1423
                    Surrogate loss: 0.0118
             Mean action noise std: 0.97
                       Mean reward: 391.94
               Mean episode length: 285.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 4.49s
                        Total time: 2102.32s
                               ETA: 16025.7s

################################################################################
                     [1m Learning iteration 464/4000 [0m

                       Computation: 1802 steps/s (collection: 0.601s, learning 3.943s)
               Value function loss: 213.8907
                    Surrogate loss: 0.0103
             Mean action noise std: 0.97
                       Mean reward: 398.06
               Mean episode length: 287.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3809280
                    Iteration time: 4.54s
                        Total time: 2106.87s
                               ETA: 16021.2s

################################################################################
                     [1m Learning iteration 465/4000 [0m

                       Computation: 1826 steps/s (collection: 0.597s, learning 3.888s)
               Value function loss: 164.5647
                    Surrogate loss: 0.0136
             Mean action noise std: 0.97
                       Mean reward: 404.20
               Mean episode length: 291.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 4.49s
                        Total time: 2111.35s
                               ETA: 16016.4s

################################################################################
                     [1m Learning iteration 466/4000 [0m

                       Computation: 1822 steps/s (collection: 0.570s, learning 3.925s)
               Value function loss: 127.3053
                    Surrogate loss: 0.0141
             Mean action noise std: 0.97
                       Mean reward: 406.02
               Mean episode length: 292.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3825664
                    Iteration time: 4.49s
                        Total time: 2115.85s
                               ETA: 16011.6s

################################################################################
                     [1m Learning iteration 467/4000 [0m

                       Computation: 1833 steps/s (collection: 0.569s, learning 3.899s)
               Value function loss: 191.6181
                    Surrogate loss: 0.0113
             Mean action noise std: 0.97
                       Mean reward: 411.62
               Mean episode length: 296.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 4.47s
                        Total time: 2120.31s
                               ETA: 16006.6s

################################################################################
                     [1m Learning iteration 468/4000 [0m

                       Computation: 1797 steps/s (collection: 0.579s, learning 3.977s)
               Value function loss: 215.7617
                    Surrogate loss: 0.0113
             Mean action noise std: 0.97
                       Mean reward: 430.83
               Mean episode length: 307.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3842048
                    Iteration time: 4.56s
                        Total time: 2124.87s
                               ETA: 16002.2s

################################################################################
                     [1m Learning iteration 469/4000 [0m

                       Computation: 1803 steps/s (collection: 0.592s, learning 3.951s)
               Value function loss: 160.8631
                    Surrogate loss: 0.0136
             Mean action noise std: 0.97
                       Mean reward: 430.36
               Mean episode length: 305.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 4.54s
                        Total time: 2129.41s
                               ETA: 15997.8s

################################################################################
                     [1m Learning iteration 470/4000 [0m

                       Computation: 1855 steps/s (collection: 0.552s, learning 3.864s)
               Value function loss: 141.1921
                    Surrogate loss: 0.0118
             Mean action noise std: 0.97
                       Mean reward: 450.37
               Mean episode length: 317.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3858432
                    Iteration time: 4.42s
                        Total time: 2133.83s
                               ETA: 15992.4s

################################################################################
                     [1m Learning iteration 471/4000 [0m

                       Computation: 1828 steps/s (collection: 0.553s, learning 3.926s)
               Value function loss: 146.2190
                    Surrogate loss: 0.0142
             Mean action noise std: 0.97
                       Mean reward: 463.57
               Mean episode length: 326.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 4.48s
                        Total time: 2138.31s
                               ETA: 15987.5s

################################################################################
                     [1m Learning iteration 472/4000 [0m

                       Computation: 1842 steps/s (collection: 0.557s, learning 3.889s)
               Value function loss: 202.9764
                    Surrogate loss: 0.0101
             Mean action noise std: 0.97
                       Mean reward: 469.09
               Mean episode length: 328.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3874816
                    Iteration time: 4.45s
                        Total time: 2142.75s
                               ETA: 15982.3s

################################################################################
                     [1m Learning iteration 473/4000 [0m

                       Computation: 1803 steps/s (collection: 0.597s, learning 3.945s)
               Value function loss: 172.4178
                    Surrogate loss: 0.0145
             Mean action noise std: 0.97
                       Mean reward: 480.45
               Mean episode length: 335.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 4.54s
                        Total time: 2147.30s
                               ETA: 15977.9s

################################################################################
                     [1m Learning iteration 474/4000 [0m

                       Computation: 1835 steps/s (collection: 0.582s, learning 3.880s)
               Value function loss: 196.5183
                    Surrogate loss: 0.0110
             Mean action noise std: 0.97
                       Mean reward: 501.27
               Mean episode length: 350.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3891200
                    Iteration time: 4.46s
                        Total time: 2151.76s
                               ETA: 15972.8s

################################################################################
                     [1m Learning iteration 475/4000 [0m

                       Computation: 1822 steps/s (collection: 0.563s, learning 3.932s)
               Value function loss: 153.7398
                    Surrogate loss: 0.0103
             Mean action noise std: 0.97
                       Mean reward: 487.24
               Mean episode length: 338.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 4.50s
                        Total time: 2156.25s
                               ETA: 15968.1s

################################################################################
                     [1m Learning iteration 476/4000 [0m

                       Computation: 1809 steps/s (collection: 0.618s, learning 3.909s)
               Value function loss: 177.7622
                    Surrogate loss: 0.0125
             Mean action noise std: 0.97
                       Mean reward: 487.81
               Mean episode length: 340.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3907584
                    Iteration time: 4.53s
                        Total time: 2160.78s
                               ETA: 15963.5s

################################################################################
                     [1m Learning iteration 477/4000 [0m

                       Computation: 1808 steps/s (collection: 0.550s, learning 3.979s)
               Value function loss: 202.4524
                    Surrogate loss: 0.0166
             Mean action noise std: 0.97
                       Mean reward: 490.87
               Mean episode length: 340.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 4.53s
                        Total time: 2165.31s
                               ETA: 15959.0s

################################################################################
                     [1m Learning iteration 478/4000 [0m

                       Computation: 1797 steps/s (collection: 0.604s, learning 3.953s)
               Value function loss: 162.1404
                    Surrogate loss: 0.0128
             Mean action noise std: 0.97
                       Mean reward: 490.22
               Mean episode length: 340.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3923968
                    Iteration time: 4.56s
                        Total time: 2169.87s
                               ETA: 15954.6s

################################################################################
                     [1m Learning iteration 479/4000 [0m

                       Computation: 1820 steps/s (collection: 0.566s, learning 3.934s)
               Value function loss: 132.3214
                    Surrogate loss: 0.0133
             Mean action noise std: 0.97
                       Mean reward: 487.74
               Mean episode length: 340.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 4.50s
                        Total time: 2174.37s
                               ETA: 15949.9s

################################################################################
                     [1m Learning iteration 480/4000 [0m

                       Computation: 1832 steps/s (collection: 0.587s, learning 3.884s)
               Value function loss: 197.8332
                    Surrogate loss: 0.0096
             Mean action noise std: 0.97
                       Mean reward: 497.05
               Mean episode length: 347.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3940352
                    Iteration time: 4.47s
                        Total time: 2178.84s
                               ETA: 15944.9s

################################################################################
                     [1m Learning iteration 481/4000 [0m

                       Computation: 1824 steps/s (collection: 0.555s, learning 3.935s)
               Value function loss: 195.1554
                    Surrogate loss: 0.0100
             Mean action noise std: 0.97
                       Mean reward: 485.89
               Mean episode length: 339.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 4.49s
                        Total time: 2183.33s
                               ETA: 15940.1s

################################################################################
                     [1m Learning iteration 482/4000 [0m

                       Computation: 1791 steps/s (collection: 0.611s, learning 3.960s)
               Value function loss: 135.0723
                    Surrogate loss: 0.0105
             Mean action noise std: 0.97
                       Mean reward: 471.51
               Mean episode length: 330.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3956736
                    Iteration time: 4.57s
                        Total time: 2187.90s
                               ETA: 15935.9s

################################################################################
                     [1m Learning iteration 483/4000 [0m

                       Computation: 1776 steps/s (collection: 0.625s, learning 3.987s)
               Value function loss: 189.5170
                    Surrogate loss: 0.0104
             Mean action noise std: 0.97
                       Mean reward: 481.00
               Mean episode length: 338.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 4.61s
                        Total time: 2192.51s
                               ETA: 15931.9s

################################################################################
                     [1m Learning iteration 484/4000 [0m

                       Computation: 1848 steps/s (collection: 0.531s, learning 3.901s)
               Value function loss: 178.8846
                    Surrogate loss: 0.0146
             Mean action noise std: 0.97
                       Mean reward: 476.55
               Mean episode length: 335.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3973120
                    Iteration time: 4.43s
                        Total time: 2196.94s
                               ETA: 15926.7s

################################################################################
                     [1m Learning iteration 485/4000 [0m

                       Computation: 1840 steps/s (collection: 0.546s, learning 3.906s)
               Value function loss: 156.6041
                    Surrogate loss: 0.0128
             Mean action noise std: 0.97
                       Mean reward: 479.89
               Mean episode length: 338.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 4.45s
                        Total time: 2201.39s
                               ETA: 15921.6s

################################################################################
                     [1m Learning iteration 486/4000 [0m

                       Computation: 1839 steps/s (collection: 0.509s, learning 3.944s)
               Value function loss: 110.2822
                    Surrogate loss: 0.0113
             Mean action noise std: 0.97
                       Mean reward: 469.51
               Mean episode length: 331.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3989504
                    Iteration time: 4.45s
                        Total time: 2205.85s
                               ETA: 15916.5s

################################################################################
                     [1m Learning iteration 487/4000 [0m

                       Computation: 1863 steps/s (collection: 0.532s, learning 3.865s)
               Value function loss: 198.2905
                    Surrogate loss: 0.0124
             Mean action noise std: 0.97
                       Mean reward: 476.47
               Mean episode length: 333.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 4.40s
                        Total time: 2210.24s
                               ETA: 15911.0s

################################################################################
                     [1m Learning iteration 488/4000 [0m

                       Computation: 1821 steps/s (collection: 0.549s, learning 3.948s)
               Value function loss: 214.6289
                    Surrogate loss: 0.0091
             Mean action noise std: 0.97
                       Mean reward: 480.94
               Mean episode length: 334.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4005888
                    Iteration time: 4.50s
                        Total time: 2214.74s
                               ETA: 15906.3s

################################################################################
                     [1m Learning iteration 489/4000 [0m

                       Computation: 1801 steps/s (collection: 0.616s, learning 3.931s)
               Value function loss: 211.1949
                    Surrogate loss: 0.0087
             Mean action noise std: 0.97
                       Mean reward: 491.89
               Mean episode length: 342.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 4.55s
                        Total time: 2219.29s
                               ETA: 15901.9s

################################################################################
                     [1m Learning iteration 490/4000 [0m

                       Computation: 1852 steps/s (collection: 0.520s, learning 3.902s)
               Value function loss: 207.3974
                    Surrogate loss: 0.0140
             Mean action noise std: 0.97
                       Mean reward: 494.78
               Mean episode length: 343.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4022272
                    Iteration time: 4.42s
                        Total time: 2223.71s
                               ETA: 15896.6s

################################################################################
                     [1m Learning iteration 491/4000 [0m

                       Computation: 1842 steps/s (collection: 0.573s, learning 3.875s)
               Value function loss: 133.5119
                    Surrogate loss: 0.0153
             Mean action noise std: 0.97
                       Mean reward: 486.19
               Mean episode length: 335.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 4.45s
                        Total time: 2228.16s
                               ETA: 15891.5s

################################################################################
                     [1m Learning iteration 492/4000 [0m

                       Computation: 1845 steps/s (collection: 0.542s, learning 3.896s)
               Value function loss: 167.1263
                    Surrogate loss: 0.0130
             Mean action noise std: 0.97
                       Mean reward: 482.66
               Mean episode length: 332.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4038656
                    Iteration time: 4.44s
                        Total time: 2232.59s
                               ETA: 15886.3s

################################################################################
                     [1m Learning iteration 493/4000 [0m

                       Computation: 1813 steps/s (collection: 0.556s, learning 3.961s)
               Value function loss: 165.4673
                    Surrogate loss: 0.0094
             Mean action noise std: 0.97
                       Mean reward: 480.81
               Mean episode length: 330.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 4.52s
                        Total time: 2237.11s
                               ETA: 15881.7s

################################################################################
                     [1m Learning iteration 494/4000 [0m

                       Computation: 1830 steps/s (collection: 0.578s, learning 3.896s)
               Value function loss: 127.6298
                    Surrogate loss: 0.0128
             Mean action noise std: 0.97
                       Mean reward: 481.23
               Mean episode length: 332.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4055040
                    Iteration time: 4.47s
                        Total time: 2241.59s
                               ETA: 15876.8s

################################################################################
                     [1m Learning iteration 495/4000 [0m

                       Computation: 1854 steps/s (collection: 0.530s, learning 3.887s)
               Value function loss: 212.8891
                    Surrogate loss: 0.0089
             Mean action noise std: 0.97
                       Mean reward: 468.17
               Mean episode length: 322.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 4.42s
                        Total time: 2246.00s
                               ETA: 15871.4s

################################################################################
                     [1m Learning iteration 496/4000 [0m

                       Computation: 1826 steps/s (collection: 0.563s, learning 3.923s)
               Value function loss: 123.6702
                    Surrogate loss: 0.0104
             Mean action noise std: 0.97
                       Mean reward: 466.83
               Mean episode length: 322.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4071424
                    Iteration time: 4.49s
                        Total time: 2250.49s
                               ETA: 15866.6s

################################################################################
                     [1m Learning iteration 497/4000 [0m

                       Computation: 1790 steps/s (collection: 0.616s, learning 3.959s)
               Value function loss: 176.8123
                    Surrogate loss: 0.0110
             Mean action noise std: 0.97
                       Mean reward: 449.88
               Mean episode length: 311.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 4.57s
                        Total time: 2255.06s
                               ETA: 15862.4s

################################################################################
                     [1m Learning iteration 498/4000 [0m

                       Computation: 1809 steps/s (collection: 0.605s, learning 3.922s)
               Value function loss: 120.7644
                    Surrogate loss: 0.0214
             Mean action noise std: 0.97
                       Mean reward: 447.05
               Mean episode length: 309.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4087808
                    Iteration time: 4.53s
                        Total time: 2259.59s
                               ETA: 15857.9s

################################################################################
                     [1m Learning iteration 499/4000 [0m

                       Computation: 1798 steps/s (collection: 0.607s, learning 3.946s)
               Value function loss: 180.2282
                    Surrogate loss: 0.0160
             Mean action noise std: 0.97
                       Mean reward: 467.79
               Mean episode length: 324.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 4.55s
                        Total time: 2264.14s
                               ETA: 15853.5s

################################################################################
                     [1m Learning iteration 500/4000 [0m

                       Computation: 1835 steps/s (collection: 0.589s, learning 3.874s)
               Value function loss: 127.7416
                    Surrogate loss: 0.0116
             Mean action noise std: 0.97
                       Mean reward: 470.38
               Mean episode length: 326.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4104192
                    Iteration time: 4.46s
                        Total time: 2268.61s
                               ETA: 15848.6s

################################################################################
                     [1m Learning iteration 501/4000 [0m

                       Computation: 1799 steps/s (collection: 0.565s, learning 3.987s)
               Value function loss: 155.3228
                    Surrogate loss: 0.0122
             Mean action noise std: 0.97
                       Mean reward: 468.99
               Mean episode length: 326.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 4.55s
                        Total time: 2273.16s
                               ETA: 15844.2s

################################################################################
                     [1m Learning iteration 502/4000 [0m

                       Computation: 1805 steps/s (collection: 0.605s, learning 3.933s)
               Value function loss: 156.6379
                    Surrogate loss: 0.0109
             Mean action noise std: 0.96
                       Mean reward: 472.37
               Mean episode length: 327.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4120576
                    Iteration time: 4.54s
                        Total time: 2277.70s
                               ETA: 15839.7s

################################################################################
                     [1m Learning iteration 503/4000 [0m

                       Computation: 1847 steps/s (collection: 0.545s, learning 3.889s)
               Value function loss: 232.8717
                    Surrogate loss: 0.0126
             Mean action noise std: 0.97
                       Mean reward: 475.74
               Mean episode length: 327.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 4.43s
                        Total time: 2282.13s
                               ETA: 15834.5s

################################################################################
                     [1m Learning iteration 504/4000 [0m

                       Computation: 1787 steps/s (collection: 0.551s, learning 4.031s)
               Value function loss: 140.8257
                    Surrogate loss: 0.0135
             Mean action noise std: 0.96
                       Mean reward: 460.54
               Mean episode length: 316.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4136960
                    Iteration time: 4.58s
                        Total time: 2286.71s
                               ETA: 15830.4s

################################################################################
                     [1m Learning iteration 505/4000 [0m

                       Computation: 1726 steps/s (collection: 0.681s, learning 4.065s)
               Value function loss: 141.2412
                    Surrogate loss: 0.0149
             Mean action noise std: 0.97
                       Mean reward: 472.85
               Mean episode length: 322.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 4.75s
                        Total time: 2291.46s
                               ETA: 15827.4s

################################################################################
                     [1m Learning iteration 506/4000 [0m

                       Computation: 1728 steps/s (collection: 0.718s, learning 4.022s)
               Value function loss: 187.4541
                    Surrogate loss: 0.0106
             Mean action noise std: 0.96
                       Mean reward: 444.65
               Mean episode length: 303.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4153344
                    Iteration time: 4.74s
                        Total time: 2296.20s
                               ETA: 15824.3s

################################################################################
                     [1m Learning iteration 507/4000 [0m

                       Computation: 1745 steps/s (collection: 0.636s, learning 4.057s)
               Value function loss: 138.1631
                    Surrogate loss: 0.0131
             Mean action noise std: 0.96
                       Mean reward: 426.32
               Mean episode length: 289.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 4.69s
                        Total time: 2300.89s
                               ETA: 15820.9s

################################################################################
                     [1m Learning iteration 508/4000 [0m

                       Computation: 1774 steps/s (collection: 0.613s, learning 4.005s)
               Value function loss: 150.3931
                    Surrogate loss: 0.0156
             Mean action noise std: 0.96
                       Mean reward: 437.50
               Mean episode length: 295.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4169728
                    Iteration time: 4.62s
                        Total time: 2305.51s
                               ETA: 15817.0s

################################################################################
                     [1m Learning iteration 509/4000 [0m

                       Computation: 1811 steps/s (collection: 0.603s, learning 3.921s)
               Value function loss: 168.4073
                    Surrogate loss: 0.0123
             Mean action noise std: 0.96
                       Mean reward: 426.99
               Mean episode length: 288.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 4.52s
                        Total time: 2310.03s
                               ETA: 15812.4s

################################################################################
                     [1m Learning iteration 510/4000 [0m

                       Computation: 1814 steps/s (collection: 0.586s, learning 3.928s)
               Value function loss: 136.2107
                    Surrogate loss: 0.0097
             Mean action noise std: 0.96
                       Mean reward: 428.88
               Mean episode length: 293.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4186112
                    Iteration time: 4.51s
                        Total time: 2314.55s
                               ETA: 15807.8s

################################################################################
                     [1m Learning iteration 511/4000 [0m

                       Computation: 1809 steps/s (collection: 0.617s, learning 3.909s)
               Value function loss: 196.4080
                    Surrogate loss: 0.0117
             Mean action noise std: 0.96
                       Mean reward: 435.51
               Mean episode length: 298.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 4.53s
                        Total time: 2319.07s
                               ETA: 15803.2s

################################################################################
                     [1m Learning iteration 512/4000 [0m

                       Computation: 1788 steps/s (collection: 0.637s, learning 3.944s)
               Value function loss: 187.5676
                    Surrogate loss: 0.0128
             Mean action noise std: 0.96
                       Mean reward: 444.19
               Mean episode length: 303.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4202496
                    Iteration time: 4.58s
                        Total time: 2323.66s
                               ETA: 15799.0s

################################################################################
                     [1m Learning iteration 513/4000 [0m

                       Computation: 1823 steps/s (collection: 0.571s, learning 3.921s)
               Value function loss: 137.0644
                    Surrogate loss: 0.0126
             Mean action noise std: 0.96
                       Mean reward: 454.47
               Mean episode length: 311.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 4.49s
                        Total time: 2328.15s
                               ETA: 15794.3s

################################################################################
                     [1m Learning iteration 514/4000 [0m

                       Computation: 1804 steps/s (collection: 0.633s, learning 3.908s)
               Value function loss: 199.7970
                    Surrogate loss: 0.0095
             Mean action noise std: 0.97
                       Mean reward: 460.01
               Mean episode length: 317.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4218880
                    Iteration time: 4.54s
                        Total time: 2332.69s
                               ETA: 15789.8s

################################################################################
                     [1m Learning iteration 515/4000 [0m

                       Computation: 1787 steps/s (collection: 0.608s, learning 3.975s)
               Value function loss: 144.8193
                    Surrogate loss: 0.0122
             Mean action noise std: 0.97
                       Mean reward: 466.43
               Mean episode length: 321.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 4.58s
                        Total time: 2337.27s
                               ETA: 15785.6s

################################################################################
                     [1m Learning iteration 516/4000 [0m

                       Computation: 1766 steps/s (collection: 0.629s, learning 4.009s)
               Value function loss: 225.8277
                    Surrogate loss: 0.0135
             Mean action noise std: 0.97
                       Mean reward: 461.97
               Mean episode length: 319.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4235264
                    Iteration time: 4.64s
                        Total time: 2341.91s
                               ETA: 15781.8s

################################################################################
                     [1m Learning iteration 517/4000 [0m

                       Computation: 1757 steps/s (collection: 0.617s, learning 4.045s)
               Value function loss: 181.2909
                    Surrogate loss: 0.0093
             Mean action noise std: 0.96
                       Mean reward: 479.81
               Mean episode length: 333.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 4.66s
                        Total time: 2346.57s
                               ETA: 15778.2s

################################################################################
                     [1m Learning iteration 518/4000 [0m

                       Computation: 1793 steps/s (collection: 0.626s, learning 3.942s)
               Value function loss: 156.7752
                    Surrogate loss: 0.0122
             Mean action noise std: 0.96
                       Mean reward: 479.60
               Mean episode length: 332.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4251648
                    Iteration time: 4.57s
                        Total time: 2351.14s
                               ETA: 15773.9s

################################################################################
                     [1m Learning iteration 519/4000 [0m

                       Computation: 1784 steps/s (collection: 0.604s, learning 3.986s)
               Value function loss: 231.0933
                    Surrogate loss: 0.0124
             Mean action noise std: 0.96
                       Mean reward: 479.04
               Mean episode length: 329.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 4.59s
                        Total time: 2355.73s
                               ETA: 15769.8s

################################################################################
                     [1m Learning iteration 520/4000 [0m

                       Computation: 1750 steps/s (collection: 0.667s, learning 4.012s)
               Value function loss: 190.9019
                    Surrogate loss: 0.0103
             Mean action noise std: 0.96
                       Mean reward: 501.31
               Mean episode length: 344.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4268032
                    Iteration time: 4.68s
                        Total time: 2360.41s
                               ETA: 15766.2s

################################################################################
                     [1m Learning iteration 521/4000 [0m

                       Computation: 1807 steps/s (collection: 0.596s, learning 3.937s)
               Value function loss: 147.2494
                    Surrogate loss: 0.0116
             Mean action noise std: 0.96
                       Mean reward: 515.73
               Mean episode length: 355.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 4.53s
                        Total time: 2364.94s
                               ETA: 15761.7s

################################################################################
                     [1m Learning iteration 522/4000 [0m

                       Computation: 1816 steps/s (collection: 0.574s, learning 3.937s)
               Value function loss: 190.8021
                    Surrogate loss: 0.0114
             Mean action noise std: 0.96
                       Mean reward: 515.50
               Mean episode length: 354.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4284416
                    Iteration time: 4.51s
                        Total time: 2369.45s
                               ETA: 15757.1s

################################################################################
                     [1m Learning iteration 523/4000 [0m

                       Computation: 1840 steps/s (collection: 0.542s, learning 3.909s)
               Value function loss: 140.3436
                    Surrogate loss: 0.0132
             Mean action noise std: 0.96
                       Mean reward: 523.02
               Mean episode length: 358.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 4.45s
                        Total time: 2373.90s
                               ETA: 15752.0s

################################################################################
                     [1m Learning iteration 524/4000 [0m

                       Computation: 1827 steps/s (collection: 0.574s, learning 3.908s)
               Value function loss: 205.0042
                    Surrogate loss: 0.0123
             Mean action noise std: 0.96
                       Mean reward: 522.34
               Mean episode length: 359.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4300800
                    Iteration time: 4.48s
                        Total time: 2378.38s
                               ETA: 15747.2s

################################################################################
                     [1m Learning iteration 525/4000 [0m

                       Computation: 1807 steps/s (collection: 0.592s, learning 3.942s)
               Value function loss: 227.6664
                    Surrogate loss: 0.0113
             Mean action noise std: 0.96
                       Mean reward: 535.54
               Mean episode length: 370.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 4.53s
                        Total time: 2382.92s
                               ETA: 15742.7s

################################################################################
                     [1m Learning iteration 526/4000 [0m

                       Computation: 1810 steps/s (collection: 0.595s, learning 3.931s)
               Value function loss: 104.0889
                    Surrogate loss: 0.0124
             Mean action noise std: 0.96
                       Mean reward: 531.82
               Mean episode length: 367.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 4317184
                    Iteration time: 4.53s
                        Total time: 2387.44s
                               ETA: 15738.1s

################################################################################
                     [1m Learning iteration 527/4000 [0m

                       Computation: 1816 steps/s (collection: 0.594s, learning 3.914s)
               Value function loss: 204.8180
                    Surrogate loss: 0.0129
             Mean action noise std: 0.96
                       Mean reward: 534.53
               Mean episode length: 369.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 4.51s
                        Total time: 2391.95s
                               ETA: 15733.4s

################################################################################
                     [1m Learning iteration 528/4000 [0m

                       Computation: 1799 steps/s (collection: 0.639s, learning 3.914s)
               Value function loss: 203.9009
                    Surrogate loss: 0.0101
             Mean action noise std: 0.96
                       Mean reward: 525.99
               Mean episode length: 365.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4333568
                    Iteration time: 4.55s
                        Total time: 2396.50s
                               ETA: 15729.0s

################################################################################
                     [1m Learning iteration 529/4000 [0m

                       Computation: 1843 steps/s (collection: 0.562s, learning 3.881s)
               Value function loss: 219.6493
                    Surrogate loss: 0.0115
             Mean action noise std: 0.96
                       Mean reward: 496.63
               Mean episode length: 344.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 4.44s
                        Total time: 2400.95s
                               ETA: 15723.9s

################################################################################
                     [1m Learning iteration 530/4000 [0m

                       Computation: 1824 steps/s (collection: 0.570s, learning 3.919s)
               Value function loss: 171.5448
                    Surrogate loss: 0.0092
             Mean action noise std: 0.96
                       Mean reward: 489.74
               Mean episode length: 338.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4349952
                    Iteration time: 4.49s
                        Total time: 2405.44s
                               ETA: 15719.1s

################################################################################
                     [1m Learning iteration 531/4000 [0m

                       Computation: 1773 steps/s (collection: 0.661s, learning 3.958s)
               Value function loss: 187.6093
                    Surrogate loss: 0.0114
             Mean action noise std: 0.96
                       Mean reward: 487.15
               Mean episode length: 333.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 4.62s
                        Total time: 2410.06s
                               ETA: 15715.2s

################################################################################
                     [1m Learning iteration 532/4000 [0m

                       Computation: 1811 steps/s (collection: 0.579s, learning 3.943s)
               Value function loss: 187.2943
                    Surrogate loss: 0.0104
             Mean action noise std: 0.96
                       Mean reward: 501.55
               Mean episode length: 340.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4366336
                    Iteration time: 4.52s
                        Total time: 2414.58s
                               ETA: 15710.6s

################################################################################
                     [1m Learning iteration 533/4000 [0m

                       Computation: 1808 steps/s (collection: 0.599s, learning 3.930s)
               Value function loss: 123.1274
                    Surrogate loss: 0.0111
             Mean action noise std: 0.96
                       Mean reward: 483.16
               Mean episode length: 326.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 4.53s
                        Total time: 2419.11s
                               ETA: 15706.1s

################################################################################
                     [1m Learning iteration 534/4000 [0m

                       Computation: 1803 steps/s (collection: 0.589s, learning 3.952s)
               Value function loss: 147.8190
                    Surrogate loss: 0.0125
             Mean action noise std: 0.96
                       Mean reward: 472.31
               Mean episode length: 319.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4382720
                    Iteration time: 4.54s
                        Total time: 2423.65s
                               ETA: 15701.6s

################################################################################
                     [1m Learning iteration 535/4000 [0m

                       Computation: 1795 steps/s (collection: 0.649s, learning 3.914s)
               Value function loss: 224.2959
                    Surrogate loss: 0.0138
             Mean action noise std: 0.96
                       Mean reward: 468.51
               Mean episode length: 316.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 4.56s
                        Total time: 2428.21s
                               ETA: 15697.3s

################################################################################
                     [1m Learning iteration 536/4000 [0m

                       Computation: 1816 steps/s (collection: 0.569s, learning 3.941s)
               Value function loss: 140.6552
                    Surrogate loss: 0.0142
             Mean action noise std: 0.96
                       Mean reward: 461.35
               Mean episode length: 309.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4399104
                    Iteration time: 4.51s
                        Total time: 2432.72s
                               ETA: 15692.6s

################################################################################
                     [1m Learning iteration 537/4000 [0m

                       Computation: 1796 steps/s (collection: 0.656s, learning 3.905s)
               Value function loss: 197.9636
                    Surrogate loss: 0.0099
             Mean action noise std: 0.96
                       Mean reward: 454.13
               Mean episode length: 304.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 4.56s
                        Total time: 2437.28s
                               ETA: 15688.3s

################################################################################
                     [1m Learning iteration 538/4000 [0m

                       Computation: 1807 steps/s (collection: 0.596s, learning 3.936s)
               Value function loss: 182.1390
                    Surrogate loss: 0.0088
             Mean action noise std: 0.96
                       Mean reward: 459.73
               Mean episode length: 309.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4415488
                    Iteration time: 4.53s
                        Total time: 2441.81s
                               ETA: 15683.8s

################################################################################
                     [1m Learning iteration 539/4000 [0m

                       Computation: 1769 steps/s (collection: 0.670s, learning 3.960s)
               Value function loss: 137.8455
                    Surrogate loss: 0.0137
             Mean action noise std: 0.96
                       Mean reward: 466.54
               Mean episode length: 313.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 4.63s
                        Total time: 2446.44s
                               ETA: 15679.9s

################################################################################
                     [1m Learning iteration 540/4000 [0m

                       Computation: 1810 steps/s (collection: 0.565s, learning 3.959s)
               Value function loss: 152.4699
                    Surrogate loss: 0.0153
             Mean action noise std: 0.96
                       Mean reward: 474.97
               Mean episode length: 318.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4431872
                    Iteration time: 4.52s
                        Total time: 2450.97s
                               ETA: 15675.3s

################################################################################
                     [1m Learning iteration 541/4000 [0m

                       Computation: 1829 steps/s (collection: 0.556s, learning 3.923s)
               Value function loss: 135.7612
                    Surrogate loss: 0.0097
             Mean action noise std: 0.96
                       Mean reward: 484.64
               Mean episode length: 324.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 4.48s
                        Total time: 2455.45s
                               ETA: 15670.5s

################################################################################
                     [1m Learning iteration 542/4000 [0m

                       Computation: 1804 steps/s (collection: 0.586s, learning 3.954s)
               Value function loss: 132.6095
                    Surrogate loss: 0.0124
             Mean action noise std: 0.96
                       Mean reward: 492.53
               Mean episode length: 327.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4448256
                    Iteration time: 4.54s
                        Total time: 2459.99s
                               ETA: 15666.0s

################################################################################
                     [1m Learning iteration 543/4000 [0m

                       Computation: 1802 steps/s (collection: 0.604s, learning 3.942s)
               Value function loss: 172.2706
                    Surrogate loss: 0.0130
             Mean action noise std: 0.96
                       Mean reward: 488.33
               Mean episode length: 326.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 4.55s
                        Total time: 2464.53s
                               ETA: 15661.6s

################################################################################
                     [1m Learning iteration 544/4000 [0m

                       Computation: 1815 steps/s (collection: 0.582s, learning 3.931s)
               Value function loss: 169.8550
                    Surrogate loss: 0.0132
             Mean action noise std: 0.96
                       Mean reward: 504.91
               Mean episode length: 336.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4464640
                    Iteration time: 4.51s
                        Total time: 2469.05s
                               ETA: 15656.9s

################################################################################
                     [1m Learning iteration 545/4000 [0m

                       Computation: 1806 steps/s (collection: 0.585s, learning 3.948s)
               Value function loss: 176.2004
                    Surrogate loss: 0.0114
             Mean action noise std: 0.96
                       Mean reward: 533.56
               Mean episode length: 353.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 4.53s
                        Total time: 2473.58s
                               ETA: 15652.4s

################################################################################
                     [1m Learning iteration 546/4000 [0m

                       Computation: 1804 steps/s (collection: 0.568s, learning 3.971s)
               Value function loss: 132.0705
                    Surrogate loss: 0.0152
             Mean action noise std: 0.96
                       Mean reward: 507.33
               Mean episode length: 335.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4481024
                    Iteration time: 4.54s
                        Total time: 2478.12s
                               ETA: 15647.9s

################################################################################
                     [1m Learning iteration 547/4000 [0m

                       Computation: 1801 steps/s (collection: 0.584s, learning 3.963s)
               Value function loss: 174.8201
                    Surrogate loss: 0.0155
             Mean action noise std: 0.96
                       Mean reward: 505.84
               Mean episode length: 335.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 4.55s
                        Total time: 2482.67s
                               ETA: 15643.5s

################################################################################
                     [1m Learning iteration 548/4000 [0m

                       Computation: 1792 steps/s (collection: 0.591s, learning 3.980s)
               Value function loss: 161.2204
                    Surrogate loss: 0.0140
             Mean action noise std: 0.96
                       Mean reward: 484.33
               Mean episode length: 322.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4497408
                    Iteration time: 4.57s
                        Total time: 2487.24s
                               ETA: 15639.2s

################################################################################
                     [1m Learning iteration 549/4000 [0m

                       Computation: 1840 steps/s (collection: 0.545s, learning 3.905s)
               Value function loss: 137.8758
                    Surrogate loss: 0.0112
             Mean action noise std: 0.96
                       Mean reward: 475.71
               Mean episode length: 317.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 4.45s
                        Total time: 2491.69s
                               ETA: 15634.2s

################################################################################
                     [1m Learning iteration 550/4000 [0m

                       Computation: 1803 steps/s (collection: 0.624s, learning 3.917s)
               Value function loss: 154.7460
                    Surrogate loss: 0.0131
             Mean action noise std: 0.96
                       Mean reward: 472.94
               Mean episode length: 315.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4513792
                    Iteration time: 4.54s
                        Total time: 2496.23s
                               ETA: 15629.7s

################################################################################
                     [1m Learning iteration 551/4000 [0m

                       Computation: 1813 steps/s (collection: 0.592s, learning 3.925s)
               Value function loss: 235.8136
                    Surrogate loss: 0.0112
             Mean action noise std: 0.96
                       Mean reward: 487.76
               Mean episode length: 322.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 4.52s
                        Total time: 2500.75s
                               ETA: 15625.1s

################################################################################
                     [1m Learning iteration 552/4000 [0m

                       Computation: 1820 steps/s (collection: 0.592s, learning 3.908s)
               Value function loss: 210.3539
                    Surrogate loss: 0.0138
             Mean action noise std: 0.96
                       Mean reward: 496.97
               Mean episode length: 332.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4530176
                    Iteration time: 4.50s
                        Total time: 2505.25s
                               ETA: 15620.4s

################################################################################
                     [1m Learning iteration 553/4000 [0m

                       Computation: 1828 steps/s (collection: 0.583s, learning 3.897s)
               Value function loss: 151.2775
                    Surrogate loss: 0.0176
             Mean action noise std: 0.96
                       Mean reward: 495.87
               Mean episode length: 331.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 4.48s
                        Total time: 2509.73s
                               ETA: 15615.6s

################################################################################
                     [1m Learning iteration 554/4000 [0m

                       Computation: 1806 steps/s (collection: 0.603s, learning 3.931s)
               Value function loss: 135.3502
                    Surrogate loss: 0.0206
             Mean action noise std: 0.96
                       Mean reward: 495.84
               Mean episode length: 332.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4546560
                    Iteration time: 4.53s
                        Total time: 2514.26s
                               ETA: 15611.1s

################################################################################
                     [1m Learning iteration 555/4000 [0m

                       Computation: 1807 steps/s (collection: 0.619s, learning 3.912s)
               Value function loss: 149.5784
                    Surrogate loss: 0.0134
             Mean action noise std: 0.96
                       Mean reward: 494.96
               Mean episode length: 330.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 4.53s
                        Total time: 2518.79s
                               ETA: 15606.5s

################################################################################
                     [1m Learning iteration 556/4000 [0m

                       Computation: 1823 steps/s (collection: 0.565s, learning 3.927s)
               Value function loss: 171.3767
                    Surrogate loss: 0.0124
             Mean action noise std: 0.96
                       Mean reward: 491.60
               Mean episode length: 327.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4562944
                    Iteration time: 4.49s
                        Total time: 2523.28s
                               ETA: 15601.8s

################################################################################
                     [1m Learning iteration 557/4000 [0m

                       Computation: 1818 steps/s (collection: 0.582s, learning 3.924s)
               Value function loss: 136.4603
                    Surrogate loss: 0.0132
             Mean action noise std: 0.96
                       Mean reward: 489.55
               Mean episode length: 324.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 4.51s
                        Total time: 2527.79s
                               ETA: 15597.1s

################################################################################
                     [1m Learning iteration 558/4000 [0m

                       Computation: 1801 steps/s (collection: 0.571s, learning 3.977s)
               Value function loss: 213.8169
                    Surrogate loss: 0.0124
             Mean action noise std: 0.96
                       Mean reward: 487.43
               Mean episode length: 323.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4579328
                    Iteration time: 4.55s
                        Total time: 2532.34s
                               ETA: 15592.7s

################################################################################
                     [1m Learning iteration 559/4000 [0m

                       Computation: 1786 steps/s (collection: 0.575s, learning 4.010s)
               Value function loss: 213.2871
                    Surrogate loss: 0.0122
             Mean action noise std: 0.96
                       Mean reward: 477.74
               Mean episode length: 317.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 4.58s
                        Total time: 2536.92s
                               ETA: 15588.5s

################################################################################
                     [1m Learning iteration 560/4000 [0m

                       Computation: 1823 steps/s (collection: 0.583s, learning 3.909s)
               Value function loss: 238.2357
                    Surrogate loss: 0.0106
             Mean action noise std: 0.96
                       Mean reward: 478.31
               Mean episode length: 315.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4595712
                    Iteration time: 4.49s
                        Total time: 2541.41s
                               ETA: 15583.7s

################################################################################
                     [1m Learning iteration 561/4000 [0m

                       Computation: 1809 steps/s (collection: 0.577s, learning 3.952s)
               Value function loss: 211.2653
                    Surrogate loss: 0.0092
             Mean action noise std: 0.96
                       Mean reward: 485.72
               Mean episode length: 321.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 4.53s
                        Total time: 2545.94s
                               ETA: 15579.2s

################################################################################
                     [1m Learning iteration 562/4000 [0m

                       Computation: 1821 steps/s (collection: 0.529s, learning 3.968s)
               Value function loss: 188.1395
                    Surrogate loss: 0.0112
             Mean action noise std: 0.96
                       Mean reward: 495.22
               Mean episode length: 328.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4612096
                    Iteration time: 4.50s
                        Total time: 2550.44s
                               ETA: 15574.4s

################################################################################
                     [1m Learning iteration 563/4000 [0m

                       Computation: 1791 steps/s (collection: 0.588s, learning 3.985s)
               Value function loss: 216.3563
                    Surrogate loss: 0.0162
             Mean action noise std: 0.96
                       Mean reward: 502.73
               Mean episode length: 331.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 4.57s
                        Total time: 2555.01s
                               ETA: 15570.2s

################################################################################
                     [1m Learning iteration 564/4000 [0m

                       Computation: 1805 steps/s (collection: 0.566s, learning 3.971s)
               Value function loss: 195.8132
                    Surrogate loss: 0.0116
             Mean action noise std: 0.96
                       Mean reward: 497.41
               Mean episode length: 328.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4628480
                    Iteration time: 4.54s
                        Total time: 2559.55s
                               ETA: 15565.7s

################################################################################
                     [1m Learning iteration 565/4000 [0m

                       Computation: 1772 steps/s (collection: 0.588s, learning 4.032s)
               Value function loss: 187.2303
                    Surrogate loss: 0.0101
             Mean action noise std: 0.96
                       Mean reward: 478.37
               Mean episode length: 317.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 4.62s
                        Total time: 2564.17s
                               ETA: 15561.7s

################################################################################
                     [1m Learning iteration 566/4000 [0m

                       Computation: 1800 steps/s (collection: 0.596s, learning 3.954s)
               Value function loss: 177.0743
                    Surrogate loss: 0.0111
             Mean action noise std: 0.96
                       Mean reward: 478.79
               Mean episode length: 317.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4644864
                    Iteration time: 4.55s
                        Total time: 2568.72s
                               ETA: 15557.3s

################################################################################
                     [1m Learning iteration 567/4000 [0m

                       Computation: 1791 steps/s (collection: 0.595s, learning 3.977s)
               Value function loss: 195.8067
                    Surrogate loss: 0.0139
             Mean action noise std: 0.96
                       Mean reward: 464.73
               Mean episode length: 306.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 4.57s
                        Total time: 2573.29s
                               ETA: 15553.0s

################################################################################
                     [1m Learning iteration 568/4000 [0m

                       Computation: 1791 steps/s (collection: 0.609s, learning 3.963s)
               Value function loss: 163.4782
                    Surrogate loss: 0.0130
             Mean action noise std: 0.96
                       Mean reward: 461.84
               Mean episode length: 306.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4661248
                    Iteration time: 4.57s
                        Total time: 2577.86s
                               ETA: 15548.7s

################################################################################
                     [1m Learning iteration 569/4000 [0m

                       Computation: 1833 steps/s (collection: 0.559s, learning 3.909s)
               Value function loss: 184.9450
                    Surrogate loss: 0.0169
             Mean action noise std: 0.96
                       Mean reward: 450.68
               Mean episode length: 301.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 4.47s
                        Total time: 2582.33s
                               ETA: 15543.8s

################################################################################
                     [1m Learning iteration 570/4000 [0m

                       Computation: 1815 steps/s (collection: 0.579s, learning 3.933s)
               Value function loss: 157.9379
                    Surrogate loss: 0.0171
             Mean action noise std: 0.96
                       Mean reward: 463.28
               Mean episode length: 309.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4677632
                    Iteration time: 4.51s
                        Total time: 2586.84s
                               ETA: 15539.2s

################################################################################
                     [1m Learning iteration 571/4000 [0m

                       Computation: 1835 steps/s (collection: 0.545s, learning 3.919s)
               Value function loss: 190.2216
                    Surrogate loss: 0.0105
             Mean action noise std: 0.96
                       Mean reward: 469.02
               Mean episode length: 315.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 4.46s
                        Total time: 2591.31s
                               ETA: 15534.2s

################################################################################
                     [1m Learning iteration 572/4000 [0m

                       Computation: 1814 steps/s (collection: 0.597s, learning 3.917s)
               Value function loss: 181.2453
                    Surrogate loss: 0.0112
             Mean action noise std: 0.96
                       Mean reward: 474.55
               Mean episode length: 320.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4694016
                    Iteration time: 4.51s
                        Total time: 2595.82s
                               ETA: 15529.6s

################################################################################
                     [1m Learning iteration 573/4000 [0m

                       Computation: 1820 steps/s (collection: 0.584s, learning 3.917s)
               Value function loss: 131.7833
                    Surrogate loss: 0.0131
             Mean action noise std: 0.96
                       Mean reward: 492.50
               Mean episode length: 332.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 4.50s
                        Total time: 2600.32s
                               ETA: 15524.9s

################################################################################
                     [1m Learning iteration 574/4000 [0m

                       Computation: 1834 steps/s (collection: 0.566s, learning 3.900s)
               Value function loss: 159.2708
                    Surrogate loss: 0.0151
             Mean action noise std: 0.96
                       Mean reward: 508.46
               Mean episode length: 343.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4710400
                    Iteration time: 4.47s
                        Total time: 2604.79s
                               ETA: 15520.0s

################################################################################
                     [1m Learning iteration 575/4000 [0m

                       Computation: 1832 steps/s (collection: 0.538s, learning 3.931s)
               Value function loss: 176.2382
                    Surrogate loss: 0.0102
             Mean action noise std: 0.96
                       Mean reward: 507.05
               Mean episode length: 342.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 4.47s
                        Total time: 2609.26s
                               ETA: 15515.1s

################################################################################
                     [1m Learning iteration 576/4000 [0m

                       Computation: 1835 steps/s (collection: 0.547s, learning 3.915s)
               Value function loss: 134.8320
                    Surrogate loss: 0.0105
             Mean action noise std: 0.96
                       Mean reward: 490.04
               Mean episode length: 331.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4726784
                    Iteration time: 4.46s
                        Total time: 2613.72s
                               ETA: 15510.2s

################################################################################
                     [1m Learning iteration 577/4000 [0m

                       Computation: 1831 steps/s (collection: 0.563s, learning 3.909s)
               Value function loss: 159.8716
                    Surrogate loss: 0.0097
             Mean action noise std: 0.96
                       Mean reward: 503.15
               Mean episode length: 339.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 4.47s
                        Total time: 2618.19s
                               ETA: 15505.3s

################################################################################
                     [1m Learning iteration 578/4000 [0m

                       Computation: 1810 steps/s (collection: 0.582s, learning 3.942s)
               Value function loss: 221.1392
                    Surrogate loss: 0.0123
             Mean action noise std: 0.96
                       Mean reward: 508.54
               Mean episode length: 343.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4743168
                    Iteration time: 4.52s
                        Total time: 2622.71s
                               ETA: 15500.7s

################################################################################
                     [1m Learning iteration 579/4000 [0m

                       Computation: 1834 steps/s (collection: 0.564s, learning 3.901s)
               Value function loss: 255.4074
                    Surrogate loss: 0.0112
             Mean action noise std: 0.96
                       Mean reward: 524.04
               Mean episode length: 353.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 4.46s
                        Total time: 2627.18s
                               ETA: 15495.8s

################################################################################
                     [1m Learning iteration 580/4000 [0m

                       Computation: 1820 steps/s (collection: 0.575s, learning 3.924s)
               Value function loss: 278.6180
                    Surrogate loss: 0.0105
             Mean action noise std: 0.96
                       Mean reward: 505.70
               Mean episode length: 341.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 4759552
                    Iteration time: 4.50s
                        Total time: 2631.68s
                               ETA: 15491.1s

################################################################################
                     [1m Learning iteration 581/4000 [0m

                       Computation: 1819 steps/s (collection: 0.580s, learning 3.923s)
               Value function loss: 201.9672
                    Surrogate loss: 0.0127
             Mean action noise std: 0.96
                       Mean reward: 524.23
               Mean episode length: 352.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 4.50s
                        Total time: 2636.18s
                               ETA: 15486.4s

################################################################################
                     [1m Learning iteration 582/4000 [0m

                       Computation: 1836 steps/s (collection: 0.527s, learning 3.934s)
               Value function loss: 238.6735
                    Surrogate loss: 0.0166
             Mean action noise std: 0.96
                       Mean reward: 523.77
               Mean episode length: 353.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4775936
                    Iteration time: 4.46s
                        Total time: 2640.64s
                               ETA: 15481.5s

################################################################################
                     [1m Learning iteration 583/4000 [0m

                       Computation: 1822 steps/s (collection: 0.517s, learning 3.977s)
               Value function loss: 190.8803
                    Surrogate loss: 0.0112
             Mean action noise std: 0.96
                       Mean reward: 517.42
               Mean episode length: 350.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 4.49s
                        Total time: 2645.14s
                               ETA: 15476.8s

################################################################################
                     [1m Learning iteration 584/4000 [0m

                       Computation: 1855 steps/s (collection: 0.531s, learning 3.885s)
               Value function loss: 192.2331
                    Surrogate loss: 0.0134
             Mean action noise std: 0.96
                       Mean reward: 510.60
               Mean episode length: 343.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4792320
                    Iteration time: 4.42s
                        Total time: 2649.55s
                               ETA: 15471.6s

################################################################################
                     [1m Learning iteration 585/4000 [0m

                       Computation: 1845 steps/s (collection: 0.525s, learning 3.913s)
               Value function loss: 170.3040
                    Surrogate loss: 0.0131
             Mean action noise std: 0.96
                       Mean reward: 489.03
               Mean episode length: 327.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 4.44s
                        Total time: 2653.99s
                               ETA: 15466.5s

################################################################################
                     [1m Learning iteration 586/4000 [0m

                       Computation: 1828 steps/s (collection: 0.543s, learning 3.937s)
               Value function loss: 273.9231
                    Surrogate loss: 0.0117
             Mean action noise std: 0.96
                       Mean reward: 487.48
               Mean episode length: 325.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4808704
                    Iteration time: 4.48s
                        Total time: 2658.47s
                               ETA: 15461.7s

################################################################################
                     [1m Learning iteration 587/4000 [0m

                       Computation: 1836 steps/s (collection: 0.527s, learning 3.935s)
               Value function loss: 115.7676
                    Surrogate loss: 0.0113
             Mean action noise std: 0.96
                       Mean reward: 472.65
               Mean episode length: 314.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 4.46s
                        Total time: 2662.93s
                               ETA: 15456.8s

################################################################################
                     [1m Learning iteration 588/4000 [0m

                       Computation: 1815 steps/s (collection: 0.555s, learning 3.957s)
               Value function loss: 188.3668
                    Surrogate loss: 0.0170
             Mean action noise std: 0.96
                       Mean reward: 470.65
               Mean episode length: 314.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4825088
                    Iteration time: 4.51s
                        Total time: 2667.45s
                               ETA: 15452.2s

################################################################################
                     [1m Learning iteration 589/4000 [0m

                       Computation: 1807 steps/s (collection: 0.579s, learning 3.953s)
               Value function loss: 168.4933
                    Surrogate loss: 0.0133
             Mean action noise std: 0.96
                       Mean reward: 454.30
               Mean episode length: 304.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 4.53s
                        Total time: 2671.98s
                               ETA: 15447.6s

################################################################################
                     [1m Learning iteration 590/4000 [0m

                       Computation: 1795 steps/s (collection: 0.529s, learning 4.033s)
               Value function loss: 145.9660
                    Surrogate loss: 0.0121
             Mean action noise std: 0.96
                       Mean reward: 458.81
               Mean episode length: 306.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 4841472
                    Iteration time: 4.56s
                        Total time: 2676.54s
                               ETA: 15443.3s

################################################################################
                     [1m Learning iteration 591/4000 [0m

                       Computation: 1805 steps/s (collection: 0.569s, learning 3.969s)
               Value function loss: 185.0426
                    Surrogate loss: 0.0105
             Mean action noise std: 0.96
                       Mean reward: 462.73
               Mean episode length: 308.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 4.54s
                        Total time: 2681.08s
                               ETA: 15438.8s

################################################################################
                     [1m Learning iteration 592/4000 [0m

                       Computation: 1779 steps/s (collection: 0.555s, learning 4.050s)
               Value function loss: 127.5889
                    Surrogate loss: 0.0136
             Mean action noise std: 0.96
                       Mean reward: 460.81
               Mean episode length: 307.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4857856
                    Iteration time: 4.60s
                        Total time: 2685.68s
                               ETA: 15434.7s

################################################################################
                     [1m Learning iteration 593/4000 [0m

                       Computation: 1802 steps/s (collection: 0.557s, learning 3.987s)
               Value function loss: 174.2510
                    Surrogate loss: 0.0111
             Mean action noise std: 0.96
                       Mean reward: 455.96
               Mean episode length: 305.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 4.54s
                        Total time: 2690.23s
                               ETA: 15430.3s

################################################################################
                     [1m Learning iteration 594/4000 [0m

                       Computation: 1788 steps/s (collection: 0.584s, learning 3.997s)
               Value function loss: 188.0019
                    Surrogate loss: 0.0125
             Mean action noise std: 0.96
                       Mean reward: 468.50
               Mean episode length: 312.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4874240
                    Iteration time: 4.58s
                        Total time: 2694.81s
                               ETA: 15426.1s

################################################################################
                     [1m Learning iteration 595/4000 [0m

                       Computation: 1812 steps/s (collection: 0.591s, learning 3.929s)
               Value function loss: 344.8018
                    Surrogate loss: 0.0124
             Mean action noise std: 0.96
                       Mean reward: 486.33
               Mean episode length: 324.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 4.52s
                        Total time: 2699.33s
                               ETA: 15421.5s

################################################################################
                     [1m Learning iteration 596/4000 [0m

                       Computation: 1786 steps/s (collection: 0.630s, learning 3.957s)
               Value function loss: 232.7156
                    Surrogate loss: 0.0102
             Mean action noise std: 0.96
                       Mean reward: 498.21
               Mean episode length: 331.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4890624
                    Iteration time: 4.59s
                        Total time: 2703.91s
                               ETA: 15417.3s

################################################################################
                     [1m Learning iteration 597/4000 [0m

                       Computation: 1789 steps/s (collection: 0.645s, learning 3.933s)
               Value function loss: 186.6610
                    Surrogate loss: 0.0112
             Mean action noise std: 0.96
                       Mean reward: 516.84
               Mean episode length: 342.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 4.58s
                        Total time: 2708.49s
                               ETA: 15413.0s

################################################################################
                     [1m Learning iteration 598/4000 [0m

                       Computation: 1817 steps/s (collection: 0.567s, learning 3.939s)
               Value function loss: 222.5792
                    Surrogate loss: 0.0130
             Mean action noise std: 0.96
                       Mean reward: 539.46
               Mean episode length: 355.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4907008
                    Iteration time: 4.51s
                        Total time: 2713.00s
                               ETA: 15408.4s

################################################################################
                     [1m Learning iteration 599/4000 [0m

                       Computation: 1840 steps/s (collection: 0.556s, learning 3.894s)
               Value function loss: 258.3734
                    Surrogate loss: 0.0099
             Mean action noise std: 0.96
                       Mean reward: 539.13
               Mean episode length: 355.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 4.45s
                        Total time: 2717.45s
                               ETA: 15403.4s

################################################################################
                     [1m Learning iteration 600/4000 [0m

                       Computation: 1779 steps/s (collection: 0.619s, learning 3.984s)
               Value function loss: 173.3446
                    Surrogate loss: 0.0109
             Mean action noise std: 0.96
                       Mean reward: 538.30
               Mean episode length: 355.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4923392
                    Iteration time: 4.60s
                        Total time: 2722.05s
                               ETA: 15399.3s

################################################################################
                     [1m Learning iteration 601/4000 [0m

                       Computation: 1815 steps/s (collection: 0.608s, learning 3.905s)
               Value function loss: 236.3763
                    Surrogate loss: 0.0101
             Mean action noise std: 0.96
                       Mean reward: 561.79
               Mean episode length: 369.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 4.51s
                        Total time: 2726.56s
                               ETA: 15394.7s

################################################################################
                     [1m Learning iteration 602/4000 [0m

                       Computation: 1837 steps/s (collection: 0.548s, learning 3.910s)
               Value function loss: 258.5747
                    Surrogate loss: 0.0109
             Mean action noise std: 0.96
                       Mean reward: 591.85
               Mean episode length: 389.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4939776
                    Iteration time: 4.46s
                        Total time: 2731.02s
                               ETA: 15389.7s

################################################################################
                     [1m Learning iteration 603/4000 [0m

                       Computation: 1834 steps/s (collection: 0.525s, learning 3.941s)
               Value function loss: 126.2813
                    Surrogate loss: 0.0135
             Mean action noise std: 0.96
                       Mean reward: 590.75
               Mean episode length: 387.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 4.47s
                        Total time: 2735.49s
                               ETA: 15384.9s

################################################################################
                     [1m Learning iteration 604/4000 [0m

                       Computation: 1814 steps/s (collection: 0.543s, learning 3.971s)
               Value function loss: 186.1846
                    Surrogate loss: 0.0111
             Mean action noise std: 0.96
                       Mean reward: 605.20
               Mean episode length: 394.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4956160
                    Iteration time: 4.51s
                        Total time: 2740.00s
                               ETA: 15380.2s

################################################################################
                     [1m Learning iteration 605/4000 [0m

                       Computation: 1841 steps/s (collection: 0.524s, learning 3.925s)
               Value function loss: 212.4801
                    Surrogate loss: 0.0120
             Mean action noise std: 0.96
                       Mean reward: 594.70
               Mean episode length: 387.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 4.45s
                        Total time: 2744.45s
                               ETA: 15375.3s

################################################################################
                     [1m Learning iteration 606/4000 [0m

                       Computation: 1819 steps/s (collection: 0.574s, learning 3.930s)
               Value function loss: 168.9040
                    Surrogate loss: 0.0130
             Mean action noise std: 0.96
                       Mean reward: 609.66
               Mean episode length: 399.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4972544
                    Iteration time: 4.50s
                        Total time: 2748.95s
                               ETA: 15370.6s

################################################################################
                     [1m Learning iteration 607/4000 [0m

                       Computation: 1836 steps/s (collection: 0.539s, learning 3.921s)
               Value function loss: 228.5902
                    Surrogate loss: 0.0192
             Mean action noise std: 0.96
                       Mean reward: 592.42
               Mean episode length: 388.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 4.46s
                        Total time: 2753.41s
                               ETA: 15365.7s

################################################################################
                     [1m Learning iteration 608/4000 [0m

                       Computation: 1824 steps/s (collection: 0.554s, learning 3.936s)
               Value function loss: 112.8027
                    Surrogate loss: 0.0229
             Mean action noise std: 0.96
                       Mean reward: 577.53
               Mean episode length: 378.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4988928
                    Iteration time: 4.49s
                        Total time: 2757.90s
                               ETA: 15360.9s

################################################################################
                     [1m Learning iteration 609/4000 [0m

                       Computation: 1801 steps/s (collection: 0.602s, learning 3.944s)
               Value function loss: 180.8545
                    Surrogate loss: 0.0203
             Mean action noise std: 0.96
                       Mean reward: 576.38
               Mean episode length: 376.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 4.55s
                        Total time: 2762.45s
                               ETA: 15356.5s

################################################################################
                     [1m Learning iteration 610/4000 [0m

                       Computation: 1830 steps/s (collection: 0.585s, learning 3.890s)
               Value function loss: 264.7385
                    Surrogate loss: 0.0116
             Mean action noise std: 0.96
                       Mean reward: 577.54
               Mean episode length: 377.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5005312
                    Iteration time: 4.47s
                        Total time: 2766.92s
                               ETA: 15351.7s

################################################################################
                     [1m Learning iteration 611/4000 [0m

                       Computation: 1796 steps/s (collection: 0.597s, learning 3.963s)
               Value function loss: 239.2849
                    Surrogate loss: 0.0075
             Mean action noise std: 0.96
                       Mean reward: 548.25
               Mean episode length: 358.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 4.56s
                        Total time: 2771.48s
                               ETA: 15347.3s

################################################################################
                     [1m Learning iteration 612/4000 [0m

                       Computation: 1827 steps/s (collection: 0.541s, learning 3.941s)
               Value function loss: 256.5957
                    Surrogate loss: 0.0081
             Mean action noise std: 0.96
                       Mean reward: 555.18
               Mean episode length: 363.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5021696
                    Iteration time: 4.48s
                        Total time: 2775.97s
                               ETA: 15342.5s

################################################################################
                     [1m Learning iteration 613/4000 [0m

                       Computation: 1813 steps/s (collection: 0.610s, learning 3.906s)
               Value function loss: 123.7700
                    Surrogate loss: 0.0088
             Mean action noise std: 0.96
                       Mean reward: 521.16
               Mean episode length: 343.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 4.52s
                        Total time: 2780.48s
                               ETA: 15337.9s

################################################################################
                     [1m Learning iteration 614/4000 [0m

                       Computation: 1796 steps/s (collection: 0.637s, learning 3.923s)
               Value function loss: 182.0156
                    Surrogate loss: 0.0116
             Mean action noise std: 0.96
                       Mean reward: 515.68
               Mean episode length: 339.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5038080
                    Iteration time: 4.56s
                        Total time: 2785.04s
                               ETA: 15333.6s

################################################################################
                     [1m Learning iteration 615/4000 [0m

                       Computation: 1825 steps/s (collection: 0.593s, learning 3.894s)
               Value function loss: 269.0316
                    Surrogate loss: 0.0082
             Mean action noise std: 0.96
                       Mean reward: 535.30
               Mean episode length: 349.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 4.49s
                        Total time: 2789.53s
                               ETA: 15328.8s

################################################################################
                     [1m Learning iteration 616/4000 [0m

                       Computation: 1809 steps/s (collection: 0.585s, learning 3.943s)
               Value function loss: 197.8616
                    Surrogate loss: 0.0097
             Mean action noise std: 0.96
                       Mean reward: 546.33
               Mean episode length: 355.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5054464
                    Iteration time: 4.53s
                        Total time: 2794.06s
                               ETA: 15324.3s

################################################################################
                     [1m Learning iteration 617/4000 [0m

                       Computation: 1811 steps/s (collection: 0.556s, learning 3.965s)
               Value function loss: 136.9698
                    Surrogate loss: 0.0128
             Mean action noise std: 0.96
                       Mean reward: 558.14
               Mean episode length: 363.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 4.52s
                        Total time: 2798.58s
                               ETA: 15319.7s

################################################################################
                     [1m Learning iteration 618/4000 [0m

                       Computation: 1792 steps/s (collection: 0.608s, learning 3.963s)
               Value function loss: 176.2048
                    Surrogate loss: 0.0105
             Mean action noise std: 0.96
                       Mean reward: 566.15
               Mean episode length: 367.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5070848
                    Iteration time: 4.57s
                        Total time: 2803.15s
                               ETA: 15315.4s

################################################################################
                     [1m Learning iteration 619/4000 [0m

                       Computation: 1818 steps/s (collection: 0.590s, learning 3.914s)
               Value function loss: 134.5932
                    Surrogate loss: 0.0078
             Mean action noise std: 0.96
                       Mean reward: 548.61
               Mean episode length: 356.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 4.50s
                        Total time: 2807.65s
                               ETA: 15310.8s

################################################################################
                     [1m Learning iteration 620/4000 [0m

                       Computation: 1814 steps/s (collection: 0.590s, learning 3.924s)
               Value function loss: 162.9315
                    Surrogate loss: 0.0100
             Mean action noise std: 0.96
                       Mean reward: 552.47
               Mean episode length: 360.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5087232
                    Iteration time: 4.51s
                        Total time: 2812.17s
                               ETA: 15306.2s

################################################################################
                     [1m Learning iteration 621/4000 [0m

                       Computation: 1811 steps/s (collection: 0.614s, learning 3.909s)
               Value function loss: 197.9179
                    Surrogate loss: 0.0130
             Mean action noise std: 0.96
                       Mean reward: 548.16
               Mean episode length: 357.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 4.52s
                        Total time: 2816.69s
                               ETA: 15301.6s

################################################################################
                     [1m Learning iteration 622/4000 [0m

                       Computation: 1828 steps/s (collection: 0.571s, learning 3.911s)
               Value function loss: 91.9793
                    Surrogate loss: 0.0180
             Mean action noise std: 0.96
                       Mean reward: 556.54
               Mean episode length: 364.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5103616
                    Iteration time: 4.48s
                        Total time: 2821.17s
                               ETA: 15296.8s

################################################################################
                     [1m Learning iteration 623/4000 [0m

                       Computation: 1822 steps/s (collection: 0.571s, learning 3.923s)
               Value function loss: 175.0852
                    Surrogate loss: 0.0093
             Mean action noise std: 0.96
                       Mean reward: 551.58
               Mean episode length: 359.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 4.49s
                        Total time: 2825.67s
                               ETA: 15292.1s

################################################################################
                     [1m Learning iteration 624/4000 [0m

                       Computation: 1792 steps/s (collection: 0.606s, learning 3.965s)
               Value function loss: 209.0307
                    Surrogate loss: 0.0122
             Mean action noise std: 0.96
                       Mean reward: 538.05
               Mean episode length: 350.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5120000
                    Iteration time: 4.57s
                        Total time: 2830.24s
                               ETA: 15287.8s

################################################################################
                     [1m Learning iteration 625/4000 [0m

                       Computation: 1842 steps/s (collection: 0.565s, learning 3.882s)
               Value function loss: 180.2891
                    Surrogate loss: 0.0139
             Mean action noise std: 0.96
                       Mean reward: 527.66
               Mean episode length: 345.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 4.45s
                        Total time: 2834.68s
                               ETA: 15282.8s

################################################################################
                     [1m Learning iteration 626/4000 [0m

                       Computation: 1812 steps/s (collection: 0.579s, learning 3.941s)
               Value function loss: 298.4844
                    Surrogate loss: 0.0115
             Mean action noise std: 0.96
                       Mean reward: 533.94
               Mean episode length: 346.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5136384
                    Iteration time: 4.52s
                        Total time: 2839.20s
                               ETA: 15278.3s

################################################################################
                     [1m Learning iteration 627/4000 [0m

                       Computation: 1813 steps/s (collection: 0.576s, learning 3.942s)
               Value function loss: 216.7095
                    Surrogate loss: 0.0100
             Mean action noise std: 0.96
                       Mean reward: 528.92
               Mean episode length: 344.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 4.52s
                        Total time: 2843.72s
                               ETA: 15273.7s

################################################################################
                     [1m Learning iteration 628/4000 [0m

                       Computation: 1847 steps/s (collection: 0.532s, learning 3.903s)
               Value function loss: 215.9338
                    Surrogate loss: 0.0079
             Mean action noise std: 0.96
                       Mean reward: 533.38
               Mean episode length: 345.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5152768
                    Iteration time: 4.44s
                        Total time: 2848.16s
                               ETA: 15268.7s

################################################################################
                     [1m Learning iteration 629/4000 [0m

                       Computation: 1814 steps/s (collection: 0.559s, learning 3.955s)
               Value function loss: 250.1298
                    Surrogate loss: 0.0119
             Mean action noise std: 0.96
                       Mean reward: 530.96
               Mean episode length: 346.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 4.51s
                        Total time: 2852.67s
                               ETA: 15264.1s

################################################################################
                     [1m Learning iteration 630/4000 [0m

                       Computation: 1820 steps/s (collection: 0.560s, learning 3.939s)
               Value function loss: 209.0143
                    Surrogate loss: 0.0084
             Mean action noise std: 0.96
                       Mean reward: 540.23
               Mean episode length: 350.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5169152
                    Iteration time: 4.50s
                        Total time: 2857.17s
                               ETA: 15259.4s

################################################################################
                     [1m Learning iteration 631/4000 [0m

                       Computation: 1829 steps/s (collection: 0.544s, learning 3.933s)
               Value function loss: 165.3837
                    Surrogate loss: 0.0108
             Mean action noise std: 0.96
                       Mean reward: 552.02
               Mean episode length: 359.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 4.48s
                        Total time: 2861.65s
                               ETA: 15254.6s

################################################################################
                     [1m Learning iteration 632/4000 [0m

                       Computation: 1821 steps/s (collection: 0.566s, learning 3.932s)
               Value function loss: 146.0170
                    Surrogate loss: 0.0108
             Mean action noise std: 0.96
                       Mean reward: 553.24
               Mean episode length: 360.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5185536
                    Iteration time: 4.50s
                        Total time: 2866.14s
                               ETA: 15249.9s

################################################################################
                     [1m Learning iteration 633/4000 [0m

                       Computation: 1800 steps/s (collection: 0.624s, learning 3.925s)
               Value function loss: 162.2169
                    Surrogate loss: 0.0112
             Mean action noise std: 0.96
                       Mean reward: 544.44
               Mean episode length: 355.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 4.55s
                        Total time: 2870.69s
                               ETA: 15245.5s

################################################################################
                     [1m Learning iteration 634/4000 [0m

                       Computation: 1826 steps/s (collection: 0.556s, learning 3.929s)
               Value function loss: 170.4852
                    Surrogate loss: 0.0105
             Mean action noise std: 0.96
                       Mean reward: 518.31
               Mean episode length: 340.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5201920
                    Iteration time: 4.49s
                        Total time: 2875.18s
                               ETA: 15240.7s

################################################################################
                     [1m Learning iteration 635/4000 [0m

                       Computation: 1826 steps/s (collection: 0.576s, learning 3.909s)
               Value function loss: 264.2117
                    Surrogate loss: 0.0104
             Mean action noise std: 0.96
                       Mean reward: 522.89
               Mean episode length: 343.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 4.49s
                        Total time: 2879.66s
                               ETA: 15236.0s

################################################################################
                     [1m Learning iteration 636/4000 [0m

                       Computation: 1819 steps/s (collection: 0.542s, learning 3.961s)
               Value function loss: 153.3855
                    Surrogate loss: 0.0125
             Mean action noise std: 0.96
                       Mean reward: 504.77
               Mean episode length: 331.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5218304
                    Iteration time: 4.50s
                        Total time: 2884.17s
                               ETA: 15231.3s

################################################################################
                     [1m Learning iteration 637/4000 [0m

                       Computation: 1799 steps/s (collection: 0.576s, learning 3.976s)
               Value function loss: 243.0691
                    Surrogate loss: 0.0109
             Mean action noise std: 0.96
                       Mean reward: 500.75
               Mean episode length: 327.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 4.55s
                        Total time: 2888.72s
                               ETA: 15226.9s

################################################################################
                     [1m Learning iteration 638/4000 [0m

                       Computation: 1813 steps/s (collection: 0.586s, learning 3.931s)
               Value function loss: 138.8918
                    Surrogate loss: 0.0090
             Mean action noise std: 0.96
                       Mean reward: 463.92
               Mean episode length: 304.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5234688
                    Iteration time: 4.52s
                        Total time: 2893.24s
                               ETA: 15222.3s

################################################################################
                     [1m Learning iteration 639/4000 [0m

                       Computation: 1830 steps/s (collection: 0.576s, learning 3.899s)
               Value function loss: 149.9349
                    Surrogate loss: 0.0076
             Mean action noise std: 0.96
                       Mean reward: 435.22
               Mean episode length: 285.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 4.48s
                        Total time: 2897.71s
                               ETA: 15217.5s

################################################################################
                     [1m Learning iteration 640/4000 [0m

                       Computation: 1807 steps/s (collection: 0.571s, learning 3.962s)
               Value function loss: 216.8977
                    Surrogate loss: 0.0118
             Mean action noise std: 0.96
                       Mean reward: 442.97
               Mean episode length: 288.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5251072
                    Iteration time: 4.53s
                        Total time: 2902.25s
                               ETA: 15213.0s

################################################################################
                     [1m Learning iteration 641/4000 [0m

                       Computation: 1835 steps/s (collection: 0.559s, learning 3.904s)
               Value function loss: 200.9234
                    Surrogate loss: 0.0125
             Mean action noise std: 0.96
                       Mean reward: 442.66
               Mean episode length: 288.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 4.46s
                        Total time: 2906.71s
                               ETA: 15208.2s

################################################################################
                     [1m Learning iteration 642/4000 [0m

                       Computation: 1739 steps/s (collection: 0.606s, learning 4.104s)
               Value function loss: 192.0713
                    Surrogate loss: 0.0148
             Mean action noise std: 0.96
                       Mean reward: 435.90
               Mean episode length: 285.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5267456
                    Iteration time: 4.71s
                        Total time: 2911.42s
                               ETA: 15204.6s

################################################################################
                     [1m Learning iteration 643/4000 [0m

                       Computation: 1807 steps/s (collection: 0.563s, learning 3.970s)
               Value function loss: 178.0883
                    Surrogate loss: 0.0089
             Mean action noise std: 0.96
                       Mean reward: 444.15
               Mean episode length: 292.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 4.53s
                        Total time: 2915.95s
                               ETA: 15200.1s

################################################################################
                     [1m Learning iteration 644/4000 [0m

                       Computation: 1810 steps/s (collection: 0.582s, learning 3.942s)
               Value function loss: 189.7000
                    Surrogate loss: 0.0094
             Mean action noise std: 0.96
                       Mean reward: 448.14
               Mean episode length: 294.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5283840
                    Iteration time: 4.52s
                        Total time: 2920.48s
                               ETA: 15195.5s

################################################################################
                     [1m Learning iteration 645/4000 [0m

                       Computation: 1809 steps/s (collection: 0.554s, learning 3.973s)
               Value function loss: 183.9162
                    Surrogate loss: 0.0105
             Mean action noise std: 0.96
                       Mean reward: 462.19
               Mean episode length: 303.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 4.53s
                        Total time: 2925.00s
                               ETA: 15191.0s

################################################################################
                     [1m Learning iteration 646/4000 [0m

                       Computation: 1804 steps/s (collection: 0.551s, learning 3.988s)
               Value function loss: 224.2122
                    Surrogate loss: 0.0112
             Mean action noise std: 0.96
                       Mean reward: 485.25
               Mean episode length: 317.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5300224
                    Iteration time: 4.54s
                        Total time: 2929.54s
                               ETA: 15186.5s

################################################################################
                     [1m Learning iteration 647/4000 [0m

                       Computation: 1806 steps/s (collection: 0.564s, learning 3.971s)
               Value function loss: 188.1756
                    Surrogate loss: 0.0102
             Mean action noise std: 0.96
                       Mean reward: 505.15
               Mean episode length: 330.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 4.53s
                        Total time: 2934.08s
                               ETA: 15182.0s

################################################################################
                     [1m Learning iteration 648/4000 [0m

                       Computation: 1852 steps/s (collection: 0.527s, learning 3.895s)
               Value function loss: 152.5216
                    Surrogate loss: 0.0142
             Mean action noise std: 0.96
                       Mean reward: 529.11
               Mean episode length: 345.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5316608
                    Iteration time: 4.42s
                        Total time: 2938.50s
                               ETA: 15177.0s

################################################################################
                     [1m Learning iteration 649/4000 [0m

                       Computation: 1829 steps/s (collection: 0.576s, learning 3.901s)
               Value function loss: 240.4193
                    Surrogate loss: 0.0119
             Mean action noise std: 0.96
                       Mean reward: 526.22
               Mean episode length: 345.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 4.48s
                        Total time: 2942.98s
                               ETA: 15172.2s

################################################################################
                     [1m Learning iteration 650/4000 [0m

                       Computation: 1831 steps/s (collection: 0.540s, learning 3.933s)
               Value function loss: 189.2139
                    Surrogate loss: 0.0111
             Mean action noise std: 0.96
                       Mean reward: 522.45
               Mean episode length: 343.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5332992
                    Iteration time: 4.47s
                        Total time: 2947.45s
                               ETA: 15167.4s

################################################################################
                     [1m Learning iteration 651/4000 [0m

                       Computation: 1838 steps/s (collection: 0.528s, learning 3.928s)
               Value function loss: 239.8171
                    Surrogate loss: 0.0096
             Mean action noise std: 0.96
                       Mean reward: 546.62
               Mean episode length: 359.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 4.46s
                        Total time: 2951.91s
                               ETA: 15162.5s

################################################################################
                     [1m Learning iteration 652/4000 [0m

                       Computation: 1823 steps/s (collection: 0.563s, learning 3.929s)
               Value function loss: 166.4123
                    Surrogate loss: 0.0113
             Mean action noise std: 0.96
                       Mean reward: 551.74
               Mean episode length: 361.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5349376
                    Iteration time: 4.49s
                        Total time: 2956.40s
                               ETA: 15157.8s

################################################################################
                     [1m Learning iteration 653/4000 [0m

                       Computation: 1824 steps/s (collection: 0.561s, learning 3.928s)
               Value function loss: 248.8554
                    Surrogate loss: 0.0130
             Mean action noise std: 0.95
                       Mean reward: 563.64
               Mean episode length: 371.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 4.49s
                        Total time: 2960.89s
                               ETA: 15153.0s

################################################################################
                     [1m Learning iteration 654/4000 [0m

                       Computation: 1834 steps/s (collection: 0.538s, learning 3.928s)
               Value function loss: 166.1398
                    Surrogate loss: 0.0127
             Mean action noise std: 0.95
                       Mean reward: 565.87
               Mean episode length: 373.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5365760
                    Iteration time: 4.47s
                        Total time: 2965.35s
                               ETA: 15148.2s

################################################################################
                     [1m Learning iteration 655/4000 [0m

                       Computation: 1828 steps/s (collection: 0.553s, learning 3.928s)
               Value function loss: 241.0117
                    Surrogate loss: 0.0122
             Mean action noise std: 0.95
                       Mean reward: 563.21
               Mean episode length: 372.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 4.48s
                        Total time: 2969.83s
                               ETA: 15143.4s

################################################################################
                     [1m Learning iteration 656/4000 [0m

                       Computation: 1847 steps/s (collection: 0.543s, learning 3.890s)
               Value function loss: 254.4755
                    Surrogate loss: 0.0136
             Mean action noise std: 0.95
                       Mean reward: 575.49
               Mean episode length: 380.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5382144
                    Iteration time: 4.43s
                        Total time: 2974.27s
                               ETA: 15138.4s

################################################################################
                     [1m Learning iteration 657/4000 [0m

                       Computation: 1846 steps/s (collection: 0.512s, learning 3.925s)
               Value function loss: 183.8772
                    Surrogate loss: 0.0098
             Mean action noise std: 0.95
                       Mean reward: 587.71
               Mean episode length: 387.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 4.44s
                        Total time: 2978.70s
                               ETA: 15133.4s

################################################################################
                     [1m Learning iteration 658/4000 [0m

                       Computation: 1833 steps/s (collection: 0.565s, learning 3.904s)
               Value function loss: 194.8212
                    Surrogate loss: 0.0104
             Mean action noise std: 0.95
                       Mean reward: 593.83
               Mean episode length: 391.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5398528
                    Iteration time: 4.47s
                        Total time: 2983.17s
                               ETA: 15128.6s

################################################################################
                     [1m Learning iteration 659/4000 [0m

                       Computation: 1829 steps/s (collection: 0.542s, learning 3.937s)
               Value function loss: 234.2498
                    Surrogate loss: 0.0101
             Mean action noise std: 0.95
                       Mean reward: 597.24
               Mean episode length: 391.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 4.48s
                        Total time: 2987.65s
                               ETA: 15123.9s

################################################################################
                     [1m Learning iteration 660/4000 [0m

                       Computation: 1832 steps/s (collection: 0.578s, learning 3.893s)
               Value function loss: 187.6631
                    Surrogate loss: 0.0118
             Mean action noise std: 0.95
                       Mean reward: 570.69
               Mean episode length: 374.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5414912
                    Iteration time: 4.47s
                        Total time: 2992.12s
                               ETA: 15119.0s

################################################################################
                     [1m Learning iteration 661/4000 [0m

                       Computation: 1830 steps/s (collection: 0.561s, learning 3.914s)
               Value function loss: 227.9840
                    Surrogate loss: 0.0108
             Mean action noise std: 0.95
                       Mean reward: 544.52
               Mean episode length: 360.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 4.48s
                        Total time: 2996.60s
                               ETA: 15114.3s

################################################################################
                     [1m Learning iteration 662/4000 [0m

                       Computation: 1823 steps/s (collection: 0.567s, learning 3.925s)
               Value function loss: 202.1013
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 545.77
               Mean episode length: 358.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5431296
                    Iteration time: 4.49s
                        Total time: 3001.09s
                               ETA: 15109.6s

################################################################################
                     [1m Learning iteration 663/4000 [0m

                       Computation: 1846 steps/s (collection: 0.532s, learning 3.904s)
               Value function loss: 206.3021
                    Surrogate loss: 0.0116
             Mean action noise std: 0.95
                       Mean reward: 559.03
               Mean episode length: 365.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 4.44s
                        Total time: 3005.53s
                               ETA: 15104.6s

################################################################################
                     [1m Learning iteration 664/4000 [0m

                       Computation: 1850 steps/s (collection: 0.531s, learning 3.894s)
               Value function loss: 206.6675
                    Surrogate loss: 0.0094
             Mean action noise std: 0.95
                       Mean reward: 557.36
               Mean episode length: 363.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5447680
                    Iteration time: 4.43s
                        Total time: 3009.95s
                               ETA: 15099.5s

################################################################################
                     [1m Learning iteration 665/4000 [0m

                       Computation: 1846 steps/s (collection: 0.506s, learning 3.931s)
               Value function loss: 234.0536
                    Surrogate loss: 0.0097
             Mean action noise std: 0.95
                       Mean reward: 558.70
               Mean episode length: 361.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 4.44s
                        Total time: 3014.39s
                               ETA: 15094.6s

################################################################################
                     [1m Learning iteration 666/4000 [0m

                       Computation: 1843 steps/s (collection: 0.530s, learning 3.914s)
               Value function loss: 223.4954
                    Surrogate loss: 0.0117
             Mean action noise std: 0.95
                       Mean reward: 554.59
               Mean episode length: 361.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5464064
                    Iteration time: 4.44s
                        Total time: 3018.83s
                               ETA: 15089.6s

################################################################################
                     [1m Learning iteration 667/4000 [0m

                       Computation: 1859 steps/s (collection: 0.533s, learning 3.872s)
               Value function loss: 188.8794
                    Surrogate loss: 0.0146
             Mean action noise std: 0.95
                       Mean reward: 556.76
               Mean episode length: 364.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 4.40s
                        Total time: 3023.24s
                               ETA: 15084.5s

################################################################################
                     [1m Learning iteration 668/4000 [0m

                       Computation: 1818 steps/s (collection: 0.498s, learning 4.006s)
               Value function loss: 163.6205
                    Surrogate loss: 0.0131
             Mean action noise std: 0.95
                       Mean reward: 548.87
               Mean episode length: 359.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5480448
                    Iteration time: 4.50s
                        Total time: 3027.74s
                               ETA: 15079.9s

################################################################################
                     [1m Learning iteration 669/4000 [0m

                       Computation: 1810 steps/s (collection: 0.540s, learning 3.985s)
               Value function loss: 258.6862
                    Surrogate loss: 0.0106
             Mean action noise std: 0.95
                       Mean reward: 566.52
               Mean episode length: 372.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 4.53s
                        Total time: 3032.27s
                               ETA: 15075.3s

################################################################################
                     [1m Learning iteration 670/4000 [0m

                       Computation: 1812 steps/s (collection: 0.557s, learning 3.962s)
               Value function loss: 215.7979
                    Surrogate loss: 0.0128
             Mean action noise std: 0.95
                       Mean reward: 583.20
               Mean episode length: 381.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5496832
                    Iteration time: 4.52s
                        Total time: 3036.79s
                               ETA: 15070.8s

################################################################################
                     [1m Learning iteration 671/4000 [0m

                       Computation: 1786 steps/s (collection: 0.549s, learning 4.037s)
               Value function loss: 221.2292
                    Surrogate loss: 0.0105
             Mean action noise std: 0.96
                       Mean reward: 580.95
               Mean episode length: 379.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 4.59s
                        Total time: 3041.37s
                               ETA: 15066.6s

################################################################################
                     [1m Learning iteration 672/4000 [0m

                       Computation: 1798 steps/s (collection: 0.553s, learning 4.003s)
               Value function loss: 177.0064
                    Surrogate loss: 0.0111
             Mean action noise std: 0.96
                       Mean reward: 584.83
               Mean episode length: 382.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5513216
                    Iteration time: 4.56s
                        Total time: 3045.93s
                               ETA: 15062.2s

################################################################################
                     [1m Learning iteration 673/4000 [0m

                       Computation: 1827 steps/s (collection: 0.508s, learning 3.974s)
               Value function loss: 131.1772
                    Surrogate loss: 0.0122
             Mean action noise std: 0.96
                       Mean reward: 570.94
               Mean episode length: 375.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 4.48s
                        Total time: 3050.41s
                               ETA: 15057.4s

################################################################################
                     [1m Learning iteration 674/4000 [0m

                       Computation: 1795 steps/s (collection: 0.545s, learning 4.019s)
               Value function loss: 216.6462
                    Surrogate loss: 0.0133
             Mean action noise std: 0.95
                       Mean reward: 511.40
               Mean episode length: 336.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 5529600
                    Iteration time: 4.56s
                        Total time: 3054.97s
                               ETA: 15053.1s

################################################################################
                     [1m Learning iteration 675/4000 [0m

                       Computation: 1793 steps/s (collection: 0.630s, learning 3.939s)
               Value function loss: 170.9133
                    Surrogate loss: 0.0129
             Mean action noise std: 0.96
                       Mean reward: 516.33
               Mean episode length: 341.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 4.57s
                        Total time: 3059.54s
                               ETA: 15048.8s

################################################################################
                     [1m Learning iteration 676/4000 [0m

                       Computation: 1810 steps/s (collection: 0.573s, learning 3.951s)
               Value function loss: 168.8920
                    Surrogate loss: 0.0120
             Mean action noise std: 0.96
                       Mean reward: 494.82
               Mean episode length: 324.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5545984
                    Iteration time: 4.52s
                        Total time: 3064.07s
                               ETA: 15044.2s

################################################################################
                     [1m Learning iteration 677/4000 [0m

                       Computation: 1790 steps/s (collection: 0.633s, learning 3.942s)
               Value function loss: 200.9545
                    Surrogate loss: 0.0152
             Mean action noise std: 0.95
                       Mean reward: 472.32
               Mean episode length: 310.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 4.57s
                        Total time: 3068.64s
                               ETA: 15040.0s

################################################################################
                     [1m Learning iteration 678/4000 [0m

                       Computation: 1812 steps/s (collection: 0.590s, learning 3.930s)
               Value function loss: 91.4127
                    Surrogate loss: 0.0334
             Mean action noise std: 0.95
                       Mean reward: 457.59
               Mean episode length: 301.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5562368
                    Iteration time: 4.52s
                        Total time: 3073.16s
                               ETA: 15035.4s

################################################################################
                     [1m Learning iteration 679/4000 [0m

                       Computation: 1803 steps/s (collection: 0.612s, learning 3.930s)
               Value function loss: 136.9920
                    Surrogate loss: 0.0247
             Mean action noise std: 0.95
                       Mean reward: 439.16
               Mean episode length: 287.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 4.54s
                        Total time: 3077.70s
                               ETA: 15031.0s

################################################################################
                     [1m Learning iteration 680/4000 [0m

                       Computation: 1811 steps/s (collection: 0.555s, learning 3.967s)
               Value function loss: 147.0539
                    Surrogate loss: 0.0117
             Mean action noise std: 0.95
                       Mean reward: 433.45
               Mean episode length: 284.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5578752
                    Iteration time: 4.52s
                        Total time: 3082.22s
                               ETA: 15026.4s

################################################################################
                     [1m Learning iteration 681/4000 [0m

                       Computation: 1811 steps/s (collection: 0.577s, learning 3.946s)
               Value function loss: 163.2528
                    Surrogate loss: 0.0120
             Mean action noise std: 0.95
                       Mean reward: 436.01
               Mean episode length: 283.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 4.52s
                        Total time: 3086.75s
                               ETA: 15021.9s

################################################################################
                     [1m Learning iteration 682/4000 [0m

                       Computation: 1800 steps/s (collection: 0.597s, learning 3.954s)
               Value function loss: 293.7093
                    Surrogate loss: 0.0116
             Mean action noise std: 0.95
                       Mean reward: 474.74
               Mean episode length: 308.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5595136
                    Iteration time: 4.55s
                        Total time: 3091.30s
                               ETA: 15017.5s

################################################################################
                     [1m Learning iteration 683/4000 [0m

                       Computation: 1832 steps/s (collection: 0.549s, learning 3.921s)
               Value function loss: 178.7867
                    Surrogate loss: 0.0126
             Mean action noise std: 0.95
                       Mean reward: 464.66
               Mean episode length: 302.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 4.47s
                        Total time: 3095.77s
                               ETA: 15012.7s

################################################################################
                     [1m Learning iteration 684/4000 [0m

                       Computation: 1842 steps/s (collection: 0.517s, learning 3.929s)
               Value function loss: 183.1544
                    Surrogate loss: 0.0108
             Mean action noise std: 0.95
                       Mean reward: 474.88
               Mean episode length: 308.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5611520
                    Iteration time: 4.45s
                        Total time: 3100.21s
                               ETA: 15007.7s

################################################################################
                     [1m Learning iteration 685/4000 [0m

                       Computation: 1821 steps/s (collection: 0.566s, learning 3.933s)
               Value function loss: 189.5163
                    Surrogate loss: 0.0149
             Mean action noise std: 0.95
                       Mean reward: 503.82
               Mean episode length: 327.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 4.50s
                        Total time: 3104.71s
                               ETA: 15003.1s

################################################################################
                     [1m Learning iteration 686/4000 [0m

                       Computation: 1819 steps/s (collection: 0.553s, learning 3.950s)
               Value function loss: 123.9788
                    Surrogate loss: 0.0119
             Mean action noise std: 0.95
                       Mean reward: 503.63
               Mean episode length: 327.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5627904
                    Iteration time: 4.50s
                        Total time: 3109.21s
                               ETA: 14998.5s

################################################################################
                     [1m Learning iteration 687/4000 [0m

                       Computation: 1805 steps/s (collection: 0.602s, learning 3.936s)
               Value function loss: 200.9573
                    Surrogate loss: 0.0096
             Mean action noise std: 0.95
                       Mean reward: 515.58
               Mean episode length: 337.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 4.54s
                        Total time: 3113.75s
                               ETA: 14994.0s

################################################################################
                     [1m Learning iteration 688/4000 [0m

                       Computation: 1829 steps/s (collection: 0.554s, learning 3.924s)
               Value function loss: 157.2375
                    Surrogate loss: 0.0112
             Mean action noise std: 0.95
                       Mean reward: 501.11
               Mean episode length: 327.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5644288
                    Iteration time: 4.48s
                        Total time: 3118.23s
                               ETA: 14989.2s

################################################################################
                     [1m Learning iteration 689/4000 [0m

                       Computation: 1803 steps/s (collection: 0.551s, learning 3.990s)
               Value function loss: 295.6448
                    Surrogate loss: 0.0128
             Mean action noise std: 0.95
                       Mean reward: 510.62
               Mean episode length: 331.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 4.54s
                        Total time: 3122.77s
                               ETA: 14984.8s

################################################################################
                     [1m Learning iteration 690/4000 [0m

                       Computation: 1795 steps/s (collection: 0.614s, learning 3.948s)
               Value function loss: 311.2761
                    Surrogate loss: 0.0085
             Mean action noise std: 0.95
                       Mean reward: 536.93
               Mean episode length: 345.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5660672
                    Iteration time: 4.56s
                        Total time: 3127.33s
                               ETA: 14980.4s

################################################################################
                     [1m Learning iteration 691/4000 [0m

                       Computation: 1826 steps/s (collection: 0.569s, learning 3.916s)
               Value function loss: 172.2952
                    Surrogate loss: 0.0126
             Mean action noise std: 0.95
                       Mean reward: 539.91
               Mean episode length: 345.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 4.48s
                        Total time: 3131.82s
                               ETA: 14975.7s

################################################################################
                     [1m Learning iteration 692/4000 [0m

                       Computation: 1827 steps/s (collection: 0.550s, learning 3.933s)
               Value function loss: 176.9348
                    Surrogate loss: 0.0098
             Mean action noise std: 0.95
                       Mean reward: 549.84
               Mean episode length: 351.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5677056
                    Iteration time: 4.48s
                        Total time: 3136.30s
                               ETA: 14971.0s

################################################################################
                     [1m Learning iteration 693/4000 [0m

                       Computation: 1813 steps/s (collection: 0.581s, learning 3.935s)
               Value function loss: 207.9818
                    Surrogate loss: 0.0109
             Mean action noise std: 0.95
                       Mean reward: 554.12
               Mean episode length: 354.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 4.52s
                        Total time: 3140.82s
                               ETA: 14966.4s

################################################################################
                     [1m Learning iteration 694/4000 [0m

                       Computation: 1797 steps/s (collection: 0.571s, learning 3.987s)
               Value function loss: 138.6823
                    Surrogate loss: 0.0105
             Mean action noise std: 0.95
                       Mean reward: 559.08
               Mean episode length: 357.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 5693440
                    Iteration time: 4.56s
                        Total time: 3145.37s
                               ETA: 14962.0s

################################################################################
                     [1m Learning iteration 695/4000 [0m

                       Computation: 1798 steps/s (collection: 0.586s, learning 3.969s)
               Value function loss: 135.9633
                    Surrogate loss: 0.0102
             Mean action noise std: 0.95
                       Mean reward: 553.61
               Mean episode length: 353.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 4.55s
                        Total time: 3149.93s
                               ETA: 14957.6s

################################################################################
                     [1m Learning iteration 696/4000 [0m

                       Computation: 1782 steps/s (collection: 0.586s, learning 4.009s)
               Value function loss: 277.7984
                    Surrogate loss: 0.0091
             Mean action noise std: 0.95
                       Mean reward: 566.93
               Mean episode length: 360.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5709824
                    Iteration time: 4.60s
                        Total time: 3154.52s
                               ETA: 14953.4s

################################################################################
                     [1m Learning iteration 697/4000 [0m

                       Computation: 1769 steps/s (collection: 0.627s, learning 4.002s)
               Value function loss: 167.5713
                    Surrogate loss: 0.0103
             Mean action noise std: 0.95
                       Mean reward: 581.38
               Mean episode length: 370.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 4.63s
                        Total time: 3159.15s
                               ETA: 14949.4s

################################################################################
                     [1m Learning iteration 698/4000 [0m

                       Computation: 1825 steps/s (collection: 0.540s, learning 3.948s)
               Value function loss: 206.9402
                    Surrogate loss: 0.0092
             Mean action noise std: 0.95
                       Mean reward: 619.43
               Mean episode length: 393.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5726208
                    Iteration time: 4.49s
                        Total time: 3163.64s
                               ETA: 14944.7s

################################################################################
                     [1m Learning iteration 699/4000 [0m

                       Computation: 1820 steps/s (collection: 0.535s, learning 3.964s)
               Value function loss: 225.2907
                    Surrogate loss: 0.0083
             Mean action noise std: 0.95
                       Mean reward: 631.16
               Mean episode length: 400.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 4.50s
                        Total time: 3168.14s
                               ETA: 14940.0s

################################################################################
                     [1m Learning iteration 700/4000 [0m

                       Computation: 1800 steps/s (collection: 0.592s, learning 3.957s)
               Value function loss: 242.3459
                    Surrogate loss: 0.0136
             Mean action noise std: 0.95
                       Mean reward: 633.95
               Mean episode length: 404.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5742592
                    Iteration time: 4.55s
                        Total time: 3172.69s
                               ETA: 14935.6s

################################################################################
                     [1m Learning iteration 701/4000 [0m

                       Computation: 1823 steps/s (collection: 0.556s, learning 3.937s)
               Value function loss: 223.6034
                    Surrogate loss: 0.0125
             Mean action noise std: 0.95
                       Mean reward: 619.41
               Mean episode length: 395.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 4.49s
                        Total time: 3177.18s
                               ETA: 14930.9s

################################################################################
                     [1m Learning iteration 702/4000 [0m

                       Computation: 1784 steps/s (collection: 0.632s, learning 3.958s)
               Value function loss: 289.0959
                    Surrogate loss: 0.0123
             Mean action noise std: 0.95
                       Mean reward: 603.87
               Mean episode length: 385.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5758976
                    Iteration time: 4.59s
                        Total time: 3181.77s
                               ETA: 14926.7s

################################################################################
                     [1m Learning iteration 703/4000 [0m

                       Computation: 1813 steps/s (collection: 0.608s, learning 3.909s)
               Value function loss: 240.6874
                    Surrogate loss: 0.0101
             Mean action noise std: 0.95
                       Mean reward: 606.16
               Mean episode length: 387.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 4.52s
                        Total time: 3186.29s
                               ETA: 14922.2s

################################################################################
                     [1m Learning iteration 704/4000 [0m

                       Computation: 1805 steps/s (collection: 0.569s, learning 3.970s)
               Value function loss: 223.0327
                    Surrogate loss: 0.0105
             Mean action noise std: 0.95
                       Mean reward: 613.82
               Mean episode length: 391.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5775360
                    Iteration time: 4.54s
                        Total time: 3190.83s
                               ETA: 14917.7s

################################################################################
                     [1m Learning iteration 705/4000 [0m

                       Computation: 1825 steps/s (collection: 0.563s, learning 3.924s)
               Value function loss: 305.2925
                    Surrogate loss: 0.0109
             Mean action noise std: 0.95
                       Mean reward: 613.61
               Mean episode length: 392.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 4.49s
                        Total time: 3195.31s
                               ETA: 14913.0s

################################################################################
                     [1m Learning iteration 706/4000 [0m

                       Computation: 1808 steps/s (collection: 0.565s, learning 3.965s)
               Value function loss: 197.1953
                    Surrogate loss: 0.0188
             Mean action noise std: 0.95
                       Mean reward: 603.51
               Mean episode length: 385.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5791744
                    Iteration time: 4.53s
                        Total time: 3199.84s
                               ETA: 14908.5s

################################################################################
                     [1m Learning iteration 707/4000 [0m

                       Computation: 1838 steps/s (collection: 0.575s, learning 3.881s)
               Value function loss: 184.2892
                    Surrogate loss: 0.0115
             Mean action noise std: 0.95
                       Mean reward: 602.10
               Mean episode length: 384.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 4.46s
                        Total time: 3204.30s
                               ETA: 14903.6s

################################################################################
                     [1m Learning iteration 708/4000 [0m

                       Computation: 1819 steps/s (collection: 0.584s, learning 3.918s)
               Value function loss: 184.7153
                    Surrogate loss: 0.0133
             Mean action noise std: 0.95
                       Mean reward: 573.28
               Mean episode length: 365.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5808128
                    Iteration time: 4.50s
                        Total time: 3208.80s
                               ETA: 14899.0s

################################################################################
                     [1m Learning iteration 709/4000 [0m

                       Computation: 1818 steps/s (collection: 0.586s, learning 3.918s)
               Value function loss: 107.9949
                    Surrogate loss: 0.0110
             Mean action noise std: 0.95
                       Mean reward: 568.44
               Mean episode length: 365.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 4.50s
                        Total time: 3213.31s
                               ETA: 14894.4s

################################################################################
                     [1m Learning iteration 710/4000 [0m

                       Computation: 1805 steps/s (collection: 0.601s, learning 3.937s)
               Value function loss: 169.0435
                    Surrogate loss: 0.0107
             Mean action noise std: 0.95
                       Mean reward: 567.61
               Mean episode length: 364.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5824512
                    Iteration time: 4.54s
                        Total time: 3217.85s
                               ETA: 14889.9s

################################################################################
                     [1m Learning iteration 711/4000 [0m

                       Computation: 1807 steps/s (collection: 0.566s, learning 3.965s)
               Value function loss: 140.5160
                    Surrogate loss: 0.0183
             Mean action noise std: 0.95
                       Mean reward: 578.91
               Mean episode length: 373.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 4.53s
                        Total time: 3222.38s
                               ETA: 14885.4s

################################################################################
                     [1m Learning iteration 712/4000 [0m

                       Computation: 1849 steps/s (collection: 0.530s, learning 3.899s)
               Value function loss: 301.8089
                    Surrogate loss: 0.0121
             Mean action noise std: 0.95
                       Mean reward: 593.57
               Mean episode length: 382.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5840896
                    Iteration time: 4.43s
                        Total time: 3226.81s
                               ETA: 14880.4s

################################################################################
                     [1m Learning iteration 713/4000 [0m

                       Computation: 1820 steps/s (collection: 0.582s, learning 3.919s)
               Value function loss: 272.5779
                    Surrogate loss: 0.0099
             Mean action noise std: 0.95
                       Mean reward: 601.81
               Mean episode length: 388.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 4.50s
                        Total time: 3231.31s
                               ETA: 14875.8s

################################################################################
                     [1m Learning iteration 714/4000 [0m

                       Computation: 1819 steps/s (collection: 0.616s, learning 3.887s)
               Value function loss: 288.3191
                    Surrogate loss: 0.0123
             Mean action noise std: 0.95
                       Mean reward: 600.51
               Mean episode length: 389.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5857280
                    Iteration time: 4.50s
                        Total time: 3235.81s
                               ETA: 14871.1s

################################################################################
                     [1m Learning iteration 715/4000 [0m

                       Computation: 1795 steps/s (collection: 0.618s, learning 3.945s)
               Value function loss: 205.5403
                    Surrogate loss: 0.0097
             Mean action noise std: 0.95
                       Mean reward: 586.58
               Mean episode length: 379.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 4.56s
                        Total time: 3240.37s
                               ETA: 14866.8s

################################################################################
                     [1m Learning iteration 716/4000 [0m

                       Computation: 1837 steps/s (collection: 0.543s, learning 3.914s)
               Value function loss: 184.9831
                    Surrogate loss: 0.0138
             Mean action noise std: 0.95
                       Mean reward: 591.52
               Mean episode length: 383.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5873664
                    Iteration time: 4.46s
                        Total time: 3244.83s
                               ETA: 14862.0s

################################################################################
                     [1m Learning iteration 717/4000 [0m

                       Computation: 1814 steps/s (collection: 0.577s, learning 3.939s)
               Value function loss: 240.0760
                    Surrogate loss: 0.0142
             Mean action noise std: 0.95
                       Mean reward: 589.92
               Mean episode length: 384.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 4.52s
                        Total time: 3249.35s
                               ETA: 14857.4s

################################################################################
                     [1m Learning iteration 718/4000 [0m

                       Computation: 1806 steps/s (collection: 0.602s, learning 3.933s)
               Value function loss: 231.3397
                    Surrogate loss: 0.0089
             Mean action noise std: 0.95
                       Mean reward: 612.60
               Mean episode length: 400.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5890048
                    Iteration time: 4.53s
                        Total time: 3253.88s
                               ETA: 14852.9s

################################################################################
                     [1m Learning iteration 719/4000 [0m

                       Computation: 1795 steps/s (collection: 0.596s, learning 3.967s)
               Value function loss: 355.6735
                    Surrogate loss: 0.0076
             Mean action noise std: 0.95
                       Mean reward: 611.86
               Mean episode length: 399.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 4.56s
                        Total time: 3258.44s
                               ETA: 14848.5s

################################################################################
                     [1m Learning iteration 720/4000 [0m

                       Computation: 1819 steps/s (collection: 0.585s, learning 3.918s)
               Value function loss: 181.2488
                    Surrogate loss: 0.0122
             Mean action noise std: 0.95
                       Mean reward: 607.42
               Mean episode length: 395.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5906432
                    Iteration time: 4.50s
                        Total time: 3262.95s
                               ETA: 14843.9s

################################################################################
                     [1m Learning iteration 721/4000 [0m

                       Computation: 1801 steps/s (collection: 0.558s, learning 3.990s)
               Value function loss: 269.4975
                    Surrogate loss: 0.0100
             Mean action noise std: 0.95
                       Mean reward: 609.42
               Mean episode length: 397.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 4.55s
                        Total time: 3267.49s
                               ETA: 14839.5s

################################################################################
                     [1m Learning iteration 722/4000 [0m

                       Computation: 1787 steps/s (collection: 0.581s, learning 4.002s)
               Value function loss: 177.7212
                    Surrogate loss: 0.0082
             Mean action noise std: 0.95
                       Mean reward: 592.87
               Mean episode length: 386.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5922816
                    Iteration time: 4.58s
                        Total time: 3272.08s
                               ETA: 14835.2s

################################################################################
                     [1m Learning iteration 723/4000 [0m

                       Computation: 1783 steps/s (collection: 0.622s, learning 3.970s)
               Value function loss: 165.5475
                    Surrogate loss: 0.0100
             Mean action noise std: 0.95
                       Mean reward: 581.68
               Mean episode length: 378.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 4.59s
                        Total time: 3276.67s
                               ETA: 14831.0s

################################################################################
                     [1m Learning iteration 724/4000 [0m

                       Computation: 1792 steps/s (collection: 0.599s, learning 3.970s)
               Value function loss: 137.4161
                    Surrogate loss: 0.0109
             Mean action noise std: 0.95
                       Mean reward: 592.37
               Mean episode length: 385.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5939200
                    Iteration time: 4.57s
                        Total time: 3281.24s
                               ETA: 14826.7s

################################################################################
                     [1m Learning iteration 725/4000 [0m

                       Computation: 1760 steps/s (collection: 0.612s, learning 4.041s)
               Value function loss: 137.5651
                    Surrogate loss: 0.0112
             Mean action noise std: 0.95
                       Mean reward: 574.25
               Mean episode length: 374.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 4.65s
                        Total time: 3285.89s
                               ETA: 14822.7s

################################################################################
                     [1m Learning iteration 726/4000 [0m

                       Computation: 1799 steps/s (collection: 0.559s, learning 3.993s)
               Value function loss: 176.0772
                    Surrogate loss: 0.0108
             Mean action noise std: 0.95
                       Mean reward: 576.54
               Mean episode length: 376.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5955584
                    Iteration time: 4.55s
                        Total time: 3290.45s
                               ETA: 14818.3s

################################################################################
                     [1m Learning iteration 727/4000 [0m

                       Computation: 1836 steps/s (collection: 0.550s, learning 3.911s)
               Value function loss: 202.8354
                    Surrogate loss: 0.0096
             Mean action noise std: 0.95
                       Mean reward: 576.41
               Mean episode length: 374.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 4.46s
                        Total time: 3294.91s
                               ETA: 14813.5s

################################################################################
                     [1m Learning iteration 728/4000 [0m

                       Computation: 1839 steps/s (collection: 0.525s, learning 3.928s)
               Value function loss: 183.1822
                    Surrogate loss: 0.0109
             Mean action noise std: 0.95
                       Mean reward: 562.78
               Mean episode length: 363.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5971968
                    Iteration time: 4.45s
                        Total time: 3299.36s
                               ETA: 14808.6s

################################################################################
                     [1m Learning iteration 729/4000 [0m

                       Computation: 1801 steps/s (collection: 0.620s, learning 3.928s)
               Value function loss: 278.0318
                    Surrogate loss: 0.0115
             Mean action noise std: 0.95
                       Mean reward: 565.85
               Mean episode length: 366.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 4.55s
                        Total time: 3303.91s
                               ETA: 14804.2s

################################################################################
                     [1m Learning iteration 730/4000 [0m

                       Computation: 1836 steps/s (collection: 0.562s, learning 3.897s)
               Value function loss: 254.3601
                    Surrogate loss: 0.0109
             Mean action noise std: 0.95
                       Mean reward: 559.13
               Mean episode length: 360.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5988352
                    Iteration time: 4.46s
                        Total time: 3308.37s
                               ETA: 14799.4s

################################################################################
                     [1m Learning iteration 731/4000 [0m

                       Computation: 1797 steps/s (collection: 0.577s, learning 3.980s)
               Value function loss: 222.3914
                    Surrogate loss: 0.0123
             Mean action noise std: 0.95
                       Mean reward: 567.21
               Mean episode length: 362.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 4.56s
                        Total time: 3312.92s
                               ETA: 14795.0s

################################################################################
                     [1m Learning iteration 732/4000 [0m

                       Computation: 1821 steps/s (collection: 0.568s, learning 3.930s)
               Value function loss: 188.5392
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 591.36
               Mean episode length: 377.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6004736
                    Iteration time: 4.50s
                        Total time: 3317.42s
                               ETA: 14790.4s

################################################################################
                     [1m Learning iteration 733/4000 [0m

                       Computation: 1815 steps/s (collection: 0.546s, learning 3.967s)
               Value function loss: 198.8327
                    Surrogate loss: 0.0115
             Mean action noise std: 0.95
                       Mean reward: 570.37
               Mean episode length: 363.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 4.51s
                        Total time: 3321.93s
                               ETA: 14785.8s

################################################################################
                     [1m Learning iteration 734/4000 [0m

                       Computation: 1843 steps/s (collection: 0.553s, learning 3.890s)
               Value function loss: 225.7858
                    Surrogate loss: 0.0111
             Mean action noise std: 0.95
                       Mean reward: 591.61
               Mean episode length: 373.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6021120
                    Iteration time: 4.44s
                        Total time: 3326.38s
                               ETA: 14780.9s

################################################################################
                     [1m Learning iteration 735/4000 [0m

                       Computation: 1831 steps/s (collection: 0.601s, learning 3.873s)
               Value function loss: 236.1625
                    Surrogate loss: 0.0097
             Mean action noise std: 0.95
                       Mean reward: 588.47
               Mean episode length: 372.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 4.47s
                        Total time: 3330.85s
                               ETA: 14776.1s

################################################################################
                     [1m Learning iteration 736/4000 [0m

                       Computation: 1844 steps/s (collection: 0.543s, learning 3.900s)
               Value function loss: 155.0263
                    Surrogate loss: 0.0123
             Mean action noise std: 0.95
                       Mean reward: 598.77
               Mean episode length: 378.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6037504
                    Iteration time: 4.44s
                        Total time: 3335.29s
                               ETA: 14771.2s

################################################################################
                     [1m Learning iteration 737/4000 [0m

                       Computation: 1829 steps/s (collection: 0.551s, learning 3.925s)
               Value function loss: 240.1220
                    Surrogate loss: 0.0118
             Mean action noise std: 0.95
                       Mean reward: 595.42
               Mean episode length: 374.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 4.48s
                        Total time: 3339.77s
                               ETA: 14766.5s

################################################################################
                     [1m Learning iteration 738/4000 [0m

                       Computation: 1822 steps/s (collection: 0.541s, learning 3.955s)
               Value function loss: 192.8758
                    Surrogate loss: 0.0098
             Mean action noise std: 0.95
                       Mean reward: 590.86
               Mean episode length: 371.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6053888
                    Iteration time: 4.50s
                        Total time: 3344.27s
                               ETA: 14761.8s

################################################################################
                     [1m Learning iteration 739/4000 [0m

                       Computation: 1801 steps/s (collection: 0.595s, learning 3.951s)
               Value function loss: 138.6585
                    Surrogate loss: 0.0164
             Mean action noise std: 0.95
                       Mean reward: 593.44
               Mean episode length: 373.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 4.55s
                        Total time: 3348.81s
                               ETA: 14757.4s

################################################################################
                     [1m Learning iteration 740/4000 [0m

                       Computation: 1840 steps/s (collection: 0.544s, learning 3.907s)
               Value function loss: 202.2076
                    Surrogate loss: 0.0120
             Mean action noise std: 0.95
                       Mean reward: 598.01
               Mean episode length: 377.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6070272
                    Iteration time: 4.45s
                        Total time: 3353.26s
                               ETA: 14752.5s

################################################################################
                     [1m Learning iteration 741/4000 [0m

                       Computation: 1835 steps/s (collection: 0.544s, learning 3.918s)
               Value function loss: 185.3021
                    Surrogate loss: 0.0158
             Mean action noise std: 0.95
                       Mean reward: 577.28
               Mean episode length: 365.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 4.46s
                        Total time: 3357.73s
                               ETA: 14747.7s

################################################################################
                     [1m Learning iteration 742/4000 [0m

                       Computation: 1832 steps/s (collection: 0.575s, learning 3.896s)
               Value function loss: 175.9481
                    Surrogate loss: 0.0092
             Mean action noise std: 0.95
                       Mean reward: 595.69
               Mean episode length: 374.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6086656
                    Iteration time: 4.47s
                        Total time: 3362.20s
                               ETA: 14743.0s

################################################################################
                     [1m Learning iteration 743/4000 [0m

                       Computation: 1821 steps/s (collection: 0.588s, learning 3.909s)
               Value function loss: 237.2110
                    Surrogate loss: 0.0098
             Mean action noise std: 0.95
                       Mean reward: 600.67
               Mean episode length: 378.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 4.50s
                        Total time: 3366.69s
                               ETA: 14738.3s

################################################################################
                     [1m Learning iteration 744/4000 [0m

                       Computation: 1843 steps/s (collection: 0.523s, learning 3.921s)
               Value function loss: 170.1976
                    Surrogate loss: 0.0123
             Mean action noise std: 0.95
                       Mean reward: 585.83
               Mean episode length: 370.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6103040
                    Iteration time: 4.44s
                        Total time: 3371.14s
                               ETA: 14733.5s

################################################################################
                     [1m Learning iteration 745/4000 [0m

                       Computation: 1824 steps/s (collection: 0.591s, learning 3.900s)
               Value function loss: 300.8737
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 581.69
               Mean episode length: 368.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 4.49s
                        Total time: 3375.63s
                               ETA: 14728.8s

################################################################################
                     [1m Learning iteration 746/4000 [0m

                       Computation: 1842 steps/s (collection: 0.513s, learning 3.932s)
               Value function loss: 179.3317
                    Surrogate loss: 0.0084
             Mean action noise std: 0.95
                       Mean reward: 582.81
               Mean episode length: 371.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6119424
                    Iteration time: 4.45s
                        Total time: 3380.07s
                               ETA: 14723.9s

################################################################################
                     [1m Learning iteration 747/4000 [0m

                       Computation: 1816 steps/s (collection: 0.610s, learning 3.901s)
               Value function loss: 239.3964
                    Surrogate loss: 0.0083
             Mean action noise std: 0.95
                       Mean reward: 577.44
               Mean episode length: 368.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 4.51s
                        Total time: 3384.58s
                               ETA: 14719.3s

################################################################################
                     [1m Learning iteration 748/4000 [0m

                       Computation: 1837 steps/s (collection: 0.544s, learning 3.915s)
               Value function loss: 225.5793
                    Surrogate loss: 0.0097
             Mean action noise std: 0.95
                       Mean reward: 582.13
               Mean episode length: 370.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6135808
                    Iteration time: 4.46s
                        Total time: 3389.04s
                               ETA: 14714.5s

################################################################################
                     [1m Learning iteration 749/4000 [0m

                       Computation: 1798 steps/s (collection: 0.580s, learning 3.975s)
               Value function loss: 184.6817
                    Surrogate loss: 0.0088
             Mean action noise std: 0.95
                       Mean reward: 583.49
               Mean episode length: 372.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 4.55s
                        Total time: 3393.60s
                               ETA: 14710.1s

################################################################################
                     [1m Learning iteration 750/4000 [0m

                       Computation: 1843 steps/s (collection: 0.558s, learning 3.885s)
               Value function loss: 265.0543
                    Surrogate loss: 0.0087
             Mean action noise std: 0.95
                       Mean reward: 604.36
               Mean episode length: 386.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6152192
                    Iteration time: 4.44s
                        Total time: 3398.04s
                               ETA: 14705.2s

################################################################################
                     [1m Learning iteration 751/4000 [0m

                       Computation: 1786 steps/s (collection: 0.555s, learning 4.031s)
               Value function loss: 146.2677
                    Surrogate loss: 0.0099
             Mean action noise std: 0.95
                       Mean reward: 608.27
               Mean episode length: 390.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 4.59s
                        Total time: 3402.63s
                               ETA: 14701.0s

################################################################################
                     [1m Learning iteration 752/4000 [0m

                       Computation: 1804 steps/s (collection: 0.539s, learning 4.002s)
               Value function loss: 294.5568
                    Surrogate loss: 0.0101
             Mean action noise std: 0.95
                       Mean reward: 610.51
               Mean episode length: 393.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6168576
                    Iteration time: 4.54s
                        Total time: 3407.17s
                               ETA: 14696.5s

################################################################################
                     [1m Learning iteration 753/4000 [0m

                       Computation: 1798 steps/s (collection: 0.546s, learning 4.009s)
               Value function loss: 137.5269
                    Surrogate loss: 0.0082
             Mean action noise std: 0.95
                       Mean reward: 613.09
               Mean episode length: 395.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 4.55s
                        Total time: 3411.72s
                               ETA: 14692.1s

################################################################################
                     [1m Learning iteration 754/4000 [0m

                       Computation: 1801 steps/s (collection: 0.571s, learning 3.977s)
               Value function loss: 167.6340
                    Surrogate loss: 0.0106
             Mean action noise std: 0.95
                       Mean reward: 603.02
               Mean episode length: 392.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6184960
                    Iteration time: 4.55s
                        Total time: 3416.27s
                               ETA: 14687.7s

################################################################################
                     [1m Learning iteration 755/4000 [0m

                       Computation: 1803 steps/s (collection: 0.615s, learning 3.927s)
               Value function loss: 168.2079
                    Surrogate loss: 0.0097
             Mean action noise std: 0.95
                       Mean reward: 604.40
               Mean episode length: 393.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 4.54s
                        Total time: 3420.81s
                               ETA: 14683.2s

################################################################################
                     [1m Learning iteration 756/4000 [0m

                       Computation: 1813 steps/s (collection: 0.600s, learning 3.918s)
               Value function loss: 190.1565
                    Surrogate loss: 0.0083
             Mean action noise std: 0.95
                       Mean reward: 595.52
               Mean episode length: 387.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6201344
                    Iteration time: 4.52s
                        Total time: 3425.33s
                               ETA: 14678.7s

################################################################################
                     [1m Learning iteration 757/4000 [0m

                       Computation: 1781 steps/s (collection: 0.657s, learning 3.941s)
               Value function loss: 224.5888
                    Surrogate loss: 0.0103
             Mean action noise std: 0.95
                       Mean reward: 612.09
               Mean episode length: 398.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 4.60s
                        Total time: 3429.93s
                               ETA: 14674.5s

################################################################################
                     [1m Learning iteration 758/4000 [0m

                       Computation: 1829 steps/s (collection: 0.588s, learning 3.888s)
               Value function loss: 160.4323
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 614.43
               Mean episode length: 399.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6217728
                    Iteration time: 4.48s
                        Total time: 3434.41s
                               ETA: 14669.8s

################################################################################
                     [1m Learning iteration 759/4000 [0m

                       Computation: 1820 steps/s (collection: 0.559s, learning 3.942s)
               Value function loss: 245.8183
                    Surrogate loss: 0.0076
             Mean action noise std: 0.95
                       Mean reward: 618.09
               Mean episode length: 400.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 4.50s
                        Total time: 3438.91s
                               ETA: 14665.1s

################################################################################
                     [1m Learning iteration 760/4000 [0m

                       Computation: 1805 steps/s (collection: 0.605s, learning 3.932s)
               Value function loss: 180.9408
                    Surrogate loss: 0.0111
             Mean action noise std: 0.95
                       Mean reward: 602.05
               Mean episode length: 391.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6234112
                    Iteration time: 4.54s
                        Total time: 3443.44s
                               ETA: 14660.7s

################################################################################
                     [1m Learning iteration 761/4000 [0m

                       Computation: 1814 steps/s (collection: 0.566s, learning 3.949s)
               Value function loss: 300.3821
                    Surrogate loss: 0.0099
             Mean action noise std: 0.95
                       Mean reward: 602.79
               Mean episode length: 390.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 4.51s
                        Total time: 3447.96s
                               ETA: 14656.1s

################################################################################
                     [1m Learning iteration 762/4000 [0m

                       Computation: 1820 steps/s (collection: 0.544s, learning 3.956s)
               Value function loss: 269.5146
                    Surrogate loss: 0.0106
             Mean action noise std: 0.95
                       Mean reward: 599.00
               Mean episode length: 387.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6250496
                    Iteration time: 4.50s
                        Total time: 3452.46s
                               ETA: 14651.5s

################################################################################
                     [1m Learning iteration 763/4000 [0m

                       Computation: 1806 steps/s (collection: 0.611s, learning 3.923s)
               Value function loss: 292.5204
                    Surrogate loss: 0.0119
             Mean action noise std: 0.95
                       Mean reward: 602.76
               Mean episode length: 388.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 4.53s
                        Total time: 3456.99s
                               ETA: 14647.0s

################################################################################
                     [1m Learning iteration 764/4000 [0m

                       Computation: 1836 steps/s (collection: 0.570s, learning 3.891s)
               Value function loss: 259.4810
                    Surrogate loss: 0.0094
             Mean action noise std: 0.95
                       Mean reward: 600.38
               Mean episode length: 386.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6266880
                    Iteration time: 4.46s
                        Total time: 3461.45s
                               ETA: 14642.2s

################################################################################
                     [1m Learning iteration 765/4000 [0m

                       Computation: 1839 steps/s (collection: 0.543s, learning 3.911s)
               Value function loss: 172.0164
                    Surrogate loss: 0.0115
             Mean action noise std: 0.95
                       Mean reward: 604.54
               Mean episode length: 389.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 4.45s
                        Total time: 3465.91s
                               ETA: 14637.4s

################################################################################
                     [1m Learning iteration 766/4000 [0m

                       Computation: 1820 steps/s (collection: 0.572s, learning 3.927s)
               Value function loss: 268.8435
                    Surrogate loss: 0.0128
             Mean action noise std: 0.95
                       Mean reward: 591.77
               Mean episode length: 382.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6283264
                    Iteration time: 4.50s
                        Total time: 3470.41s
                               ETA: 14632.7s

################################################################################
                     [1m Learning iteration 767/4000 [0m

                       Computation: 1819 steps/s (collection: 0.562s, learning 3.941s)
               Value function loss: 192.6870
                    Surrogate loss: 0.0105
             Mean action noise std: 0.95
                       Mean reward: 588.25
               Mean episode length: 381.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 4.50s
                        Total time: 3474.91s
                               ETA: 14628.1s

################################################################################
                     [1m Learning iteration 768/4000 [0m

                       Computation: 1796 steps/s (collection: 0.603s, learning 3.958s)
               Value function loss: 274.9012
                    Surrogate loss: 0.0109
             Mean action noise std: 0.95
                       Mean reward: 601.55
               Mean episode length: 388.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6299648
                    Iteration time: 4.56s
                        Total time: 3479.47s
                               ETA: 14623.7s

################################################################################
                     [1m Learning iteration 769/4000 [0m

                       Computation: 1834 steps/s (collection: 0.545s, learning 3.922s)
               Value function loss: 278.5261
                    Surrogate loss: 0.0114
             Mean action noise std: 0.95
                       Mean reward: 577.19
               Mean episode length: 373.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 4.47s
                        Total time: 3483.94s
                               ETA: 14619.0s

################################################################################
                     [1m Learning iteration 770/4000 [0m

                       Computation: 1802 steps/s (collection: 0.627s, learning 3.917s)
               Value function loss: 190.0661
                    Surrogate loss: 0.0091
             Mean action noise std: 0.95
                       Mean reward: 567.74
               Mean episode length: 367.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6316032
                    Iteration time: 4.54s
                        Total time: 3488.48s
                               ETA: 14614.5s

################################################################################
                     [1m Learning iteration 771/4000 [0m

                       Computation: 1842 steps/s (collection: 0.535s, learning 3.911s)
               Value function loss: 181.6876
                    Surrogate loss: 0.0114
             Mean action noise std: 0.95
                       Mean reward: 558.15
               Mean episode length: 361.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 4.45s
                        Total time: 3492.93s
                               ETA: 14609.7s

################################################################################
                     [1m Learning iteration 772/4000 [0m

                       Computation: 1810 steps/s (collection: 0.579s, learning 3.945s)
               Value function loss: 189.1099
                    Surrogate loss: 0.0116
             Mean action noise std: 0.95
                       Mean reward: 539.63
               Mean episode length: 348.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6332416
                    Iteration time: 4.52s
                        Total time: 3497.45s
                               ETA: 14605.1s

################################################################################
                     [1m Learning iteration 773/4000 [0m

                       Computation: 1830 steps/s (collection: 0.590s, learning 3.885s)
               Value function loss: 207.7428
                    Surrogate loss: 0.0123
             Mean action noise std: 0.95
                       Mean reward: 537.70
               Mean episode length: 347.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 4.48s
                        Total time: 3501.93s
                               ETA: 14600.4s

################################################################################
                     [1m Learning iteration 774/4000 [0m

                       Computation: 1784 steps/s (collection: 0.611s, learning 3.980s)
               Value function loss: 165.4260
                    Surrogate loss: 0.0110
             Mean action noise std: 0.95
                       Mean reward: 531.44
               Mean episode length: 341.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6348800
                    Iteration time: 4.59s
                        Total time: 3506.52s
                               ETA: 14596.2s

################################################################################
                     [1m Learning iteration 775/4000 [0m

                       Computation: 1810 steps/s (collection: 0.589s, learning 3.937s)
               Value function loss: 181.2929
                    Surrogate loss: 0.0120
             Mean action noise std: 0.95
                       Mean reward: 551.26
               Mean episode length: 353.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 4.53s
                        Total time: 3511.04s
                               ETA: 14591.6s

################################################################################
                     [1m Learning iteration 776/4000 [0m

                       Computation: 1822 steps/s (collection: 0.560s, learning 3.934s)
               Value function loss: 258.2681
                    Surrogate loss: 0.0118
             Mean action noise std: 0.95
                       Mean reward: 566.55
               Mean episode length: 362.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6365184
                    Iteration time: 4.49s
                        Total time: 3515.54s
                               ETA: 14587.0s

################################################################################
                     [1m Learning iteration 777/4000 [0m

                       Computation: 1835 steps/s (collection: 0.534s, learning 3.929s)
               Value function loss: 171.7836
                    Surrogate loss: 0.0109
             Mean action noise std: 0.95
                       Mean reward: 571.37
               Mean episode length: 363.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 4.46s
                        Total time: 3520.00s
                               ETA: 14582.2s

################################################################################
                     [1m Learning iteration 778/4000 [0m

                       Computation: 1782 steps/s (collection: 0.581s, learning 4.014s)
               Value function loss: 238.3568
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 571.86
               Mean episode length: 362.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6381568
                    Iteration time: 4.60s
                        Total time: 3524.59s
                               ETA: 14578.0s

################################################################################
                     [1m Learning iteration 779/4000 [0m

                       Computation: 1826 steps/s (collection: 0.587s, learning 3.898s)
               Value function loss: 297.7569
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 602.87
               Mean episode length: 381.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 4.48s
                        Total time: 3529.08s
                               ETA: 14573.3s

################################################################################
                     [1m Learning iteration 780/4000 [0m

                       Computation: 1834 steps/s (collection: 0.565s, learning 3.900s)
               Value function loss: 198.8792
                    Surrogate loss: 0.0132
             Mean action noise std: 0.95
                       Mean reward: 608.18
               Mean episode length: 386.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6397952
                    Iteration time: 4.47s
                        Total time: 3533.54s
                               ETA: 14568.5s

################################################################################
                     [1m Learning iteration 781/4000 [0m

                       Computation: 1813 steps/s (collection: 0.568s, learning 3.950s)
               Value function loss: 167.8678
                    Surrogate loss: 0.0134
             Mean action noise std: 0.95
                       Mean reward: 620.44
               Mean episode length: 393.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 4.52s
                        Total time: 3538.06s
                               ETA: 14564.0s

################################################################################
                     [1m Learning iteration 782/4000 [0m

                       Computation: 1793 steps/s (collection: 0.620s, learning 3.948s)
               Value function loss: 200.8879
                    Surrogate loss: 0.0102
             Mean action noise std: 0.95
                       Mean reward: 636.40
               Mean episode length: 403.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6414336
                    Iteration time: 4.57s
                        Total time: 3542.63s
                               ETA: 14559.6s

################################################################################
                     [1m Learning iteration 783/4000 [0m

                       Computation: 1814 steps/s (collection: 0.605s, learning 3.909s)
               Value function loss: 144.5691
                    Surrogate loss: 0.0162
             Mean action noise std: 0.95
                       Mean reward: 651.79
               Mean episode length: 412.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 4.51s
                        Total time: 3547.15s
                               ETA: 14555.1s

################################################################################
                     [1m Learning iteration 784/4000 [0m

                       Computation: 1799 steps/s (collection: 0.613s, learning 3.939s)
               Value function loss: 188.0217
                    Surrogate loss: 0.0124
             Mean action noise std: 0.95
                       Mean reward: 662.60
               Mean episode length: 420.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6430720
                    Iteration time: 4.55s
                        Total time: 3551.70s
                               ETA: 14550.6s

################################################################################
                     [1m Learning iteration 785/4000 [0m

                       Computation: 1843 steps/s (collection: 0.562s, learning 3.881s)
               Value function loss: 275.6305
                    Surrogate loss: 0.0106
             Mean action noise std: 0.95
                       Mean reward: 628.11
               Mean episode length: 399.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 4.44s
                        Total time: 3556.14s
                               ETA: 14545.8s

################################################################################
                     [1m Learning iteration 786/4000 [0m

                       Computation: 1772 steps/s (collection: 0.658s, learning 3.962s)
               Value function loss: 178.1600
                    Surrogate loss: 0.0153
             Mean action noise std: 0.95
                       Mean reward: 614.64
               Mean episode length: 390.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6447104
                    Iteration time: 4.62s
                        Total time: 3560.76s
                               ETA: 14541.7s

################################################################################
                     [1m Learning iteration 787/4000 [0m

                       Computation: 1830 steps/s (collection: 0.572s, learning 3.904s)
               Value function loss: 166.0393
                    Surrogate loss: 0.0207
             Mean action noise std: 0.95
                       Mean reward: 601.77
               Mean episode length: 381.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 4.48s
                        Total time: 3565.24s
                               ETA: 14536.9s

################################################################################
                     [1m Learning iteration 788/4000 [0m

                       Computation: 1829 steps/s (collection: 0.545s, learning 3.933s)
               Value function loss: 248.3047
                    Surrogate loss: 0.0089
             Mean action noise std: 0.95
                       Mean reward: 587.29
               Mean episode length: 371.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6463488
                    Iteration time: 4.48s
                        Total time: 3569.71s
                               ETA: 14532.2s

################################################################################
                     [1m Learning iteration 789/4000 [0m

                       Computation: 1828 steps/s (collection: 0.570s, learning 3.909s)
               Value function loss: 161.7184
                    Surrogate loss: 0.0121
             Mean action noise std: 0.95
                       Mean reward: 589.58
               Mean episode length: 370.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 4.48s
                        Total time: 3574.19s
                               ETA: 14527.5s

################################################################################
                     [1m Learning iteration 790/4000 [0m

                       Computation: 1839 steps/s (collection: 0.535s, learning 3.918s)
               Value function loss: 278.8114
                    Surrogate loss: 0.0100
             Mean action noise std: 0.95
                       Mean reward: 588.28
               Mean episode length: 368.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6479872
                    Iteration time: 4.45s
                        Total time: 3578.65s
                               ETA: 14522.7s

################################################################################
                     [1m Learning iteration 791/4000 [0m

                       Computation: 1842 steps/s (collection: 0.557s, learning 3.890s)
               Value function loss: 143.5754
                    Surrogate loss: 0.0146
             Mean action noise std: 0.95
                       Mean reward: 572.15
               Mean episode length: 359.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 4.45s
                        Total time: 3583.09s
                               ETA: 14517.9s

################################################################################
                     [1m Learning iteration 792/4000 [0m

                       Computation: 1808 steps/s (collection: 0.580s, learning 3.949s)
               Value function loss: 215.0809
                    Surrogate loss: 0.0094
             Mean action noise std: 0.95
                       Mean reward: 565.89
               Mean episode length: 356.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6496256
                    Iteration time: 4.53s
                        Total time: 3587.62s
                               ETA: 14513.4s

################################################################################
                     [1m Learning iteration 793/4000 [0m

                       Computation: 1799 steps/s (collection: 0.620s, learning 3.933s)
               Value function loss: 182.2020
                    Surrogate loss: 0.0103
             Mean action noise std: 0.95
                       Mean reward: 569.22
               Mean episode length: 356.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 4.55s
                        Total time: 3592.18s
                               ETA: 14509.0s

################################################################################
                     [1m Learning iteration 794/4000 [0m

                       Computation: 1845 steps/s (collection: 0.545s, learning 3.894s)
               Value function loss: 271.1048
                    Surrogate loss: 0.0121
             Mean action noise std: 0.95
                       Mean reward: 584.55
               Mean episode length: 369.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6512640
                    Iteration time: 4.44s
                        Total time: 3596.62s
                               ETA: 14504.1s

################################################################################
                     [1m Learning iteration 795/4000 [0m

                       Computation: 1813 steps/s (collection: 0.604s, learning 3.914s)
               Value function loss: 233.5405
                    Surrogate loss: 0.0108
             Mean action noise std: 0.95
                       Mean reward: 590.48
               Mean episode length: 373.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 4.52s
                        Total time: 3601.13s
                               ETA: 14499.5s

################################################################################
                     [1m Learning iteration 796/4000 [0m

                       Computation: 1856 steps/s (collection: 0.513s, learning 3.900s)
               Value function loss: 286.3227
                    Surrogate loss: 0.0106
             Mean action noise std: 0.95
                       Mean reward: 607.92
               Mean episode length: 386.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6529024
                    Iteration time: 4.41s
                        Total time: 3605.55s
                               ETA: 14494.6s

################################################################################
                     [1m Learning iteration 797/4000 [0m

                       Computation: 1812 steps/s (collection: 0.588s, learning 3.931s)
               Value function loss: 167.7268
                    Surrogate loss: 0.0107
             Mean action noise std: 0.95
                       Mean reward: 611.70
               Mean episode length: 390.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 4.52s
                        Total time: 3610.07s
                               ETA: 14490.0s

################################################################################
                     [1m Learning iteration 798/4000 [0m

                       Computation: 1808 steps/s (collection: 0.603s, learning 3.927s)
               Value function loss: 178.6848
                    Surrogate loss: 0.0136
             Mean action noise std: 0.95
                       Mean reward: 599.60
               Mean episode length: 386.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6545408
                    Iteration time: 4.53s
                        Total time: 3614.60s
                               ETA: 14485.5s

################################################################################
                     [1m Learning iteration 799/4000 [0m

                       Computation: 1801 steps/s (collection: 0.601s, learning 3.945s)
               Value function loss: 218.2224
                    Surrogate loss: 0.0116
             Mean action noise std: 0.95
                       Mean reward: 593.55
               Mean episode length: 384.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 4.55s
                        Total time: 3619.14s
                               ETA: 14481.1s

################################################################################
                     [1m Learning iteration 800/4000 [0m

                       Computation: 1815 steps/s (collection: 0.565s, learning 3.947s)
               Value function loss: 253.4375
                    Surrogate loss: 0.0106
             Mean action noise std: 0.95
                       Mean reward: 600.32
               Mean episode length: 390.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6561792
                    Iteration time: 4.51s
                        Total time: 3623.65s
                               ETA: 14476.5s

################################################################################
                     [1m Learning iteration 801/4000 [0m

                       Computation: 1799 steps/s (collection: 0.597s, learning 3.954s)
               Value function loss: 266.9367
                    Surrogate loss: 0.0138
             Mean action noise std: 0.95
                       Mean reward: 608.58
               Mean episode length: 399.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 4.55s
                        Total time: 3628.21s
                               ETA: 14472.1s

################################################################################
                     [1m Learning iteration 802/4000 [0m

                       Computation: 1773 steps/s (collection: 0.586s, learning 4.034s)
               Value function loss: 207.4159
                    Surrogate loss: 0.0118
             Mean action noise std: 0.95
                       Mean reward: 596.91
               Mean episode length: 393.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6578176
                    Iteration time: 4.62s
                        Total time: 3632.83s
                               ETA: 14468.0s

################################################################################
                     [1m Learning iteration 803/4000 [0m

                       Computation: 1832 steps/s (collection: 0.577s, learning 3.893s)
               Value function loss: 220.2679
                    Surrogate loss: 0.0111
             Mean action noise std: 0.95
                       Mean reward: 604.84
               Mean episode length: 397.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 4.47s
                        Total time: 3637.30s
                               ETA: 14463.2s

################################################################################
                     [1m Learning iteration 804/4000 [0m

                       Computation: 1823 steps/s (collection: 0.519s, learning 3.974s)
               Value function loss: 172.4888
                    Surrogate loss: 0.0126
             Mean action noise std: 0.95
                       Mean reward: 584.73
               Mean episode length: 385.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6594560
                    Iteration time: 4.49s
                        Total time: 3641.79s
                               ETA: 14458.6s

################################################################################
                     [1m Learning iteration 805/4000 [0m

                       Computation: 1798 steps/s (collection: 0.609s, learning 3.947s)
               Value function loss: 185.4499
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 589.39
               Mean episode length: 390.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 4.56s
                        Total time: 3646.34s
                               ETA: 14454.2s

################################################################################
                     [1m Learning iteration 806/4000 [0m

                       Computation: 1790 steps/s (collection: 0.586s, learning 3.989s)
               Value function loss: 200.6257
                    Surrogate loss: 0.0101
             Mean action noise std: 0.95
                       Mean reward: 582.98
               Mean episode length: 387.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6610944
                    Iteration time: 4.58s
                        Total time: 3650.92s
                               ETA: 14449.9s

################################################################################
                     [1m Learning iteration 807/4000 [0m

                       Computation: 1804 steps/s (collection: 0.604s, learning 3.936s)
               Value function loss: 206.4062
                    Surrogate loss: 0.0098
             Mean action noise std: 0.95
                       Mean reward: 568.28
               Mean episode length: 377.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 4.54s
                        Total time: 3655.46s
                               ETA: 14445.4s

################################################################################
                     [1m Learning iteration 808/4000 [0m

                       Computation: 1827 steps/s (collection: 0.545s, learning 3.938s)
               Value function loss: 224.2190
                    Surrogate loss: 0.0099
             Mean action noise std: 0.95
                       Mean reward: 576.77
               Mean episode length: 381.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6627328
                    Iteration time: 4.48s
                        Total time: 3659.94s
                               ETA: 14440.7s

################################################################################
                     [1m Learning iteration 809/4000 [0m

                       Computation: 1811 steps/s (collection: 0.551s, learning 3.971s)
               Value function loss: 214.3314
                    Surrogate loss: 0.0102
             Mean action noise std: 0.95
                       Mean reward: 597.10
               Mean episode length: 394.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 4.52s
                        Total time: 3664.47s
                               ETA: 14436.2s

################################################################################
                     [1m Learning iteration 810/4000 [0m

                       Computation: 1790 steps/s (collection: 0.627s, learning 3.949s)
               Value function loss: 234.6786
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 606.01
               Mean episode length: 398.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6643712
                    Iteration time: 4.58s
                        Total time: 3669.04s
                               ETA: 14431.9s

################################################################################
                     [1m Learning iteration 811/4000 [0m

                       Computation: 1818 steps/s (collection: 0.604s, learning 3.900s)
               Value function loss: 225.8743
                    Surrogate loss: 0.0098
             Mean action noise std: 0.95
                       Mean reward: 612.91
               Mean episode length: 401.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 4.50s
                        Total time: 3673.55s
                               ETA: 14427.3s

################################################################################
                     [1m Learning iteration 812/4000 [0m

                       Computation: 1849 steps/s (collection: 0.546s, learning 3.883s)
               Value function loss: 156.0694
                    Surrogate loss: 0.0073
             Mean action noise std: 0.95
                       Mean reward: 592.99
               Mean episode length: 388.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6660096
                    Iteration time: 4.43s
                        Total time: 3677.98s
                               ETA: 14422.4s

################################################################################
                     [1m Learning iteration 813/4000 [0m

                       Computation: 1854 steps/s (collection: 0.530s, learning 3.888s)
               Value function loss: 242.7687
                    Surrogate loss: 0.0084
             Mean action noise std: 0.95
                       Mean reward: 596.42
               Mean episode length: 387.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 4.42s
                        Total time: 3682.39s
                               ETA: 14417.4s

################################################################################
                     [1m Learning iteration 814/4000 [0m

                       Computation: 1856 steps/s (collection: 0.521s, learning 3.892s)
               Value function loss: 135.2228
                    Surrogate loss: 0.0092
             Mean action noise std: 0.95
                       Mean reward: 577.94
               Mean episode length: 375.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6676480
                    Iteration time: 4.41s
                        Total time: 3686.81s
                               ETA: 14412.5s

################################################################################
                     [1m Learning iteration 815/4000 [0m

                       Computation: 1838 steps/s (collection: 0.545s, learning 3.910s)
               Value function loss: 204.0220
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 586.50
               Mean episode length: 380.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 4.45s
                        Total time: 3691.26s
                               ETA: 14407.7s

################################################################################
                     [1m Learning iteration 816/4000 [0m

                       Computation: 1841 steps/s (collection: 0.578s, learning 3.871s)
               Value function loss: 270.9058
                    Surrogate loss: 0.0117
             Mean action noise std: 0.95
                       Mean reward: 590.94
               Mean episode length: 380.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6692864
                    Iteration time: 4.45s
                        Total time: 3695.71s
                               ETA: 14402.9s

################################################################################
                     [1m Learning iteration 817/4000 [0m

                       Computation: 1835 steps/s (collection: 0.581s, learning 3.883s)
               Value function loss: 193.8725
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 587.69
               Mean episode length: 377.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 4.46s
                        Total time: 3700.17s
                               ETA: 14398.1s

################################################################################
                     [1m Learning iteration 818/4000 [0m

                       Computation: 1823 steps/s (collection: 0.586s, learning 3.907s)
               Value function loss: 150.8039
                    Surrogate loss: 0.0120
             Mean action noise std: 0.95
                       Mean reward: 555.58
               Mean episode length: 358.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6709248
                    Iteration time: 4.49s
                        Total time: 3704.67s
                               ETA: 14393.5s

################################################################################
                     [1m Learning iteration 819/4000 [0m

                       Computation: 1853 steps/s (collection: 0.536s, learning 3.884s)
               Value function loss: 256.3499
                    Surrogate loss: 0.0120
             Mean action noise std: 0.95
                       Mean reward: 548.11
               Mean episode length: 352.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 4.42s
                        Total time: 3709.09s
                               ETA: 14388.5s

################################################################################
                     [1m Learning iteration 820/4000 [0m

                       Computation: 1809 steps/s (collection: 0.637s, learning 3.892s)
               Value function loss: 171.9343
                    Surrogate loss: 0.0110
             Mean action noise std: 0.95
                       Mean reward: 544.61
               Mean episode length: 350.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6725632
                    Iteration time: 4.53s
                        Total time: 3713.62s
                               ETA: 14384.0s

################################################################################
                     [1m Learning iteration 821/4000 [0m

                       Computation: 1820 steps/s (collection: 0.564s, learning 3.935s)
               Value function loss: 210.5193
                    Surrogate loss: 0.0131
             Mean action noise std: 0.95
                       Mean reward: 558.79
               Mean episode length: 358.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 4.50s
                        Total time: 3718.12s
                               ETA: 14379.4s

################################################################################
                     [1m Learning iteration 822/4000 [0m

                       Computation: 1836 steps/s (collection: 0.552s, learning 3.909s)
               Value function loss: 173.9428
                    Surrogate loss: 0.0117
             Mean action noise std: 0.95
                       Mean reward: 563.34
               Mean episode length: 362.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6742016
                    Iteration time: 4.46s
                        Total time: 3722.58s
                               ETA: 14374.7s

################################################################################
                     [1m Learning iteration 823/4000 [0m

                       Computation: 1790 steps/s (collection: 0.602s, learning 3.975s)
               Value function loss: 205.3866
                    Surrogate loss: 0.0149
             Mean action noise std: 0.95
                       Mean reward: 577.25
               Mean episode length: 371.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 4.58s
                        Total time: 3727.15s
                               ETA: 14370.3s

################################################################################
                     [1m Learning iteration 824/4000 [0m

                       Computation: 1819 steps/s (collection: 0.630s, learning 3.874s)
               Value function loss: 208.6245
                    Surrogate loss: 0.0104
             Mean action noise std: 0.95
                       Mean reward: 580.25
               Mean episode length: 373.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6758400
                    Iteration time: 4.50s
                        Total time: 3731.66s
                               ETA: 14365.7s

################################################################################
                     [1m Learning iteration 825/4000 [0m

                       Computation: 1843 steps/s (collection: 0.536s, learning 3.909s)
               Value function loss: 188.4206
                    Surrogate loss: 0.0109
             Mean action noise std: 0.95
                       Mean reward: 566.33
               Mean episode length: 365.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 4.44s
                        Total time: 3736.10s
                               ETA: 14360.9s

################################################################################
                     [1m Learning iteration 826/4000 [0m

                       Computation: 1810 steps/s (collection: 0.629s, learning 3.897s)
               Value function loss: 281.9721
                    Surrogate loss: 0.0115
             Mean action noise std: 0.95
                       Mean reward: 578.79
               Mean episode length: 374.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6774784
                    Iteration time: 4.53s
                        Total time: 3740.63s
                               ETA: 14356.4s

################################################################################
                     [1m Learning iteration 827/4000 [0m

                       Computation: 1825 steps/s (collection: 0.565s, learning 3.923s)
               Value function loss: 205.9764
                    Surrogate loss: 0.0110
             Mean action noise std: 0.95
                       Mean reward: 588.94
               Mean episode length: 377.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 4.49s
                        Total time: 3745.11s
                               ETA: 14351.7s

################################################################################
                     [1m Learning iteration 828/4000 [0m

                       Computation: 1792 steps/s (collection: 0.657s, learning 3.914s)
               Value function loss: 234.3617
                    Surrogate loss: 0.0118
             Mean action noise std: 0.95
                       Mean reward: 598.67
               Mean episode length: 382.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6791168
                    Iteration time: 4.57s
                        Total time: 3749.68s
                               ETA: 14347.4s

################################################################################
                     [1m Learning iteration 829/4000 [0m

                       Computation: 1821 steps/s (collection: 0.576s, learning 3.923s)
               Value function loss: 209.9500
                    Surrogate loss: 0.0104
             Mean action noise std: 0.95
                       Mean reward: 613.27
               Mean episode length: 389.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 4.50s
                        Total time: 3754.18s
                               ETA: 14342.8s

################################################################################
                     [1m Learning iteration 830/4000 [0m

                       Computation: 1832 steps/s (collection: 0.532s, learning 3.939s)
               Value function loss: 216.4507
                    Surrogate loss: 0.0123
             Mean action noise std: 0.95
                       Mean reward: 614.20
               Mean episode length: 389.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6807552
                    Iteration time: 4.47s
                        Total time: 3758.65s
                               ETA: 14338.1s

################################################################################
                     [1m Learning iteration 831/4000 [0m

                       Computation: 1765 steps/s (collection: 0.611s, learning 4.029s)
               Value function loss: 134.8930
                    Surrogate loss: 0.0105
             Mean action noise std: 0.95
                       Mean reward: 617.28
               Mean episode length: 389.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 4.64s
                        Total time: 3763.29s
                               ETA: 14334.0s

################################################################################
                     [1m Learning iteration 832/4000 [0m

                       Computation: 1799 steps/s (collection: 0.577s, learning 3.975s)
               Value function loss: 236.4578
                    Surrogate loss: 0.0122
             Mean action noise std: 0.95
                       Mean reward: 621.11
               Mean episode length: 390.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6823936
                    Iteration time: 4.55s
                        Total time: 3767.85s
                               ETA: 14329.6s

################################################################################
                     [1m Learning iteration 833/4000 [0m

                       Computation: 1802 steps/s (collection: 0.557s, learning 3.988s)
               Value function loss: 194.6528
                    Surrogate loss: 0.0129
             Mean action noise std: 0.95
                       Mean reward: 633.45
               Mean episode length: 398.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 4.54s
                        Total time: 3772.39s
                               ETA: 14325.1s

################################################################################
                     [1m Learning iteration 834/4000 [0m

                       Computation: 1841 steps/s (collection: 0.528s, learning 3.921s)
               Value function loss: 163.0698
                    Surrogate loss: 0.0086
             Mean action noise std: 0.95
                       Mean reward: 637.95
               Mean episode length: 399.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6840320
                    Iteration time: 4.45s
                        Total time: 3776.84s
                               ETA: 14320.3s

################################################################################
                     [1m Learning iteration 835/4000 [0m

                       Computation: 1831 steps/s (collection: 0.554s, learning 3.918s)
               Value function loss: 295.2402
                    Surrogate loss: 0.0104
             Mean action noise std: 0.95
                       Mean reward: 664.75
               Mean episode length: 414.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 4.47s
                        Total time: 3781.31s
                               ETA: 14315.6s

################################################################################
                     [1m Learning iteration 836/4000 [0m

                       Computation: 1820 steps/s (collection: 0.564s, learning 3.936s)
               Value function loss: 172.3899
                    Surrogate loss: 0.0141
             Mean action noise std: 0.95
                       Mean reward: 667.24
               Mean episode length: 414.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6856704
                    Iteration time: 4.50s
                        Total time: 3785.81s
                               ETA: 14311.0s

################################################################################
                     [1m Learning iteration 837/4000 [0m

                       Computation: 1800 steps/s (collection: 0.569s, learning 3.981s)
               Value function loss: 240.2579
                    Surrogate loss: 0.0130
             Mean action noise std: 0.95
                       Mean reward: 687.48
               Mean episode length: 426.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 4.55s
                        Total time: 3790.36s
                               ETA: 14306.6s

################################################################################
                     [1m Learning iteration 838/4000 [0m

                       Computation: 1823 steps/s (collection: 0.564s, learning 3.929s)
               Value function loss: 237.9867
                    Surrogate loss: 0.0151
             Mean action noise std: 0.95
                       Mean reward: 693.08
               Mean episode length: 428.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6873088
                    Iteration time: 4.49s
                        Total time: 3794.86s
                               ETA: 14301.9s

################################################################################
                     [1m Learning iteration 839/4000 [0m

                       Computation: 1836 steps/s (collection: 0.545s, learning 3.915s)
               Value function loss: 254.7208
                    Surrogate loss: 0.0119
             Mean action noise std: 0.95
                       Mean reward: 685.95
               Mean episode length: 423.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 4.46s
                        Total time: 3799.32s
                               ETA: 14297.2s

################################################################################
                     [1m Learning iteration 840/4000 [0m

                       Computation: 1802 steps/s (collection: 0.576s, learning 3.970s)
               Value function loss: 191.5218
                    Surrogate loss: 0.0112
             Mean action noise std: 0.95
                       Mean reward: 670.18
               Mean episode length: 413.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6889472
                    Iteration time: 4.55s
                        Total time: 3803.86s
                               ETA: 14292.7s

################################################################################
                     [1m Learning iteration 841/4000 [0m

                       Computation: 1821 steps/s (collection: 0.564s, learning 3.934s)
               Value function loss: 229.1517
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 672.12
               Mean episode length: 415.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 4.50s
                        Total time: 3808.36s
                               ETA: 14288.1s

################################################################################
                     [1m Learning iteration 842/4000 [0m

                       Computation: 1801 steps/s (collection: 0.596s, learning 3.952s)
               Value function loss: 328.5249
                    Surrogate loss: 0.0108
             Mean action noise std: 0.95
                       Mean reward: 679.53
               Mean episode length: 417.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6905856
                    Iteration time: 4.55s
                        Total time: 3812.91s
                               ETA: 14283.7s

################################################################################
                     [1m Learning iteration 843/4000 [0m

                       Computation: 1845 steps/s (collection: 0.533s, learning 3.905s)
               Value function loss: 232.5420
                    Surrogate loss: 0.0133
             Mean action noise std: 0.95
                       Mean reward: 673.91
               Mean episode length: 412.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 4.44s
                        Total time: 3817.35s
                               ETA: 14278.9s

################################################################################
                     [1m Learning iteration 844/4000 [0m

                       Computation: 1818 steps/s (collection: 0.566s, learning 3.939s)
               Value function loss: 191.6066
                    Surrogate loss: 0.0126
             Mean action noise std: 0.95
                       Mean reward: 649.71
               Mean episode length: 397.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6922240
                    Iteration time: 4.51s
                        Total time: 3821.85s
                               ETA: 14274.3s

################################################################################
                     [1m Learning iteration 845/4000 [0m

                       Computation: 1837 steps/s (collection: 0.534s, learning 3.925s)
               Value function loss: 178.7471
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 615.02
               Mean episode length: 378.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 4.46s
                        Total time: 3826.31s
                               ETA: 14269.5s

################################################################################
                     [1m Learning iteration 846/4000 [0m

                       Computation: 1820 steps/s (collection: 0.590s, learning 3.908s)
               Value function loss: 184.4000
                    Surrogate loss: 0.0114
             Mean action noise std: 0.95
                       Mean reward: 604.14
               Mean episode length: 373.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6938624
                    Iteration time: 4.50s
                        Total time: 3830.81s
                               ETA: 14264.9s

################################################################################
                     [1m Learning iteration 847/4000 [0m

                       Computation: 1838 steps/s (collection: 0.563s, learning 3.892s)
               Value function loss: 192.5442
                    Surrogate loss: 0.0082
             Mean action noise std: 0.95
                       Mean reward: 598.83
               Mean episode length: 369.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 4.46s
                        Total time: 3835.27s
                               ETA: 14260.1s

################################################################################
                     [1m Learning iteration 848/4000 [0m

                       Computation: 1784 steps/s (collection: 0.658s, learning 3.933s)
               Value function loss: 187.5754
                    Surrogate loss: 0.0098
             Mean action noise std: 0.95
                       Mean reward: 603.67
               Mean episode length: 371.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6955008
                    Iteration time: 4.59s
                        Total time: 3839.86s
                               ETA: 14255.9s

################################################################################
                     [1m Learning iteration 849/4000 [0m

                       Computation: 1803 steps/s (collection: 0.641s, learning 3.902s)
               Value function loss: 153.8132
                    Surrogate loss: 0.0138
             Mean action noise std: 0.95
                       Mean reward: 589.65
               Mean episode length: 363.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 4.54s
                        Total time: 3844.40s
                               ETA: 14251.4s

################################################################################
                     [1m Learning iteration 850/4000 [0m

                       Computation: 1808 steps/s (collection: 0.593s, learning 3.936s)
               Value function loss: 198.6415
                    Surrogate loss: 0.0099
             Mean action noise std: 0.95
                       Mean reward: 594.18
               Mean episode length: 366.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6971392
                    Iteration time: 4.53s
                        Total time: 3848.93s
                               ETA: 14246.9s

################################################################################
                     [1m Learning iteration 851/4000 [0m

                       Computation: 1826 steps/s (collection: 0.587s, learning 3.898s)
               Value function loss: 141.7311
                    Surrogate loss: 0.0118
             Mean action noise std: 0.95
                       Mean reward: 584.46
               Mean episode length: 361.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 4.48s
                        Total time: 3853.41s
                               ETA: 14242.2s

################################################################################
                     [1m Learning iteration 852/4000 [0m

                       Computation: 1824 steps/s (collection: 0.556s, learning 3.934s)
               Value function loss: 145.0375
                    Surrogate loss: 0.0139
             Mean action noise std: 0.95
                       Mean reward: 571.63
               Mean episode length: 354.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6987776
                    Iteration time: 4.49s
                        Total time: 3857.90s
                               ETA: 14237.6s

################################################################################
                     [1m Learning iteration 853/4000 [0m

                       Computation: 1745 steps/s (collection: 0.693s, learning 4.000s)
               Value function loss: 206.5274
                    Surrogate loss: 0.0156
             Mean action noise std: 0.95
                       Mean reward: 570.22
               Mean episode length: 355.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 4.69s
                        Total time: 3862.60s
                               ETA: 14233.7s

################################################################################
                     [1m Learning iteration 854/4000 [0m

                       Computation: 1818 steps/s (collection: 0.581s, learning 3.924s)
               Value function loss: 229.4228
                    Surrogate loss: 0.0100
             Mean action noise std: 0.95
                       Mean reward: 582.21
               Mean episode length: 364.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7004160
                    Iteration time: 4.50s
                        Total time: 3867.10s
                               ETA: 14229.1s

################################################################################
                     [1m Learning iteration 855/4000 [0m

                       Computation: 1781 steps/s (collection: 0.683s, learning 3.916s)
               Value function loss: 216.4548
                    Surrogate loss: 0.0111
             Mean action noise std: 0.95
                       Mean reward: 592.11
               Mean episode length: 369.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 4.60s
                        Total time: 3871.70s
                               ETA: 14224.9s

################################################################################
                     [1m Learning iteration 856/4000 [0m

                       Computation: 1810 steps/s (collection: 0.569s, learning 3.956s)
               Value function loss: 257.6716
                    Surrogate loss: 0.0117
             Mean action noise std: 0.95
                       Mean reward: 607.11
               Mean episode length: 376.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7020544
                    Iteration time: 4.52s
                        Total time: 3876.22s
                               ETA: 14220.4s

################################################################################
                     [1m Learning iteration 857/4000 [0m

                       Computation: 1837 steps/s (collection: 0.549s, learning 3.908s)
               Value function loss: 260.3349
                    Surrogate loss: 0.0118
             Mean action noise std: 0.95
                       Mean reward: 615.79
               Mean episode length: 383.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 4.46s
                        Total time: 3880.68s
                               ETA: 14215.6s

################################################################################
                     [1m Learning iteration 858/4000 [0m

                       Computation: 1832 steps/s (collection: 0.569s, learning 3.902s)
               Value function loss: 243.2520
                    Surrogate loss: 0.0135
             Mean action noise std: 0.95
                       Mean reward: 619.22
               Mean episode length: 389.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7036928
                    Iteration time: 4.47s
                        Total time: 3885.15s
                               ETA: 14210.9s

################################################################################
                     [1m Learning iteration 859/4000 [0m

                       Computation: 1806 steps/s (collection: 0.575s, learning 3.959s)
               Value function loss: 190.2447
                    Surrogate loss: 0.0111
             Mean action noise std: 0.95
                       Mean reward: 637.82
               Mean episode length: 398.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 4.53s
                        Total time: 3889.69s
                               ETA: 14206.4s

################################################################################
                     [1m Learning iteration 860/4000 [0m

                       Computation: 1811 steps/s (collection: 0.616s, learning 3.907s)
               Value function loss: 149.2247
                    Surrogate loss: 0.0160
             Mean action noise std: 0.95
                       Mean reward: 641.98
               Mean episode length: 402.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7053312
                    Iteration time: 4.52s
                        Total time: 3894.21s
                               ETA: 14201.9s

################################################################################
                     [1m Learning iteration 861/4000 [0m

                       Computation: 1803 steps/s (collection: 0.567s, learning 3.974s)
               Value function loss: 168.5035
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 654.05
               Mean episode length: 409.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 4.54s
                        Total time: 3898.75s
                               ETA: 14197.4s

################################################################################
                     [1m Learning iteration 862/4000 [0m

                       Computation: 1782 steps/s (collection: 0.629s, learning 3.966s)
               Value function loss: 185.8837
                    Surrogate loss: 0.0165
             Mean action noise std: 0.95
                       Mean reward: 655.38
               Mean episode length: 413.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7069696
                    Iteration time: 4.59s
                        Total time: 3903.34s
                               ETA: 14193.2s

################################################################################
                     [1m Learning iteration 863/4000 [0m

                       Computation: 1825 steps/s (collection: 0.533s, learning 3.954s)
               Value function loss: 170.1502
                    Surrogate loss: 0.0147
             Mean action noise std: 0.95
                       Mean reward: 646.20
               Mean episode length: 408.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 4.49s
                        Total time: 3907.83s
                               ETA: 14188.5s

################################################################################
                     [1m Learning iteration 864/4000 [0m

                       Computation: 1795 steps/s (collection: 0.598s, learning 3.963s)
               Value function loss: 126.5680
                    Surrogate loss: 0.0168
             Mean action noise std: 0.95
                       Mean reward: 630.80
               Mean episode length: 398.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7086080
                    Iteration time: 4.56s
                        Total time: 3912.39s
                               ETA: 14184.1s

################################################################################
                     [1m Learning iteration 865/4000 [0m

                       Computation: 1819 steps/s (collection: 0.563s, learning 3.940s)
               Value function loss: 143.4704
                    Surrogate loss: 0.0148
             Mean action noise std: 0.95
                       Mean reward: 647.16
               Mean episode length: 408.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 4.50s
                        Total time: 3916.90s
                               ETA: 14179.5s

################################################################################
                     [1m Learning iteration 866/4000 [0m

                       Computation: 1821 steps/s (collection: 0.571s, learning 3.926s)
               Value function loss: 169.8665
                    Surrogate loss: 0.0134
             Mean action noise std: 0.95
                       Mean reward: 648.06
               Mean episode length: 409.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7102464
                    Iteration time: 4.50s
                        Total time: 3921.39s
                               ETA: 14174.9s

################################################################################
                     [1m Learning iteration 867/4000 [0m

                       Computation: 1808 steps/s (collection: 0.560s, learning 3.969s)
               Value function loss: 149.3558
                    Surrogate loss: 0.0126
             Mean action noise std: 0.95
                       Mean reward: 624.60
               Mean episode length: 393.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 4.53s
                        Total time: 3925.92s
                               ETA: 14170.4s

################################################################################
                     [1m Learning iteration 868/4000 [0m

                       Computation: 1807 steps/s (collection: 0.609s, learning 3.924s)
               Value function loss: 279.3495
                    Surrogate loss: 0.0083
             Mean action noise std: 0.95
                       Mean reward: 636.79
               Mean episode length: 399.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7118848
                    Iteration time: 4.53s
                        Total time: 3930.46s
                               ETA: 14165.9s

################################################################################
                     [1m Learning iteration 869/4000 [0m

                       Computation: 1835 steps/s (collection: 0.548s, learning 3.916s)
               Value function loss: 213.6302
                    Surrogate loss: 0.0108
             Mean action noise std: 0.95
                       Mean reward: 639.28
               Mean episode length: 399.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 4.46s
                        Total time: 3934.92s
                               ETA: 14161.2s

################################################################################
                     [1m Learning iteration 870/4000 [0m

                       Computation: 1804 steps/s (collection: 0.612s, learning 3.927s)
               Value function loss: 202.3750
                    Surrogate loss: 0.0110
             Mean action noise std: 0.95
                       Mean reward: 632.68
               Mean episode length: 394.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7135232
                    Iteration time: 4.54s
                        Total time: 3939.46s
                               ETA: 14156.7s

################################################################################
                     [1m Learning iteration 871/4000 [0m

                       Computation: 1800 steps/s (collection: 0.592s, learning 3.957s)
               Value function loss: 190.8100
                    Surrogate loss: 0.0081
             Mean action noise std: 0.95
                       Mean reward: 607.92
               Mean episode length: 377.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 4.55s
                        Total time: 3944.01s
                               ETA: 14152.3s

################################################################################
                     [1m Learning iteration 872/4000 [0m

                       Computation: 1805 steps/s (collection: 0.601s, learning 3.937s)
               Value function loss: 166.1623
                    Surrogate loss: 0.0104
             Mean action noise std: 0.95
                       Mean reward: 609.33
               Mean episode length: 374.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7151616
                    Iteration time: 4.54s
                        Total time: 3948.54s
                               ETA: 14147.8s

################################################################################
                     [1m Learning iteration 873/4000 [0m

                       Computation: 1822 steps/s (collection: 0.596s, learning 3.899s)
               Value function loss: 265.4147
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 638.20
               Mean episode length: 393.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 4.50s
                        Total time: 3953.04s
                               ETA: 14143.2s

################################################################################
                     [1m Learning iteration 874/4000 [0m

                       Computation: 1794 steps/s (collection: 0.599s, learning 3.966s)
               Value function loss: 215.3264
                    Surrogate loss: 0.0098
             Mean action noise std: 0.95
                       Mean reward: 655.37
               Mean episode length: 402.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7168000
                    Iteration time: 4.57s
                        Total time: 3957.60s
                               ETA: 14138.8s

################################################################################
                     [1m Learning iteration 875/4000 [0m

                       Computation: 1800 steps/s (collection: 0.582s, learning 3.967s)
               Value function loss: 188.9141
                    Surrogate loss: 0.0095
             Mean action noise std: 0.95
                       Mean reward: 652.47
               Mean episode length: 403.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 4.55s
                        Total time: 3962.15s
                               ETA: 14134.4s

################################################################################
                     [1m Learning iteration 876/4000 [0m

                       Computation: 1798 steps/s (collection: 0.593s, learning 3.963s)
               Value function loss: 157.4434
                    Surrogate loss: 0.0085
             Mean action noise std: 0.95
                       Mean reward: 656.70
               Mean episode length: 407.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7184384
                    Iteration time: 4.56s
                        Total time: 3966.71s
                               ETA: 14130.0s

################################################################################
                     [1m Learning iteration 877/4000 [0m

                       Computation: 1794 steps/s (collection: 0.596s, learning 3.969s)
               Value function loss: 169.8339
                    Surrogate loss: 0.0108
             Mean action noise std: 0.95
                       Mean reward: 676.78
               Mean episode length: 418.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 4.56s
                        Total time: 3971.27s
                               ETA: 14125.6s

################################################################################
                     [1m Learning iteration 878/4000 [0m

                       Computation: 1802 steps/s (collection: 0.596s, learning 3.949s)
               Value function loss: 248.6321
                    Surrogate loss: 0.0085
             Mean action noise std: 0.95
                       Mean reward: 665.45
               Mean episode length: 411.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7200768
                    Iteration time: 4.55s
                        Total time: 3975.82s
                               ETA: 14121.2s

################################################################################
                     [1m Learning iteration 879/4000 [0m

                       Computation: 1789 steps/s (collection: 0.650s, learning 3.927s)
               Value function loss: 148.4782
                    Surrogate loss: 0.0099
             Mean action noise std: 0.95
                       Mean reward: 669.73
               Mean episode length: 417.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 4.58s
                        Total time: 3980.40s
                               ETA: 14116.8s

################################################################################
                     [1m Learning iteration 880/4000 [0m

                       Computation: 1805 steps/s (collection: 0.578s, learning 3.959s)
               Value function loss: 215.8012
                    Surrogate loss: 0.0095
             Mean action noise std: 0.95
                       Mean reward: 668.60
               Mean episode length: 417.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7217152
                    Iteration time: 4.54s
                        Total time: 3984.93s
                               ETA: 14112.4s

################################################################################
                     [1m Learning iteration 881/4000 [0m

                       Computation: 1795 steps/s (collection: 0.598s, learning 3.964s)
               Value function loss: 183.8087
                    Surrogate loss: 0.0117
             Mean action noise std: 0.95
                       Mean reward: 672.75
               Mean episode length: 421.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 4.56s
                        Total time: 3989.50s
                               ETA: 14108.0s

################################################################################
                     [1m Learning iteration 882/4000 [0m

                       Computation: 1796 steps/s (collection: 0.605s, learning 3.956s)
               Value function loss: 185.7051
                    Surrogate loss: 0.0096
             Mean action noise std: 0.95
                       Mean reward: 667.69
               Mean episode length: 419.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7233536
                    Iteration time: 4.56s
                        Total time: 3994.06s
                               ETA: 14103.6s

################################################################################
                     [1m Learning iteration 883/4000 [0m

                       Computation: 1831 steps/s (collection: 0.521s, learning 3.951s)
               Value function loss: 184.1275
                    Surrogate loss: 0.0103
             Mean action noise std: 0.95
                       Mean reward: 660.37
               Mean episode length: 413.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 4.47s
                        Total time: 3998.53s
                               ETA: 14098.9s

################################################################################
                     [1m Learning iteration 884/4000 [0m

                       Computation: 1803 steps/s (collection: 0.603s, learning 3.940s)
               Value function loss: 313.2755
                    Surrogate loss: 0.0108
             Mean action noise std: 0.95
                       Mean reward: 652.83
               Mean episode length: 410.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7249920
                    Iteration time: 4.54s
                        Total time: 4003.07s
                               ETA: 14094.4s

################################################################################
                     [1m Learning iteration 885/4000 [0m

                       Computation: 1797 steps/s (collection: 0.618s, learning 3.940s)
               Value function loss: 191.5046
                    Surrogate loss: 0.0105
             Mean action noise std: 0.95
                       Mean reward: 643.65
               Mean episode length: 405.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 4.56s
                        Total time: 4007.63s
                               ETA: 14090.0s

################################################################################
                     [1m Learning iteration 886/4000 [0m

                       Computation: 1808 steps/s (collection: 0.585s, learning 3.945s)
               Value function loss: 312.3522
                    Surrogate loss: 0.0082
             Mean action noise std: 0.95
                       Mean reward: 659.42
               Mean episode length: 414.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7266304
                    Iteration time: 4.53s
                        Total time: 4012.16s
                               ETA: 14085.5s

################################################################################
                     [1m Learning iteration 887/4000 [0m

                       Computation: 1816 steps/s (collection: 0.578s, learning 3.932s)
               Value function loss: 193.1186
                    Surrogate loss: 0.0109
             Mean action noise std: 0.95
                       Mean reward: 650.53
               Mean episode length: 409.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 4.51s
                        Total time: 4016.67s
                               ETA: 14081.0s

################################################################################
                     [1m Learning iteration 888/4000 [0m

                       Computation: 1825 steps/s (collection: 0.574s, learning 3.914s)
               Value function loss: 160.1554
                    Surrogate loss: 0.0095
             Mean action noise std: 0.95
                       Mean reward: 667.80
               Mean episode length: 419.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7282688
                    Iteration time: 4.49s
                        Total time: 4021.16s
                               ETA: 14076.3s

################################################################################
                     [1m Learning iteration 889/4000 [0m

                       Computation: 1819 steps/s (collection: 0.569s, learning 3.934s)
               Value function loss: 290.0386
                    Surrogate loss: 0.0107
             Mean action noise std: 0.95
                       Mean reward: 669.90
               Mean episode length: 418.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 4.50s
                        Total time: 4025.66s
                               ETA: 14071.7s

################################################################################
                     [1m Learning iteration 890/4000 [0m

                       Computation: 1839 steps/s (collection: 0.544s, learning 3.909s)
               Value function loss: 148.6525
                    Surrogate loss: 0.0097
             Mean action noise std: 0.95
                       Mean reward: 665.41
               Mean episode length: 417.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 7299072
                    Iteration time: 4.45s
                        Total time: 4030.11s
                               ETA: 14066.9s

################################################################################
                     [1m Learning iteration 891/4000 [0m

                       Computation: 1827 steps/s (collection: 0.558s, learning 3.924s)
               Value function loss: 197.9477
                    Surrogate loss: 0.0080
             Mean action noise std: 0.95
                       Mean reward: 674.79
               Mean episode length: 422.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 4.48s
                        Total time: 4034.60s
                               ETA: 14062.3s

################################################################################
                     [1m Learning iteration 892/4000 [0m

                       Computation: 1823 steps/s (collection: 0.585s, learning 3.909s)
               Value function loss: 211.6066
                    Surrogate loss: 0.0073
             Mean action noise std: 0.95
                       Mean reward: 672.80
               Mean episode length: 421.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7315456
                    Iteration time: 4.49s
                        Total time: 4039.09s
                               ETA: 14057.7s

################################################################################
                     [1m Learning iteration 893/4000 [0m

                       Computation: 1830 steps/s (collection: 0.573s, learning 3.901s)
               Value function loss: 234.3410
                    Surrogate loss: 0.0074
             Mean action noise std: 0.95
                       Mean reward: 670.12
               Mean episode length: 420.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 4.47s
                        Total time: 4043.56s
                               ETA: 14053.0s

################################################################################
                     [1m Learning iteration 894/4000 [0m

                       Computation: 1796 steps/s (collection: 0.593s, learning 3.966s)
               Value function loss: 229.2900
                    Surrogate loss: 0.0093
             Mean action noise std: 0.95
                       Mean reward: 667.00
               Mean episode length: 420.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7331840
                    Iteration time: 4.56s
                        Total time: 4048.12s
                               ETA: 14048.6s

################################################################################
                     [1m Learning iteration 895/4000 [0m

                       Computation: 1815 steps/s (collection: 0.631s, learning 3.881s)
               Value function loss: 209.2536
                    Surrogate loss: 0.0097
             Mean action noise std: 0.95
                       Mean reward: 665.79
               Mean episode length: 419.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 4.51s
                        Total time: 4052.63s
                               ETA: 14044.0s

################################################################################
                     [1m Learning iteration 896/4000 [0m

                       Computation: 1838 steps/s (collection: 0.548s, learning 3.909s)
               Value function loss: 175.0561
                    Surrogate loss: 0.0093
             Mean action noise std: 0.95
                       Mean reward: 652.58
               Mean episode length: 409.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7348224
                    Iteration time: 4.46s
                        Total time: 4057.09s
                               ETA: 14039.3s

################################################################################
                     [1m Learning iteration 897/4000 [0m

                       Computation: 1841 steps/s (collection: 0.557s, learning 3.892s)
               Value function loss: 265.0108
                    Surrogate loss: 0.0099
             Mean action noise std: 0.95
                       Mean reward: 650.63
               Mean episode length: 407.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 4.45s
                        Total time: 4061.54s
                               ETA: 14034.5s

################################################################################
                     [1m Learning iteration 898/4000 [0m

                       Computation: 1832 steps/s (collection: 0.566s, learning 3.905s)
               Value function loss: 238.2109
                    Surrogate loss: 0.0085
             Mean action noise std: 0.95
                       Mean reward: 649.95
               Mean episode length: 407.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7364608
                    Iteration time: 4.47s
                        Total time: 4066.01s
                               ETA: 14029.8s

################################################################################
                     [1m Learning iteration 899/4000 [0m

                       Computation: 1839 steps/s (collection: 0.563s, learning 3.890s)
               Value function loss: 203.8242
                    Surrogate loss: 0.0093
             Mean action noise std: 0.95
                       Mean reward: 650.42
               Mean episode length: 407.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 4.45s
                        Total time: 4070.46s
                               ETA: 14025.0s

################################################################################
                     [1m Learning iteration 900/4000 [0m

                       Computation: 1816 steps/s (collection: 0.607s, learning 3.902s)
               Value function loss: 203.9874
                    Surrogate loss: 0.0098
             Mean action noise std: 0.95
                       Mean reward: 645.87
               Mean episode length: 404.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7380992
                    Iteration time: 4.51s
                        Total time: 4074.97s
                               ETA: 14020.4s

################################################################################
                     [1m Learning iteration 901/4000 [0m

                       Computation: 1840 steps/s (collection: 0.547s, learning 3.903s)
               Value function loss: 166.5096
                    Surrogate loss: 0.0097
             Mean action noise std: 0.95
                       Mean reward: 636.31
               Mean episode length: 399.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 4.45s
                        Total time: 4079.42s
                               ETA: 14015.7s

################################################################################
                     [1m Learning iteration 902/4000 [0m

                       Computation: 1817 steps/s (collection: 0.564s, learning 3.944s)
               Value function loss: 240.4903
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 646.23
               Mean episode length: 403.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7397376
                    Iteration time: 4.51s
                        Total time: 4083.93s
                               ETA: 14011.1s

################################################################################
                     [1m Learning iteration 903/4000 [0m

                       Computation: 1835 steps/s (collection: 0.534s, learning 3.929s)
               Value function loss: 188.4291
                    Surrogate loss: 0.0102
             Mean action noise std: 0.95
                       Mean reward: 646.33
               Mean episode length: 403.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 4.46s
                        Total time: 4088.39s
                               ETA: 14006.4s

################################################################################
                     [1m Learning iteration 904/4000 [0m

                       Computation: 1817 steps/s (collection: 0.569s, learning 3.938s)
               Value function loss: 235.4068
                    Surrogate loss: 0.0078
             Mean action noise std: 0.95
                       Mean reward: 639.74
               Mean episode length: 398.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7413760
                    Iteration time: 4.51s
                        Total time: 4092.90s
                               ETA: 14001.8s

################################################################################
                     [1m Learning iteration 905/4000 [0m

                       Computation: 1825 steps/s (collection: 0.565s, learning 3.922s)
               Value function loss: 203.9679
                    Surrogate loss: 0.0084
             Mean action noise std: 0.95
                       Mean reward: 627.31
               Mean episode length: 393.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 4.49s
                        Total time: 4097.39s
                               ETA: 13997.1s

################################################################################
                     [1m Learning iteration 906/4000 [0m

                       Computation: 1823 steps/s (collection: 0.583s, learning 3.910s)
               Value function loss: 203.4233
                    Surrogate loss: 0.0084
             Mean action noise std: 0.95
                       Mean reward: 617.29
               Mean episode length: 389.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7430144
                    Iteration time: 4.49s
                        Total time: 4101.88s
                               ETA: 13992.5s

################################################################################
                     [1m Learning iteration 907/4000 [0m

                       Computation: 1814 steps/s (collection: 0.542s, learning 3.974s)
               Value function loss: 185.3196
                    Surrogate loss: 0.0122
             Mean action noise std: 0.95
                       Mean reward: 614.68
               Mean episode length: 387.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 4.52s
                        Total time: 4106.40s
                               ETA: 13988.0s

################################################################################
                     [1m Learning iteration 908/4000 [0m

                       Computation: 1809 steps/s (collection: 0.533s, learning 3.994s)
               Value function loss: 179.4982
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 598.97
               Mean episode length: 377.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7446528
                    Iteration time: 4.53s
                        Total time: 4110.92s
                               ETA: 13983.5s

################################################################################
                     [1m Learning iteration 909/4000 [0m

                       Computation: 1781 steps/s (collection: 0.592s, learning 4.006s)
               Value function loss: 295.9520
                    Surrogate loss: 0.0105
             Mean action noise std: 0.95
                       Mean reward: 584.22
               Mean episode length: 369.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 4.60s
                        Total time: 4115.52s
                               ETA: 13979.2s

################################################################################
                     [1m Learning iteration 910/4000 [0m

                       Computation: 1808 steps/s (collection: 0.526s, learning 4.004s)
               Value function loss: 203.2711
                    Surrogate loss: 0.0084
             Mean action noise std: 0.95
                       Mean reward: 578.98
               Mean episode length: 365.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7462912
                    Iteration time: 4.53s
                        Total time: 4120.05s
                               ETA: 13974.7s

################################################################################
                     [1m Learning iteration 911/4000 [0m

                       Computation: 1807 steps/s (collection: 0.531s, learning 4.001s)
               Value function loss: 164.5708
                    Surrogate loss: 0.0104
             Mean action noise std: 0.95
                       Mean reward: 567.62
               Mean episode length: 358.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 4.53s
                        Total time: 4124.58s
                               ETA: 13970.2s

################################################################################
                     [1m Learning iteration 912/4000 [0m

                       Computation: 1811 steps/s (collection: 0.561s, learning 3.962s)
               Value function loss: 195.7222
                    Surrogate loss: 0.0092
             Mean action noise std: 0.95
                       Mean reward: 556.40
               Mean episode length: 351.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7479296
                    Iteration time: 4.52s
                        Total time: 4129.11s
                               ETA: 13965.7s

################################################################################
                     [1m Learning iteration 913/4000 [0m

                       Computation: 1825 steps/s (collection: 0.553s, learning 3.935s)
               Value function loss: 270.0350
                    Surrogate loss: 0.0102
             Mean action noise std: 0.95
                       Mean reward: 548.46
               Mean episode length: 345.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 4.49s
                        Total time: 4133.59s
                               ETA: 13961.1s

################################################################################
                     [1m Learning iteration 914/4000 [0m

                       Computation: 1821 steps/s (collection: 0.576s, learning 3.921s)
               Value function loss: 169.3041
                    Surrogate loss: 0.0093
             Mean action noise std: 0.95
                       Mean reward: 555.51
               Mean episode length: 348.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7495680
                    Iteration time: 4.50s
                        Total time: 4138.09s
                               ETA: 13956.4s

################################################################################
                     [1m Learning iteration 915/4000 [0m

                       Computation: 1833 steps/s (collection: 0.581s, learning 3.888s)
               Value function loss: 211.7813
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 559.44
               Mean episode length: 351.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 4.47s
                        Total time: 4142.56s
                               ETA: 13951.7s

################################################################################
                     [1m Learning iteration 916/4000 [0m

                       Computation: 1823 steps/s (collection: 0.552s, learning 3.939s)
               Value function loss: 170.1160
                    Surrogate loss: 0.0081
             Mean action noise std: 0.95
                       Mean reward: 563.70
               Mean episode length: 356.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7512064
                    Iteration time: 4.49s
                        Total time: 4147.05s
                               ETA: 13947.1s

################################################################################
                     [1m Learning iteration 917/4000 [0m

                       Computation: 1800 steps/s (collection: 0.652s, learning 3.898s)
               Value function loss: 246.1293
                    Surrogate loss: 0.0085
             Mean action noise std: 0.95
                       Mean reward: 554.74
               Mean episode length: 351.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 4.55s
                        Total time: 4151.60s
                               ETA: 13942.7s

################################################################################
                     [1m Learning iteration 918/4000 [0m

                       Computation: 1789 steps/s (collection: 0.608s, learning 3.970s)
               Value function loss: 207.4953
                    Surrogate loss: 0.0087
             Mean action noise std: 0.95
                       Mean reward: 562.45
               Mean episode length: 354.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7528448
                    Iteration time: 4.58s
                        Total time: 4156.18s
                               ETA: 13938.3s

################################################################################
                     [1m Learning iteration 919/4000 [0m

                       Computation: 1795 steps/s (collection: 0.628s, learning 3.935s)
               Value function loss: 175.7476
                    Surrogate loss: 0.0101
             Mean action noise std: 0.95
                       Mean reward: 563.60
               Mean episode length: 354.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 4.56s
                        Total time: 4160.74s
                               ETA: 13934.0s

################################################################################
                     [1m Learning iteration 920/4000 [0m

                       Computation: 1818 steps/s (collection: 0.552s, learning 3.954s)
               Value function loss: 271.0501
                    Surrogate loss: 0.0078
             Mean action noise std: 0.95
                       Mean reward: 566.15
               Mean episode length: 358.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7544832
                    Iteration time: 4.51s
                        Total time: 4165.25s
                               ETA: 13929.4s

################################################################################
                     [1m Learning iteration 921/4000 [0m

                       Computation: 1845 steps/s (collection: 0.561s, learning 3.877s)
               Value function loss: 281.4443
                    Surrogate loss: 0.0081
             Mean action noise std: 0.95
                       Mean reward: 568.96
               Mean episode length: 361.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 4.44s
                        Total time: 4169.69s
                               ETA: 13924.6s

################################################################################
                     [1m Learning iteration 922/4000 [0m

                       Computation: 1839 steps/s (collection: 0.550s, learning 3.904s)
               Value function loss: 222.3683
                    Surrogate loss: 0.0099
             Mean action noise std: 0.95
                       Mean reward: 588.13
               Mean episode length: 374.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7561216
                    Iteration time: 4.45s
                        Total time: 4174.14s
                               ETA: 13919.8s

################################################################################
                     [1m Learning iteration 923/4000 [0m

                       Computation: 1818 steps/s (collection: 0.599s, learning 3.907s)
               Value function loss: 232.5596
                    Surrogate loss: 0.0109
             Mean action noise std: 0.95
                       Mean reward: 580.72
               Mean episode length: 370.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 4.51s
                        Total time: 4178.64s
                               ETA: 13915.2s

################################################################################
                     [1m Learning iteration 924/4000 [0m

                       Computation: 1839 steps/s (collection: 0.547s, learning 3.908s)
               Value function loss: 186.4420
                    Surrogate loss: 0.0094
             Mean action noise std: 0.95
                       Mean reward: 582.61
               Mean episode length: 370.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7577600
                    Iteration time: 4.45s
                        Total time: 4183.10s
                               ETA: 13910.5s

################################################################################
                     [1m Learning iteration 925/4000 [0m

                       Computation: 1814 steps/s (collection: 0.570s, learning 3.943s)
               Value function loss: 335.6265
                    Surrogate loss: 0.0162
             Mean action noise std: 0.95
                       Mean reward: 600.52
               Mean episode length: 380.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 4.51s
                        Total time: 4187.61s
                               ETA: 13906.0s

################################################################################
                     [1m Learning iteration 926/4000 [0m

                       Computation: 1841 steps/s (collection: 0.537s, learning 3.911s)
               Value function loss: 202.4253
                    Surrogate loss: 0.0129
             Mean action noise std: 0.95
                       Mean reward: 618.87
               Mean episode length: 391.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7593984
                    Iteration time: 4.45s
                        Total time: 4192.06s
                               ETA: 13901.2s

################################################################################
                     [1m Learning iteration 927/4000 [0m

                       Computation: 1802 steps/s (collection: 0.595s, learning 3.951s)
               Value function loss: 189.2138
                    Surrogate loss: 0.0082
             Mean action noise std: 0.95
                       Mean reward: 620.17
               Mean episode length: 394.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 4.55s
                        Total time: 4196.61s
                               ETA: 13896.7s

################################################################################
                     [1m Learning iteration 928/4000 [0m

                       Computation: 1804 steps/s (collection: 0.639s, learning 3.901s)
               Value function loss: 237.2388
                    Surrogate loss: 0.0114
             Mean action noise std: 0.95
                       Mean reward: 605.92
               Mean episode length: 384.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7610368
                    Iteration time: 4.54s
                        Total time: 4201.15s
                               ETA: 13892.3s

################################################################################
                     [1m Learning iteration 929/4000 [0m

                       Computation: 1822 steps/s (collection: 0.566s, learning 3.929s)
               Value function loss: 201.7100
                    Surrogate loss: 0.0093
             Mean action noise std: 0.95
                       Mean reward: 608.28
               Mean episode length: 384.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 4.49s
                        Total time: 4205.64s
                               ETA: 13887.7s

################################################################################
                     [1m Learning iteration 930/4000 [0m

                       Computation: 1808 steps/s (collection: 0.622s, learning 3.907s)
               Value function loss: 174.8381
                    Surrogate loss: 0.0102
             Mean action noise std: 0.95
                       Mean reward: 610.81
               Mean episode length: 385.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7626752
                    Iteration time: 4.53s
                        Total time: 4210.17s
                               ETA: 13883.2s

################################################################################
                     [1m Learning iteration 931/4000 [0m

                       Computation: 1831 steps/s (collection: 0.559s, learning 3.915s)
               Value function loss: 178.9187
                    Surrogate loss: 0.0133
             Mean action noise std: 0.95
                       Mean reward: 625.43
               Mean episode length: 393.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 4.47s
                        Total time: 4214.64s
                               ETA: 13878.5s

################################################################################
                     [1m Learning iteration 932/4000 [0m

                       Computation: 1818 steps/s (collection: 0.605s, learning 3.899s)
               Value function loss: 191.9878
                    Surrogate loss: 0.0191
             Mean action noise std: 0.95
                       Mean reward: 611.94
               Mean episode length: 385.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7643136
                    Iteration time: 4.50s
                        Total time: 4219.15s
                               ETA: 13873.9s

################################################################################
                     [1m Learning iteration 933/4000 [0m

                       Computation: 1810 steps/s (collection: 0.572s, learning 3.953s)
               Value function loss: 311.9396
                    Surrogate loss: 0.0105
             Mean action noise std: 0.95
                       Mean reward: 623.31
               Mean episode length: 391.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 4.52s
                        Total time: 4223.67s
                               ETA: 13869.4s

################################################################################
                     [1m Learning iteration 934/4000 [0m

                       Computation: 1808 steps/s (collection: 0.603s, learning 3.926s)
               Value function loss: 191.5744
                    Surrogate loss: 0.0114
             Mean action noise std: 0.95
                       Mean reward: 613.99
               Mean episode length: 387.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7659520
                    Iteration time: 4.53s
                        Total time: 4228.20s
                               ETA: 13864.9s

################################################################################
                     [1m Learning iteration 935/4000 [0m

                       Computation: 1831 steps/s (collection: 0.554s, learning 3.919s)
               Value function loss: 161.3964
                    Surrogate loss: 0.0074
             Mean action noise std: 0.95
                       Mean reward: 596.17
               Mean episode length: 374.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 4.47s
                        Total time: 4232.67s
                               ETA: 13860.2s

################################################################################
                     [1m Learning iteration 936/4000 [0m

                       Computation: 1792 steps/s (collection: 0.646s, learning 3.925s)
               Value function loss: 306.3751
                    Surrogate loss: 0.0078
             Mean action noise std: 0.95
                       Mean reward: 596.68
               Mean episode length: 374.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7675904
                    Iteration time: 4.57s
                        Total time: 4237.25s
                               ETA: 13855.8s

################################################################################
                     [1m Learning iteration 937/4000 [0m

                       Computation: 1824 steps/s (collection: 0.539s, learning 3.951s)
               Value function loss: 203.4537
                    Surrogate loss: 0.0083
             Mean action noise std: 0.95
                       Mean reward: 594.08
               Mean episode length: 372.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 4.49s
                        Total time: 4241.74s
                               ETA: 13851.2s

################################################################################
                     [1m Learning iteration 938/4000 [0m

                       Computation: 1793 steps/s (collection: 0.646s, learning 3.921s)
               Value function loss: 219.8191
                    Surrogate loss: 0.0089
             Mean action noise std: 0.95
                       Mean reward: 586.43
               Mean episode length: 367.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7692288
                    Iteration time: 4.57s
                        Total time: 4246.30s
                               ETA: 13846.8s

################################################################################
                     [1m Learning iteration 939/4000 [0m

                       Computation: 1818 steps/s (collection: 0.591s, learning 3.913s)
               Value function loss: 164.2703
                    Surrogate loss: 0.0071
             Mean action noise std: 0.95
                       Mean reward: 569.22
               Mean episode length: 357.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 4.50s
                        Total time: 4250.81s
                               ETA: 13842.3s

################################################################################
                     [1m Learning iteration 940/4000 [0m

                       Computation: 1837 steps/s (collection: 0.546s, learning 3.912s)
               Value function loss: 237.2596
                    Surrogate loss: 0.0076
             Mean action noise std: 0.95
                       Mean reward: 567.77
               Mean episode length: 356.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7708672
                    Iteration time: 4.46s
                        Total time: 4255.27s
                               ETA: 13837.5s

################################################################################
                     [1m Learning iteration 941/4000 [0m

                       Computation: 1829 steps/s (collection: 0.551s, learning 3.927s)
               Value function loss: 178.2223
                    Surrogate loss: 0.0101
             Mean action noise std: 0.95
                       Mean reward: 572.55
               Mean episode length: 359.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 4.48s
                        Total time: 4259.74s
                               ETA: 13832.9s

################################################################################
                     [1m Learning iteration 942/4000 [0m

                       Computation: 1819 steps/s (collection: 0.576s, learning 3.927s)
               Value function loss: 119.0928
                    Surrogate loss: 0.0090
             Mean action noise std: 0.95
                       Mean reward: 550.85
               Mean episode length: 346.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7725056
                    Iteration time: 4.50s
                        Total time: 4264.25s
                               ETA: 13828.3s

################################################################################
                     [1m Learning iteration 943/4000 [0m

                       Computation: 1800 steps/s (collection: 0.638s, learning 3.911s)
               Value function loss: 130.0846
                    Surrogate loss: 0.0107
             Mean action noise std: 0.95
                       Mean reward: 549.16
               Mean episode length: 344.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 4.55s
                        Total time: 4268.80s
                               ETA: 13823.8s

################################################################################
                     [1m Learning iteration 944/4000 [0m

                       Computation: 1821 steps/s (collection: 0.549s, learning 3.947s)
               Value function loss: 292.4538
                    Surrogate loss: 0.0083
             Mean action noise std: 0.95
                       Mean reward: 561.24
               Mean episode length: 350.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7741440
                    Iteration time: 4.50s
                        Total time: 4273.29s
                               ETA: 13819.2s

################################################################################
                     [1m Learning iteration 945/4000 [0m

                       Computation: 1826 steps/s (collection: 0.596s, learning 3.888s)
               Value function loss: 122.6591
                    Surrogate loss: 0.0095
             Mean action noise std: 0.95
                       Mean reward: 574.05
               Mean episode length: 358.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 4.48s
                        Total time: 4277.78s
                               ETA: 13814.6s

################################################################################
                     [1m Learning iteration 946/4000 [0m

                       Computation: 1828 steps/s (collection: 0.594s, learning 3.886s)
               Value function loss: 213.1289
                    Surrogate loss: 0.0079
             Mean action noise std: 0.95
                       Mean reward: 568.01
               Mean episode length: 353.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7757824
                    Iteration time: 4.48s
                        Total time: 4282.26s
                               ETA: 13809.9s

################################################################################
                     [1m Learning iteration 947/4000 [0m

                       Computation: 1804 steps/s (collection: 0.623s, learning 3.917s)
               Value function loss: 133.9803
                    Surrogate loss: 0.0151
             Mean action noise std: 0.95
                       Mean reward: 574.42
               Mean episode length: 358.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 4.54s
                        Total time: 4286.80s
                               ETA: 13805.5s

################################################################################
                     [1m Learning iteration 948/4000 [0m

                       Computation: 1816 steps/s (collection: 0.574s, learning 3.936s)
               Value function loss: 283.6869
                    Surrogate loss: 0.0111
             Mean action noise std: 0.95
                       Mean reward: 580.45
               Mean episode length: 362.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7774208
                    Iteration time: 4.51s
                        Total time: 4291.31s
                               ETA: 13800.9s

################################################################################
                     [1m Learning iteration 949/4000 [0m

                       Computation: 1829 steps/s (collection: 0.566s, learning 3.913s)
               Value function loss: 241.6581
                    Surrogate loss: 0.0087
             Mean action noise std: 0.95
                       Mean reward: 592.60
               Mean episode length: 369.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 4.48s
                        Total time: 4295.79s
                               ETA: 13796.3s

################################################################################
                     [1m Learning iteration 950/4000 [0m

                       Computation: 1848 steps/s (collection: 0.530s, learning 3.902s)
               Value function loss: 189.2045
                    Surrogate loss: 0.0083
             Mean action noise std: 0.95
                       Mean reward: 593.36
               Mean episode length: 369.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7790592
                    Iteration time: 4.43s
                        Total time: 4300.22s
                               ETA: 13791.4s

################################################################################
                     [1m Learning iteration 951/4000 [0m

                       Computation: 1812 steps/s (collection: 0.571s, learning 3.950s)
               Value function loss: 232.0381
                    Surrogate loss: 0.0085
             Mean action noise std: 0.95
                       Mean reward: 573.81
               Mean episode length: 357.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 4.52s
                        Total time: 4304.74s
                               ETA: 13786.9s

################################################################################
                     [1m Learning iteration 952/4000 [0m

                       Computation: 1813 steps/s (collection: 0.572s, learning 3.945s)
               Value function loss: 232.1361
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 565.62
               Mean episode length: 352.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7806976
                    Iteration time: 4.52s
                        Total time: 4309.25s
                               ETA: 13782.4s

################################################################################
                     [1m Learning iteration 953/4000 [0m

                       Computation: 1810 steps/s (collection: 0.593s, learning 3.932s)
               Value function loss: 216.7462
                    Surrogate loss: 0.0123
             Mean action noise std: 0.94
                       Mean reward: 552.57
               Mean episode length: 345.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 4.53s
                        Total time: 4313.78s
                               ETA: 13777.9s

################################################################################
                     [1m Learning iteration 954/4000 [0m

                       Computation: 1818 steps/s (collection: 0.559s, learning 3.947s)
               Value function loss: 221.8520
                    Surrogate loss: 0.0125
             Mean action noise std: 0.94
                       Mean reward: 544.25
               Mean episode length: 341.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7823360
                    Iteration time: 4.51s
                        Total time: 4318.29s
                               ETA: 13773.3s

################################################################################
                     [1m Learning iteration 955/4000 [0m

                       Computation: 1788 steps/s (collection: 0.609s, learning 3.973s)
               Value function loss: 182.6052
                    Surrogate loss: 0.0098
             Mean action noise std: 0.94
                       Mean reward: 531.76
               Mean episode length: 335.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 4.58s
                        Total time: 4322.87s
                               ETA: 13769.0s

################################################################################
                     [1m Learning iteration 956/4000 [0m

                       Computation: 1832 steps/s (collection: 0.572s, learning 3.898s)
               Value function loss: 173.4967
                    Surrogate loss: 0.0117
             Mean action noise std: 0.94
                       Mean reward: 511.77
               Mean episode length: 323.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7839744
                    Iteration time: 4.47s
                        Total time: 4327.34s
                               ETA: 13764.3s

################################################################################
                     [1m Learning iteration 957/4000 [0m

                       Computation: 1807 steps/s (collection: 0.587s, learning 3.945s)
               Value function loss: 176.3672
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 509.61
               Mean episode length: 323.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 4.53s
                        Total time: 4331.87s
                               ETA: 13759.8s

################################################################################
                     [1m Learning iteration 958/4000 [0m

                       Computation: 1832 steps/s (collection: 0.576s, learning 3.895s)
               Value function loss: 162.9442
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 520.80
               Mean episode length: 330.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7856128
                    Iteration time: 4.47s
                        Total time: 4336.34s
                               ETA: 13755.1s

################################################################################
                     [1m Learning iteration 959/4000 [0m

                       Computation: 1798 steps/s (collection: 0.553s, learning 4.002s)
               Value function loss: 193.9307
                    Surrogate loss: 0.0115
             Mean action noise std: 0.94
                       Mean reward: 508.73
               Mean episode length: 324.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 4.55s
                        Total time: 4340.89s
                               ETA: 13750.7s

################################################################################
                     [1m Learning iteration 960/4000 [0m

                       Computation: 1800 steps/s (collection: 0.582s, learning 3.968s)
               Value function loss: 279.0612
                    Surrogate loss: 0.0096
             Mean action noise std: 0.94
                       Mean reward: 504.41
               Mean episode length: 322.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7872512
                    Iteration time: 4.55s
                        Total time: 4345.44s
                               ETA: 13746.3s

################################################################################
                     [1m Learning iteration 961/4000 [0m

                       Computation: 1782 steps/s (collection: 0.599s, learning 3.996s)
               Value function loss: 164.6289
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 497.82
               Mean episode length: 318.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 4.59s
                        Total time: 4350.04s
                               ETA: 13742.0s

################################################################################
                     [1m Learning iteration 962/4000 [0m

                       Computation: 1802 steps/s (collection: 0.551s, learning 3.993s)
               Value function loss: 165.7434
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 506.16
               Mean episode length: 323.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7888896
                    Iteration time: 4.54s
                        Total time: 4354.58s
                               ETA: 13737.5s

################################################################################
                     [1m Learning iteration 963/4000 [0m

                       Computation: 1798 steps/s (collection: 0.569s, learning 3.986s)
               Value function loss: 217.7642
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 507.40
               Mean episode length: 323.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 4.56s
                        Total time: 4359.14s
                               ETA: 13733.1s

################################################################################
                     [1m Learning iteration 964/4000 [0m

                       Computation: 1790 steps/s (collection: 0.573s, learning 4.003s)
               Value function loss: 214.0728
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 485.12
               Mean episode length: 307.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7905280
                    Iteration time: 4.58s
                        Total time: 4363.71s
                               ETA: 13728.7s

################################################################################
                     [1m Learning iteration 965/4000 [0m

                       Computation: 1783 steps/s (collection: 0.605s, learning 3.987s)
               Value function loss: 218.6487
                    Surrogate loss: 0.0108
             Mean action noise std: 0.94
                       Mean reward: 496.61
               Mean episode length: 316.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 4.59s
                        Total time: 4368.31s
                               ETA: 13724.4s

################################################################################
                     [1m Learning iteration 966/4000 [0m

                       Computation: 1801 steps/s (collection: 0.558s, learning 3.989s)
               Value function loss: 197.0002
                    Surrogate loss: 0.0094
             Mean action noise std: 0.94
                       Mean reward: 505.58
               Mean episode length: 322.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7921664
                    Iteration time: 4.55s
                        Total time: 4372.85s
                               ETA: 13720.0s

################################################################################
                     [1m Learning iteration 967/4000 [0m

                       Computation: 1830 steps/s (collection: 0.587s, learning 3.888s)
               Value function loss: 331.6272
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 538.33
               Mean episode length: 340.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 4.47s
                        Total time: 4377.33s
                               ETA: 13715.3s

################################################################################
                     [1m Learning iteration 968/4000 [0m

                       Computation: 1804 steps/s (collection: 0.600s, learning 3.940s)
               Value function loss: 237.9938
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 549.16
               Mean episode length: 347.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7938048
                    Iteration time: 4.54s
                        Total time: 4381.87s
                               ETA: 13710.9s

################################################################################
                     [1m Learning iteration 969/4000 [0m

                       Computation: 1845 steps/s (collection: 0.514s, learning 3.925s)
               Value function loss: 186.8317
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 562.85
               Mean episode length: 357.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 4.44s
                        Total time: 4386.31s
                               ETA: 13706.1s

################################################################################
                     [1m Learning iteration 970/4000 [0m

                       Computation: 1826 steps/s (collection: 0.552s, learning 3.933s)
               Value function loss: 204.0494
                    Surrogate loss: 0.0112
             Mean action noise std: 0.94
                       Mean reward: 561.21
               Mean episode length: 357.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7954432
                    Iteration time: 4.49s
                        Total time: 4390.79s
                               ETA: 13701.4s

################################################################################
                     [1m Learning iteration 971/4000 [0m

                       Computation: 1824 steps/s (collection: 0.575s, learning 3.916s)
               Value function loss: 186.5888
                    Surrogate loss: 0.0104
             Mean action noise std: 0.94
                       Mean reward: 580.10
               Mean episode length: 369.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 4.49s
                        Total time: 4395.28s
                               ETA: 13696.8s

################################################################################
                     [1m Learning iteration 972/4000 [0m

                       Computation: 1842 steps/s (collection: 0.528s, learning 3.918s)
               Value function loss: 215.2017
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 584.22
               Mean episode length: 372.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7970816
                    Iteration time: 4.45s
                        Total time: 4399.73s
                               ETA: 13692.1s

################################################################################
                     [1m Learning iteration 973/4000 [0m

                       Computation: 1807 steps/s (collection: 0.584s, learning 3.947s)
               Value function loss: 118.5441
                    Surrogate loss: 0.0116
             Mean action noise std: 0.94
                       Mean reward: 606.44
               Mean episode length: 385.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 4.53s
                        Total time: 4404.26s
                               ETA: 13687.6s

################################################################################
                     [1m Learning iteration 974/4000 [0m

                       Computation: 1789 steps/s (collection: 0.609s, learning 3.969s)
               Value function loss: 208.1389
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 606.45
               Mean episode length: 386.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7987200
                    Iteration time: 4.58s
                        Total time: 4408.84s
                               ETA: 13683.2s

################################################################################
                     [1m Learning iteration 975/4000 [0m

                       Computation: 1837 steps/s (collection: 0.530s, learning 3.928s)
               Value function loss: 206.2848
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 601.79
               Mean episode length: 382.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 4.46s
                        Total time: 4413.30s
                               ETA: 13678.5s

################################################################################
                     [1m Learning iteration 976/4000 [0m

                       Computation: 1856 steps/s (collection: 0.533s, learning 3.880s)
               Value function loss: 137.7187
                    Surrogate loss: 0.0209
             Mean action noise std: 0.94
                       Mean reward: 600.05
               Mean episode length: 382.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8003584
                    Iteration time: 4.41s
                        Total time: 4417.71s
                               ETA: 13673.6s

################################################################################
                     [1m Learning iteration 977/4000 [0m

                       Computation: 1837 steps/s (collection: 0.523s, learning 3.936s)
               Value function loss: 200.4599
                    Surrogate loss: 0.0175
             Mean action noise std: 0.94
                       Mean reward: 583.71
               Mean episode length: 370.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 4.46s
                        Total time: 4422.17s
                               ETA: 13668.9s

################################################################################
                     [1m Learning iteration 978/4000 [0m

                       Computation: 1831 steps/s (collection: 0.571s, learning 3.902s)
               Value function loss: 200.9547
                    Surrogate loss: 0.0103
             Mean action noise std: 0.94
                       Mean reward: 548.39
               Mean episode length: 347.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8019968
                    Iteration time: 4.47s
                        Total time: 4426.64s
                               ETA: 13664.3s

################################################################################
                     [1m Learning iteration 979/4000 [0m

                       Computation: 1858 steps/s (collection: 0.537s, learning 3.872s)
               Value function loss: 206.6987
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 540.02
               Mean episode length: 340.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 4.41s
                        Total time: 4431.05s
                               ETA: 13659.4s

################################################################################
                     [1m Learning iteration 980/4000 [0m

                       Computation: 1832 steps/s (collection: 0.573s, learning 3.898s)
               Value function loss: 229.0695
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 535.20
               Mean episode length: 335.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8036352
                    Iteration time: 4.47s
                        Total time: 4435.52s
                               ETA: 13654.7s

################################################################################
                     [1m Learning iteration 981/4000 [0m

                       Computation: 1830 steps/s (collection: 0.539s, learning 3.936s)
               Value function loss: 209.4943
                    Surrogate loss: 0.0113
             Mean action noise std: 0.94
                       Mean reward: 524.72
               Mean episode length: 329.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 4.47s
                        Total time: 4440.00s
                               ETA: 13650.0s

################################################################################
                     [1m Learning iteration 982/4000 [0m

                       Computation: 1829 steps/s (collection: 0.558s, learning 3.921s)
               Value function loss: 213.7653
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 511.87
               Mean episode length: 320.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8052736
                    Iteration time: 4.48s
                        Total time: 4444.47s
                               ETA: 13645.4s

################################################################################
                     [1m Learning iteration 983/4000 [0m

                       Computation: 1839 steps/s (collection: 0.579s, learning 3.875s)
               Value function loss: 254.0371
                    Surrogate loss: 0.0080
             Mean action noise std: 0.94
                       Mean reward: 520.65
               Mean episode length: 327.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 4.45s
                        Total time: 4448.93s
                               ETA: 13640.7s

################################################################################
                     [1m Learning iteration 984/4000 [0m

                       Computation: 1834 steps/s (collection: 0.576s, learning 3.890s)
               Value function loss: 265.4840
                    Surrogate loss: 0.0108
             Mean action noise std: 0.94
                       Mean reward: 515.14
               Mean episode length: 324.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8069120
                    Iteration time: 4.47s
                        Total time: 4453.39s
                               ETA: 13636.0s

################################################################################
                     [1m Learning iteration 985/4000 [0m

                       Computation: 1829 steps/s (collection: 0.563s, learning 3.915s)
               Value function loss: 231.5564
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 533.95
               Mean episode length: 335.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 4.48s
                        Total time: 4457.87s
                               ETA: 13631.3s

################################################################################
                     [1m Learning iteration 986/4000 [0m

                       Computation: 1830 steps/s (collection: 0.562s, learning 3.913s)
               Value function loss: 189.4435
                    Surrogate loss: 0.0070
             Mean action noise std: 0.94
                       Mean reward: 537.68
               Mean episode length: 338.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8085504
                    Iteration time: 4.48s
                        Total time: 4462.35s
                               ETA: 13626.7s

################################################################################
                     [1m Learning iteration 987/4000 [0m

                       Computation: 1797 steps/s (collection: 0.567s, learning 3.990s)
               Value function loss: 147.4111
                    Surrogate loss: 0.0076
             Mean action noise std: 0.94
                       Mean reward: 538.79
               Mean episode length: 340.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 4.56s
                        Total time: 4466.90s
                               ETA: 13622.2s

################################################################################
                     [1m Learning iteration 988/4000 [0m

                       Computation: 1811 steps/s (collection: 0.541s, learning 3.982s)
               Value function loss: 206.5907
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 535.56
               Mean episode length: 338.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8101888
                    Iteration time: 4.52s
                        Total time: 4471.43s
                               ETA: 13617.7s

################################################################################
                     [1m Learning iteration 989/4000 [0m

                       Computation: 1815 steps/s (collection: 0.542s, learning 3.969s)
               Value function loss: 144.9213
                    Surrogate loss: 0.0091
             Mean action noise std: 0.94
                       Mean reward: 517.29
               Mean episode length: 328.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 4.51s
                        Total time: 4475.94s
                               ETA: 13613.2s

################################################################################
                     [1m Learning iteration 990/4000 [0m

                       Computation: 1829 steps/s (collection: 0.542s, learning 3.936s)
               Value function loss: 165.6716
                    Surrogate loss: 0.0080
             Mean action noise std: 0.94
                       Mean reward: 535.37
               Mean episode length: 340.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8118272
                    Iteration time: 4.48s
                        Total time: 4480.42s
                               ETA: 13608.5s

################################################################################
                     [1m Learning iteration 991/4000 [0m

                       Computation: 1811 steps/s (collection: 0.547s, learning 3.975s)
               Value function loss: 190.8607
                    Surrogate loss: 0.0069
             Mean action noise std: 0.94
                       Mean reward: 539.06
               Mean episode length: 343.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 4.52s
                        Total time: 4484.94s
                               ETA: 13604.0s

################################################################################
                     [1m Learning iteration 992/4000 [0m

                       Computation: 1801 steps/s (collection: 0.542s, learning 4.007s)
               Value function loss: 152.8232
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 534.59
               Mean episode length: 342.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8134656
                    Iteration time: 4.55s
                        Total time: 4489.49s
                               ETA: 13599.6s

################################################################################
                     [1m Learning iteration 993/4000 [0m

                       Computation: 1819 steps/s (collection: 0.570s, learning 3.932s)
               Value function loss: 198.5298
                    Surrogate loss: 0.0077
             Mean action noise std: 0.94
                       Mean reward: 550.83
               Mean episode length: 351.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 4.50s
                        Total time: 4493.99s
                               ETA: 13595.0s

################################################################################
                     [1m Learning iteration 994/4000 [0m

                       Computation: 1805 steps/s (collection: 0.595s, learning 3.942s)
               Value function loss: 203.5873
                    Surrogate loss: 0.0084
             Mean action noise std: 0.94
                       Mean reward: 534.16
               Mean episode length: 343.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8151040
                    Iteration time: 4.54s
                        Total time: 4498.52s
                               ETA: 13590.5s

################################################################################
                     [1m Learning iteration 995/4000 [0m

                       Computation: 1823 steps/s (collection: 0.562s, learning 3.930s)
               Value function loss: 232.4786
                    Surrogate loss: 0.0101
             Mean action noise std: 0.94
                       Mean reward: 547.40
               Mean episode length: 350.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 4.49s
                        Total time: 4503.02s
                               ETA: 13585.9s

################################################################################
                     [1m Learning iteration 996/4000 [0m

                       Computation: 1807 steps/s (collection: 0.566s, learning 3.966s)
               Value function loss: 253.2773
                    Surrogate loss: 0.0105
             Mean action noise std: 0.94
                       Mean reward: 552.69
               Mean episode length: 353.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8167424
                    Iteration time: 4.53s
                        Total time: 4507.55s
                               ETA: 13581.4s

################################################################################
                     [1m Learning iteration 997/4000 [0m

                       Computation: 1852 steps/s (collection: 0.533s, learning 3.890s)
               Value function loss: 180.4359
                    Surrogate loss: 0.0102
             Mean action noise std: 0.94
                       Mean reward: 567.74
               Mean episode length: 362.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 4.42s
                        Total time: 4511.97s
                               ETA: 13576.6s

################################################################################
                     [1m Learning iteration 998/4000 [0m

                       Computation: 1797 steps/s (collection: 0.626s, learning 3.932s)
               Value function loss: 296.4885
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 591.00
               Mean episode length: 379.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8183808
                    Iteration time: 4.56s
                        Total time: 4516.53s
                               ETA: 13572.2s

################################################################################
                     [1m Learning iteration 999/4000 [0m

                       Computation: 1821 steps/s (collection: 0.584s, learning 3.915s)
               Value function loss: 155.1273
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 589.83
               Mean episode length: 377.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 4.50s
                        Total time: 4521.03s
                               ETA: 13567.6s

################################################################################
                     [1m Learning iteration 1000/4000 [0m

                       Computation: 1825 steps/s (collection: 0.548s, learning 3.939s)
               Value function loss: 245.5521
                    Surrogate loss: 0.0104
             Mean action noise std: 0.94
                       Mean reward: 604.55
               Mean episode length: 383.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8200192
                    Iteration time: 4.49s
                        Total time: 4525.51s
                               ETA: 13563.0s

################################################################################
                     [1m Learning iteration 1001/4000 [0m

                       Computation: 1829 steps/s (collection: 0.572s, learning 3.906s)
               Value function loss: 230.9584
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 601.98
               Mean episode length: 382.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 4.48s
                        Total time: 4529.99s
                               ETA: 13558.3s

################################################################################
                     [1m Learning iteration 1002/4000 [0m

                       Computation: 1836 steps/s (collection: 0.550s, learning 3.910s)
               Value function loss: 191.8221
                    Surrogate loss: 0.0076
             Mean action noise std: 0.94
                       Mean reward: 614.51
               Mean episode length: 389.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8216576
                    Iteration time: 4.46s
                        Total time: 4534.45s
                               ETA: 13553.6s

################################################################################
                     [1m Learning iteration 1003/4000 [0m

                       Computation: 1836 steps/s (collection: 0.559s, learning 3.902s)
               Value function loss: 170.4604
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 630.94
               Mean episode length: 399.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 4.46s
                        Total time: 4538.91s
                               ETA: 13548.9s

################################################################################
                     [1m Learning iteration 1004/4000 [0m

                       Computation: 1826 steps/s (collection: 0.562s, learning 3.922s)
               Value function loss: 181.1933
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 635.70
               Mean episode length: 403.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8232960
                    Iteration time: 4.48s
                        Total time: 4543.40s
                               ETA: 13544.3s

################################################################################
                     [1m Learning iteration 1005/4000 [0m

                       Computation: 1823 steps/s (collection: 0.545s, learning 3.949s)
               Value function loss: 178.5805
                    Surrogate loss: 0.0111
             Mean action noise std: 0.94
                       Mean reward: 621.27
               Mean episode length: 394.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 4.49s
                        Total time: 4547.89s
                               ETA: 13539.7s

################################################################################
                     [1m Learning iteration 1006/4000 [0m

                       Computation: 1796 steps/s (collection: 0.578s, learning 3.981s)
               Value function loss: 115.5118
                    Surrogate loss: 0.0117
             Mean action noise std: 0.94
                       Mean reward: 621.07
               Mean episode length: 394.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8249344
                    Iteration time: 4.56s
                        Total time: 4552.45s
                               ETA: 13535.3s

################################################################################
                     [1m Learning iteration 1007/4000 [0m

                       Computation: 1831 steps/s (collection: 0.567s, learning 3.905s)
               Value function loss: 173.8352
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 622.01
               Mean episode length: 395.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 4.47s
                        Total time: 4556.92s
                               ETA: 13530.6s

################################################################################
                     [1m Learning iteration 1008/4000 [0m

                       Computation: 1842 steps/s (collection: 0.539s, learning 3.907s)
               Value function loss: 140.1610
                    Surrogate loss: 0.0102
             Mean action noise std: 0.94
                       Mean reward: 611.56
               Mean episode length: 389.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8265728
                    Iteration time: 4.45s
                        Total time: 4561.37s
                               ETA: 13525.9s

################################################################################
                     [1m Learning iteration 1009/4000 [0m

                       Computation: 1805 steps/s (collection: 0.615s, learning 3.923s)
               Value function loss: 256.4804
                    Surrogate loss: 0.0084
             Mean action noise std: 0.94
                       Mean reward: 610.61
               Mean episode length: 385.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 4.54s
                        Total time: 4565.91s
                               ETA: 13521.4s

################################################################################
                     [1m Learning iteration 1010/4000 [0m

                       Computation: 1829 steps/s (collection: 0.567s, learning 3.911s)
               Value function loss: 196.2145
                    Surrogate loss: 0.0111
             Mean action noise std: 0.94
                       Mean reward: 592.15
               Mean episode length: 375.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8282112
                    Iteration time: 4.48s
                        Total time: 4570.38s
                               ETA: 13516.8s

################################################################################
                     [1m Learning iteration 1011/4000 [0m

                       Computation: 1793 steps/s (collection: 0.636s, learning 3.932s)
               Value function loss: 274.2059
                    Surrogate loss: 0.0069
             Mean action noise std: 0.94
                       Mean reward: 607.87
               Mean episode length: 382.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 4.57s
                        Total time: 4574.95s
                               ETA: 13512.4s

################################################################################
                     [1m Learning iteration 1012/4000 [0m

                       Computation: 1849 steps/s (collection: 0.528s, learning 3.901s)
               Value function loss: 191.9315
                    Surrogate loss: 0.0114
             Mean action noise std: 0.94
                       Mean reward: 614.36
               Mean episode length: 387.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8298496
                    Iteration time: 4.43s
                        Total time: 4579.38s
                               ETA: 13507.6s

################################################################################
                     [1m Learning iteration 1013/4000 [0m

                       Computation: 1778 steps/s (collection: 0.653s, learning 3.952s)
               Value function loss: 156.8266
                    Surrogate loss: 0.0110
             Mean action noise std: 0.94
                       Mean reward: 607.09
               Mean episode length: 381.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 4.61s
                        Total time: 4583.99s
                               ETA: 13503.3s

################################################################################
                     [1m Learning iteration 1014/4000 [0m

                       Computation: 1817 steps/s (collection: 0.540s, learning 3.966s)
               Value function loss: 221.5321
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 615.68
               Mean episode length: 385.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8314880
                    Iteration time: 4.51s
                        Total time: 4588.49s
                               ETA: 13498.8s

################################################################################
                     [1m Learning iteration 1015/4000 [0m

                       Computation: 1832 steps/s (collection: 0.550s, learning 3.920s)
               Value function loss: 236.5526
                    Surrogate loss: 0.0076
             Mean action noise std: 0.94
                       Mean reward: 630.04
               Mean episode length: 392.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 4.47s
                        Total time: 4592.96s
                               ETA: 13494.1s

################################################################################
                     [1m Learning iteration 1016/4000 [0m

                       Computation: 1814 steps/s (collection: 0.554s, learning 3.962s)
               Value function loss: 182.6952
                    Surrogate loss: 0.0081
             Mean action noise std: 0.94
                       Mean reward: 632.59
               Mean episode length: 392.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8331264
                    Iteration time: 4.52s
                        Total time: 4597.48s
                               ETA: 13489.6s

################################################################################
                     [1m Learning iteration 1017/4000 [0m

                       Computation: 1812 steps/s (collection: 0.552s, learning 3.968s)
               Value function loss: 267.2046
                    Surrogate loss: 0.0074
             Mean action noise std: 0.94
                       Mean reward: 638.35
               Mean episode length: 393.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 4.52s
                        Total time: 4602.00s
                               ETA: 13485.0s

################################################################################
                     [1m Learning iteration 1018/4000 [0m

                       Computation: 1831 steps/s (collection: 0.537s, learning 3.937s)
               Value function loss: 155.2035
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 630.43
               Mean episode length: 389.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8347648
                    Iteration time: 4.47s
                        Total time: 4606.47s
                               ETA: 13480.4s

################################################################################
                     [1m Learning iteration 1019/4000 [0m

                       Computation: 1800 steps/s (collection: 0.573s, learning 3.976s)
               Value function loss: 142.8994
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 643.22
               Mean episode length: 394.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 4.55s
                        Total time: 4611.02s
                               ETA: 13475.9s

################################################################################
                     [1m Learning iteration 1020/4000 [0m

                       Computation: 1812 steps/s (collection: 0.554s, learning 3.965s)
               Value function loss: 203.9742
                    Surrogate loss: 0.0122
             Mean action noise std: 0.94
                       Mean reward: 640.70
               Mean episode length: 392.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8364032
                    Iteration time: 4.52s
                        Total time: 4615.54s
                               ETA: 13471.4s

################################################################################
                     [1m Learning iteration 1021/4000 [0m

                       Computation: 1812 steps/s (collection: 0.563s, learning 3.957s)
               Value function loss: 182.5111
                    Surrogate loss: 0.0110
             Mean action noise std: 0.94
                       Mean reward: 630.07
               Mean episode length: 383.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 4.52s
                        Total time: 4620.06s
                               ETA: 13466.9s

################################################################################
                     [1m Learning iteration 1022/4000 [0m

                       Computation: 1798 steps/s (collection: 0.586s, learning 3.968s)
               Value function loss: 147.1902
                    Surrogate loss: 0.0134
             Mean action noise std: 0.94
                       Mean reward: 611.03
               Mean episode length: 372.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8380416
                    Iteration time: 4.55s
                        Total time: 4624.61s
                               ETA: 13462.5s

################################################################################
                     [1m Learning iteration 1023/4000 [0m

                       Computation: 1794 steps/s (collection: 0.593s, learning 3.973s)
               Value function loss: 166.8813
                    Surrogate loss: 0.0133
             Mean action noise std: 0.94
                       Mean reward: 574.15
               Mean episode length: 349.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 4.57s
                        Total time: 4629.18s
                               ETA: 13458.1s

################################################################################
                     [1m Learning iteration 1024/4000 [0m

                       Computation: 1848 steps/s (collection: 0.544s, learning 3.888s)
               Value function loss: 172.3382
                    Surrogate loss: 0.0117
             Mean action noise std: 0.94
                       Mean reward: 579.55
               Mean episode length: 352.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8396800
                    Iteration time: 4.43s
                        Total time: 4633.61s
                               ETA: 13453.3s

################################################################################
                     [1m Learning iteration 1025/4000 [0m

                       Computation: 1837 steps/s (collection: 0.510s, learning 3.948s)
               Value function loss: 264.7191
                    Surrogate loss: 0.0105
             Mean action noise std: 0.94
                       Mean reward: 567.53
               Mean episode length: 344.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 4.46s
                        Total time: 4638.07s
                               ETA: 13448.6s

################################################################################
                     [1m Learning iteration 1026/4000 [0m

                       Computation: 1796 steps/s (collection: 0.592s, learning 3.969s)
               Value function loss: 192.8219
                    Surrogate loss: 0.0094
             Mean action noise std: 0.94
                       Mean reward: 565.02
               Mean episode length: 344.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8413184
                    Iteration time: 4.56s
                        Total time: 4642.63s
                               ETA: 13444.2s

################################################################################
                     [1m Learning iteration 1027/4000 [0m

                       Computation: 1832 steps/s (collection: 0.530s, learning 3.941s)
               Value function loss: 214.1257
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 585.30
               Mean episode length: 355.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 4.47s
                        Total time: 4647.10s
                               ETA: 13439.5s

################################################################################
                     [1m Learning iteration 1028/4000 [0m

                       Computation: 1822 steps/s (collection: 0.592s, learning 3.902s)
               Value function loss: 237.4485
                    Surrogate loss: 0.0108
             Mean action noise std: 0.94
                       Mean reward: 563.61
               Mean episode length: 340.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8429568
                    Iteration time: 4.49s
                        Total time: 4651.60s
                               ETA: 13434.9s

################################################################################
                     [1m Learning iteration 1029/4000 [0m

                       Computation: 1800 steps/s (collection: 0.577s, learning 3.973s)
               Value function loss: 256.0212
                    Surrogate loss: 0.0146
             Mean action noise std: 0.94
                       Mean reward: 547.31
               Mean episode length: 332.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 4.55s
                        Total time: 4656.15s
                               ETA: 13430.5s

################################################################################
                     [1m Learning iteration 1030/4000 [0m

                       Computation: 1813 steps/s (collection: 0.625s, learning 3.893s)
               Value function loss: 211.2716
                    Surrogate loss: 0.0070
             Mean action noise std: 0.94
                       Mean reward: 556.65
               Mean episode length: 337.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8445952
                    Iteration time: 4.52s
                        Total time: 4660.66s
                               ETA: 13426.0s

################################################################################
                     [1m Learning iteration 1031/4000 [0m

                       Computation: 1818 steps/s (collection: 0.571s, learning 3.935s)
               Value function loss: 167.1812
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 574.76
               Mean episode length: 347.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 4.51s
                        Total time: 4665.17s
                               ETA: 13421.4s

################################################################################
                     [1m Learning iteration 1032/4000 [0m

                       Computation: 1808 steps/s (collection: 0.614s, learning 3.914s)
               Value function loss: 265.2369
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 559.13
               Mean episode length: 337.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8462336
                    Iteration time: 4.53s
                        Total time: 4669.70s
                               ETA: 13416.9s

################################################################################
                     [1m Learning iteration 1033/4000 [0m

                       Computation: 1815 steps/s (collection: 0.576s, learning 3.937s)
               Value function loss: 214.9709
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 573.19
               Mean episode length: 345.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 4.51s
                        Total time: 4674.21s
                               ETA: 13412.4s

################################################################################
                     [1m Learning iteration 1034/4000 [0m

                       Computation: 1800 steps/s (collection: 0.585s, learning 3.966s)
               Value function loss: 154.3339
                    Surrogate loss: 0.0142
             Mean action noise std: 0.94
                       Mean reward: 560.48
               Mean episode length: 336.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8478720
                    Iteration time: 4.55s
                        Total time: 4678.76s
                               ETA: 13407.9s

################################################################################
                     [1m Learning iteration 1035/4000 [0m

                       Computation: 1805 steps/s (collection: 0.594s, learning 3.944s)
               Value function loss: 173.5647
                    Surrogate loss: 0.0132
             Mean action noise std: 0.94
                       Mean reward: 550.03
               Mean episode length: 328.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 4.54s
                        Total time: 4683.30s
                               ETA: 13403.5s

################################################################################
                     [1m Learning iteration 1036/4000 [0m

                       Computation: 1829 steps/s (collection: 0.556s, learning 3.921s)
               Value function loss: 181.7209
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 557.81
               Mean episode length: 333.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8495104
                    Iteration time: 4.48s
                        Total time: 4687.78s
                               ETA: 13398.8s

################################################################################
                     [1m Learning iteration 1037/4000 [0m

                       Computation: 1821 steps/s (collection: 0.555s, learning 3.942s)
               Value function loss: 188.0537
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 568.88
               Mean episode length: 341.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 4.50s
                        Total time: 4692.27s
                               ETA: 13394.2s

################################################################################
                     [1m Learning iteration 1038/4000 [0m

                       Computation: 1810 steps/s (collection: 0.597s, learning 3.927s)
               Value function loss: 195.2172
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 572.18
               Mean episode length: 341.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8511488
                    Iteration time: 4.52s
                        Total time: 4696.80s
                               ETA: 13389.7s

################################################################################
                     [1m Learning iteration 1039/4000 [0m

                       Computation: 1811 steps/s (collection: 0.567s, learning 3.956s)
               Value function loss: 221.7853
                    Surrogate loss: 0.0127
             Mean action noise std: 0.94
                       Mean reward: 568.91
               Mean episode length: 339.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 4.52s
                        Total time: 4701.32s
                               ETA: 13385.2s

################################################################################
                     [1m Learning iteration 1040/4000 [0m

                       Computation: 1811 steps/s (collection: 0.579s, learning 3.942s)
               Value function loss: 232.5664
                    Surrogate loss: 0.0136
             Mean action noise std: 0.94
                       Mean reward: 592.99
               Mean episode length: 353.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8527872
                    Iteration time: 4.52s
                        Total time: 4705.84s
                               ETA: 13380.7s

################################################################################
                     [1m Learning iteration 1041/4000 [0m

                       Computation: 1826 steps/s (collection: 0.545s, learning 3.939s)
               Value function loss: 230.7951
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 590.85
               Mean episode length: 355.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 4.48s
                        Total time: 4710.33s
                               ETA: 13376.1s

################################################################################
                     [1m Learning iteration 1042/4000 [0m

                       Computation: 1811 steps/s (collection: 0.550s, learning 3.972s)
               Value function loss: 180.5577
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 573.36
               Mean episode length: 345.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8544256
                    Iteration time: 4.52s
                        Total time: 4714.85s
                               ETA: 13371.5s

################################################################################
                     [1m Learning iteration 1043/4000 [0m

                       Computation: 1808 steps/s (collection: 0.561s, learning 3.968s)
               Value function loss: 173.0152
                    Surrogate loss: 0.0121
             Mean action noise std: 0.94
                       Mean reward: 590.69
               Mean episode length: 356.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 4.53s
                        Total time: 4719.38s
                               ETA: 13367.1s

################################################################################
                     [1m Learning iteration 1044/4000 [0m

                       Computation: 1797 steps/s (collection: 0.586s, learning 3.973s)
               Value function loss: 287.1502
                    Surrogate loss: 0.0104
             Mean action noise std: 0.94
                       Mean reward: 621.56
               Mean episode length: 374.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8560640
                    Iteration time: 4.56s
                        Total time: 4723.94s
                               ETA: 13362.6s

################################################################################
                     [1m Learning iteration 1045/4000 [0m

                       Computation: 1802 steps/s (collection: 0.598s, learning 3.947s)
               Value function loss: 316.8699
                    Surrogate loss: 0.0101
             Mean action noise std: 0.94
                       Mean reward: 627.89
               Mean episode length: 377.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 4.54s
                        Total time: 4728.48s
                               ETA: 13358.2s

################################################################################
                     [1m Learning iteration 1046/4000 [0m

                       Computation: 1805 steps/s (collection: 0.573s, learning 3.965s)
               Value function loss: 186.8042
                    Surrogate loss: 0.0108
             Mean action noise std: 0.94
                       Mean reward: 639.40
               Mean episode length: 382.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8577024
                    Iteration time: 4.54s
                        Total time: 4733.02s
                               ETA: 13353.7s

################################################################################
                     [1m Learning iteration 1047/4000 [0m

                       Computation: 1816 steps/s (collection: 0.582s, learning 3.927s)
               Value function loss: 84.7732
                    Surrogate loss: 0.0184
             Mean action noise std: 0.94
                       Mean reward: 650.78
               Mean episode length: 390.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 4.51s
                        Total time: 4737.53s
                               ETA: 13349.2s

################################################################################
                     [1m Learning iteration 1048/4000 [0m

                       Computation: 1832 steps/s (collection: 0.522s, learning 3.948s)
               Value function loss: 278.9914
                    Surrogate loss: 0.0102
             Mean action noise std: 0.94
                       Mean reward: 689.05
               Mean episode length: 410.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8593408
                    Iteration time: 4.47s
                        Total time: 4742.00s
                               ETA: 13344.5s

################################################################################
                     [1m Learning iteration 1049/4000 [0m

                       Computation: 1841 steps/s (collection: 0.583s, learning 3.865s)
               Value function loss: 251.7346
                    Surrogate loss: 0.0071
             Mean action noise std: 0.94
                       Mean reward: 685.28
               Mean episode length: 408.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 4.45s
                        Total time: 4746.45s
                               ETA: 13339.8s

################################################################################
                     [1m Learning iteration 1050/4000 [0m

                       Computation: 1844 steps/s (collection: 0.545s, learning 3.896s)
               Value function loss: 166.4990
                    Surrogate loss: 0.0096
             Mean action noise std: 0.94
                       Mean reward: 676.03
               Mean episode length: 401.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8609792
                    Iteration time: 4.44s
                        Total time: 4750.89s
                               ETA: 13335.0s

################################################################################
                     [1m Learning iteration 1051/4000 [0m

                       Computation: 1838 steps/s (collection: 0.573s, learning 3.884s)
               Value function loss: 160.7315
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 666.22
               Mean episode length: 394.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 4.46s
                        Total time: 4755.34s
                               ETA: 13330.3s

################################################################################
                     [1m Learning iteration 1052/4000 [0m

                       Computation: 1850 steps/s (collection: 0.518s, learning 3.910s)
               Value function loss: 319.1533
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 651.97
               Mean episode length: 385.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8626176
                    Iteration time: 4.43s
                        Total time: 4759.77s
                               ETA: 13325.6s

################################################################################
                     [1m Learning iteration 1053/4000 [0m

                       Computation: 1843 steps/s (collection: 0.548s, learning 3.895s)
               Value function loss: 166.6498
                    Surrogate loss: 0.0111
             Mean action noise std: 0.94
                       Mean reward: 639.87
               Mean episode length: 377.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 4.44s
                        Total time: 4764.21s
                               ETA: 13320.8s

################################################################################
                     [1m Learning iteration 1054/4000 [0m

                       Computation: 1818 steps/s (collection: 0.530s, learning 3.976s)
               Value function loss: 236.7440
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 630.33
               Mean episode length: 374.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8642560
                    Iteration time: 4.51s
                        Total time: 4768.72s
                               ETA: 13316.3s

################################################################################
                     [1m Learning iteration 1055/4000 [0m

                       Computation: 1835 steps/s (collection: 0.557s, learning 3.905s)
               Value function loss: 146.6468
                    Surrogate loss: 0.0112
             Mean action noise std: 0.94
                       Mean reward: 610.72
               Mean episode length: 364.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 4.46s
                        Total time: 4773.18s
                               ETA: 13311.6s

################################################################################
                     [1m Learning iteration 1056/4000 [0m

                       Computation: 1833 steps/s (collection: 0.547s, learning 3.922s)
               Value function loss: 280.0011
                    Surrogate loss: 0.0111
             Mean action noise std: 0.94
                       Mean reward: 580.07
               Mean episode length: 348.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8658944
                    Iteration time: 4.47s
                        Total time: 4777.65s
                               ETA: 13306.9s

################################################################################
                     [1m Learning iteration 1057/4000 [0m

                       Computation: 1839 steps/s (collection: 0.549s, learning 3.903s)
               Value function loss: 243.6500
                    Surrogate loss: 0.0142
             Mean action noise std: 0.94
                       Mean reward: 574.71
               Mean episode length: 346.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 4.45s
                        Total time: 4782.10s
                               ETA: 13302.2s

################################################################################
                     [1m Learning iteration 1058/4000 [0m

                       Computation: 1850 steps/s (collection: 0.514s, learning 3.913s)
               Value function loss: 198.5720
                    Surrogate loss: 0.0147
             Mean action noise std: 0.94
                       Mean reward: 577.36
               Mean episode length: 348.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8675328
                    Iteration time: 4.43s
                        Total time: 4786.53s
                               ETA: 13297.4s

################################################################################
                     [1m Learning iteration 1059/4000 [0m

                       Computation: 1851 steps/s (collection: 0.524s, learning 3.900s)
               Value function loss: 244.3683
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 575.34
               Mean episode length: 348.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 4.42s
                        Total time: 4790.95s
                               ETA: 13292.6s

################################################################################
                     [1m Learning iteration 1060/4000 [0m

                       Computation: 1837 steps/s (collection: 0.554s, learning 3.904s)
               Value function loss: 296.5887
                    Surrogate loss: 0.0080
             Mean action noise std: 0.94
                       Mean reward: 589.65
               Mean episode length: 357.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8691712
                    Iteration time: 4.46s
                        Total time: 4795.41s
                               ETA: 13287.9s

################################################################################
                     [1m Learning iteration 1061/4000 [0m

                       Computation: 1845 steps/s (collection: 0.551s, learning 3.888s)
               Value function loss: 265.7798
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 596.43
               Mean episode length: 362.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 4.44s
                        Total time: 4799.85s
                               ETA: 13283.2s

################################################################################
                     [1m Learning iteration 1062/4000 [0m

                       Computation: 1805 steps/s (collection: 0.611s, learning 3.927s)
               Value function loss: 223.1855
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 576.61
               Mean episode length: 350.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8708096
                    Iteration time: 4.54s
                        Total time: 4804.39s
                               ETA: 13278.7s

################################################################################
                     [1m Learning iteration 1063/4000 [0m

                       Computation: 1827 steps/s (collection: 0.574s, learning 3.910s)
               Value function loss: 221.5734
                    Surrogate loss: 0.0106
             Mean action noise std: 0.94
                       Mean reward: 573.73
               Mean episode length: 350.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 4.48s
                        Total time: 4808.87s
                               ETA: 13274.1s

################################################################################
                     [1m Learning iteration 1064/4000 [0m

                       Computation: 1805 steps/s (collection: 0.598s, learning 3.939s)
               Value function loss: 192.9418
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 582.80
               Mean episode length: 354.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8724480
                    Iteration time: 4.54s
                        Total time: 4813.41s
                               ETA: 13269.6s

################################################################################
                     [1m Learning iteration 1065/4000 [0m

                       Computation: 1831 steps/s (collection: 0.586s, learning 3.887s)
               Value function loss: 205.5747
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 582.33
               Mean episode length: 352.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 4.47s
                        Total time: 4817.88s
                               ETA: 13265.0s

################################################################################
                     [1m Learning iteration 1066/4000 [0m

                       Computation: 1817 steps/s (collection: 0.591s, learning 3.916s)
               Value function loss: 198.2916
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 572.27
               Mean episode length: 347.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8740864
                    Iteration time: 4.51s
                        Total time: 4822.39s
                               ETA: 13260.4s

################################################################################
                     [1m Learning iteration 1067/4000 [0m

                       Computation: 1832 steps/s (collection: 0.545s, learning 3.925s)
               Value function loss: 175.7376
                    Surrogate loss: 0.0096
             Mean action noise std: 0.94
                       Mean reward: 571.09
               Mean episode length: 346.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 4.47s
                        Total time: 4826.86s
                               ETA: 13255.8s

################################################################################
                     [1m Learning iteration 1068/4000 [0m

                       Computation: 1825 steps/s (collection: 0.543s, learning 3.944s)
               Value function loss: 251.2752
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 583.05
               Mean episode length: 355.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8757248
                    Iteration time: 4.49s
                        Total time: 4831.35s
                               ETA: 13251.2s

################################################################################
                     [1m Learning iteration 1069/4000 [0m

                       Computation: 1801 steps/s (collection: 0.546s, learning 4.001s)
               Value function loss: 236.5557
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 590.08
               Mean episode length: 361.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 4.55s
                        Total time: 4835.89s
                               ETA: 13246.7s

################################################################################
                     [1m Learning iteration 1070/4000 [0m

                       Computation: 1793 steps/s (collection: 0.546s, learning 4.023s)
               Value function loss: 221.9036
                    Surrogate loss: 0.0065
             Mean action noise std: 0.94
                       Mean reward: 575.96
               Mean episode length: 350.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8773632
                    Iteration time: 4.57s
                        Total time: 4840.46s
                               ETA: 13242.3s

################################################################################
                     [1m Learning iteration 1071/4000 [0m

                       Computation: 1781 steps/s (collection: 0.555s, learning 4.044s)
               Value function loss: 170.7403
                    Surrogate loss: 0.0098
             Mean action noise std: 0.94
                       Mean reward: 575.25
               Mean episode length: 349.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 4.60s
                        Total time: 4845.06s
                               ETA: 13238.0s

################################################################################
                     [1m Learning iteration 1072/4000 [0m

                       Computation: 1804 steps/s (collection: 0.534s, learning 4.005s)
               Value function loss: 226.2626
                    Surrogate loss: 0.0109
             Mean action noise std: 0.94
                       Mean reward: 589.18
               Mean episode length: 355.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8790016
                    Iteration time: 4.54s
                        Total time: 4849.60s
                               ETA: 13233.6s

################################################################################
                     [1m Learning iteration 1073/4000 [0m

                       Computation: 1804 steps/s (collection: 0.571s, learning 3.967s)
               Value function loss: 233.6120
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 574.06
               Mean episode length: 348.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 4.54s
                        Total time: 4854.14s
                               ETA: 13229.1s

################################################################################
                     [1m Learning iteration 1074/4000 [0m

                       Computation: 1802 steps/s (collection: 0.600s, learning 3.944s)
               Value function loss: 245.0069
                    Surrogate loss: 0.0124
             Mean action noise std: 0.94
                       Mean reward: 588.81
               Mean episode length: 358.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8806400
                    Iteration time: 4.54s
                        Total time: 4858.68s
                               ETA: 13224.7s

################################################################################
                     [1m Learning iteration 1075/4000 [0m

                       Computation: 1807 steps/s (collection: 0.596s, learning 3.936s)
               Value function loss: 186.8856
                    Surrogate loss: 0.0105
             Mean action noise std: 0.94
                       Mean reward: 584.32
               Mean episode length: 356.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 4.53s
                        Total time: 4863.21s
                               ETA: 13220.2s

################################################################################
                     [1m Learning iteration 1076/4000 [0m

                       Computation: 1803 steps/s (collection: 0.603s, learning 3.939s)
               Value function loss: 208.0343
                    Surrogate loss: 0.0091
             Mean action noise std: 0.94
                       Mean reward: 586.70
               Mean episode length: 355.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8822784
                    Iteration time: 4.54s
                        Total time: 4867.76s
                               ETA: 13215.7s

################################################################################
                     [1m Learning iteration 1077/4000 [0m

                       Computation: 1805 steps/s (collection: 0.594s, learning 3.943s)
               Value function loss: 255.9727
                    Surrogate loss: 0.0121
             Mean action noise std: 0.94
                       Mean reward: 575.09
               Mean episode length: 348.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 4.54s
                        Total time: 4872.29s
                               ETA: 13211.2s

################################################################################
                     [1m Learning iteration 1078/4000 [0m

                       Computation: 1805 steps/s (collection: 0.599s, learning 3.939s)
               Value function loss: 191.7036
                    Surrogate loss: 0.0102
             Mean action noise std: 0.94
                       Mean reward: 565.91
               Mean episode length: 344.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8839168
                    Iteration time: 4.54s
                        Total time: 4876.83s
                               ETA: 13206.8s

################################################################################
                     [1m Learning iteration 1079/4000 [0m

                       Computation: 1820 steps/s (collection: 0.554s, learning 3.946s)
               Value function loss: 188.9349
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 557.81
               Mean episode length: 340.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 4.50s
                        Total time: 4881.33s
                               ETA: 13202.2s

################################################################################
                     [1m Learning iteration 1080/4000 [0m

                       Computation: 1823 steps/s (collection: 0.565s, learning 3.927s)
               Value function loss: 131.5593
                    Surrogate loss: 0.0114
             Mean action noise std: 0.94
                       Mean reward: 559.84
               Mean episode length: 343.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8855552
                    Iteration time: 4.49s
                        Total time: 4885.82s
                               ETA: 13197.6s

################################################################################
                     [1m Learning iteration 1081/4000 [0m

                       Computation: 1821 steps/s (collection: 0.562s, learning 3.936s)
               Value function loss: 222.2465
                    Surrogate loss: 0.0079
             Mean action noise std: 0.94
                       Mean reward: 552.01
               Mean episode length: 337.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 4.50s
                        Total time: 4890.32s
                               ETA: 13193.0s

################################################################################
                     [1m Learning iteration 1082/4000 [0m

                       Computation: 1807 steps/s (collection: 0.553s, learning 3.980s)
               Value function loss: 137.2322
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 534.15
               Mean episode length: 328.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8871936
                    Iteration time: 4.53s
                        Total time: 4894.85s
                               ETA: 13188.5s

################################################################################
                     [1m Learning iteration 1083/4000 [0m

                       Computation: 1838 steps/s (collection: 0.536s, learning 3.921s)
               Value function loss: 203.4218
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 535.51
               Mean episode length: 327.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 4.46s
                        Total time: 4899.31s
                               ETA: 13183.8s

################################################################################
                     [1m Learning iteration 1084/4000 [0m

                       Computation: 1822 steps/s (collection: 0.534s, learning 3.960s)
               Value function loss: 159.4479
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 552.47
               Mean episode length: 338.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8888320
                    Iteration time: 4.49s
                        Total time: 4903.80s
                               ETA: 13179.3s

################################################################################
                     [1m Learning iteration 1085/4000 [0m

                       Computation: 1817 steps/s (collection: 0.610s, learning 3.898s)
               Value function loss: 262.2795
                    Surrogate loss: 0.0079
             Mean action noise std: 0.94
                       Mean reward: 552.18
               Mean episode length: 338.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 4.51s
                        Total time: 4908.31s
                               ETA: 13174.7s

################################################################################
                     [1m Learning iteration 1086/4000 [0m

                       Computation: 1830 steps/s (collection: 0.570s, learning 3.906s)
               Value function loss: 226.9013
                    Surrogate loss: 0.0062
             Mean action noise std: 0.94
                       Mean reward: 557.46
               Mean episode length: 342.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8904704
                    Iteration time: 4.48s
                        Total time: 4912.79s
                               ETA: 13170.1s

################################################################################
                     [1m Learning iteration 1087/4000 [0m

                       Computation: 1809 steps/s (collection: 0.597s, learning 3.929s)
               Value function loss: 297.3220
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 580.92
               Mean episode length: 355.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 4.53s
                        Total time: 4917.32s
                               ETA: 13165.6s

################################################################################
                     [1m Learning iteration 1088/4000 [0m

                       Computation: 1820 steps/s (collection: 0.556s, learning 3.943s)
               Value function loss: 168.8457
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 589.76
               Mean episode length: 361.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8921088
                    Iteration time: 4.50s
                        Total time: 4921.81s
                               ETA: 13161.0s

################################################################################
                     [1m Learning iteration 1089/4000 [0m

                       Computation: 1839 steps/s (collection: 0.587s, learning 3.866s)
               Value function loss: 277.3689
                    Surrogate loss: 0.0118
             Mean action noise std: 0.94
                       Mean reward: 591.86
               Mean episode length: 363.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 4.45s
                        Total time: 4926.27s
                               ETA: 13156.3s

################################################################################
                     [1m Learning iteration 1090/4000 [0m

                       Computation: 1807 steps/s (collection: 0.586s, learning 3.946s)
               Value function loss: 230.1730
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 604.72
               Mean episode length: 369.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8937472
                    Iteration time: 4.53s
                        Total time: 4930.80s
                               ETA: 13151.8s

################################################################################
                     [1m Learning iteration 1091/4000 [0m

                       Computation: 1805 steps/s (collection: 0.639s, learning 3.897s)
               Value function loss: 206.7773
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 619.07
               Mean episode length: 377.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 4.54s
                        Total time: 4935.34s
                               ETA: 13147.3s

################################################################################
                     [1m Learning iteration 1092/4000 [0m

                       Computation: 1814 steps/s (collection: 0.579s, learning 3.936s)
               Value function loss: 297.2546
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 612.43
               Mean episode length: 374.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8953856
                    Iteration time: 4.51s
                        Total time: 4939.85s
                               ETA: 13142.8s

################################################################################
                     [1m Learning iteration 1093/4000 [0m

                       Computation: 1800 steps/s (collection: 0.597s, learning 3.952s)
               Value function loss: 141.0440
                    Surrogate loss: 0.0105
             Mean action noise std: 0.94
                       Mean reward: 608.58
               Mean episode length: 371.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 4.55s
                        Total time: 4944.40s
                               ETA: 13138.4s

################################################################################
                     [1m Learning iteration 1094/4000 [0m

                       Computation: 1798 steps/s (collection: 0.599s, learning 3.956s)
               Value function loss: 174.2218
                    Surrogate loss: 0.0121
             Mean action noise std: 0.94
                       Mean reward: 620.72
               Mean episode length: 379.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8970240
                    Iteration time: 4.55s
                        Total time: 4948.95s
                               ETA: 13133.9s

################################################################################
                     [1m Learning iteration 1095/4000 [0m

                       Computation: 1797 steps/s (collection: 0.589s, learning 3.969s)
               Value function loss: 178.2669
                    Surrogate loss: 0.0116
             Mean action noise std: 0.94
                       Mean reward: 619.27
               Mean episode length: 376.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 4.56s
                        Total time: 4953.51s
                               ETA: 13129.5s

################################################################################
                     [1m Learning iteration 1096/4000 [0m

                       Computation: 1803 steps/s (collection: 0.573s, learning 3.969s)
               Value function loss: 88.6189
                    Surrogate loss: 0.0105
             Mean action noise std: 0.94
                       Mean reward: 612.91
               Mean episode length: 374.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 8986624
                    Iteration time: 4.54s
                        Total time: 4958.05s
                               ETA: 13125.1s

################################################################################
                     [1m Learning iteration 1097/4000 [0m

                       Computation: 1802 steps/s (collection: 0.625s, learning 3.920s)
               Value function loss: 189.7473
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 610.78
               Mean episode length: 372.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 4.54s
                        Total time: 4962.60s
                               ETA: 13120.6s

################################################################################
                     [1m Learning iteration 1098/4000 [0m

                       Computation: 1793 steps/s (collection: 0.560s, learning 4.008s)
               Value function loss: 191.0538
                    Surrogate loss: 0.0099
             Mean action noise std: 0.93
                       Mean reward: 608.69
               Mean episode length: 369.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9003008
                    Iteration time: 4.57s
                        Total time: 4967.17s
                               ETA: 13116.2s

################################################################################
                     [1m Learning iteration 1099/4000 [0m

                       Computation: 1786 steps/s (collection: 0.594s, learning 3.993s)
               Value function loss: 275.5031
                    Surrogate loss: 0.0122
             Mean action noise std: 0.93
                       Mean reward: 608.54
               Mean episode length: 370.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 4.59s
                        Total time: 4971.75s
                               ETA: 13111.9s

################################################################################
                     [1m Learning iteration 1100/4000 [0m

                       Computation: 1813 steps/s (collection: 0.598s, learning 3.918s)
               Value function loss: 188.0238
                    Surrogate loss: 0.0077
             Mean action noise std: 0.93
                       Mean reward: 622.81
               Mean episode length: 378.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9019392
                    Iteration time: 4.52s
                        Total time: 4976.27s
                               ETA: 13107.3s

################################################################################
                     [1m Learning iteration 1101/4000 [0m

                       Computation: 1810 steps/s (collection: 0.588s, learning 3.938s)
               Value function loss: 215.9012
                    Surrogate loss: 0.0096
             Mean action noise std: 0.93
                       Mean reward: 636.21
               Mean episode length: 385.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 4.53s
                        Total time: 4980.80s
                               ETA: 13102.8s

################################################################################
                     [1m Learning iteration 1102/4000 [0m

                       Computation: 1829 steps/s (collection: 0.584s, learning 3.893s)
               Value function loss: 144.0991
                    Surrogate loss: 0.0089
             Mean action noise std: 0.93
                       Mean reward: 648.06
               Mean episode length: 393.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9035776
                    Iteration time: 4.48s
                        Total time: 4985.27s
                               ETA: 13098.2s

################################################################################
                     [1m Learning iteration 1103/4000 [0m

                       Computation: 1794 steps/s (collection: 0.612s, learning 3.953s)
               Value function loss: 267.9935
                    Surrogate loss: 0.0116
             Mean action noise std: 0.93
                       Mean reward: 640.24
               Mean episode length: 387.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 4.57s
                        Total time: 4989.84s
                               ETA: 13093.8s

################################################################################
                     [1m Learning iteration 1104/4000 [0m

                       Computation: 1835 steps/s (collection: 0.569s, learning 3.893s)
               Value function loss: 199.1695
                    Surrogate loss: 0.0078
             Mean action noise std: 0.93
                       Mean reward: 655.05
               Mean episode length: 396.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9052160
                    Iteration time: 4.46s
                        Total time: 4994.30s
                               ETA: 13089.1s

################################################################################
                     [1m Learning iteration 1105/4000 [0m

                       Computation: 1817 steps/s (collection: 0.583s, learning 3.925s)
               Value function loss: 301.5572
                    Surrogate loss: 0.0072
             Mean action noise std: 0.93
                       Mean reward: 671.65
               Mean episode length: 403.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 4.51s
                        Total time: 4998.81s
                               ETA: 13084.6s

################################################################################
                     [1m Learning iteration 1106/4000 [0m

                       Computation: 1809 steps/s (collection: 0.613s, learning 3.914s)
               Value function loss: 261.3603
                    Surrogate loss: 0.0093
             Mean action noise std: 0.93
                       Mean reward: 688.33
               Mean episode length: 414.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9068544
                    Iteration time: 4.53s
                        Total time: 5003.34s
                               ETA: 13080.1s

################################################################################
                     [1m Learning iteration 1107/4000 [0m

                       Computation: 1824 steps/s (collection: 0.573s, learning 3.918s)
               Value function loss: 209.6670
                    Surrogate loss: 0.0088
             Mean action noise std: 0.93
                       Mean reward: 696.85
               Mean episode length: 418.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 4.49s
                        Total time: 5007.83s
                               ETA: 13075.5s

################################################################################
                     [1m Learning iteration 1108/4000 [0m

                       Computation: 1825 steps/s (collection: 0.589s, learning 3.897s)
               Value function loss: 230.1083
                    Surrogate loss: 0.0107
             Mean action noise std: 0.93
                       Mean reward: 715.11
               Mean episode length: 429.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9084928
                    Iteration time: 4.49s
                        Total time: 5012.31s
                               ETA: 13070.9s

################################################################################
                     [1m Learning iteration 1109/4000 [0m

                       Computation: 1824 steps/s (collection: 0.570s, learning 3.920s)
               Value function loss: 225.3123
                    Surrogate loss: 0.0103
             Mean action noise std: 0.93
                       Mean reward: 713.55
               Mean episode length: 430.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 4.49s
                        Total time: 5016.80s
                               ETA: 13066.3s

################################################################################
                     [1m Learning iteration 1110/4000 [0m

                       Computation: 1821 steps/s (collection: 0.590s, learning 3.907s)
               Value function loss: 225.7084
                    Surrogate loss: 0.0106
             Mean action noise std: 0.93
                       Mean reward: 721.74
               Mean episode length: 434.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9101312
                    Iteration time: 4.50s
                        Total time: 5021.30s
                               ETA: 13061.7s

################################################################################
                     [1m Learning iteration 1111/4000 [0m

                       Computation: 1826 steps/s (collection: 0.609s, learning 3.876s)
               Value function loss: 139.0830
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 725.84
               Mean episode length: 438.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 4.49s
                        Total time: 5025.78s
                               ETA: 13057.1s

################################################################################
                     [1m Learning iteration 1112/4000 [0m

                       Computation: 1802 steps/s (collection: 0.605s, learning 3.938s)
               Value function loss: 142.9375
                    Surrogate loss: 0.0109
             Mean action noise std: 0.93
                       Mean reward: 719.77
               Mean episode length: 434.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 9117696
                    Iteration time: 4.54s
                        Total time: 5030.33s
                               ETA: 13052.6s

################################################################################
                     [1m Learning iteration 1113/4000 [0m

                       Computation: 1795 steps/s (collection: 0.649s, learning 3.914s)
               Value function loss: 156.1937
                    Surrogate loss: 0.0122
             Mean action noise std: 0.93
                       Mean reward: 725.74
               Mean episode length: 438.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 4.56s
                        Total time: 5034.89s
                               ETA: 13048.2s

################################################################################
                     [1m Learning iteration 1114/4000 [0m

                       Computation: 1802 steps/s (collection: 0.605s, learning 3.940s)
               Value function loss: 247.0908
                    Surrogate loss: 0.0209
             Mean action noise std: 0.93
                       Mean reward: 733.05
               Mean episode length: 443.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9134080
                    Iteration time: 4.55s
                        Total time: 5039.44s
                               ETA: 13043.8s

################################################################################
                     [1m Learning iteration 1115/4000 [0m

                       Computation: 1807 steps/s (collection: 0.602s, learning 3.931s)
               Value function loss: 237.8146
                    Surrogate loss: 0.0090
             Mean action noise std: 0.93
                       Mean reward: 735.12
               Mean episode length: 447.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 4.53s
                        Total time: 5043.97s
                               ETA: 13039.3s

################################################################################
                     [1m Learning iteration 1116/4000 [0m

                       Computation: 1771 steps/s (collection: 0.651s, learning 3.974s)
               Value function loss: 263.6039
                    Surrogate loss: 0.0114
             Mean action noise std: 0.93
                       Mean reward: 734.48
               Mean episode length: 447.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9150464
                    Iteration time: 4.62s
                        Total time: 5048.59s
                               ETA: 13035.0s

################################################################################
                     [1m Learning iteration 1117/4000 [0m

                       Computation: 1812 steps/s (collection: 0.576s, learning 3.943s)
               Value function loss: 248.8578
                    Surrogate loss: 0.0108
             Mean action noise std: 0.93
                       Mean reward: 724.51
               Mean episode length: 440.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 4.52s
                        Total time: 5053.11s
                               ETA: 13030.5s

################################################################################
                     [1m Learning iteration 1118/4000 [0m

                       Computation: 1774 steps/s (collection: 0.577s, learning 4.040s)
               Value function loss: 182.5088
                    Surrogate loss: 0.0123
             Mean action noise std: 0.93
                       Mean reward: 726.56
               Mean episode length: 443.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9166848
                    Iteration time: 4.62s
                        Total time: 5057.73s
                               ETA: 13026.3s

################################################################################
                     [1m Learning iteration 1119/4000 [0m

                       Computation: 1835 steps/s (collection: 0.549s, learning 3.913s)
               Value function loss: 229.6530
                    Surrogate loss: 0.0112
             Mean action noise std: 0.93
                       Mean reward: 734.55
               Mean episode length: 445.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 4.46s
                        Total time: 5062.19s
                               ETA: 13021.6s

################################################################################
                     [1m Learning iteration 1120/4000 [0m

                       Computation: 1787 steps/s (collection: 0.623s, learning 3.961s)
               Value function loss: 236.7234
                    Surrogate loss: 0.0086
             Mean action noise std: 0.93
                       Mean reward: 741.39
               Mean episode length: 449.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9183232
                    Iteration time: 4.58s
                        Total time: 5066.78s
                               ETA: 13017.2s

################################################################################
                     [1m Learning iteration 1121/4000 [0m

                       Computation: 1788 steps/s (collection: 0.568s, learning 4.012s)
               Value function loss: 183.3556
                    Surrogate loss: 0.0093
             Mean action noise std: 0.93
                       Mean reward: 749.75
               Mean episode length: 453.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 4.58s
                        Total time: 5071.36s
                               ETA: 13012.9s

################################################################################
                     [1m Learning iteration 1122/4000 [0m

                       Computation: 1799 steps/s (collection: 0.606s, learning 3.946s)
               Value function loss: 230.6131
                    Surrogate loss: 0.0097
             Mean action noise std: 0.93
                       Mean reward: 752.77
               Mean episode length: 455.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9199616
                    Iteration time: 4.55s
                        Total time: 5075.91s
                               ETA: 13008.4s

################################################################################
                     [1m Learning iteration 1123/4000 [0m

                       Computation: 1767 steps/s (collection: 0.614s, learning 4.022s)
               Value function loss: 222.3219
                    Surrogate loss: 0.0080
             Mean action noise std: 0.93
                       Mean reward: 759.96
               Mean episode length: 461.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 4.64s
                        Total time: 5080.55s
                               ETA: 13004.2s

################################################################################
                     [1m Learning iteration 1124/4000 [0m

                       Computation: 1797 steps/s (collection: 0.572s, learning 3.984s)
               Value function loss: 164.6571
                    Surrogate loss: 0.0113
             Mean action noise std: 0.93
                       Mean reward: 764.58
               Mean episode length: 464.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9216000
                    Iteration time: 4.56s
                        Total time: 5085.10s
                               ETA: 12999.8s

################################################################################
                     [1m Learning iteration 1125/4000 [0m

                       Computation: 1789 steps/s (collection: 0.580s, learning 3.998s)
               Value function loss: 172.7695
                    Surrogate loss: 0.0110
             Mean action noise std: 0.93
                       Mean reward: 769.88
               Mean episode length: 465.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 4.58s
                        Total time: 5089.68s
                               ETA: 12995.4s

################################################################################
                     [1m Learning iteration 1126/4000 [0m

                       Computation: 1801 steps/s (collection: 0.594s, learning 3.954s)
               Value function loss: 144.3132
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 777.30
               Mean episode length: 469.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9232384
                    Iteration time: 4.55s
                        Total time: 5094.23s
                               ETA: 12991.0s

################################################################################
                     [1m Learning iteration 1127/4000 [0m

                       Computation: 1811 steps/s (collection: 0.612s, learning 3.911s)
               Value function loss: 108.9101
                    Surrogate loss: 0.0124
             Mean action noise std: 0.93
                       Mean reward: 771.03
               Mean episode length: 465.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 4.52s
                        Total time: 5098.75s
                               ETA: 12986.4s

################################################################################
                     [1m Learning iteration 1128/4000 [0m

                       Computation: 1832 steps/s (collection: 0.585s, learning 3.885s)
               Value function loss: 169.6612
                    Surrogate loss: 0.0123
             Mean action noise std: 0.93
                       Mean reward: 770.51
               Mean episode length: 465.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9248768
                    Iteration time: 4.47s
                        Total time: 5103.22s
                               ETA: 12981.8s

################################################################################
                     [1m Learning iteration 1129/4000 [0m

                       Computation: 1795 steps/s (collection: 0.609s, learning 3.954s)
               Value function loss: 139.8101
                    Surrogate loss: 0.0118
             Mean action noise std: 0.93
                       Mean reward: 766.34
               Mean episode length: 462.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 4.56s
                        Total time: 5107.78s
                               ETA: 12977.4s

################################################################################
                     [1m Learning iteration 1130/4000 [0m

                       Computation: 1809 steps/s (collection: 0.581s, learning 3.948s)
               Value function loss: 245.8975
                    Surrogate loss: 0.0092
             Mean action noise std: 0.93
                       Mean reward: 756.92
               Mean episode length: 459.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9265152
                    Iteration time: 4.53s
                        Total time: 5112.31s
                               ETA: 12972.9s

################################################################################
                     [1m Learning iteration 1131/4000 [0m

                       Computation: 1802 steps/s (collection: 0.636s, learning 3.909s)
               Value function loss: 125.8180
                    Surrogate loss: 0.0079
             Mean action noise std: 0.93
                       Mean reward: 750.15
               Mean episode length: 455.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 4.54s
                        Total time: 5116.86s
                               ETA: 12968.4s

################################################################################
                     [1m Learning iteration 1132/4000 [0m

                       Computation: 1843 steps/s (collection: 0.523s, learning 3.922s)
               Value function loss: 155.4295
                    Surrogate loss: 0.0106
             Mean action noise std: 0.93
                       Mean reward: 757.01
               Mean episode length: 457.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9281536
                    Iteration time: 4.44s
                        Total time: 5121.30s
                               ETA: 12963.7s

################################################################################
                     [1m Learning iteration 1133/4000 [0m

                       Computation: 1814 steps/s (collection: 0.586s, learning 3.929s)
               Value function loss: 214.9788
                    Surrogate loss: 0.0089
             Mean action noise std: 0.93
                       Mean reward: 751.27
               Mean episode length: 450.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 4.51s
                        Total time: 5125.82s
                               ETA: 12959.2s

################################################################################
                     [1m Learning iteration 1134/4000 [0m

                       Computation: 1850 steps/s (collection: 0.534s, learning 3.892s)
               Value function loss: 278.7468
                    Surrogate loss: 0.0075
             Mean action noise std: 0.93
                       Mean reward: 734.99
               Mean episode length: 439.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9297920
                    Iteration time: 4.43s
                        Total time: 5130.24s
                               ETA: 12954.4s

################################################################################
                     [1m Learning iteration 1135/4000 [0m

                       Computation: 1835 steps/s (collection: 0.534s, learning 3.928s)
               Value function loss: 241.5054
                    Surrogate loss: 0.0082
             Mean action noise std: 0.93
                       Mean reward: 730.82
               Mean episode length: 435.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 4.46s
                        Total time: 5134.71s
                               ETA: 12949.8s

################################################################################
                     [1m Learning iteration 1136/4000 [0m

                       Computation: 1844 steps/s (collection: 0.543s, learning 3.899s)
               Value function loss: 177.5610
                    Surrogate loss: 0.0086
             Mean action noise std: 0.93
                       Mean reward: 720.74
               Mean episode length: 429.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9314304
                    Iteration time: 4.44s
                        Total time: 5139.15s
                               ETA: 12945.0s

################################################################################
                     [1m Learning iteration 1137/4000 [0m

                       Computation: 1856 steps/s (collection: 0.538s, learning 3.873s)
               Value function loss: 155.7022
                    Surrogate loss: 0.0105
             Mean action noise std: 0.93
                       Mean reward: 725.53
               Mean episode length: 430.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 4.41s
                        Total time: 5143.56s
                               ETA: 12940.3s

################################################################################
                     [1m Learning iteration 1138/4000 [0m

                       Computation: 1855 steps/s (collection: 0.536s, learning 3.879s)
               Value function loss: 288.5297
                    Surrogate loss: 0.0097
             Mean action noise std: 0.93
                       Mean reward: 733.25
               Mean episode length: 434.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9330688
                    Iteration time: 4.41s
                        Total time: 5147.97s
                               ETA: 12935.5s

################################################################################
                     [1m Learning iteration 1139/4000 [0m

                       Computation: 1860 steps/s (collection: 0.523s, learning 3.880s)
               Value function loss: 210.2637
                    Surrogate loss: 0.0086
             Mean action noise std: 0.93
                       Mean reward: 731.02
               Mean episode length: 431.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 4.40s
                        Total time: 5152.38s
                               ETA: 12930.7s

################################################################################
                     [1m Learning iteration 1140/4000 [0m

                       Computation: 1856 steps/s (collection: 0.512s, learning 3.900s)
               Value function loss: 231.9849
                    Surrogate loss: 0.0102
             Mean action noise std: 0.93
                       Mean reward: 743.99
               Mean episode length: 437.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9347072
                    Iteration time: 4.41s
                        Total time: 5156.79s
                               ETA: 12925.9s

################################################################################
                     [1m Learning iteration 1141/4000 [0m

                       Computation: 1851 steps/s (collection: 0.521s, learning 3.904s)
               Value function loss: 210.2894
                    Surrogate loss: 0.0089
             Mean action noise std: 0.93
                       Mean reward: 735.91
               Mean episode length: 432.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 4.42s
                        Total time: 5161.21s
                               ETA: 12921.1s

################################################################################
                     [1m Learning iteration 1142/4000 [0m

                       Computation: 1828 steps/s (collection: 0.550s, learning 3.930s)
               Value function loss: 143.1211
                    Surrogate loss: 0.0109
             Mean action noise std: 0.93
                       Mean reward: 727.72
               Mean episode length: 429.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9363456
                    Iteration time: 4.48s
                        Total time: 5165.69s
                               ETA: 12916.5s

################################################################################
                     [1m Learning iteration 1143/4000 [0m

                       Computation: 1842 steps/s (collection: 0.521s, learning 3.926s)
               Value function loss: 93.0241
                    Surrogate loss: 0.0117
             Mean action noise std: 0.93
                       Mean reward: 722.01
               Mean episode length: 426.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 31.27
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 4.45s
                        Total time: 5170.14s
                               ETA: 12911.8s

################################################################################
                     [1m Learning iteration 1144/4000 [0m

                       Computation: 1825 steps/s (collection: 0.530s, learning 3.959s)
               Value function loss: 170.9110
                    Surrogate loss: 0.0105
             Mean action noise std: 0.93
                       Mean reward: 721.43
               Mean episode length: 426.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9379840
                    Iteration time: 4.49s
                        Total time: 5174.63s
                               ETA: 12907.2s

################################################################################
                     [1m Learning iteration 1145/4000 [0m

                       Computation: 1831 steps/s (collection: 0.558s, learning 3.914s)
               Value function loss: 208.4566
                    Surrogate loss: 0.0098
             Mean action noise std: 0.93
                       Mean reward: 726.25
               Mean episode length: 427.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 4.47s
                        Total time: 5179.10s
                               ETA: 12902.6s

################################################################################
                     [1m Learning iteration 1146/4000 [0m

                       Computation: 1844 steps/s (collection: 0.532s, learning 3.909s)
               Value function loss: 245.5532
                    Surrogate loss: 0.0085
             Mean action noise std: 0.93
                       Mean reward: 724.40
               Mean episode length: 426.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9396224
                    Iteration time: 4.44s
                        Total time: 5183.54s
                               ETA: 12897.8s

################################################################################
                     [1m Learning iteration 1147/4000 [0m

                       Computation: 1799 steps/s (collection: 0.527s, learning 4.024s)
               Value function loss: 196.0289
                    Surrogate loss: 0.0105
             Mean action noise std: 0.93
                       Mean reward: 731.59
               Mean episode length: 432.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 4.55s
                        Total time: 5188.09s
                               ETA: 12893.4s

################################################################################
                     [1m Learning iteration 1148/4000 [0m

                       Computation: 1802 steps/s (collection: 0.564s, learning 3.981s)
               Value function loss: 183.2279
                    Surrogate loss: 0.0132
             Mean action noise std: 0.93
                       Mean reward: 706.09
               Mean episode length: 420.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9412608
                    Iteration time: 4.55s
                        Total time: 5192.64s
                               ETA: 12889.0s

################################################################################
                     [1m Learning iteration 1149/4000 [0m

                       Computation: 1818 steps/s (collection: 0.545s, learning 3.959s)
               Value function loss: 189.9228
                    Surrogate loss: 0.0115
             Mean action noise std: 0.93
                       Mean reward: 701.07
               Mean episode length: 415.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 4.50s
                        Total time: 5197.14s
                               ETA: 12884.4s

################################################################################
                     [1m Learning iteration 1150/4000 [0m

                       Computation: 1801 steps/s (collection: 0.544s, learning 4.003s)
               Value function loss: 215.9031
                    Surrogate loss: 0.0076
             Mean action noise std: 0.93
                       Mean reward: 703.27
               Mean episode length: 417.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9428992
                    Iteration time: 4.55s
                        Total time: 5201.69s
                               ETA: 12879.9s

################################################################################
                     [1m Learning iteration 1151/4000 [0m

                       Computation: 1814 steps/s (collection: 0.541s, learning 3.974s)
               Value function loss: 173.3339
                    Surrogate loss: 0.0066
             Mean action noise std: 0.93
                       Mean reward: 689.71
               Mean episode length: 411.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 4.51s
                        Total time: 5206.20s
                               ETA: 12875.4s

################################################################################
                     [1m Learning iteration 1152/4000 [0m

                       Computation: 1805 steps/s (collection: 0.561s, learning 3.976s)
               Value function loss: 197.0378
                    Surrogate loss: 0.0089
             Mean action noise std: 0.93
                       Mean reward: 712.26
               Mean episode length: 422.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9445376
                    Iteration time: 4.54s
                        Total time: 5210.74s
                               ETA: 12870.9s

################################################################################
                     [1m Learning iteration 1153/4000 [0m

                       Computation: 1804 steps/s (collection: 0.584s, learning 3.956s)
               Value function loss: 179.6185
                    Surrogate loss: 0.0171
             Mean action noise std: 0.93
                       Mean reward: 714.00
               Mean episode length: 424.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 4.54s
                        Total time: 5215.28s
                               ETA: 12866.5s

################################################################################
                     [1m Learning iteration 1154/4000 [0m

                       Computation: 1828 steps/s (collection: 0.538s, learning 3.942s)
               Value function loss: 249.4974
                    Surrogate loss: 0.0082
             Mean action noise std: 0.93
                       Mean reward: 733.34
               Mean episode length: 434.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9461760
                    Iteration time: 4.48s
                        Total time: 5219.76s
                               ETA: 12861.9s

################################################################################
                     [1m Learning iteration 1155/4000 [0m

                       Computation: 1783 steps/s (collection: 0.639s, learning 3.953s)
               Value function loss: 166.5987
                    Surrogate loss: 0.0090
             Mean action noise std: 0.93
                       Mean reward: 733.53
               Mean episode length: 435.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 4.59s
                        Total time: 5224.35s
                               ETA: 12857.5s

################################################################################
                     [1m Learning iteration 1156/4000 [0m

                       Computation: 1794 steps/s (collection: 0.641s, learning 3.923s)
               Value function loss: 228.4655
                    Surrogate loss: 0.0079
             Mean action noise std: 0.93
                       Mean reward: 727.42
               Mean episode length: 432.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9478144
                    Iteration time: 4.56s
                        Total time: 5228.92s
                               ETA: 12853.1s

################################################################################
                     [1m Learning iteration 1157/4000 [0m

                       Computation: 1820 steps/s (collection: 0.566s, learning 3.933s)
               Value function loss: 249.9951
                    Surrogate loss: 0.0072
             Mean action noise std: 0.93
                       Mean reward: 730.75
               Mean episode length: 432.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 4.50s
                        Total time: 5233.42s
                               ETA: 12848.5s

################################################################################
                     [1m Learning iteration 1158/4000 [0m

                       Computation: 1800 steps/s (collection: 0.605s, learning 3.945s)
               Value function loss: 103.3271
                    Surrogate loss: 0.0104
             Mean action noise std: 0.93
                       Mean reward: 736.12
               Mean episode length: 435.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 9494528
                    Iteration time: 4.55s
                        Total time: 5237.97s
                               ETA: 12844.1s

################################################################################
                     [1m Learning iteration 1159/4000 [0m

                       Computation: 1821 steps/s (collection: 0.539s, learning 3.958s)
               Value function loss: 169.1545
                    Surrogate loss: 0.0066
             Mean action noise std: 0.93
                       Mean reward: 736.10
               Mean episode length: 432.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 4.50s
                        Total time: 5242.46s
                               ETA: 12839.5s

################################################################################
                     [1m Learning iteration 1160/4000 [0m

                       Computation: 1828 steps/s (collection: 0.572s, learning 3.910s)
               Value function loss: 116.2860
                    Surrogate loss: 0.0084
             Mean action noise std: 0.93
                       Mean reward: 738.54
               Mean episode length: 433.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9510912
                    Iteration time: 4.48s
                        Total time: 5246.94s
                               ETA: 12834.9s

################################################################################
                     [1m Learning iteration 1161/4000 [0m

                       Computation: 1819 steps/s (collection: 0.565s, learning 3.937s)
               Value function loss: 199.4217
                    Surrogate loss: 0.0082
             Mean action noise std: 0.93
                       Mean reward: 743.24
               Mean episode length: 435.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 4.50s
                        Total time: 5251.45s
                               ETA: 12830.3s

################################################################################
                     [1m Learning iteration 1162/4000 [0m

                       Computation: 1826 steps/s (collection: 0.549s, learning 3.935s)
               Value function loss: 233.8737
                    Surrogate loss: 0.0059
             Mean action noise std: 0.93
                       Mean reward: 746.35
               Mean episode length: 436.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9527296
                    Iteration time: 4.48s
                        Total time: 5255.93s
                               ETA: 12825.7s

################################################################################
                     [1m Learning iteration 1163/4000 [0m

                       Computation: 1827 steps/s (collection: 0.546s, learning 3.937s)
               Value function loss: 276.2245
                    Surrogate loss: 0.0087
             Mean action noise std: 0.93
                       Mean reward: 732.63
               Mean episode length: 429.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 4.48s
                        Total time: 5260.41s
                               ETA: 12821.1s

################################################################################
                     [1m Learning iteration 1164/4000 [0m

                       Computation: 1837 steps/s (collection: 0.550s, learning 3.908s)
               Value function loss: 255.3659
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 727.11
               Mean episode length: 426.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9543680
                    Iteration time: 4.46s
                        Total time: 5264.87s
                               ETA: 12816.5s

################################################################################
                     [1m Learning iteration 1165/4000 [0m

                       Computation: 1826 steps/s (collection: 0.565s, learning 3.920s)
               Value function loss: 287.6029
                    Surrogate loss: 0.0062
             Mean action noise std: 0.93
                       Mean reward: 715.46
               Mean episode length: 421.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 4.48s
                        Total time: 5269.36s
                               ETA: 12811.9s

################################################################################
                     [1m Learning iteration 1166/4000 [0m

                       Computation: 1811 steps/s (collection: 0.559s, learning 3.963s)
               Value function loss: 193.0009
                    Surrogate loss: 0.0087
             Mean action noise std: 0.93
                       Mean reward: 706.86
               Mean episode length: 415.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9560064
                    Iteration time: 4.52s
                        Total time: 5273.88s
                               ETA: 12807.3s

################################################################################
                     [1m Learning iteration 1167/4000 [0m

                       Computation: 1820 steps/s (collection: 0.560s, learning 3.940s)
               Value function loss: 133.9443
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 696.51
               Mean episode length: 410.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 4.50s
                        Total time: 5278.38s
                               ETA: 12802.8s

################################################################################
                     [1m Learning iteration 1168/4000 [0m

                       Computation: 1798 steps/s (collection: 0.618s, learning 3.937s)
               Value function loss: 125.9243
                    Surrogate loss: 0.0110
             Mean action noise std: 0.93
                       Mean reward: 708.05
               Mean episode length: 415.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9576448
                    Iteration time: 4.56s
                        Total time: 5282.93s
                               ETA: 12798.3s

################################################################################
                     [1m Learning iteration 1169/4000 [0m

                       Computation: 1825 steps/s (collection: 0.601s, learning 3.887s)
               Value function loss: 131.3434
                    Surrogate loss: 0.0107
             Mean action noise std: 0.93
                       Mean reward: 700.96
               Mean episode length: 412.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 4.49s
                        Total time: 5287.42s
                               ETA: 12793.8s

################################################################################
                     [1m Learning iteration 1170/4000 [0m

                       Computation: 1834 steps/s (collection: 0.561s, learning 3.905s)
               Value function loss: 185.2219
                    Surrogate loss: 0.0087
             Mean action noise std: 0.93
                       Mean reward: 712.54
               Mean episode length: 417.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9592832
                    Iteration time: 4.47s
                        Total time: 5291.89s
                               ETA: 12789.1s

################################################################################
                     [1m Learning iteration 1171/4000 [0m

                       Computation: 1829 steps/s (collection: 0.568s, learning 3.910s)
               Value function loss: 217.0488
                    Surrogate loss: 0.0079
             Mean action noise std: 0.93
                       Mean reward: 710.79
               Mean episode length: 415.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 4.48s
                        Total time: 5296.37s
                               ETA: 12784.5s

################################################################################
                     [1m Learning iteration 1172/4000 [0m

                       Computation: 1822 steps/s (collection: 0.539s, learning 3.956s)
               Value function loss: 180.0425
                    Surrogate loss: 0.0069
             Mean action noise std: 0.93
                       Mean reward: 722.42
               Mean episode length: 420.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9609216
                    Iteration time: 4.49s
                        Total time: 5300.86s
                               ETA: 12779.9s

################################################################################
                     [1m Learning iteration 1173/4000 [0m

                       Computation: 1820 steps/s (collection: 0.571s, learning 3.928s)
               Value function loss: 137.7842
                    Surrogate loss: 0.0068
             Mean action noise std: 0.93
                       Mean reward: 723.68
               Mean episode length: 420.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 4.50s
                        Total time: 5305.36s
                               ETA: 12775.3s

################################################################################
                     [1m Learning iteration 1174/4000 [0m

                       Computation: 1819 steps/s (collection: 0.561s, learning 3.943s)
               Value function loss: 111.7611
                    Surrogate loss: 0.0074
             Mean action noise std: 0.93
                       Mean reward: 728.76
               Mean episode length: 422.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 31.27
--------------------------------------------------------------------------------
                   Total timesteps: 9625600
                    Iteration time: 4.50s
                        Total time: 5309.86s
                               ETA: 12770.8s

################################################################################
                     [1m Learning iteration 1175/4000 [0m

                       Computation: 1798 steps/s (collection: 0.624s, learning 3.930s)
               Value function loss: 140.6730
                    Surrogate loss: 0.0081
             Mean action noise std: 0.93
                       Mean reward: 735.14
               Mean episode length: 424.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 4.55s
                        Total time: 5314.42s
                               ETA: 12766.4s

################################################################################
                     [1m Learning iteration 1176/4000 [0m

                       Computation: 1815 steps/s (collection: 0.578s, learning 3.933s)
               Value function loss: 203.5745
                    Surrogate loss: 0.0112
             Mean action noise std: 0.93
                       Mean reward: 744.50
               Mean episode length: 428.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9641984
                    Iteration time: 4.51s
                        Total time: 5318.93s
                               ETA: 12761.8s

################################################################################
                     [1m Learning iteration 1177/4000 [0m

                       Computation: 1789 steps/s (collection: 0.618s, learning 3.960s)
               Value function loss: 272.1415
                    Surrogate loss: 0.0065
             Mean action noise std: 0.93
                       Mean reward: 746.35
               Mean episode length: 426.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 4.58s
                        Total time: 5323.51s
                               ETA: 12757.4s

################################################################################
                     [1m Learning iteration 1178/4000 [0m

                       Computation: 1831 steps/s (collection: 0.567s, learning 3.905s)
               Value function loss: 176.3898
                    Surrogate loss: 0.0079
             Mean action noise std: 0.93
                       Mean reward: 742.44
               Mean episode length: 424.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9658368
                    Iteration time: 4.47s
                        Total time: 5327.98s
                               ETA: 12752.8s

################################################################################
                     [1m Learning iteration 1179/4000 [0m

                       Computation: 1802 steps/s (collection: 0.577s, learning 3.968s)
               Value function loss: 200.4059
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 745.61
               Mean episode length: 427.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 4.55s
                        Total time: 5332.52s
                               ETA: 12748.3s

################################################################################
                     [1m Learning iteration 1180/4000 [0m

                       Computation: 1822 steps/s (collection: 0.594s, learning 3.902s)
               Value function loss: 162.0574
                    Surrogate loss: 0.0076
             Mean action noise std: 0.93
                       Mean reward: 740.80
               Mean episode length: 425.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9674752
                    Iteration time: 4.50s
                        Total time: 5337.02s
                               ETA: 12743.8s

################################################################################
                     [1m Learning iteration 1181/4000 [0m

                       Computation: 1826 steps/s (collection: 0.584s, learning 3.902s)
               Value function loss: 280.3665
                    Surrogate loss: 0.0083
             Mean action noise std: 0.93
                       Mean reward: 733.44
               Mean episode length: 421.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 4.49s
                        Total time: 5341.51s
                               ETA: 12739.2s

################################################################################
                     [1m Learning iteration 1182/4000 [0m

                       Computation: 1816 steps/s (collection: 0.588s, learning 3.922s)
               Value function loss: 240.7447
                    Surrogate loss: 0.0084
             Mean action noise std: 0.93
                       Mean reward: 720.63
               Mean episode length: 415.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9691136
                    Iteration time: 4.51s
                        Total time: 5346.02s
                               ETA: 12734.6s

################################################################################
                     [1m Learning iteration 1183/4000 [0m

                       Computation: 1806 steps/s (collection: 0.595s, learning 3.941s)
               Value function loss: 188.5347
                    Surrogate loss: 0.0089
             Mean action noise std: 0.93
                       Mean reward: 709.77
               Mean episode length: 408.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 4.54s
                        Total time: 5350.55s
                               ETA: 12730.2s

################################################################################
                     [1m Learning iteration 1184/4000 [0m

                       Computation: 1818 steps/s (collection: 0.608s, learning 3.896s)
               Value function loss: 189.6994
                    Surrogate loss: 0.0080
             Mean action noise std: 0.93
                       Mean reward: 694.79
               Mean episode length: 401.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9707520
                    Iteration time: 4.50s
                        Total time: 5355.06s
                               ETA: 12725.6s

################################################################################
                     [1m Learning iteration 1185/4000 [0m

                       Computation: 1821 steps/s (collection: 0.540s, learning 3.956s)
               Value function loss: 156.7731
                    Surrogate loss: 0.0086
             Mean action noise std: 0.93
                       Mean reward: 705.79
               Mean episode length: 408.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 4.50s
                        Total time: 5359.55s
                               ETA: 12721.0s

################################################################################
                     [1m Learning iteration 1186/4000 [0m

                       Computation: 1814 steps/s (collection: 0.577s, learning 3.938s)
               Value function loss: 183.8531
                    Surrogate loss: 0.0085
             Mean action noise std: 0.93
                       Mean reward: 699.02
               Mean episode length: 404.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9723904
                    Iteration time: 4.51s
                        Total time: 5364.07s
                               ETA: 12716.5s

################################################################################
                     [1m Learning iteration 1187/4000 [0m

                       Computation: 1791 steps/s (collection: 0.640s, learning 3.932s)
               Value function loss: 237.2381
                    Surrogate loss: 0.0105
             Mean action noise std: 0.93
                       Mean reward: 714.33
               Mean episode length: 413.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 4.57s
                        Total time: 5368.64s
                               ETA: 12712.1s

################################################################################
                     [1m Learning iteration 1188/4000 [0m

                       Computation: 1837 steps/s (collection: 0.557s, learning 3.902s)
               Value function loss: 165.8991
                    Surrogate loss: 0.0100
             Mean action noise std: 0.93
                       Mean reward: 720.02
               Mean episode length: 416.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9740288
                    Iteration time: 4.46s
                        Total time: 5373.10s
                               ETA: 12707.4s

################################################################################
                     [1m Learning iteration 1189/4000 [0m

                       Computation: 1811 steps/s (collection: 0.536s, learning 3.987s)
               Value function loss: 110.9617
                    Surrogate loss: 0.0111
             Mean action noise std: 0.93
                       Mean reward: 698.19
               Mean episode length: 403.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 4.52s
                        Total time: 5377.62s
                               ETA: 12702.9s

################################################################################
                     [1m Learning iteration 1190/4000 [0m

                       Computation: 1830 steps/s (collection: 0.584s, learning 3.891s)
               Value function loss: 119.8508
                    Surrogate loss: 0.0076
             Mean action noise std: 0.93
                       Mean reward: 692.03
               Mean episode length: 400.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 9756672
                    Iteration time: 4.47s
                        Total time: 5382.09s
                               ETA: 12698.3s

################################################################################
                     [1m Learning iteration 1191/4000 [0m

                       Computation: 1846 steps/s (collection: 0.528s, learning 3.909s)
               Value function loss: 149.6778
                    Surrogate loss: 0.0063
             Mean action noise std: 0.93
                       Mean reward: 686.08
               Mean episode length: 397.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 4.44s
                        Total time: 5386.53s
                               ETA: 12693.6s

################################################################################
                     [1m Learning iteration 1192/4000 [0m

                       Computation: 1824 steps/s (collection: 0.557s, learning 3.934s)
               Value function loss: 265.4386
                    Surrogate loss: 0.0110
             Mean action noise std: 0.93
                       Mean reward: 689.75
               Mean episode length: 400.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9773056
                    Iteration time: 4.49s
                        Total time: 5391.02s
                               ETA: 12689.0s

################################################################################
                     [1m Learning iteration 1193/4000 [0m

                       Computation: 1810 steps/s (collection: 0.582s, learning 3.942s)
               Value function loss: 358.0159
                    Surrogate loss: 0.0140
             Mean action noise std: 0.93
                       Mean reward: 705.06
               Mean episode length: 409.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 4.52s
                        Total time: 5395.55s
                               ETA: 12684.5s

################################################################################
                     [1m Learning iteration 1194/4000 [0m

                       Computation: 1829 steps/s (collection: 0.549s, learning 3.929s)
               Value function loss: 290.8714
                    Surrogate loss: 0.0124
             Mean action noise std: 0.93
                       Mean reward: 694.54
               Mean episode length: 404.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9789440
                    Iteration time: 4.48s
                        Total time: 5400.02s
                               ETA: 12679.9s

################################################################################
                     [1m Learning iteration 1195/4000 [0m

                       Computation: 1828 steps/s (collection: 0.559s, learning 3.920s)
               Value function loss: 259.5014
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 707.79
               Mean episode length: 411.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 4.48s
                        Total time: 5404.50s
                               ETA: 12675.3s

################################################################################
                     [1m Learning iteration 1196/4000 [0m

                       Computation: 1805 steps/s (collection: 0.597s, learning 3.940s)
               Value function loss: 149.8362
                    Surrogate loss: 0.0103
             Mean action noise std: 0.93
                       Mean reward: 686.95
               Mean episode length: 401.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9805824
                    Iteration time: 4.54s
                        Total time: 5409.04s
                               ETA: 12670.8s

################################################################################
                     [1m Learning iteration 1197/4000 [0m

                       Computation: 1809 steps/s (collection: 0.586s, learning 3.941s)
               Value function loss: 265.0403
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 679.90
               Mean episode length: 398.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 4.53s
                        Total time: 5413.57s
                               ETA: 12666.3s

################################################################################
                     [1m Learning iteration 1198/4000 [0m

                       Computation: 1795 steps/s (collection: 0.549s, learning 4.013s)
               Value function loss: 187.1212
                    Surrogate loss: 0.0117
             Mean action noise std: 0.93
                       Mean reward: 692.74
               Mean episode length: 405.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9822208
                    Iteration time: 4.56s
                        Total time: 5418.13s
                               ETA: 12661.9s

################################################################################
                     [1m Learning iteration 1199/4000 [0m

                       Computation: 1793 steps/s (collection: 0.602s, learning 3.967s)
               Value function loss: 249.8930
                    Surrogate loss: 0.0105
             Mean action noise std: 0.93
                       Mean reward: 719.55
               Mean episode length: 420.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 4.57s
                        Total time: 5422.70s
                               ETA: 12657.5s

################################################################################
                     [1m Learning iteration 1200/4000 [0m

                       Computation: 1823 steps/s (collection: 0.556s, learning 3.937s)
               Value function loss: 184.6550
                    Surrogate loss: 0.0105
             Mean action noise std: 0.93
                       Mean reward: 711.07
               Mean episode length: 416.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9838592
                    Iteration time: 4.49s
                        Total time: 5427.19s
                               ETA: 12652.9s

################################################################################
                     [1m Learning iteration 1201/4000 [0m

                       Computation: 1810 steps/s (collection: 0.568s, learning 3.957s)
               Value function loss: 167.5517
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 704.17
               Mean episode length: 412.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 4.53s
                        Total time: 5431.71s
                               ETA: 12648.4s

################################################################################
                     [1m Learning iteration 1202/4000 [0m

                       Computation: 1812 steps/s (collection: 0.557s, learning 3.964s)
               Value function loss: 265.9104
                    Surrogate loss: 0.0110
             Mean action noise std: 0.93
                       Mean reward: 706.34
               Mean episode length: 414.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9854976
                    Iteration time: 4.52s
                        Total time: 5436.24s
                               ETA: 12643.9s

################################################################################
                     [1m Learning iteration 1203/4000 [0m

                       Computation: 1789 steps/s (collection: 0.635s, learning 3.943s)
               Value function loss: 168.5913
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 703.98
               Mean episode length: 413.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 4.58s
                        Total time: 5440.81s
                               ETA: 12639.5s

################################################################################
                     [1m Learning iteration 1204/4000 [0m

                       Computation: 1799 steps/s (collection: 0.623s, learning 3.930s)
               Value function loss: 282.7935
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 716.12
               Mean episode length: 421.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9871360
                    Iteration time: 4.55s
                        Total time: 5445.37s
                               ETA: 12635.1s

################################################################################
                     [1m Learning iteration 1205/4000 [0m

                       Computation: 1818 steps/s (collection: 0.559s, learning 3.946s)
               Value function loss: 141.4765
                    Surrogate loss: 0.0094
             Mean action noise std: 0.93
                       Mean reward: 715.22
               Mean episode length: 421.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 4.50s
                        Total time: 5449.87s
                               ETA: 12630.5s

################################################################################
                     [1m Learning iteration 1206/4000 [0m

                       Computation: 1806 steps/s (collection: 0.577s, learning 3.958s)
               Value function loss: 141.3545
                    Surrogate loss: 0.0101
             Mean action noise std: 0.93
                       Mean reward: 706.33
               Mean episode length: 418.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 9887744
                    Iteration time: 4.54s
                        Total time: 5454.41s
                               ETA: 12626.0s

################################################################################
                     [1m Learning iteration 1207/4000 [0m

                       Computation: 1824 steps/s (collection: 0.562s, learning 3.928s)
               Value function loss: 148.1473
                    Surrogate loss: 0.0109
             Mean action noise std: 0.93
                       Mean reward: 707.11
               Mean episode length: 419.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 4.49s
                        Total time: 5458.90s
                               ETA: 12621.4s

################################################################################
                     [1m Learning iteration 1208/4000 [0m

                       Computation: 1836 steps/s (collection: 0.536s, learning 3.925s)
               Value function loss: 276.3663
                    Surrogate loss: 0.0100
             Mean action noise std: 0.93
                       Mean reward: 700.68
               Mean episode length: 415.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 9904128
                    Iteration time: 4.46s
                        Total time: 5463.36s
                               ETA: 12616.8s

################################################################################
                     [1m Learning iteration 1209/4000 [0m

                       Computation: 1851 steps/s (collection: 0.530s, learning 3.895s)
               Value function loss: 283.9672
                    Surrogate loss: 0.0087
             Mean action noise std: 0.93
                       Mean reward: 696.85
               Mean episode length: 416.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 4.42s
                        Total time: 5467.78s
                               ETA: 12612.0s

################################################################################
                     [1m Learning iteration 1210/4000 [0m

                       Computation: 1826 steps/s (collection: 0.576s, learning 3.907s)
               Value function loss: 280.0119
                    Surrogate loss: 0.0089
             Mean action noise std: 0.93
                       Mean reward: 704.52
               Mean episode length: 423.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9920512
                    Iteration time: 4.48s
                        Total time: 5472.27s
                               ETA: 12607.4s

################################################################################
                     [1m Learning iteration 1211/4000 [0m

                       Computation: 1820 steps/s (collection: 0.593s, learning 3.907s)
               Value function loss: 239.3472
                    Surrogate loss: 0.0082
             Mean action noise std: 0.93
                       Mean reward: 701.34
               Mean episode length: 421.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 4.50s
                        Total time: 5476.77s
                               ETA: 12602.9s

################################################################################
                     [1m Learning iteration 1212/4000 [0m

                       Computation: 1817 steps/s (collection: 0.603s, learning 3.905s)
               Value function loss: 255.1673
                    Surrogate loss: 0.0097
             Mean action noise std: 0.93
                       Mean reward: 714.83
               Mean episode length: 429.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9936896
                    Iteration time: 4.51s
                        Total time: 5481.27s
                               ETA: 12598.3s

################################################################################
                     [1m Learning iteration 1213/4000 [0m

                       Computation: 1821 steps/s (collection: 0.581s, learning 3.916s)
               Value function loss: 334.5227
                    Surrogate loss: 0.0084
             Mean action noise std: 0.93
                       Mean reward: 723.98
               Mean episode length: 434.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 4.50s
                        Total time: 5485.77s
                               ETA: 12593.8s

################################################################################
                     [1m Learning iteration 1214/4000 [0m

                       Computation: 1853 steps/s (collection: 0.515s, learning 3.906s)
               Value function loss: 264.2207
                    Surrogate loss: 0.0114
             Mean action noise std: 0.93
                       Mean reward: 716.44
               Mean episode length: 431.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9953280
                    Iteration time: 4.42s
                        Total time: 5490.19s
                               ETA: 12589.0s

################################################################################
                     [1m Learning iteration 1215/4000 [0m

                       Computation: 1831 steps/s (collection: 0.570s, learning 3.903s)
               Value function loss: 249.6331
                    Surrogate loss: 0.0107
             Mean action noise std: 0.93
                       Mean reward: 720.69
               Mean episode length: 431.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 4.47s
                        Total time: 5494.66s
                               ETA: 12584.4s

################################################################################
                     [1m Learning iteration 1216/4000 [0m

                       Computation: 1857 steps/s (collection: 0.526s, learning 3.885s)
               Value function loss: 170.9316
                    Surrogate loss: 0.0093
             Mean action noise std: 0.93
                       Mean reward: 719.59
               Mean episode length: 428.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9969664
                    Iteration time: 4.41s
                        Total time: 5499.07s
                               ETA: 12579.6s

################################################################################
                     [1m Learning iteration 1217/4000 [0m

                       Computation: 1843 steps/s (collection: 0.539s, learning 3.903s)
               Value function loss: 204.8329
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 725.07
               Mean episode length: 431.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 4.44s
                        Total time: 5503.52s
                               ETA: 12574.9s

################################################################################
                     [1m Learning iteration 1218/4000 [0m

                       Computation: 1823 steps/s (collection: 0.541s, learning 3.952s)
               Value function loss: 231.4420
                    Surrogate loss: 0.0113
             Mean action noise std: 0.93
                       Mean reward: 738.26
               Mean episode length: 436.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9986048
                    Iteration time: 4.49s
                        Total time: 5508.01s
                               ETA: 12570.4s

################################################################################
                     [1m Learning iteration 1219/4000 [0m

                       Computation: 1854 steps/s (collection: 0.536s, learning 3.881s)
               Value function loss: 218.6022
                    Surrogate loss: 0.0096
             Mean action noise std: 0.93
                       Mean reward: 728.85
               Mean episode length: 429.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 4.42s
                        Total time: 5512.43s
                               ETA: 12565.6s

################################################################################
                     [1m Learning iteration 1220/4000 [0m

                       Computation: 1852 steps/s (collection: 0.522s, learning 3.900s)
               Value function loss: 185.2542
                    Surrogate loss: 0.0081
             Mean action noise std: 0.93
                       Mean reward: 734.83
               Mean episode length: 432.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10002432
                    Iteration time: 4.42s
                        Total time: 5516.85s
                               ETA: 12560.9s

################################################################################
                     [1m Learning iteration 1221/4000 [0m

                       Computation: 1817 steps/s (collection: 0.611s, learning 3.896s)
               Value function loss: 120.3562
                    Surrogate loss: 0.0080
             Mean action noise std: 0.93
                       Mean reward: 731.04
               Mean episode length: 429.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 4.51s
                        Total time: 5521.35s
                               ETA: 12556.3s

################################################################################
                     [1m Learning iteration 1222/4000 [0m

                       Computation: 1843 steps/s (collection: 0.550s, learning 3.895s)
               Value function loss: 187.4648
                    Surrogate loss: 0.0092
             Mean action noise std: 0.93
                       Mean reward: 722.57
               Mean episode length: 423.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10018816
                    Iteration time: 4.44s
                        Total time: 5525.80s
                               ETA: 12551.7s

################################################################################
                     [1m Learning iteration 1223/4000 [0m

                       Computation: 1815 steps/s (collection: 0.565s, learning 3.947s)
               Value function loss: 288.1664
                    Surrogate loss: 0.0105
             Mean action noise std: 0.92
                       Mean reward: 726.27
               Mean episode length: 423.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 4.51s
                        Total time: 5530.31s
                               ETA: 12547.1s

################################################################################
                     [1m Learning iteration 1224/4000 [0m

                       Computation: 1848 steps/s (collection: 0.528s, learning 3.903s)
               Value function loss: 377.3908
                    Surrogate loss: 0.0094
             Mean action noise std: 0.93
                       Mean reward: 732.39
               Mean episode length: 426.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10035200
                    Iteration time: 4.43s
                        Total time: 5534.74s
                               ETA: 12542.4s

################################################################################
                     [1m Learning iteration 1225/4000 [0m

                       Computation: 1831 steps/s (collection: 0.559s, learning 3.915s)
               Value function loss: 247.6664
                    Surrogate loss: 0.0126
             Mean action noise std: 0.92
                       Mean reward: 739.19
               Mean episode length: 429.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 4.47s
                        Total time: 5539.22s
                               ETA: 12537.8s

################################################################################
                     [1m Learning iteration 1226/4000 [0m

                       Computation: 1856 steps/s (collection: 0.538s, learning 3.875s)
               Value function loss: 171.8815
                    Surrogate loss: 0.0085
             Mean action noise std: 0.93
                       Mean reward: 727.14
               Mean episode length: 426.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10051584
                    Iteration time: 4.41s
                        Total time: 5543.63s
                               ETA: 12533.0s

################################################################################
                     [1m Learning iteration 1227/4000 [0m

                       Computation: 1814 steps/s (collection: 0.514s, learning 4.000s)
               Value function loss: 257.2100
                    Surrogate loss: 0.0089
             Mean action noise std: 0.93
                       Mean reward: 719.88
               Mean episode length: 422.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 4.51s
                        Total time: 5548.14s
                               ETA: 12528.5s

################################################################################
                     [1m Learning iteration 1228/4000 [0m

                       Computation: 1807 steps/s (collection: 0.570s, learning 3.964s)
               Value function loss: 335.5633
                    Surrogate loss: 0.0073
             Mean action noise std: 0.93
                       Mean reward: 718.85
               Mean episode length: 424.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10067968
                    Iteration time: 4.53s
                        Total time: 5552.68s
                               ETA: 12524.0s

################################################################################
                     [1m Learning iteration 1229/4000 [0m

                       Computation: 1827 steps/s (collection: 0.513s, learning 3.970s)
               Value function loss: 156.1076
                    Surrogate loss: 0.0096
             Mean action noise std: 0.93
                       Mean reward: 728.02
               Mean episode length: 428.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 4.48s
                        Total time: 5557.16s
                               ETA: 12519.4s

################################################################################
                     [1m Learning iteration 1230/4000 [0m

                       Computation: 1792 steps/s (collection: 0.558s, learning 4.011s)
               Value function loss: 216.0082
                    Surrogate loss: 0.0081
             Mean action noise std: 0.93
                       Mean reward: 722.26
               Mean episode length: 426.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10084352
                    Iteration time: 4.57s
                        Total time: 5561.73s
                               ETA: 12515.0s

################################################################################
                     [1m Learning iteration 1231/4000 [0m

                       Computation: 1798 steps/s (collection: 0.506s, learning 4.049s)
               Value function loss: 176.5960
                    Surrogate loss: 0.0116
             Mean action noise std: 0.93
                       Mean reward: 736.52
               Mean episode length: 435.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 4.56s
                        Total time: 5566.28s
                               ETA: 12510.6s

################################################################################
                     [1m Learning iteration 1232/4000 [0m

                       Computation: 1793 steps/s (collection: 0.572s, learning 3.996s)
               Value function loss: 102.6189
                    Surrogate loss: 0.0099
             Mean action noise std: 0.93
                       Mean reward: 737.99
               Mean episode length: 436.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 10100736
                    Iteration time: 4.57s
                        Total time: 5570.85s
                               ETA: 12506.2s

################################################################################
                     [1m Learning iteration 1233/4000 [0m

                       Computation: 1829 steps/s (collection: 0.525s, learning 3.953s)
               Value function loss: 238.2911
                    Surrogate loss: 0.0086
             Mean action noise std: 0.93
                       Mean reward: 744.47
               Mean episode length: 441.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 4.48s
                        Total time: 5575.33s
                               ETA: 12501.6s

################################################################################
                     [1m Learning iteration 1234/4000 [0m

                       Computation: 1831 steps/s (collection: 0.555s, learning 3.919s)
               Value function loss: 216.8496
                    Surrogate loss: 0.0075
             Mean action noise std: 0.92
                       Mean reward: 709.07
               Mean episode length: 424.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10117120
                    Iteration time: 4.47s
                        Total time: 5579.80s
                               ETA: 12497.0s

################################################################################
                     [1m Learning iteration 1235/4000 [0m

                       Computation: 1809 steps/s (collection: 0.578s, learning 3.949s)
               Value function loss: 182.2984
                    Surrogate loss: 0.0086
             Mean action noise std: 0.92
                       Mean reward: 710.28
               Mean episode length: 425.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 4.53s
                        Total time: 5584.33s
                               ETA: 12492.5s

################################################################################
                     [1m Learning iteration 1236/4000 [0m

                       Computation: 1795 steps/s (collection: 0.636s, learning 3.928s)
               Value function loss: 148.0958
                    Surrogate loss: 0.0084
             Mean action noise std: 0.92
                       Mean reward: 719.07
               Mean episode length: 429.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10133504
                    Iteration time: 4.56s
                        Total time: 5588.89s
                               ETA: 12488.0s

################################################################################
                     [1m Learning iteration 1237/4000 [0m

                       Computation: 1833 steps/s (collection: 0.560s, learning 3.909s)
               Value function loss: 145.9253
                    Surrogate loss: 0.0081
             Mean action noise std: 0.92
                       Mean reward: 716.61
               Mean episode length: 426.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 4.47s
                        Total time: 5593.36s
                               ETA: 12483.4s

################################################################################
                     [1m Learning iteration 1238/4000 [0m

                       Computation: 1788 steps/s (collection: 0.631s, learning 3.949s)
               Value function loss: 149.9756
                    Surrogate loss: 0.0086
             Mean action noise std: 0.92
                       Mean reward: 715.39
               Mean episode length: 425.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10149888
                    Iteration time: 4.58s
                        Total time: 5597.94s
                               ETA: 12479.0s

################################################################################
                     [1m Learning iteration 1239/4000 [0m

                       Computation: 1796 steps/s (collection: 0.591s, learning 3.969s)
               Value function loss: 325.8141
                    Surrogate loss: 0.0111
             Mean action noise std: 0.92
                       Mean reward: 720.61
               Mean episode length: 429.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 4.56s
                        Total time: 5602.50s
                               ETA: 12474.6s

################################################################################
                     [1m Learning iteration 1240/4000 [0m

                       Computation: 1805 steps/s (collection: 0.611s, learning 3.927s)
               Value function loss: 267.2432
                    Surrogate loss: 0.0116
             Mean action noise std: 0.92
                       Mean reward: 729.83
               Mean episode length: 435.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10166272
                    Iteration time: 4.54s
                        Total time: 5607.04s
                               ETA: 12470.1s

################################################################################
                     [1m Learning iteration 1241/4000 [0m

                       Computation: 1804 steps/s (collection: 0.618s, learning 3.922s)
               Value function loss: 283.2354
                    Surrogate loss: 0.0103
             Mean action noise std: 0.92
                       Mean reward: 720.62
               Mean episode length: 430.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 4.54s
                        Total time: 5611.58s
                               ETA: 12465.7s

################################################################################
                     [1m Learning iteration 1242/4000 [0m

                       Computation: 1828 steps/s (collection: 0.564s, learning 3.916s)
               Value function loss: 205.4557
                    Surrogate loss: 0.0107
             Mean action noise std: 0.92
                       Mean reward: 705.21
               Mean episode length: 422.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10182656
                    Iteration time: 4.48s
                        Total time: 5616.06s
                               ETA: 12461.1s

################################################################################
                     [1m Learning iteration 1243/4000 [0m

                       Computation: 1823 steps/s (collection: 0.563s, learning 3.930s)
               Value function loss: 274.9142
                    Surrogate loss: 0.0114
             Mean action noise std: 0.92
                       Mean reward: 703.17
               Mean episode length: 419.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 4.49s
                        Total time: 5620.55s
                               ETA: 12456.5s

################################################################################
                     [1m Learning iteration 1244/4000 [0m

                       Computation: 1829 steps/s (collection: 0.577s, learning 3.900s)
               Value function loss: 279.1671
                    Surrogate loss: 0.0127
             Mean action noise std: 0.92
                       Mean reward: 723.94
               Mean episode length: 430.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10199040
                    Iteration time: 4.48s
                        Total time: 5625.03s
                               ETA: 12451.9s

################################################################################
                     [1m Learning iteration 1245/4000 [0m

                       Computation: 1794 steps/s (collection: 0.598s, learning 3.966s)
               Value function loss: 155.7077
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 692.38
               Mean episode length: 411.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 4.56s
                        Total time: 5629.59s
                               ETA: 12447.5s

################################################################################
                     [1m Learning iteration 1246/4000 [0m

                       Computation: 1816 steps/s (collection: 0.595s, learning 3.915s)
               Value function loss: 138.8644
                    Surrogate loss: 0.0100
             Mean action noise std: 0.92
                       Mean reward: 679.77
               Mean episode length: 404.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10215424
                    Iteration time: 4.51s
                        Total time: 5634.10s
                               ETA: 12442.9s

################################################################################
                     [1m Learning iteration 1247/4000 [0m

                       Computation: 1804 steps/s (collection: 0.580s, learning 3.961s)
               Value function loss: 209.0404
                    Surrogate loss: 0.0091
             Mean action noise std: 0.92
                       Mean reward: 674.21
               Mean episode length: 400.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 4.54s
                        Total time: 5638.65s
                               ETA: 12438.5s

################################################################################
                     [1m Learning iteration 1248/4000 [0m

                       Computation: 1785 steps/s (collection: 0.631s, learning 3.957s)
               Value function loss: 135.5835
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 673.38
               Mean episode length: 400.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10231808
                    Iteration time: 4.59s
                        Total time: 5643.23s
                               ETA: 12434.1s

################################################################################
                     [1m Learning iteration 1249/4000 [0m

                       Computation: 1811 steps/s (collection: 0.587s, learning 3.936s)
               Value function loss: 304.5349
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 665.67
               Mean episode length: 394.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 4.52s
                        Total time: 5647.76s
                               ETA: 12429.6s

################################################################################
                     [1m Learning iteration 1250/4000 [0m

                       Computation: 1803 steps/s (collection: 0.638s, learning 3.905s)
               Value function loss: 192.8315
                    Surrogate loss: 0.0125
             Mean action noise std: 0.93
                       Mean reward: 666.27
               Mean episode length: 394.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10248192
                    Iteration time: 4.54s
                        Total time: 5652.30s
                               ETA: 12425.1s

################################################################################
                     [1m Learning iteration 1251/4000 [0m

                       Computation: 1809 steps/s (collection: 0.608s, learning 3.919s)
               Value function loss: 156.8760
                    Surrogate loss: 0.0148
             Mean action noise std: 0.92
                       Mean reward: 666.72
               Mean episode length: 394.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 4.53s
                        Total time: 5656.83s
                               ETA: 12420.6s

################################################################################
                     [1m Learning iteration 1252/4000 [0m

                       Computation: 1793 steps/s (collection: 0.580s, learning 3.988s)
               Value function loss: 106.8503
                    Surrogate loss: 0.0108
             Mean action noise std: 0.92
                       Mean reward: 672.49
               Mean episode length: 397.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10264576
                    Iteration time: 4.57s
                        Total time: 5661.39s
                               ETA: 12416.2s

################################################################################
                     [1m Learning iteration 1253/4000 [0m

                       Computation: 1788 steps/s (collection: 0.616s, learning 3.965s)
               Value function loss: 215.2835
                    Surrogate loss: 0.0106
             Mean action noise std: 0.92
                       Mean reward: 653.89
               Mean episode length: 388.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 4.58s
                        Total time: 5665.97s
                               ETA: 12411.8s

################################################################################
                     [1m Learning iteration 1254/4000 [0m

                       Computation: 1808 steps/s (collection: 0.592s, learning 3.939s)
               Value function loss: 196.7603
                    Surrogate loss: 0.0097
             Mean action noise std: 0.92
                       Mean reward: 655.28
               Mean episode length: 390.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10280960
                    Iteration time: 4.53s
                        Total time: 5670.50s
                               ETA: 12407.3s

################################################################################
                     [1m Learning iteration 1255/4000 [0m

                       Computation: 1778 steps/s (collection: 0.617s, learning 3.989s)
               Value function loss: 158.2896
                    Surrogate loss: 0.0071
             Mean action noise std: 0.92
                       Mean reward: 648.67
               Mean episode length: 387.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 4.61s
                        Total time: 5675.11s
                               ETA: 12403.0s

################################################################################
                     [1m Learning iteration 1256/4000 [0m

                       Computation: 1803 steps/s (collection: 0.590s, learning 3.953s)
               Value function loss: 186.4181
                    Surrogate loss: 0.0076
             Mean action noise std: 0.92
                       Mean reward: 660.33
               Mean episode length: 395.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10297344
                    Iteration time: 4.54s
                        Total time: 5679.65s
                               ETA: 12398.5s

################################################################################
                     [1m Learning iteration 1257/4000 [0m

                       Computation: 1820 steps/s (collection: 0.572s, learning 3.927s)
               Value function loss: 262.0123
                    Surrogate loss: 0.0120
             Mean action noise std: 0.92
                       Mean reward: 673.59
               Mean episode length: 404.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 4.50s
                        Total time: 5684.15s
                               ETA: 12394.0s

################################################################################
                     [1m Learning iteration 1258/4000 [0m

                       Computation: 1828 steps/s (collection: 0.563s, learning 3.916s)
               Value function loss: 200.1726
                    Surrogate loss: 0.0118
             Mean action noise std: 0.92
                       Mean reward: 692.83
               Mean episode length: 415.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10313728
                    Iteration time: 4.48s
                        Total time: 5688.63s
                               ETA: 12389.4s

################################################################################
                     [1m Learning iteration 1259/4000 [0m

                       Computation: 1803 steps/s (collection: 0.585s, learning 3.956s)
               Value function loss: 229.8045
                    Surrogate loss: 0.0105
             Mean action noise std: 0.92
                       Mean reward: 699.65
               Mean episode length: 421.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 4.54s
                        Total time: 5693.17s
                               ETA: 12384.9s

################################################################################
                     [1m Learning iteration 1260/4000 [0m

                       Computation: 1817 steps/s (collection: 0.594s, learning 3.913s)
               Value function loss: 254.7853
                    Surrogate loss: 0.0115
             Mean action noise std: 0.92
                       Mean reward: 693.61
               Mean episode length: 416.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10330112
                    Iteration time: 4.51s
                        Total time: 5697.68s
                               ETA: 12380.4s

################################################################################
                     [1m Learning iteration 1261/4000 [0m

                       Computation: 1797 steps/s (collection: 0.607s, learning 3.950s)
               Value function loss: 274.1165
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 702.21
               Mean episode length: 422.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 4.56s
                        Total time: 5702.24s
                               ETA: 12375.9s

################################################################################
                     [1m Learning iteration 1262/4000 [0m

                       Computation: 1828 steps/s (collection: 0.571s, learning 3.909s)
               Value function loss: 138.9691
                    Surrogate loss: 0.0098
             Mean action noise std: 0.92
                       Mean reward: 691.31
               Mean episode length: 415.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10346496
                    Iteration time: 4.48s
                        Total time: 5706.72s
                               ETA: 12371.3s

################################################################################
                     [1m Learning iteration 1263/4000 [0m

                       Computation: 1789 steps/s (collection: 0.612s, learning 3.966s)
               Value function loss: 208.5322
                    Surrogate loss: 0.0114
             Mean action noise std: 0.92
                       Mean reward: 682.33
               Mean episode length: 410.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 4.58s
                        Total time: 5711.30s
                               ETA: 12366.9s

################################################################################
                     [1m Learning iteration 1264/4000 [0m

                       Computation: 1824 steps/s (collection: 0.557s, learning 3.932s)
               Value function loss: 264.4375
                    Surrogate loss: 0.0083
             Mean action noise std: 0.92
                       Mean reward: 675.47
               Mean episode length: 405.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10362880
                    Iteration time: 4.49s
                        Total time: 5715.78s
                               ETA: 12362.4s

################################################################################
                     [1m Learning iteration 1265/4000 [0m

                       Computation: 1788 steps/s (collection: 0.644s, learning 3.937s)
               Value function loss: 242.9334
                    Surrogate loss: 0.0099
             Mean action noise std: 0.92
                       Mean reward: 685.45
               Mean episode length: 412.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 4.58s
                        Total time: 5720.37s
                               ETA: 12358.0s

################################################################################
                     [1m Learning iteration 1266/4000 [0m

                       Computation: 1828 steps/s (collection: 0.562s, learning 3.919s)
               Value function loss: 184.9856
                    Surrogate loss: 0.0095
             Mean action noise std: 0.92
                       Mean reward: 693.73
               Mean episode length: 418.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10379264
                    Iteration time: 4.48s
                        Total time: 5724.85s
                               ETA: 12353.4s

################################################################################
                     [1m Learning iteration 1267/4000 [0m

                       Computation: 1788 steps/s (collection: 0.620s, learning 3.960s)
               Value function loss: 177.3726
                    Surrogate loss: 0.0101
             Mean action noise std: 0.92
                       Mean reward: 686.64
               Mean episode length: 414.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 4.58s
                        Total time: 5729.43s
                               ETA: 12349.0s

################################################################################
                     [1m Learning iteration 1268/4000 [0m

                       Computation: 1808 steps/s (collection: 0.620s, learning 3.909s)
               Value function loss: 110.1006
                    Surrogate loss: 0.0089
             Mean action noise std: 0.92
                       Mean reward: 668.90
               Mean episode length: 404.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10395648
                    Iteration time: 4.53s
                        Total time: 5733.96s
                               ETA: 12344.5s

################################################################################
                     [1m Learning iteration 1269/4000 [0m

                       Computation: 1816 steps/s (collection: 0.580s, learning 3.929s)
               Value function loss: 196.8596
                    Surrogate loss: 0.0074
             Mean action noise std: 0.92
                       Mean reward: 672.84
               Mean episode length: 409.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 4.51s
                        Total time: 5738.47s
                               ETA: 12340.0s

################################################################################
                     [1m Learning iteration 1270/4000 [0m

                       Computation: 1824 steps/s (collection: 0.569s, learning 3.921s)
               Value function loss: 228.7516
                    Surrogate loss: 0.0105
             Mean action noise std: 0.92
                       Mean reward: 659.31
               Mean episode length: 402.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10412032
                    Iteration time: 4.49s
                        Total time: 5742.96s
                               ETA: 12335.4s

################################################################################
                     [1m Learning iteration 1271/4000 [0m

                       Computation: 1804 steps/s (collection: 0.601s, learning 3.939s)
               Value function loss: 208.6121
                    Surrogate loss: 0.0089
             Mean action noise std: 0.93
                       Mean reward: 657.86
               Mean episode length: 403.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 4.54s
                        Total time: 5747.50s
                               ETA: 12330.9s

################################################################################
                     [1m Learning iteration 1272/4000 [0m

                       Computation: 1817 steps/s (collection: 0.608s, learning 3.898s)
               Value function loss: 293.3204
                    Surrogate loss: 0.0112
             Mean action noise std: 0.93
                       Mean reward: 668.96
               Mean episode length: 410.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10428416
                    Iteration time: 4.51s
                        Total time: 5752.00s
                               ETA: 12326.4s

################################################################################
                     [1m Learning iteration 1273/4000 [0m

                       Computation: 1806 steps/s (collection: 0.579s, learning 3.955s)
               Value function loss: 234.5417
                    Surrogate loss: 0.0085
             Mean action noise std: 0.93
                       Mean reward: 673.58
               Mean episode length: 411.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 4.53s
                        Total time: 5756.54s
                               ETA: 12321.9s

################################################################################
                     [1m Learning iteration 1274/4000 [0m

                       Computation: 1803 steps/s (collection: 0.624s, learning 3.918s)
               Value function loss: 208.1088
                    Surrogate loss: 0.0090
             Mean action noise std: 0.93
                       Mean reward: 672.63
               Mean episode length: 413.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10444800
                    Iteration time: 4.54s
                        Total time: 5761.08s
                               ETA: 12317.4s

################################################################################
                     [1m Learning iteration 1275/4000 [0m

                       Computation: 1844 steps/s (collection: 0.540s, learning 3.901s)
               Value function loss: 194.9218
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 658.48
               Mean episode length: 405.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 4.44s
                        Total time: 5765.52s
                               ETA: 12312.7s

################################################################################
                     [1m Learning iteration 1276/4000 [0m

                       Computation: 1817 steps/s (collection: 0.598s, learning 3.909s)
               Value function loss: 183.7279
                    Surrogate loss: 0.0093
             Mean action noise std: 0.93
                       Mean reward: 643.03
               Mean episode length: 396.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10461184
                    Iteration time: 4.51s
                        Total time: 5770.03s
                               ETA: 12308.2s

################################################################################
                     [1m Learning iteration 1277/4000 [0m

                       Computation: 1832 steps/s (collection: 0.573s, learning 3.897s)
               Value function loss: 164.2788
                    Surrogate loss: 0.0073
             Mean action noise std: 0.93
                       Mean reward: 660.90
               Mean episode length: 406.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 4.47s
                        Total time: 5774.50s
                               ETA: 12303.6s

################################################################################
                     [1m Learning iteration 1278/4000 [0m

                       Computation: 1783 steps/s (collection: 0.647s, learning 3.947s)
               Value function loss: 182.1391
                    Surrogate loss: 0.0081
             Mean action noise std: 0.93
                       Mean reward: 665.33
               Mean episode length: 408.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10477568
                    Iteration time: 4.59s
                        Total time: 5779.09s
                               ETA: 12299.2s

################################################################################
                     [1m Learning iteration 1279/4000 [0m

                       Computation: 1837 steps/s (collection: 0.522s, learning 3.936s)
               Value function loss: 159.9518
                    Surrogate loss: 0.0080
             Mean action noise std: 0.93
                       Mean reward: 682.95
               Mean episode length: 418.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 4.46s
                        Total time: 5783.55s
                               ETA: 12294.6s

################################################################################
                     [1m Learning iteration 1280/4000 [0m

                       Computation: 1799 steps/s (collection: 0.559s, learning 3.992s)
               Value function loss: 234.2067
                    Surrogate loss: 0.0086
             Mean action noise std: 0.93
                       Mean reward: 684.55
               Mean episode length: 418.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10493952
                    Iteration time: 4.55s
                        Total time: 5788.10s
                               ETA: 12290.1s

################################################################################
                     [1m Learning iteration 1281/4000 [0m

                       Computation: 1805 steps/s (collection: 0.571s, learning 3.966s)
               Value function loss: 140.1444
                    Surrogate loss: 0.0098
             Mean action noise std: 0.93
                       Mean reward: 649.24
               Mean episode length: 397.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 4.54s
                        Total time: 5792.64s
                               ETA: 12285.6s

################################################################################
                     [1m Learning iteration 1282/4000 [0m

                       Computation: 1827 steps/s (collection: 0.514s, learning 3.968s)
               Value function loss: 141.5935
                    Surrogate loss: 0.0056
             Mean action noise std: 0.93
                       Mean reward: 648.03
               Mean episode length: 398.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10510336
                    Iteration time: 4.48s
                        Total time: 5797.12s
                               ETA: 12281.0s

################################################################################
                     [1m Learning iteration 1283/4000 [0m

                       Computation: 1821 steps/s (collection: 0.561s, learning 3.938s)
               Value function loss: 167.3167
                    Surrogate loss: 0.0082
             Mean action noise std: 0.93
                       Mean reward: 647.78
               Mean episode length: 396.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 4.50s
                        Total time: 5801.62s
                               ETA: 12276.5s

################################################################################
                     [1m Learning iteration 1284/4000 [0m

                       Computation: 1837 steps/s (collection: 0.538s, learning 3.921s)
               Value function loss: 204.6444
                    Surrogate loss: 0.0088
             Mean action noise std: 0.93
                       Mean reward: 646.39
               Mean episode length: 395.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10526720
                    Iteration time: 4.46s
                        Total time: 5806.08s
                               ETA: 12271.8s

################################################################################
                     [1m Learning iteration 1285/4000 [0m

                       Computation: 1843 steps/s (collection: 0.595s, learning 3.848s)
               Value function loss: 169.0257
                    Surrogate loss: 0.0081
             Mean action noise std: 0.93
                       Mean reward: 648.93
               Mean episode length: 396.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 4.44s
                        Total time: 5810.52s
                               ETA: 12267.2s

################################################################################
                     [1m Learning iteration 1286/4000 [0m

                       Computation: 1840 steps/s (collection: 0.550s, learning 3.900s)
               Value function loss: 158.7168
                    Surrogate loss: 0.0085
             Mean action noise std: 0.93
                       Mean reward: 629.86
               Mean episode length: 383.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10543104
                    Iteration time: 4.45s
                        Total time: 5814.97s
                               ETA: 12262.5s

################################################################################
                     [1m Learning iteration 1287/4000 [0m

                       Computation: 1836 steps/s (collection: 0.545s, learning 3.915s)
               Value function loss: 169.7770
                    Surrogate loss: 0.0089
             Mean action noise std: 0.93
                       Mean reward: 612.37
               Mean episode length: 372.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 4.46s
                        Total time: 5819.43s
                               ETA: 12257.9s

################################################################################
                     [1m Learning iteration 1288/4000 [0m

                       Computation: 1824 steps/s (collection: 0.620s, learning 3.871s)
               Value function loss: 294.7499
                    Surrogate loss: 0.0090
             Mean action noise std: 0.93
                       Mean reward: 603.15
               Mean episode length: 364.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10559488
                    Iteration time: 4.49s
                        Total time: 5823.92s
                               ETA: 12253.3s

################################################################################
                     [1m Learning iteration 1289/4000 [0m

                       Computation: 1827 steps/s (collection: 0.564s, learning 3.919s)
               Value function loss: 249.2252
                    Surrogate loss: 0.0083
             Mean action noise std: 0.93
                       Mean reward: 606.73
               Mean episode length: 364.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 4.48s
                        Total time: 5828.40s
                               ETA: 12248.7s

################################################################################
                     [1m Learning iteration 1290/4000 [0m

                       Computation: 1813 steps/s (collection: 0.639s, learning 3.878s)
               Value function loss: 255.2674
                    Surrogate loss: 0.0107
             Mean action noise std: 0.92
                       Mean reward: 619.56
               Mean episode length: 370.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10575872
                    Iteration time: 4.52s
                        Total time: 5832.92s
                               ETA: 12244.2s

################################################################################
                     [1m Learning iteration 1291/4000 [0m

                       Computation: 1861 steps/s (collection: 0.539s, learning 3.862s)
               Value function loss: 191.7348
                    Surrogate loss: 0.0095
             Mean action noise std: 0.92
                       Mean reward: 618.70
               Mean episode length: 370.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 4.40s
                        Total time: 5837.32s
                               ETA: 12239.4s

################################################################################
                     [1m Learning iteration 1292/4000 [0m

                       Computation: 1830 steps/s (collection: 0.524s, learning 3.950s)
               Value function loss: 219.5861
                    Surrogate loss: 0.0108
             Mean action noise std: 0.92
                       Mean reward: 625.12
               Mean episode length: 374.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10592256
                    Iteration time: 4.47s
                        Total time: 5841.80s
                               ETA: 12234.8s

################################################################################
                     [1m Learning iteration 1293/4000 [0m

                       Computation: 1813 steps/s (collection: 0.585s, learning 3.933s)
               Value function loss: 150.8660
                    Surrogate loss: 0.0115
             Mean action noise std: 0.92
                       Mean reward: 631.98
               Mean episode length: 376.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 4.52s
                        Total time: 5846.31s
                               ETA: 12230.3s

################################################################################
                     [1m Learning iteration 1294/4000 [0m

                       Computation: 1830 steps/s (collection: 0.547s, learning 3.927s)
               Value function loss: 126.6786
                    Surrogate loss: 0.0099
             Mean action noise std: 0.92
                       Mean reward: 637.00
               Mean episode length: 380.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10608640
                    Iteration time: 4.47s
                        Total time: 5850.79s
                               ETA: 12225.7s

################################################################################
                     [1m Learning iteration 1295/4000 [0m

                       Computation: 1824 steps/s (collection: 0.569s, learning 3.920s)
               Value function loss: 157.3157
                    Surrogate loss: 0.0103
             Mean action noise std: 0.93
                       Mean reward: 608.91
               Mean episode length: 364.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 4.49s
                        Total time: 5855.28s
                               ETA: 12221.1s

################################################################################
                     [1m Learning iteration 1296/4000 [0m

                       Computation: 1842 steps/s (collection: 0.553s, learning 3.893s)
               Value function loss: 287.8977
                    Surrogate loss: 0.0096
             Mean action noise std: 0.93
                       Mean reward: 608.27
               Mean episode length: 366.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10625024
                    Iteration time: 4.45s
                        Total time: 5859.72s
                               ETA: 12216.4s

################################################################################
                     [1m Learning iteration 1297/4000 [0m

                       Computation: 1830 steps/s (collection: 0.577s, learning 3.897s)
               Value function loss: 179.9233
                    Surrogate loss: 0.0118
             Mean action noise std: 0.93
                       Mean reward: 585.49
               Mean episode length: 354.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 4.47s
                        Total time: 5864.20s
                               ETA: 12211.8s

################################################################################
                     [1m Learning iteration 1298/4000 [0m

                       Computation: 1864 steps/s (collection: 0.541s, learning 3.852s)
               Value function loss: 241.6333
                    Surrogate loss: 0.0122
             Mean action noise std: 0.93
                       Mean reward: 570.05
               Mean episode length: 347.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10641408
                    Iteration time: 4.39s
                        Total time: 5868.59s
                               ETA: 12207.0s

################################################################################
                     [1m Learning iteration 1299/4000 [0m

                       Computation: 1868 steps/s (collection: 0.537s, learning 3.846s)
               Value function loss: 161.2324
                    Surrogate loss: 0.0097
             Mean action noise std: 0.93
                       Mean reward: 580.29
               Mean episode length: 353.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 4.38s
                        Total time: 5872.97s
                               ETA: 12202.2s

################################################################################
                     [1m Learning iteration 1300/4000 [0m

                       Computation: 1837 steps/s (collection: 0.512s, learning 3.945s)
               Value function loss: 212.3566
                    Surrogate loss: 0.0100
             Mean action noise std: 0.93
                       Mean reward: 574.79
               Mean episode length: 350.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10657792
                    Iteration time: 4.46s
                        Total time: 5877.43s
                               ETA: 12197.6s

################################################################################
                     [1m Learning iteration 1301/4000 [0m

                       Computation: 1841 steps/s (collection: 0.563s, learning 3.885s)
               Value function loss: 225.6601
                    Surrogate loss: 0.0098
             Mean action noise std: 0.93
                       Mean reward: 543.35
               Mean episode length: 329.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 4.45s
                        Total time: 5881.88s
                               ETA: 12192.9s

################################################################################
                     [1m Learning iteration 1302/4000 [0m

                       Computation: 1848 steps/s (collection: 0.540s, learning 3.893s)
               Value function loss: 321.9856
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 535.55
               Mean episode length: 324.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 10674176
                    Iteration time: 4.43s
                        Total time: 5886.31s
                               ETA: 12188.2s

################################################################################
                     [1m Learning iteration 1303/4000 [0m

                       Computation: 1820 steps/s (collection: 0.590s, learning 3.910s)
               Value function loss: 227.6028
                    Surrogate loss: 0.0105
             Mean action noise std: 0.93
                       Mean reward: 562.44
               Mean episode length: 341.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 4.50s
                        Total time: 5890.81s
                               ETA: 12183.7s

################################################################################
                     [1m Learning iteration 1304/4000 [0m

                       Computation: 1849 steps/s (collection: 0.521s, learning 3.907s)
               Value function loss: 207.9403
                    Surrogate loss: 0.0098
             Mean action noise std: 0.93
                       Mean reward: 569.27
               Mean episode length: 343.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10690560
                    Iteration time: 4.43s
                        Total time: 5895.24s
                               ETA: 12179.0s

################################################################################
                     [1m Learning iteration 1305/4000 [0m

                       Computation: 1844 steps/s (collection: 0.524s, learning 3.918s)
               Value function loss: 222.6255
                    Surrogate loss: 0.0103
             Mean action noise std: 0.93
                       Mean reward: 577.87
               Mean episode length: 346.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 4.44s
                        Total time: 5899.68s
                               ETA: 12174.3s

################################################################################
                     [1m Learning iteration 1306/4000 [0m

                       Computation: 1834 steps/s (collection: 0.567s, learning 3.899s)
               Value function loss: 154.6718
                    Surrogate loss: 0.0095
             Mean action noise std: 0.93
                       Mean reward: 585.19
               Mean episode length: 350.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10706944
                    Iteration time: 4.47s
                        Total time: 5904.15s
                               ETA: 12169.7s

################################################################################
                     [1m Learning iteration 1307/4000 [0m

                       Computation: 1820 steps/s (collection: 0.554s, learning 3.946s)
               Value function loss: 189.3234
                    Surrogate loss: 0.0119
             Mean action noise std: 0.93
                       Mean reward: 586.74
               Mean episode length: 350.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 4.50s
                        Total time: 5908.65s
                               ETA: 12165.1s

################################################################################
                     [1m Learning iteration 1308/4000 [0m

                       Computation: 1838 steps/s (collection: 0.521s, learning 3.936s)
               Value function loss: 168.3283
                    Surrogate loss: 0.0095
             Mean action noise std: 0.93
                       Mean reward: 589.26
               Mean episode length: 352.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10723328
                    Iteration time: 4.46s
                        Total time: 5913.11s
                               ETA: 12160.5s

################################################################################
                     [1m Learning iteration 1309/4000 [0m

                       Computation: 1831 steps/s (collection: 0.505s, learning 3.969s)
               Value function loss: 148.1483
                    Surrogate loss: 0.0144
             Mean action noise std: 0.93
                       Mean reward: 602.43
               Mean episode length: 362.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 4.47s
                        Total time: 5917.58s
                               ETA: 12155.9s

################################################################################
                     [1m Learning iteration 1310/4000 [0m

                       Computation: 1829 steps/s (collection: 0.547s, learning 3.930s)
               Value function loss: 98.2660
                    Surrogate loss: 0.0108
             Mean action noise std: 0.93
                       Mean reward: 595.45
               Mean episode length: 360.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10739712
                    Iteration time: 4.48s
                        Total time: 5922.06s
                               ETA: 12151.3s

################################################################################
                     [1m Learning iteration 1311/4000 [0m

                       Computation: 1801 steps/s (collection: 0.597s, learning 3.950s)
               Value function loss: 159.3875
                    Surrogate loss: 0.0122
             Mean action noise std: 0.93
                       Mean reward: 605.14
               Mean episode length: 367.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 4.55s
                        Total time: 5926.60s
                               ETA: 12146.8s

################################################################################
                     [1m Learning iteration 1312/4000 [0m

                       Computation: 1833 steps/s (collection: 0.565s, learning 3.902s)
               Value function loss: 184.2907
                    Surrogate loss: 0.0112
             Mean action noise std: 0.93
                       Mean reward: 591.43
               Mean episode length: 360.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10756096
                    Iteration time: 4.47s
                        Total time: 5931.07s
                               ETA: 12142.2s

################################################################################
                     [1m Learning iteration 1313/4000 [0m

                       Computation: 1812 steps/s (collection: 0.537s, learning 3.982s)
               Value function loss: 148.6436
                    Surrogate loss: 0.0097
             Mean action noise std: 0.93
                       Mean reward: 594.37
               Mean episode length: 361.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 4.52s
                        Total time: 5935.59s
                               ETA: 12137.7s

################################################################################
                     [1m Learning iteration 1314/4000 [0m

                       Computation: 1833 steps/s (collection: 0.562s, learning 3.905s)
               Value function loss: 185.4302
                    Surrogate loss: 0.0105
             Mean action noise std: 0.93
                       Mean reward: 586.93
               Mean episode length: 359.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10772480
                    Iteration time: 4.47s
                        Total time: 5940.06s
                               ETA: 12133.1s

################################################################################
                     [1m Learning iteration 1315/4000 [0m

                       Computation: 1794 steps/s (collection: 0.606s, learning 3.959s)
               Value function loss: 112.7584
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 587.94
               Mean episode length: 361.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 4.56s
                        Total time: 5944.62s
                               ETA: 12128.7s

################################################################################
                     [1m Learning iteration 1316/4000 [0m

                       Computation: 1826 steps/s (collection: 0.548s, learning 3.938s)
               Value function loss: 157.9485
                    Surrogate loss: 0.0098
             Mean action noise std: 0.93
                       Mean reward: 596.92
               Mean episode length: 367.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10788864
                    Iteration time: 4.49s
                        Total time: 5949.11s
                               ETA: 12124.1s

################################################################################
                     [1m Learning iteration 1317/4000 [0m

                       Computation: 1768 steps/s (collection: 0.673s, learning 3.958s)
               Value function loss: 250.7788
                    Surrogate loss: 0.0095
             Mean action noise std: 0.93
                       Mean reward: 608.32
               Mean episode length: 375.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 4.63s
                        Total time: 5953.74s
                               ETA: 12119.8s

################################################################################
                     [1m Learning iteration 1318/4000 [0m

                       Computation: 1787 steps/s (collection: 0.637s, learning 3.945s)
               Value function loss: 325.6445
                    Surrogate loss: 0.0103
             Mean action noise std: 0.93
                       Mean reward: 606.40
               Mean episode length: 371.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 10805248
                    Iteration time: 4.58s
                        Total time: 5958.32s
                               ETA: 12115.4s

################################################################################
                     [1m Learning iteration 1319/4000 [0m

                       Computation: 1770 steps/s (collection: 0.678s, learning 3.949s)
               Value function loss: 226.7428
                    Surrogate loss: 0.0083
             Mean action noise std: 0.93
                       Mean reward: 625.40
               Mean episode length: 382.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 4.63s
                        Total time: 5962.95s
                               ETA: 12111.1s

################################################################################
                     [1m Learning iteration 1320/4000 [0m

                       Computation: 1849 steps/s (collection: 0.533s, learning 3.896s)
               Value function loss: 183.6510
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 635.13
               Mean episode length: 386.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10821632
                    Iteration time: 4.43s
                        Total time: 5967.38s
                               ETA: 12106.4s

################################################################################
                     [1m Learning iteration 1321/4000 [0m

                       Computation: 1815 steps/s (collection: 0.560s, learning 3.952s)
               Value function loss: 214.3601
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 652.70
               Mean episode length: 397.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 4.51s
                        Total time: 5971.89s
                               ETA: 12101.9s

################################################################################
                     [1m Learning iteration 1322/4000 [0m

                       Computation: 1824 steps/s (collection: 0.556s, learning 3.934s)
               Value function loss: 76.4341
                    Surrogate loss: 0.0200
             Mean action noise std: 0.93
                       Mean reward: 654.80
               Mean episode length: 399.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10838016
                    Iteration time: 4.49s
                        Total time: 5976.38s
                               ETA: 12097.3s

################################################################################
                     [1m Learning iteration 1323/4000 [0m

                       Computation: 1822 steps/s (collection: 0.575s, learning 3.921s)
               Value function loss: 164.1972
                    Surrogate loss: 0.0135
             Mean action noise std: 0.93
                       Mean reward: 663.12
               Mean episode length: 404.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 4.50s
                        Total time: 5980.87s
                               ETA: 12092.8s

################################################################################
                     [1m Learning iteration 1324/4000 [0m

                       Computation: 1834 steps/s (collection: 0.576s, learning 3.891s)
               Value function loss: 177.9872
                    Surrogate loss: 0.0095
             Mean action noise std: 0.93
                       Mean reward: 668.26
               Mean episode length: 406.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10854400
                    Iteration time: 4.47s
                        Total time: 5985.34s
                               ETA: 12088.1s

################################################################################
                     [1m Learning iteration 1325/4000 [0m

                       Computation: 1830 steps/s (collection: 0.522s, learning 3.953s)
               Value function loss: 166.4580
                    Surrogate loss: 0.0090
             Mean action noise std: 0.93
                       Mean reward: 664.05
               Mean episode length: 403.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 4.47s
                        Total time: 5989.82s
                               ETA: 12083.5s

################################################################################
                     [1m Learning iteration 1326/4000 [0m

                       Computation: 1835 steps/s (collection: 0.546s, learning 3.918s)
               Value function loss: 113.6956
                    Surrogate loss: 0.0085
             Mean action noise std: 0.93
                       Mean reward: 659.62
               Mean episode length: 401.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10870784
                    Iteration time: 4.46s
                        Total time: 5994.28s
                               ETA: 12078.9s

################################################################################
                     [1m Learning iteration 1327/4000 [0m

                       Computation: 1785 steps/s (collection: 0.645s, learning 3.943s)
               Value function loss: 192.7437
                    Surrogate loss: 0.0098
             Mean action noise std: 0.93
                       Mean reward: 671.79
               Mean episode length: 408.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 4.59s
                        Total time: 5998.87s
                               ETA: 12074.5s

################################################################################
                     [1m Learning iteration 1328/4000 [0m

                       Computation: 1802 steps/s (collection: 0.625s, learning 3.920s)
               Value function loss: 238.0490
                    Surrogate loss: 0.0081
             Mean action noise std: 0.93
                       Mean reward: 681.01
               Mean episode length: 414.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10887168
                    Iteration time: 4.55s
                        Total time: 6003.41s
                               ETA: 12070.1s

################################################################################
                     [1m Learning iteration 1329/4000 [0m

                       Computation: 1846 steps/s (collection: 0.563s, learning 3.873s)
               Value function loss: 202.1640
                    Surrogate loss: 0.0102
             Mean action noise std: 0.93
                       Mean reward: 680.71
               Mean episode length: 415.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 4.44s
                        Total time: 6007.85s
                               ETA: 12065.4s

################################################################################
                     [1m Learning iteration 1330/4000 [0m

                       Computation: 1823 steps/s (collection: 0.543s, learning 3.949s)
               Value function loss: 208.2670
                    Surrogate loss: 0.0094
             Mean action noise std: 0.93
                       Mean reward: 699.71
               Mean episode length: 425.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10903552
                    Iteration time: 4.49s
                        Total time: 6012.34s
                               ETA: 12060.8s

################################################################################
                     [1m Learning iteration 1331/4000 [0m

                       Computation: 1833 steps/s (collection: 0.557s, learning 3.910s)
               Value function loss: 212.3434
                    Surrogate loss: 0.0098
             Mean action noise std: 0.93
                       Mean reward: 694.77
               Mean episode length: 419.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 4.47s
                        Total time: 6016.81s
                               ETA: 12056.2s

################################################################################
                     [1m Learning iteration 1332/4000 [0m

                       Computation: 1769 steps/s (collection: 0.683s, learning 3.946s)
               Value function loss: 251.4180
                    Surrogate loss: 0.0101
             Mean action noise std: 0.93
                       Mean reward: 697.91
               Mean episode length: 421.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10919936
                    Iteration time: 4.63s
                        Total time: 6021.44s
                               ETA: 12051.9s

################################################################################
                     [1m Learning iteration 1333/4000 [0m

                       Computation: 1815 steps/s (collection: 0.618s, learning 3.894s)
               Value function loss: 281.2466
                    Surrogate loss: 0.0095
             Mean action noise std: 0.93
                       Mean reward: 710.60
               Mean episode length: 427.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 4.51s
                        Total time: 6025.95s
                               ETA: 12047.4s

################################################################################
                     [1m Learning iteration 1334/4000 [0m

                       Computation: 1794 steps/s (collection: 0.616s, learning 3.950s)
               Value function loss: 287.2943
                    Surrogate loss: 0.0080
             Mean action noise std: 0.93
                       Mean reward: 712.86
               Mean episode length: 427.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10936320
                    Iteration time: 4.57s
                        Total time: 6030.51s
                               ETA: 12043.0s

################################################################################
                     [1m Learning iteration 1335/4000 [0m

                       Computation: 1832 steps/s (collection: 0.552s, learning 3.918s)
               Value function loss: 233.5991
                    Surrogate loss: 0.0101
             Mean action noise std: 0.93
                       Mean reward: 734.48
               Mean episode length: 439.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 4.47s
                        Total time: 6034.98s
                               ETA: 12038.3s

################################################################################
                     [1m Learning iteration 1336/4000 [0m

                       Computation: 1825 steps/s (collection: 0.579s, learning 3.908s)
               Value function loss: 232.7445
                    Surrogate loss: 0.0067
             Mean action noise std: 0.93
                       Mean reward: 743.86
               Mean episode length: 445.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10952704
                    Iteration time: 4.49s
                        Total time: 6039.47s
                               ETA: 12033.8s

################################################################################
                     [1m Learning iteration 1337/4000 [0m

                       Computation: 1825 steps/s (collection: 0.583s, learning 3.904s)
               Value function loss: 234.0671
                    Surrogate loss: 0.0081
             Mean action noise std: 0.93
                       Mean reward: 746.92
               Mean episode length: 444.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 4.49s
                        Total time: 6043.96s
                               ETA: 12029.2s

################################################################################
                     [1m Learning iteration 1338/4000 [0m

                       Computation: 1841 steps/s (collection: 0.539s, learning 3.910s)
               Value function loss: 196.4543
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 732.00
               Mean episode length: 434.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10969088
                    Iteration time: 4.45s
                        Total time: 6048.41s
                               ETA: 12024.5s

################################################################################
                     [1m Learning iteration 1339/4000 [0m

                       Computation: 1833 steps/s (collection: 0.586s, learning 3.883s)
               Value function loss: 195.1186
                    Surrogate loss: 0.0095
             Mean action noise std: 0.93
                       Mean reward: 728.76
               Mean episode length: 433.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 4.47s
                        Total time: 6052.87s
                               ETA: 12019.9s

################################################################################
                     [1m Learning iteration 1340/4000 [0m

                       Computation: 1834 steps/s (collection: 0.572s, learning 3.893s)
               Value function loss: 168.3268
                    Surrogate loss: 0.0080
             Mean action noise std: 0.93
                       Mean reward: 714.31
               Mean episode length: 425.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10985472
                    Iteration time: 4.47s
                        Total time: 6057.34s
                               ETA: 12015.3s

################################################################################
                     [1m Learning iteration 1341/4000 [0m

                       Computation: 1824 steps/s (collection: 0.586s, learning 3.905s)
               Value function loss: 236.8218
                    Surrogate loss: 0.0080
             Mean action noise std: 0.93
                       Mean reward: 706.75
               Mean episode length: 420.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 4.49s
                        Total time: 6061.83s
                               ETA: 12010.7s

################################################################################
                     [1m Learning iteration 1342/4000 [0m

                       Computation: 1834 steps/s (collection: 0.572s, learning 3.895s)
               Value function loss: 147.0472
                    Surrogate loss: 0.0120
             Mean action noise std: 0.93
                       Mean reward: 705.44
               Mean episode length: 419.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11001856
                    Iteration time: 4.47s
                        Total time: 6066.30s
                               ETA: 12006.1s

################################################################################
                     [1m Learning iteration 1343/4000 [0m

                       Computation: 1817 steps/s (collection: 0.563s, learning 3.945s)
               Value function loss: 211.7142
                    Surrogate loss: 0.0123
             Mean action noise std: 0.93
                       Mean reward: 671.16
               Mean episode length: 400.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 4.51s
                        Total time: 6070.81s
                               ETA: 12001.6s

################################################################################
                     [1m Learning iteration 1344/4000 [0m

                       Computation: 1817 steps/s (collection: 0.549s, learning 3.958s)
               Value function loss: 165.1408
                    Surrogate loss: 0.0113
             Mean action noise std: 0.93
                       Mean reward: 668.04
               Mean episode length: 398.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11018240
                    Iteration time: 4.51s
                        Total time: 6075.31s
                               ETA: 11997.0s

################################################################################
                     [1m Learning iteration 1345/4000 [0m

                       Computation: 1824 steps/s (collection: 0.560s, learning 3.930s)
               Value function loss: 265.0127
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 675.21
               Mean episode length: 403.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 4.49s
                        Total time: 6079.80s
                               ETA: 11992.5s

################################################################################
                     [1m Learning iteration 1346/4000 [0m

                       Computation: 1814 steps/s (collection: 0.581s, learning 3.933s)
               Value function loss: 192.4624
                    Surrogate loss: 0.0081
             Mean action noise std: 0.93
                       Mean reward: 662.07
               Mean episode length: 394.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11034624
                    Iteration time: 4.51s
                        Total time: 6084.32s
                               ETA: 11988.0s

################################################################################
                     [1m Learning iteration 1347/4000 [0m

                       Computation: 1782 steps/s (collection: 0.619s, learning 3.977s)
               Value function loss: 156.8412
                    Surrogate loss: 0.0104
             Mean action noise std: 0.93
                       Mean reward: 653.92
               Mean episode length: 388.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 4.60s
                        Total time: 6088.91s
                               ETA: 11983.6s

################################################################################
                     [1m Learning iteration 1348/4000 [0m

                       Computation: 1836 steps/s (collection: 0.564s, learning 3.896s)
               Value function loss: 264.9894
                    Surrogate loss: 0.0103
             Mean action noise std: 0.93
                       Mean reward: 650.86
               Mean episode length: 389.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11051008
                    Iteration time: 4.46s
                        Total time: 6093.37s
                               ETA: 11979.0s

################################################################################
                     [1m Learning iteration 1349/4000 [0m

                       Computation: 1834 steps/s (collection: 0.568s, learning 3.898s)
               Value function loss: 277.8836
                    Surrogate loss: 0.0082
             Mean action noise std: 0.93
                       Mean reward: 667.32
               Mean episode length: 399.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 4.47s
                        Total time: 6097.84s
                               ETA: 11974.4s

################################################################################
                     [1m Learning iteration 1350/4000 [0m

                       Computation: 1816 steps/s (collection: 0.544s, learning 3.965s)
               Value function loss: 264.5859
                    Surrogate loss: 0.0095
             Mean action noise std: 0.93
                       Mean reward: 669.27
               Mean episode length: 399.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11067392
                    Iteration time: 4.51s
                        Total time: 6102.35s
                               ETA: 11969.8s

################################################################################
                     [1m Learning iteration 1351/4000 [0m

                       Computation: 1827 steps/s (collection: 0.565s, learning 3.917s)
               Value function loss: 194.7128
                    Surrogate loss: 0.0118
             Mean action noise std: 0.93
                       Mean reward: 664.88
               Mean episode length: 397.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 4.48s
                        Total time: 6106.83s
                               ETA: 11965.2s

################################################################################
                     [1m Learning iteration 1352/4000 [0m

                       Computation: 1834 steps/s (collection: 0.551s, learning 3.913s)
               Value function loss: 201.5895
                    Surrogate loss: 0.0104
             Mean action noise std: 0.93
                       Mean reward: 680.53
               Mean episode length: 408.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11083776
                    Iteration time: 4.46s
                        Total time: 6111.30s
                               ETA: 11960.6s

################################################################################
                     [1m Learning iteration 1353/4000 [0m

                       Computation: 1810 steps/s (collection: 0.583s, learning 3.941s)
               Value function loss: 144.7729
                    Surrogate loss: 0.0120
             Mean action noise std: 0.93
                       Mean reward: 676.30
               Mean episode length: 407.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 4.52s
                        Total time: 6115.82s
                               ETA: 11956.1s

################################################################################
                     [1m Learning iteration 1354/4000 [0m

                       Computation: 1795 steps/s (collection: 0.625s, learning 3.938s)
               Value function loss: 153.6528
                    Surrogate loss: 0.0076
             Mean action noise std: 0.93
                       Mean reward: 676.02
               Mean episode length: 407.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11100160
                    Iteration time: 4.56s
                        Total time: 6120.38s
                               ETA: 11951.7s

################################################################################
                     [1m Learning iteration 1355/4000 [0m

                       Computation: 1811 steps/s (collection: 0.562s, learning 3.961s)
               Value function loss: 199.7586
                    Surrogate loss: 0.0098
             Mean action noise std: 0.93
                       Mean reward: 674.58
               Mean episode length: 406.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 4.52s
                        Total time: 6124.91s
                               ETA: 11947.2s

################################################################################
                     [1m Learning iteration 1356/4000 [0m

                       Computation: 1775 steps/s (collection: 0.619s, learning 3.995s)
               Value function loss: 189.9847
                    Surrogate loss: 0.0089
             Mean action noise std: 0.93
                       Mean reward: 690.83
               Mean episode length: 415.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11116544
                    Iteration time: 4.61s
                        Total time: 6129.52s
                               ETA: 11942.9s

################################################################################
                     [1m Learning iteration 1357/4000 [0m

                       Computation: 1825 steps/s (collection: 0.547s, learning 3.940s)
               Value function loss: 168.0274
                    Surrogate loss: 0.0098
             Mean action noise std: 0.93
                       Mean reward: 675.98
               Mean episode length: 407.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 4.49s
                        Total time: 6134.01s
                               ETA: 11938.3s

################################################################################
                     [1m Learning iteration 1358/4000 [0m

                       Computation: 1815 steps/s (collection: 0.568s, learning 3.943s)
               Value function loss: 210.3105
                    Surrogate loss: 0.0089
             Mean action noise std: 0.93
                       Mean reward: 673.66
               Mean episode length: 406.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11132928
                    Iteration time: 4.51s
                        Total time: 6138.52s
                               ETA: 11933.8s

################################################################################
                     [1m Learning iteration 1359/4000 [0m

                       Computation: 1828 steps/s (collection: 0.568s, learning 3.913s)
               Value function loss: 165.1642
                    Surrogate loss: 0.0075
             Mean action noise std: 0.93
                       Mean reward: 676.32
               Mean episode length: 407.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 4.48s
                        Total time: 6143.00s
                               ETA: 11929.2s

################################################################################
                     [1m Learning iteration 1360/4000 [0m

                       Computation: 1800 steps/s (collection: 0.596s, learning 3.953s)
               Value function loss: 172.2986
                    Surrogate loss: 0.0092
             Mean action noise std: 0.93
                       Mean reward: 666.75
               Mean episode length: 400.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11149312
                    Iteration time: 4.55s
                        Total time: 6147.55s
                               ETA: 11924.7s

################################################################################
                     [1m Learning iteration 1361/4000 [0m

                       Computation: 1823 steps/s (collection: 0.528s, learning 3.965s)
               Value function loss: 191.5615
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 676.06
               Mean episode length: 406.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 4.49s
                        Total time: 6152.04s
                               ETA: 11920.1s

################################################################################
                     [1m Learning iteration 1362/4000 [0m

                       Computation: 1789 steps/s (collection: 0.619s, learning 3.959s)
               Value function loss: 247.3857
                    Surrogate loss: 0.0092
             Mean action noise std: 0.93
                       Mean reward: 679.29
               Mean episode length: 410.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11165696
                    Iteration time: 4.58s
                        Total time: 6156.62s
                               ETA: 11915.7s

################################################################################
                     [1m Learning iteration 1363/4000 [0m

                       Computation: 1793 steps/s (collection: 0.592s, learning 3.976s)
               Value function loss: 154.5377
                    Surrogate loss: 0.0073
             Mean action noise std: 0.93
                       Mean reward: 680.00
               Mean episode length: 409.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 4.57s
                        Total time: 6161.19s
                               ETA: 11911.3s

################################################################################
                     [1m Learning iteration 1364/4000 [0m

                       Computation: 1792 steps/s (collection: 0.558s, learning 4.011s)
               Value function loss: 228.2674
                    Surrogate loss: 0.0101
             Mean action noise std: 0.93
                       Mean reward: 674.94
               Mean episode length: 404.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11182080
                    Iteration time: 4.57s
                        Total time: 6165.76s
                               ETA: 11906.9s

################################################################################
                     [1m Learning iteration 1365/4000 [0m

                       Computation: 1802 steps/s (collection: 0.547s, learning 3.997s)
               Value function loss: 313.2131
                    Surrogate loss: 0.0095
             Mean action noise std: 0.93
                       Mean reward: 691.65
               Mean episode length: 415.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 4.54s
                        Total time: 6170.30s
                               ETA: 11902.4s

################################################################################
                     [1m Learning iteration 1366/4000 [0m

                       Computation: 1830 steps/s (collection: 0.566s, learning 3.909s)
               Value function loss: 256.4359
                    Surrogate loss: 0.0126
             Mean action noise std: 0.93
                       Mean reward: 691.93
               Mean episode length: 415.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11198464
                    Iteration time: 4.48s
                        Total time: 6174.78s
                               ETA: 11897.8s

################################################################################
                     [1m Learning iteration 1367/4000 [0m

                       Computation: 1790 steps/s (collection: 0.621s, learning 3.954s)
               Value function loss: 251.9261
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 709.63
               Mean episode length: 426.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 4.57s
                        Total time: 6179.35s
                               ETA: 11893.4s

################################################################################
                     [1m Learning iteration 1368/4000 [0m

                       Computation: 1838 steps/s (collection: 0.555s, learning 3.901s)
               Value function loss: 222.2585
                    Surrogate loss: 0.0096
             Mean action noise std: 0.93
                       Mean reward: 700.16
               Mean episode length: 421.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11214848
                    Iteration time: 4.46s
                        Total time: 6183.81s
                               ETA: 11888.8s

################################################################################
                     [1m Learning iteration 1369/4000 [0m

                       Computation: 1808 steps/s (collection: 0.582s, learning 3.947s)
               Value function loss: 138.0481
                    Surrogate loss: 0.0086
             Mean action noise std: 0.93
                       Mean reward: 701.00
               Mean episode length: 423.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 4.53s
                        Total time: 6188.34s
                               ETA: 11884.3s

################################################################################
                     [1m Learning iteration 1370/4000 [0m

                       Computation: 1808 steps/s (collection: 0.594s, learning 3.935s)
               Value function loss: 174.8744
                    Surrogate loss: 0.0110
             Mean action noise std: 0.93
                       Mean reward: 705.62
               Mean episode length: 425.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11231232
                    Iteration time: 4.53s
                        Total time: 6192.87s
                               ETA: 11879.8s

################################################################################
                     [1m Learning iteration 1371/4000 [0m

                       Computation: 1815 steps/s (collection: 0.600s, learning 3.911s)
               Value function loss: 205.9889
                    Surrogate loss: 0.0106
             Mean action noise std: 0.93
                       Mean reward: 702.22
               Mean episode length: 422.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 4.51s
                        Total time: 6197.38s
                               ETA: 11875.3s

################################################################################
                     [1m Learning iteration 1372/4000 [0m

                       Computation: 1834 steps/s (collection: 0.559s, learning 3.907s)
               Value function loss: 225.6185
                    Surrogate loss: 0.0101
             Mean action noise std: 0.93
                       Mean reward: 700.88
               Mean episode length: 420.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11247616
                    Iteration time: 4.47s
                        Total time: 6201.84s
                               ETA: 11870.7s

################################################################################
                     [1m Learning iteration 1373/4000 [0m

                       Computation: 1815 steps/s (collection: 0.579s, learning 3.932s)
               Value function loss: 148.7312
                    Surrogate loss: 0.0116
             Mean action noise std: 0.93
                       Mean reward: 699.28
               Mean episode length: 418.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 4.51s
                        Total time: 6206.35s
                               ETA: 11866.2s

################################################################################
                     [1m Learning iteration 1374/4000 [0m

                       Computation: 1833 steps/s (collection: 0.520s, learning 3.947s)
               Value function loss: 290.8663
                    Surrogate loss: 0.0108
             Mean action noise std: 0.93
                       Mean reward: 702.70
               Mean episode length: 420.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11264000
                    Iteration time: 4.47s
                        Total time: 6210.82s
                               ETA: 11861.5s

################################################################################
                     [1m Learning iteration 1375/4000 [0m

                       Computation: 1835 steps/s (collection: 0.535s, learning 3.928s)
               Value function loss: 171.0265
                    Surrogate loss: 0.0121
             Mean action noise std: 0.93
                       Mean reward: 695.59
               Mean episode length: 416.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 4.46s
                        Total time: 6215.28s
                               ETA: 11856.9s

################################################################################
                     [1m Learning iteration 1376/4000 [0m

                       Computation: 1851 steps/s (collection: 0.520s, learning 3.905s)
               Value function loss: 187.5670
                    Surrogate loss: 0.0120
             Mean action noise std: 0.93
                       Mean reward: 688.72
               Mean episode length: 411.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11280384
                    Iteration time: 4.42s
                        Total time: 6219.71s
                               ETA: 11852.2s

################################################################################
                     [1m Learning iteration 1377/4000 [0m

                       Computation: 1834 steps/s (collection: 0.531s, learning 3.935s)
               Value function loss: 194.8287
                    Surrogate loss: 0.0092
             Mean action noise std: 0.93
                       Mean reward: 692.69
               Mean episode length: 412.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 4.47s
                        Total time: 6224.17s
                               ETA: 11847.6s

################################################################################
                     [1m Learning iteration 1378/4000 [0m

                       Computation: 1830 steps/s (collection: 0.535s, learning 3.941s)
               Value function loss: 241.7958
                    Surrogate loss: 0.0109
             Mean action noise std: 0.93
                       Mean reward: 689.15
               Mean episode length: 408.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11296768
                    Iteration time: 4.48s
                        Total time: 6228.65s
                               ETA: 11843.0s

################################################################################
                     [1m Learning iteration 1379/4000 [0m

                       Computation: 1856 steps/s (collection: 0.530s, learning 3.883s)
               Value function loss: 202.1722
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 702.48
               Mean episode length: 416.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 4.41s
                        Total time: 6233.06s
                               ETA: 11838.3s

################################################################################
                     [1m Learning iteration 1380/4000 [0m

                       Computation: 1841 steps/s (collection: 0.513s, learning 3.937s)
               Value function loss: 313.8947
                    Surrogate loss: 0.0086
             Mean action noise std: 0.93
                       Mean reward: 718.51
               Mean episode length: 424.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11313152
                    Iteration time: 4.45s
                        Total time: 6237.51s
                               ETA: 11833.7s

################################################################################
                     [1m Learning iteration 1381/4000 [0m

                       Computation: 1832 steps/s (collection: 0.559s, learning 3.911s)
               Value function loss: 219.9600
                    Surrogate loss: 0.0103
             Mean action noise std: 0.93
                       Mean reward: 724.89
               Mean episode length: 428.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 4.47s
                        Total time: 6241.98s
                               ETA: 11829.1s

################################################################################
                     [1m Learning iteration 1382/4000 [0m

                       Computation: 1841 steps/s (collection: 0.524s, learning 3.924s)
               Value function loss: 256.3593
                    Surrogate loss: 0.0087
             Mean action noise std: 0.93
                       Mean reward: 733.66
               Mean episode length: 434.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11329536
                    Iteration time: 4.45s
                        Total time: 6246.43s
                               ETA: 11824.4s

################################################################################
                     [1m Learning iteration 1383/4000 [0m

                       Computation: 1849 steps/s (collection: 0.514s, learning 3.916s)
               Value function loss: 262.4772
                    Surrogate loss: 0.0079
             Mean action noise std: 0.93
                       Mean reward: 734.08
               Mean episode length: 434.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 4.43s
                        Total time: 6250.86s
                               ETA: 11819.7s

################################################################################
                     [1m Learning iteration 1384/4000 [0m

                       Computation: 1847 steps/s (collection: 0.547s, learning 3.888s)
               Value function loss: 151.3045
                    Surrogate loss: 0.0125
             Mean action noise std: 0.93
                       Mean reward: 737.11
               Mean episode length: 437.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11345920
                    Iteration time: 4.43s
                        Total time: 6255.29s
                               ETA: 11815.1s

################################################################################
                     [1m Learning iteration 1385/4000 [0m

                       Computation: 1835 steps/s (collection: 0.530s, learning 3.934s)
               Value function loss: 195.9319
                    Surrogate loss: 0.0083
             Mean action noise std: 0.92
                       Mean reward: 744.48
               Mean episode length: 439.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 4.46s
                        Total time: 6259.76s
                               ETA: 11810.4s

################################################################################
                     [1m Learning iteration 1386/4000 [0m

                       Computation: 1834 steps/s (collection: 0.546s, learning 3.920s)
               Value function loss: 148.0903
                    Surrogate loss: 0.0093
             Mean action noise std: 0.92
                       Mean reward: 735.69
               Mean episode length: 434.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11362304
                    Iteration time: 4.47s
                        Total time: 6264.22s
                               ETA: 11805.8s

################################################################################
                     [1m Learning iteration 1387/4000 [0m

                       Computation: 1824 steps/s (collection: 0.523s, learning 3.967s)
               Value function loss: 153.1583
                    Surrogate loss: 0.0067
             Mean action noise std: 0.92
                       Mean reward: 739.28
               Mean episode length: 437.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 4.49s
                        Total time: 6268.71s
                               ETA: 11801.3s

################################################################################
                     [1m Learning iteration 1388/4000 [0m

                       Computation: 1793 steps/s (collection: 0.562s, learning 4.007s)
               Value function loss: 250.2492
                    Surrogate loss: 0.0066
             Mean action noise std: 0.92
                       Mean reward: 750.36
               Mean episode length: 445.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11378688
                    Iteration time: 4.57s
                        Total time: 6273.28s
                               ETA: 11796.8s

################################################################################
                     [1m Learning iteration 1389/4000 [0m

                       Computation: 1817 steps/s (collection: 0.548s, learning 3.960s)
               Value function loss: 182.2132
                    Surrogate loss: 0.0074
             Mean action noise std: 0.92
                       Mean reward: 756.12
               Mean episode length: 448.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 4.51s
                        Total time: 6277.79s
                               ETA: 11792.3s

################################################################################
                     [1m Learning iteration 1390/4000 [0m

                       Computation: 1798 steps/s (collection: 0.588s, learning 3.967s)
               Value function loss: 305.1355
                    Surrogate loss: 0.0070
             Mean action noise std: 0.92
                       Mean reward: 758.80
               Mean episode length: 449.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11395072
                    Iteration time: 4.55s
                        Total time: 6282.35s
                               ETA: 11787.9s

################################################################################
                     [1m Learning iteration 1391/4000 [0m

                       Computation: 1812 steps/s (collection: 0.535s, learning 3.985s)
               Value function loss: 177.0515
                    Surrogate loss: 0.0119
             Mean action noise std: 0.92
                       Mean reward: 759.77
               Mean episode length: 449.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 4.52s
                        Total time: 6286.87s
                               ETA: 11783.4s

################################################################################
                     [1m Learning iteration 1392/4000 [0m

                       Computation: 1772 steps/s (collection: 0.614s, learning 4.008s)
               Value function loss: 236.2127
                    Surrogate loss: 0.0121
             Mean action noise std: 0.92
                       Mean reward: 759.81
               Mean episode length: 449.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11411456
                    Iteration time: 4.62s
                        Total time: 6291.49s
                               ETA: 11779.0s

################################################################################
                     [1m Learning iteration 1393/4000 [0m

                       Computation: 1832 steps/s (collection: 0.540s, learning 3.930s)
               Value function loss: 227.0369
                    Surrogate loss: 0.0119
             Mean action noise std: 0.92
                       Mean reward: 759.46
               Mean episode length: 449.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 4.47s
                        Total time: 6295.96s
                               ETA: 11774.4s

################################################################################
                     [1m Learning iteration 1394/4000 [0m

                       Computation: 1815 steps/s (collection: 0.571s, learning 3.942s)
               Value function loss: 245.0131
                    Surrogate loss: 0.0097
             Mean action noise std: 0.92
                       Mean reward: 758.38
               Mean episode length: 447.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11427840
                    Iteration time: 4.51s
                        Total time: 6300.47s
                               ETA: 11769.9s

################################################################################
                     [1m Learning iteration 1395/4000 [0m

                       Computation: 1810 steps/s (collection: 0.597s, learning 3.928s)
               Value function loss: 257.5796
                    Surrogate loss: 0.0115
             Mean action noise std: 0.92
                       Mean reward: 771.14
               Mean episode length: 454.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 4.53s
                        Total time: 6305.00s
                               ETA: 11765.4s

################################################################################
                     [1m Learning iteration 1396/4000 [0m

                       Computation: 1818 steps/s (collection: 0.551s, learning 3.954s)
               Value function loss: 235.1110
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 783.20
               Mean episode length: 461.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11444224
                    Iteration time: 4.50s
                        Total time: 6309.50s
                               ETA: 11760.9s

################################################################################
                     [1m Learning iteration 1397/4000 [0m

                       Computation: 1812 steps/s (collection: 0.593s, learning 3.928s)
               Value function loss: 191.0967
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 792.56
               Mean episode length: 465.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 4.52s
                        Total time: 6314.02s
                               ETA: 11756.4s

################################################################################
                     [1m Learning iteration 1398/4000 [0m

                       Computation: 1819 steps/s (collection: 0.608s, learning 3.893s)
               Value function loss: 232.6747
                    Surrogate loss: 0.0085
             Mean action noise std: 0.92
                       Mean reward: 801.90
               Mean episode length: 471.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11460608
                    Iteration time: 4.50s
                        Total time: 6318.52s
                               ETA: 11751.8s

################################################################################
                     [1m Learning iteration 1399/4000 [0m

                       Computation: 1791 steps/s (collection: 0.614s, learning 3.959s)
               Value function loss: 307.9079
                    Surrogate loss: 0.0089
             Mean action noise std: 0.92
                       Mean reward: 803.67
               Mean episode length: 471.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 4.57s
                        Total time: 6323.09s
                               ETA: 11747.4s

################################################################################
                     [1m Learning iteration 1400/4000 [0m

                       Computation: 1827 steps/s (collection: 0.562s, learning 3.921s)
               Value function loss: 190.0256
                    Surrogate loss: 0.0092
             Mean action noise std: 0.92
                       Mean reward: 792.48
               Mean episode length: 465.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11476992
                    Iteration time: 4.48s
                        Total time: 6327.58s
                               ETA: 11742.8s

################################################################################
                     [1m Learning iteration 1401/4000 [0m

                       Computation: 1830 steps/s (collection: 0.559s, learning 3.917s)
               Value function loss: 150.8756
                    Surrogate loss: 0.0106
             Mean action noise std: 0.92
                       Mean reward: 787.80
               Mean episode length: 462.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 4.48s
                        Total time: 6332.05s
                               ETA: 11738.2s

################################################################################
                     [1m Learning iteration 1402/4000 [0m

                       Computation: 1800 steps/s (collection: 0.569s, learning 3.980s)
               Value function loss: 154.8289
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 775.45
               Mean episode length: 456.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11493376
                    Iteration time: 4.55s
                        Total time: 6336.60s
                               ETA: 11733.8s

################################################################################
                     [1m Learning iteration 1403/4000 [0m

                       Computation: 1830 steps/s (collection: 0.559s, learning 3.916s)
               Value function loss: 192.3133
                    Surrogate loss: 0.0097
             Mean action noise std: 0.92
                       Mean reward: 787.05
               Mean episode length: 462.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 4.47s
                        Total time: 6341.08s
                               ETA: 11729.2s

################################################################################
                     [1m Learning iteration 1404/4000 [0m

                       Computation: 1818 steps/s (collection: 0.572s, learning 3.932s)
               Value function loss: 182.8826
                    Surrogate loss: 0.0119
             Mean action noise std: 0.92
                       Mean reward: 767.98
               Mean episode length: 451.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11509760
                    Iteration time: 4.50s
                        Total time: 6345.58s
                               ETA: 11724.6s

################################################################################
                     [1m Learning iteration 1405/4000 [0m

                       Computation: 1815 steps/s (collection: 0.529s, learning 3.984s)
               Value function loss: 263.0831
                    Surrogate loss: 0.0096
             Mean action noise std: 0.92
                       Mean reward: 767.86
               Mean episode length: 451.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 4.51s
                        Total time: 6350.09s
                               ETA: 11720.1s

################################################################################
                     [1m Learning iteration 1406/4000 [0m

                       Computation: 1829 steps/s (collection: 0.594s, learning 3.884s)
               Value function loss: 213.9441
                    Surrogate loss: 0.0076
             Mean action noise std: 0.92
                       Mean reward: 770.81
               Mean episode length: 452.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11526144
                    Iteration time: 4.48s
                        Total time: 6354.57s
                               ETA: 11715.5s

################################################################################
                     [1m Learning iteration 1407/4000 [0m

                       Computation: 1827 steps/s (collection: 0.545s, learning 3.938s)
               Value function loss: 168.4384
                    Surrogate loss: 0.0102
             Mean action noise std: 0.92
                       Mean reward: 763.86
               Mean episode length: 447.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 4.48s
                        Total time: 6359.05s
                               ETA: 11711.0s

################################################################################
                     [1m Learning iteration 1408/4000 [0m

                       Computation: 1769 steps/s (collection: 0.682s, learning 3.948s)
               Value function loss: 206.7716
                    Surrogate loss: 0.0081
             Mean action noise std: 0.92
                       Mean reward: 759.75
               Mean episode length: 444.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11542528
                    Iteration time: 4.63s
                        Total time: 6363.68s
                               ETA: 11706.6s

################################################################################
                     [1m Learning iteration 1409/4000 [0m

                       Computation: 1826 steps/s (collection: 0.565s, learning 3.921s)
               Value function loss: 267.2538
                    Surrogate loss: 0.0104
             Mean action noise std: 0.92
                       Mean reward: 746.30
               Mean episode length: 435.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 4.49s
                        Total time: 6368.17s
                               ETA: 11702.1s

################################################################################
                     [1m Learning iteration 1410/4000 [0m

                       Computation: 1828 steps/s (collection: 0.576s, learning 3.904s)
               Value function loss: 227.5563
                    Surrogate loss: 0.0087
             Mean action noise std: 0.92
                       Mean reward: 741.82
               Mean episode length: 432.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11558912
                    Iteration time: 4.48s
                        Total time: 6372.65s
                               ETA: 11697.5s

################################################################################
                     [1m Learning iteration 1411/4000 [0m

                       Computation: 1845 steps/s (collection: 0.565s, learning 3.873s)
               Value function loss: 198.3353
                    Surrogate loss: 0.0126
             Mean action noise std: 0.92
                       Mean reward: 761.10
               Mean episode length: 441.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 4.44s
                        Total time: 6377.09s
                               ETA: 11692.8s

################################################################################
                     [1m Learning iteration 1412/4000 [0m

                       Computation: 1823 steps/s (collection: 0.567s, learning 3.925s)
               Value function loss: 257.6774
                    Surrogate loss: 0.0090
             Mean action noise std: 0.92
                       Mean reward: 760.60
               Mean episode length: 440.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11575296
                    Iteration time: 4.49s
                        Total time: 6381.58s
                               ETA: 11688.3s

################################################################################
                     [1m Learning iteration 1413/4000 [0m

                       Computation: 1796 steps/s (collection: 0.579s, learning 3.981s)
               Value function loss: 224.8133
                    Surrogate loss: 0.0114
             Mean action noise std: 0.92
                       Mean reward: 768.15
               Mean episode length: 443.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 4.56s
                        Total time: 6386.14s
                               ETA: 11683.8s

################################################################################
                     [1m Learning iteration 1414/4000 [0m

                       Computation: 1801 steps/s (collection: 0.584s, learning 3.963s)
               Value function loss: 321.5691
                    Surrogate loss: 0.0118
             Mean action noise std: 0.92
                       Mean reward: 768.09
               Mean episode length: 441.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11591680
                    Iteration time: 4.55s
                        Total time: 6390.69s
                               ETA: 11679.4s

################################################################################
                     [1m Learning iteration 1415/4000 [0m

                       Computation: 1826 steps/s (collection: 0.557s, learning 3.927s)
               Value function loss: 245.3895
                    Surrogate loss: 0.0101
             Mean action noise std: 0.92
                       Mean reward: 771.83
               Mean episode length: 444.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 4.48s
                        Total time: 6395.17s
                               ETA: 11674.8s

################################################################################
                     [1m Learning iteration 1416/4000 [0m

                       Computation: 1798 steps/s (collection: 0.593s, learning 3.962s)
               Value function loss: 189.9775
                    Surrogate loss: 0.0081
             Mean action noise std: 0.92
                       Mean reward: 771.55
               Mean episode length: 444.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11608064
                    Iteration time: 4.56s
                        Total time: 6399.73s
                               ETA: 11670.4s

################################################################################
                     [1m Learning iteration 1417/4000 [0m

                       Computation: 1805 steps/s (collection: 0.573s, learning 3.965s)
               Value function loss: 225.3643
                    Surrogate loss: 0.0105
             Mean action noise std: 0.92
                       Mean reward: 777.10
               Mean episode length: 449.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 4.54s
                        Total time: 6404.26s
                               ETA: 11665.9s

################################################################################
                     [1m Learning iteration 1418/4000 [0m

                       Computation: 1810 steps/s (collection: 0.546s, learning 3.978s)
               Value function loss: 176.0991
                    Surrogate loss: 0.0084
             Mean action noise std: 0.92
                       Mean reward: 779.08
               Mean episode length: 450.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11624448
                    Iteration time: 4.52s
                        Total time: 6408.79s
                               ETA: 11661.4s

################################################################################
                     [1m Learning iteration 1419/4000 [0m

                       Computation: 1800 steps/s (collection: 0.612s, learning 3.938s)
               Value function loss: 286.9243
                    Surrogate loss: 0.0112
             Mean action noise std: 0.92
                       Mean reward: 777.69
               Mean episode length: 448.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 4.55s
                        Total time: 6413.34s
                               ETA: 11656.9s

################################################################################
                     [1m Learning iteration 1420/4000 [0m

                       Computation: 1797 steps/s (collection: 0.638s, learning 3.918s)
               Value function loss: 160.4189
                    Surrogate loss: 0.0114
             Mean action noise std: 0.92
                       Mean reward: 777.91
               Mean episode length: 450.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11640832
                    Iteration time: 4.56s
                        Total time: 6417.89s
                               ETA: 11652.5s

################################################################################
                     [1m Learning iteration 1421/4000 [0m

                       Computation: 1802 steps/s (collection: 0.604s, learning 3.941s)
               Value function loss: 237.8543
                    Surrogate loss: 0.0102
             Mean action noise std: 0.92
                       Mean reward: 782.08
               Mean episode length: 452.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 4.55s
                        Total time: 6422.44s
                               ETA: 11648.0s

################################################################################
                     [1m Learning iteration 1422/4000 [0m

                       Computation: 1818 steps/s (collection: 0.554s, learning 3.951s)
               Value function loss: 195.5912
                    Surrogate loss: 0.0111
             Mean action noise std: 0.92
                       Mean reward: 769.69
               Mean episode length: 445.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11657216
                    Iteration time: 4.51s
                        Total time: 6426.95s
                               ETA: 11643.5s

################################################################################
                     [1m Learning iteration 1423/4000 [0m

                       Computation: 1804 steps/s (collection: 0.592s, learning 3.948s)
               Value function loss: 206.0080
                    Surrogate loss: 0.0116
             Mean action noise std: 0.92
                       Mean reward: 771.14
               Mean episode length: 448.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 4.54s
                        Total time: 6431.48s
                               ETA: 11639.0s

################################################################################
                     [1m Learning iteration 1424/4000 [0m

                       Computation: 1816 steps/s (collection: 0.579s, learning 3.931s)
               Value function loss: 267.7913
                    Surrogate loss: 0.0108
             Mean action noise std: 0.92
                       Mean reward: 762.47
               Mean episode length: 444.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11673600
                    Iteration time: 4.51s
                        Total time: 6435.99s
                               ETA: 11634.5s

################################################################################
                     [1m Learning iteration 1425/4000 [0m

                       Computation: 1816 steps/s (collection: 0.560s, learning 3.951s)
               Value function loss: 288.0107
                    Surrogate loss: 0.0110
             Mean action noise std: 0.92
                       Mean reward: 764.85
               Mean episode length: 446.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 4.51s
                        Total time: 6440.50s
                               ETA: 11629.9s

################################################################################
                     [1m Learning iteration 1426/4000 [0m

                       Computation: 1811 steps/s (collection: 0.563s, learning 3.959s)
               Value function loss: 224.5630
                    Surrogate loss: 0.0116
             Mean action noise std: 0.92
                       Mean reward: 771.65
               Mean episode length: 451.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11689984
                    Iteration time: 4.52s
                        Total time: 6445.03s
                               ETA: 11625.4s

################################################################################
                     [1m Learning iteration 1427/4000 [0m

                       Computation: 1785 steps/s (collection: 0.616s, learning 3.971s)
               Value function loss: 283.0882
                    Surrogate loss: 0.0111
             Mean action noise std: 0.92
                       Mean reward: 761.69
               Mean episode length: 444.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 4.59s
                        Total time: 6449.62s
                               ETA: 11621.1s

################################################################################
                     [1m Learning iteration 1428/4000 [0m

                       Computation: 1820 steps/s (collection: 0.560s, learning 3.939s)
               Value function loss: 228.4116
                    Surrogate loss: 0.0118
             Mean action noise std: 0.92
                       Mean reward: 761.90
               Mean episode length: 444.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11706368
                    Iteration time: 4.50s
                        Total time: 6454.11s
                               ETA: 11616.5s

################################################################################
                     [1m Learning iteration 1429/4000 [0m

                       Computation: 1814 steps/s (collection: 0.571s, learning 3.945s)
               Value function loss: 249.5291
                    Surrogate loss: 0.0105
             Mean action noise std: 0.92
                       Mean reward: 757.89
               Mean episode length: 443.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 4.52s
                        Total time: 6458.63s
                               ETA: 11612.0s

################################################################################
                     [1m Learning iteration 1430/4000 [0m

                       Computation: 1838 steps/s (collection: 0.544s, learning 3.911s)
               Value function loss: 356.6498
                    Surrogate loss: 0.0092
             Mean action noise std: 0.92
                       Mean reward: 758.81
               Mean episode length: 445.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11722752
                    Iteration time: 4.45s
                        Total time: 6463.08s
                               ETA: 11607.4s

################################################################################
                     [1m Learning iteration 1431/4000 [0m

                       Computation: 1823 steps/s (collection: 0.567s, learning 3.926s)
               Value function loss: 195.5346
                    Surrogate loss: 0.0069
             Mean action noise std: 0.92
                       Mean reward: 743.49
               Mean episode length: 436.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 4.49s
                        Total time: 6467.58s
                               ETA: 11602.8s

################################################################################
                     [1m Learning iteration 1432/4000 [0m

                       Computation: 1802 steps/s (collection: 0.631s, learning 3.913s)
               Value function loss: 211.3499
                    Surrogate loss: 0.0132
             Mean action noise std: 0.92
                       Mean reward: 725.91
               Mean episode length: 428.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11739136
                    Iteration time: 4.54s
                        Total time: 6472.12s
                               ETA: 11598.3s

################################################################################
                     [1m Learning iteration 1433/4000 [0m

                       Computation: 1825 steps/s (collection: 0.586s, learning 3.902s)
               Value function loss: 255.4184
                    Surrogate loss: 0.0109
             Mean action noise std: 0.92
                       Mean reward: 736.15
               Mean episode length: 433.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 4.49s
                        Total time: 6476.61s
                               ETA: 11593.8s

################################################################################
                     [1m Learning iteration 1434/4000 [0m

                       Computation: 1815 steps/s (collection: 0.546s, learning 3.966s)
               Value function loss: 104.9124
                    Surrogate loss: 0.0101
             Mean action noise std: 0.92
                       Mean reward: 728.77
               Mean episode length: 430.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 11755520
                    Iteration time: 4.51s
                        Total time: 6481.12s
                               ETA: 11589.2s

################################################################################
                     [1m Learning iteration 1435/4000 [0m

                       Computation: 1824 steps/s (collection: 0.548s, learning 3.942s)
               Value function loss: 297.0772
                    Surrogate loss: 0.0112
             Mean action noise std: 0.92
                       Mean reward: 723.86
               Mean episode length: 426.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 4.49s
                        Total time: 6485.61s
                               ETA: 11584.7s

################################################################################
                     [1m Learning iteration 1436/4000 [0m

                       Computation: 1817 steps/s (collection: 0.575s, learning 3.931s)
               Value function loss: 216.4981
                    Surrogate loss: 0.0100
             Mean action noise std: 0.92
                       Mean reward: 725.86
               Mean episode length: 426.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11771904
                    Iteration time: 4.51s
                        Total time: 6490.12s
                               ETA: 11580.1s

################################################################################
                     [1m Learning iteration 1437/4000 [0m

                       Computation: 1815 steps/s (collection: 0.543s, learning 3.970s)
               Value function loss: 210.7677
                    Surrogate loss: 0.0107
             Mean action noise std: 0.92
                       Mean reward: 734.42
               Mean episode length: 431.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 4.51s
                        Total time: 6494.63s
                               ETA: 11575.6s

################################################################################
                     [1m Learning iteration 1438/4000 [0m

                       Computation: 1821 steps/s (collection: 0.564s, learning 3.934s)
               Value function loss: 157.3819
                    Surrogate loss: 0.0087
             Mean action noise std: 0.92
                       Mean reward: 724.66
               Mean episode length: 424.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11788288
                    Iteration time: 4.50s
                        Total time: 6499.13s
                               ETA: 11571.1s

################################################################################
                     [1m Learning iteration 1439/4000 [0m

                       Computation: 1838 steps/s (collection: 0.530s, learning 3.926s)
               Value function loss: 165.3648
                    Surrogate loss: 0.0090
             Mean action noise std: 0.92
                       Mean reward: 727.48
               Mean episode length: 425.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 4.46s
                        Total time: 6503.58s
                               ETA: 11566.4s

################################################################################
                     [1m Learning iteration 1440/4000 [0m

                       Computation: 1796 steps/s (collection: 0.604s, learning 3.956s)
               Value function loss: 278.5018
                    Surrogate loss: 0.0087
             Mean action noise std: 0.92
                       Mean reward: 734.79
               Mean episode length: 428.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11804672
                    Iteration time: 4.56s
                        Total time: 6508.14s
                               ETA: 11562.0s

################################################################################
                     [1m Learning iteration 1441/4000 [0m

                       Computation: 1798 steps/s (collection: 0.582s, learning 3.974s)
               Value function loss: 219.2953
                    Surrogate loss: 0.0094
             Mean action noise std: 0.92
                       Mean reward: 720.97
               Mean episode length: 419.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 4.56s
                        Total time: 6512.70s
                               ETA: 11557.6s

################################################################################
                     [1m Learning iteration 1442/4000 [0m

                       Computation: 1801 steps/s (collection: 0.559s, learning 3.989s)
               Value function loss: 252.0036
                    Surrogate loss: 0.0084
             Mean action noise std: 0.92
                       Mean reward: 735.05
               Mean episode length: 425.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11821056
                    Iteration time: 4.55s
                        Total time: 6517.25s
                               ETA: 11553.1s

################################################################################
                     [1m Learning iteration 1443/4000 [0m

                       Computation: 1816 steps/s (collection: 0.559s, learning 3.951s)
               Value function loss: 208.4053
                    Surrogate loss: 0.0062
             Mean action noise std: 0.92
                       Mean reward: 723.75
               Mean episode length: 418.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 4.51s
                        Total time: 6521.76s
                               ETA: 11548.6s

################################################################################
                     [1m Learning iteration 1444/4000 [0m

                       Computation: 1780 steps/s (collection: 0.603s, learning 3.997s)
               Value function loss: 215.6453
                    Surrogate loss: 0.0077
             Mean action noise std: 0.92
                       Mean reward: 720.04
               Mean episode length: 414.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11837440
                    Iteration time: 4.60s
                        Total time: 6526.36s
                               ETA: 11544.2s

################################################################################
                     [1m Learning iteration 1445/4000 [0m

                       Computation: 1799 steps/s (collection: 0.586s, learning 3.967s)
               Value function loss: 256.6075
                    Surrogate loss: 0.0088
             Mean action noise std: 0.92
                       Mean reward: 720.08
               Mean episode length: 414.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 4.55s
                        Total time: 6530.91s
                               ETA: 11539.7s

################################################################################
                     [1m Learning iteration 1446/4000 [0m

                       Computation: 1814 steps/s (collection: 0.594s, learning 3.922s)
               Value function loss: 220.6879
                    Surrogate loss: 0.0077
             Mean action noise std: 0.92
                       Mean reward: 723.23
               Mean episode length: 412.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11853824
                    Iteration time: 4.52s
                        Total time: 6535.43s
                               ETA: 11535.2s

################################################################################
                     [1m Learning iteration 1447/4000 [0m

                       Computation: 1827 steps/s (collection: 0.538s, learning 3.943s)
               Value function loss: 199.3607
                    Surrogate loss: 0.0086
             Mean action noise std: 0.92
                       Mean reward: 724.59
               Mean episode length: 414.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 4.48s
                        Total time: 6539.91s
                               ETA: 11530.7s

################################################################################
                     [1m Learning iteration 1448/4000 [0m

                       Computation: 1819 steps/s (collection: 0.582s, learning 3.919s)
               Value function loss: 194.0095
                    Surrogate loss: 0.0085
             Mean action noise std: 0.92
                       Mean reward: 729.57
               Mean episode length: 418.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11870208
                    Iteration time: 4.50s
                        Total time: 6544.41s
                               ETA: 11526.1s

################################################################################
                     [1m Learning iteration 1449/4000 [0m

                       Computation: 1836 steps/s (collection: 0.568s, learning 3.893s)
               Value function loss: 207.5556
                    Surrogate loss: 0.0082
             Mean action noise std: 0.92
                       Mean reward: 730.97
               Mean episode length: 419.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 4.46s
                        Total time: 6548.87s
                               ETA: 11521.5s

################################################################################
                     [1m Learning iteration 1450/4000 [0m

                       Computation: 1824 steps/s (collection: 0.539s, learning 3.951s)
               Value function loss: 158.3655
                    Surrogate loss: 0.0080
             Mean action noise std: 0.92
                       Mean reward: 725.15
               Mean episode length: 416.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 11886592
                    Iteration time: 4.49s
                        Total time: 6553.36s
                               ETA: 11516.9s

################################################################################
                     [1m Learning iteration 1451/4000 [0m

                       Computation: 1830 steps/s (collection: 0.553s, learning 3.922s)
               Value function loss: 266.1856
                    Surrogate loss: 0.0081
             Mean action noise std: 0.92
                       Mean reward: 737.96
               Mean episode length: 424.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 4.48s
                        Total time: 6557.84s
                               ETA: 11512.3s

################################################################################
                     [1m Learning iteration 1452/4000 [0m

                       Computation: 1809 steps/s (collection: 0.598s, learning 3.930s)
               Value function loss: 248.2986
                    Surrogate loss: 0.0083
             Mean action noise std: 0.92
                       Mean reward: 731.94
               Mean episode length: 421.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11902976
                    Iteration time: 4.53s
                        Total time: 6562.36s
                               ETA: 11507.8s

################################################################################
                     [1m Learning iteration 1453/4000 [0m

                       Computation: 1833 steps/s (collection: 0.580s, learning 3.887s)
               Value function loss: 183.9303
                    Surrogate loss: 0.0077
             Mean action noise std: 0.92
                       Mean reward: 746.79
               Mean episode length: 431.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 4.47s
                        Total time: 6566.83s
                               ETA: 11503.2s

################################################################################
                     [1m Learning iteration 1454/4000 [0m

                       Computation: 1817 steps/s (collection: 0.570s, learning 3.936s)
               Value function loss: 216.2335
                    Surrogate loss: 0.0065
             Mean action noise std: 0.92
                       Mean reward: 758.41
               Mean episode length: 438.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11919360
                    Iteration time: 4.51s
                        Total time: 6571.34s
                               ETA: 11498.7s

################################################################################
                     [1m Learning iteration 1455/4000 [0m

                       Computation: 1841 steps/s (collection: 0.548s, learning 3.901s)
               Value function loss: 199.5475
                    Surrogate loss: 0.0077
             Mean action noise std: 0.92
                       Mean reward: 768.39
               Mean episode length: 444.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 4.45s
                        Total time: 6575.79s
                               ETA: 11494.1s

################################################################################
                     [1m Learning iteration 1456/4000 [0m

                       Computation: 1849 steps/s (collection: 0.536s, learning 3.893s)
               Value function loss: 266.3444
                    Surrogate loss: 0.0089
             Mean action noise std: 0.92
                       Mean reward: 784.42
               Mean episode length: 455.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11935744
                    Iteration time: 4.43s
                        Total time: 6580.22s
                               ETA: 11489.4s

################################################################################
                     [1m Learning iteration 1457/4000 [0m

                       Computation: 1844 steps/s (collection: 0.518s, learning 3.922s)
               Value function loss: 271.2164
                    Surrogate loss: 0.0095
             Mean action noise std: 0.92
                       Mean reward: 796.07
               Mean episode length: 462.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 4.44s
                        Total time: 6584.66s
                               ETA: 11484.8s

################################################################################
                     [1m Learning iteration 1458/4000 [0m

                       Computation: 1850 steps/s (collection: 0.536s, learning 3.892s)
               Value function loss: 233.1298
                    Surrogate loss: 0.0077
             Mean action noise std: 0.92
                       Mean reward: 790.79
               Mean episode length: 462.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11952128
                    Iteration time: 4.43s
                        Total time: 6589.08s
                               ETA: 11480.1s

################################################################################
                     [1m Learning iteration 1459/4000 [0m

                       Computation: 1848 steps/s (collection: 0.529s, learning 3.903s)
               Value function loss: 183.9606
                    Surrogate loss: 0.0097
             Mean action noise std: 0.92
                       Mean reward: 791.37
               Mean episode length: 462.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 4.43s
                        Total time: 6593.52s
                               ETA: 11475.4s

################################################################################
                     [1m Learning iteration 1460/4000 [0m

                       Computation: 1843 steps/s (collection: 0.524s, learning 3.920s)
               Value function loss: 217.3717
                    Surrogate loss: 0.0090
             Mean action noise std: 0.92
                       Mean reward: 788.09
               Mean episode length: 462.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11968512
                    Iteration time: 4.44s
                        Total time: 6597.96s
                               ETA: 11470.8s

################################################################################
                     [1m Learning iteration 1461/4000 [0m

                       Computation: 1824 steps/s (collection: 0.556s, learning 3.935s)
               Value function loss: 353.6343
                    Surrogate loss: 0.0099
             Mean action noise std: 0.92
                       Mean reward: 789.73
               Mean episode length: 466.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 4.49s
                        Total time: 6602.45s
                               ETA: 11466.2s

################################################################################
                     [1m Learning iteration 1462/4000 [0m

                       Computation: 1839 steps/s (collection: 0.551s, learning 3.903s)
               Value function loss: 168.0510
                    Surrogate loss: 0.0076
             Mean action noise std: 0.92
                       Mean reward: 785.77
               Mean episode length: 463.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11984896
                    Iteration time: 4.45s
                        Total time: 6606.91s
                               ETA: 11461.6s

################################################################################
                     [1m Learning iteration 1463/4000 [0m

                       Computation: 1856 steps/s (collection: 0.521s, learning 3.891s)
               Value function loss: 225.5145
                    Surrogate loss: 0.0093
             Mean action noise std: 0.92
                       Mean reward: 792.00
               Mean episode length: 468.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 4.41s
                        Total time: 6611.32s
                               ETA: 11456.9s

################################################################################
                     [1m Learning iteration 1464/4000 [0m

                       Computation: 1852 steps/s (collection: 0.526s, learning 3.897s)
               Value function loss: 159.8468
                    Surrogate loss: 0.0112
             Mean action noise std: 0.92
                       Mean reward: 793.26
               Mean episode length: 470.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12001280
                    Iteration time: 4.42s
                        Total time: 6615.74s
                               ETA: 11452.2s

################################################################################
                     [1m Learning iteration 1465/4000 [0m

                       Computation: 1844 steps/s (collection: 0.522s, learning 3.920s)
               Value function loss: 208.5813
                    Surrogate loss: 0.0118
             Mean action noise std: 0.92
                       Mean reward: 782.18
               Mean episode length: 466.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 4.44s
                        Total time: 6620.18s
                               ETA: 11447.6s

################################################################################
                     [1m Learning iteration 1466/4000 [0m

                       Computation: 1822 steps/s (collection: 0.522s, learning 3.972s)
               Value function loss: 253.6249
                    Surrogate loss: 0.0094
             Mean action noise std: 0.92
                       Mean reward: 774.94
               Mean episode length: 461.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12017664
                    Iteration time: 4.49s
                        Total time: 6624.68s
                               ETA: 11443.0s

################################################################################
                     [1m Learning iteration 1467/4000 [0m

                       Computation: 1835 steps/s (collection: 0.538s, learning 3.925s)
               Value function loss: 276.4839
                    Surrogate loss: 0.0101
             Mean action noise std: 0.92
                       Mean reward: 742.84
               Mean episode length: 444.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 4.46s
                        Total time: 6629.14s
                               ETA: 11438.4s

################################################################################
                     [1m Learning iteration 1468/4000 [0m

                       Computation: 1825 steps/s (collection: 0.541s, learning 3.947s)
               Value function loss: 227.8850
                    Surrogate loss: 0.0084
             Mean action noise std: 0.92
                       Mean reward: 732.08
               Mean episode length: 439.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12034048
                    Iteration time: 4.49s
                        Total time: 6633.63s
                               ETA: 11433.9s

################################################################################
                     [1m Learning iteration 1469/4000 [0m

                       Computation: 1829 steps/s (collection: 0.526s, learning 3.953s)
               Value function loss: 216.0726
                    Surrogate loss: 0.0106
             Mean action noise std: 0.92
                       Mean reward: 731.26
               Mean episode length: 439.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 4.48s
                        Total time: 6638.11s
                               ETA: 11429.3s

################################################################################
                     [1m Learning iteration 1470/4000 [0m

                       Computation: 1814 steps/s (collection: 0.514s, learning 4.002s)
               Value function loss: 225.9537
                    Surrogate loss: 0.0099
             Mean action noise std: 0.92
                       Mean reward: 730.79
               Mean episode length: 439.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12050432
                    Iteration time: 4.52s
                        Total time: 6642.62s
                               ETA: 11424.8s

################################################################################
                     [1m Learning iteration 1471/4000 [0m

                       Computation: 1834 steps/s (collection: 0.561s, learning 3.906s)
               Value function loss: 239.5430
                    Surrogate loss: 0.0090
             Mean action noise std: 0.92
                       Mean reward: 733.11
               Mean episode length: 438.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 4.47s
                        Total time: 6647.09s
                               ETA: 11420.2s

################################################################################
                     [1m Learning iteration 1472/4000 [0m

                       Computation: 1816 steps/s (collection: 0.515s, learning 3.993s)
               Value function loss: 269.6842
                    Surrogate loss: 0.0127
             Mean action noise std: 0.92
                       Mean reward: 732.78
               Mean episode length: 439.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12066816
                    Iteration time: 4.51s
                        Total time: 6651.60s
                               ETA: 11415.6s

################################################################################
                     [1m Learning iteration 1473/4000 [0m

                       Computation: 1832 steps/s (collection: 0.529s, learning 3.941s)
               Value function loss: 201.8386
                    Surrogate loss: 0.0089
             Mean action noise std: 0.92
                       Mean reward: 732.55
               Mean episode length: 440.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 4.47s
                        Total time: 6656.07s
                               ETA: 11411.0s

################################################################################
                     [1m Learning iteration 1474/4000 [0m

                       Computation: 1807 steps/s (collection: 0.568s, learning 3.964s)
               Value function loss: 239.3596
                    Surrogate loss: 0.0097
             Mean action noise std: 0.92
                       Mean reward: 721.45
               Mean episode length: 433.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12083200
                    Iteration time: 4.53s
                        Total time: 6660.60s
                               ETA: 11406.6s

################################################################################
                     [1m Learning iteration 1475/4000 [0m

                       Computation: 1801 steps/s (collection: 0.597s, learning 3.950s)
               Value function loss: 188.2822
                    Surrogate loss: 0.0098
             Mean action noise std: 0.92
                       Mean reward: 705.96
               Mean episode length: 423.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 4.55s
                        Total time: 6665.15s
                               ETA: 11402.1s

################################################################################
                     [1m Learning iteration 1476/4000 [0m

                       Computation: 1824 steps/s (collection: 0.576s, learning 3.913s)
               Value function loss: 235.3680
                    Surrogate loss: 0.0108
             Mean action noise std: 0.92
                       Mean reward: 712.39
               Mean episode length: 428.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12099584
                    Iteration time: 4.49s
                        Total time: 6669.64s
                               ETA: 11397.5s

################################################################################
                     [1m Learning iteration 1477/4000 [0m

                       Computation: 1782 steps/s (collection: 0.624s, learning 3.972s)
               Value function loss: 348.9535
                    Surrogate loss: 0.0108
             Mean action noise std: 0.92
                       Mean reward: 727.32
               Mean episode length: 438.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 4.60s
                        Total time: 6674.23s
                               ETA: 11393.2s

################################################################################
                     [1m Learning iteration 1478/4000 [0m

                       Computation: 1821 steps/s (collection: 0.591s, learning 3.907s)
               Value function loss: 195.5552
                    Surrogate loss: 0.0099
             Mean action noise std: 0.92
                       Mean reward: 721.67
               Mean episode length: 436.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12115968
                    Iteration time: 4.50s
                        Total time: 6678.73s
                               ETA: 11388.6s

################################################################################
                     [1m Learning iteration 1479/4000 [0m

                       Computation: 1808 steps/s (collection: 0.528s, learning 4.002s)
               Value function loss: 115.4610
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 721.96
               Mean episode length: 436.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 4.53s
                        Total time: 6683.26s
                               ETA: 11384.1s

################################################################################
                     [1m Learning iteration 1480/4000 [0m

                       Computation: 1850 steps/s (collection: 0.521s, learning 3.906s)
               Value function loss: 214.8569
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 706.19
               Mean episode length: 429.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12132352
                    Iteration time: 4.43s
                        Total time: 6687.69s
                               ETA: 11379.5s

################################################################################
                     [1m Learning iteration 1481/4000 [0m

                       Computation: 1840 steps/s (collection: 0.512s, learning 3.940s)
               Value function loss: 166.6025
                    Surrogate loss: 0.0108
             Mean action noise std: 0.92
                       Mean reward: 698.63
               Mean episode length: 426.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 4.45s
                        Total time: 6692.14s
                               ETA: 11374.8s

################################################################################
                     [1m Learning iteration 1482/4000 [0m

                       Computation: 1812 steps/s (collection: 0.591s, learning 3.927s)
               Value function loss: 315.0509
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 692.29
               Mean episode length: 423.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12148736
                    Iteration time: 4.52s
                        Total time: 6696.66s
                               ETA: 11370.3s

################################################################################
                     [1m Learning iteration 1483/4000 [0m

                       Computation: 1822 steps/s (collection: 0.577s, learning 3.919s)
               Value function loss: 230.8783
                    Surrogate loss: 0.0092
             Mean action noise std: 0.92
                       Mean reward: 683.19
               Mean episode length: 419.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 4.50s
                        Total time: 6701.15s
                               ETA: 11365.8s

################################################################################
                     [1m Learning iteration 1484/4000 [0m

                       Computation: 1823 steps/s (collection: 0.556s, learning 3.935s)
               Value function loss: 233.6166
                    Surrogate loss: 0.0100
             Mean action noise std: 0.92
                       Mean reward: 684.96
               Mean episode length: 419.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12165120
                    Iteration time: 4.49s
                        Total time: 6705.65s
                               ETA: 11361.2s

################################################################################
                     [1m Learning iteration 1485/4000 [0m

                       Computation: 1812 steps/s (collection: 0.526s, learning 3.994s)
               Value function loss: 230.8787
                    Surrogate loss: 0.0100
             Mean action noise std: 0.92
                       Mean reward: 692.96
               Mean episode length: 424.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 4.52s
                        Total time: 6710.17s
                               ETA: 11356.7s

################################################################################
                     [1m Learning iteration 1486/4000 [0m

                       Computation: 1827 steps/s (collection: 0.553s, learning 3.930s)
               Value function loss: 191.1185
                    Surrogate loss: 0.0111
             Mean action noise std: 0.92
                       Mean reward: 696.60
               Mean episode length: 427.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12181504
                    Iteration time: 4.48s
                        Total time: 6714.65s
                               ETA: 11352.1s

################################################################################
                     [1m Learning iteration 1487/4000 [0m

                       Computation: 1822 steps/s (collection: 0.559s, learning 3.935s)
               Value function loss: 258.4021
                    Surrogate loss: 0.0111
             Mean action noise std: 0.92
                       Mean reward: 697.69
               Mean episode length: 427.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 4.49s
                        Total time: 6719.14s
                               ETA: 11347.6s

################################################################################
                     [1m Learning iteration 1488/4000 [0m

                       Computation: 1783 steps/s (collection: 0.647s, learning 3.947s)
               Value function loss: 193.7306
                    Surrogate loss: 0.0075
             Mean action noise std: 0.92
                       Mean reward: 696.64
               Mean episode length: 428.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12197888
                    Iteration time: 4.59s
                        Total time: 6723.74s
                               ETA: 11343.2s

################################################################################
                     [1m Learning iteration 1489/4000 [0m

                       Computation: 1825 steps/s (collection: 0.589s, learning 3.900s)
               Value function loss: 206.7652
                    Surrogate loss: 0.0103
             Mean action noise std: 0.92
                       Mean reward: 693.22
               Mean episode length: 425.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 4.49s
                        Total time: 6728.23s
                               ETA: 11338.6s

################################################################################
                     [1m Learning iteration 1490/4000 [0m

                       Computation: 1803 steps/s (collection: 0.624s, learning 3.919s)
               Value function loss: 206.3563
                    Surrogate loss: 0.0099
             Mean action noise std: 0.92
                       Mean reward: 704.12
               Mean episode length: 431.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12214272
                    Iteration time: 4.54s
                        Total time: 6732.77s
                               ETA: 11334.2s

################################################################################
                     [1m Learning iteration 1491/4000 [0m

                       Computation: 1844 steps/s (collection: 0.554s, learning 3.888s)
               Value function loss: 216.1432
                    Surrogate loss: 0.0099
             Mean action noise std: 0.92
                       Mean reward: 711.16
               Mean episode length: 434.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 4.44s
                        Total time: 6737.21s
                               ETA: 11329.5s

################################################################################
                     [1m Learning iteration 1492/4000 [0m

                       Computation: 1802 steps/s (collection: 0.618s, learning 3.926s)
               Value function loss: 248.8186
                    Surrogate loss: 0.0100
             Mean action noise std: 0.92
                       Mean reward: 715.81
               Mean episode length: 435.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12230656
                    Iteration time: 4.54s
                        Total time: 6741.76s
                               ETA: 11325.1s

################################################################################
                     [1m Learning iteration 1493/4000 [0m

                       Computation: 1809 steps/s (collection: 0.582s, learning 3.946s)
               Value function loss: 195.2128
                    Surrogate loss: 0.0112
             Mean action noise std: 0.92
                       Mean reward: 715.74
               Mean episode length: 433.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 4.53s
                        Total time: 6746.28s
                               ETA: 11320.6s

################################################################################
                     [1m Learning iteration 1494/4000 [0m

                       Computation: 1822 steps/s (collection: 0.544s, learning 3.950s)
               Value function loss: 166.2017
                    Surrogate loss: 0.0078
             Mean action noise std: 0.92
                       Mean reward: 716.83
               Mean episode length: 434.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12247040
                    Iteration time: 4.49s
                        Total time: 6750.78s
                               ETA: 11316.0s

################################################################################
                     [1m Learning iteration 1495/4000 [0m

                       Computation: 1815 steps/s (collection: 0.589s, learning 3.923s)
               Value function loss: 139.7407
                    Surrogate loss: 0.0093
             Mean action noise std: 0.92
                       Mean reward: 715.72
               Mean episode length: 434.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 4.51s
                        Total time: 6755.29s
                               ETA: 11311.5s

################################################################################
                     [1m Learning iteration 1496/4000 [0m

                       Computation: 1801 steps/s (collection: 0.569s, learning 3.978s)
               Value function loss: 237.1505
                    Surrogate loss: 0.0091
             Mean action noise std: 0.92
                       Mean reward: 705.43
               Mean episode length: 427.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12263424
                    Iteration time: 4.55s
                        Total time: 6759.84s
                               ETA: 11307.0s

################################################################################
                     [1m Learning iteration 1497/4000 [0m

                       Computation: 1790 steps/s (collection: 0.623s, learning 3.954s)
               Value function loss: 157.7801
                    Surrogate loss: 0.0097
             Mean action noise std: 0.92
                       Mean reward: 707.18
               Mean episode length: 427.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 4.58s
                        Total time: 6764.41s
                               ETA: 11302.6s

################################################################################
                     [1m Learning iteration 1498/4000 [0m

                       Computation: 1811 steps/s (collection: 0.594s, learning 3.929s)
               Value function loss: 264.4370
                    Surrogate loss: 0.0109
             Mean action noise std: 0.92
                       Mean reward: 722.20
               Mean episode length: 435.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12279808
                    Iteration time: 4.52s
                        Total time: 6768.94s
                               ETA: 11298.1s

################################################################################
                     [1m Learning iteration 1499/4000 [0m

                       Computation: 1846 steps/s (collection: 0.569s, learning 3.868s)
               Value function loss: 175.1669
                    Surrogate loss: 0.0100
             Mean action noise std: 0.92
                       Mean reward: 720.40
               Mean episode length: 433.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 4.44s
                        Total time: 6773.37s
                               ETA: 11293.5s

################################################################################
                     [1m Learning iteration 1500/4000 [0m

                       Computation: 1791 steps/s (collection: 0.573s, learning 4.000s)
               Value function loss: 241.4221
                    Surrogate loss: 0.0119
             Mean action noise std: 0.92
                       Mean reward: 723.63
               Mean episode length: 436.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12296192
                    Iteration time: 4.57s
                        Total time: 6777.95s
                               ETA: 11289.0s

################################################################################
                     [1m Learning iteration 1501/4000 [0m

                       Computation: 1791 steps/s (collection: 0.645s, learning 3.927s)
               Value function loss: 217.4099
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 725.35
               Mean episode length: 436.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 4.57s
                        Total time: 6782.52s
                               ETA: 11284.6s

################################################################################
                     [1m Learning iteration 1502/4000 [0m

                       Computation: 1823 steps/s (collection: 0.556s, learning 3.936s)
               Value function loss: 186.4713
                    Surrogate loss: 0.0121
             Mean action noise std: 0.92
                       Mean reward: 714.29
               Mean episode length: 428.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12312576
                    Iteration time: 4.49s
                        Total time: 6787.01s
                               ETA: 11280.1s

################################################################################
                     [1m Learning iteration 1503/4000 [0m

                       Computation: 1820 steps/s (collection: 0.572s, learning 3.929s)
               Value function loss: 313.8507
                    Surrogate loss: 0.0120
             Mean action noise std: 0.92
                       Mean reward: 716.77
               Mean episode length: 431.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 4.50s
                        Total time: 6791.51s
                               ETA: 11275.5s

################################################################################
                     [1m Learning iteration 1504/4000 [0m

                       Computation: 1811 steps/s (collection: 0.597s, learning 3.924s)
               Value function loss: 182.7869
                    Surrogate loss: 0.0117
             Mean action noise std: 0.92
                       Mean reward: 723.90
               Mean episode length: 434.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12328960
                    Iteration time: 4.52s
                        Total time: 6796.03s
                               ETA: 11271.0s

################################################################################
                     [1m Learning iteration 1505/4000 [0m

                       Computation: 1811 steps/s (collection: 0.572s, learning 3.950s)
               Value function loss: 304.3397
                    Surrogate loss: 0.0105
             Mean action noise std: 0.92
                       Mean reward: 713.40
               Mean episode length: 429.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 4.52s
                        Total time: 6800.55s
                               ETA: 11266.5s

################################################################################
                     [1m Learning iteration 1506/4000 [0m

                       Computation: 1848 steps/s (collection: 0.544s, learning 3.888s)
               Value function loss: 185.9622
                    Surrogate loss: 0.0102
             Mean action noise std: 0.92
                       Mean reward: 721.97
               Mean episode length: 434.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12345344
                    Iteration time: 4.43s
                        Total time: 6804.99s
                               ETA: 11261.9s

################################################################################
                     [1m Learning iteration 1507/4000 [0m

                       Computation: 1822 steps/s (collection: 0.557s, learning 3.937s)
               Value function loss: 191.6108
                    Surrogate loss: 0.0090
             Mean action noise std: 0.92
                       Mean reward: 723.69
               Mean episode length: 436.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 4.49s
                        Total time: 6809.48s
                               ETA: 11257.3s

################################################################################
                     [1m Learning iteration 1508/4000 [0m

                       Computation: 1805 steps/s (collection: 0.617s, learning 3.920s)
               Value function loss: 294.6635
                    Surrogate loss: 0.0089
             Mean action noise std: 0.92
                       Mean reward: 728.47
               Mean episode length: 440.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12361728
                    Iteration time: 4.54s
                        Total time: 6814.02s
                               ETA: 11252.8s

################################################################################
                     [1m Learning iteration 1509/4000 [0m

                       Computation: 1826 steps/s (collection: 0.552s, learning 3.934s)
               Value function loss: 160.9409
                    Surrogate loss: 0.0126
             Mean action noise std: 0.92
                       Mean reward: 724.10
               Mean episode length: 436.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 4.49s
                        Total time: 6818.50s
                               ETA: 11248.3s

################################################################################
                     [1m Learning iteration 1510/4000 [0m

                       Computation: 1816 steps/s (collection: 0.581s, learning 3.929s)
               Value function loss: 120.4914
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 733.18
               Mean episode length: 441.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 12378112
                    Iteration time: 4.51s
                        Total time: 6823.01s
                               ETA: 11243.7s

################################################################################
                     [1m Learning iteration 1511/4000 [0m

                       Computation: 1820 steps/s (collection: 0.582s, learning 3.917s)
               Value function loss: 194.0630
                    Surrogate loss: 0.0095
             Mean action noise std: 0.92
                       Mean reward: 738.34
               Mean episode length: 443.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 4.50s
                        Total time: 6827.51s
                               ETA: 11239.2s

################################################################################
                     [1m Learning iteration 1512/4000 [0m

                       Computation: 1834 steps/s (collection: 0.583s, learning 3.882s)
               Value function loss: 165.1263
                    Surrogate loss: 0.0116
             Mean action noise std: 0.92
                       Mean reward: 725.52
               Mean episode length: 437.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12394496
                    Iteration time: 4.46s
                        Total time: 6831.98s
                               ETA: 11234.6s

################################################################################
                     [1m Learning iteration 1513/4000 [0m

                       Computation: 1825 steps/s (collection: 0.564s, learning 3.923s)
               Value function loss: 176.5928
                    Surrogate loss: 0.0092
             Mean action noise std: 0.92
                       Mean reward: 723.96
               Mean episode length: 435.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 4.49s
                        Total time: 6836.46s
                               ETA: 11230.0s

################################################################################
                     [1m Learning iteration 1514/4000 [0m

                       Computation: 1802 steps/s (collection: 0.587s, learning 3.956s)
               Value function loss: 259.9294
                    Surrogate loss: 0.0095
             Mean action noise std: 0.92
                       Mean reward: 715.14
               Mean episode length: 428.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12410880
                    Iteration time: 4.54s
                        Total time: 6841.01s
                               ETA: 11225.6s

################################################################################
                     [1m Learning iteration 1515/4000 [0m

                       Computation: 1799 steps/s (collection: 0.588s, learning 3.963s)
               Value function loss: 182.6173
                    Surrogate loss: 0.0093
             Mean action noise std: 0.92
                       Mean reward: 706.72
               Mean episode length: 423.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 4.55s
                        Total time: 6845.56s
                               ETA: 11221.1s

################################################################################
                     [1m Learning iteration 1516/4000 [0m

                       Computation: 1814 steps/s (collection: 0.567s, learning 3.947s)
               Value function loss: 145.2998
                    Surrogate loss: 0.0104
             Mean action noise std: 0.92
                       Mean reward: 702.40
               Mean episode length: 419.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12427264
                    Iteration time: 4.51s
                        Total time: 6850.07s
                               ETA: 11216.6s

################################################################################
                     [1m Learning iteration 1517/4000 [0m

                       Computation: 1816 steps/s (collection: 0.575s, learning 3.934s)
               Value function loss: 209.0518
                    Surrogate loss: 0.0113
             Mean action noise std: 0.92
                       Mean reward: 701.08
               Mean episode length: 421.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 4.51s
                        Total time: 6854.58s
                               ETA: 11212.1s

################################################################################
                     [1m Learning iteration 1518/4000 [0m

                       Computation: 1801 steps/s (collection: 0.606s, learning 3.940s)
               Value function loss: 205.1862
                    Surrogate loss: 0.0095
             Mean action noise std: 0.92
                       Mean reward: 689.15
               Mean episode length: 413.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12443648
                    Iteration time: 4.55s
                        Total time: 6859.13s
                               ETA: 11207.6s

################################################################################
                     [1m Learning iteration 1519/4000 [0m

                       Computation: 1786 steps/s (collection: 0.615s, learning 3.969s)
               Value function loss: 286.6780
                    Surrogate loss: 0.0094
             Mean action noise std: 0.92
                       Mean reward: 689.71
               Mean episode length: 411.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 4.58s
                        Total time: 6863.71s
                               ETA: 11203.2s

################################################################################
                     [1m Learning iteration 1520/4000 [0m

                       Computation: 1812 steps/s (collection: 0.549s, learning 3.970s)
               Value function loss: 255.1265
                    Surrogate loss: 0.0112
             Mean action noise std: 0.92
                       Mean reward: 685.70
               Mean episode length: 408.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12460032
                    Iteration time: 4.52s
                        Total time: 6868.23s
                               ETA: 11198.7s

################################################################################
                     [1m Learning iteration 1521/4000 [0m

                       Computation: 1783 steps/s (collection: 0.578s, learning 4.014s)
               Value function loss: 245.3976
                    Surrogate loss: 0.0118
             Mean action noise std: 0.92
                       Mean reward: 676.90
               Mean episode length: 403.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 4.59s
                        Total time: 6872.82s
                               ETA: 11194.3s

################################################################################
                     [1m Learning iteration 1522/4000 [0m

                       Computation: 1802 steps/s (collection: 0.572s, learning 3.974s)
               Value function loss: 182.5398
                    Surrogate loss: 0.0098
             Mean action noise std: 0.92
                       Mean reward: 687.90
               Mean episode length: 410.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12476416
                    Iteration time: 4.55s
                        Total time: 6877.37s
                               ETA: 11189.8s

################################################################################
                     [1m Learning iteration 1523/4000 [0m

                       Computation: 1808 steps/s (collection: 0.535s, learning 3.996s)
               Value function loss: 182.9240
                    Surrogate loss: 0.0088
             Mean action noise std: 0.92
                       Mean reward: 692.07
               Mean episode length: 413.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 4.53s
                        Total time: 6881.90s
                               ETA: 11185.3s

################################################################################
                     [1m Learning iteration 1524/4000 [0m

                       Computation: 1834 steps/s (collection: 0.537s, learning 3.929s)
               Value function loss: 255.4197
                    Surrogate loss: 0.0095
             Mean action noise std: 0.92
                       Mean reward: 707.38
               Mean episode length: 421.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 12492800
                    Iteration time: 4.47s
                        Total time: 6886.37s
                               ETA: 11180.7s

################################################################################
                     [1m Learning iteration 1525/4000 [0m

                       Computation: 1793 steps/s (collection: 0.574s, learning 3.994s)
               Value function loss: 183.7069
                    Surrogate loss: 0.0115
             Mean action noise std: 0.92
                       Mean reward: 708.63
               Mean episode length: 423.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 4.57s
                        Total time: 6890.93s
                               ETA: 11176.3s

################################################################################
                     [1m Learning iteration 1526/4000 [0m

                       Computation: 1845 steps/s (collection: 0.536s, learning 3.902s)
               Value function loss: 118.2911
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 702.80
               Mean episode length: 420.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 31.27
--------------------------------------------------------------------------------
                   Total timesteps: 12509184
                    Iteration time: 4.44s
                        Total time: 6895.37s
                               ETA: 11171.7s

################################################################################
                     [1m Learning iteration 1527/4000 [0m

                       Computation: 1848 steps/s (collection: 0.527s, learning 3.904s)
               Value function loss: 175.4953
                    Surrogate loss: 0.0093
             Mean action noise std: 0.92
                       Mean reward: 713.59
               Mean episode length: 426.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 4.43s
                        Total time: 6899.80s
                               ETA: 11167.0s

################################################################################
                     [1m Learning iteration 1528/4000 [0m

                       Computation: 1832 steps/s (collection: 0.552s, learning 3.919s)
               Value function loss: 192.3822
                    Surrogate loss: 0.0082
             Mean action noise std: 0.92
                       Mean reward: 718.97
               Mean episode length: 427.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12525568
                    Iteration time: 4.47s
                        Total time: 6904.27s
                               ETA: 11162.4s

################################################################################
                     [1m Learning iteration 1529/4000 [0m

                       Computation: 1835 steps/s (collection: 0.579s, learning 3.883s)
               Value function loss: 229.1027
                    Surrogate loss: 0.0101
             Mean action noise std: 0.92
                       Mean reward: 717.11
               Mean episode length: 426.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 4.46s
                        Total time: 6908.74s
                               ETA: 11157.8s

################################################################################
                     [1m Learning iteration 1530/4000 [0m

                       Computation: 1849 steps/s (collection: 0.520s, learning 3.910s)
               Value function loss: 204.6711
                    Surrogate loss: 0.0086
             Mean action noise std: 0.92
                       Mean reward: 721.46
               Mean episode length: 429.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12541952
                    Iteration time: 4.43s
                        Total time: 6913.17s
                               ETA: 11153.2s

################################################################################
                     [1m Learning iteration 1531/4000 [0m

                       Computation: 1834 steps/s (collection: 0.573s, learning 3.893s)
               Value function loss: 211.6869
                    Surrogate loss: 0.0112
             Mean action noise std: 0.92
                       Mean reward: 718.42
               Mean episode length: 429.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 4.47s
                        Total time: 6917.63s
                               ETA: 11148.6s

################################################################################
                     [1m Learning iteration 1532/4000 [0m

                       Computation: 1809 steps/s (collection: 0.591s, learning 3.937s)
               Value function loss: 214.5748
                    Surrogate loss: 0.0107
             Mean action noise std: 0.92
                       Mean reward: 714.50
               Mean episode length: 426.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12558336
                    Iteration time: 4.53s
                        Total time: 6922.16s
                               ETA: 11144.1s

################################################################################
                     [1m Learning iteration 1533/4000 [0m

                       Computation: 1837 steps/s (collection: 0.562s, learning 3.896s)
               Value function loss: 228.7248
                    Surrogate loss: 0.0089
             Mean action noise std: 0.92
                       Mean reward: 721.89
               Mean episode length: 430.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 4.46s
                        Total time: 6926.62s
                               ETA: 11139.5s

################################################################################
                     [1m Learning iteration 1534/4000 [0m

                       Computation: 1837 steps/s (collection: 0.563s, learning 3.894s)
               Value function loss: 233.3510
                    Surrogate loss: 0.0094
             Mean action noise std: 0.92
                       Mean reward: 713.54
               Mean episode length: 426.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12574720
                    Iteration time: 4.46s
                        Total time: 6931.07s
                               ETA: 11134.9s

################################################################################
                     [1m Learning iteration 1535/4000 [0m

                       Computation: 1847 steps/s (collection: 0.556s, learning 3.878s)
               Value function loss: 213.5204
                    Surrogate loss: 0.0082
             Mean action noise std: 0.92
                       Mean reward: 726.10
               Mean episode length: 433.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 4.43s
                        Total time: 6935.51s
                               ETA: 11130.2s

################################################################################
                     [1m Learning iteration 1536/4000 [0m

                       Computation: 1826 steps/s (collection: 0.588s, learning 3.897s)
               Value function loss: 306.2850
                    Surrogate loss: 0.0109
             Mean action noise std: 0.92
                       Mean reward: 739.31
               Mean episode length: 440.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12591104
                    Iteration time: 4.48s
                        Total time: 6939.99s
                               ETA: 11125.7s

################################################################################
                     [1m Learning iteration 1537/4000 [0m

                       Computation: 1849 steps/s (collection: 0.539s, learning 3.889s)
               Value function loss: 354.7334
                    Surrogate loss: 0.0116
             Mean action noise std: 0.92
                       Mean reward: 720.56
               Mean episode length: 428.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 4.43s
                        Total time: 6944.42s
                               ETA: 11121.0s

################################################################################
                     [1m Learning iteration 1538/4000 [0m

                       Computation: 1826 steps/s (collection: 0.555s, learning 3.929s)
               Value function loss: 206.3495
                    Surrogate loss: 0.0088
             Mean action noise std: 0.92
                       Mean reward: 724.44
               Mean episode length: 430.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12607488
                    Iteration time: 4.48s
                        Total time: 6948.91s
                               ETA: 11116.4s

################################################################################
                     [1m Learning iteration 1539/4000 [0m

                       Computation: 1835 steps/s (collection: 0.544s, learning 3.919s)
               Value function loss: 270.6196
                    Surrogate loss: 0.0084
             Mean action noise std: 0.92
                       Mean reward: 721.64
               Mean episode length: 429.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 4.46s
                        Total time: 6953.37s
                               ETA: 11111.8s

################################################################################
                     [1m Learning iteration 1540/4000 [0m

                       Computation: 1832 steps/s (collection: 0.529s, learning 3.941s)
               Value function loss: 195.3574
                    Surrogate loss: 0.0096
             Mean action noise std: 0.92
                       Mean reward: 719.73
               Mean episode length: 428.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12623872
                    Iteration time: 4.47s
                        Total time: 6957.84s
                               ETA: 11107.3s

################################################################################
                     [1m Learning iteration 1541/4000 [0m

                       Computation: 1820 steps/s (collection: 0.571s, learning 3.930s)
               Value function loss: 147.3244
                    Surrogate loss: 0.0099
             Mean action noise std: 0.92
                       Mean reward: 727.49
               Mean episode length: 431.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 4.50s
                        Total time: 6962.34s
                               ETA: 11102.7s

################################################################################
                     [1m Learning iteration 1542/4000 [0m

                       Computation: 1843 steps/s (collection: 0.544s, learning 3.900s)
               Value function loss: 113.4287
                    Surrogate loss: 0.0100
             Mean action noise std: 0.92
                       Mean reward: 728.59
               Mean episode length: 431.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 31.27
--------------------------------------------------------------------------------
                   Total timesteps: 12640256
                    Iteration time: 4.44s
                        Total time: 6966.78s
                               ETA: 11098.1s

################################################################################
                     [1m Learning iteration 1543/4000 [0m

                       Computation: 1845 steps/s (collection: 0.524s, learning 3.916s)
               Value function loss: 213.9799
                    Surrogate loss: 0.0083
             Mean action noise std: 0.92
                       Mean reward: 741.25
               Mean episode length: 438.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 4.44s
                        Total time: 6971.22s
                               ETA: 11093.5s

################################################################################
                     [1m Learning iteration 1544/4000 [0m

                       Computation: 1842 steps/s (collection: 0.557s, learning 3.890s)
               Value function loss: 279.9739
                    Surrogate loss: 0.0089
             Mean action noise std: 0.92
                       Mean reward: 747.06
               Mean episode length: 443.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12656640
                    Iteration time: 4.45s
                        Total time: 6975.67s
                               ETA: 11088.8s

################################################################################
                     [1m Learning iteration 1545/4000 [0m

                       Computation: 1831 steps/s (collection: 0.542s, learning 3.931s)
               Value function loss: 209.2158
                    Surrogate loss: 0.0101
             Mean action noise std: 0.92
                       Mean reward: 744.26
               Mean episode length: 441.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 4.47s
                        Total time: 6980.14s
                               ETA: 11084.3s

################################################################################
                     [1m Learning iteration 1546/4000 [0m

                       Computation: 1813 steps/s (collection: 0.561s, learning 3.957s)
               Value function loss: 155.3905
                    Surrogate loss: 0.0094
             Mean action noise std: 0.93
                       Mean reward: 726.38
               Mean episode length: 430.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12673024
                    Iteration time: 4.52s
                        Total time: 6984.66s
                               ETA: 11079.7s

################################################################################
                     [1m Learning iteration 1547/4000 [0m

                       Computation: 1787 steps/s (collection: 0.542s, learning 4.042s)
               Value function loss: 206.6993
                    Surrogate loss: 0.0097
             Mean action noise std: 0.93
                       Mean reward: 716.33
               Mean episode length: 425.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 4.58s
                        Total time: 6989.25s
                               ETA: 11075.3s

################################################################################
                     [1m Learning iteration 1548/4000 [0m

                       Computation: 1797 steps/s (collection: 0.578s, learning 3.978s)
               Value function loss: 224.0724
                    Surrogate loss: 0.0084
             Mean action noise std: 0.93
                       Mean reward: 722.74
               Mean episode length: 430.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12689408
                    Iteration time: 4.56s
                        Total time: 6993.80s
                               ETA: 11070.9s

################################################################################
                     [1m Learning iteration 1549/4000 [0m

                       Computation: 1809 steps/s (collection: 0.548s, learning 3.980s)
               Value function loss: 280.8127
                    Surrogate loss: 0.0075
             Mean action noise std: 0.93
                       Mean reward: 704.59
               Mean episode length: 422.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 4.53s
                        Total time: 6998.33s
                               ETA: 11066.4s

################################################################################
                     [1m Learning iteration 1550/4000 [0m

                       Computation: 1803 steps/s (collection: 0.550s, learning 3.993s)
               Value function loss: 238.6547
                    Surrogate loss: 0.0127
             Mean action noise std: 0.93
                       Mean reward: 707.64
               Mean episode length: 424.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12705792
                    Iteration time: 4.54s
                        Total time: 7002.87s
                               ETA: 11061.9s

################################################################################
                     [1m Learning iteration 1551/4000 [0m

                       Computation: 1763 steps/s (collection: 0.586s, learning 4.058s)
               Value function loss: 195.7022
                    Surrogate loss: 0.0122
             Mean action noise std: 0.93
                       Mean reward: 695.38
               Mean episode length: 418.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 4.64s
                        Total time: 7007.52s
                               ETA: 11057.6s

################################################################################
                     [1m Learning iteration 1552/4000 [0m

                       Computation: 1821 steps/s (collection: 0.549s, learning 3.947s)
               Value function loss: 277.4128
                    Surrogate loss: 0.0102
             Mean action noise std: 0.93
                       Mean reward: 699.98
               Mean episode length: 422.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12722176
                    Iteration time: 4.50s
                        Total time: 7012.01s
                               ETA: 11053.1s

################################################################################
                     [1m Learning iteration 1553/4000 [0m

                       Computation: 1807 steps/s (collection: 0.557s, learning 3.975s)
               Value function loss: 255.4072
                    Surrogate loss: 0.0108
             Mean action noise std: 0.93
                       Mean reward: 695.76
               Mean episode length: 419.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 4.53s
                        Total time: 7016.55s
                               ETA: 11048.6s

################################################################################
                     [1m Learning iteration 1554/4000 [0m

                       Computation: 1805 steps/s (collection: 0.608s, learning 3.929s)
               Value function loss: 139.3811
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 702.10
               Mean episode length: 422.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 12738560
                    Iteration time: 4.54s
                        Total time: 7021.08s
                               ETA: 11044.1s

################################################################################
                     [1m Learning iteration 1555/4000 [0m

                       Computation: 1785 steps/s (collection: 0.617s, learning 3.972s)
               Value function loss: 247.1338
                    Surrogate loss: 0.0122
             Mean action noise std: 0.93
                       Mean reward: 701.01
               Mean episode length: 423.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 4.59s
                        Total time: 7025.67s
                               ETA: 11039.7s

################################################################################
                     [1m Learning iteration 1556/4000 [0m

                       Computation: 1817 steps/s (collection: 0.585s, learning 3.923s)
               Value function loss: 125.0404
                    Surrogate loss: 0.0111
             Mean action noise std: 0.93
                       Mean reward: 717.92
               Mean episode length: 433.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12754944
                    Iteration time: 4.51s
                        Total time: 7030.18s
                               ETA: 11035.2s

################################################################################
                     [1m Learning iteration 1557/4000 [0m

                       Computation: 1787 steps/s (collection: 0.609s, learning 3.972s)
               Value function loss: 84.9546
                    Surrogate loss: 0.0114
             Mean action noise std: 0.93
                       Mean reward: 716.20
               Mean episode length: 431.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 4.58s
                        Total time: 7034.76s
                               ETA: 11030.8s

################################################################################
                     [1m Learning iteration 1558/4000 [0m

                       Computation: 1819 steps/s (collection: 0.548s, learning 3.955s)
               Value function loss: 138.2684
                    Surrogate loss: 0.0095
             Mean action noise std: 0.93
                       Mean reward: 704.15
               Mean episode length: 424.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12771328
                    Iteration time: 4.50s
                        Total time: 7039.26s
                               ETA: 11026.2s

################################################################################
                     [1m Learning iteration 1559/4000 [0m

                       Computation: 1811 steps/s (collection: 0.573s, learning 3.950s)
               Value function loss: 158.0861
                    Surrogate loss: 0.0094
             Mean action noise std: 0.93
                       Mean reward: 719.17
               Mean episode length: 431.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 4.52s
                        Total time: 7043.78s
                               ETA: 11021.7s

################################################################################
                     [1m Learning iteration 1560/4000 [0m

                       Computation: 1816 steps/s (collection: 0.558s, learning 3.953s)
               Value function loss: 221.2179
                    Surrogate loss: 0.0078
             Mean action noise std: 0.93
                       Mean reward: 739.45
               Mean episode length: 442.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12787712
                    Iteration time: 4.51s
                        Total time: 7048.29s
                               ETA: 11017.2s

################################################################################
                     [1m Learning iteration 1561/4000 [0m

                       Computation: 1804 steps/s (collection: 0.607s, learning 3.932s)
               Value function loss: 194.2201
                    Surrogate loss: 0.0074
             Mean action noise std: 0.93
                       Mean reward: 732.46
               Mean episode length: 439.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 4.54s
                        Total time: 7052.83s
                               ETA: 11012.7s

################################################################################
                     [1m Learning iteration 1562/4000 [0m

                       Computation: 1837 steps/s (collection: 0.542s, learning 3.915s)
               Value function loss: 244.6595
                    Surrogate loss: 0.0089
             Mean action noise std: 0.93
                       Mean reward: 744.19
               Mean episode length: 443.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12804096
                    Iteration time: 4.46s
                        Total time: 7057.29s
                               ETA: 11008.1s

################################################################################
                     [1m Learning iteration 1563/4000 [0m

                       Computation: 1808 steps/s (collection: 0.599s, learning 3.930s)
               Value function loss: 122.4541
                    Surrogate loss: 0.0126
             Mean action noise std: 0.93
                       Mean reward: 737.07
               Mean episode length: 439.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 4.53s
                        Total time: 7061.82s
                               ETA: 11003.6s

################################################################################
                     [1m Learning iteration 1564/4000 [0m

                       Computation: 1824 steps/s (collection: 0.566s, learning 3.924s)
               Value function loss: 221.3226
                    Surrogate loss: 0.0127
             Mean action noise std: 0.93
                       Mean reward: 741.03
               Mean episode length: 442.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12820480
                    Iteration time: 4.49s
                        Total time: 7066.31s
                               ETA: 10999.1s

################################################################################
                     [1m Learning iteration 1565/4000 [0m

                       Computation: 1850 steps/s (collection: 0.548s, learning 3.880s)
               Value function loss: 298.7516
                    Surrogate loss: 0.0119
             Mean action noise std: 0.93
                       Mean reward: 756.32
               Mean episode length: 451.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 4.43s
                        Total time: 7070.74s
                               ETA: 10994.4s

################################################################################
                     [1m Learning iteration 1566/4000 [0m

                       Computation: 1830 steps/s (collection: 0.558s, learning 3.917s)
               Value function loss: 246.4064
                    Surrogate loss: 0.0101
             Mean action noise std: 0.93
                       Mean reward: 763.04
               Mean episode length: 452.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12836864
                    Iteration time: 4.47s
                        Total time: 7075.21s
                               ETA: 10989.8s

################################################################################
                     [1m Learning iteration 1567/4000 [0m

                       Computation: 1785 steps/s (collection: 0.617s, learning 3.972s)
               Value function loss: 211.9142
                    Surrogate loss: 0.0101
             Mean action noise std: 0.93
                       Mean reward: 774.72
               Mean episode length: 460.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 4.59s
                        Total time: 7079.80s
                               ETA: 10985.4s

################################################################################
                     [1m Learning iteration 1568/4000 [0m

                       Computation: 1797 steps/s (collection: 0.593s, learning 3.965s)
               Value function loss: 287.8957
                    Surrogate loss: 0.0086
             Mean action noise std: 0.93
                       Mean reward: 783.01
               Mean episode length: 464.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12853248
                    Iteration time: 4.56s
                        Total time: 7084.36s
                               ETA: 10981.0s

################################################################################
                     [1m Learning iteration 1569/4000 [0m

                       Computation: 1788 steps/s (collection: 0.596s, learning 3.983s)
               Value function loss: 253.4116
                    Surrogate loss: 0.0100
             Mean action noise std: 0.93
                       Mean reward: 802.78
               Mean episode length: 476.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 4.58s
                        Total time: 7088.94s
                               ETA: 10976.6s

################################################################################
                     [1m Learning iteration 1570/4000 [0m

                       Computation: 1785 steps/s (collection: 0.643s, learning 3.944s)
               Value function loss: 188.6874
                    Surrogate loss: 0.0110
             Mean action noise std: 0.93
                       Mean reward: 795.32
               Mean episode length: 470.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12869632
                    Iteration time: 4.59s
                        Total time: 7093.53s
                               ETA: 10972.2s

################################################################################
                     [1m Learning iteration 1571/4000 [0m

                       Computation: 1809 steps/s (collection: 0.605s, learning 3.923s)
               Value function loss: 260.2169
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 778.23
               Mean episode length: 461.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 4.53s
                        Total time: 7098.06s
                               ETA: 10967.7s

################################################################################
                     [1m Learning iteration 1572/4000 [0m

                       Computation: 1822 steps/s (collection: 0.591s, learning 3.905s)
               Value function loss: 177.0510
                    Surrogate loss: 0.0079
             Mean action noise std: 0.93
                       Mean reward: 770.60
               Mean episode length: 457.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12886016
                    Iteration time: 4.50s
                        Total time: 7102.55s
                               ETA: 10963.1s

################################################################################
                     [1m Learning iteration 1573/4000 [0m

                       Computation: 1804 steps/s (collection: 0.592s, learning 3.948s)
               Value function loss: 95.7808
                    Surrogate loss: 0.0105
             Mean action noise std: 0.93
                       Mean reward: 761.60
               Mean episode length: 453.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 4.54s
                        Total time: 7107.09s
                               ETA: 10958.6s

################################################################################
                     [1m Learning iteration 1574/4000 [0m

                       Computation: 1790 steps/s (collection: 0.627s, learning 3.949s)
               Value function loss: 144.0231
                    Surrogate loss: 0.0107
             Mean action noise std: 0.93
                       Mean reward: 764.59
               Mean episode length: 455.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12902400
                    Iteration time: 4.58s
                        Total time: 7111.67s
                               ETA: 10954.2s

################################################################################
                     [1m Learning iteration 1575/4000 [0m

                       Computation: 1824 steps/s (collection: 0.562s, learning 3.928s)
               Value function loss: 218.5188
                    Surrogate loss: 0.0080
             Mean action noise std: 0.93
                       Mean reward: 768.38
               Mean episode length: 455.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 4.49s
                        Total time: 7116.16s
                               ETA: 10949.7s

################################################################################
                     [1m Learning iteration 1576/4000 [0m

                       Computation: 1800 steps/s (collection: 0.607s, learning 3.943s)
               Value function loss: 198.6976
                    Surrogate loss: 0.0093
             Mean action noise std: 0.93
                       Mean reward: 771.00
               Mean episode length: 455.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12918784
                    Iteration time: 4.55s
                        Total time: 7120.71s
                               ETA: 10945.2s

################################################################################
                     [1m Learning iteration 1577/4000 [0m

                       Computation: 1841 steps/s (collection: 0.547s, learning 3.902s)
               Value function loss: 249.5279
                    Surrogate loss: 0.0083
             Mean action noise std: 0.93
                       Mean reward: 768.48
               Mean episode length: 455.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 4.45s
                        Total time: 7125.16s
                               ETA: 10940.6s

################################################################################
                     [1m Learning iteration 1578/4000 [0m

                       Computation: 1783 steps/s (collection: 0.619s, learning 3.974s)
               Value function loss: 233.7204
                    Surrogate loss: 0.0087
             Mean action noise std: 0.93
                       Mean reward: 780.78
               Mean episode length: 461.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12935168
                    Iteration time: 4.59s
                        Total time: 7129.75s
                               ETA: 10936.2s

################################################################################
                     [1m Learning iteration 1579/4000 [0m

                       Computation: 1826 steps/s (collection: 0.550s, learning 3.935s)
               Value function loss: 205.9548
                    Surrogate loss: 0.0099
             Mean action noise std: 0.93
                       Mean reward: 783.41
               Mean episode length: 461.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 4.49s
                        Total time: 7134.23s
                               ETA: 10931.6s

################################################################################
                     [1m Learning iteration 1580/4000 [0m

                       Computation: 1811 steps/s (collection: 0.589s, learning 3.934s)
               Value function loss: 190.7587
                    Surrogate loss: 0.0069
             Mean action noise std: 0.93
                       Mean reward: 769.63
               Mean episode length: 454.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12951552
                    Iteration time: 4.52s
                        Total time: 7138.76s
                               ETA: 10927.1s

################################################################################
                     [1m Learning iteration 1581/4000 [0m

                       Computation: 1826 steps/s (collection: 0.593s, learning 3.893s)
               Value function loss: 227.0261
                    Surrogate loss: 0.0086
             Mean action noise std: 0.93
                       Mean reward: 764.34
               Mean episode length: 451.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 4.49s
                        Total time: 7143.24s
                               ETA: 10922.6s

################################################################################
                     [1m Learning iteration 1582/4000 [0m

                       Computation: 1791 steps/s (collection: 0.613s, learning 3.960s)
               Value function loss: 191.3042
                    Surrogate loss: 0.0116
             Mean action noise std: 0.93
                       Mean reward: 762.67
               Mean episode length: 452.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12967936
                    Iteration time: 4.57s
                        Total time: 7147.82s
                               ETA: 10918.1s

################################################################################
                     [1m Learning iteration 1583/4000 [0m

                       Computation: 1810 steps/s (collection: 0.621s, learning 3.904s)
               Value function loss: 286.2971
                    Surrogate loss: 0.0074
             Mean action noise std: 0.93
                       Mean reward: 781.30
               Mean episode length: 462.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 4.53s
                        Total time: 7152.34s
                               ETA: 10913.6s

################################################################################
                     [1m Learning iteration 1584/4000 [0m

                       Computation: 1826 steps/s (collection: 0.562s, learning 3.922s)
               Value function loss: 241.3153
                    Surrogate loss: 0.0111
             Mean action noise std: 0.93
                       Mean reward: 788.77
               Mean episode length: 466.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12984320
                    Iteration time: 4.48s
                        Total time: 7156.82s
                               ETA: 10909.1s

################################################################################
                     [1m Learning iteration 1585/4000 [0m

                       Computation: 1811 steps/s (collection: 0.608s, learning 3.916s)
               Value function loss: 185.6254
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 772.24
               Mean episode length: 457.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 4.52s
                        Total time: 7161.35s
                               ETA: 10904.6s

################################################################################
                     [1m Learning iteration 1586/4000 [0m

                       Computation: 1819 steps/s (collection: 0.555s, learning 3.948s)
               Value function loss: 293.8823
                    Surrogate loss: 0.0135
             Mean action noise std: 0.93
                       Mean reward: 762.04
               Mean episode length: 453.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13000704
                    Iteration time: 4.50s
                        Total time: 7165.85s
                               ETA: 10900.0s

################################################################################
                     [1m Learning iteration 1587/4000 [0m

                       Computation: 1799 steps/s (collection: 0.604s, learning 3.948s)
               Value function loss: 216.0424
                    Surrogate loss: 0.0077
             Mean action noise std: 0.93
                       Mean reward: 721.72
               Mean episode length: 430.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 4.55s
                        Total time: 7170.40s
                               ETA: 10895.6s

################################################################################
                     [1m Learning iteration 1588/4000 [0m

                       Computation: 1821 steps/s (collection: 0.579s, learning 3.917s)
               Value function loss: 201.7383
                    Surrogate loss: 0.0125
             Mean action noise std: 0.93
                       Mean reward: 701.00
               Mean episode length: 418.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13017088
                    Iteration time: 4.50s
                        Total time: 7174.90s
                               ETA: 10891.0s

################################################################################
                     [1m Learning iteration 1589/4000 [0m

                       Computation: 1817 steps/s (collection: 0.551s, learning 3.956s)
               Value function loss: 154.8104
                    Surrogate loss: 0.0106
             Mean action noise std: 0.93
                       Mean reward: 713.22
               Mean episode length: 424.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 4.51s
                        Total time: 7179.41s
                               ETA: 10886.5s

################################################################################
                     [1m Learning iteration 1590/4000 [0m

                       Computation: 1818 steps/s (collection: 0.601s, learning 3.904s)
               Value function loss: 172.6470
                    Surrogate loss: 0.0088
             Mean action noise std: 0.93
                       Mean reward: 707.48
               Mean episode length: 422.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13033472
                    Iteration time: 4.51s
                        Total time: 7183.91s
                               ETA: 10882.0s

################################################################################
                     [1m Learning iteration 1591/4000 [0m

                       Computation: 1802 steps/s (collection: 0.623s, learning 3.922s)
               Value function loss: 201.4298
                    Surrogate loss: 0.0107
             Mean action noise std: 0.93
                       Mean reward: 693.36
               Mean episode length: 413.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 4.55s
                        Total time: 7188.46s
                               ETA: 10877.5s

################################################################################
                     [1m Learning iteration 1592/4000 [0m

                       Computation: 1822 steps/s (collection: 0.584s, learning 3.912s)
               Value function loss: 219.2229
                    Surrogate loss: 0.0088
             Mean action noise std: 0.93
                       Mean reward: 691.66
               Mean episode length: 411.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13049856
                    Iteration time: 4.50s
                        Total time: 7192.95s
                               ETA: 10873.0s

################################################################################
                     [1m Learning iteration 1593/4000 [0m

                       Computation: 1808 steps/s (collection: 0.620s, learning 3.909s)
               Value function loss: 156.3800
                    Surrogate loss: 0.0124
             Mean action noise std: 0.93
                       Mean reward: 687.35
               Mean episode length: 409.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 4.53s
                        Total time: 7197.48s
                               ETA: 10868.5s

################################################################################
                     [1m Learning iteration 1594/4000 [0m

                       Computation: 1830 steps/s (collection: 0.548s, learning 3.926s)
               Value function loss: 183.7933
                    Surrogate loss: 0.0104
             Mean action noise std: 0.93
                       Mean reward: 667.94
               Mean episode length: 398.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13066240
                    Iteration time: 4.47s
                        Total time: 7201.96s
                               ETA: 10863.9s

################################################################################
                     [1m Learning iteration 1595/4000 [0m

                       Computation: 1804 steps/s (collection: 0.582s, learning 3.959s)
               Value function loss: 195.5604
                    Surrogate loss: 0.0108
             Mean action noise std: 0.93
                       Mean reward: 669.17
               Mean episode length: 400.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 4.54s
                        Total time: 7206.50s
                               ETA: 10859.4s

################################################################################
                     [1m Learning iteration 1596/4000 [0m

                       Computation: 1815 steps/s (collection: 0.575s, learning 3.937s)
               Value function loss: 225.0582
                    Surrogate loss: 0.0107
             Mean action noise std: 0.93
                       Mean reward: 666.02
               Mean episode length: 399.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13082624
                    Iteration time: 4.51s
                        Total time: 7211.01s
                               ETA: 10854.9s

################################################################################
                     [1m Learning iteration 1597/4000 [0m

                       Computation: 1834 steps/s (collection: 0.566s, learning 3.900s)
               Value function loss: 248.5585
                    Surrogate loss: 0.0093
             Mean action noise std: 0.93
                       Mean reward: 676.39
               Mean episode length: 405.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 4.47s
                        Total time: 7215.47s
                               ETA: 10850.3s

################################################################################
                     [1m Learning iteration 1598/4000 [0m

                       Computation: 1811 steps/s (collection: 0.590s, learning 3.931s)
               Value function loss: 183.6548
                    Surrogate loss: 0.0102
             Mean action noise std: 0.93
                       Mean reward: 675.32
               Mean episode length: 405.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13099008
                    Iteration time: 4.52s
                        Total time: 7220.00s
                               ETA: 10845.8s

################################################################################
                     [1m Learning iteration 1599/4000 [0m

                       Computation: 1825 steps/s (collection: 0.540s, learning 3.948s)
               Value function loss: 209.9277
                    Surrogate loss: 0.0117
             Mean action noise std: 0.93
                       Mean reward: 679.51
               Mean episode length: 411.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 4.49s
                        Total time: 7224.48s
                               ETA: 10841.2s

################################################################################
                     [1m Learning iteration 1600/4000 [0m

                       Computation: 1805 steps/s (collection: 0.561s, learning 3.976s)
               Value function loss: 238.0927
                    Surrogate loss: 0.0127
             Mean action noise std: 0.93
                       Mean reward: 684.43
               Mean episode length: 416.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13115392
                    Iteration time: 4.54s
                        Total time: 7229.02s
                               ETA: 10836.8s

################################################################################
                     [1m Learning iteration 1601/4000 [0m

                       Computation: 1797 steps/s (collection: 0.581s, learning 3.975s)
               Value function loss: 174.3105
                    Surrogate loss: 0.0101
             Mean action noise std: 0.93
                       Mean reward: 669.03
               Mean episode length: 408.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 4.56s
                        Total time: 7233.58s
                               ETA: 10832.3s

################################################################################
                     [1m Learning iteration 1602/4000 [0m

                       Computation: 1775 steps/s (collection: 0.588s, learning 4.027s)
               Value function loss: 404.6292
                    Surrogate loss: 0.0107
             Mean action noise std: 0.93
                       Mean reward: 676.31
               Mean episode length: 412.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13131776
                    Iteration time: 4.61s
                        Total time: 7238.19s
                               ETA: 10827.9s

################################################################################
                     [1m Learning iteration 1603/4000 [0m

                       Computation: 1802 steps/s (collection: 0.574s, learning 3.972s)
               Value function loss: 169.3990
                    Surrogate loss: 0.0119
             Mean action noise std: 0.93
                       Mean reward: 678.27
               Mean episode length: 416.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 4.55s
                        Total time: 7242.74s
                               ETA: 10823.5s

################################################################################
                     [1m Learning iteration 1604/4000 [0m

                       Computation: 1821 steps/s (collection: 0.527s, learning 3.970s)
               Value function loss: 182.5065
                    Surrogate loss: 0.0120
             Mean action noise std: 0.93
                       Mean reward: 675.46
               Mean episode length: 414.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13148160
                    Iteration time: 4.50s
                        Total time: 7247.23s
                               ETA: 10818.9s

################################################################################
                     [1m Learning iteration 1605/4000 [0m

                       Computation: 1817 steps/s (collection: 0.576s, learning 3.931s)
               Value function loss: 163.0176
                    Surrogate loss: 0.0111
             Mean action noise std: 0.93
                       Mean reward: 666.60
               Mean episode length: 409.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 4.51s
                        Total time: 7251.74s
                               ETA: 10814.4s

################################################################################
                     [1m Learning iteration 1606/4000 [0m

                       Computation: 1833 steps/s (collection: 0.556s, learning 3.911s)
               Value function loss: 224.2207
                    Surrogate loss: 0.0105
             Mean action noise std: 0.93
                       Mean reward: 672.87
               Mean episode length: 412.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13164544
                    Iteration time: 4.47s
                        Total time: 7256.21s
                               ETA: 10809.8s

################################################################################
                     [1m Learning iteration 1607/4000 [0m

                       Computation: 1823 steps/s (collection: 0.567s, learning 3.925s)
               Value function loss: 172.8124
                    Surrogate loss: 0.0109
             Mean action noise std: 0.93
                       Mean reward: 648.95
               Mean episode length: 401.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 4.49s
                        Total time: 7260.70s
                               ETA: 10805.3s

################################################################################
                     [1m Learning iteration 1608/4000 [0m

                       Computation: 1845 steps/s (collection: 0.548s, learning 3.890s)
               Value function loss: 185.0546
                    Surrogate loss: 0.0110
             Mean action noise std: 0.93
                       Mean reward: 666.51
               Mean episode length: 411.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13180928
                    Iteration time: 4.44s
                        Total time: 7265.14s
                               ETA: 10800.6s

################################################################################
                     [1m Learning iteration 1609/4000 [0m

                       Computation: 1816 steps/s (collection: 0.544s, learning 3.964s)
               Value function loss: 278.1820
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 679.79
               Mean episode length: 416.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 4.51s
                        Total time: 7269.65s
                               ETA: 10796.1s

################################################################################
                     [1m Learning iteration 1610/4000 [0m

                       Computation: 1867 steps/s (collection: 0.528s, learning 3.858s)
               Value function loss: 192.4185
                    Surrogate loss: 0.0077
             Mean action noise std: 0.93
                       Mean reward: 673.79
               Mean episode length: 411.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13197312
                    Iteration time: 4.39s
                        Total time: 7274.03s
                               ETA: 10791.4s

################################################################################
                     [1m Learning iteration 1611/4000 [0m

                       Computation: 1820 steps/s (collection: 0.594s, learning 3.904s)
               Value function loss: 296.6614
                    Surrogate loss: 0.0102
             Mean action noise std: 0.93
                       Mean reward: 678.11
               Mean episode length: 413.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 4.50s
                        Total time: 7278.53s
                               ETA: 10786.9s

################################################################################
                     [1m Learning iteration 1612/4000 [0m

                       Computation: 1836 steps/s (collection: 0.546s, learning 3.915s)
               Value function loss: 199.0395
                    Surrogate loss: 0.0095
             Mean action noise std: 0.93
                       Mean reward: 683.60
               Mean episode length: 417.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13213696
                    Iteration time: 4.46s
                        Total time: 7282.99s
                               ETA: 10782.3s

################################################################################
                     [1m Learning iteration 1613/4000 [0m

                       Computation: 1847 steps/s (collection: 0.529s, learning 3.904s)
               Value function loss: 184.7562
                    Surrogate loss: 0.0100
             Mean action noise std: 0.93
                       Mean reward: 670.79
               Mean episode length: 408.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 4.43s
                        Total time: 7287.43s
                               ETA: 10777.6s

################################################################################
                     [1m Learning iteration 1614/4000 [0m

                       Computation: 1828 steps/s (collection: 0.555s, learning 3.926s)
               Value function loss: 248.9641
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 685.59
               Mean episode length: 415.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13230080
                    Iteration time: 4.48s
                        Total time: 7291.91s
                               ETA: 10773.1s

################################################################################
                     [1m Learning iteration 1615/4000 [0m

                       Computation: 1833 steps/s (collection: 0.564s, learning 3.904s)
               Value function loss: 235.7346
                    Surrogate loss: 0.0101
             Mean action noise std: 0.93
                       Mean reward: 713.66
               Mean episode length: 431.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 4.47s
                        Total time: 7296.37s
                               ETA: 10768.5s

################################################################################
                     [1m Learning iteration 1616/4000 [0m

                       Computation: 1845 steps/s (collection: 0.538s, learning 3.902s)
               Value function loss: 208.7070
                    Surrogate loss: 0.0106
             Mean action noise std: 0.93
                       Mean reward: 700.76
               Mean episode length: 422.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13246464
                    Iteration time: 4.44s
                        Total time: 7300.81s
                               ETA: 10763.8s

################################################################################
                     [1m Learning iteration 1617/4000 [0m

                       Computation: 1834 steps/s (collection: 0.563s, learning 3.903s)
               Value function loss: 165.9586
                    Surrogate loss: 0.0120
             Mean action noise std: 0.93
                       Mean reward: 708.94
               Mean episode length: 427.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 4.47s
                        Total time: 7305.28s
                               ETA: 10759.3s

################################################################################
                     [1m Learning iteration 1618/4000 [0m

                       Computation: 1834 steps/s (collection: 0.556s, learning 3.909s)
               Value function loss: 276.1943
                    Surrogate loss: 0.0102
             Mean action noise std: 0.93
                       Mean reward: 699.32
               Mean episode length: 422.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13262848
                    Iteration time: 4.46s
                        Total time: 7309.75s
                               ETA: 10754.7s

################################################################################
                     [1m Learning iteration 1619/4000 [0m

                       Computation: 1853 steps/s (collection: 0.539s, learning 3.882s)
               Value function loss: 182.7228
                    Surrogate loss: 0.0103
             Mean action noise std: 0.93
                       Mean reward: 701.72
               Mean episode length: 426.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 4.42s
                        Total time: 7314.17s
                               ETA: 10750.0s

################################################################################
                     [1m Learning iteration 1620/4000 [0m

                       Computation: 1841 steps/s (collection: 0.525s, learning 3.923s)
               Value function loss: 155.7934
                    Surrogate loss: 0.0096
             Mean action noise std: 0.93
                       Mean reward: 701.30
               Mean episode length: 426.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13279232
                    Iteration time: 4.45s
                        Total time: 7318.61s
                               ETA: 10745.4s

################################################################################
                     [1m Learning iteration 1621/4000 [0m

                       Computation: 1846 steps/s (collection: 0.520s, learning 3.916s)
               Value function loss: 210.1962
                    Surrogate loss: 0.0103
             Mean action noise std: 0.93
                       Mean reward: 693.15
               Mean episode length: 422.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 4.44s
                        Total time: 7323.05s
                               ETA: 10740.8s

################################################################################
                     [1m Learning iteration 1622/4000 [0m

                       Computation: 1843 steps/s (collection: 0.547s, learning 3.897s)
               Value function loss: 191.2054
                    Surrogate loss: 0.0100
             Mean action noise std: 0.93
                       Mean reward: 691.62
               Mean episode length: 421.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13295616
                    Iteration time: 4.44s
                        Total time: 7327.49s
                               ETA: 10736.2s

################################################################################
                     [1m Learning iteration 1623/4000 [0m

                       Computation: 1863 steps/s (collection: 0.518s, learning 3.879s)
               Value function loss: 154.3896
                    Surrogate loss: 0.0117
             Mean action noise std: 0.93
                       Mean reward: 687.53
               Mean episode length: 419.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 4.40s
                        Total time: 7331.89s
                               ETA: 10731.5s

################################################################################
                     [1m Learning iteration 1624/4000 [0m

                       Computation: 1856 steps/s (collection: 0.516s, learning 3.895s)
               Value function loss: 165.7924
                    Surrogate loss: 0.0095
             Mean action noise std: 0.93
                       Mean reward: 683.29
               Mean episode length: 417.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13312000
                    Iteration time: 4.41s
                        Total time: 7336.30s
                               ETA: 10726.8s

################################################################################
                     [1m Learning iteration 1625/4000 [0m

                       Computation: 1836 steps/s (collection: 0.569s, learning 3.893s)
               Value function loss: 221.1552
                    Surrogate loss: 0.0094
             Mean action noise std: 0.93
                       Mean reward: 665.68
               Mean episode length: 405.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 4.46s
                        Total time: 7340.76s
                               ETA: 10722.2s

################################################################################
                     [1m Learning iteration 1626/4000 [0m

                       Computation: 1820 steps/s (collection: 0.543s, learning 3.958s)
               Value function loss: 175.4903
                    Surrogate loss: 0.0109
             Mean action noise std: 0.93
                       Mean reward: 673.75
               Mean episode length: 409.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13328384
                    Iteration time: 4.50s
                        Total time: 7345.27s
                               ETA: 10717.7s

################################################################################
                     [1m Learning iteration 1627/4000 [0m

                       Computation: 1814 steps/s (collection: 0.604s, learning 3.910s)
               Value function loss: 272.6133
                    Surrogate loss: 0.0105
             Mean action noise std: 0.93
                       Mean reward: 691.00
               Mean episode length: 420.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 4.51s
                        Total time: 7349.78s
                               ETA: 10713.2s

################################################################################
                     [1m Learning iteration 1628/4000 [0m

                       Computation: 1814 steps/s (collection: 0.573s, learning 3.942s)
               Value function loss: 190.5826
                    Surrogate loss: 0.0100
             Mean action noise std: 0.93
                       Mean reward: 687.27
               Mean episode length: 418.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13344768
                    Iteration time: 4.52s
                        Total time: 7354.29s
                               ETA: 10708.6s

################################################################################
                     [1m Learning iteration 1629/4000 [0m

                       Computation: 1790 steps/s (collection: 0.609s, learning 3.967s)
               Value function loss: 235.1173
                    Surrogate loss: 0.0086
             Mean action noise std: 0.93
                       Mean reward: 699.77
               Mean episode length: 427.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 4.58s
                        Total time: 7358.87s
                               ETA: 10704.2s

################################################################################
                     [1m Learning iteration 1630/4000 [0m

                       Computation: 1822 steps/s (collection: 0.557s, learning 3.937s)
               Value function loss: 286.9932
                    Surrogate loss: 0.0098
             Mean action noise std: 0.93
                       Mean reward: 683.15
               Mean episode length: 417.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13361152
                    Iteration time: 4.49s
                        Total time: 7363.36s
                               ETA: 10699.7s

################################################################################
                     [1m Learning iteration 1631/4000 [0m

                       Computation: 1792 steps/s (collection: 0.657s, learning 3.914s)
               Value function loss: 225.1337
                    Surrogate loss: 0.0090
             Mean action noise std: 0.93
                       Mean reward: 689.99
               Mean episode length: 421.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 4.57s
                        Total time: 7367.93s
                               ETA: 10695.2s

################################################################################
                     [1m Learning iteration 1632/4000 [0m

                       Computation: 1836 steps/s (collection: 0.566s, learning 3.894s)
               Value function loss: 231.5193
                    Surrogate loss: 0.0095
             Mean action noise std: 0.93
                       Mean reward: 705.94
               Mean episode length: 430.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13377536
                    Iteration time: 4.46s
                        Total time: 7372.39s
                               ETA: 10690.6s

################################################################################
                     [1m Learning iteration 1633/4000 [0m

                       Computation: 1796 steps/s (collection: 0.610s, learning 3.950s)
               Value function loss: 299.9884
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 710.59
               Mean episode length: 433.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 4.56s
                        Total time: 7376.96s
                               ETA: 10686.2s

################################################################################
                     [1m Learning iteration 1634/4000 [0m

                       Computation: 1825 steps/s (collection: 0.590s, learning 3.898s)
               Value function loss: 192.5078
                    Surrogate loss: 0.0106
             Mean action noise std: 0.93
                       Mean reward: 713.14
               Mean episode length: 436.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13393920
                    Iteration time: 4.49s
                        Total time: 7381.44s
                               ETA: 10681.6s

################################################################################
                     [1m Learning iteration 1635/4000 [0m

                       Computation: 1814 steps/s (collection: 0.566s, learning 3.949s)
               Value function loss: 126.0340
                    Surrogate loss: 0.0117
             Mean action noise std: 0.93
                       Mean reward: 710.07
               Mean episode length: 435.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 4.51s
                        Total time: 7385.96s
                               ETA: 10677.1s

################################################################################
                     [1m Learning iteration 1636/4000 [0m

                       Computation: 1792 steps/s (collection: 0.642s, learning 3.928s)
               Value function loss: 159.5798
                    Surrogate loss: 0.0119
             Mean action noise std: 0.93
                       Mean reward: 702.07
               Mean episode length: 430.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13410304
                    Iteration time: 4.57s
                        Total time: 7390.53s
                               ETA: 10672.7s

################################################################################
                     [1m Learning iteration 1637/4000 [0m

                       Computation: 1799 steps/s (collection: 0.615s, learning 3.936s)
               Value function loss: 200.1646
                    Surrogate loss: 0.0093
             Mean action noise std: 0.93
                       Mean reward: 703.46
               Mean episode length: 431.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 4.55s
                        Total time: 7395.08s
                               ETA: 10668.2s

################################################################################
                     [1m Learning iteration 1638/4000 [0m

                       Computation: 1799 steps/s (collection: 0.570s, learning 3.984s)
               Value function loss: 162.8440
                    Surrogate loss: 0.0110
             Mean action noise std: 0.93
                       Mean reward: 708.22
               Mean episode length: 433.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13426688
                    Iteration time: 4.55s
                        Total time: 7399.63s
                               ETA: 10663.8s

################################################################################
                     [1m Learning iteration 1639/4000 [0m

                       Computation: 1828 steps/s (collection: 0.580s, learning 3.901s)
               Value function loss: 206.4973
                    Surrogate loss: 0.0099
             Mean action noise std: 0.93
                       Mean reward: 696.41
               Mean episode length: 424.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 4.48s
                        Total time: 7404.11s
                               ETA: 10659.2s

################################################################################
                     [1m Learning iteration 1640/4000 [0m

                       Computation: 1838 steps/s (collection: 0.538s, learning 3.918s)
               Value function loss: 171.8578
                    Surrogate loss: 0.0100
             Mean action noise std: 0.93
                       Mean reward: 700.54
               Mean episode length: 427.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13443072
                    Iteration time: 4.46s
                        Total time: 7408.57s
                               ETA: 10654.6s

################################################################################
                     [1m Learning iteration 1641/4000 [0m

                       Computation: 1829 steps/s (collection: 0.549s, learning 3.928s)
               Value function loss: 162.7692
                    Surrogate loss: 0.0129
             Mean action noise std: 0.93
                       Mean reward: 705.90
               Mean episode length: 431.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 4.48s
                        Total time: 7413.05s
                               ETA: 10650.0s

################################################################################
                     [1m Learning iteration 1642/4000 [0m

                       Computation: 1827 steps/s (collection: 0.555s, learning 3.927s)
               Value function loss: 213.7935
                    Surrogate loss: 0.0115
             Mean action noise std: 0.93
                       Mean reward: 705.43
               Mean episode length: 430.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13459456
                    Iteration time: 4.48s
                        Total time: 7417.53s
                               ETA: 10645.5s

################################################################################
                     [1m Learning iteration 1643/4000 [0m

                       Computation: 1837 steps/s (collection: 0.543s, learning 3.915s)
               Value function loss: 275.5424
                    Surrogate loss: 0.0124
             Mean action noise std: 0.93
                       Mean reward: 711.85
               Mean episode length: 433.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 4.46s
                        Total time: 7421.99s
                               ETA: 10640.9s

################################################################################
                     [1m Learning iteration 1644/4000 [0m

                       Computation: 1850 steps/s (collection: 0.534s, learning 3.895s)
               Value function loss: 173.3624
                    Surrogate loss: 0.0108
             Mean action noise std: 0.93
                       Mean reward: 715.39
               Mean episode length: 435.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13475840
                    Iteration time: 4.43s
                        Total time: 7426.42s
                               ETA: 10636.3s

################################################################################
                     [1m Learning iteration 1645/4000 [0m

                       Computation: 1834 steps/s (collection: 0.531s, learning 3.935s)
               Value function loss: 227.5632
                    Surrogate loss: 0.0082
             Mean action noise std: 0.93
                       Mean reward: 700.22
               Mean episode length: 428.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 4.47s
                        Total time: 7430.88s
                               ETA: 10631.7s

################################################################################
                     [1m Learning iteration 1646/4000 [0m

                       Computation: 1823 steps/s (collection: 0.535s, learning 3.956s)
               Value function loss: 225.8311
                    Surrogate loss: 0.0096
             Mean action noise std: 0.93
                       Mean reward: 719.65
               Mean episode length: 441.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13492224
                    Iteration time: 4.49s
                        Total time: 7435.37s
                               ETA: 10627.1s

################################################################################
                     [1m Learning iteration 1647/4000 [0m

                       Computation: 1818 steps/s (collection: 0.574s, learning 3.931s)
               Value function loss: 186.5632
                    Surrogate loss: 0.0088
             Mean action noise std: 0.93
                       Mean reward: 708.98
               Mean episode length: 434.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 4.51s
                        Total time: 7439.88s
                               ETA: 10622.6s

################################################################################
                     [1m Learning iteration 1648/4000 [0m

                       Computation: 1775 steps/s (collection: 0.676s, learning 3.937s)
               Value function loss: 240.0718
                    Surrogate loss: 0.0085
             Mean action noise std: 0.93
                       Mean reward: 703.45
               Mean episode length: 431.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13508608
                    Iteration time: 4.61s
                        Total time: 7444.49s
                               ETA: 10618.2s

################################################################################
                     [1m Learning iteration 1649/4000 [0m

                       Computation: 1791 steps/s (collection: 0.631s, learning 3.941s)
               Value function loss: 329.4891
                    Surrogate loss: 0.0106
             Mean action noise std: 0.93
                       Mean reward: 707.08
               Mean episode length: 432.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 4.57s
                        Total time: 7449.06s
                               ETA: 10613.8s

################################################################################
                     [1m Learning iteration 1650/4000 [0m

                       Computation: 1830 steps/s (collection: 0.539s, learning 3.938s)
               Value function loss: 158.3303
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 695.40
               Mean episode length: 425.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13524992
                    Iteration time: 4.48s
                        Total time: 7453.54s
                               ETA: 10609.2s

################################################################################
                     [1m Learning iteration 1651/4000 [0m

                       Computation: 1766 steps/s (collection: 0.743s, learning 3.894s)
               Value function loss: 243.2471
                    Surrogate loss: 0.0084
             Mean action noise std: 0.93
                       Mean reward: 691.01
               Mean episode length: 423.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 4.64s
                        Total time: 7458.18s
                               ETA: 10604.9s

################################################################################
                     [1m Learning iteration 1652/4000 [0m

                       Computation: 1814 steps/s (collection: 0.603s, learning 3.911s)
               Value function loss: 170.0950
                    Surrogate loss: 0.0114
             Mean action noise std: 0.93
                       Mean reward: 658.79
               Mean episode length: 405.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13541376
                    Iteration time: 4.51s
                        Total time: 7462.69s
                               ETA: 10600.4s

################################################################################
                     [1m Learning iteration 1653/4000 [0m

                       Computation: 1786 steps/s (collection: 0.621s, learning 3.964s)
               Value function loss: 174.6881
                    Surrogate loss: 0.0093
             Mean action noise std: 0.93
                       Mean reward: 624.24
               Mean episode length: 384.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 4.59s
                        Total time: 7467.28s
                               ETA: 10596.0s

################################################################################
                     [1m Learning iteration 1654/4000 [0m

                       Computation: 1799 steps/s (collection: 0.607s, learning 3.946s)
               Value function loss: 218.6283
                    Surrogate loss: 0.0081
             Mean action noise std: 0.93
                       Mean reward: 604.04
               Mean episode length: 370.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13557760
                    Iteration time: 4.55s
                        Total time: 7471.83s
                               ETA: 10591.5s

################################################################################
                     [1m Learning iteration 1655/4000 [0m

                       Computation: 1766 steps/s (collection: 0.594s, learning 4.044s)
               Value function loss: 155.9987
                    Surrogate loss: 0.0096
             Mean action noise std: 0.93
                       Mean reward: 590.14
               Mean episode length: 362.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 4.64s
                        Total time: 7476.47s
                               ETA: 10587.1s

################################################################################
                     [1m Learning iteration 1656/4000 [0m

                       Computation: 1812 steps/s (collection: 0.549s, learning 3.970s)
               Value function loss: 234.8220
                    Surrogate loss: 0.0080
             Mean action noise std: 0.93
                       Mean reward: 586.74
               Mean episode length: 361.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13574144
                    Iteration time: 4.52s
                        Total time: 7480.99s
                               ETA: 10582.6s

################################################################################
                     [1m Learning iteration 1657/4000 [0m

                       Computation: 1788 steps/s (collection: 0.593s, learning 3.987s)
               Value function loss: 138.1870
                    Surrogate loss: 0.0073
             Mean action noise std: 0.93
                       Mean reward: 584.27
               Mean episode length: 359.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 4.58s
                        Total time: 7485.57s
                               ETA: 10578.2s

################################################################################
                     [1m Learning iteration 1658/4000 [0m

                       Computation: 1808 steps/s (collection: 0.606s, learning 3.923s)
               Value function loss: 221.2510
                    Surrogate loss: 0.0101
             Mean action noise std: 0.93
                       Mean reward: 563.35
               Mean episode length: 348.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13590528
                    Iteration time: 4.53s
                        Total time: 7490.10s
                               ETA: 10573.7s

################################################################################
                     [1m Learning iteration 1659/4000 [0m

                       Computation: 1806 steps/s (collection: 0.605s, learning 3.930s)
               Value function loss: 216.7284
                    Surrogate loss: 0.0085
             Mean action noise std: 0.93
                       Mean reward: 565.72
               Mean episode length: 349.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 4.54s
                        Total time: 7494.63s
                               ETA: 10569.2s

################################################################################
                     [1m Learning iteration 1660/4000 [0m

                       Computation: 1806 steps/s (collection: 0.561s, learning 3.974s)
               Value function loss: 172.9411
                    Surrogate loss: 0.0087
             Mean action noise std: 0.93
                       Mean reward: 567.80
               Mean episode length: 350.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13606912
                    Iteration time: 4.53s
                        Total time: 7499.17s
                               ETA: 10564.8s

################################################################################
                     [1m Learning iteration 1661/4000 [0m

                       Computation: 1797 steps/s (collection: 0.602s, learning 3.955s)
               Value function loss: 224.4502
                    Surrogate loss: 0.0090
             Mean action noise std: 0.93
                       Mean reward: 579.13
               Mean episode length: 357.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 4.56s
                        Total time: 7503.72s
                               ETA: 10560.3s

################################################################################
                     [1m Learning iteration 1662/4000 [0m

                       Computation: 1812 steps/s (collection: 0.592s, learning 3.928s)
               Value function loss: 199.5037
                    Surrogate loss: 0.0087
             Mean action noise std: 0.93
                       Mean reward: 600.37
               Mean episode length: 368.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13623296
                    Iteration time: 4.52s
                        Total time: 7508.24s
                               ETA: 10555.8s

################################################################################
                     [1m Learning iteration 1663/4000 [0m

                       Computation: 1806 steps/s (collection: 0.632s, learning 3.903s)
               Value function loss: 217.6863
                    Surrogate loss: 0.0098
             Mean action noise std: 0.93
                       Mean reward: 620.68
               Mean episode length: 380.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 4.54s
                        Total time: 7512.78s
                               ETA: 10551.3s

################################################################################
                     [1m Learning iteration 1664/4000 [0m

                       Computation: 1840 steps/s (collection: 0.539s, learning 3.912s)
               Value function loss: 284.4185
                    Surrogate loss: 0.0116
             Mean action noise std: 0.93
                       Mean reward: 638.62
               Mean episode length: 389.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13639680
                    Iteration time: 4.45s
                        Total time: 7517.23s
                               ETA: 10546.7s

################################################################################
                     [1m Learning iteration 1665/4000 [0m

                       Computation: 1809 steps/s (collection: 0.589s, learning 3.937s)
               Value function loss: 238.2554
                    Surrogate loss: 0.0125
             Mean action noise std: 0.93
                       Mean reward: 646.06
               Mean episode length: 392.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 4.53s
                        Total time: 7521.76s
                               ETA: 10542.2s

################################################################################
                     [1m Learning iteration 1666/4000 [0m

                       Computation: 1813 steps/s (collection: 0.534s, learning 3.983s)
               Value function loss: 116.1597
                    Surrogate loss: 0.0108
             Mean action noise std: 0.93
                       Mean reward: 651.86
               Mean episode length: 396.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13656064
                    Iteration time: 4.52s
                        Total time: 7526.27s
                               ETA: 10537.7s

################################################################################
                     [1m Learning iteration 1667/4000 [0m

                       Computation: 1850 steps/s (collection: 0.544s, learning 3.881s)
               Value function loss: 189.6842
                    Surrogate loss: 0.0113
             Mean action noise std: 0.93
                       Mean reward: 655.06
               Mean episode length: 398.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 4.43s
                        Total time: 7530.70s
                               ETA: 10533.0s

################################################################################
                     [1m Learning iteration 1668/4000 [0m

                       Computation: 1789 steps/s (collection: 0.592s, learning 3.986s)
               Value function loss: 218.3923
                    Surrogate loss: 0.0113
             Mean action noise std: 0.93
                       Mean reward: 643.00
               Mean episode length: 388.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13672448
                    Iteration time: 4.58s
                        Total time: 7535.28s
                               ETA: 10528.6s

################################################################################
                     [1m Learning iteration 1669/4000 [0m

                       Computation: 1795 steps/s (collection: 0.592s, learning 3.971s)
               Value function loss: 193.4043
                    Surrogate loss: 0.0061
             Mean action noise std: 0.93
                       Mean reward: 640.88
               Mean episode length: 388.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 4.56s
                        Total time: 7539.84s
                               ETA: 10524.2s

################################################################################
                     [1m Learning iteration 1670/4000 [0m

                       Computation: 1812 steps/s (collection: 0.596s, learning 3.924s)
               Value function loss: 262.9386
                    Surrogate loss: 0.0099
             Mean action noise std: 0.93
                       Mean reward: 640.33
               Mean episode length: 385.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13688832
                    Iteration time: 4.52s
                        Total time: 7544.36s
                               ETA: 10519.7s

################################################################################
                     [1m Learning iteration 1671/4000 [0m

                       Computation: 1839 steps/s (collection: 0.528s, learning 3.926s)
               Value function loss: 176.7156
                    Surrogate loss: 0.0100
             Mean action noise std: 0.93
                       Mean reward: 637.73
               Mean episode length: 383.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 4.45s
                        Total time: 7548.81s
                               ETA: 10515.1s

################################################################################
                     [1m Learning iteration 1672/4000 [0m

                       Computation: 1804 steps/s (collection: 0.591s, learning 3.949s)
               Value function loss: 226.0542
                    Surrogate loss: 0.0088
             Mean action noise std: 0.93
                       Mean reward: 641.32
               Mean episode length: 387.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13705216
                    Iteration time: 4.54s
                        Total time: 7553.35s
                               ETA: 10510.6s

################################################################################
                     [1m Learning iteration 1673/4000 [0m

                       Computation: 1834 steps/s (collection: 0.546s, learning 3.919s)
               Value function loss: 115.6260
                    Surrogate loss: 0.0116
             Mean action noise std: 0.93
                       Mean reward: 644.86
               Mean episode length: 388.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 4.46s
                        Total time: 7557.82s
                               ETA: 10506.0s

################################################################################
                     [1m Learning iteration 1674/4000 [0m

                       Computation: 1803 steps/s (collection: 0.618s, learning 3.924s)
               Value function loss: 168.3809
                    Surrogate loss: 0.0089
             Mean action noise std: 0.93
                       Mean reward: 656.61
               Mean episode length: 396.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13721600
                    Iteration time: 4.54s
                        Total time: 7562.36s
                               ETA: 10501.5s

################################################################################
                     [1m Learning iteration 1675/4000 [0m

                       Computation: 1802 steps/s (collection: 0.594s, learning 3.949s)
               Value function loss: 198.9092
                    Surrogate loss: 0.0079
             Mean action noise std: 0.93
                       Mean reward: 650.97
               Mean episode length: 393.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 4.54s
                        Total time: 7566.91s
                               ETA: 10497.1s

################################################################################
                     [1m Learning iteration 1676/4000 [0m

                       Computation: 1805 steps/s (collection: 0.558s, learning 3.979s)
               Value function loss: 193.4114
                    Surrogate loss: 0.0072
             Mean action noise std: 0.93
                       Mean reward: 659.35
               Mean episode length: 398.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13737984
                    Iteration time: 4.54s
                        Total time: 7571.44s
                               ETA: 10492.6s

################################################################################
                     [1m Learning iteration 1677/4000 [0m

                       Computation: 1820 steps/s (collection: 0.579s, learning 3.921s)
               Value function loss: 183.5645
                    Surrogate loss: 0.0095
             Mean action noise std: 0.93
                       Mean reward: 657.92
               Mean episode length: 396.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 4.50s
                        Total time: 7575.94s
                               ETA: 10488.0s

################################################################################
                     [1m Learning iteration 1678/4000 [0m

                       Computation: 1822 steps/s (collection: 0.583s, learning 3.913s)
               Value function loss: 150.5657
                    Surrogate loss: 0.0095
             Mean action noise std: 0.93
                       Mean reward: 669.10
               Mean episode length: 403.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13754368
                    Iteration time: 4.50s
                        Total time: 7580.44s
                               ETA: 10483.5s

################################################################################
                     [1m Learning iteration 1679/4000 [0m

                       Computation: 1812 steps/s (collection: 0.567s, learning 3.954s)
               Value function loss: 206.7401
                    Surrogate loss: 0.0078
             Mean action noise std: 0.93
                       Mean reward: 686.34
               Mean episode length: 411.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 4.52s
                        Total time: 7584.96s
                               ETA: 10479.0s

################################################################################
                     [1m Learning iteration 1680/4000 [0m

                       Computation: 1801 steps/s (collection: 0.577s, learning 3.971s)
               Value function loss: 320.0255
                    Surrogate loss: 0.0090
             Mean action noise std: 0.93
                       Mean reward: 708.32
               Mean episode length: 425.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13770752
                    Iteration time: 4.55s
                        Total time: 7589.51s
                               ETA: 10474.5s

################################################################################
                     [1m Learning iteration 1681/4000 [0m

                       Computation: 1808 steps/s (collection: 0.536s, learning 3.993s)
               Value function loss: 209.0885
                    Surrogate loss: 0.0097
             Mean action noise std: 0.93
                       Mean reward: 683.45
               Mean episode length: 410.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 4.53s
                        Total time: 7594.04s
                               ETA: 10470.0s

################################################################################
                     [1m Learning iteration 1682/4000 [0m

                       Computation: 1815 steps/s (collection: 0.556s, learning 3.957s)
               Value function loss: 206.2587
                    Surrogate loss: 0.0078
             Mean action noise std: 0.93
                       Mean reward: 666.88
               Mean episode length: 401.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13787136
                    Iteration time: 4.51s
                        Total time: 7598.55s
                               ETA: 10465.5s

################################################################################
                     [1m Learning iteration 1683/4000 [0m

                       Computation: 1786 steps/s (collection: 0.586s, learning 3.999s)
               Value function loss: 169.8397
                    Surrogate loss: 0.0084
             Mean action noise std: 0.93
                       Mean reward: 658.99
               Mean episode length: 397.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 4.59s
                        Total time: 7603.13s
                               ETA: 10461.1s

################################################################################
                     [1m Learning iteration 1684/4000 [0m

                       Computation: 1803 steps/s (collection: 0.586s, learning 3.956s)
               Value function loss: 146.3812
                    Surrogate loss: 0.0067
             Mean action noise std: 0.93
                       Mean reward: 657.01
               Mean episode length: 395.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13803520
                    Iteration time: 4.54s
                        Total time: 7607.68s
                               ETA: 10456.6s

################################################################################
                     [1m Learning iteration 1685/4000 [0m

                       Computation: 1824 steps/s (collection: 0.565s, learning 3.924s)
               Value function loss: 204.9727
                    Surrogate loss: 0.0081
             Mean action noise std: 0.94
                       Mean reward: 646.27
               Mean episode length: 388.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 4.49s
                        Total time: 7612.16s
                               ETA: 10452.1s

################################################################################
                     [1m Learning iteration 1686/4000 [0m

                       Computation: 1829 steps/s (collection: 0.555s, learning 3.924s)
               Value function loss: 220.4759
                    Surrogate loss: 0.0081
             Mean action noise std: 0.94
                       Mean reward: 658.97
               Mean episode length: 396.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13819904
                    Iteration time: 4.48s
                        Total time: 7616.64s
                               ETA: 10447.5s

################################################################################
                     [1m Learning iteration 1687/4000 [0m

                       Computation: 1821 steps/s (collection: 0.561s, learning 3.936s)
               Value function loss: 155.5896
                    Surrogate loss: 0.0091
             Mean action noise std: 0.94
                       Mean reward: 668.06
               Mean episode length: 401.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 4.50s
                        Total time: 7621.14s
                               ETA: 10442.9s

################################################################################
                     [1m Learning iteration 1688/4000 [0m

                       Computation: 1801 steps/s (collection: 0.558s, learning 3.989s)
               Value function loss: 137.9867
                    Surrogate loss: 0.0099
             Mean action noise std: 0.93
                       Mean reward: 671.42
               Mean episode length: 402.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13836288
                    Iteration time: 4.55s
                        Total time: 7625.69s
                               ETA: 10438.5s

################################################################################
                     [1m Learning iteration 1689/4000 [0m

                       Computation: 1855 steps/s (collection: 0.515s, learning 3.901s)
               Value function loss: 156.7666
                    Surrogate loss: 0.0060
             Mean action noise std: 0.93
                       Mean reward: 665.10
               Mean episode length: 399.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 4.42s
                        Total time: 7630.10s
                               ETA: 10433.8s

################################################################################
                     [1m Learning iteration 1690/4000 [0m

                       Computation: 1807 steps/s (collection: 0.564s, learning 3.969s)
               Value function loss: 174.1929
                    Surrogate loss: 0.0076
             Mean action noise std: 0.93
                       Mean reward: 651.05
               Mean episode length: 391.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13852672
                    Iteration time: 4.53s
                        Total time: 7634.64s
                               ETA: 10429.3s

################################################################################
                     [1m Learning iteration 1691/4000 [0m

                       Computation: 1842 steps/s (collection: 0.550s, learning 3.897s)
               Value function loss: 181.2008
                    Surrogate loss: 0.0080
             Mean action noise std: 0.93
                       Mean reward: 666.22
               Mean episode length: 401.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 4.45s
                        Total time: 7639.08s
                               ETA: 10424.7s

################################################################################
                     [1m Learning iteration 1692/4000 [0m

                       Computation: 1838 steps/s (collection: 0.580s, learning 3.875s)
               Value function loss: 173.0126
                    Surrogate loss: 0.0073
             Mean action noise std: 0.93
                       Mean reward: 683.59
               Mean episode length: 412.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13869056
                    Iteration time: 4.45s
                        Total time: 7643.54s
                               ETA: 10420.1s

################################################################################
                     [1m Learning iteration 1693/4000 [0m

                       Computation: 1814 steps/s (collection: 0.595s, learning 3.920s)
               Value function loss: 166.0735
                    Surrogate loss: 0.0097
             Mean action noise std: 0.93
                       Mean reward: 675.07
               Mean episode length: 405.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 4.52s
                        Total time: 7648.05s
                               ETA: 10415.6s

################################################################################
                     [1m Learning iteration 1694/4000 [0m

                       Computation: 1836 steps/s (collection: 0.567s, learning 3.893s)
               Value function loss: 207.4868
                    Surrogate loss: 0.0076
             Mean action noise std: 0.94
                       Mean reward: 693.96
               Mean episode length: 416.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13885440
                    Iteration time: 4.46s
                        Total time: 7652.51s
                               ETA: 10411.0s

################################################################################
                     [1m Learning iteration 1695/4000 [0m

                       Computation: 1823 steps/s (collection: 0.594s, learning 3.899s)
               Value function loss: 243.7214
                    Surrogate loss: 0.0072
             Mean action noise std: 0.94
                       Mean reward: 702.94
               Mean episode length: 423.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 4.49s
                        Total time: 7657.01s
                               ETA: 10406.5s

################################################################################
                     [1m Learning iteration 1696/4000 [0m

                       Computation: 1814 steps/s (collection: 0.565s, learning 3.950s)
               Value function loss: 266.5011
                    Surrogate loss: 0.0067
             Mean action noise std: 0.94
                       Mean reward: 721.17
               Mean episode length: 435.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13901824
                    Iteration time: 4.51s
                        Total time: 7661.52s
                               ETA: 10402.0s

################################################################################
                     [1m Learning iteration 1697/4000 [0m

                       Computation: 1829 steps/s (collection: 0.571s, learning 3.906s)
               Value function loss: 210.7539
                    Surrogate loss: 0.0084
             Mean action noise std: 0.93
                       Mean reward: 722.77
               Mean episode length: 437.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 4.48s
                        Total time: 7666.00s
                               ETA: 10397.4s

################################################################################
                     [1m Learning iteration 1698/4000 [0m

                       Computation: 1829 steps/s (collection: 0.551s, learning 3.925s)
               Value function loss: 328.3779
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 722.81
               Mean episode length: 438.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13918208
                    Iteration time: 4.48s
                        Total time: 7670.47s
                               ETA: 10392.8s

################################################################################
                     [1m Learning iteration 1699/4000 [0m

                       Computation: 1839 steps/s (collection: 0.533s, learning 3.920s)
               Value function loss: 187.6757
                    Surrogate loss: 0.0099
             Mean action noise std: 0.93
                       Mean reward: 728.81
               Mean episode length: 440.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 4.45s
                        Total time: 7674.93s
                               ETA: 10388.2s

################################################################################
                     [1m Learning iteration 1700/4000 [0m

                       Computation: 1840 steps/s (collection: 0.531s, learning 3.920s)
               Value function loss: 180.3306
                    Surrogate loss: 0.0072
             Mean action noise std: 0.93
                       Mean reward: 711.18
               Mean episode length: 430.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13934592
                    Iteration time: 4.45s
                        Total time: 7679.38s
                               ETA: 10383.6s

################################################################################
                     [1m Learning iteration 1701/4000 [0m

                       Computation: 1846 steps/s (collection: 0.524s, learning 3.913s)
               Value function loss: 264.6416
                    Surrogate loss: 0.0075
             Mean action noise std: 0.93
                       Mean reward: 719.55
               Mean episode length: 435.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 4.44s
                        Total time: 7683.82s
                               ETA: 10379.0s

################################################################################
                     [1m Learning iteration 1702/4000 [0m

                       Computation: 1851 steps/s (collection: 0.516s, learning 3.907s)
               Value function loss: 175.5428
                    Surrogate loss: 0.0088
             Mean action noise std: 0.93
                       Mean reward: 715.52
               Mean episode length: 430.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13950976
                    Iteration time: 4.42s
                        Total time: 7688.24s
                               ETA: 10374.4s

################################################################################
                     [1m Learning iteration 1703/4000 [0m

                       Computation: 1822 steps/s (collection: 0.560s, learning 3.934s)
               Value function loss: 167.9511
                    Surrogate loss: 0.0085
             Mean action noise std: 0.93
                       Mean reward: 706.03
               Mean episode length: 426.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 4.49s
                        Total time: 7692.73s
                               ETA: 10369.8s

################################################################################
                     [1m Learning iteration 1704/4000 [0m

                       Computation: 1846 steps/s (collection: 0.540s, learning 3.897s)
               Value function loss: 174.3038
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 694.38
               Mean episode length: 419.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13967360
                    Iteration time: 4.44s
                        Total time: 7697.17s
                               ETA: 10365.2s

################################################################################
                     [1m Learning iteration 1705/4000 [0m

                       Computation: 1840 steps/s (collection: 0.537s, learning 3.914s)
               Value function loss: 221.0047
                    Surrogate loss: 0.0083
             Mean action noise std: 0.93
                       Mean reward: 689.21
               Mean episode length: 414.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 4.45s
                        Total time: 7701.62s
                               ETA: 10360.6s

################################################################################
                     [1m Learning iteration 1706/4000 [0m

                       Computation: 1825 steps/s (collection: 0.538s, learning 3.949s)
               Value function loss: 279.4865
                    Surrogate loss: 0.0102
             Mean action noise std: 0.93
                       Mean reward: 678.58
               Mean episode length: 407.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13983744
                    Iteration time: 4.49s
                        Total time: 7706.11s
                               ETA: 10356.1s

################################################################################
                     [1m Learning iteration 1707/4000 [0m

                       Computation: 1799 steps/s (collection: 0.602s, learning 3.952s)
               Value function loss: 206.2028
                    Surrogate loss: 0.0096
             Mean action noise std: 0.93
                       Mean reward: 660.36
               Mean episode length: 395.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 4.55s
                        Total time: 7710.66s
                               ETA: 10351.6s

################################################################################
                     [1m Learning iteration 1708/4000 [0m

                       Computation: 1767 steps/s (collection: 0.569s, learning 4.067s)
               Value function loss: 198.0154
                    Surrogate loss: 0.0087
             Mean action noise std: 0.93
                       Mean reward: 663.17
               Mean episode length: 396.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14000128
                    Iteration time: 4.64s
                        Total time: 7715.30s
                               ETA: 10347.3s

################################################################################
                     [1m Learning iteration 1709/4000 [0m

                       Computation: 1796 steps/s (collection: 0.576s, learning 3.985s)
               Value function loss: 105.4002
                    Surrogate loss: 0.0093
             Mean action noise std: 0.93
                       Mean reward: 663.09
               Mean episode length: 396.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 4.56s
                        Total time: 7719.86s
                               ETA: 10342.8s

################################################################################
                     [1m Learning iteration 1710/4000 [0m

                       Computation: 1784 steps/s (collection: 0.620s, learning 3.972s)
               Value function loss: 267.1606
                    Surrogate loss: 0.0087
             Mean action noise std: 0.93
                       Mean reward: 672.24
               Mean episode length: 401.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14016512
                    Iteration time: 4.59s
                        Total time: 7724.45s
                               ETA: 10338.4s

################################################################################
                     [1m Learning iteration 1711/4000 [0m

                       Computation: 1788 steps/s (collection: 0.571s, learning 4.009s)
               Value function loss: 306.3063
                    Surrogate loss: 0.0077
             Mean action noise std: 0.93
                       Mean reward: 672.18
               Mean episode length: 401.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 4.58s
                        Total time: 7729.03s
                               ETA: 10334.0s

################################################################################
                     [1m Learning iteration 1712/4000 [0m

                       Computation: 1833 steps/s (collection: 0.522s, learning 3.946s)
               Value function loss: 206.7962
                    Surrogate loss: 0.0104
             Mean action noise std: 0.93
                       Mean reward: 664.52
               Mean episode length: 395.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14032896
                    Iteration time: 4.47s
                        Total time: 7733.50s
                               ETA: 10329.4s

################################################################################
                     [1m Learning iteration 1713/4000 [0m

                       Computation: 1814 steps/s (collection: 0.580s, learning 3.934s)
               Value function loss: 212.9972
                    Surrogate loss: 0.0086
             Mean action noise std: 0.93
                       Mean reward: 674.69
               Mean episode length: 400.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 4.51s
                        Total time: 7738.01s
                               ETA: 10324.9s

################################################################################
                     [1m Learning iteration 1714/4000 [0m

                       Computation: 1817 steps/s (collection: 0.582s, learning 3.925s)
               Value function loss: 184.3880
                    Surrogate loss: 0.0087
             Mean action noise std: 0.93
                       Mean reward: 675.21
               Mean episode length: 400.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14049280
                    Iteration time: 4.51s
                        Total time: 7742.52s
                               ETA: 10320.4s

################################################################################
                     [1m Learning iteration 1715/4000 [0m

                       Computation: 1802 steps/s (collection: 0.620s, learning 3.926s)
               Value function loss: 216.2980
                    Surrogate loss: 0.0094
             Mean action noise std: 0.93
                       Mean reward: 679.23
               Mean episode length: 404.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 4.55s
                        Total time: 7747.07s
                               ETA: 10315.9s

################################################################################
                     [1m Learning iteration 1716/4000 [0m

                       Computation: 1818 steps/s (collection: 0.548s, learning 3.958s)
               Value function loss: 206.6220
                    Surrogate loss: 0.0102
             Mean action noise std: 0.93
                       Mean reward: 677.12
               Mean episode length: 404.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14065664
                    Iteration time: 4.51s
                        Total time: 7751.57s
                               ETA: 10311.4s

################################################################################
                     [1m Learning iteration 1717/4000 [0m

                       Computation: 1800 steps/s (collection: 0.635s, learning 3.914s)
               Value function loss: 245.1318
                    Surrogate loss: 0.0071
             Mean action noise std: 0.93
                       Mean reward: 671.67
               Mean episode length: 403.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 4.55s
                        Total time: 7756.12s
                               ETA: 10306.9s

################################################################################
                     [1m Learning iteration 1718/4000 [0m

                       Computation: 1815 steps/s (collection: 0.554s, learning 3.958s)
               Value function loss: 157.5274
                    Surrogate loss: 0.0102
             Mean action noise std: 0.93
                       Mean reward: 656.12
               Mean episode length: 395.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14082048
                    Iteration time: 4.51s
                        Total time: 7760.63s
                               ETA: 10302.4s

################################################################################
                     [1m Learning iteration 1719/4000 [0m

                       Computation: 1791 steps/s (collection: 0.605s, learning 3.967s)
               Value function loss: 161.6785
                    Surrogate loss: 0.0110
             Mean action noise std: 0.93
                       Mean reward: 654.88
               Mean episode length: 394.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 4.57s
                        Total time: 7765.21s
                               ETA: 10297.9s

################################################################################
                     [1m Learning iteration 1720/4000 [0m

                       Computation: 1829 steps/s (collection: 0.565s, learning 3.912s)
               Value function loss: 174.4699
                    Surrogate loss: 0.0085
             Mean action noise std: 0.93
                       Mean reward: 657.24
               Mean episode length: 396.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14098432
                    Iteration time: 4.48s
                        Total time: 7769.68s
                               ETA: 10293.4s

################################################################################
                     [1m Learning iteration 1721/4000 [0m

                       Computation: 1828 steps/s (collection: 0.545s, learning 3.934s)
               Value function loss: 166.1386
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 635.55
               Mean episode length: 383.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 4.48s
                        Total time: 7774.16s
                               ETA: 10288.8s

################################################################################
                     [1m Learning iteration 1722/4000 [0m

                       Computation: 1812 steps/s (collection: 0.576s, learning 3.944s)
               Value function loss: 276.3015
                    Surrogate loss: 0.0101
             Mean action noise std: 0.93
                       Mean reward: 624.57
               Mean episode length: 377.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14114816
                    Iteration time: 4.52s
                        Total time: 7778.68s
                               ETA: 10284.3s

################################################################################
                     [1m Learning iteration 1723/4000 [0m

                       Computation: 1827 steps/s (collection: 0.563s, learning 3.921s)
               Value function loss: 284.1215
                    Surrogate loss: 0.0109
             Mean action noise std: 0.93
                       Mean reward: 601.31
               Mean episode length: 364.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 4.48s
                        Total time: 7783.17s
                               ETA: 10279.7s

################################################################################
                     [1m Learning iteration 1724/4000 [0m

                       Computation: 1835 steps/s (collection: 0.561s, learning 3.902s)
               Value function loss: 173.5217
                    Surrogate loss: 0.0109
             Mean action noise std: 0.93
                       Mean reward: 597.62
               Mean episode length: 360.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14131200
                    Iteration time: 4.46s
                        Total time: 7787.63s
                               ETA: 10275.2s

################################################################################
                     [1m Learning iteration 1725/4000 [0m

                       Computation: 1840 steps/s (collection: 0.568s, learning 3.883s)
               Value function loss: 147.2473
                    Surrogate loss: 0.0094
             Mean action noise std: 0.93
                       Mean reward: 589.59
               Mean episode length: 355.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 4.45s
                        Total time: 7792.08s
                               ETA: 10270.6s

################################################################################
                     [1m Learning iteration 1726/4000 [0m

                       Computation: 1826 steps/s (collection: 0.554s, learning 3.931s)
               Value function loss: 232.3802
                    Surrogate loss: 0.0087
             Mean action noise std: 0.93
                       Mean reward: 603.12
               Mean episode length: 362.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14147584
                    Iteration time: 4.48s
                        Total time: 7796.56s
                               ETA: 10266.0s

################################################################################
                     [1m Learning iteration 1727/4000 [0m

                       Computation: 1828 steps/s (collection: 0.536s, learning 3.943s)
               Value function loss: 300.3671
                    Surrogate loss: 0.0107
             Mean action noise std: 0.93
                       Mean reward: 605.89
               Mean episode length: 361.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 4.48s
                        Total time: 7801.04s
                               ETA: 10261.4s

################################################################################
                     [1m Learning iteration 1728/4000 [0m

                       Computation: 1776 steps/s (collection: 0.662s, learning 3.949s)
               Value function loss: 250.9842
                    Surrogate loss: 0.0075
             Mean action noise std: 0.93
                       Mean reward: 607.84
               Mean episode length: 362.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14163968
                    Iteration time: 4.61s
                        Total time: 7805.65s
                               ETA: 10257.1s

################################################################################
                     [1m Learning iteration 1729/4000 [0m

                       Computation: 1836 steps/s (collection: 0.538s, learning 3.923s)
               Value function loss: 242.4344
                    Surrogate loss: 0.0097
             Mean action noise std: 0.93
                       Mean reward: 605.79
               Mean episode length: 360.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 4.46s
                        Total time: 7810.11s
                               ETA: 10252.5s

################################################################################
                     [1m Learning iteration 1730/4000 [0m

                       Computation: 1797 steps/s (collection: 0.602s, learning 3.955s)
               Value function loss: 241.4916
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 606.17
               Mean episode length: 360.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14180352
                    Iteration time: 4.56s
                        Total time: 7814.67s
                               ETA: 10248.0s

################################################################################
                     [1m Learning iteration 1731/4000 [0m

                       Computation: 1820 steps/s (collection: 0.565s, learning 3.934s)
               Value function loss: 142.7978
                    Surrogate loss: 0.0102
             Mean action noise std: 0.93
                       Mean reward: 606.31
               Mean episode length: 360.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 4.50s
                        Total time: 7819.17s
                               ETA: 10243.5s

################################################################################
                     [1m Learning iteration 1732/4000 [0m

                       Computation: 1798 steps/s (collection: 0.608s, learning 3.947s)
               Value function loss: 260.2813
                    Surrogate loss: 0.0100
             Mean action noise std: 0.93
                       Mean reward: 619.43
               Mean episode length: 368.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14196736
                    Iteration time: 4.56s
                        Total time: 7823.73s
                               ETA: 10239.0s

################################################################################
                     [1m Learning iteration 1733/4000 [0m

                       Computation: 1807 steps/s (collection: 0.583s, learning 3.948s)
               Value function loss: 173.9451
                    Surrogate loss: 0.0096
             Mean action noise std: 0.93
                       Mean reward: 627.51
               Mean episode length: 371.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 4.53s
                        Total time: 7828.26s
                               ETA: 10234.5s

################################################################################
                     [1m Learning iteration 1734/4000 [0m

                       Computation: 1793 steps/s (collection: 0.591s, learning 3.978s)
               Value function loss: 192.4890
                    Surrogate loss: 0.0109
             Mean action noise std: 0.93
                       Mean reward: 605.24
               Mean episode length: 358.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14213120
                    Iteration time: 4.57s
                        Total time: 7832.83s
                               ETA: 10230.1s

################################################################################
                     [1m Learning iteration 1735/4000 [0m

                       Computation: 1787 steps/s (collection: 0.648s, learning 3.935s)
               Value function loss: 124.0085
                    Surrogate loss: 0.0160
             Mean action noise std: 0.93
                       Mean reward: 598.13
               Mean episode length: 354.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 4.58s
                        Total time: 7837.41s
                               ETA: 10225.7s

################################################################################
                     [1m Learning iteration 1736/4000 [0m

                       Computation: 1809 steps/s (collection: 0.591s, learning 3.938s)
               Value function loss: 212.6880
                    Surrogate loss: 0.0103
             Mean action noise std: 0.93
                       Mean reward: 595.16
               Mean episode length: 353.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14229504
                    Iteration time: 4.53s
                        Total time: 7841.94s
                               ETA: 10221.2s

################################################################################
                     [1m Learning iteration 1737/4000 [0m

                       Computation: 1817 steps/s (collection: 0.560s, learning 3.948s)
               Value function loss: 238.4279
                    Surrogate loss: 0.0114
             Mean action noise std: 0.93
                       Mean reward: 600.57
               Mean episode length: 355.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 4.51s
                        Total time: 7846.45s
                               ETA: 10216.6s

################################################################################
                     [1m Learning iteration 1738/4000 [0m

                       Computation: 1825 steps/s (collection: 0.587s, learning 3.902s)
               Value function loss: 229.7076
                    Surrogate loss: 0.0087
             Mean action noise std: 0.93
                       Mean reward: 614.76
               Mean episode length: 364.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14245888
                    Iteration time: 4.49s
                        Total time: 7850.94s
                               ETA: 10212.1s

################################################################################
                     [1m Learning iteration 1739/4000 [0m

                       Computation: 1788 steps/s (collection: 0.615s, learning 3.966s)
               Value function loss: 194.2383
                    Surrogate loss: 0.0089
             Mean action noise std: 0.93
                       Mean reward: 614.77
               Mean episode length: 363.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 4.58s
                        Total time: 7855.52s
                               ETA: 10207.7s

################################################################################
                     [1m Learning iteration 1740/4000 [0m

                       Computation: 1817 steps/s (collection: 0.578s, learning 3.929s)
               Value function loss: 168.9180
                    Surrogate loss: 0.0094
             Mean action noise std: 0.93
                       Mean reward: 604.91
               Mean episode length: 358.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14262272
                    Iteration time: 4.51s
                        Total time: 7860.02s
                               ETA: 10203.1s

################################################################################
                     [1m Learning iteration 1741/4000 [0m

                       Computation: 1802 steps/s (collection: 0.559s, learning 3.985s)
               Value function loss: 191.5599
                    Surrogate loss: 0.0103
             Mean action noise std: 0.93
                       Mean reward: 597.37
               Mean episode length: 353.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 4.54s
                        Total time: 7864.57s
                               ETA: 10198.7s

################################################################################
                     [1m Learning iteration 1742/4000 [0m

                       Computation: 1828 steps/s (collection: 0.576s, learning 3.903s)
               Value function loss: 148.8650
                    Surrogate loss: 0.0115
             Mean action noise std: 0.93
                       Mean reward: 601.48
               Mean episode length: 357.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14278656
                    Iteration time: 4.48s
                        Total time: 7869.05s
                               ETA: 10194.1s

################################################################################
                     [1m Learning iteration 1743/4000 [0m

                       Computation: 1797 steps/s (collection: 0.603s, learning 3.954s)
               Value function loss: 271.0543
                    Surrogate loss: 0.0101
             Mean action noise std: 0.93
                       Mean reward: 617.40
               Mean episode length: 366.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 4.56s
                        Total time: 7873.61s
                               ETA: 10189.6s

################################################################################
                     [1m Learning iteration 1744/4000 [0m

                       Computation: 1824 steps/s (collection: 0.580s, learning 3.911s)
               Value function loss: 185.6932
                    Surrogate loss: 0.0082
             Mean action noise std: 0.93
                       Mean reward: 634.68
               Mean episode length: 376.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14295040
                    Iteration time: 4.49s
                        Total time: 7878.10s
                               ETA: 10185.1s

################################################################################
                     [1m Learning iteration 1745/4000 [0m

                       Computation: 1843 steps/s (collection: 0.552s, learning 3.892s)
               Value function loss: 207.1562
                    Surrogate loss: 0.0082
             Mean action noise std: 0.93
                       Mean reward: 629.68
               Mean episode length: 373.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 4.44s
                        Total time: 7882.54s
                               ETA: 10180.5s

################################################################################
                     [1m Learning iteration 1746/4000 [0m

                       Computation: 1797 steps/s (collection: 0.577s, learning 3.982s)
               Value function loss: 208.2964
                    Surrogate loss: 0.0079
             Mean action noise std: 0.93
                       Mean reward: 656.31
               Mean episode length: 388.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14311424
                    Iteration time: 4.56s
                        Total time: 7887.10s
                               ETA: 10176.0s

################################################################################
                     [1m Learning iteration 1747/4000 [0m

                       Computation: 1802 steps/s (collection: 0.590s, learning 3.955s)
               Value function loss: 155.8086
                    Surrogate loss: 0.0101
             Mean action noise std: 0.93
                       Mean reward: 650.33
               Mean episode length: 384.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 4.55s
                        Total time: 7891.64s
                               ETA: 10171.6s

################################################################################
                     [1m Learning iteration 1748/4000 [0m

                       Computation: 1804 steps/s (collection: 0.588s, learning 3.952s)
               Value function loss: 235.5681
                    Surrogate loss: 0.0062
             Mean action noise std: 0.93
                       Mean reward: 665.70
               Mean episode length: 392.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14327808
                    Iteration time: 4.54s
                        Total time: 7896.18s
                               ETA: 10167.1s

################################################################################
                     [1m Learning iteration 1749/4000 [0m

                       Computation: 1823 steps/s (collection: 0.573s, learning 3.920s)
               Value function loss: 261.7428
                    Surrogate loss: 0.0076
             Mean action noise std: 0.93
                       Mean reward: 663.75
               Mean episode length: 392.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 4.49s
                        Total time: 7900.68s
                               ETA: 10162.5s

################################################################################
                     [1m Learning iteration 1750/4000 [0m

                       Computation: 1815 steps/s (collection: 0.585s, learning 3.928s)
               Value function loss: 275.7865
                    Surrogate loss: 0.0090
             Mean action noise std: 0.93
                       Mean reward: 684.64
               Mean episode length: 403.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14344192
                    Iteration time: 4.51s
                        Total time: 7905.19s
                               ETA: 10158.0s

################################################################################
                     [1m Learning iteration 1751/4000 [0m

                       Computation: 1834 steps/s (collection: 0.537s, learning 3.928s)
               Value function loss: 124.2560
                    Surrogate loss: 0.0082
             Mean action noise std: 0.93
                       Mean reward: 689.12
               Mean episode length: 407.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 4.46s
                        Total time: 7909.65s
                               ETA: 10153.4s

################################################################################
                     [1m Learning iteration 1752/4000 [0m

                       Computation: 1800 steps/s (collection: 0.587s, learning 3.961s)
               Value function loss: 274.6875
                    Surrogate loss: 0.0090
             Mean action noise std: 0.93
                       Mean reward: 677.71
               Mean episode length: 401.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14360576
                    Iteration time: 4.55s
                        Total time: 7914.20s
                               ETA: 10149.0s

################################################################################
                     [1m Learning iteration 1753/4000 [0m

                       Computation: 1825 steps/s (collection: 0.545s, learning 3.941s)
               Value function loss: 186.6644
                    Surrogate loss: 0.0075
             Mean action noise std: 0.93
                       Mean reward: 684.36
               Mean episode length: 404.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 4.49s
                        Total time: 7918.69s
                               ETA: 10144.4s

################################################################################
                     [1m Learning iteration 1754/4000 [0m

                       Computation: 1803 steps/s (collection: 0.613s, learning 3.928s)
               Value function loss: 324.3631
                    Surrogate loss: 0.0106
             Mean action noise std: 0.93
                       Mean reward: 680.27
               Mean episode length: 401.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14376960
                    Iteration time: 4.54s
                        Total time: 7923.23s
                               ETA: 10139.9s

################################################################################
                     [1m Learning iteration 1755/4000 [0m

                       Computation: 1816 steps/s (collection: 0.581s, learning 3.930s)
               Value function loss: 235.3573
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 665.46
               Mean episode length: 394.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 4.51s
                        Total time: 7927.74s
                               ETA: 10135.4s

################################################################################
                     [1m Learning iteration 1756/4000 [0m

                       Computation: 1818 steps/s (collection: 0.567s, learning 3.939s)
               Value function loss: 182.0486
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 649.27
               Mean episode length: 385.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14393344
                    Iteration time: 4.51s
                        Total time: 7932.25s
                               ETA: 10130.9s

################################################################################
                     [1m Learning iteration 1757/4000 [0m

                       Computation: 1815 steps/s (collection: 0.580s, learning 3.933s)
               Value function loss: 212.4138
                    Surrogate loss: 0.0103
             Mean action noise std: 0.94
                       Mean reward: 642.21
               Mean episode length: 382.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 4.51s
                        Total time: 7936.76s
                               ETA: 10126.4s

################################################################################
                     [1m Learning iteration 1758/4000 [0m

                       Computation: 1822 steps/s (collection: 0.562s, learning 3.934s)
               Value function loss: 147.7188
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 636.87
               Mean episode length: 378.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14409728
                    Iteration time: 4.50s
                        Total time: 7941.26s
                               ETA: 10121.8s

################################################################################
                     [1m Learning iteration 1759/4000 [0m

                       Computation: 1813 steps/s (collection: 0.549s, learning 3.969s)
               Value function loss: 211.8656
                    Surrogate loss: 0.0071
             Mean action noise std: 0.93
                       Mean reward: 627.40
               Mean episode length: 373.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 4.52s
                        Total time: 7945.77s
                               ETA: 10117.3s

################################################################################
                     [1m Learning iteration 1760/4000 [0m

                       Computation: 1795 steps/s (collection: 0.598s, learning 3.964s)
               Value function loss: 198.6150
                    Surrogate loss: 0.0080
             Mean action noise std: 0.93
                       Mean reward: 635.04
               Mean episode length: 376.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14426112
                    Iteration time: 4.56s
                        Total time: 7950.34s
                               ETA: 10112.9s

################################################################################
                     [1m Learning iteration 1761/4000 [0m

                       Computation: 1781 steps/s (collection: 0.580s, learning 4.020s)
               Value function loss: 279.1583
                    Surrogate loss: 0.0052
             Mean action noise std: 0.93
                       Mean reward: 643.21
               Mean episode length: 379.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 4.60s
                        Total time: 7954.94s
                               ETA: 10108.5s

################################################################################
                     [1m Learning iteration 1762/4000 [0m

                       Computation: 1775 steps/s (collection: 0.587s, learning 4.027s)
               Value function loss: 140.9302
                    Surrogate loss: 0.0078
             Mean action noise std: 0.93
                       Mean reward: 648.24
               Mean episode length: 382.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14442496
                    Iteration time: 4.61s
                        Total time: 7959.55s
                               ETA: 10104.1s

################################################################################
                     [1m Learning iteration 1763/4000 [0m

                       Computation: 1822 steps/s (collection: 0.537s, learning 3.959s)
               Value function loss: 225.5493
                    Surrogate loss: 0.0091
             Mean action noise std: 0.93
                       Mean reward: 645.92
               Mean episode length: 380.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 4.50s
                        Total time: 7964.05s
                               ETA: 10099.5s

################################################################################
                     [1m Learning iteration 1764/4000 [0m

                       Computation: 1797 steps/s (collection: 0.564s, learning 3.993s)
               Value function loss: 188.6323
                    Surrogate loss: 0.0096
             Mean action noise std: 0.93
                       Mean reward: 659.31
               Mean episode length: 387.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14458880
                    Iteration time: 4.56s
                        Total time: 7968.60s
                               ETA: 10095.1s

################################################################################
                     [1m Learning iteration 1765/4000 [0m

                       Computation: 1802 steps/s (collection: 0.584s, learning 3.960s)
               Value function loss: 195.3971
                    Surrogate loss: 0.0102
             Mean action noise std: 0.93
                       Mean reward: 657.10
               Mean episode length: 385.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 4.54s
                        Total time: 7973.15s
                               ETA: 10090.6s

################################################################################
                     [1m Learning iteration 1766/4000 [0m

                       Computation: 1837 steps/s (collection: 0.550s, learning 3.908s)
               Value function loss: 200.4402
                    Surrogate loss: 0.0104
             Mean action noise std: 0.93
                       Mean reward: 682.46
               Mean episode length: 398.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14475264
                    Iteration time: 4.46s
                        Total time: 7977.60s
                               ETA: 10086.0s

################################################################################
                     [1m Learning iteration 1767/4000 [0m

                       Computation: 1820 steps/s (collection: 0.571s, learning 3.928s)
               Value function loss: 219.6198
                    Surrogate loss: 0.0112
             Mean action noise std: 0.93
                       Mean reward: 678.24
               Mean episode length: 395.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 4.50s
                        Total time: 7982.10s
                               ETA: 10081.5s

################################################################################
                     [1m Learning iteration 1768/4000 [0m

                       Computation: 1820 steps/s (collection: 0.549s, learning 3.951s)
               Value function loss: 295.1472
                    Surrogate loss: 0.0084
             Mean action noise std: 0.93
                       Mean reward: 701.44
               Mean episode length: 407.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14491648
                    Iteration time: 4.50s
                        Total time: 7986.60s
                               ETA: 10076.9s

################################################################################
                     [1m Learning iteration 1769/4000 [0m

                       Computation: 1814 steps/s (collection: 0.562s, learning 3.953s)
               Value function loss: 220.8994
                    Surrogate loss: 0.0081
             Mean action noise std: 0.93
                       Mean reward: 711.58
               Mean episode length: 414.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 4.51s
                        Total time: 7991.12s
                               ETA: 10072.4s

################################################################################
                     [1m Learning iteration 1770/4000 [0m

                       Computation: 1825 steps/s (collection: 0.566s, learning 3.921s)
               Value function loss: 300.7129
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 732.81
               Mean episode length: 427.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14508032
                    Iteration time: 4.49s
                        Total time: 7995.61s
                               ETA: 10067.9s

################################################################################
                     [1m Learning iteration 1771/4000 [0m

                       Computation: 1812 steps/s (collection: 0.593s, learning 3.927s)
               Value function loss: 170.4038
                    Surrogate loss: 0.0117
             Mean action noise std: 0.94
                       Mean reward: 734.90
               Mean episode length: 428.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 4.52s
                        Total time: 8000.13s
                               ETA: 10063.4s

################################################################################
                     [1m Learning iteration 1772/4000 [0m

                       Computation: 1853 steps/s (collection: 0.547s, learning 3.873s)
               Value function loss: 240.1390
                    Surrogate loss: 0.0101
             Mean action noise std: 0.94
                       Mean reward: 717.60
               Mean episode length: 420.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14524416
                    Iteration time: 4.42s
                        Total time: 8004.55s
                               ETA: 10058.7s

################################################################################
                     [1m Learning iteration 1773/4000 [0m

                       Computation: 1819 steps/s (collection: 0.552s, learning 3.951s)
               Value function loss: 188.6456
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 735.34
               Mean episode length: 431.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 4.50s
                        Total time: 8009.05s
                               ETA: 10054.2s

################################################################################
                     [1m Learning iteration 1774/4000 [0m

                       Computation: 1843 steps/s (collection: 0.551s, learning 3.894s)
               Value function loss: 196.9978
                    Surrogate loss: 0.0094
             Mean action noise std: 0.94
                       Mean reward: 734.98
               Mean episode length: 431.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14540800
                    Iteration time: 4.44s
                        Total time: 8013.49s
                               ETA: 10049.6s

################################################################################
                     [1m Learning iteration 1775/4000 [0m

                       Computation: 1850 steps/s (collection: 0.552s, learning 3.876s)
               Value function loss: 270.8800
                    Surrogate loss: 0.0067
             Mean action noise std: 0.94
                       Mean reward: 724.21
               Mean episode length: 425.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 4.43s
                        Total time: 8017.92s
                               ETA: 10045.0s

################################################################################
                     [1m Learning iteration 1776/4000 [0m

                       Computation: 1850 steps/s (collection: 0.513s, learning 3.914s)
               Value function loss: 228.9320
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 731.41
               Mean episode length: 430.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14557184
                    Iteration time: 4.43s
                        Total time: 8022.35s
                               ETA: 10040.4s

################################################################################
                     [1m Learning iteration 1777/4000 [0m

                       Computation: 1812 steps/s (collection: 0.567s, learning 3.953s)
               Value function loss: 252.3031
                    Surrogate loss: 0.0062
             Mean action noise std: 0.94
                       Mean reward: 741.83
               Mean episode length: 437.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 4.52s
                        Total time: 8026.87s
                               ETA: 10035.8s

################################################################################
                     [1m Learning iteration 1778/4000 [0m

                       Computation: 1838 steps/s (collection: 0.553s, learning 3.903s)
               Value function loss: 214.0214
                    Surrogate loss: 0.0104
             Mean action noise std: 0.94
                       Mean reward: 729.05
               Mean episode length: 431.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14573568
                    Iteration time: 4.46s
                        Total time: 8031.33s
                               ETA: 10031.3s

################################################################################
                     [1m Learning iteration 1779/4000 [0m

                       Computation: 1823 steps/s (collection: 0.567s, learning 3.926s)
               Value function loss: 227.3088
                    Surrogate loss: 0.0124
             Mean action noise std: 0.94
                       Mean reward: 724.40
               Mean episode length: 428.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 4.49s
                        Total time: 8035.82s
                               ETA: 10026.7s

################################################################################
                     [1m Learning iteration 1780/4000 [0m

                       Computation: 1867 steps/s (collection: 0.505s, learning 3.883s)
               Value function loss: 179.2953
                    Surrogate loss: 0.0073
             Mean action noise std: 0.94
                       Mean reward: 733.87
               Mean episode length: 432.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14589952
                    Iteration time: 4.39s
                        Total time: 8040.21s
                               ETA: 10022.0s

################################################################################
                     [1m Learning iteration 1781/4000 [0m

                       Computation: 1836 steps/s (collection: 0.563s, learning 3.897s)
               Value function loss: 300.7635
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 724.08
               Mean episode length: 427.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 4.46s
                        Total time: 8044.67s
                               ETA: 10017.5s

################################################################################
                     [1m Learning iteration 1782/4000 [0m

                       Computation: 1821 steps/s (collection: 0.554s, learning 3.942s)
               Value function loss: 181.2415
                    Surrogate loss: 0.0081
             Mean action noise std: 0.94
                       Mean reward: 715.60
               Mean episode length: 422.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14606336
                    Iteration time: 4.50s
                        Total time: 8049.16s
                               ETA: 10012.9s

################################################################################
                     [1m Learning iteration 1783/4000 [0m

                       Computation: 1835 steps/s (collection: 0.520s, learning 3.943s)
               Value function loss: 169.5693
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 735.50
               Mean episode length: 432.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 4.46s
                        Total time: 8053.62s
                               ETA: 10008.3s

################################################################################
                     [1m Learning iteration 1784/4000 [0m

                       Computation: 1843 steps/s (collection: 0.544s, learning 3.899s)
               Value function loss: 300.9455
                    Surrogate loss: 0.0110
             Mean action noise std: 0.94
                       Mean reward: 714.56
               Mean episode length: 421.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14622720
                    Iteration time: 4.44s
                        Total time: 8058.07s
                               ETA: 10003.7s

################################################################################
                     [1m Learning iteration 1785/4000 [0m

                       Computation: 1814 steps/s (collection: 0.581s, learning 3.933s)
               Value function loss: 234.8002
                    Surrogate loss: 0.0071
             Mean action noise std: 0.94
                       Mean reward: 716.19
               Mean episode length: 422.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 4.51s
                        Total time: 8062.58s
                               ETA: 9999.2s

################################################################################
                     [1m Learning iteration 1786/4000 [0m

                       Computation: 1807 steps/s (collection: 0.543s, learning 3.988s)
               Value function loss: 212.4092
                    Surrogate loss: 0.0073
             Mean action noise std: 0.94
                       Mean reward: 702.14
               Mean episode length: 415.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14639104
                    Iteration time: 4.53s
                        Total time: 8067.11s
                               ETA: 9994.7s

################################################################################
                     [1m Learning iteration 1787/4000 [0m

                       Computation: 1811 steps/s (collection: 0.538s, learning 3.985s)
               Value function loss: 156.7462
                    Surrogate loss: 0.0070
             Mean action noise std: 0.94
                       Mean reward: 695.12
               Mean episode length: 411.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 4.52s
                        Total time: 8071.64s
                               ETA: 9990.2s

################################################################################
                     [1m Learning iteration 1788/4000 [0m

                       Computation: 1834 steps/s (collection: 0.532s, learning 3.934s)
               Value function loss: 148.9863
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 687.60
               Mean episode length: 408.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14655488
                    Iteration time: 4.47s
                        Total time: 8076.10s
                               ETA: 9985.7s

################################################################################
                     [1m Learning iteration 1789/4000 [0m

                       Computation: 1818 steps/s (collection: 0.552s, learning 3.953s)
               Value function loss: 131.3819
                    Surrogate loss: 0.0109
             Mean action noise std: 0.94
                       Mean reward: 667.55
               Mean episode length: 396.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 4.51s
                        Total time: 8080.61s
                               ETA: 9981.1s

################################################################################
                     [1m Learning iteration 1790/4000 [0m

                       Computation: 1784 steps/s (collection: 0.582s, learning 4.009s)
               Value function loss: 202.6907
                    Surrogate loss: 0.0091
             Mean action noise std: 0.94
                       Mean reward: 661.93
               Mean episode length: 392.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14671872
                    Iteration time: 4.59s
                        Total time: 8085.20s
                               ETA: 9976.7s

################################################################################
                     [1m Learning iteration 1791/4000 [0m

                       Computation: 1793 steps/s (collection: 0.550s, learning 4.017s)
               Value function loss: 220.3715
                    Surrogate loss: 0.0103
             Mean action noise std: 0.94
                       Mean reward: 665.27
               Mean episode length: 394.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 4.57s
                        Total time: 8089.76s
                               ETA: 9972.3s

################################################################################
                     [1m Learning iteration 1792/4000 [0m

                       Computation: 1845 steps/s (collection: 0.509s, learning 3.929s)
               Value function loss: 240.1811
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 673.25
               Mean episode length: 400.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14688256
                    Iteration time: 4.44s
                        Total time: 8094.20s
                               ETA: 9967.7s

################################################################################
                     [1m Learning iteration 1793/4000 [0m

                       Computation: 1818 steps/s (collection: 0.550s, learning 3.954s)
               Value function loss: 148.7870
                    Surrogate loss: 0.0084
             Mean action noise std: 0.94
                       Mean reward: 675.19
               Mean episode length: 400.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 4.50s
                        Total time: 8098.71s
                               ETA: 9963.1s

################################################################################
                     [1m Learning iteration 1794/4000 [0m

                       Computation: 1804 steps/s (collection: 0.637s, learning 3.903s)
               Value function loss: 324.9196
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 694.50
               Mean episode length: 411.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14704640
                    Iteration time: 4.54s
                        Total time: 8103.25s
                               ETA: 9958.6s

################################################################################
                     [1m Learning iteration 1795/4000 [0m

                       Computation: 1804 steps/s (collection: 0.575s, learning 3.965s)
               Value function loss: 177.5210
                    Surrogate loss: 0.0080
             Mean action noise std: 0.94
                       Mean reward: 702.77
               Mean episode length: 415.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 4.54s
                        Total time: 8107.79s
                               ETA: 9954.2s

################################################################################
                     [1m Learning iteration 1796/4000 [0m

                       Computation: 1810 steps/s (collection: 0.608s, learning 3.918s)
               Value function loss: 215.4548
                    Surrogate loss: 0.0102
             Mean action noise std: 0.94
                       Mean reward: 688.16
               Mean episode length: 404.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14721024
                    Iteration time: 4.53s
                        Total time: 8112.31s
                               ETA: 9949.7s

################################################################################
                     [1m Learning iteration 1797/4000 [0m

                       Computation: 1819 steps/s (collection: 0.553s, learning 3.949s)
               Value function loss: 260.8873
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 692.48
               Mean episode length: 407.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 4.50s
                        Total time: 8116.81s
                               ETA: 9945.1s

################################################################################
                     [1m Learning iteration 1798/4000 [0m

                       Computation: 1790 steps/s (collection: 0.650s, learning 3.925s)
               Value function loss: 192.2027
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 704.32
               Mean episode length: 413.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14737408
                    Iteration time: 4.58s
                        Total time: 8121.39s
                               ETA: 9940.7s

################################################################################
                     [1m Learning iteration 1799/4000 [0m

                       Computation: 1801 steps/s (collection: 0.555s, learning 3.991s)
               Value function loss: 299.7464
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 727.15
               Mean episode length: 425.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 4.55s
                        Total time: 8125.94s
                               ETA: 9936.2s

################################################################################
                     [1m Learning iteration 1800/4000 [0m

                       Computation: 1788 steps/s (collection: 0.595s, learning 3.985s)
               Value function loss: 278.7722
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 734.20
               Mean episode length: 430.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14753792
                    Iteration time: 4.58s
                        Total time: 8130.52s
                               ETA: 9931.8s

################################################################################
                     [1m Learning iteration 1801/4000 [0m

                       Computation: 1796 steps/s (collection: 0.591s, learning 3.970s)
               Value function loss: 276.0594
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 739.04
               Mean episode length: 432.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 4.56s
                        Total time: 8135.08s
                               ETA: 9927.3s

################################################################################
                     [1m Learning iteration 1802/4000 [0m

                       Computation: 1795 steps/s (collection: 0.620s, learning 3.942s)
               Value function loss: 231.8642
                    Surrogate loss: 0.0108
             Mean action noise std: 0.94
                       Mean reward: 710.46
               Mean episode length: 417.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14770176
                    Iteration time: 4.56s
                        Total time: 8139.64s
                               ETA: 9922.9s

################################################################################
                     [1m Learning iteration 1803/4000 [0m

                       Computation: 1851 steps/s (collection: 0.523s, learning 3.901s)
               Value function loss: 211.4924
                    Surrogate loss: 0.0081
             Mean action noise std: 0.94
                       Mean reward: 716.21
               Mean episode length: 420.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 4.42s
                        Total time: 8144.06s
                               ETA: 9918.2s

################################################################################
                     [1m Learning iteration 1804/4000 [0m

                       Computation: 1814 steps/s (collection: 0.582s, learning 3.931s)
               Value function loss: 221.0600
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 716.97
               Mean episode length: 420.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14786560
                    Iteration time: 4.51s
                        Total time: 8148.58s
                               ETA: 9913.7s

################################################################################
                     [1m Learning iteration 1805/4000 [0m

                       Computation: 1854 steps/s (collection: 0.509s, learning 3.909s)
               Value function loss: 120.9035
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 708.99
               Mean episode length: 415.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 4.42s
                        Total time: 8152.99s
                               ETA: 9909.1s

################################################################################
                     [1m Learning iteration 1806/4000 [0m

                       Computation: 1778 steps/s (collection: 0.669s, learning 3.937s)
               Value function loss: 273.6910
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 700.62
               Mean episode length: 410.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14802944
                    Iteration time: 4.61s
                        Total time: 8157.60s
                               ETA: 9904.7s

################################################################################
                     [1m Learning iteration 1807/4000 [0m

                       Computation: 1828 steps/s (collection: 0.559s, learning 3.920s)
               Value function loss: 158.7733
                    Surrogate loss: 0.0108
             Mean action noise std: 0.94
                       Mean reward: 704.29
               Mean episode length: 412.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 4.48s
                        Total time: 8162.08s
                               ETA: 9900.1s

################################################################################
                     [1m Learning iteration 1808/4000 [0m

                       Computation: 1803 steps/s (collection: 0.596s, learning 3.946s)
               Value function loss: 282.5218
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 693.05
               Mean episode length: 406.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14819328
                    Iteration time: 4.54s
                        Total time: 8166.62s
                               ETA: 9895.7s

################################################################################
                     [1m Learning iteration 1809/4000 [0m

                       Computation: 1826 steps/s (collection: 0.577s, learning 3.908s)
               Value function loss: 197.4995
                    Surrogate loss: 0.0109
             Mean action noise std: 0.94
                       Mean reward: 666.64
               Mean episode length: 392.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 4.48s
                        Total time: 8171.11s
                               ETA: 9891.1s

################################################################################
                     [1m Learning iteration 1810/4000 [0m

                       Computation: 1803 steps/s (collection: 0.583s, learning 3.960s)
               Value function loss: 247.8668
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 646.14
               Mean episode length: 379.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14835712
                    Iteration time: 4.54s
                        Total time: 8175.65s
                               ETA: 9886.6s

################################################################################
                     [1m Learning iteration 1811/4000 [0m

                       Computation: 1768 steps/s (collection: 0.686s, learning 3.948s)
               Value function loss: 219.2367
                    Surrogate loss: 0.0104
             Mean action noise std: 0.94
                       Mean reward: 639.70
               Mean episode length: 373.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 4.63s
                        Total time: 8180.28s
                               ETA: 9882.3s

################################################################################
                     [1m Learning iteration 1812/4000 [0m

                       Computation: 1813 steps/s (collection: 0.605s, learning 3.912s)
               Value function loss: 238.7269
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 637.45
               Mean episode length: 371.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14852096
                    Iteration time: 4.52s
                        Total time: 8184.80s
                               ETA: 9877.7s

################################################################################
                     [1m Learning iteration 1813/4000 [0m

                       Computation: 1771 steps/s (collection: 0.706s, learning 3.918s)
               Value function loss: 198.2215
                    Surrogate loss: 0.0109
             Mean action noise std: 0.94
                       Mean reward: 631.15
               Mean episode length: 369.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 4.62s
                        Total time: 8189.42s
                               ETA: 9873.4s

################################################################################
                     [1m Learning iteration 1814/4000 [0m

                       Computation: 1813 steps/s (collection: 0.587s, learning 3.930s)
               Value function loss: 215.0584
                    Surrogate loss: 0.0065
             Mean action noise std: 0.94
                       Mean reward: 636.48
               Mean episode length: 373.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14868480
                    Iteration time: 4.52s
                        Total time: 8193.94s
                               ETA: 9868.8s

################################################################################
                     [1m Learning iteration 1815/4000 [0m

                       Computation: 1822 steps/s (collection: 0.577s, learning 3.917s)
               Value function loss: 220.3364
                    Surrogate loss: 0.0096
             Mean action noise std: 0.94
                       Mean reward: 649.88
               Mean episode length: 381.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 4.49s
                        Total time: 8198.43s
                               ETA: 9864.3s

################################################################################
                     [1m Learning iteration 1816/4000 [0m

                       Computation: 1830 steps/s (collection: 0.575s, learning 3.900s)
               Value function loss: 285.5194
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 658.14
               Mean episode length: 385.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14884864
                    Iteration time: 4.48s
                        Total time: 8202.91s
                               ETA: 9859.7s

################################################################################
                     [1m Learning iteration 1817/4000 [0m

                       Computation: 1787 steps/s (collection: 0.629s, learning 3.954s)
               Value function loss: 192.8626
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 668.13
               Mean episode length: 391.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 4.58s
                        Total time: 8207.49s
                               ETA: 9855.3s

################################################################################
                     [1m Learning iteration 1818/4000 [0m

                       Computation: 1812 steps/s (collection: 0.564s, learning 3.956s)
               Value function loss: 209.8183
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 659.92
               Mean episode length: 384.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14901248
                    Iteration time: 4.52s
                        Total time: 8212.01s
                               ETA: 9850.8s

################################################################################
                     [1m Learning iteration 1819/4000 [0m

                       Computation: 1796 steps/s (collection: 0.575s, learning 3.985s)
               Value function loss: 207.6855
                    Surrogate loss: 0.0106
             Mean action noise std: 0.94
                       Mean reward: 676.02
               Mean episode length: 394.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 4.56s
                        Total time: 8216.57s
                               ETA: 9846.3s

################################################################################
                     [1m Learning iteration 1820/4000 [0m

                       Computation: 1824 steps/s (collection: 0.534s, learning 3.955s)
               Value function loss: 184.6816
                    Surrogate loss: 0.0101
             Mean action noise std: 0.94
                       Mean reward: 683.21
               Mean episode length: 399.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14917632
                    Iteration time: 4.49s
                        Total time: 8221.06s
                               ETA: 9841.8s

################################################################################
                     [1m Learning iteration 1821/4000 [0m

                       Computation: 1807 steps/s (collection: 0.577s, learning 3.956s)
               Value function loss: 158.5323
                    Surrogate loss: 0.0062
             Mean action noise std: 0.94
                       Mean reward: 696.26
               Mean episode length: 409.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 4.53s
                        Total time: 8225.59s
                               ETA: 9837.3s

################################################################################
                     [1m Learning iteration 1822/4000 [0m

                       Computation: 1806 steps/s (collection: 0.644s, learning 3.890s)
               Value function loss: 222.7943
                    Surrogate loss: 0.0102
             Mean action noise std: 0.94
                       Mean reward: 705.08
               Mean episode length: 415.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14934016
                    Iteration time: 4.53s
                        Total time: 8230.13s
                               ETA: 9832.8s

################################################################################
                     [1m Learning iteration 1823/4000 [0m

                       Computation: 1803 steps/s (collection: 0.593s, learning 3.948s)
               Value function loss: 228.1779
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 716.55
               Mean episode length: 421.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 4.54s
                        Total time: 8234.67s
                               ETA: 9828.3s

################################################################################
                     [1m Learning iteration 1824/4000 [0m

                       Computation: 1813 steps/s (collection: 0.600s, learning 3.916s)
               Value function loss: 230.9866
                    Surrogate loss: 0.0081
             Mean action noise std: 0.94
                       Mean reward: 697.87
               Mean episode length: 410.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14950400
                    Iteration time: 4.52s
                        Total time: 8239.19s
                               ETA: 9823.8s

################################################################################
                     [1m Learning iteration 1825/4000 [0m

                       Computation: 1853 steps/s (collection: 0.547s, learning 3.872s)
               Value function loss: 273.6505
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 716.91
               Mean episode length: 421.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 4.42s
                        Total time: 8243.61s
                               ETA: 9819.2s

################################################################################
                     [1m Learning iteration 1826/4000 [0m

                       Computation: 1801 steps/s (collection: 0.629s, learning 3.919s)
               Value function loss: 204.6539
                    Surrogate loss: 0.0065
             Mean action noise std: 0.94
                       Mean reward: 697.56
               Mean episode length: 412.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14966784
                    Iteration time: 4.55s
                        Total time: 8248.15s
                               ETA: 9814.7s

################################################################################
                     [1m Learning iteration 1827/4000 [0m

                       Computation: 1820 steps/s (collection: 0.582s, learning 3.918s)
               Value function loss: 251.8531
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 682.55
               Mean episode length: 403.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 4.50s
                        Total time: 8252.65s
                               ETA: 9810.2s

################################################################################
                     [1m Learning iteration 1828/4000 [0m

                       Computation: 1817 steps/s (collection: 0.579s, learning 3.927s)
               Value function loss: 218.2074
                    Surrogate loss: 0.0102
             Mean action noise std: 0.94
                       Mean reward: 667.25
               Mean episode length: 394.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14983168
                    Iteration time: 4.51s
                        Total time: 8257.16s
                               ETA: 9805.7s

################################################################################
                     [1m Learning iteration 1829/4000 [0m

                       Computation: 1841 steps/s (collection: 0.548s, learning 3.900s)
               Value function loss: 184.6674
                    Surrogate loss: 0.0094
             Mean action noise std: 0.94
                       Mean reward: 643.57
               Mean episode length: 380.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 4.45s
                        Total time: 8261.61s
                               ETA: 9801.1s

################################################################################
                     [1m Learning iteration 1830/4000 [0m

                       Computation: 1782 steps/s (collection: 0.640s, learning 3.955s)
               Value function loss: 246.1310
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 636.54
               Mean episode length: 373.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14999552
                    Iteration time: 4.59s
                        Total time: 8266.20s
                               ETA: 9796.6s

################################################################################
                     [1m Learning iteration 1831/4000 [0m

                       Computation: 1823 steps/s (collection: 0.569s, learning 3.924s)
               Value function loss: 233.1491
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 604.03
               Mean episode length: 357.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 4.49s
                        Total time: 8270.70s
                               ETA: 9792.1s

################################################################################
                     [1m Learning iteration 1832/4000 [0m

                       Computation: 1810 steps/s (collection: 0.580s, learning 3.944s)
               Value function loss: 207.4176
                    Surrogate loss: 0.0105
             Mean action noise std: 0.94
                       Mean reward: 612.47
               Mean episode length: 362.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15015936
                    Iteration time: 4.52s
                        Total time: 8275.22s
                               ETA: 9787.6s

################################################################################
                     [1m Learning iteration 1833/4000 [0m

                       Computation: 1808 steps/s (collection: 0.584s, learning 3.946s)
               Value function loss: 189.8425
                    Surrogate loss: 0.0121
             Mean action noise std: 0.94
                       Mean reward: 613.43
               Mean episode length: 362.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 4.53s
                        Total time: 8279.75s
                               ETA: 9783.1s

################################################################################
                     [1m Learning iteration 1834/4000 [0m

                       Computation: 1814 steps/s (collection: 0.588s, learning 3.927s)
               Value function loss: 204.6141
                    Surrogate loss: 0.0106
             Mean action noise std: 0.94
                       Mean reward: 607.52
               Mean episode length: 358.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15032320
                    Iteration time: 4.52s
                        Total time: 8284.27s
                               ETA: 9778.6s

################################################################################
                     [1m Learning iteration 1835/4000 [0m

                       Computation: 1822 steps/s (collection: 0.576s, learning 3.919s)
               Value function loss: 164.2251
                    Surrogate loss: 0.0094
             Mean action noise std: 0.94
                       Mean reward: 615.37
               Mean episode length: 362.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 4.49s
                        Total time: 8288.76s
                               ETA: 9774.1s

################################################################################
                     [1m Learning iteration 1836/4000 [0m

                       Computation: 1819 steps/s (collection: 0.567s, learning 3.934s)
               Value function loss: 142.0249
                    Surrogate loss: 0.0111
             Mean action noise std: 0.94
                       Mean reward: 610.71
               Mean episode length: 360.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15048704
                    Iteration time: 4.50s
                        Total time: 8293.26s
                               ETA: 9769.5s

################################################################################
                     [1m Learning iteration 1837/4000 [0m

                       Computation: 1815 steps/s (collection: 0.558s, learning 3.953s)
               Value function loss: 135.1985
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 624.94
               Mean episode length: 368.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 4.51s
                        Total time: 8297.77s
                               ETA: 9765.0s

################################################################################
                     [1m Learning iteration 1838/4000 [0m

                       Computation: 1817 steps/s (collection: 0.571s, learning 3.935s)
               Value function loss: 166.6080
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 627.12
               Mean episode length: 370.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15065088
                    Iteration time: 4.51s
                        Total time: 8302.28s
                               ETA: 9760.5s

################################################################################
                     [1m Learning iteration 1839/4000 [0m

                       Computation: 1801 steps/s (collection: 0.552s, learning 3.994s)
               Value function loss: 360.2397
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 671.58
               Mean episode length: 395.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 4.55s
                        Total time: 8306.83s
                               ETA: 9756.0s

################################################################################
                     [1m Learning iteration 1840/4000 [0m

                       Computation: 1808 steps/s (collection: 0.564s, learning 3.966s)
               Value function loss: 206.6115
                    Surrogate loss: 0.0075
             Mean action noise std: 0.94
                       Mean reward: 691.47
               Mean episode length: 404.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15081472
                    Iteration time: 4.53s
                        Total time: 8311.36s
                               ETA: 9751.5s

################################################################################
                     [1m Learning iteration 1841/4000 [0m

                       Computation: 1782 steps/s (collection: 0.595s, learning 4.001s)
               Value function loss: 340.6596
                    Surrogate loss: 0.0071
             Mean action noise std: 0.94
                       Mean reward: 709.40
               Mean episode length: 412.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 4.60s
                        Total time: 8315.95s
                               ETA: 9747.1s

################################################################################
                     [1m Learning iteration 1842/4000 [0m

                       Computation: 1821 steps/s (collection: 0.540s, learning 3.958s)
               Value function loss: 230.2423
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 722.88
               Mean episode length: 417.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15097856
                    Iteration time: 4.50s
                        Total time: 8320.45s
                               ETA: 9742.6s

################################################################################
                     [1m Learning iteration 1843/4000 [0m

                       Computation: 1813 steps/s (collection: 0.559s, learning 3.958s)
               Value function loss: 233.4872
                    Surrogate loss: 0.0101
             Mean action noise std: 0.94
                       Mean reward: 704.44
               Mean episode length: 407.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 4.52s
                        Total time: 8324.97s
                               ETA: 9738.0s

################################################################################
                     [1m Learning iteration 1844/4000 [0m

                       Computation: 1786 steps/s (collection: 0.568s, learning 4.018s)
               Value function loss: 261.4418
                    Surrogate loss: 0.0096
             Mean action noise std: 0.94
                       Mean reward: 722.73
               Mean episode length: 418.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15114240
                    Iteration time: 4.59s
                        Total time: 8329.55s
                               ETA: 9733.6s

################################################################################
                     [1m Learning iteration 1845/4000 [0m

                       Computation: 1794 steps/s (collection: 0.574s, learning 3.992s)
               Value function loss: 251.2651
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 719.14
               Mean episode length: 416.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 4.57s
                        Total time: 8334.12s
                               ETA: 9729.2s

################################################################################
                     [1m Learning iteration 1846/4000 [0m

                       Computation: 1816 steps/s (collection: 0.543s, learning 3.966s)
               Value function loss: 213.9782
                    Surrogate loss: 0.0127
             Mean action noise std: 0.94
                       Mean reward: 732.20
               Mean episode length: 423.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15130624
                    Iteration time: 4.51s
                        Total time: 8338.63s
                               ETA: 9724.6s

################################################################################
                     [1m Learning iteration 1847/4000 [0m

                       Computation: 1797 steps/s (collection: 0.571s, learning 3.987s)
               Value function loss: 216.2613
                    Surrogate loss: 0.0111
             Mean action noise std: 0.94
                       Mean reward: 722.49
               Mean episode length: 417.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 4.56s
                        Total time: 8343.19s
                               ETA: 9720.2s

################################################################################
                     [1m Learning iteration 1848/4000 [0m

                       Computation: 1814 steps/s (collection: 0.591s, learning 3.925s)
               Value function loss: 236.8526
                    Surrogate loss: 0.0094
             Mean action noise std: 0.94
                       Mean reward: 716.41
               Mean episode length: 414.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15147008
                    Iteration time: 4.52s
                        Total time: 8347.70s
                               ETA: 9715.7s

################################################################################
                     [1m Learning iteration 1849/4000 [0m

                       Computation: 1830 steps/s (collection: 0.562s, learning 3.912s)
               Value function loss: 172.4181
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 682.52
               Mean episode length: 396.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 4.47s
                        Total time: 8352.18s
                               ETA: 9711.1s

################################################################################
                     [1m Learning iteration 1850/4000 [0m

                       Computation: 1831 steps/s (collection: 0.536s, learning 3.935s)
               Value function loss: 166.5684
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 671.63
               Mean episode length: 391.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15163392
                    Iteration time: 4.47s
                        Total time: 8356.65s
                               ETA: 9706.5s

################################################################################
                     [1m Learning iteration 1851/4000 [0m

                       Computation: 1820 steps/s (collection: 0.564s, learning 3.936s)
               Value function loss: 207.1852
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 658.64
               Mean episode length: 385.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 4.50s
                        Total time: 8361.15s
                               ETA: 9702.0s

################################################################################
                     [1m Learning iteration 1852/4000 [0m

                       Computation: 1834 steps/s (collection: 0.555s, learning 3.910s)
               Value function loss: 148.9273
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 658.89
               Mean episode length: 386.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15179776
                    Iteration time: 4.47s
                        Total time: 8365.61s
                               ETA: 9697.4s

################################################################################
                     [1m Learning iteration 1853/4000 [0m

                       Computation: 1840 steps/s (collection: 0.552s, learning 3.899s)
               Value function loss: 173.1745
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 650.91
               Mean episode length: 382.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 4.45s
                        Total time: 8370.07s
                               ETA: 9692.8s

################################################################################
                     [1m Learning iteration 1854/4000 [0m

                       Computation: 1831 steps/s (collection: 0.542s, learning 3.930s)
               Value function loss: 119.8352
                    Surrogate loss: 0.0117
             Mean action noise std: 0.94
                       Mean reward: 633.24
               Mean episode length: 373.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15196160
                    Iteration time: 4.47s
                        Total time: 8374.54s
                               ETA: 9688.3s

################################################################################
                     [1m Learning iteration 1855/4000 [0m

                       Computation: 1834 steps/s (collection: 0.574s, learning 3.891s)
               Value function loss: 317.2186
                    Surrogate loss: 0.0103
             Mean action noise std: 0.94
                       Mean reward: 616.81
               Mean episode length: 362.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 4.46s
                        Total time: 8379.00s
                               ETA: 9683.7s

################################################################################
                     [1m Learning iteration 1856/4000 [0m

                       Computation: 1836 steps/s (collection: 0.533s, learning 3.927s)
               Value function loss: 232.8535
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 615.35
               Mean episode length: 361.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15212544
                    Iteration time: 4.46s
                        Total time: 8383.46s
                               ETA: 9679.1s

################################################################################
                     [1m Learning iteration 1857/4000 [0m

                       Computation: 1837 steps/s (collection: 0.572s, learning 3.887s)
               Value function loss: 240.9104
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 610.43
               Mean episode length: 359.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 4.46s
                        Total time: 8387.92s
                               ETA: 9674.6s

################################################################################
                     [1m Learning iteration 1858/4000 [0m

                       Computation: 1832 steps/s (collection: 0.582s, learning 3.889s)
               Value function loss: 175.3087
                    Surrogate loss: 0.0076
             Mean action noise std: 0.94
                       Mean reward: 618.93
               Mean episode length: 365.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15228928
                    Iteration time: 4.47s
                        Total time: 8392.39s
                               ETA: 9670.0s

################################################################################
                     [1m Learning iteration 1859/4000 [0m

                       Computation: 1800 steps/s (collection: 0.622s, learning 3.927s)
               Value function loss: 263.5799
                    Surrogate loss: 0.0068
             Mean action noise std: 0.94
                       Mean reward: 600.06
               Mean episode length: 353.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 4.55s
                        Total time: 8396.94s
                               ETA: 9665.5s

################################################################################
                     [1m Learning iteration 1860/4000 [0m

                       Computation: 1833 steps/s (collection: 0.529s, learning 3.939s)
               Value function loss: 225.3849
                    Surrogate loss: 0.0079
             Mean action noise std: 0.94
                       Mean reward: 617.30
               Mean episode length: 364.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15245312
                    Iteration time: 4.47s
                        Total time: 8401.41s
                               ETA: 9660.9s

################################################################################
                     [1m Learning iteration 1861/4000 [0m

                       Computation: 1814 steps/s (collection: 0.559s, learning 3.956s)
               Value function loss: 209.9672
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 636.03
               Mean episode length: 374.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 4.51s
                        Total time: 8405.92s
                               ETA: 9656.4s

################################################################################
                     [1m Learning iteration 1862/4000 [0m

                       Computation: 1820 steps/s (collection: 0.592s, learning 3.909s)
               Value function loss: 205.5333
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 643.01
               Mean episode length: 377.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15261696
                    Iteration time: 4.50s
                        Total time: 8410.43s
                               ETA: 9651.9s

################################################################################
                     [1m Learning iteration 1863/4000 [0m

                       Computation: 1856 steps/s (collection: 0.496s, learning 3.916s)
               Value function loss: 205.3213
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 660.40
               Mean episode length: 386.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 4.41s
                        Total time: 8414.84s
                               ETA: 9647.3s

################################################################################
                     [1m Learning iteration 1864/4000 [0m

                       Computation: 1838 steps/s (collection: 0.567s, learning 3.889s)
               Value function loss: 216.6298
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 657.19
               Mean episode length: 385.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15278080
                    Iteration time: 4.46s
                        Total time: 8419.29s
                               ETA: 9642.7s

################################################################################
                     [1m Learning iteration 1865/4000 [0m

                       Computation: 1837 steps/s (collection: 0.549s, learning 3.910s)
               Value function loss: 214.6948
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 678.45
               Mean episode length: 396.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 4.46s
                        Total time: 8423.75s
                               ETA: 9638.1s

################################################################################
                     [1m Learning iteration 1866/4000 [0m

                       Computation: 1831 steps/s (collection: 0.528s, learning 3.945s)
               Value function loss: 163.8295
                    Surrogate loss: 0.0075
             Mean action noise std: 0.94
                       Mean reward: 639.08
               Mean episode length: 373.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15294464
                    Iteration time: 4.47s
                        Total time: 8428.23s
                               ETA: 9633.5s

################################################################################
                     [1m Learning iteration 1867/4000 [0m

                       Computation: 1786 steps/s (collection: 0.597s, learning 3.988s)
               Value function loss: 215.1984
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 653.68
               Mean episode length: 380.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 4.59s
                        Total time: 8432.81s
                               ETA: 9629.1s

################################################################################
                     [1m Learning iteration 1868/4000 [0m

                       Computation: 1819 steps/s (collection: 0.544s, learning 3.957s)
               Value function loss: 160.8331
                    Surrogate loss: 0.0081
             Mean action noise std: 0.94
                       Mean reward: 657.25
               Mean episode length: 382.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15310848
                    Iteration time: 4.50s
                        Total time: 8437.31s
                               ETA: 9624.6s

################################################################################
                     [1m Learning iteration 1869/4000 [0m

                       Computation: 1810 steps/s (collection: 0.561s, learning 3.965s)
               Value function loss: 160.1882
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 663.52
               Mean episode length: 386.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 4.53s
                        Total time: 8441.84s
                               ETA: 9620.1s

################################################################################
                     [1m Learning iteration 1870/4000 [0m

                       Computation: 1826 steps/s (collection: 0.522s, learning 3.963s)
               Value function loss: 293.9952
                    Surrogate loss: 0.0109
             Mean action noise std: 0.94
                       Mean reward: 659.86
               Mean episode length: 381.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15327232
                    Iteration time: 4.48s
                        Total time: 8446.32s
                               ETA: 9615.5s

################################################################################
                     [1m Learning iteration 1871/4000 [0m

                       Computation: 1814 steps/s (collection: 0.563s, learning 3.951s)
               Value function loss: 164.0008
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 658.32
               Mean episode length: 381.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 4.51s
                        Total time: 8450.84s
                               ETA: 9611.0s

################################################################################
                     [1m Learning iteration 1872/4000 [0m

                       Computation: 1850 steps/s (collection: 0.535s, learning 3.891s)
               Value function loss: 296.1315
                    Surrogate loss: 0.0077
             Mean action noise std: 0.94
                       Mean reward: 657.68
               Mean episode length: 381.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15343616
                    Iteration time: 4.43s
                        Total time: 8455.26s
                               ETA: 9606.4s

################################################################################
                     [1m Learning iteration 1873/4000 [0m

                       Computation: 1828 steps/s (collection: 0.584s, learning 3.896s)
               Value function loss: 220.0274
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 653.99
               Mean episode length: 379.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 4.48s
                        Total time: 8459.74s
                               ETA: 9601.9s

################################################################################
                     [1m Learning iteration 1874/4000 [0m

                       Computation: 1789 steps/s (collection: 0.635s, learning 3.943s)
               Value function loss: 264.5899
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 644.78
               Mean episode length: 376.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15360000
                    Iteration time: 4.58s
                        Total time: 8464.32s
                               ETA: 9597.4s

################################################################################
                     [1m Learning iteration 1875/4000 [0m

                       Computation: 1838 steps/s (collection: 0.567s, learning 3.890s)
               Value function loss: 226.6170
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 677.35
               Mean episode length: 394.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 4.46s
                        Total time: 8468.78s
                               ETA: 9592.8s

################################################################################
                     [1m Learning iteration 1876/4000 [0m

                       Computation: 1813 steps/s (collection: 0.599s, learning 3.919s)
               Value function loss: 215.0448
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 681.01
               Mean episode length: 396.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15376384
                    Iteration time: 4.52s
                        Total time: 8473.30s
                               ETA: 9588.3s

################################################################################
                     [1m Learning iteration 1877/4000 [0m

                       Computation: 1821 steps/s (collection: 0.571s, learning 3.926s)
               Value function loss: 162.2103
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 689.99
               Mean episode length: 402.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 4.50s
                        Total time: 8477.79s
                               ETA: 9583.8s

################################################################################
                     [1m Learning iteration 1878/4000 [0m

                       Computation: 1814 steps/s (collection: 0.561s, learning 3.954s)
               Value function loss: 267.8539
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 705.91
               Mean episode length: 412.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15392768
                    Iteration time: 4.52s
                        Total time: 8482.31s
                               ETA: 9579.3s

################################################################################
                     [1m Learning iteration 1879/4000 [0m

                       Computation: 1820 steps/s (collection: 0.556s, learning 3.944s)
               Value function loss: 217.3291
                    Surrogate loss: 0.0098
             Mean action noise std: 0.94
                       Mean reward: 693.71
               Mean episode length: 405.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 4.50s
                        Total time: 8486.81s
                               ETA: 9574.7s

################################################################################
                     [1m Learning iteration 1880/4000 [0m

                       Computation: 1832 steps/s (collection: 0.554s, learning 3.916s)
               Value function loss: 296.7285
                    Surrogate loss: 0.0105
             Mean action noise std: 0.94
                       Mean reward: 682.44
               Mean episode length: 400.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15409152
                    Iteration time: 4.47s
                        Total time: 8491.28s
                               ETA: 9570.2s

################################################################################
                     [1m Learning iteration 1881/4000 [0m

                       Computation: 1836 steps/s (collection: 0.541s, learning 3.919s)
               Value function loss: 170.6832
                    Surrogate loss: 0.0077
             Mean action noise std: 0.94
                       Mean reward: 672.37
               Mean episode length: 395.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 4.46s
                        Total time: 8495.74s
                               ETA: 9565.6s

################################################################################
                     [1m Learning iteration 1882/4000 [0m

                       Computation: 1837 steps/s (collection: 0.551s, learning 3.907s)
               Value function loss: 266.7977
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 688.05
               Mean episode length: 404.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15425536
                    Iteration time: 4.46s
                        Total time: 8500.20s
                               ETA: 9561.0s

################################################################################
                     [1m Learning iteration 1883/4000 [0m

                       Computation: 1855 steps/s (collection: 0.526s, learning 3.889s)
               Value function loss: 213.0764
                    Surrogate loss: 0.0080
             Mean action noise std: 0.94
                       Mean reward: 688.68
               Mean episode length: 404.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 4.42s
                        Total time: 8504.61s
                               ETA: 9556.4s

################################################################################
                     [1m Learning iteration 1884/4000 [0m

                       Computation: 1818 steps/s (collection: 0.561s, learning 3.944s)
               Value function loss: 241.2222
                    Surrogate loss: 0.0072
             Mean action noise std: 0.94
                       Mean reward: 695.83
               Mean episode length: 408.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15441920
                    Iteration time: 4.50s
                        Total time: 8509.12s
                               ETA: 9551.9s

################################################################################
                     [1m Learning iteration 1885/4000 [0m

                       Computation: 1838 steps/s (collection: 0.542s, learning 3.914s)
               Value function loss: 183.8618
                    Surrogate loss: 0.0066
             Mean action noise std: 0.94
                       Mean reward: 696.50
               Mean episode length: 407.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 4.46s
                        Total time: 8513.57s
                               ETA: 9547.3s

################################################################################
                     [1m Learning iteration 1886/4000 [0m

                       Computation: 1833 steps/s (collection: 0.555s, learning 3.913s)
               Value function loss: 300.4861
                    Surrogate loss: 0.0068
             Mean action noise std: 0.94
                       Mean reward: 702.18
               Mean episode length: 411.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15458304
                    Iteration time: 4.47s
                        Total time: 8518.04s
                               ETA: 9542.7s

################################################################################
                     [1m Learning iteration 1887/4000 [0m

                       Computation: 1819 steps/s (collection: 0.566s, learning 3.937s)
               Value function loss: 126.0932
                    Surrogate loss: 0.0080
             Mean action noise std: 0.94
                       Mean reward: 704.83
               Mean episode length: 412.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 4.50s
                        Total time: 8522.54s
                               ETA: 9538.2s

################################################################################
                     [1m Learning iteration 1888/4000 [0m

                       Computation: 1805 steps/s (collection: 0.555s, learning 3.981s)
               Value function loss: 214.8263
                    Surrogate loss: 0.0070
             Mean action noise std: 0.94
                       Mean reward: 708.73
               Mean episode length: 414.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15474688
                    Iteration time: 4.54s
                        Total time: 8527.08s
                               ETA: 9533.7s

################################################################################
                     [1m Learning iteration 1889/4000 [0m

                       Computation: 1779 steps/s (collection: 0.631s, learning 3.972s)
               Value function loss: 258.5575
                    Surrogate loss: 0.0071
             Mean action noise std: 0.94
                       Mean reward: 736.33
               Mean episode length: 430.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 4.60s
                        Total time: 8531.68s
                               ETA: 9529.3s

################################################################################
                     [1m Learning iteration 1890/4000 [0m

                       Computation: 1825 steps/s (collection: 0.538s, learning 3.950s)
               Value function loss: 219.5849
                    Surrogate loss: 0.0079
             Mean action noise std: 0.94
                       Mean reward: 738.58
               Mean episode length: 430.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15491072
                    Iteration time: 4.49s
                        Total time: 8536.17s
                               ETA: 9524.8s

################################################################################
                     [1m Learning iteration 1891/4000 [0m

                       Computation: 1818 steps/s (collection: 0.555s, learning 3.951s)
               Value function loss: 259.7962
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 750.43
               Mean episode length: 436.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 4.51s
                        Total time: 8540.68s
                               ETA: 9520.2s

################################################################################
                     [1m Learning iteration 1892/4000 [0m

                       Computation: 1784 steps/s (collection: 0.618s, learning 3.973s)
               Value function loss: 176.6493
                    Surrogate loss: 0.0084
             Mean action noise std: 0.94
                       Mean reward: 747.25
               Mean episode length: 433.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15507456
                    Iteration time: 4.59s
                        Total time: 8545.27s
                               ETA: 9515.8s

################################################################################
                     [1m Learning iteration 1893/4000 [0m

                       Computation: 1768 steps/s (collection: 0.650s, learning 3.983s)
               Value function loss: 198.9620
                    Surrogate loss: 0.0121
             Mean action noise std: 0.94
                       Mean reward: 753.82
               Mean episode length: 437.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 4.63s
                        Total time: 8549.90s
                               ETA: 9511.4s

################################################################################
                     [1m Learning iteration 1894/4000 [0m

                       Computation: 1789 steps/s (collection: 0.610s, learning 3.969s)
               Value function loss: 275.5291
                    Surrogate loss: 0.0067
             Mean action noise std: 0.94
                       Mean reward: 747.48
               Mean episode length: 434.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15523840
                    Iteration time: 4.58s
                        Total time: 8554.48s
                               ETA: 9507.0s

################################################################################
                     [1m Learning iteration 1895/4000 [0m

                       Computation: 1798 steps/s (collection: 0.591s, learning 3.965s)
               Value function loss: 239.4932
                    Surrogate loss: 0.0079
             Mean action noise std: 0.94
                       Mean reward: 742.81
               Mean episode length: 431.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 4.56s
                        Total time: 8559.03s
                               ETA: 9502.5s

################################################################################
                     [1m Learning iteration 1896/4000 [0m

                       Computation: 1759 steps/s (collection: 0.627s, learning 4.029s)
               Value function loss: 214.1073
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 752.02
               Mean episode length: 437.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15540224
                    Iteration time: 4.66s
                        Total time: 8563.69s
                               ETA: 9498.2s

################################################################################
                     [1m Learning iteration 1897/4000 [0m

                       Computation: 1837 steps/s (collection: 0.554s, learning 3.906s)
               Value function loss: 241.4667
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 743.62
               Mean episode length: 432.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 4.46s
                        Total time: 8568.15s
                               ETA: 9493.6s

################################################################################
                     [1m Learning iteration 1898/4000 [0m

                       Computation: 1810 steps/s (collection: 0.580s, learning 3.945s)
               Value function loss: 255.4526
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 740.34
               Mean episode length: 429.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15556608
                    Iteration time: 4.52s
                        Total time: 8572.67s
                               ETA: 9489.1s

################################################################################
                     [1m Learning iteration 1899/4000 [0m

                       Computation: 1820 steps/s (collection: 0.578s, learning 3.922s)
               Value function loss: 202.3545
                    Surrogate loss: 0.0054
             Mean action noise std: 0.94
                       Mean reward: 748.96
               Mean episode length: 434.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 4.50s
                        Total time: 8577.18s
                               ETA: 9484.6s

################################################################################
                     [1m Learning iteration 1900/4000 [0m

                       Computation: 1834 steps/s (collection: 0.561s, learning 3.905s)
               Value function loss: 282.9427
                    Surrogate loss: 0.0065
             Mean action noise std: 0.94
                       Mean reward: 724.67
               Mean episode length: 422.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15572992
                    Iteration time: 4.47s
                        Total time: 8581.64s
                               ETA: 9480.0s

################################################################################
                     [1m Learning iteration 1901/4000 [0m

                       Computation: 1792 steps/s (collection: 0.621s, learning 3.949s)
               Value function loss: 216.1372
                    Surrogate loss: 0.0073
             Mean action noise std: 0.94
                       Mean reward: 720.08
               Mean episode length: 420.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 4.57s
                        Total time: 8586.21s
                               ETA: 9475.5s

################################################################################
                     [1m Learning iteration 1902/4000 [0m

                       Computation: 1791 steps/s (collection: 0.595s, learning 3.978s)
               Value function loss: 223.6129
                    Surrogate loss: 0.0067
             Mean action noise std: 0.94
                       Mean reward: 702.51
               Mean episode length: 411.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15589376
                    Iteration time: 4.57s
                        Total time: 8590.78s
                               ETA: 9471.1s

################################################################################
                     [1m Learning iteration 1903/4000 [0m

                       Computation: 1824 steps/s (collection: 0.552s, learning 3.938s)
               Value function loss: 211.8038
                    Surrogate loss: 0.0084
             Mean action noise std: 0.94
                       Mean reward: 680.64
               Mean episode length: 400.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 4.49s
                        Total time: 8595.27s
                               ETA: 9466.5s

################################################################################
                     [1m Learning iteration 1904/4000 [0m

                       Computation: 1837 steps/s (collection: 0.534s, learning 3.925s)
               Value function loss: 256.8300
                    Surrogate loss: 0.0077
             Mean action noise std: 0.94
                       Mean reward: 693.25
               Mean episode length: 408.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15605760
                    Iteration time: 4.46s
                        Total time: 8599.73s
                               ETA: 9462.0s

################################################################################
                     [1m Learning iteration 1905/4000 [0m

                       Computation: 1826 steps/s (collection: 0.589s, learning 3.896s)
               Value function loss: 158.3474
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 692.76
               Mean episode length: 407.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 4.48s
                        Total time: 8604.22s
                               ETA: 9457.4s

################################################################################
                     [1m Learning iteration 1906/4000 [0m

                       Computation: 1823 steps/s (collection: 0.593s, learning 3.900s)
               Value function loss: 305.6101
                    Surrogate loss: 0.0061
             Mean action noise std: 0.94
                       Mean reward: 689.63
               Mean episode length: 405.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15622144
                    Iteration time: 4.49s
                        Total time: 8608.71s
                               ETA: 9452.9s

################################################################################
                     [1m Learning iteration 1907/4000 [0m

                       Computation: 1823 steps/s (collection: 0.560s, learning 3.932s)
               Value function loss: 249.0563
                    Surrogate loss: 0.0060
             Mean action noise std: 0.94
                       Mean reward: 692.61
               Mean episode length: 406.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 4.49s
                        Total time: 8613.20s
                               ETA: 9448.3s

################################################################################
                     [1m Learning iteration 1908/4000 [0m

                       Computation: 1805 steps/s (collection: 0.575s, learning 3.961s)
               Value function loss: 166.5367
                    Surrogate loss: 0.0137
             Mean action noise std: 0.94
                       Mean reward: 682.95
               Mean episode length: 399.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15638528
                    Iteration time: 4.54s
                        Total time: 8617.74s
                               ETA: 9443.9s

################################################################################
                     [1m Learning iteration 1909/4000 [0m

                       Computation: 1811 steps/s (collection: 0.590s, learning 3.932s)
               Value function loss: 214.7960
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 669.91
               Mean episode length: 392.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 4.52s
                        Total time: 8622.26s
                               ETA: 9439.3s

################################################################################
                     [1m Learning iteration 1910/4000 [0m

                       Computation: 1816 steps/s (collection: 0.562s, learning 3.949s)
               Value function loss: 197.5263
                    Surrogate loss: 0.0058
             Mean action noise std: 0.94
                       Mean reward: 679.83
               Mean episode length: 396.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15654912
                    Iteration time: 4.51s
                        Total time: 8626.77s
                               ETA: 9434.8s

################################################################################
                     [1m Learning iteration 1911/4000 [0m

                       Computation: 1830 steps/s (collection: 0.568s, learning 3.907s)
               Value function loss: 295.5124
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 685.86
               Mean episode length: 398.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 4.47s
                        Total time: 8631.25s
                               ETA: 9430.3s

################################################################################
                     [1m Learning iteration 1912/4000 [0m

                       Computation: 1809 steps/s (collection: 0.623s, learning 3.903s)
               Value function loss: 204.5855
                    Surrogate loss: 0.0058
             Mean action noise std: 0.94
                       Mean reward: 692.69
               Mean episode length: 400.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15671296
                    Iteration time: 4.53s
                        Total time: 8635.77s
                               ETA: 9425.8s

################################################################################
                     [1m Learning iteration 1913/4000 [0m

                       Computation: 1818 steps/s (collection: 0.556s, learning 3.949s)
               Value function loss: 187.9535
                    Surrogate loss: 0.0103
             Mean action noise std: 0.94
                       Mean reward: 701.79
               Mean episode length: 405.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 4.50s
                        Total time: 8640.28s
                               ETA: 9421.2s

################################################################################
                     [1m Learning iteration 1914/4000 [0m

                       Computation: 1817 steps/s (collection: 0.568s, learning 3.939s)
               Value function loss: 249.1608
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 698.95
               Mean episode length: 403.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15687680
                    Iteration time: 4.51s
                        Total time: 8644.78s
                               ETA: 9416.7s

################################################################################
                     [1m Learning iteration 1915/4000 [0m

                       Computation: 1816 steps/s (collection: 0.594s, learning 3.917s)
               Value function loss: 191.1063
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 704.09
               Mean episode length: 406.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 4.51s
                        Total time: 8649.29s
                               ETA: 9412.2s

################################################################################
                     [1m Learning iteration 1916/4000 [0m

                       Computation: 1811 steps/s (collection: 0.593s, learning 3.929s)
               Value function loss: 222.9566
                    Surrogate loss: 0.0084
             Mean action noise std: 0.94
                       Mean reward: 716.59
               Mean episode length: 414.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15704064
                    Iteration time: 4.52s
                        Total time: 8653.82s
                               ETA: 9407.7s

################################################################################
                     [1m Learning iteration 1917/4000 [0m

                       Computation: 1824 steps/s (collection: 0.562s, learning 3.927s)
               Value function loss: 152.8577
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 722.33
               Mean episode length: 418.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 4.49s
                        Total time: 8658.30s
                               ETA: 9403.2s

################################################################################
                     [1m Learning iteration 1918/4000 [0m

                       Computation: 1807 steps/s (collection: 0.588s, learning 3.943s)
               Value function loss: 189.0816
                    Surrogate loss: 0.0075
             Mean action noise std: 0.94
                       Mean reward: 725.87
               Mean episode length: 419.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15720448
                    Iteration time: 4.53s
                        Total time: 8662.84s
                               ETA: 9398.7s

################################################################################
                     [1m Learning iteration 1919/4000 [0m

                       Computation: 1825 steps/s (collection: 0.544s, learning 3.944s)
               Value function loss: 254.0850
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 740.82
               Mean episode length: 428.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 4.49s
                        Total time: 8667.32s
                               ETA: 9394.1s

################################################################################
                     [1m Learning iteration 1920/4000 [0m

                       Computation: 1803 steps/s (collection: 0.571s, learning 3.971s)
               Value function loss: 238.7629
                    Surrogate loss: 0.0073
             Mean action noise std: 0.94
                       Mean reward: 754.19
               Mean episode length: 435.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15736832
                    Iteration time: 4.54s
                        Total time: 8671.87s
                               ETA: 9389.6s

################################################################################
                     [1m Learning iteration 1921/4000 [0m

                       Computation: 1790 steps/s (collection: 0.539s, learning 4.036s)
               Value function loss: 186.4215
                    Surrogate loss: 0.0080
             Mean action noise std: 0.94
                       Mean reward: 761.37
               Mean episode length: 440.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 4.57s
                        Total time: 8676.44s
                               ETA: 9385.2s

################################################################################
                     [1m Learning iteration 1922/4000 [0m

                       Computation: 1785 steps/s (collection: 0.576s, learning 4.013s)
               Value function loss: 233.3273
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 766.75
               Mean episode length: 444.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15753216
                    Iteration time: 4.59s
                        Total time: 8681.03s
                               ETA: 9380.7s

################################################################################
                     [1m Learning iteration 1923/4000 [0m

                       Computation: 1805 steps/s (collection: 0.564s, learning 3.973s)
               Value function loss: 240.9647
                    Surrogate loss: 0.0081
             Mean action noise std: 0.94
                       Mean reward: 773.26
               Mean episode length: 448.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 4.54s
                        Total time: 8685.57s
                               ETA: 9376.3s

################################################################################
                     [1m Learning iteration 1924/4000 [0m

                       Computation: 1789 steps/s (collection: 0.603s, learning 3.975s)
               Value function loss: 264.0990
                    Surrogate loss: 0.0111
             Mean action noise std: 0.94
                       Mean reward: 771.10
               Mean episode length: 446.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15769600
                    Iteration time: 4.58s
                        Total time: 8690.15s
                               ETA: 9371.8s

################################################################################
                     [1m Learning iteration 1925/4000 [0m

                       Computation: 1811 steps/s (collection: 0.601s, learning 3.921s)
               Value function loss: 227.7462
                    Surrogate loss: 0.0102
             Mean action noise std: 0.94
                       Mean reward: 784.64
               Mean episode length: 454.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 4.52s
                        Total time: 8694.67s
                               ETA: 9367.3s

################################################################################
                     [1m Learning iteration 1926/4000 [0m

                       Computation: 1841 steps/s (collection: 0.545s, learning 3.904s)
               Value function loss: 235.8373
                    Surrogate loss: 0.0079
             Mean action noise std: 0.94
                       Mean reward: 784.77
               Mean episode length: 453.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15785984
                    Iteration time: 4.45s
                        Total time: 8699.12s
                               ETA: 9362.7s

################################################################################
                     [1m Learning iteration 1927/4000 [0m

                       Computation: 1825 steps/s (collection: 0.594s, learning 3.894s)
               Value function loss: 294.0942
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 778.40
               Mean episode length: 449.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 4.49s
                        Total time: 8703.60s
                               ETA: 9358.2s

################################################################################
                     [1m Learning iteration 1928/4000 [0m

                       Computation: 1817 steps/s (collection: 0.572s, learning 3.936s)
               Value function loss: 265.0233
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 772.11
               Mean episode length: 445.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15802368
                    Iteration time: 4.51s
                        Total time: 8708.11s
                               ETA: 9353.7s

################################################################################
                     [1m Learning iteration 1929/4000 [0m

                       Computation: 1847 steps/s (collection: 0.540s, learning 3.894s)
               Value function loss: 302.4008
                    Surrogate loss: 0.0081
             Mean action noise std: 0.94
                       Mean reward: 751.87
               Mean episode length: 434.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 4.43s
                        Total time: 8712.55s
                               ETA: 9349.1s

################################################################################
                     [1m Learning iteration 1930/4000 [0m

                       Computation: 1827 steps/s (collection: 0.544s, learning 3.939s)
               Value function loss: 183.5993
                    Surrogate loss: 0.0084
             Mean action noise std: 0.94
                       Mean reward: 737.25
               Mean episode length: 426.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15818752
                    Iteration time: 4.48s
                        Total time: 8717.03s
                               ETA: 9344.5s

################################################################################
                     [1m Learning iteration 1931/4000 [0m

                       Computation: 1808 steps/s (collection: 0.572s, learning 3.957s)
               Value function loss: 291.3154
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 743.11
               Mean episode length: 429.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 4.53s
                        Total time: 8721.56s
                               ETA: 9340.0s

################################################################################
                     [1m Learning iteration 1932/4000 [0m

                       Computation: 1854 steps/s (collection: 0.508s, learning 3.908s)
               Value function loss: 130.4038
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 740.22
               Mean episode length: 426.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15835136
                    Iteration time: 4.42s
                        Total time: 8725.98s
                               ETA: 9335.4s

################################################################################
                     [1m Learning iteration 1933/4000 [0m

                       Computation: 1831 steps/s (collection: 0.589s, learning 3.883s)
               Value function loss: 240.4822
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 754.06
               Mean episode length: 433.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 4.47s
                        Total time: 8730.45s
                               ETA: 9330.8s

################################################################################
                     [1m Learning iteration 1934/4000 [0m

                       Computation: 1836 steps/s (collection: 0.536s, learning 3.926s)
               Value function loss: 171.1879
                    Surrogate loss: 0.0094
             Mean action noise std: 0.94
                       Mean reward: 754.29
               Mean episode length: 432.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15851520
                    Iteration time: 4.46s
                        Total time: 8734.91s
                               ETA: 9326.3s

################################################################################
                     [1m Learning iteration 1935/4000 [0m

                       Computation: 1834 steps/s (collection: 0.589s, learning 3.876s)
               Value function loss: 325.1780
                    Surrogate loss: 0.0119
             Mean action noise std: 0.94
                       Mean reward: 759.42
               Mean episode length: 434.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 4.47s
                        Total time: 8739.37s
                               ETA: 9321.7s

################################################################################
                     [1m Learning iteration 1936/4000 [0m

                       Computation: 1856 steps/s (collection: 0.545s, learning 3.868s)
               Value function loss: 156.8423
                    Surrogate loss: 0.0109
             Mean action noise std: 0.94
                       Mean reward: 751.48
               Mean episode length: 431.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15867904
                    Iteration time: 4.41s
                        Total time: 8743.79s
                               ETA: 9317.1s

################################################################################
                     [1m Learning iteration 1937/4000 [0m

                       Computation: 1830 steps/s (collection: 0.556s, learning 3.921s)
               Value function loss: 261.4532
                    Surrogate loss: 0.0094
             Mean action noise std: 0.94
                       Mean reward: 759.79
               Mean episode length: 434.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 4.48s
                        Total time: 8748.26s
                               ETA: 9312.5s

################################################################################
                     [1m Learning iteration 1938/4000 [0m

                       Computation: 1836 steps/s (collection: 0.572s, learning 3.889s)
               Value function loss: 220.9645
                    Surrogate loss: 0.0079
             Mean action noise std: 0.94
                       Mean reward: 744.95
               Mean episode length: 424.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15884288
                    Iteration time: 4.46s
                        Total time: 8752.73s
                               ETA: 9308.0s

################################################################################
                     [1m Learning iteration 1939/4000 [0m

                       Computation: 1830 steps/s (collection: 0.546s, learning 3.930s)
               Value function loss: 185.1367
                    Surrogate loss: 0.0094
             Mean action noise std: 0.94
                       Mean reward: 751.59
               Mean episode length: 428.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 4.48s
                        Total time: 8757.20s
                               ETA: 9303.4s

################################################################################
                     [1m Learning iteration 1940/4000 [0m

                       Computation: 1842 steps/s (collection: 0.526s, learning 3.919s)
               Value function loss: 240.2414
                    Surrogate loss: 0.0081
             Mean action noise std: 0.94
                       Mean reward: 756.10
               Mean episode length: 430.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15900672
                    Iteration time: 4.45s
                        Total time: 8761.65s
                               ETA: 9298.8s

################################################################################
                     [1m Learning iteration 1941/4000 [0m

                       Computation: 1820 steps/s (collection: 0.541s, learning 3.958s)
               Value function loss: 204.2435
                    Surrogate loss: 0.0116
             Mean action noise std: 0.94
                       Mean reward: 771.15
               Mean episode length: 437.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 4.50s
                        Total time: 8766.15s
                               ETA: 9294.3s

################################################################################
                     [1m Learning iteration 1942/4000 [0m

                       Computation: 1843 steps/s (collection: 0.549s, learning 3.895s)
               Value function loss: 296.7111
                    Surrogate loss: 0.0103
             Mean action noise std: 0.94
                       Mean reward: 761.44
               Mean episode length: 433.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15917056
                    Iteration time: 4.44s
                        Total time: 8770.59s
                               ETA: 9289.7s

################################################################################
                     [1m Learning iteration 1943/4000 [0m

                       Computation: 1818 steps/s (collection: 0.590s, learning 3.914s)
               Value function loss: 258.9720
                    Surrogate loss: 0.0074
             Mean action noise std: 0.94
                       Mean reward: 768.73
               Mean episode length: 438.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 4.50s
                        Total time: 8775.09s
                               ETA: 9285.2s

################################################################################
                     [1m Learning iteration 1944/4000 [0m

                       Computation: 1838 steps/s (collection: 0.562s, learning 3.894s)
               Value function loss: 249.2683
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 760.58
               Mean episode length: 434.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15933440
                    Iteration time: 4.46s
                        Total time: 8779.55s
                               ETA: 9280.6s

################################################################################
                     [1m Learning iteration 1945/4000 [0m

                       Computation: 1807 steps/s (collection: 0.533s, learning 3.999s)
               Value function loss: 275.0721
                    Surrogate loss: 0.0094
             Mean action noise std: 0.94
                       Mean reward: 743.14
               Mean episode length: 424.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 4.53s
                        Total time: 8784.08s
                               ETA: 9276.1s

################################################################################
                     [1m Learning iteration 1946/4000 [0m

                       Computation: 1818 steps/s (collection: 0.554s, learning 3.950s)
               Value function loss: 203.0784
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 749.10
               Mean episode length: 427.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15949824
                    Iteration time: 4.50s
                        Total time: 8788.59s
                               ETA: 9271.6s

################################################################################
                     [1m Learning iteration 1947/4000 [0m

                       Computation: 1810 steps/s (collection: 0.561s, learning 3.964s)
               Value function loss: 241.7221
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 734.24
               Mean episode length: 420.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 4.52s
                        Total time: 8793.11s
                               ETA: 9267.1s

################################################################################
                     [1m Learning iteration 1948/4000 [0m

                       Computation: 1815 steps/s (collection: 0.565s, learning 3.946s)
               Value function loss: 106.8459
                    Surrogate loss: 0.0112
             Mean action noise std: 0.94
                       Mean reward: 747.04
               Mean episode length: 426.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 15966208
                    Iteration time: 4.51s
                        Total time: 8797.62s
                               ETA: 9262.6s

################################################################################
                     [1m Learning iteration 1949/4000 [0m

                       Computation: 1807 steps/s (collection: 0.577s, learning 3.956s)
               Value function loss: 177.1174
                    Surrogate loss: 0.0080
             Mean action noise std: 0.94
                       Mean reward: 736.88
               Mean episode length: 423.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 4.53s
                        Total time: 8802.16s
                               ETA: 9258.1s

################################################################################
                     [1m Learning iteration 1950/4000 [0m

                       Computation: 1798 steps/s (collection: 0.539s, learning 4.017s)
               Value function loss: 193.7379
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 737.69
               Mean episode length: 423.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15982592
                    Iteration time: 4.56s
                        Total time: 8806.71s
                               ETA: 9253.6s

################################################################################
                     [1m Learning iteration 1951/4000 [0m

                       Computation: 1818 steps/s (collection: 0.553s, learning 3.953s)
               Value function loss: 239.8534
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 736.28
               Mean episode length: 423.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 4.51s
                        Total time: 8811.22s
                               ETA: 9249.1s

################################################################################
                     [1m Learning iteration 1952/4000 [0m

                       Computation: 1821 steps/s (collection: 0.574s, learning 3.922s)
               Value function loss: 238.3478
                    Surrogate loss: 0.0094
             Mean action noise std: 0.94
                       Mean reward: 740.69
               Mean episode length: 426.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15998976
                    Iteration time: 4.50s
                        Total time: 8815.71s
                               ETA: 9244.5s

################################################################################
                     [1m Learning iteration 1953/4000 [0m

                       Computation: 1826 steps/s (collection: 0.574s, learning 3.910s)
               Value function loss: 270.1761
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 723.70
               Mean episode length: 415.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 4.48s
                        Total time: 8820.20s
                               ETA: 9240.0s

################################################################################
                     [1m Learning iteration 1954/4000 [0m

                       Computation: 1788 steps/s (collection: 0.629s, learning 3.951s)
               Value function loss: 264.8680
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 711.89
               Mean episode length: 410.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16015360
                    Iteration time: 4.58s
                        Total time: 8824.78s
                               ETA: 9235.5s

################################################################################
                     [1m Learning iteration 1955/4000 [0m

                       Computation: 1828 steps/s (collection: 0.550s, learning 3.930s)
               Value function loss: 212.6126
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 720.75
               Mean episode length: 416.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 4.48s
                        Total time: 8829.26s
                               ETA: 9231.0s

################################################################################
                     [1m Learning iteration 1956/4000 [0m

                       Computation: 1826 steps/s (collection: 0.588s, learning 3.898s)
               Value function loss: 179.9027
                    Surrogate loss: 0.0084
             Mean action noise std: 0.94
                       Mean reward: 711.16
               Mean episode length: 411.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16031744
                    Iteration time: 4.49s
                        Total time: 8833.74s
                               ETA: 9226.5s

################################################################################
                     [1m Learning iteration 1957/4000 [0m

                       Computation: 1793 steps/s (collection: 0.639s, learning 3.928s)
               Value function loss: 173.0814
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 723.73
               Mean episode length: 419.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 4.57s
                        Total time: 8838.31s
                               ETA: 9222.0s

################################################################################
                     [1m Learning iteration 1958/4000 [0m

                       Computation: 1786 steps/s (collection: 0.615s, learning 3.970s)
               Value function loss: 235.1492
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 738.05
               Mean episode length: 428.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16048128
                    Iteration time: 4.58s
                        Total time: 8842.89s
                               ETA: 9217.6s

################################################################################
                     [1m Learning iteration 1959/4000 [0m

                       Computation: 1831 steps/s (collection: 0.558s, learning 3.915s)
               Value function loss: 210.7848
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 744.07
               Mean episode length: 432.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 4.47s
                        Total time: 8847.37s
                               ETA: 9213.0s

################################################################################
                     [1m Learning iteration 1960/4000 [0m

                       Computation: 1823 steps/s (collection: 0.563s, learning 3.929s)
               Value function loss: 245.5100
                    Surrogate loss: 0.0110
             Mean action noise std: 0.94
                       Mean reward: 724.07
               Mean episode length: 422.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16064512
                    Iteration time: 4.49s
                        Total time: 8851.86s
                               ETA: 9208.5s

################################################################################
                     [1m Learning iteration 1961/4000 [0m

                       Computation: 1817 steps/s (collection: 0.552s, learning 3.955s)
               Value function loss: 196.7893
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 730.56
               Mean episode length: 426.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 4.51s
                        Total time: 8856.36s
                               ETA: 9203.9s

################################################################################
                     [1m Learning iteration 1962/4000 [0m

                       Computation: 1820 steps/s (collection: 0.552s, learning 3.947s)
               Value function loss: 308.7578
                    Surrogate loss: 0.0111
             Mean action noise std: 0.94
                       Mean reward: 728.86
               Mean episode length: 426.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16080896
                    Iteration time: 4.50s
                        Total time: 8860.86s
                               ETA: 9199.4s

################################################################################
                     [1m Learning iteration 1963/4000 [0m

                       Computation: 1811 steps/s (collection: 0.584s, learning 3.938s)
               Value function loss: 231.9485
                    Surrogate loss: 0.0109
             Mean action noise std: 0.94
                       Mean reward: 736.92
               Mean episode length: 432.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 4.52s
                        Total time: 8865.39s
                               ETA: 9194.9s

################################################################################
                     [1m Learning iteration 1964/4000 [0m

                       Computation: 1833 steps/s (collection: 0.553s, learning 3.914s)
               Value function loss: 130.5880
                    Surrogate loss: 0.0109
             Mean action noise std: 0.94
                       Mean reward: 743.15
               Mean episode length: 435.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 16097280
                    Iteration time: 4.47s
                        Total time: 8869.85s
                               ETA: 9190.3s

################################################################################
                     [1m Learning iteration 1965/4000 [0m

                       Computation: 1822 steps/s (collection: 0.571s, learning 3.925s)
               Value function loss: 184.3414
                    Surrogate loss: 0.0077
             Mean action noise std: 0.94
                       Mean reward: 739.14
               Mean episode length: 433.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 4.50s
                        Total time: 8874.35s
                               ETA: 9185.8s

################################################################################
                     [1m Learning iteration 1966/4000 [0m

                       Computation: 1831 steps/s (collection: 0.559s, learning 3.913s)
               Value function loss: 253.6766
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 737.81
               Mean episode length: 434.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16113664
                    Iteration time: 4.47s
                        Total time: 8878.82s
                               ETA: 9181.3s

################################################################################
                     [1m Learning iteration 1967/4000 [0m

                       Computation: 1817 steps/s (collection: 0.593s, learning 3.915s)
               Value function loss: 221.5212
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 748.97
               Mean episode length: 441.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 4.51s
                        Total time: 8883.33s
                               ETA: 9176.7s

################################################################################
                     [1m Learning iteration 1968/4000 [0m

                       Computation: 1806 steps/s (collection: 0.599s, learning 3.936s)
               Value function loss: 192.6885
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 760.62
               Mean episode length: 446.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16130048
                    Iteration time: 4.54s
                        Total time: 8887.86s
                               ETA: 9172.2s

################################################################################
                     [1m Learning iteration 1969/4000 [0m

                       Computation: 1784 steps/s (collection: 0.634s, learning 3.956s)
               Value function loss: 248.4584
                    Surrogate loss: 0.0094
             Mean action noise std: 0.94
                       Mean reward: 755.79
               Mean episode length: 444.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 4.59s
                        Total time: 8892.45s
                               ETA: 9167.8s

################################################################################
                     [1m Learning iteration 1970/4000 [0m

                       Computation: 1824 steps/s (collection: 0.603s, learning 3.887s)
               Value function loss: 213.3098
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 749.17
               Mean episode length: 439.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16146432
                    Iteration time: 4.49s
                        Total time: 8896.94s
                               ETA: 9163.3s

################################################################################
                     [1m Learning iteration 1971/4000 [0m

                       Computation: 1806 steps/s (collection: 0.579s, learning 3.954s)
               Value function loss: 233.5899
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 765.28
               Mean episode length: 447.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 4.53s
                        Total time: 8901.48s
                               ETA: 9158.8s

################################################################################
                     [1m Learning iteration 1972/4000 [0m

                       Computation: 1813 steps/s (collection: 0.600s, learning 3.918s)
               Value function loss: 244.4165
                    Surrogate loss: 0.0104
             Mean action noise std: 0.94
                       Mean reward: 751.62
               Mean episode length: 438.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16162816
                    Iteration time: 4.52s
                        Total time: 8906.00s
                               ETA: 9154.3s

################################################################################
                     [1m Learning iteration 1973/4000 [0m

                       Computation: 1824 steps/s (collection: 0.582s, learning 3.907s)
               Value function loss: 180.6025
                    Surrogate loss: 0.0096
             Mean action noise std: 0.94
                       Mean reward: 751.84
               Mean episode length: 438.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 4.49s
                        Total time: 8910.49s
                               ETA: 9149.7s

################################################################################
                     [1m Learning iteration 1974/4000 [0m

                       Computation: 1819 steps/s (collection: 0.586s, learning 3.915s)
               Value function loss: 261.2666
                    Surrogate loss: 0.0067
             Mean action noise std: 0.94
                       Mean reward: 752.37
               Mean episode length: 438.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16179200
                    Iteration time: 4.50s
                        Total time: 8914.99s
                               ETA: 9145.2s

################################################################################
                     [1m Learning iteration 1975/4000 [0m

                       Computation: 1839 steps/s (collection: 0.554s, learning 3.899s)
               Value function loss: 179.1534
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 753.29
               Mean episode length: 439.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 4.45s
                        Total time: 8919.44s
                               ETA: 9140.6s

################################################################################
                     [1m Learning iteration 1976/4000 [0m

                       Computation: 1825 steps/s (collection: 0.567s, learning 3.921s)
               Value function loss: 310.0584
                    Surrogate loss: 0.0103
             Mean action noise std: 0.94
                       Mean reward: 770.41
               Mean episode length: 448.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16195584
                    Iteration time: 4.49s
                        Total time: 8923.93s
                               ETA: 9136.1s

################################################################################
                     [1m Learning iteration 1977/4000 [0m

                       Computation: 1827 steps/s (collection: 0.568s, learning 3.916s)
               Value function loss: 180.5816
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 784.83
               Mean episode length: 455.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 4.48s
                        Total time: 8928.41s
                               ETA: 9131.5s

################################################################################
                     [1m Learning iteration 1978/4000 [0m

                       Computation: 1787 steps/s (collection: 0.651s, learning 3.932s)
               Value function loss: 238.3877
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 783.87
               Mean episode length: 455.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16211968
                    Iteration time: 4.58s
                        Total time: 8932.99s
                               ETA: 9127.1s

################################################################################
                     [1m Learning iteration 1979/4000 [0m

                       Computation: 1813 steps/s (collection: 0.585s, learning 3.931s)
               Value function loss: 124.6142
                    Surrogate loss: 0.0106
             Mean action noise std: 0.94
                       Mean reward: 777.07
               Mean episode length: 451.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 4.52s
                        Total time: 8937.51s
                               ETA: 9122.6s

################################################################################
                     [1m Learning iteration 1980/4000 [0m

                       Computation: 1832 steps/s (collection: 0.538s, learning 3.933s)
               Value function loss: 164.1616
                    Surrogate loss: 0.0098
             Mean action noise std: 0.94
                       Mean reward: 771.08
               Mean episode length: 448.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16228352
                    Iteration time: 4.47s
                        Total time: 8941.98s
                               ETA: 9118.0s

################################################################################
                     [1m Learning iteration 1981/4000 [0m

                       Computation: 1809 steps/s (collection: 0.597s, learning 3.929s)
               Value function loss: 268.8192
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 785.02
               Mean episode length: 455.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 4.53s
                        Total time: 8946.51s
                               ETA: 9113.5s

################################################################################
                     [1m Learning iteration 1982/4000 [0m

                       Computation: 1828 steps/s (collection: 0.544s, learning 3.935s)
               Value function loss: 252.9374
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 782.10
               Mean episode length: 455.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16244736
                    Iteration time: 4.48s
                        Total time: 8950.99s
                               ETA: 9109.0s

################################################################################
                     [1m Learning iteration 1983/4000 [0m

                       Computation: 1808 steps/s (collection: 0.600s, learning 3.930s)
               Value function loss: 180.7372
                    Surrogate loss: 0.0096
             Mean action noise std: 0.94
                       Mean reward: 771.08
               Mean episode length: 450.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 4.53s
                        Total time: 8955.52s
                               ETA: 9104.5s

################################################################################
                     [1m Learning iteration 1984/4000 [0m

                       Computation: 1842 steps/s (collection: 0.542s, learning 3.905s)
               Value function loss: 237.1112
                    Surrogate loss: 0.0074
             Mean action noise std: 0.94
                       Mean reward: 774.12
               Mean episode length: 452.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16261120
                    Iteration time: 4.45s
                        Total time: 8959.96s
                               ETA: 9099.9s

################################################################################
                     [1m Learning iteration 1985/4000 [0m

                       Computation: 1839 steps/s (collection: 0.562s, learning 3.891s)
               Value function loss: 327.7134
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 783.03
               Mean episode length: 457.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 4.45s
                        Total time: 8964.41s
                               ETA: 9095.3s

################################################################################
                     [1m Learning iteration 1986/4000 [0m

                       Computation: 1805 steps/s (collection: 0.631s, learning 3.906s)
               Value function loss: 199.3317
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 781.34
               Mean episode length: 456.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16277504
                    Iteration time: 4.54s
                        Total time: 8968.95s
                               ETA: 9090.8s

################################################################################
                     [1m Learning iteration 1987/4000 [0m

                       Computation: 1826 steps/s (collection: 0.580s, learning 3.906s)
               Value function loss: 200.2949
                    Surrogate loss: 0.0114
             Mean action noise std: 0.94
                       Mean reward: 785.30
               Mean episode length: 458.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 4.49s
                        Total time: 8973.44s
                               ETA: 9086.3s

################################################################################
                     [1m Learning iteration 1988/4000 [0m

                       Computation: 1821 steps/s (collection: 0.579s, learning 3.918s)
               Value function loss: 263.5115
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 787.86
               Mean episode length: 457.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16293888
                    Iteration time: 4.50s
                        Total time: 8977.94s
                               ETA: 9081.8s

################################################################################
                     [1m Learning iteration 1989/4000 [0m

                       Computation: 1843 steps/s (collection: 0.551s, learning 3.894s)
               Value function loss: 258.3684
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 788.87
               Mean episode length: 457.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 4.44s
                        Total time: 8982.38s
                               ETA: 9077.2s

################################################################################
                     [1m Learning iteration 1990/4000 [0m

                       Computation: 1815 steps/s (collection: 0.617s, learning 3.897s)
               Value function loss: 235.4549
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 784.92
               Mean episode length: 455.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16310272
                    Iteration time: 4.51s
                        Total time: 8986.89s
                               ETA: 9072.7s

################################################################################
                     [1m Learning iteration 1991/4000 [0m

                       Computation: 1821 steps/s (collection: 0.606s, learning 3.893s)
               Value function loss: 249.4916
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 782.48
               Mean episode length: 453.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 4.50s
                        Total time: 8991.39s
                               ETA: 9068.1s

################################################################################
                     [1m Learning iteration 1992/4000 [0m

                       Computation: 1804 steps/s (collection: 0.578s, learning 3.962s)
               Value function loss: 283.7418
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 781.26
               Mean episode length: 452.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16326656
                    Iteration time: 4.54s
                        Total time: 8995.93s
                               ETA: 9063.6s

################################################################################
                     [1m Learning iteration 1993/4000 [0m

                       Computation: 1814 steps/s (collection: 0.549s, learning 3.965s)
               Value function loss: 176.7064
                    Surrogate loss: 0.0098
             Mean action noise std: 0.94
                       Mean reward: 783.11
               Mean episode length: 452.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 4.51s
                        Total time: 9000.45s
                               ETA: 9059.1s

################################################################################
                     [1m Learning iteration 1994/4000 [0m

                       Computation: 1796 steps/s (collection: 0.617s, learning 3.943s)
               Value function loss: 294.7477
                    Surrogate loss: 0.0075
             Mean action noise std: 0.94
                       Mean reward: 799.13
               Mean episode length: 459.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16343040
                    Iteration time: 4.56s
                        Total time: 9005.01s
                               ETA: 9054.7s

################################################################################
                     [1m Learning iteration 1995/4000 [0m

                       Computation: 1811 steps/s (collection: 0.549s, learning 3.972s)
               Value function loss: 103.0006
                    Surrogate loss: 0.0077
             Mean action noise std: 0.94
                       Mean reward: 799.28
               Mean episode length: 459.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 31.27
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 4.52s
                        Total time: 9009.53s
                               ETA: 9050.2s

################################################################################
                     [1m Learning iteration 1996/4000 [0m

                       Computation: 1775 steps/s (collection: 0.604s, learning 4.011s)
               Value function loss: 185.8407
                    Surrogate loss: 0.0042
             Mean action noise std: 0.94
                       Mean reward: 784.67
               Mean episode length: 451.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16359424
                    Iteration time: 4.62s
                        Total time: 9014.14s
                               ETA: 9045.7s

################################################################################
                     [1m Learning iteration 1997/4000 [0m

                       Computation: 1831 steps/s (collection: 0.533s, learning 3.941s)
               Value function loss: 310.7656
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 793.03
               Mean episode length: 455.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 4.47s
                        Total time: 9018.62s
                               ETA: 9041.2s

################################################################################
                     [1m Learning iteration 1998/4000 [0m

                       Computation: 1822 steps/s (collection: 0.562s, learning 3.932s)
               Value function loss: 203.8206
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 798.64
               Mean episode length: 459.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16375808
                    Iteration time: 4.49s
                        Total time: 9023.11s
                               ETA: 9036.7s

################################################################################
                     [1m Learning iteration 1999/4000 [0m

                       Computation: 1788 steps/s (collection: 0.597s, learning 3.983s)
               Value function loss: 208.4915
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 792.58
               Mean episode length: 456.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 4.58s
                        Total time: 9027.69s
                               ETA: 9032.2s

################################################################################
                     [1m Learning iteration 2000/4000 [0m

                       Computation: 1794 steps/s (collection: 0.573s, learning 3.992s)
               Value function loss: 268.6797
                    Surrogate loss: 0.0117
             Mean action noise std: 0.94
                       Mean reward: 780.44
               Mean episode length: 451.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16392192
                    Iteration time: 4.57s
                        Total time: 9032.26s
                               ETA: 9027.7s

################################################################################
                     [1m Learning iteration 2001/4000 [0m

                       Computation: 1801 steps/s (collection: 0.617s, learning 3.929s)
               Value function loss: 256.1454
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 761.91
               Mean episode length: 442.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 4.55s
                        Total time: 9036.80s
                               ETA: 9023.3s

################################################################################
                     [1m Learning iteration 2002/4000 [0m

                       Computation: 1790 steps/s (collection: 0.614s, learning 3.962s)
               Value function loss: 205.0799
                    Surrogate loss: 0.0094
             Mean action noise std: 0.94
                       Mean reward: 768.93
               Mean episode length: 447.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16408576
                    Iteration time: 4.58s
                        Total time: 9041.38s
                               ETA: 9018.8s

################################################################################
                     [1m Learning iteration 2003/4000 [0m

                       Computation: 1789 steps/s (collection: 0.623s, learning 3.956s)
               Value function loss: 195.3807
                    Surrogate loss: 0.0091
             Mean action noise std: 0.94
                       Mean reward: 777.33
               Mean episode length: 451.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 4.58s
                        Total time: 9045.96s
                               ETA: 9014.4s

################################################################################
                     [1m Learning iteration 2004/4000 [0m

                       Computation: 1800 steps/s (collection: 0.569s, learning 3.982s)
               Value function loss: 152.5190
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 780.33
               Mean episode length: 454.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16424960
                    Iteration time: 4.55s
                        Total time: 9050.51s
                               ETA: 9009.9s

################################################################################
                     [1m Learning iteration 2005/4000 [0m

                       Computation: 1806 steps/s (collection: 0.595s, learning 3.941s)
               Value function loss: 306.5994
                    Surrogate loss: 0.0071
             Mean action noise std: 0.94
                       Mean reward: 759.47
               Mean episode length: 444.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 4.54s
                        Total time: 9055.05s
                               ETA: 9005.4s

################################################################################
                     [1m Learning iteration 2006/4000 [0m

                       Computation: 1845 steps/s (collection: 0.553s, learning 3.887s)
               Value function loss: 163.5290
                    Surrogate loss: 0.0116
             Mean action noise std: 0.94
                       Mean reward: 770.07
               Mean episode length: 451.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16441344
                    Iteration time: 4.44s
                        Total time: 9059.48s
                               ETA: 9000.8s

################################################################################
                     [1m Learning iteration 2007/4000 [0m

                       Computation: 1823 steps/s (collection: 0.588s, learning 3.905s)
               Value function loss: 327.4298
                    Surrogate loss: 0.0084
             Mean action noise std: 0.94
                       Mean reward: 792.02
               Mean episode length: 463.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 4.49s
                        Total time: 9063.98s
                               ETA: 8996.3s

################################################################################
                     [1m Learning iteration 2008/4000 [0m

                       Computation: 1815 steps/s (collection: 0.573s, learning 3.938s)
               Value function loss: 161.0462
                    Surrogate loss: 0.0098
             Mean action noise std: 0.94
                       Mean reward: 767.53
               Mean episode length: 451.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16457728
                    Iteration time: 4.51s
                        Total time: 9068.49s
                               ETA: 8991.8s

################################################################################
                     [1m Learning iteration 2009/4000 [0m

                       Computation: 1804 steps/s (collection: 0.571s, learning 3.970s)
               Value function loss: 244.2305
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 753.79
               Mean episode length: 444.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 4.54s
                        Total time: 9073.03s
                               ETA: 8987.3s

################################################################################
                     [1m Learning iteration 2010/4000 [0m

                       Computation: 1814 steps/s (collection: 0.585s, learning 3.929s)
               Value function loss: 242.7462
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 759.36
               Mean episode length: 446.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16474112
                    Iteration time: 4.51s
                        Total time: 9077.54s
                               ETA: 8982.8s

################################################################################
                     [1m Learning iteration 2011/4000 [0m

                       Computation: 1828 steps/s (collection: 0.552s, learning 3.929s)
               Value function loss: 150.3511
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 754.82
               Mean episode length: 443.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 4.48s
                        Total time: 9082.02s
                               ETA: 8978.2s

################################################################################
                     [1m Learning iteration 2012/4000 [0m

                       Computation: 1847 steps/s (collection: 0.524s, learning 3.911s)
               Value function loss: 214.3244
                    Surrogate loss: 0.0076
             Mean action noise std: 0.94
                       Mean reward: 757.95
               Mean episode length: 444.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16490496
                    Iteration time: 4.43s
                        Total time: 9086.46s
                               ETA: 8973.6s

################################################################################
                     [1m Learning iteration 2013/4000 [0m

                       Computation: 1813 steps/s (collection: 0.554s, learning 3.963s)
               Value function loss: 255.2848
                    Surrogate loss: 0.0113
             Mean action noise std: 0.94
                       Mean reward: 774.83
               Mean episode length: 454.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 4.52s
                        Total time: 9090.98s
                               ETA: 8969.1s

################################################################################
                     [1m Learning iteration 2014/4000 [0m

                       Computation: 1845 steps/s (collection: 0.554s, learning 3.885s)
               Value function loss: 154.0342
                    Surrogate loss: 0.0125
             Mean action noise std: 0.94
                       Mean reward: 767.83
               Mean episode length: 451.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16506880
                    Iteration time: 4.44s
                        Total time: 9095.41s
                               ETA: 8964.5s

################################################################################
                     [1m Learning iteration 2015/4000 [0m

                       Computation: 1835 steps/s (collection: 0.539s, learning 3.924s)
               Value function loss: 241.9979
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 749.67
               Mean episode length: 439.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 4.46s
                        Total time: 9099.88s
                               ETA: 8959.9s

################################################################################
                     [1m Learning iteration 2016/4000 [0m

                       Computation: 1853 steps/s (collection: 0.540s, learning 3.879s)
               Value function loss: 244.6540
                    Surrogate loss: 0.0114
             Mean action noise std: 0.94
                       Mean reward: 741.58
               Mean episode length: 437.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16523264
                    Iteration time: 4.42s
                        Total time: 9104.30s
                               ETA: 8955.3s

################################################################################
                     [1m Learning iteration 2017/4000 [0m

                       Computation: 1832 steps/s (collection: 0.578s, learning 3.893s)
               Value function loss: 180.6909
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 733.04
               Mean episode length: 432.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 4.47s
                        Total time: 9108.77s
                               ETA: 8950.8s

################################################################################
                     [1m Learning iteration 2018/4000 [0m

                       Computation: 1833 steps/s (collection: 0.555s, learning 3.914s)
               Value function loss: 230.7866
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 731.60
               Mean episode length: 432.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16539648
                    Iteration time: 4.47s
                        Total time: 9113.24s
                               ETA: 8946.2s

################################################################################
                     [1m Learning iteration 2019/4000 [0m

                       Computation: 1806 steps/s (collection: 0.593s, learning 3.941s)
               Value function loss: 217.5470
                    Surrogate loss: 0.0104
             Mean action noise std: 0.94
                       Mean reward: 748.28
               Mean episode length: 440.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 4.53s
                        Total time: 9117.77s
                               ETA: 8941.7s

################################################################################
                     [1m Learning iteration 2020/4000 [0m

                       Computation: 1834 steps/s (collection: 0.559s, learning 3.907s)
               Value function loss: 144.8474
                    Surrogate loss: 0.0104
             Mean action noise std: 0.94
                       Mean reward: 757.57
               Mean episode length: 445.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 16556032
                    Iteration time: 4.47s
                        Total time: 9122.24s
                               ETA: 8937.2s

################################################################################
                     [1m Learning iteration 2021/4000 [0m

                       Computation: 1830 steps/s (collection: 0.567s, learning 3.908s)
               Value function loss: 255.7471
                    Surrogate loss: 0.0068
             Mean action noise std: 0.94
                       Mean reward: 752.56
               Mean episode length: 442.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 4.47s
                        Total time: 9126.71s
                               ETA: 8932.6s

################################################################################
                     [1m Learning iteration 2022/4000 [0m

                       Computation: 1845 steps/s (collection: 0.541s, learning 3.898s)
               Value function loss: 213.2731
                    Surrogate loss: 0.0072
             Mean action noise std: 0.94
                       Mean reward: 749.73
               Mean episode length: 441.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16572416
                    Iteration time: 4.44s
                        Total time: 9131.15s
                               ETA: 8928.0s

################################################################################
                     [1m Learning iteration 2023/4000 [0m

                       Computation: 1837 steps/s (collection: 0.592s, learning 3.865s)
               Value function loss: 301.7848
                    Surrogate loss: 0.0074
             Mean action noise std: 0.94
                       Mean reward: 748.63
               Mean episode length: 438.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 4.46s
                        Total time: 9135.61s
                               ETA: 8923.5s

################################################################################
                     [1m Learning iteration 2024/4000 [0m

                       Computation: 1842 steps/s (collection: 0.560s, learning 3.887s)
               Value function loss: 180.4640
                    Surrogate loss: 0.0081
             Mean action noise std: 0.94
                       Mean reward: 744.65
               Mean episode length: 435.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16588800
                    Iteration time: 4.45s
                        Total time: 9140.05s
                               ETA: 8918.9s

################################################################################
                     [1m Learning iteration 2025/4000 [0m

                       Computation: 1780 steps/s (collection: 0.601s, learning 3.999s)
               Value function loss: 289.0274
                    Surrogate loss: 0.0076
             Mean action noise std: 0.94
                       Mean reward: 754.65
               Mean episode length: 440.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 4.60s
                        Total time: 9144.65s
                               ETA: 8914.5s

################################################################################
                     [1m Learning iteration 2026/4000 [0m

                       Computation: 1799 steps/s (collection: 0.551s, learning 4.002s)
               Value function loss: 157.7887
                    Surrogate loss: 0.0084
             Mean action noise std: 0.94
                       Mean reward: 737.06
               Mean episode length: 428.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16605184
                    Iteration time: 4.55s
                        Total time: 9149.21s
                               ETA: 8910.0s

################################################################################
                     [1m Learning iteration 2027/4000 [0m

                       Computation: 1805 steps/s (collection: 0.523s, learning 4.013s)
               Value function loss: 149.7530
                    Surrogate loss: 0.0106
             Mean action noise std: 0.94
                       Mean reward: 740.08
               Mean episode length: 427.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 4.54s
                        Total time: 9153.74s
                               ETA: 8905.5s

################################################################################
                     [1m Learning iteration 2028/4000 [0m

                       Computation: 1814 steps/s (collection: 0.544s, learning 3.971s)
               Value function loss: 212.3375
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 736.14
               Mean episode length: 425.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16621568
                    Iteration time: 4.51s
                        Total time: 9158.26s
                               ETA: 8901.0s

################################################################################
                     [1m Learning iteration 2029/4000 [0m

                       Computation: 1833 steps/s (collection: 0.519s, learning 3.949s)
               Value function loss: 212.5538
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 739.45
               Mean episode length: 426.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 4.47s
                        Total time: 9162.73s
                               ETA: 8896.4s

################################################################################
                     [1m Learning iteration 2030/4000 [0m

                       Computation: 1804 steps/s (collection: 0.556s, learning 3.983s)
               Value function loss: 168.9658
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 719.76
               Mean episode length: 414.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16637952
                    Iteration time: 4.54s
                        Total time: 9167.27s
                               ETA: 8891.9s

################################################################################
                     [1m Learning iteration 2031/4000 [0m

                       Computation: 1815 steps/s (collection: 0.533s, learning 3.978s)
               Value function loss: 324.3322
                    Surrogate loss: 0.0104
             Mean action noise std: 0.94
                       Mean reward: 729.48
               Mean episode length: 417.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 4.51s
                        Total time: 9171.78s
                               ETA: 8887.4s

################################################################################
                     [1m Learning iteration 2032/4000 [0m

                       Computation: 1829 steps/s (collection: 0.564s, learning 3.915s)
               Value function loss: 307.6083
                    Surrogate loss: 0.0077
             Mean action noise std: 0.94
                       Mean reward: 727.55
               Mean episode length: 415.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16654336
                    Iteration time: 4.48s
                        Total time: 9176.26s
                               ETA: 8882.9s

################################################################################
                     [1m Learning iteration 2033/4000 [0m

                       Computation: 1828 steps/s (collection: 0.539s, learning 3.942s)
               Value function loss: 207.2382
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 718.23
               Mean episode length: 412.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 4.48s
                        Total time: 9180.74s
                               ETA: 8878.3s

################################################################################
                     [1m Learning iteration 2034/4000 [0m

                       Computation: 1816 steps/s (collection: 0.636s, learning 3.874s)
               Value function loss: 141.4384
                    Surrogate loss: 0.0084
             Mean action noise std: 0.94
                       Mean reward: 710.83
               Mean episode length: 407.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16670720
                    Iteration time: 4.51s
                        Total time: 9185.25s
                               ETA: 8873.8s

################################################################################
                     [1m Learning iteration 2035/4000 [0m

                       Computation: 1823 steps/s (collection: 0.587s, learning 3.906s)
               Value function loss: 164.8002
                    Surrogate loss: 0.0074
             Mean action noise std: 0.94
                       Mean reward: 705.09
               Mean episode length: 404.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 4.49s
                        Total time: 9189.74s
                               ETA: 8869.3s

################################################################################
                     [1m Learning iteration 2036/4000 [0m

                       Computation: 1810 steps/s (collection: 0.584s, learning 3.940s)
               Value function loss: 216.1951
                    Surrogate loss: 0.0106
             Mean action noise std: 0.94
                       Mean reward: 706.34
               Mean episode length: 406.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16687104
                    Iteration time: 4.52s
                        Total time: 9194.26s
                               ETA: 8864.8s

################################################################################
                     [1m Learning iteration 2037/4000 [0m

                       Computation: 1845 steps/s (collection: 0.546s, learning 3.892s)
               Value function loss: 206.9596
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 702.74
               Mean episode length: 407.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 4.44s
                        Total time: 9198.70s
                               ETA: 8860.2s

################################################################################
                     [1m Learning iteration 2038/4000 [0m

                       Computation: 1788 steps/s (collection: 0.641s, learning 3.939s)
               Value function loss: 250.4354
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 690.30
               Mean episode length: 401.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16703488
                    Iteration time: 4.58s
                        Total time: 9203.28s
                               ETA: 8855.7s

################################################################################
                     [1m Learning iteration 2039/4000 [0m

                       Computation: 1828 steps/s (collection: 0.571s, learning 3.910s)
               Value function loss: 295.5952
                    Surrogate loss: 0.0104
             Mean action noise std: 0.94
                       Mean reward: 707.44
               Mean episode length: 410.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 4.48s
                        Total time: 9207.76s
                               ETA: 8851.2s

################################################################################
                     [1m Learning iteration 2040/4000 [0m

                       Computation: 1821 steps/s (collection: 0.577s, learning 3.920s)
               Value function loss: 202.4191
                    Surrogate loss: 0.0101
             Mean action noise std: 0.94
                       Mean reward: 704.86
               Mean episode length: 410.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16719872
                    Iteration time: 4.50s
                        Total time: 9212.26s
                               ETA: 8846.7s

################################################################################
                     [1m Learning iteration 2041/4000 [0m

                       Computation: 1815 steps/s (collection: 0.576s, learning 3.936s)
               Value function loss: 247.4092
                    Surrogate loss: 0.0091
             Mean action noise std: 0.94
                       Mean reward: 701.41
               Mean episode length: 409.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 4.51s
                        Total time: 9216.77s
                               ETA: 8842.1s

################################################################################
                     [1m Learning iteration 2042/4000 [0m

                       Computation: 1858 steps/s (collection: 0.510s, learning 3.898s)
               Value function loss: 111.6534
                    Surrogate loss: 0.0111
             Mean action noise std: 0.94
                       Mean reward: 684.50
               Mean episode length: 398.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16736256
                    Iteration time: 4.41s
                        Total time: 9221.18s
                               ETA: 8837.5s

################################################################################
                     [1m Learning iteration 2043/4000 [0m

                       Computation: 1818 steps/s (collection: 0.575s, learning 3.930s)
               Value function loss: 194.2222
                    Surrogate loss: 0.0123
             Mean action noise std: 0.94
                       Mean reward: 695.25
               Mean episode length: 404.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 4.50s
                        Total time: 9225.68s
                               ETA: 8833.0s

################################################################################
                     [1m Learning iteration 2044/4000 [0m

                       Computation: 1826 steps/s (collection: 0.591s, learning 3.895s)
               Value function loss: 228.0079
                    Surrogate loss: 0.0114
             Mean action noise std: 0.94
                       Mean reward: 695.00
               Mean episode length: 404.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16752640
                    Iteration time: 4.49s
                        Total time: 9230.17s
                               ETA: 8828.5s

################################################################################
                     [1m Learning iteration 2045/4000 [0m

                       Computation: 1838 steps/s (collection: 0.553s, learning 3.902s)
               Value function loss: 253.4538
                    Surrogate loss: 0.0098
             Mean action noise std: 0.94
                       Mean reward: 708.52
               Mean episode length: 411.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 4.45s
                        Total time: 9234.62s
                               ETA: 8823.9s

################################################################################
                     [1m Learning iteration 2046/4000 [0m

                       Computation: 1815 steps/s (collection: 0.562s, learning 3.949s)
               Value function loss: 296.4695
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 705.29
               Mean episode length: 408.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16769024
                    Iteration time: 4.51s
                        Total time: 9239.14s
                               ETA: 8819.4s

################################################################################
                     [1m Learning iteration 2047/4000 [0m

                       Computation: 1806 steps/s (collection: 0.613s, learning 3.922s)
               Value function loss: 318.6780
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 716.22
               Mean episode length: 412.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 4.53s
                        Total time: 9243.67s
                               ETA: 8814.9s

################################################################################
                     [1m Learning iteration 2048/4000 [0m

                       Computation: 1827 steps/s (collection: 0.571s, learning 3.910s)
               Value function loss: 246.2837
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 718.39
               Mean episode length: 412.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16785408
                    Iteration time: 4.48s
                        Total time: 9248.15s
                               ETA: 8810.3s

################################################################################
                     [1m Learning iteration 2049/4000 [0m

                       Computation: 1762 steps/s (collection: 0.706s, learning 3.941s)
               Value function loss: 272.0028
                    Surrogate loss: 0.0096
             Mean action noise std: 0.94
                       Mean reward: 692.24
               Mean episode length: 399.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 4.65s
                        Total time: 9252.80s
                               ETA: 8806.0s

################################################################################
                     [1m Learning iteration 2050/4000 [0m

                       Computation: 1817 steps/s (collection: 0.583s, learning 3.923s)
               Value function loss: 206.7277
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 663.24
               Mean episode length: 381.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16801792
                    Iteration time: 4.51s
                        Total time: 9257.31s
                               ETA: 8801.4s

################################################################################
                     [1m Learning iteration 2051/4000 [0m

                       Computation: 1814 steps/s (collection: 0.612s, learning 3.902s)
               Value function loss: 142.9094
                    Surrogate loss: 0.0106
             Mean action noise std: 0.94
                       Mean reward: 673.91
               Mean episode length: 388.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 4.51s
                        Total time: 9261.82s
                               ETA: 8796.9s

################################################################################
                     [1m Learning iteration 2052/4000 [0m

                       Computation: 1806 steps/s (collection: 0.594s, learning 3.941s)
               Value function loss: 254.9584
                    Surrogate loss: 0.0103
             Mean action noise std: 0.94
                       Mean reward: 650.61
               Mean episode length: 376.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 16818176
                    Iteration time: 4.54s
                        Total time: 9266.36s
                               ETA: 8792.4s

################################################################################
                     [1m Learning iteration 2053/4000 [0m

                       Computation: 1814 steps/s (collection: 0.574s, learning 3.940s)
               Value function loss: 194.3887
                    Surrogate loss: 0.0084
             Mean action noise std: 0.94
                       Mean reward: 658.73
               Mean episode length: 381.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 4.51s
                        Total time: 9270.87s
                               ETA: 8787.9s

################################################################################
                     [1m Learning iteration 2054/4000 [0m

                       Computation: 1819 steps/s (collection: 0.553s, learning 3.951s)
               Value function loss: 307.4274
                    Surrogate loss: 0.0113
             Mean action noise std: 0.94
                       Mean reward: 667.02
               Mean episode length: 386.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16834560
                    Iteration time: 4.50s
                        Total time: 9275.37s
                               ETA: 8783.4s

################################################################################
                     [1m Learning iteration 2055/4000 [0m

                       Computation: 1799 steps/s (collection: 0.598s, learning 3.955s)
               Value function loss: 164.2716
                    Surrogate loss: 0.0116
             Mean action noise std: 0.94
                       Mean reward: 678.59
               Mean episode length: 393.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 4.55s
                        Total time: 9279.93s
                               ETA: 8778.9s

################################################################################
                     [1m Learning iteration 2056/4000 [0m

                       Computation: 1822 steps/s (collection: 0.582s, learning 3.913s)
               Value function loss: 217.4687
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 661.73
               Mean episode length: 384.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16850944
                    Iteration time: 4.49s
                        Total time: 9284.42s
                               ETA: 8774.4s

################################################################################
                     [1m Learning iteration 2057/4000 [0m

                       Computation: 1788 steps/s (collection: 0.649s, learning 3.932s)
               Value function loss: 231.4293
                    Surrogate loss: 0.0108
             Mean action noise std: 0.94
                       Mean reward: 652.50
               Mean episode length: 379.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 4.58s
                        Total time: 9289.00s
                               ETA: 8769.9s

################################################################################
                     [1m Learning iteration 2058/4000 [0m

                       Computation: 1845 steps/s (collection: 0.547s, learning 3.893s)
               Value function loss: 113.4226
                    Surrogate loss: 0.0140
             Mean action noise std: 0.94
                       Mean reward: 660.93
               Mean episode length: 384.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 16867328
                    Iteration time: 4.44s
                        Total time: 9293.44s
                               ETA: 8765.4s

################################################################################
                     [1m Learning iteration 2059/4000 [0m

                       Computation: 1808 steps/s (collection: 0.606s, learning 3.925s)
               Value function loss: 195.7383
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 654.86
               Mean episode length: 382.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 4.53s
                        Total time: 9297.97s
                               ETA: 8760.9s

################################################################################
                     [1m Learning iteration 2060/4000 [0m

                       Computation: 1830 steps/s (collection: 0.556s, learning 3.919s)
               Value function loss: 201.7167
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 646.29
               Mean episode length: 378.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16883712
                    Iteration time: 4.48s
                        Total time: 9302.45s
                               ETA: 8756.3s

################################################################################
                     [1m Learning iteration 2061/4000 [0m

                       Computation: 1802 steps/s (collection: 0.592s, learning 3.952s)
               Value function loss: 213.6068
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 649.88
               Mean episode length: 380.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 4.54s
                        Total time: 9306.99s
                               ETA: 8751.8s

################################################################################
                     [1m Learning iteration 2062/4000 [0m

                       Computation: 1817 steps/s (collection: 0.598s, learning 3.910s)
               Value function loss: 283.9448
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 668.25
               Mean episode length: 389.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16900096
                    Iteration time: 4.51s
                        Total time: 9311.50s
                               ETA: 8747.3s

################################################################################
                     [1m Learning iteration 2063/4000 [0m

                       Computation: 1801 steps/s (collection: 0.602s, learning 3.945s)
               Value function loss: 305.9586
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 658.07
               Mean episode length: 384.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 4.55s
                        Total time: 9316.05s
                               ETA: 8742.8s

################################################################################
                     [1m Learning iteration 2064/4000 [0m

                       Computation: 1808 steps/s (collection: 0.621s, learning 3.908s)
               Value function loss: 228.6569
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 640.69
               Mean episode length: 374.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16916480
                    Iteration time: 4.53s
                        Total time: 9320.58s
                               ETA: 8738.3s

################################################################################
                     [1m Learning iteration 2065/4000 [0m

                       Computation: 1844 steps/s (collection: 0.579s, learning 3.863s)
               Value function loss: 246.2968
                    Surrogate loss: 0.0104
             Mean action noise std: 0.94
                       Mean reward: 646.58
               Mean episode length: 378.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 4.44s
                        Total time: 9325.02s
                               ETA: 8733.7s

################################################################################
                     [1m Learning iteration 2066/4000 [0m

                       Computation: 1813 steps/s (collection: 0.601s, learning 3.916s)
               Value function loss: 253.8576
                    Surrogate loss: 0.0091
             Mean action noise std: 0.94
                       Mean reward: 660.47
               Mean episode length: 385.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16932864
                    Iteration time: 4.52s
                        Total time: 9329.53s
                               ETA: 8729.2s

################################################################################
                     [1m Learning iteration 2067/4000 [0m

                       Computation: 1846 steps/s (collection: 0.565s, learning 3.871s)
               Value function loss: 191.6089
                    Surrogate loss: 0.0073
             Mean action noise std: 0.94
                       Mean reward: 669.08
               Mean episode length: 391.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 4.44s
                        Total time: 9333.97s
                               ETA: 8724.6s

################################################################################
                     [1m Learning iteration 2068/4000 [0m

                       Computation: 1792 steps/s (collection: 0.607s, learning 3.964s)
               Value function loss: 205.7055
                    Surrogate loss: 0.0064
             Mean action noise std: 0.94
                       Mean reward: 659.06
               Mean episode length: 384.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 16949248
                    Iteration time: 4.57s
                        Total time: 9338.54s
                               ETA: 8720.2s

################################################################################
                     [1m Learning iteration 2069/4000 [0m

                       Computation: 1818 steps/s (collection: 0.580s, learning 3.925s)
               Value function loss: 296.1527
                    Surrogate loss: 0.0108
             Mean action noise std: 0.94
                       Mean reward: 683.75
               Mean episode length: 399.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 4.50s
                        Total time: 9343.05s
                               ETA: 8715.7s

################################################################################
                     [1m Learning iteration 2070/4000 [0m

                       Computation: 1809 steps/s (collection: 0.580s, learning 3.949s)
               Value function loss: 218.7723
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 660.90
               Mean episode length: 386.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16965632
                    Iteration time: 4.53s
                        Total time: 9347.58s
                               ETA: 8711.2s

################################################################################
                     [1m Learning iteration 2071/4000 [0m

                       Computation: 1808 steps/s (collection: 0.573s, learning 3.956s)
               Value function loss: 187.7572
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 668.34
               Mean episode length: 390.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 4.53s
                        Total time: 9352.10s
                               ETA: 8706.7s

################################################################################
                     [1m Learning iteration 2072/4000 [0m

                       Computation: 1832 steps/s (collection: 0.528s, learning 3.942s)
               Value function loss: 268.3064
                    Surrogate loss: 0.0080
             Mean action noise std: 0.94
                       Mean reward: 651.98
               Mean episode length: 381.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16982016
                    Iteration time: 4.47s
                        Total time: 9356.57s
                               ETA: 8702.1s

################################################################################
                     [1m Learning iteration 2073/4000 [0m

                       Computation: 1821 steps/s (collection: 0.530s, learning 3.968s)
               Value function loss: 165.1976
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 667.39
               Mean episode length: 389.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 4.50s
                        Total time: 9361.07s
                               ETA: 8697.6s

################################################################################
                     [1m Learning iteration 2074/4000 [0m

                       Computation: 1817 steps/s (collection: 0.551s, learning 3.956s)
               Value function loss: 167.8361
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 676.58
               Mean episode length: 394.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 16998400
                    Iteration time: 4.51s
                        Total time: 9365.58s
                               ETA: 8693.1s

################################################################################
                     [1m Learning iteration 2075/4000 [0m

                       Computation: 1841 steps/s (collection: 0.530s, learning 3.918s)
               Value function loss: 206.8706
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 671.14
               Mean episode length: 390.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 4.45s
                        Total time: 9370.03s
                               ETA: 8688.5s

################################################################################
                     [1m Learning iteration 2076/4000 [0m

                       Computation: 1833 steps/s (collection: 0.536s, learning 3.933s)
               Value function loss: 171.8658
                    Surrogate loss: 0.0073
             Mean action noise std: 0.94
                       Mean reward: 681.33
               Mean episode length: 395.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17014784
                    Iteration time: 4.47s
                        Total time: 9374.50s
                               ETA: 8683.9s

################################################################################
                     [1m Learning iteration 2077/4000 [0m

                       Computation: 1808 steps/s (collection: 0.576s, learning 3.955s)
               Value function loss: 251.6043
                    Surrogate loss: 0.0126
             Mean action noise std: 0.94
                       Mean reward: 683.02
               Mean episode length: 396.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 4.53s
                        Total time: 9379.03s
                               ETA: 8679.4s

################################################################################
                     [1m Learning iteration 2078/4000 [0m

                       Computation: 1812 steps/s (collection: 0.582s, learning 3.939s)
               Value function loss: 303.4605
                    Surrogate loss: 0.0120
             Mean action noise std: 0.94
                       Mean reward: 698.29
               Mean episode length: 405.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 17031168
                    Iteration time: 4.52s
                        Total time: 9383.55s
                               ETA: 8674.9s

################################################################################
                     [1m Learning iteration 2079/4000 [0m

                       Computation: 1815 steps/s (collection: 0.577s, learning 3.936s)
               Value function loss: 228.2046
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 707.99
               Mean episode length: 411.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 4.51s
                        Total time: 9388.06s
                               ETA: 8670.4s

################################################################################
                     [1m Learning iteration 2080/4000 [0m

                       Computation: 1806 steps/s (collection: 0.543s, learning 3.992s)
               Value function loss: 256.6500
                    Surrogate loss: 0.0079
             Mean action noise std: 0.94
                       Mean reward: 691.59
               Mean episode length: 405.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 17047552
                    Iteration time: 4.54s
                        Total time: 9392.60s
                               ETA: 8665.9s

################################################################################
                     [1m Learning iteration 2081/4000 [0m

                       Computation: 1800 steps/s (collection: 0.589s, learning 3.960s)
               Value function loss: 206.5671
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 690.19
               Mean episode length: 405.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 4.55s
                        Total time: 9397.15s
                               ETA: 8661.4s

################################################################################
                     [1m Learning iteration 2082/4000 [0m

                       Computation: 1812 steps/s (collection: 0.547s, learning 3.973s)
               Value function loss: 131.4996
                    Surrogate loss: 0.0074
             Mean action noise std: 0.94
                       Mean reward: 693.54
               Mean episode length: 408.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17063936
                    Iteration time: 4.52s
                        Total time: 9401.67s
                               ETA: 8656.9s

################################################################################
                     [1m Learning iteration 2083/4000 [0m

                       Computation: 1861 steps/s (collection: 0.535s, learning 3.867s)
               Value function loss: 277.0246
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 683.35
               Mean episode length: 405.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 4.40s
                        Total time: 9406.07s
                               ETA: 8652.3s

################################################################################
                     [1m Learning iteration 2084/4000 [0m

                       Computation: 1790 steps/s (collection: 0.621s, learning 3.954s)
               Value function loss: 197.4059
                    Surrogate loss: 0.0091
             Mean action noise std: 0.94
                       Mean reward: 673.17
               Mean episode length: 400.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17080320
                    Iteration time: 4.58s
                        Total time: 9410.64s
                               ETA: 8647.9s

################################################################################
                     [1m Learning iteration 2085/4000 [0m

                       Computation: 1824 steps/s (collection: 0.580s, learning 3.911s)
               Value function loss: 179.2927
                    Surrogate loss: 0.0077
             Mean action noise std: 0.94
                       Mean reward: 673.61
               Mean episode length: 401.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 4.49s
                        Total time: 9415.13s
                               ETA: 8643.3s

################################################################################
                     [1m Learning iteration 2086/4000 [0m

                       Computation: 1840 steps/s (collection: 0.555s, learning 3.895s)
               Value function loss: 192.3661
                    Surrogate loss: 0.0105
             Mean action noise std: 0.94
                       Mean reward: 675.68
               Mean episode length: 404.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17096704
                    Iteration time: 4.45s
                        Total time: 9419.59s
                               ETA: 8638.8s

################################################################################
                     [1m Learning iteration 2087/4000 [0m

                       Computation: 1811 steps/s (collection: 0.591s, learning 3.931s)
               Value function loss: 202.0548
                    Surrogate loss: 0.0102
             Mean action noise std: 0.94
                       Mean reward: 681.20
               Mean episode length: 404.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 4.52s
                        Total time: 9424.11s
                               ETA: 8634.3s

################################################################################
                     [1m Learning iteration 2088/4000 [0m

                       Computation: 1830 steps/s (collection: 0.572s, learning 3.903s)
               Value function loss: 230.5166
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 671.94
               Mean episode length: 400.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17113088
                    Iteration time: 4.48s
                        Total time: 9428.58s
                               ETA: 8629.7s

################################################################################
                     [1m Learning iteration 2089/4000 [0m

                       Computation: 1813 steps/s (collection: 0.567s, learning 3.949s)
               Value function loss: 94.6462
                    Surrogate loss: 0.0119
             Mean action noise std: 0.94
                       Mean reward: 665.96
               Mean episode length: 395.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 4.52s
                        Total time: 9433.10s
                               ETA: 8625.2s

################################################################################
                     [1m Learning iteration 2090/4000 [0m

                       Computation: 1819 steps/s (collection: 0.555s, learning 3.947s)
               Value function loss: 195.3444
                    Surrogate loss: 0.0103
             Mean action noise std: 0.94
                       Mean reward: 659.14
               Mean episode length: 391.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 17129472
                    Iteration time: 4.50s
                        Total time: 9437.60s
                               ETA: 8620.7s

################################################################################
                     [1m Learning iteration 2091/4000 [0m

                       Computation: 1809 steps/s (collection: 0.601s, learning 3.926s)
               Value function loss: 156.0083
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 653.81
               Mean episode length: 388.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 4.53s
                        Total time: 9442.13s
                               ETA: 8616.2s

################################################################################
                     [1m Learning iteration 2092/4000 [0m

                       Computation: 1845 steps/s (collection: 0.523s, learning 3.915s)
               Value function loss: 155.9831
                    Surrogate loss: 0.0075
             Mean action noise std: 0.94
                       Mean reward: 654.09
               Mean episode length: 389.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17145856
                    Iteration time: 4.44s
                        Total time: 9446.57s
                               ETA: 8611.6s

################################################################################
                     [1m Learning iteration 2093/4000 [0m

                       Computation: 1825 steps/s (collection: 0.527s, learning 3.961s)
               Value function loss: 333.5833
                    Surrogate loss: 0.0081
             Mean action noise std: 0.94
                       Mean reward: 644.03
               Mean episode length: 382.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 4.49s
                        Total time: 9451.05s
                               ETA: 8607.0s

################################################################################
                     [1m Learning iteration 2094/4000 [0m

                       Computation: 1824 steps/s (collection: 0.566s, learning 3.924s)
               Value function loss: 217.8210
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 654.93
               Mean episode length: 387.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17162240
                    Iteration time: 4.49s
                        Total time: 9455.54s
                               ETA: 8602.5s

################################################################################
                     [1m Learning iteration 2095/4000 [0m

                       Computation: 1855 steps/s (collection: 0.533s, learning 3.883s)
               Value function loss: 203.2702
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 634.22
               Mean episode length: 376.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 4.42s
                        Total time: 9459.96s
                               ETA: 8597.9s

################################################################################
                     [1m Learning iteration 2096/4000 [0m

                       Computation: 1849 steps/s (collection: 0.542s, learning 3.887s)
               Value function loss: 199.2415
                    Surrogate loss: 0.0080
             Mean action noise std: 0.94
                       Mean reward: 608.91
               Mean episode length: 364.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 17178624
                    Iteration time: 4.43s
                        Total time: 9464.39s
                               ETA: 8593.3s

################################################################################
                     [1m Learning iteration 2097/4000 [0m

                       Computation: 1858 steps/s (collection: 0.546s, learning 3.862s)
               Value function loss: 242.6010
                    Surrogate loss: 0.0094
             Mean action noise std: 0.94
                       Mean reward: 624.66
               Mean episode length: 373.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 4.41s
                        Total time: 9468.80s
                               ETA: 8588.7s

################################################################################
                     [1m Learning iteration 2098/4000 [0m

                       Computation: 1872 steps/s (collection: 0.520s, learning 3.855s)
               Value function loss: 117.3521
                    Surrogate loss: 0.0084
             Mean action noise std: 0.94
                       Mean reward: 634.24
               Mean episode length: 379.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 31.39
--------------------------------------------------------------------------------
                   Total timesteps: 17195008
                    Iteration time: 4.37s
                        Total time: 9473.17s
                               ETA: 8584.1s

################################################################################
                     [1m Learning iteration 2099/4000 [0m

                       Computation: 1831 steps/s (collection: 0.547s, learning 3.926s)
               Value function loss: 267.9557
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 667.73
               Mean episode length: 398.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 4.47s
                        Total time: 9477.64s
                               ETA: 8579.5s

################################################################################
                     [1m Learning iteration 2100/4000 [0m

                       Computation: 1854 steps/s (collection: 0.553s, learning 3.865s)
               Value function loss: 202.9912
                    Surrogate loss: 0.0114
             Mean action noise std: 0.94
                       Mean reward: 672.96
               Mean episode length: 402.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17211392
                    Iteration time: 4.42s
                        Total time: 9482.06s
                               ETA: 8574.9s

################################################################################
                     [1m Learning iteration 2101/4000 [0m

                       Computation: 1860 steps/s (collection: 0.502s, learning 3.902s)
               Value function loss: 247.1353
                    Surrogate loss: 0.0119
             Mean action noise std: 0.94
                       Mean reward: 678.48
               Mean episode length: 402.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 4.40s
                        Total time: 9486.47s
                               ETA: 8570.3s

################################################################################
                     [1m Learning iteration 2102/4000 [0m

                       Computation: 1876 steps/s (collection: 0.529s, learning 3.837s)
               Value function loss: 105.1296
                    Surrogate loss: 0.0119
             Mean action noise std: 0.94
                       Mean reward: 682.62
               Mean episode length: 405.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 17227776
                    Iteration time: 4.37s
                        Total time: 9490.83s
                               ETA: 8565.7s

################################################################################
                     [1m Learning iteration 2103/4000 [0m

                       Computation: 1861 steps/s (collection: 0.548s, learning 3.853s)
               Value function loss: 226.3413
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 675.36
               Mean episode length: 402.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 4.40s
                        Total time: 9495.23s
                               ETA: 8561.1s

################################################################################
                     [1m Learning iteration 2104/4000 [0m

                       Computation: 1861 steps/s (collection: 0.502s, learning 3.898s)
               Value function loss: 215.4865
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 689.58
               Mean episode length: 410.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17244160
                    Iteration time: 4.40s
                        Total time: 9499.63s
                               ETA: 8556.4s

################################################################################
                     [1m Learning iteration 2105/4000 [0m

                       Computation: 1853 steps/s (collection: 0.498s, learning 3.921s)
               Value function loss: 157.0615
                    Surrogate loss: 0.0094
             Mean action noise std: 0.95
                       Mean reward: 692.01
               Mean episode length: 413.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 4.42s
                        Total time: 9504.05s
                               ETA: 8551.8s

################################################################################
                     [1m Learning iteration 2106/4000 [0m

                       Computation: 1845 steps/s (collection: 0.492s, learning 3.946s)
               Value function loss: 192.6863
                    Surrogate loss: 0.0099
             Mean action noise std: 0.95
                       Mean reward: 710.02
               Mean episode length: 422.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 17260544
                    Iteration time: 4.44s
                        Total time: 9508.49s
                               ETA: 8547.3s

################################################################################
                     [1m Learning iteration 2107/4000 [0m

                       Computation: 1826 steps/s (collection: 0.562s, learning 3.922s)
               Value function loss: 238.7727
                    Surrogate loss: 0.0103
             Mean action noise std: 0.95
                       Mean reward: 731.85
               Mean episode length: 434.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 4.48s
                        Total time: 9512.98s
                               ETA: 8542.7s

################################################################################
                     [1m Learning iteration 2108/4000 [0m

                       Computation: 1822 steps/s (collection: 0.545s, learning 3.949s)
               Value function loss: 278.3662
                    Surrogate loss: 0.0115
             Mean action noise std: 0.95
                       Mean reward: 739.36
               Mean episode length: 438.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17276928
                    Iteration time: 4.49s
                        Total time: 9517.47s
                               ETA: 8538.2s

################################################################################
                     [1m Learning iteration 2109/4000 [0m

                       Computation: 1800 steps/s (collection: 0.587s, learning 3.964s)
               Value function loss: 394.4706
                    Surrogate loss: 0.0103
             Mean action noise std: 0.95
                       Mean reward: 720.84
               Mean episode length: 425.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 4.55s
                        Total time: 9522.02s
                               ETA: 8533.7s

################################################################################
                     [1m Learning iteration 2110/4000 [0m

                       Computation: 1839 steps/s (collection: 0.518s, learning 3.935s)
               Value function loss: 224.3818
                    Surrogate loss: 0.0103
             Mean action noise std: 0.95
                       Mean reward: 702.72
               Mean episode length: 413.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17293312
                    Iteration time: 4.45s
                        Total time: 9526.47s
                               ETA: 8529.1s

################################################################################
                     [1m Learning iteration 2111/4000 [0m

                       Computation: 1840 steps/s (collection: 0.535s, learning 3.917s)
               Value function loss: 313.5133
                    Surrogate loss: 0.0121
             Mean action noise std: 0.95
                       Mean reward: 729.11
               Mean episode length: 429.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 4.45s
                        Total time: 9530.93s
                               ETA: 8524.6s

################################################################################
                     [1m Learning iteration 2112/4000 [0m

                       Computation: 1801 steps/s (collection: 0.567s, learning 3.980s)
               Value function loss: 211.3700
                    Surrogate loss: 0.0132
             Mean action noise std: 0.95
                       Mean reward: 717.83
               Mean episode length: 422.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17309696
                    Iteration time: 4.55s
                        Total time: 9535.47s
                               ETA: 8520.1s

################################################################################
                     [1m Learning iteration 2113/4000 [0m

                       Computation: 1828 steps/s (collection: 0.542s, learning 3.939s)
               Value function loss: 187.7580
                    Surrogate loss: 0.0099
             Mean action noise std: 0.95
                       Mean reward: 734.45
               Mean episode length: 431.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 4.48s
                        Total time: 9539.95s
                               ETA: 8515.6s

################################################################################
                     [1m Learning iteration 2114/4000 [0m

                       Computation: 1848 steps/s (collection: 0.564s, learning 3.868s)
               Value function loss: 192.8852
                    Surrogate loss: 0.0095
             Mean action noise std: 0.95
                       Mean reward: 733.70
               Mean episode length: 430.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 17326080
                    Iteration time: 4.43s
                        Total time: 9544.39s
                               ETA: 8511.0s

################################################################################
                     [1m Learning iteration 2115/4000 [0m

                       Computation: 1832 steps/s (collection: 0.556s, learning 3.914s)
               Value function loss: 201.0488
                    Surrogate loss: 0.0093
             Mean action noise std: 0.95
                       Mean reward: 724.44
               Mean episode length: 423.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 4.47s
                        Total time: 9548.85s
                               ETA: 8506.4s

################################################################################
                     [1m Learning iteration 2116/4000 [0m

                       Computation: 1808 steps/s (collection: 0.573s, learning 3.955s)
               Value function loss: 271.9061
                    Surrogate loss: 0.0114
             Mean action noise std: 0.95
                       Mean reward: 727.64
               Mean episode length: 424.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17342464
                    Iteration time: 4.53s
                        Total time: 9553.38s
                               ETA: 8501.9s

################################################################################
                     [1m Learning iteration 2117/4000 [0m

                       Computation: 1799 steps/s (collection: 0.656s, learning 3.895s)
               Value function loss: 185.9876
                    Surrogate loss: 0.0111
             Mean action noise std: 0.95
                       Mean reward: 715.08
               Mean episode length: 416.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 4.55s
                        Total time: 9557.93s
                               ETA: 8497.4s

################################################################################
                     [1m Learning iteration 2118/4000 [0m

                       Computation: 1811 steps/s (collection: 0.549s, learning 3.973s)
               Value function loss: 164.4255
                    Surrogate loss: 0.0110
             Mean action noise std: 0.95
                       Mean reward: 711.23
               Mean episode length: 413.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17358848
                    Iteration time: 4.52s
                        Total time: 9562.46s
                               ETA: 8492.9s

################################################################################
                     [1m Learning iteration 2119/4000 [0m

                       Computation: 1835 steps/s (collection: 0.572s, learning 3.892s)
               Value function loss: 257.9552
                    Surrogate loss: 0.0094
             Mean action noise std: 0.95
                       Mean reward: 719.41
               Mean episode length: 419.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 4.46s
                        Total time: 9566.92s
                               ETA: 8488.4s

################################################################################
                     [1m Learning iteration 2120/4000 [0m

                       Computation: 1842 steps/s (collection: 0.530s, learning 3.915s)
               Value function loss: 154.7934
                    Surrogate loss: 0.0090
             Mean action noise std: 0.95
                       Mean reward: 726.78
               Mean episode length: 423.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 17375232
                    Iteration time: 4.45s
                        Total time: 9571.37s
                               ETA: 8483.8s

################################################################################
                     [1m Learning iteration 2121/4000 [0m

                       Computation: 1820 steps/s (collection: 0.570s, learning 3.929s)
               Value function loss: 153.8195
                    Surrogate loss: 0.0098
             Mean action noise std: 0.95
                       Mean reward: 733.97
               Mean episode length: 426.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 4.50s
                        Total time: 9575.86s
                               ETA: 8479.3s

################################################################################
                     [1m Learning iteration 2122/4000 [0m

                       Computation: 1843 steps/s (collection: 0.522s, learning 3.921s)
               Value function loss: 149.5623
                    Surrogate loss: 0.0102
             Mean action noise std: 0.95
                       Mean reward: 726.86
               Mean episode length: 422.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 17391616
                    Iteration time: 4.44s
                        Total time: 9580.31s
                               ETA: 8474.7s

################################################################################
                     [1m Learning iteration 2123/4000 [0m

                       Computation: 1830 steps/s (collection: 0.553s, learning 3.922s)
               Value function loss: 176.1695
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 701.01
               Mean episode length: 407.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 4.47s
                        Total time: 9584.78s
                               ETA: 8470.2s

################################################################################
                     [1m Learning iteration 2124/4000 [0m

                       Computation: 1840 steps/s (collection: 0.542s, learning 3.910s)
               Value function loss: 291.1652
                    Surrogate loss: 0.0083
             Mean action noise std: 0.95
                       Mean reward: 704.71
               Mean episode length: 408.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17408000
                    Iteration time: 4.45s
                        Total time: 9589.23s
                               ETA: 8465.6s

################################################################################
                     [1m Learning iteration 2125/4000 [0m

                       Computation: 1811 steps/s (collection: 0.603s, learning 3.919s)
               Value function loss: 317.2440
                    Surrogate loss: 0.0084
             Mean action noise std: 0.95
                       Mean reward: 708.23
               Mean episode length: 410.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 4.52s
                        Total time: 9593.76s
                               ETA: 8461.1s

################################################################################
                     [1m Learning iteration 2126/4000 [0m

                       Computation: 1822 steps/s (collection: 0.593s, learning 3.903s)
               Value function loss: 260.6777
                    Surrogate loss: 0.0093
             Mean action noise std: 0.95
                       Mean reward: 724.73
               Mean episode length: 419.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17424384
                    Iteration time: 4.50s
                        Total time: 9598.25s
                               ETA: 8456.6s

################################################################################
                     [1m Learning iteration 2127/4000 [0m

                       Computation: 1804 steps/s (collection: 0.647s, learning 3.892s)
               Value function loss: 289.5450
                    Surrogate loss: 0.0111
             Mean action noise std: 0.95
                       Mean reward: 734.63
               Mean episode length: 424.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 4.54s
                        Total time: 9602.79s
                               ETA: 8452.1s

################################################################################
                     [1m Learning iteration 2128/4000 [0m

                       Computation: 1814 steps/s (collection: 0.604s, learning 3.910s)
               Value function loss: 224.9968
                    Surrogate loss: 0.0120
             Mean action noise std: 0.95
                       Mean reward: 746.35
               Mean episode length: 430.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17440768
                    Iteration time: 4.51s
                        Total time: 9607.31s
                               ETA: 8447.6s

################################################################################
                     [1m Learning iteration 2129/4000 [0m

                       Computation: 1800 steps/s (collection: 0.603s, learning 3.946s)
               Value function loss: 122.5688
                    Surrogate loss: 0.0103
             Mean action noise std: 0.95
                       Mean reward: 742.75
               Mean episode length: 429.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 4.55s
                        Total time: 9611.85s
                               ETA: 8443.1s

################################################################################
                     [1m Learning iteration 2130/4000 [0m

                       Computation: 1812 steps/s (collection: 0.602s, learning 3.917s)
               Value function loss: 219.7819
                    Surrogate loss: 0.0109
             Mean action noise std: 0.95
                       Mean reward: 756.04
               Mean episode length: 436.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17457152
                    Iteration time: 4.52s
                        Total time: 9616.37s
                               ETA: 8438.6s

################################################################################
                     [1m Learning iteration 2131/4000 [0m

                       Computation: 1791 steps/s (collection: 0.616s, learning 3.956s)
               Value function loss: 173.5162
                    Surrogate loss: 0.0099
             Mean action noise std: 0.95
                       Mean reward: 766.39
               Mean episode length: 442.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 4.57s
                        Total time: 9620.95s
                               ETA: 8434.1s

################################################################################
                     [1m Learning iteration 2132/4000 [0m

                       Computation: 1771 steps/s (collection: 0.622s, learning 4.004s)
               Value function loss: 211.1163
                    Surrogate loss: 0.0112
             Mean action noise std: 0.95
                       Mean reward: 773.01
               Mean episode length: 447.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 17473536
                    Iteration time: 4.63s
                        Total time: 9625.57s
                               ETA: 8429.7s

################################################################################
                     [1m Learning iteration 2133/4000 [0m

                       Computation: 1833 steps/s (collection: 0.547s, learning 3.920s)
               Value function loss: 178.2385
                    Surrogate loss: 0.0090
             Mean action noise std: 0.95
                       Mean reward: 780.34
               Mean episode length: 450.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 4.47s
                        Total time: 9630.04s
                               ETA: 8425.2s

################################################################################
                     [1m Learning iteration 2134/4000 [0m

                       Computation: 1811 steps/s (collection: 0.594s, learning 3.928s)
               Value function loss: 165.0754
                    Surrogate loss: 0.0078
             Mean action noise std: 0.95
                       Mean reward: 794.75
               Mean episode length: 458.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 17489920
                    Iteration time: 4.52s
                        Total time: 9634.56s
                               ETA: 8420.7s

################################################################################
                     [1m Learning iteration 2135/4000 [0m

                       Computation: 1803 steps/s (collection: 0.579s, learning 3.964s)
               Value function loss: 255.7003
                    Surrogate loss: 0.0095
             Mean action noise std: 0.95
                       Mean reward: 795.78
               Mean episode length: 459.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 4.54s
                        Total time: 9639.10s
                               ETA: 8416.2s

################################################################################
                     [1m Learning iteration 2136/4000 [0m

                       Computation: 1782 steps/s (collection: 0.607s, learning 3.989s)
               Value function loss: 105.5374
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 795.03
               Mean episode length: 457.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 17506304
                    Iteration time: 4.60s
                        Total time: 9643.70s
                               ETA: 8411.7s

################################################################################
                     [1m Learning iteration 2137/4000 [0m

                       Computation: 1821 steps/s (collection: 0.564s, learning 3.934s)
               Value function loss: 135.5634
                    Surrogate loss: 0.0095
             Mean action noise std: 0.95
                       Mean reward: 794.33
               Mean episode length: 457.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 4.50s
                        Total time: 9648.20s
                               ETA: 8407.2s

################################################################################
                     [1m Learning iteration 2138/4000 [0m

                       Computation: 1806 steps/s (collection: 0.603s, learning 3.932s)
               Value function loss: 205.4565
                    Surrogate loss: 0.0091
             Mean action noise std: 0.95
                       Mean reward: 787.48
               Mean episode length: 454.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17522688
                    Iteration time: 4.54s
                        Total time: 9652.73s
                               ETA: 8402.7s

################################################################################
                     [1m Learning iteration 2139/4000 [0m

                       Computation: 1808 steps/s (collection: 0.602s, learning 3.927s)
               Value function loss: 257.1493
                    Surrogate loss: 0.0090
             Mean action noise std: 0.95
                       Mean reward: 784.60
               Mean episode length: 454.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 4.53s
                        Total time: 9657.26s
                               ETA: 8398.2s

################################################################################
                     [1m Learning iteration 2140/4000 [0m

                       Computation: 1800 steps/s (collection: 0.589s, learning 3.960s)
               Value function loss: 340.8937
                    Surrogate loss: 0.0087
             Mean action noise std: 0.95
                       Mean reward: 787.34
               Mean episode length: 456.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 17539072
                    Iteration time: 4.55s
                        Total time: 9661.81s
                               ETA: 8393.7s

################################################################################
                     [1m Learning iteration 2141/4000 [0m

                       Computation: 1801 steps/s (collection: 0.612s, learning 3.936s)
               Value function loss: 252.0126
                    Surrogate loss: 0.0088
             Mean action noise std: 0.95
                       Mean reward: 795.74
               Mean episode length: 461.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 4.55s
                        Total time: 9666.36s
                               ETA: 8389.2s

################################################################################
                     [1m Learning iteration 2142/4000 [0m

                       Computation: 1813 steps/s (collection: 0.614s, learning 3.904s)
               Value function loss: 283.3575
                    Surrogate loss: 0.0089
             Mean action noise std: 0.95
                       Mean reward: 787.67
               Mean episode length: 457.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 17555456
                    Iteration time: 4.52s
                        Total time: 9670.88s
                               ETA: 8384.7s

################################################################################
                     [1m Learning iteration 2143/4000 [0m

                       Computation: 1814 steps/s (collection: 0.623s, learning 3.892s)
               Value function loss: 264.2394
                    Surrogate loss: 0.0050
             Mean action noise std: 0.95
                       Mean reward: 783.96
               Mean episode length: 455.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 4.51s
                        Total time: 9675.39s
                               ETA: 8380.2s

################################################################################
                     [1m Learning iteration 2144/4000 [0m

                       Computation: 1785 steps/s (collection: 0.597s, learning 3.992s)
               Value function loss: 169.8657
                    Surrogate loss: 0.0098
             Mean action noise std: 0.95
                       Mean reward: 790.28
               Mean episode length: 458.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 17571840
                    Iteration time: 4.59s
                        Total time: 9679.98s
                               ETA: 8375.8s

################################################################################
                     [1m Learning iteration 2145/4000 [0m

                       Computation: 1838 steps/s (collection: 0.553s, learning 3.903s)
               Value function loss: 130.0240
                    Surrogate loss: 0.0094
             Mean action noise std: 0.95
                       Mean reward: 783.79
               Mean episode length: 454.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 4.46s
                        Total time: 9684.44s
                               ETA: 8371.2s

################################################################################
                     [1m Learning iteration 2146/4000 [0m

                       Computation: 1805 steps/s (collection: 0.612s, learning 3.926s)
               Value function loss: 261.0723
                    Surrogate loss: 0.0102
             Mean action noise std: 0.95
                       Mean reward: 784.55
               Mean episode length: 455.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 17588224
                    Iteration time: 4.54s
                        Total time: 9688.97s
                               ETA: 8366.7s

################################################################################
                     [1m Learning iteration 2147/4000 [0m

                       Computation: 1815 steps/s (collection: 0.624s, learning 3.887s)
               Value function loss: 190.5311
                    Surrogate loss: 0.0078
             Mean action noise std: 0.95
                       Mean reward: 776.95
               Mean episode length: 450.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 4.51s
                        Total time: 9693.49s
                               ETA: 8362.2s

################################################################################
                     [1m Learning iteration 2148/4000 [0m

                       Computation: 1818 steps/s (collection: 0.570s, learning 3.936s)
               Value function loss: 238.1511
                    Surrogate loss: 0.0079
             Mean action noise std: 0.95
                       Mean reward: 781.05
               Mean episode length: 451.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17604608
                    Iteration time: 4.51s
                        Total time: 9697.99s
                               ETA: 8357.7s

################################################################################
                     [1m Learning iteration 2149/4000 [0m

                       Computation: 1823 steps/s (collection: 0.579s, learning 3.913s)
               Value function loss: 173.8479
                    Surrogate loss: 0.0090
             Mean action noise std: 0.95
                       Mean reward: 771.94
               Mean episode length: 445.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 4.49s
                        Total time: 9702.48s
                               ETA: 8353.2s

################################################################################
                     [1m Learning iteration 2150/4000 [0m

                       Computation: 1828 steps/s (collection: 0.543s, learning 3.938s)
               Value function loss: 202.0200
                    Surrogate loss: 0.0080
             Mean action noise std: 0.95
                       Mean reward: 762.12
               Mean episode length: 439.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17620992
                    Iteration time: 4.48s
                        Total time: 9706.96s
                               ETA: 8348.6s

################################################################################
                     [1m Learning iteration 2151/4000 [0m

                       Computation: 1798 steps/s (collection: 0.591s, learning 3.964s)
               Value function loss: 176.9558
                    Surrogate loss: 0.0096
             Mean action noise std: 0.95
                       Mean reward: 773.96
               Mean episode length: 446.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 4.55s
                        Total time: 9711.52s
                               ETA: 8344.1s

################################################################################
                     [1m Learning iteration 2152/4000 [0m

                       Computation: 1820 steps/s (collection: 0.566s, learning 3.933s)
               Value function loss: 153.3093
                    Surrogate loss: 0.0105
             Mean action noise std: 0.95
                       Mean reward: 775.16
               Mean episode length: 446.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 17637376
                    Iteration time: 4.50s
                        Total time: 9716.02s
                               ETA: 8339.6s

################################################################################
                     [1m Learning iteration 2153/4000 [0m

                       Computation: 1813 steps/s (collection: 0.555s, learning 3.963s)
               Value function loss: 171.8334
                    Surrogate loss: 0.0063
             Mean action noise std: 0.95
                       Mean reward: 774.73
               Mean episode length: 445.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 4.52s
                        Total time: 9720.53s
                               ETA: 8335.1s

################################################################################
                     [1m Learning iteration 2154/4000 [0m

                       Computation: 1827 steps/s (collection: 0.554s, learning 3.928s)
               Value function loss: 256.0865
                    Surrogate loss: 0.0088
             Mean action noise std: 0.95
                       Mean reward: 784.70
               Mean episode length: 450.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17653760
                    Iteration time: 4.48s
                        Total time: 9725.02s
                               ETA: 8330.6s

################################################################################
                     [1m Learning iteration 2155/4000 [0m

                       Computation: 1801 steps/s (collection: 0.624s, learning 3.924s)
               Value function loss: 318.1678
                    Surrogate loss: 0.0079
             Mean action noise std: 0.95
                       Mean reward: 759.04
               Mean episode length: 436.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 4.55s
                        Total time: 9729.56s
                               ETA: 8326.1s

################################################################################
                     [1m Learning iteration 2156/4000 [0m

                       Computation: 1825 steps/s (collection: 0.558s, learning 3.930s)
               Value function loss: 385.9744
                    Surrogate loss: 0.0105
             Mean action noise std: 0.95
                       Mean reward: 780.25
               Mean episode length: 447.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 17670144
                    Iteration time: 4.49s
                        Total time: 9734.05s
                               ETA: 8321.6s

################################################################################
                     [1m Learning iteration 2157/4000 [0m

                       Computation: 1833 steps/s (collection: 0.568s, learning 3.899s)
               Value function loss: 219.1396
                    Surrogate loss: 0.0095
             Mean action noise std: 0.95
                       Mean reward: 785.44
               Mean episode length: 450.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 4.47s
                        Total time: 9738.52s
                               ETA: 8317.0s

################################################################################
                     [1m Learning iteration 2158/4000 [0m

                       Computation: 1851 steps/s (collection: 0.552s, learning 3.872s)
               Value function loss: 295.2391
                    Surrogate loss: 0.0099
             Mean action noise std: 0.95
                       Mean reward: 798.11
               Mean episode length: 457.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17686528
                    Iteration time: 4.42s
                        Total time: 9742.94s
                               ETA: 8312.4s

################################################################################
                     [1m Learning iteration 2159/4000 [0m

                       Computation: 1826 steps/s (collection: 0.575s, learning 3.910s)
               Value function loss: 195.2839
                    Surrogate loss: 0.0096
             Mean action noise std: 0.95
                       Mean reward: 797.18
               Mean episode length: 457.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 4.49s
                        Total time: 9747.43s
                               ETA: 8307.9s

################################################################################
                     [1m Learning iteration 2160/4000 [0m

                       Computation: 1846 steps/s (collection: 0.550s, learning 3.886s)
               Value function loss: 130.0361
                    Surrogate loss: 0.0098
             Mean action noise std: 0.95
                       Mean reward: 811.63
               Mean episode length: 466.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 17702912
                    Iteration time: 4.44s
                        Total time: 9751.87s
                               ETA: 8303.3s

################################################################################
                     [1m Learning iteration 2161/4000 [0m

                       Computation: 1808 steps/s (collection: 0.575s, learning 3.954s)
               Value function loss: 240.6299
                    Surrogate loss: 0.0122
             Mean action noise std: 0.95
                       Mean reward: 817.66
               Mean episode length: 469.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 4.53s
                        Total time: 9756.39s
                               ETA: 8298.8s

################################################################################
                     [1m Learning iteration 2162/4000 [0m

                       Computation: 1850 steps/s (collection: 0.537s, learning 3.890s)
               Value function loss: 203.4352
                    Surrogate loss: 0.0082
             Mean action noise std: 0.95
                       Mean reward: 807.54
               Mean episode length: 462.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17719296
                    Iteration time: 4.43s
                        Total time: 9760.82s
                               ETA: 8294.2s

################################################################################
                     [1m Learning iteration 2163/4000 [0m

                       Computation: 1830 steps/s (collection: 0.560s, learning 3.915s)
               Value function loss: 199.0992
                    Surrogate loss: 0.0079
             Mean action noise std: 0.95
                       Mean reward: 803.25
               Mean episode length: 459.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 4.48s
                        Total time: 9765.30s
                               ETA: 8289.7s

################################################################################
                     [1m Learning iteration 2164/4000 [0m

                       Computation: 1843 steps/s (collection: 0.558s, learning 3.885s)
               Value function loss: 200.6081
                    Surrogate loss: 0.0110
             Mean action noise std: 0.95
                       Mean reward: 793.50
               Mean episode length: 456.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17735680
                    Iteration time: 4.44s
                        Total time: 9769.74s
                               ETA: 8285.1s

################################################################################
                     [1m Learning iteration 2165/4000 [0m

                       Computation: 1829 steps/s (collection: 0.562s, learning 3.916s)
               Value function loss: 216.7491
                    Surrogate loss: 0.0103
             Mean action noise std: 0.95
                       Mean reward: 807.09
               Mean episode length: 464.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 4.48s
                        Total time: 9774.22s
                               ETA: 8280.6s

################################################################################
                     [1m Learning iteration 2166/4000 [0m

                       Computation: 1828 steps/s (collection: 0.553s, learning 3.927s)
               Value function loss: 263.2741
                    Surrogate loss: 0.0105
             Mean action noise std: 0.95
                       Mean reward: 810.99
               Mean episode length: 464.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17752064
                    Iteration time: 4.48s
                        Total time: 9778.70s
                               ETA: 8276.0s

################################################################################
                     [1m Learning iteration 2167/4000 [0m

                       Computation: 1827 steps/s (collection: 0.552s, learning 3.931s)
               Value function loss: 139.9885
                    Surrogate loss: 0.0119
             Mean action noise std: 0.95
                       Mean reward: 801.87
               Mean episode length: 458.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 4.48s
                        Total time: 9783.18s
                               ETA: 8271.5s

################################################################################
                     [1m Learning iteration 2168/4000 [0m

                       Computation: 1811 steps/s (collection: 0.628s, learning 3.893s)
               Value function loss: 190.1010
                    Surrogate loss: 0.0105
             Mean action noise std: 0.95
                       Mean reward: 776.16
               Mean episode length: 444.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17768448
                    Iteration time: 4.52s
                        Total time: 9787.70s
                               ETA: 8267.0s

################################################################################
                     [1m Learning iteration 2169/4000 [0m

                       Computation: 1829 steps/s (collection: 0.569s, learning 3.908s)
               Value function loss: 199.7054
                    Surrogate loss: 0.0106
             Mean action noise std: 0.95
                       Mean reward: 769.27
               Mean episode length: 440.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 4.48s
                        Total time: 9792.18s
                               ETA: 8262.4s

################################################################################
                     [1m Learning iteration 2170/4000 [0m

                       Computation: 1842 steps/s (collection: 0.563s, learning 3.883s)
               Value function loss: 239.2988
                    Surrogate loss: 0.0097
             Mean action noise std: 0.95
                       Mean reward: 764.98
               Mean episode length: 438.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17784832
                    Iteration time: 4.45s
                        Total time: 9796.63s
                               ETA: 8257.9s

################################################################################
                     [1m Learning iteration 2171/4000 [0m

                       Computation: 1808 steps/s (collection: 0.601s, learning 3.928s)
               Value function loss: 335.6838
                    Surrogate loss: 0.0086
             Mean action noise std: 0.95
                       Mean reward: 758.42
               Mean episode length: 432.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 4.53s
                        Total time: 9801.15s
                               ETA: 8253.4s

################################################################################
                     [1m Learning iteration 2172/4000 [0m

                       Computation: 1818 steps/s (collection: 0.571s, learning 3.935s)
               Value function loss: 267.0532
                    Surrogate loss: 0.0086
             Mean action noise std: 0.95
                       Mean reward: 749.15
               Mean episode length: 429.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 17801216
                    Iteration time: 4.51s
                        Total time: 9805.66s
                               ETA: 8248.8s

################################################################################
                     [1m Learning iteration 2173/4000 [0m

                       Computation: 1837 steps/s (collection: 0.565s, learning 3.893s)
               Value function loss: 253.1969
                    Surrogate loss: 0.0089
             Mean action noise std: 0.95
                       Mean reward: 747.98
               Mean episode length: 429.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 4.46s
                        Total time: 9810.12s
                               ETA: 8244.3s

################################################################################
                     [1m Learning iteration 2174/4000 [0m

                       Computation: 1821 steps/s (collection: 0.553s, learning 3.944s)
               Value function loss: 245.1433
                    Surrogate loss: 0.0090
             Mean action noise std: 0.95
                       Mean reward: 744.72
               Mean episode length: 425.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17817600
                    Iteration time: 4.50s
                        Total time: 9814.62s
                               ETA: 8239.8s

################################################################################
                     [1m Learning iteration 2175/4000 [0m

                       Computation: 1824 steps/s (collection: 0.580s, learning 3.911s)
               Value function loss: 180.0987
                    Surrogate loss: 0.0115
             Mean action noise std: 0.95
                       Mean reward: 734.57
               Mean episode length: 421.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 4.49s
                        Total time: 9819.11s
                               ETA: 8235.2s

################################################################################
                     [1m Learning iteration 2176/4000 [0m

                       Computation: 1861 steps/s (collection: 0.515s, learning 3.886s)
               Value function loss: 101.0184
                    Surrogate loss: 0.0107
             Mean action noise std: 0.95
                       Mean reward: 727.40
               Mean episode length: 418.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 17833984
                    Iteration time: 4.40s
                        Total time: 9823.51s
                               ETA: 8230.6s

################################################################################
                     [1m Learning iteration 2177/4000 [0m

                       Computation: 1836 steps/s (collection: 0.560s, learning 3.900s)
               Value function loss: 182.9893
                    Surrogate loss: 0.0079
             Mean action noise std: 0.95
                       Mean reward: 733.40
               Mean episode length: 423.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 4.46s
                        Total time: 9827.97s
                               ETA: 8226.1s

################################################################################
                     [1m Learning iteration 2178/4000 [0m

                       Computation: 1860 steps/s (collection: 0.523s, learning 3.879s)
               Value function loss: 177.3049
                    Surrogate loss: 0.0089
             Mean action noise std: 0.95
                       Mean reward: 749.57
               Mean episode length: 432.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17850368
                    Iteration time: 4.40s
                        Total time: 9832.37s
                               ETA: 8221.5s

################################################################################
                     [1m Learning iteration 2179/4000 [0m

                       Computation: 1817 steps/s (collection: 0.582s, learning 3.925s)
               Value function loss: 192.6540
                    Surrogate loss: 0.0110
             Mean action noise std: 0.95
                       Mean reward: 742.61
               Mean episode length: 429.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 4.51s
                        Total time: 9836.88s
                               ETA: 8217.0s

################################################################################
                     [1m Learning iteration 2180/4000 [0m

                       Computation: 1857 steps/s (collection: 0.510s, learning 3.900s)
               Value function loss: 191.0818
                    Surrogate loss: 0.0068
             Mean action noise std: 0.94
                       Mean reward: 746.42
               Mean episode length: 431.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 17866752
                    Iteration time: 4.41s
                        Total time: 9841.29s
                               ETA: 8212.4s

################################################################################
                     [1m Learning iteration 2181/4000 [0m

                       Computation: 1846 steps/s (collection: 0.533s, learning 3.905s)
               Value function loss: 249.5243
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 734.67
               Mean episode length: 424.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 4.44s
                        Total time: 9845.72s
                               ETA: 8207.8s

################################################################################
                     [1m Learning iteration 2182/4000 [0m

                       Computation: 1839 steps/s (collection: 0.541s, learning 3.913s)
               Value function loss: 163.0218
                    Surrogate loss: 0.0109
             Mean action noise std: 0.94
                       Mean reward: 720.15
               Mean episode length: 416.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17883136
                    Iteration time: 4.45s
                        Total time: 9850.18s
                               ETA: 8203.2s

################################################################################
                     [1m Learning iteration 2183/4000 [0m

                       Computation: 1835 steps/s (collection: 0.537s, learning 3.925s)
               Value function loss: 158.7145
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 736.63
               Mean episode length: 425.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 4.46s
                        Total time: 9854.64s
                               ETA: 8198.7s

################################################################################
                     [1m Learning iteration 2184/4000 [0m

                       Computation: 1845 steps/s (collection: 0.545s, learning 3.894s)
               Value function loss: 165.4977
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 721.33
               Mean episode length: 416.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17899520
                    Iteration time: 4.44s
                        Total time: 9859.08s
                               ETA: 8194.1s

################################################################################
                     [1m Learning iteration 2185/4000 [0m

                       Computation: 1825 steps/s (collection: 0.522s, learning 3.965s)
               Value function loss: 261.4189
                    Surrogate loss: 0.0114
             Mean action noise std: 0.94
                       Mean reward: 702.04
               Mean episode length: 406.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 4.49s
                        Total time: 9863.57s
                               ETA: 8189.6s

################################################################################
                     [1m Learning iteration 2186/4000 [0m

                       Computation: 1797 steps/s (collection: 0.577s, learning 3.980s)
               Value function loss: 258.2182
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 705.04
               Mean episode length: 406.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 17915904
                    Iteration time: 4.56s
                        Total time: 9868.12s
                               ETA: 8185.1s

################################################################################
                     [1m Learning iteration 2187/4000 [0m

                       Computation: 1808 steps/s (collection: 0.547s, learning 3.983s)
               Value function loss: 369.1115
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 711.37
               Mean episode length: 410.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 4.53s
                        Total time: 9872.65s
                               ETA: 8180.6s

################################################################################
                     [1m Learning iteration 2188/4000 [0m

                       Computation: 1816 steps/s (collection: 0.557s, learning 3.952s)
               Value function loss: 179.3702
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 698.14
               Mean episode length: 403.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17932288
                    Iteration time: 4.51s
                        Total time: 9877.16s
                               ETA: 8176.1s

################################################################################
                     [1m Learning iteration 2189/4000 [0m

                       Computation: 1821 steps/s (collection: 0.583s, learning 3.915s)
               Value function loss: 230.3425
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 703.45
               Mean episode length: 406.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 4.50s
                        Total time: 9881.66s
                               ETA: 8171.5s

################################################################################
                     [1m Learning iteration 2190/4000 [0m

                       Computation: 1775 steps/s (collection: 0.594s, learning 4.019s)
               Value function loss: 239.0467
                    Surrogate loss: 0.0081
             Mean action noise std: 0.94
                       Mean reward: 710.50
               Mean episode length: 411.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17948672
                    Iteration time: 4.61s
                        Total time: 9886.27s
                               ETA: 8167.1s

################################################################################
                     [1m Learning iteration 2191/4000 [0m

                       Computation: 1804 steps/s (collection: 0.537s, learning 4.002s)
               Value function loss: 202.1512
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 718.98
               Mean episode length: 416.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 4.54s
                        Total time: 9890.81s
                               ETA: 8162.6s

################################################################################
                     [1m Learning iteration 2192/4000 [0m

                       Computation: 1802 steps/s (collection: 0.574s, learning 3.971s)
               Value function loss: 151.2445
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 691.38
               Mean episode length: 401.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17965056
                    Iteration time: 4.55s
                        Total time: 9895.36s
                               ETA: 8158.1s

################################################################################
                     [1m Learning iteration 2193/4000 [0m

                       Computation: 1839 steps/s (collection: 0.557s, learning 3.896s)
               Value function loss: 207.8168
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 711.73
               Mean episode length: 412.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 4.45s
                        Total time: 9899.81s
                               ETA: 8153.6s

################################################################################
                     [1m Learning iteration 2194/4000 [0m

                       Computation: 1860 steps/s (collection: 0.533s, learning 3.871s)
               Value function loss: 172.8208
                    Surrogate loss: 0.0110
             Mean action noise std: 0.94
                       Mean reward: 736.21
               Mean episode length: 425.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17981440
                    Iteration time: 4.40s
                        Total time: 9904.21s
                               ETA: 8149.0s

################################################################################
                     [1m Learning iteration 2195/4000 [0m

                       Computation: 1821 steps/s (collection: 0.609s, learning 3.888s)
               Value function loss: 215.2146
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 729.29
               Mean episode length: 422.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 4.50s
                        Total time: 9908.71s
                               ETA: 8144.5s

################################################################################
                     [1m Learning iteration 2196/4000 [0m

                       Computation: 1811 steps/s (collection: 0.619s, learning 3.903s)
               Value function loss: 158.4335
                    Surrogate loss: 0.0105
             Mean action noise std: 0.94
                       Mean reward: 724.91
               Mean episode length: 421.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17997824
                    Iteration time: 4.52s
                        Total time: 9913.23s
                               ETA: 8140.0s

################################################################################
                     [1m Learning iteration 2197/4000 [0m

                       Computation: 1811 steps/s (collection: 0.600s, learning 3.923s)
               Value function loss: 270.5170
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 726.48
               Mean episode length: 423.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 4.52s
                        Total time: 9917.76s
                               ETA: 8135.4s

################################################################################
                     [1m Learning iteration 2198/4000 [0m

                       Computation: 1794 steps/s (collection: 0.638s, learning 3.925s)
               Value function loss: 152.5832
                    Surrogate loss: 0.0091
             Mean action noise std: 0.94
                       Mean reward: 726.77
               Mean episode length: 423.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 18014208
                    Iteration time: 4.56s
                        Total time: 9922.32s
                               ETA: 8131.0s

################################################################################
                     [1m Learning iteration 2199/4000 [0m

                       Computation: 1824 steps/s (collection: 0.574s, learning 3.916s)
               Value function loss: 177.8630
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 726.04
               Mean episode length: 422.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 4.49s
                        Total time: 9926.81s
                               ETA: 8126.4s

################################################################################
                     [1m Learning iteration 2200/4000 [0m

                       Computation: 1829 steps/s (collection: 0.517s, learning 3.960s)
               Value function loss: 225.5380
                    Surrogate loss: 0.0142
             Mean action noise std: 0.94
                       Mean reward: 745.05
               Mean episode length: 433.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 18030592
                    Iteration time: 4.48s
                        Total time: 9931.29s
                               ETA: 8121.9s

################################################################################
                     [1m Learning iteration 2201/4000 [0m

                       Computation: 1823 steps/s (collection: 0.556s, learning 3.938s)
               Value function loss: 321.6788
                    Surrogate loss: 0.0114
             Mean action noise std: 0.94
                       Mean reward: 733.53
               Mean episode length: 426.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 4.49s
                        Total time: 9935.78s
                               ETA: 8117.4s

################################################################################
                     [1m Learning iteration 2202/4000 [0m

                       Computation: 1816 steps/s (collection: 0.593s, learning 3.917s)
               Value function loss: 340.9368
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 733.69
               Mean episode length: 426.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 18046976
                    Iteration time: 4.51s
                        Total time: 9940.29s
                               ETA: 8112.9s

################################################################################
                     [1m Learning iteration 2203/4000 [0m

                       Computation: 1814 steps/s (collection: 0.605s, learning 3.911s)
               Value function loss: 293.9196
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 761.01
               Mean episode length: 440.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 4.52s
                        Total time: 9944.81s
                               ETA: 8108.4s

################################################################################
                     [1m Learning iteration 2204/4000 [0m

                       Computation: 1799 steps/s (collection: 0.623s, learning 3.929s)
               Value function loss: 213.2396
                    Surrogate loss: 0.0106
             Mean action noise std: 0.94
                       Mean reward: 766.83
               Mean episode length: 443.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18063360
                    Iteration time: 4.55s
                        Total time: 9949.36s
                               ETA: 8103.9s

################################################################################
                     [1m Learning iteration 2205/4000 [0m

                       Computation: 1825 steps/s (collection: 0.554s, learning 3.934s)
               Value function loss: 241.9987
                    Surrogate loss: 0.0126
             Mean action noise std: 0.94
                       Mean reward: 769.94
               Mean episode length: 444.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 4.49s
                        Total time: 9953.85s
                               ETA: 8099.3s

################################################################################
                     [1m Learning iteration 2206/4000 [0m

                       Computation: 1817 steps/s (collection: 0.559s, learning 3.947s)
               Value function loss: 184.4570
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 784.75
               Mean episode length: 451.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18079744
                    Iteration time: 4.51s
                        Total time: 9958.35s
                               ETA: 8094.8s

################################################################################
                     [1m Learning iteration 2207/4000 [0m

                       Computation: 1837 steps/s (collection: 0.560s, learning 3.897s)
               Value function loss: 130.0179
                    Surrogate loss: 0.0101
             Mean action noise std: 0.94
                       Mean reward: 787.74
               Mean episode length: 452.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 4.46s
                        Total time: 9962.81s
                               ETA: 8090.3s

################################################################################
                     [1m Learning iteration 2208/4000 [0m

                       Computation: 1797 steps/s (collection: 0.652s, learning 3.905s)
               Value function loss: 276.3767
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 783.39
               Mean episode length: 447.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 18096128
                    Iteration time: 4.56s
                        Total time: 9967.37s
                               ETA: 8085.8s

################################################################################
                     [1m Learning iteration 2209/4000 [0m

                       Computation: 1814 steps/s (collection: 0.574s, learning 3.942s)
               Value function loss: 129.7708
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 766.45
               Mean episode length: 436.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 4.52s
                        Total time: 9971.88s
                               ETA: 8081.3s

################################################################################
                     [1m Learning iteration 2210/4000 [0m

                       Computation: 1816 steps/s (collection: 0.585s, learning 3.924s)
               Value function loss: 156.1251
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 775.70
               Mean episode length: 441.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 18112512
                    Iteration time: 4.51s
                        Total time: 9976.39s
                               ETA: 8076.8s

################################################################################
                     [1m Learning iteration 2211/4000 [0m

                       Computation: 1823 steps/s (collection: 0.590s, learning 3.903s)
               Value function loss: 199.5539
                    Surrogate loss: 0.0106
             Mean action noise std: 0.94
                       Mean reward: 765.58
               Mean episode length: 436.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 4.49s
                        Total time: 9980.88s
                               ETA: 8072.2s

################################################################################
                     [1m Learning iteration 2212/4000 [0m

                       Computation: 1791 steps/s (collection: 0.618s, learning 3.954s)
               Value function loss: 149.1701
                    Surrogate loss: 0.0096
             Mean action noise std: 0.94
                       Mean reward: 754.69
               Mean episode length: 430.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18128896
                    Iteration time: 4.57s
                        Total time: 9985.46s
                               ETA: 8067.8s

################################################################################
                     [1m Learning iteration 2213/4000 [0m

                       Computation: 1791 steps/s (collection: 0.593s, learning 3.980s)
               Value function loss: 153.2521
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 752.51
               Mean episode length: 429.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 4.57s
                        Total time: 9990.03s
                               ETA: 8063.3s

################################################################################
                     [1m Learning iteration 2214/4000 [0m

                       Computation: 1804 steps/s (collection: 0.579s, learning 3.960s)
               Value function loss: 119.7175
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 737.42
               Mean episode length: 421.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18145280
                    Iteration time: 4.54s
                        Total time: 9994.57s
                               ETA: 8058.8s

################################################################################
                     [1m Learning iteration 2215/4000 [0m

                       Computation: 1810 steps/s (collection: 0.607s, learning 3.919s)
               Value function loss: 154.0450
                    Surrogate loss: 0.0078
             Mean action noise std: 0.94
                       Mean reward: 730.98
               Mean episode length: 418.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 4.53s
                        Total time: 9999.09s
                               ETA: 8054.3s

################################################################################
                     [1m Learning iteration 2216/4000 [0m

                       Computation: 1820 steps/s (collection: 0.587s, learning 3.913s)
               Value function loss: 205.8201
                    Surrogate loss: 0.0091
             Mean action noise std: 0.94
                       Mean reward: 718.80
               Mean episode length: 413.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18161664
                    Iteration time: 4.50s
                        Total time: 10003.59s
                               ETA: 8049.8s

################################################################################
                     [1m Learning iteration 2217/4000 [0m

                       Computation: 1813 steps/s (collection: 0.590s, learning 3.928s)
               Value function loss: 265.0944
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 721.15
               Mean episode length: 414.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 4.52s
                        Total time: 10008.11s
                               ETA: 8045.3s

################################################################################
                     [1m Learning iteration 2218/4000 [0m

                       Computation: 1825 steps/s (collection: 0.570s, learning 3.917s)
               Value function loss: 358.3775
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 728.08
               Mean episode length: 419.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 18178048
                    Iteration time: 4.49s
                        Total time: 10012.60s
                               ETA: 8040.8s

################################################################################
                     [1m Learning iteration 2219/4000 [0m

                       Computation: 1797 steps/s (collection: 0.594s, learning 3.965s)
               Value function loss: 260.9031
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 727.68
               Mean episode length: 418.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 4.56s
                        Total time: 10017.16s
                               ETA: 8036.3s

################################################################################
                     [1m Learning iteration 2220/4000 [0m

                       Computation: 1812 steps/s (collection: 0.597s, learning 3.924s)
               Value function loss: 273.5212
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 726.04
               Mean episode length: 418.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18194432
                    Iteration time: 4.52s
                        Total time: 10021.68s
                               ETA: 8031.8s

################################################################################
                     [1m Learning iteration 2221/4000 [0m

                       Computation: 1791 steps/s (collection: 0.619s, learning 3.952s)
               Value function loss: 234.4655
                    Surrogate loss: 0.0079
             Mean action noise std: 0.94
                       Mean reward: 732.38
               Mean episode length: 421.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 4.57s
                        Total time: 10026.25s
                               ETA: 8027.3s

################################################################################
                     [1m Learning iteration 2222/4000 [0m

                       Computation: 1801 steps/s (collection: 0.556s, learning 3.991s)
               Value function loss: 181.2625
                    Surrogate loss: 0.0124
             Mean action noise std: 0.94
                       Mean reward: 729.21
               Mean episode length: 419.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18210816
                    Iteration time: 4.55s
                        Total time: 10030.80s
                               ETA: 8022.8s

################################################################################
                     [1m Learning iteration 2223/4000 [0m

                       Computation: 1814 steps/s (collection: 0.582s, learning 3.932s)
               Value function loss: 166.4045
                    Surrogate loss: 0.0109
             Mean action noise std: 0.95
                       Mean reward: 730.17
               Mean episode length: 419.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 4.51s
                        Total time: 10035.31s
                               ETA: 8018.3s

################################################################################
                     [1m Learning iteration 2224/4000 [0m

                       Computation: 1821 steps/s (collection: 0.589s, learning 3.908s)
               Value function loss: 268.2836
                    Surrogate loss: 0.0084
             Mean action noise std: 0.95
                       Mean reward: 727.22
               Mean episode length: 416.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 18227200
                    Iteration time: 4.50s
                        Total time: 10039.81s
                               ETA: 8013.8s

################################################################################
                     [1m Learning iteration 2225/4000 [0m

                       Computation: 1796 steps/s (collection: 0.602s, learning 3.957s)
               Value function loss: 155.2575
                    Surrogate loss: 0.0083
             Mean action noise std: 0.95
                       Mean reward: 712.34
               Mean episode length: 409.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 4.56s
                        Total time: 10044.37s
                               ETA: 8009.3s

################################################################################
                     [1m Learning iteration 2226/4000 [0m

                       Computation: 1797 steps/s (collection: 0.616s, learning 3.941s)
               Value function loss: 189.8494
                    Surrogate loss: 0.0099
             Mean action noise std: 0.95
                       Mean reward: 715.55
               Mean episode length: 410.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18243584
                    Iteration time: 4.56s
                        Total time: 10048.92s
                               ETA: 8004.8s

################################################################################
                     [1m Learning iteration 2227/4000 [0m

                       Computation: 1819 steps/s (collection: 0.562s, learning 3.940s)
               Value function loss: 235.5702
                    Surrogate loss: 0.0089
             Mean action noise std: 0.95
                       Mean reward: 717.70
               Mean episode length: 412.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 4.50s
                        Total time: 10053.43s
                               ETA: 8000.3s

################################################################################
                     [1m Learning iteration 2228/4000 [0m

                       Computation: 1815 steps/s (collection: 0.574s, learning 3.939s)
               Value function loss: 189.7630
                    Surrogate loss: 0.0105
             Mean action noise std: 0.95
                       Mean reward: 686.86
               Mean episode length: 393.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18259968
                    Iteration time: 4.51s
                        Total time: 10057.94s
                               ETA: 7995.8s

################################################################################
                     [1m Learning iteration 2229/4000 [0m

                       Computation: 1829 steps/s (collection: 0.554s, learning 3.924s)
               Value function loss: 147.7956
                    Surrogate loss: 0.0089
             Mean action noise std: 0.95
                       Mean reward: 680.80
               Mean episode length: 390.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 4.48s
                        Total time: 10062.42s
                               ETA: 7991.3s

################################################################################
                     [1m Learning iteration 2230/4000 [0m

                       Computation: 1797 steps/s (collection: 0.605s, learning 3.954s)
               Value function loss: 170.6575
                    Surrogate loss: 0.0095
             Mean action noise std: 0.95
                       Mean reward: 669.53
               Mean episode length: 385.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18276352
                    Iteration time: 4.56s
                        Total time: 10066.98s
                               ETA: 7986.8s

################################################################################
                     [1m Learning iteration 2231/4000 [0m

                       Computation: 1814 steps/s (collection: 0.565s, learning 3.951s)
               Value function loss: 201.2826
                    Surrogate loss: 0.0092
             Mean action noise std: 0.95
                       Mean reward: 644.54
               Mean episode length: 370.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 4.52s
                        Total time: 10071.49s
                               ETA: 7982.3s

################################################################################
                     [1m Learning iteration 2232/4000 [0m

                       Computation: 1821 steps/s (collection: 0.563s, learning 3.935s)
               Value function loss: 260.1755
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 646.17
               Mean episode length: 371.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 18292736
                    Iteration time: 4.50s
                        Total time: 10075.99s
                               ETA: 7977.8s

################################################################################
                     [1m Learning iteration 2233/4000 [0m

                       Computation: 1827 steps/s (collection: 0.563s, learning 3.919s)
               Value function loss: 323.7620
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 649.25
               Mean episode length: 373.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 4.48s
                        Total time: 10080.47s
                               ETA: 7973.2s

################################################################################
                     [1m Learning iteration 2234/4000 [0m

                       Computation: 1806 steps/s (collection: 0.587s, learning 3.948s)
               Value function loss: 223.3062
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 656.37
               Mean episode length: 376.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 18309120
                    Iteration time: 4.53s
                        Total time: 10085.01s
                               ETA: 7968.7s

################################################################################
                     [1m Learning iteration 2235/4000 [0m

                       Computation: 1807 steps/s (collection: 0.580s, learning 3.952s)
               Value function loss: 141.3787
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 642.25
               Mean episode length: 369.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 4.53s
                        Total time: 10089.54s
                               ETA: 7964.2s

################################################################################
                     [1m Learning iteration 2236/4000 [0m

                       Computation: 1810 steps/s (collection: 0.574s, learning 3.950s)
               Value function loss: 208.5467
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 645.80
               Mean episode length: 371.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18325504
                    Iteration time: 4.52s
                        Total time: 10094.06s
                               ETA: 7959.7s

################################################################################
                     [1m Learning iteration 2237/4000 [0m

                       Computation: 1822 steps/s (collection: 0.571s, learning 3.925s)
               Value function loss: 170.0039
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 656.81
               Mean episode length: 378.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 4.50s
                        Total time: 10098.56s
                               ETA: 7955.2s

################################################################################
                     [1m Learning iteration 2238/4000 [0m

                       Computation: 1803 steps/s (collection: 0.601s, learning 3.941s)
               Value function loss: 223.2879
                    Surrogate loss: 0.0084
             Mean action noise std: 0.94
                       Mean reward: 652.51
               Mean episode length: 375.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18341888
                    Iteration time: 4.54s
                        Total time: 10103.10s
                               ETA: 7950.7s

################################################################################
                     [1m Learning iteration 2239/4000 [0m

                       Computation: 1801 steps/s (collection: 0.589s, learning 3.958s)
               Value function loss: 258.3828
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 686.56
               Mean episode length: 394.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 4.55s
                        Total time: 10107.65s
                               ETA: 7946.2s

################################################################################
                     [1m Learning iteration 2240/4000 [0m

                       Computation: 1768 steps/s (collection: 0.581s, learning 4.052s)
               Value function loss: 125.3824
                    Surrogate loss: 0.0071
             Mean action noise std: 0.94
                       Mean reward: 694.83
               Mean episode length: 399.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18358272
                    Iteration time: 4.63s
                        Total time: 10112.28s
                               ETA: 7941.8s

################################################################################
                     [1m Learning iteration 2241/4000 [0m

                       Computation: 1821 steps/s (collection: 0.550s, learning 3.947s)
               Value function loss: 219.5847
                    Surrogate loss: 0.0104
             Mean action noise std: 0.94
                       Mean reward: 689.07
               Mean episode length: 397.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 4.50s
                        Total time: 10116.78s
                               ETA: 7937.3s

################################################################################
                     [1m Learning iteration 2242/4000 [0m

                       Computation: 1804 steps/s (collection: 0.585s, learning 3.955s)
               Value function loss: 174.8912
                    Surrogate loss: 0.0091
             Mean action noise std: 0.94
                       Mean reward: 667.38
               Mean episode length: 384.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 18374656
                    Iteration time: 4.54s
                        Total time: 10121.32s
                               ETA: 7932.8s

################################################################################
                     [1m Learning iteration 2243/4000 [0m

                       Computation: 1803 steps/s (collection: 0.571s, learning 3.972s)
               Value function loss: 239.1464
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 661.98
               Mean episode length: 380.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 4.54s
                        Total time: 10125.86s
                               ETA: 7928.3s

################################################################################
                     [1m Learning iteration 2244/4000 [0m

                       Computation: 1805 steps/s (collection: 0.562s, learning 3.976s)
               Value function loss: 177.9328
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 665.35
               Mean episode length: 382.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18391040
                    Iteration time: 4.54s
                        Total time: 10130.40s
                               ETA: 7923.8s

################################################################################
                     [1m Learning iteration 2245/4000 [0m

                       Computation: 1858 steps/s (collection: 0.527s, learning 3.880s)
               Value function loss: 143.7348
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 662.72
               Mean episode length: 380.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 4.41s
                        Total time: 10134.80s
                               ETA: 7919.2s

################################################################################
                     [1m Learning iteration 2246/4000 [0m

                       Computation: 1854 steps/s (collection: 0.527s, learning 3.891s)
               Value function loss: 242.5083
                    Surrogate loss: 0.0096
             Mean action noise std: 0.94
                       Mean reward: 675.88
               Mean episode length: 386.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18407424
                    Iteration time: 4.42s
                        Total time: 10139.22s
                               ETA: 7914.6s

################################################################################
                     [1m Learning iteration 2247/4000 [0m

                       Computation: 1819 steps/s (collection: 0.580s, learning 3.921s)
               Value function loss: 138.5067
                    Surrogate loss: 0.0072
             Mean action noise std: 0.94
                       Mean reward: 656.28
               Mean episode length: 375.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 4.50s
                        Total time: 10143.72s
                               ETA: 7910.1s

################################################################################
                     [1m Learning iteration 2248/4000 [0m

                       Computation: 1831 steps/s (collection: 0.557s, learning 3.915s)
               Value function loss: 275.9334
                    Surrogate loss: 0.0080
             Mean action noise std: 0.94
                       Mean reward: 678.20
               Mean episode length: 388.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 18423808
                    Iteration time: 4.47s
                        Total time: 10148.20s
                               ETA: 7905.6s

################################################################################
                     [1m Learning iteration 2249/4000 [0m

                       Computation: 1838 steps/s (collection: 0.563s, learning 3.893s)
               Value function loss: 286.7311
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 672.36
               Mean episode length: 383.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 4.46s
                        Total time: 10152.65s
                               ETA: 7901.0s

################################################################################
                     [1m Learning iteration 2250/4000 [0m

                       Computation: 1829 steps/s (collection: 0.564s, learning 3.915s)
               Value function loss: 162.6195
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 689.62
               Mean episode length: 393.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18440192
                    Iteration time: 4.48s
                        Total time: 10157.13s
                               ETA: 7896.5s

################################################################################
                     [1m Learning iteration 2251/4000 [0m

                       Computation: 1811 steps/s (collection: 0.600s, learning 3.923s)
               Value function loss: 209.9428
                    Surrogate loss: 0.0108
             Mean action noise std: 0.94
                       Mean reward: 713.74
               Mean episode length: 406.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 4.52s
                        Total time: 10161.65s
                               ETA: 7892.0s

################################################################################
                     [1m Learning iteration 2252/4000 [0m

                       Computation: 1837 steps/s (collection: 0.559s, learning 3.899s)
               Value function loss: 151.0101
                    Surrogate loss: 0.0109
             Mean action noise std: 0.94
                       Mean reward: 725.52
               Mean episode length: 412.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 18456576
                    Iteration time: 4.46s
                        Total time: 10166.11s
                               ETA: 7887.4s

################################################################################
                     [1m Learning iteration 2253/4000 [0m

                       Computation: 1852 steps/s (collection: 0.539s, learning 3.884s)
               Value function loss: 243.8792
                    Surrogate loss: 0.0080
             Mean action noise std: 0.94
                       Mean reward: 737.81
               Mean episode length: 420.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 4.42s
                        Total time: 10170.53s
                               ETA: 7882.8s

################################################################################
                     [1m Learning iteration 2254/4000 [0m

                       Computation: 1842 steps/s (collection: 0.553s, learning 3.892s)
               Value function loss: 203.7315
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 738.10
               Mean episode length: 420.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18472960
                    Iteration time: 4.44s
                        Total time: 10174.98s
                               ETA: 7878.3s

################################################################################
                     [1m Learning iteration 2255/4000 [0m

                       Computation: 1820 steps/s (collection: 0.587s, learning 3.913s)
               Value function loss: 328.0457
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 759.95
               Mean episode length: 433.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 4.50s
                        Total time: 10179.48s
                               ETA: 7873.8s

################################################################################
                     [1m Learning iteration 2256/4000 [0m

                       Computation: 1826 steps/s (collection: 0.593s, learning 3.892s)
               Value function loss: 140.5674
                    Surrogate loss: 0.0120
             Mean action noise std: 0.94
                       Mean reward: 752.79
               Mean episode length: 429.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 18489344
                    Iteration time: 4.48s
                        Total time: 10183.96s
                               ETA: 7869.2s

################################################################################
                     [1m Learning iteration 2257/4000 [0m

                       Computation: 1817 steps/s (collection: 0.600s, learning 3.906s)
               Value function loss: 256.3099
                    Surrogate loss: 0.0103
             Mean action noise std: 0.94
                       Mean reward: 782.54
               Mean episode length: 446.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 4.51s
                        Total time: 10188.47s
                               ETA: 7864.7s

################################################################################
                     [1m Learning iteration 2258/4000 [0m

                       Computation: 1843 steps/s (collection: 0.546s, learning 3.899s)
               Value function loss: 199.0266
                    Surrogate loss: 0.0110
             Mean action noise std: 0.94
                       Mean reward: 783.58
               Mean episode length: 447.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18505728
                    Iteration time: 4.44s
                        Total time: 10192.92s
                               ETA: 7860.1s

################################################################################
                     [1m Learning iteration 2259/4000 [0m

                       Computation: 1841 steps/s (collection: 0.559s, learning 3.889s)
               Value function loss: 150.1444
                    Surrogate loss: 0.0094
             Mean action noise std: 0.94
                       Mean reward: 784.07
               Mean episode length: 447.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 4.45s
                        Total time: 10197.36s
                               ETA: 7855.6s

################################################################################
                     [1m Learning iteration 2260/4000 [0m

                       Computation: 1833 steps/s (collection: 0.546s, learning 3.922s)
               Value function loss: 246.7619
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 784.44
               Mean episode length: 447.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18522112
                    Iteration time: 4.47s
                        Total time: 10201.83s
                               ETA: 7851.0s

################################################################################
                     [1m Learning iteration 2261/4000 [0m

                       Computation: 1835 steps/s (collection: 0.556s, learning 3.906s)
               Value function loss: 238.4034
                    Surrogate loss: 0.0091
             Mean action noise std: 0.94
                       Mean reward: 781.40
               Mean episode length: 445.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 4.46s
                        Total time: 10206.29s
                               ETA: 7846.5s

################################################################################
                     [1m Learning iteration 2262/4000 [0m

                       Computation: 1839 steps/s (collection: 0.555s, learning 3.898s)
               Value function loss: 399.7511
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 779.23
               Mean episode length: 443.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 18538496
                    Iteration time: 4.45s
                        Total time: 10210.75s
                               ETA: 7841.9s

################################################################################
                     [1m Learning iteration 2263/4000 [0m

                       Computation: 1826 steps/s (collection: 0.552s, learning 3.933s)
               Value function loss: 228.6284
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 777.91
               Mean episode length: 443.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 4.49s
                        Total time: 10215.23s
                               ETA: 7837.4s

################################################################################
                     [1m Learning iteration 2264/4000 [0m

                       Computation: 1847 steps/s (collection: 0.553s, learning 3.882s)
               Value function loss: 338.4394
                    Surrogate loss: 0.0124
             Mean action noise std: 0.94
                       Mean reward: 776.02
               Mean episode length: 443.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18554880
                    Iteration time: 4.43s
                        Total time: 10219.67s
                               ETA: 7832.8s

################################################################################
                     [1m Learning iteration 2265/4000 [0m

                       Computation: 1816 steps/s (collection: 0.525s, learning 3.985s)
               Value function loss: 321.9868
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 780.10
               Mean episode length: 445.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 4.51s
                        Total time: 10224.18s
                               ETA: 7828.3s

################################################################################
                     [1m Learning iteration 2266/4000 [0m

                       Computation: 1841 steps/s (collection: 0.565s, learning 3.883s)
               Value function loss: 200.6493
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 777.97
               Mean episode length: 445.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18571264
                    Iteration time: 4.45s
                        Total time: 10228.63s
                               ETA: 7823.7s

################################################################################
                     [1m Learning iteration 2267/4000 [0m

                       Computation: 1828 steps/s (collection: 0.562s, learning 3.918s)
               Value function loss: 230.5204
                    Surrogate loss: 0.0122
             Mean action noise std: 0.94
                       Mean reward: 760.73
               Mean episode length: 437.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 4.48s
                        Total time: 10233.11s
                               ETA: 7819.2s

################################################################################
                     [1m Learning iteration 2268/4000 [0m

                       Computation: 1806 steps/s (collection: 0.585s, learning 3.949s)
               Value function loss: 122.2974
                    Surrogate loss: 0.0105
             Mean action noise std: 0.94
                       Mean reward: 753.07
               Mean episode length: 433.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 18587648
                    Iteration time: 4.53s
                        Total time: 10237.64s
                               ETA: 7814.7s

################################################################################
                     [1m Learning iteration 2269/4000 [0m

                       Computation: 1814 steps/s (collection: 0.577s, learning 3.937s)
               Value function loss: 246.5006
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 736.30
               Mean episode length: 425.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 4.51s
                        Total time: 10242.15s
                               ETA: 7810.2s

################################################################################
                     [1m Learning iteration 2270/4000 [0m

                       Computation: 1811 steps/s (collection: 0.572s, learning 3.950s)
               Value function loss: 218.7638
                    Surrogate loss: 0.0104
             Mean action noise std: 0.94
                       Mean reward: 732.89
               Mean episode length: 424.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18604032
                    Iteration time: 4.52s
                        Total time: 10246.68s
                               ETA: 7805.7s

################################################################################
                     [1m Learning iteration 2271/4000 [0m

                       Computation: 1826 steps/s (collection: 0.587s, learning 3.898s)
               Value function loss: 240.7249
                    Surrogate loss: 0.0110
             Mean action noise std: 0.94
                       Mean reward: 737.06
               Mean episode length: 429.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 4.49s
                        Total time: 10251.16s
                               ETA: 7801.2s

################################################################################
                     [1m Learning iteration 2272/4000 [0m

                       Computation: 1804 steps/s (collection: 0.579s, learning 3.960s)
               Value function loss: 200.1353
                    Surrogate loss: 0.0090
             Mean action noise std: 0.94
                       Mean reward: 741.07
               Mean episode length: 432.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18620416
                    Iteration time: 4.54s
                        Total time: 10255.70s
                               ETA: 7796.7s

################################################################################
                     [1m Learning iteration 2273/4000 [0m

                       Computation: 1829 steps/s (collection: 0.566s, learning 3.912s)
               Value function loss: 254.8180
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 733.27
               Mean episode length: 430.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 4.48s
                        Total time: 10260.18s
                               ETA: 7792.1s

################################################################################
                     [1m Learning iteration 2274/4000 [0m

                       Computation: 1800 steps/s (collection: 0.589s, learning 3.961s)
               Value function loss: 196.4627
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 729.28
               Mean episode length: 428.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18636800
                    Iteration time: 4.55s
                        Total time: 10264.73s
                               ETA: 7787.7s

################################################################################
                     [1m Learning iteration 2275/4000 [0m

                       Computation: 1814 steps/s (collection: 0.575s, learning 3.939s)
               Value function loss: 208.7869
                    Surrogate loss: 0.0092
             Mean action noise std: 0.94
                       Mean reward: 714.25
               Mean episode length: 420.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 4.51s
                        Total time: 10269.24s
                               ETA: 7783.1s

################################################################################
                     [1m Learning iteration 2276/4000 [0m

                       Computation: 1822 steps/s (collection: 0.518s, learning 3.977s)
               Value function loss: 169.7129
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 709.80
               Mean episode length: 417.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18653184
                    Iteration time: 4.50s
                        Total time: 10273.74s
                               ETA: 7778.6s

################################################################################
                     [1m Learning iteration 2277/4000 [0m

                       Computation: 1817 steps/s (collection: 0.580s, learning 3.927s)
               Value function loss: 244.7452
                    Surrogate loss: 0.0101
             Mean action noise std: 0.94
                       Mean reward: 706.85
               Mean episode length: 416.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 4.51s
                        Total time: 10278.24s
                               ETA: 7774.1s

################################################################################
                     [1m Learning iteration 2278/4000 [0m

                       Computation: 1843 steps/s (collection: 0.538s, learning 3.907s)
               Value function loss: 212.6870
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 730.00
               Mean episode length: 428.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18669568
                    Iteration time: 4.44s
                        Total time: 10282.69s
                               ETA: 7769.5s

################################################################################
                     [1m Learning iteration 2279/4000 [0m

                       Computation: 1835 steps/s (collection: 0.568s, learning 3.896s)
               Value function loss: 252.8324
                    Surrogate loss: 0.0108
             Mean action noise std: 0.94
                       Mean reward: 721.77
               Mean episode length: 423.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 4.46s
                        Total time: 10287.15s
                               ETA: 7765.0s

################################################################################
                     [1m Learning iteration 2280/4000 [0m

                       Computation: 1834 steps/s (collection: 0.553s, learning 3.913s)
               Value function loss: 334.6859
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 720.58
               Mean episode length: 421.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 18685952
                    Iteration time: 4.47s
                        Total time: 10291.62s
                               ETA: 7760.4s

################################################################################
                     [1m Learning iteration 2281/4000 [0m

                       Computation: 1835 steps/s (collection: 0.561s, learning 3.902s)
               Value function loss: 219.5639
                    Surrogate loss: 0.0068
             Mean action noise std: 0.94
                       Mean reward: 718.31
               Mean episode length: 419.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 4.46s
                        Total time: 10296.08s
                               ETA: 7755.9s

################################################################################
                     [1m Learning iteration 2282/4000 [0m

                       Computation: 1835 steps/s (collection: 0.560s, learning 3.902s)
               Value function loss: 169.2075
                    Surrogate loss: 0.0120
             Mean action noise std: 0.94
                       Mean reward: 726.20
               Mean episode length: 424.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18702336
                    Iteration time: 4.46s
                        Total time: 10300.54s
                               ETA: 7751.4s

################################################################################
                     [1m Learning iteration 2283/4000 [0m

                       Computation: 1825 steps/s (collection: 0.556s, learning 3.931s)
               Value function loss: 201.2390
                    Surrogate loss: 0.0080
             Mean action noise std: 0.94
                       Mean reward: 736.44
               Mean episode length: 428.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 4.49s
                        Total time: 10305.03s
                               ETA: 7746.8s

################################################################################
                     [1m Learning iteration 2284/4000 [0m

                       Computation: 1836 steps/s (collection: 0.520s, learning 3.941s)
               Value function loss: 147.3419
                    Surrogate loss: 0.0098
             Mean action noise std: 0.94
                       Mean reward: 725.40
               Mean episode length: 421.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18718720
                    Iteration time: 4.46s
                        Total time: 10309.49s
                               ETA: 7742.3s

################################################################################
                     [1m Learning iteration 2285/4000 [0m

                       Computation: 1814 steps/s (collection: 0.579s, learning 3.936s)
               Value function loss: 210.7516
                    Surrogate loss: 0.0099
             Mean action noise std: 0.94
                       Mean reward: 741.70
               Mean episode length: 431.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 4.52s
                        Total time: 10314.01s
                               ETA: 7737.8s

################################################################################
                     [1m Learning iteration 2286/4000 [0m

                       Computation: 1841 steps/s (collection: 0.550s, learning 3.897s)
               Value function loss: 210.4865
                    Surrogate loss: 0.0101
             Mean action noise std: 0.94
                       Mean reward: 740.35
               Mean episode length: 431.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18735104
                    Iteration time: 4.45s
                        Total time: 10318.45s
                               ETA: 7733.2s

################################################################################
                     [1m Learning iteration 2287/4000 [0m

                       Computation: 1825 steps/s (collection: 0.560s, learning 3.929s)
               Value function loss: 152.4840
                    Surrogate loss: 0.0137
             Mean action noise std: 0.94
                       Mean reward: 740.98
               Mean episode length: 430.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 4.49s
                        Total time: 10322.94s
                               ETA: 7728.7s

################################################################################
                     [1m Learning iteration 2288/4000 [0m

                       Computation: 1830 steps/s (collection: 0.577s, learning 3.897s)
               Value function loss: 204.6298
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 735.58
               Mean episode length: 427.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18751488
                    Iteration time: 4.47s
                        Total time: 10327.42s
                               ETA: 7724.1s

################################################################################
                     [1m Learning iteration 2289/4000 [0m

                       Computation: 1787 steps/s (collection: 0.627s, learning 3.957s)
               Value function loss: 213.4986
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 743.41
               Mean episode length: 431.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 4.58s
                        Total time: 10332.00s
                               ETA: 7719.7s

################################################################################
                     [1m Learning iteration 2290/4000 [0m

                       Computation: 1797 steps/s (collection: 0.649s, learning 3.907s)
               Value function loss: 129.8418
                    Surrogate loss: 0.0091
             Mean action noise std: 0.94
                       Mean reward: 739.65
               Mean episode length: 431.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18767872
                    Iteration time: 4.56s
                        Total time: 10336.56s
                               ETA: 7715.2s

################################################################################
                     [1m Learning iteration 2291/4000 [0m

                       Computation: 1844 steps/s (collection: 0.520s, learning 3.920s)
               Value function loss: 271.7225
                    Surrogate loss: 0.0058
             Mean action noise std: 0.94
                       Mean reward: 749.04
               Mean episode length: 436.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 4.44s
                        Total time: 10341.00s
                               ETA: 7710.6s

################################################################################
                     [1m Learning iteration 2292/4000 [0m

                       Computation: 1822 steps/s (collection: 0.604s, learning 3.890s)
               Value function loss: 196.2686
                    Surrogate loss: 0.0113
             Mean action noise std: 0.94
                       Mean reward: 751.13
               Mean episode length: 436.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18784256
                    Iteration time: 4.49s
                        Total time: 10345.49s
                               ETA: 7706.1s

################################################################################
                     [1m Learning iteration 2293/4000 [0m

                       Computation: 1821 steps/s (collection: 0.574s, learning 3.923s)
               Value function loss: 296.2229
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 741.57
               Mean episode length: 431.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 4.50s
                        Total time: 10349.99s
                               ETA: 7701.6s

################################################################################
                     [1m Learning iteration 2294/4000 [0m

                       Computation: 1796 steps/s (collection: 0.625s, learning 3.934s)
               Value function loss: 192.0621
                    Surrogate loss: 0.0102
             Mean action noise std: 0.94
                       Mean reward: 739.46
               Mean episode length: 430.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18800640
                    Iteration time: 4.56s
                        Total time: 10354.55s
                               ETA: 7697.1s

################################################################################
                     [1m Learning iteration 2295/4000 [0m

                       Computation: 1805 steps/s (collection: 0.574s, learning 3.963s)
               Value function loss: 298.6481
                    Surrogate loss: 0.0101
             Mean action noise std: 0.94
                       Mean reward: 763.86
               Mean episode length: 444.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 4.54s
                        Total time: 10359.09s
                               ETA: 7692.6s

################################################################################
                     [1m Learning iteration 2296/4000 [0m

                       Computation: 1821 steps/s (collection: 0.576s, learning 3.922s)
               Value function loss: 391.7142
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 758.87
               Mean episode length: 440.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 18817024
                    Iteration time: 4.50s
                        Total time: 10363.58s
                               ETA: 7688.1s

################################################################################
                     [1m Learning iteration 2297/4000 [0m

                       Computation: 1802 steps/s (collection: 0.605s, learning 3.941s)
               Value function loss: 172.7022
                    Surrogate loss: 0.0069
             Mean action noise std: 0.94
                       Mean reward: 759.45
               Mean episode length: 441.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 4.55s
                        Total time: 10368.13s
                               ETA: 7683.6s

################################################################################
                     [1m Learning iteration 2298/4000 [0m

                       Computation: 1811 steps/s (collection: 0.574s, learning 3.949s)
               Value function loss: 144.9496
                    Surrogate loss: 0.0091
             Mean action noise std: 0.94
                       Mean reward: 767.29
               Mean episode length: 445.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18833408
                    Iteration time: 4.52s
                        Total time: 10372.65s
                               ETA: 7679.1s

################################################################################
                     [1m Learning iteration 2299/4000 [0m

                       Computation: 1828 steps/s (collection: 0.540s, learning 3.940s)
               Value function loss: 147.3589
                    Surrogate loss: 0.0082
             Mean action noise std: 0.94
                       Mean reward: 753.40
               Mean episode length: 437.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 4.48s
                        Total time: 10377.13s
                               ETA: 7674.6s

################################################################################
                     [1m Learning iteration 2300/4000 [0m

                       Computation: 1823 steps/s (collection: 0.556s, learning 3.936s)
               Value function loss: 271.2091
                    Surrogate loss: 0.0073
             Mean action noise std: 0.94
                       Mean reward: 754.94
               Mean episode length: 437.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18849792
                    Iteration time: 4.49s
                        Total time: 10381.62s
                               ETA: 7670.0s

################################################################################
                     [1m Learning iteration 2301/4000 [0m

                       Computation: 1819 steps/s (collection: 0.600s, learning 3.902s)
               Value function loss: 165.7264
                    Surrogate loss: 0.0086
             Mean action noise std: 0.94
                       Mean reward: 762.40
               Mean episode length: 442.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 4.50s
                        Total time: 10386.13s
                               ETA: 7665.5s

################################################################################
                     [1m Learning iteration 2302/4000 [0m

                       Computation: 1799 steps/s (collection: 0.609s, learning 3.944s)
               Value function loss: 276.7962
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 765.08
               Mean episode length: 442.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18866176
                    Iteration time: 4.55s
                        Total time: 10390.68s
                               ETA: 7661.0s

################################################################################
                     [1m Learning iteration 2303/4000 [0m

                       Computation: 1797 steps/s (collection: 0.602s, learning 3.955s)
               Value function loss: 195.7515
                    Surrogate loss: 0.0077
             Mean action noise std: 0.94
                       Mean reward: 761.95
               Mean episode length: 441.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 4.56s
                        Total time: 10395.24s
                               ETA: 7656.6s

################################################################################
                     [1m Learning iteration 2304/4000 [0m

                       Computation: 1831 steps/s (collection: 0.568s, learning 3.905s)
               Value function loss: 271.6060
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 755.28
               Mean episode length: 437.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18882560
                    Iteration time: 4.47s
                        Total time: 10399.71s
                               ETA: 7652.0s

################################################################################
                     [1m Learning iteration 2305/4000 [0m

                       Computation: 1816 steps/s (collection: 0.573s, learning 3.936s)
               Value function loss: 185.1018
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 753.35
               Mean episode length: 435.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 4.51s
                        Total time: 10404.22s
                               ETA: 7647.5s

################################################################################
                     [1m Learning iteration 2306/4000 [0m

                       Computation: 1834 steps/s (collection: 0.553s, learning 3.914s)
               Value function loss: 135.6558
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 729.86
               Mean episode length: 423.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18898944
                    Iteration time: 4.47s
                        Total time: 10408.69s
                               ETA: 7643.0s

################################################################################
                     [1m Learning iteration 2307/4000 [0m

                       Computation: 1812 steps/s (collection: 0.576s, learning 3.945s)
               Value function loss: 199.5633
                    Surrogate loss: 0.0052
             Mean action noise std: 0.94
                       Mean reward: 728.33
               Mean episode length: 423.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 4.52s
                        Total time: 10413.21s
                               ETA: 7638.5s

################################################################################
                     [1m Learning iteration 2308/4000 [0m

                       Computation: 1821 steps/s (collection: 0.584s, learning 3.913s)
               Value function loss: 251.5135
                    Surrogate loss: 0.0083
             Mean action noise std: 0.94
                       Mean reward: 741.31
               Mean episode length: 431.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18915328
                    Iteration time: 4.50s
                        Total time: 10417.70s
                               ETA: 7633.9s

################################################################################
                     [1m Learning iteration 2309/4000 [0m

                       Computation: 1800 steps/s (collection: 0.586s, learning 3.964s)
               Value function loss: 254.5280
                    Surrogate loss: 0.0074
             Mean action noise std: 0.94
                       Mean reward: 729.16
               Mean episode length: 424.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 4.55s
                        Total time: 10422.25s
                               ETA: 7629.5s

################################################################################
                     [1m Learning iteration 2310/4000 [0m

                       Computation: 1842 steps/s (collection: 0.553s, learning 3.892s)
               Value function loss: 243.1260
                    Surrogate loss: 0.0112
             Mean action noise std: 0.94
                       Mean reward: 735.35
               Mean episode length: 427.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 18931712
                    Iteration time: 4.44s
                        Total time: 10426.70s
                               ETA: 7624.9s

################################################################################
                     [1m Learning iteration 2311/4000 [0m

                       Computation: 1819 steps/s (collection: 0.561s, learning 3.942s)
               Value function loss: 282.9512
                    Surrogate loss: 0.0073
             Mean action noise std: 0.94
                       Mean reward: 721.37
               Mean episode length: 420.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 4.50s
                        Total time: 10431.20s
                               ETA: 7620.4s

################################################################################
                     [1m Learning iteration 2312/4000 [0m

                       Computation: 1806 steps/s (collection: 0.587s, learning 3.948s)
               Value function loss: 244.6314
                    Surrogate loss: 0.0067
             Mean action noise std: 0.94
                       Mean reward: 728.78
               Mean episode length: 425.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18948096
                    Iteration time: 4.53s
                        Total time: 10435.74s
                               ETA: 7615.9s

################################################################################
                     [1m Learning iteration 2313/4000 [0m

                       Computation: 1830 steps/s (collection: 0.550s, learning 3.925s)
               Value function loss: 145.7765
                    Surrogate loss: 0.0071
             Mean action noise std: 0.94
                       Mean reward: 734.04
               Mean episode length: 428.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 4.48s
                        Total time: 10440.21s
                               ETA: 7611.3s

################################################################################
                     [1m Learning iteration 2314/4000 [0m

                       Computation: 1814 steps/s (collection: 0.554s, learning 3.959s)
               Value function loss: 219.8915
                    Surrogate loss: 0.0079
             Mean action noise std: 0.94
                       Mean reward: 722.28
               Mean episode length: 423.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18964480
                    Iteration time: 4.51s
                        Total time: 10444.72s
                               ETA: 7606.8s

################################################################################
                     [1m Learning iteration 2315/4000 [0m

                       Computation: 1830 steps/s (collection: 0.548s, learning 3.927s)
               Value function loss: 211.0999
                    Surrogate loss: 0.0098
             Mean action noise std: 0.94
                       Mean reward: 713.20
               Mean episode length: 417.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 4.47s
                        Total time: 10449.20s
                               ETA: 7602.3s

################################################################################
                     [1m Learning iteration 2316/4000 [0m

                       Computation: 1827 steps/s (collection: 0.571s, learning 3.911s)
               Value function loss: 143.4728
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 706.21
               Mean episode length: 412.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18980864
                    Iteration time: 4.48s
                        Total time: 10453.68s
                               ETA: 7597.8s

################################################################################
                     [1m Learning iteration 2317/4000 [0m

                       Computation: 1836 steps/s (collection: 0.529s, learning 3.932s)
               Value function loss: 201.9050
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 684.60
               Mean episode length: 400.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 4.46s
                        Total time: 10458.14s
                               ETA: 7593.2s

################################################################################
                     [1m Learning iteration 2318/4000 [0m

                       Computation: 1813 steps/s (collection: 0.574s, learning 3.944s)
               Value function loss: 176.6438
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 668.91
               Mean episode length: 390.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18997248
                    Iteration time: 4.52s
                        Total time: 10462.66s
                               ETA: 7588.7s

################################################################################
                     [1m Learning iteration 2319/4000 [0m

                       Computation: 1816 steps/s (collection: 0.533s, learning 3.978s)
               Value function loss: 164.2810
                    Surrogate loss: 0.0079
             Mean action noise std: 0.94
                       Mean reward: 688.51
               Mean episode length: 401.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 4.51s
                        Total time: 10467.17s
                               ETA: 7584.2s

################################################################################
                     [1m Learning iteration 2320/4000 [0m

                       Computation: 1825 steps/s (collection: 0.559s, learning 3.930s)
               Value function loss: 253.5861
                    Surrogate loss: 0.0072
             Mean action noise std: 0.94
                       Mean reward: 683.37
               Mean episode length: 398.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 19013632
                    Iteration time: 4.49s
                        Total time: 10471.66s
                               ETA: 7579.7s

################################################################################
                     [1m Learning iteration 2321/4000 [0m

                       Computation: 1813 steps/s (collection: 0.566s, learning 3.951s)
               Value function loss: 165.0673
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 679.57
               Mean episode length: 396.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 4.52s
                        Total time: 10476.18s
                               ETA: 7575.2s

################################################################################
                     [1m Learning iteration 2322/4000 [0m

                       Computation: 1812 steps/s (collection: 0.556s, learning 3.964s)
               Value function loss: 250.6200
                    Surrogate loss: 0.0091
             Mean action noise std: 0.94
                       Mean reward: 675.95
               Mean episode length: 393.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19030016
                    Iteration time: 4.52s
                        Total time: 10480.70s
                               ETA: 7570.6s

################################################################################
                     [1m Learning iteration 2323/4000 [0m

                       Computation: 1815 steps/s (collection: 0.546s, learning 3.968s)
               Value function loss: 181.3100
                    Surrogate loss: 0.0075
             Mean action noise std: 0.94
                       Mean reward: 658.18
               Mean episode length: 384.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 4.51s
                        Total time: 10485.21s
                               ETA: 7566.1s

################################################################################
                     [1m Learning iteration 2324/4000 [0m

                       Computation: 1805 steps/s (collection: 0.589s, learning 3.948s)
               Value function loss: 273.3691
                    Surrogate loss: 0.0091
             Mean action noise std: 0.94
                       Mean reward: 652.07
               Mean episode length: 379.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19046400
                    Iteration time: 4.54s
                        Total time: 10489.75s
                               ETA: 7561.6s

################################################################################
                     [1m Learning iteration 2325/4000 [0m

                       Computation: 1836 steps/s (collection: 0.518s, learning 3.943s)
               Value function loss: 229.3335
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 677.41
               Mean episode length: 395.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 4.46s
                        Total time: 10494.21s
                               ETA: 7557.1s

################################################################################
                     [1m Learning iteration 2326/4000 [0m

                       Computation: 1830 steps/s (collection: 0.562s, learning 3.914s)
               Value function loss: 259.8486
                    Surrogate loss: 0.0103
             Mean action noise std: 0.94
                       Mean reward: 672.02
               Mean episode length: 392.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 19062784
                    Iteration time: 4.48s
                        Total time: 10498.68s
                               ETA: 7552.6s

################################################################################
                     [1m Learning iteration 2327/4000 [0m

                       Computation: 1818 steps/s (collection: 0.569s, learning 3.935s)
               Value function loss: 292.2836
                    Surrogate loss: 0.0110
             Mean action noise std: 0.94
                       Mean reward: 683.70
               Mean episode length: 400.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 4.50s
                        Total time: 10503.19s
                               ETA: 7548.0s

################################################################################
                     [1m Learning iteration 2328/4000 [0m

                       Computation: 1818 steps/s (collection: 0.598s, learning 3.906s)
               Value function loss: 175.3084
                    Surrogate loss: 0.0084
             Mean action noise std: 0.94
                       Mean reward: 666.75
               Mean episode length: 389.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19079168
                    Iteration time: 4.50s
                        Total time: 10507.69s
                               ETA: 7543.5s

################################################################################
                     [1m Learning iteration 2329/4000 [0m

                       Computation: 1832 steps/s (collection: 0.558s, learning 3.911s)
               Value function loss: 148.4603
                    Surrogate loss: 0.0095
             Mean action noise std: 0.94
                       Mean reward: 650.79
               Mean episode length: 380.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 4.47s
                        Total time: 10512.16s
                               ETA: 7539.0s

################################################################################
                     [1m Learning iteration 2330/4000 [0m

                       Computation: 1859 steps/s (collection: 0.573s, learning 3.833s)
               Value function loss: 122.2159
                    Surrogate loss: 0.0085
             Mean action noise std: 0.94
                       Mean reward: 639.99
               Mean episode length: 374.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19095552
                    Iteration time: 4.41s
                        Total time: 10516.57s
                               ETA: 7534.4s

################################################################################
                     [1m Learning iteration 2331/4000 [0m

                       Computation: 1827 steps/s (collection: 0.566s, learning 3.917s)
               Value function loss: 291.5183
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 631.06
               Mean episode length: 368.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 4.48s
                        Total time: 10521.05s
                               ETA: 7529.9s

################################################################################
                     [1m Learning iteration 2332/4000 [0m

                       Computation: 1826 steps/s (collection: 0.604s, learning 3.882s)
               Value function loss: 171.3197
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 628.67
               Mean episode length: 367.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19111936
                    Iteration time: 4.49s
                        Total time: 10525.54s
                               ETA: 7525.3s
Traceback (most recent call last):
  File "tools/train_ppo.py", line 51, in <module>
    train()
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "tools/train_ppo.py", line 47, in train
    ppo.run(num_learning_iterations=max_iterations, log_interval=cfg.train.learn.save_interval)
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/mvp/ppo/ppo.py", line 267, in run
    mean_value_loss, mean_surrogate_loss = self.update(it, num_learning_iterations)
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/mvp/ppo/ppo.py", line 415, in update
    mean_value_loss += value_loss.item()
KeyboardInterrupt
