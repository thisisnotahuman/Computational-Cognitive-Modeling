PopSpikeActor(
  (encoder): PopSpikeEncoderRegularSpike()
  (snn): SpikeMLP(
    (hidden_layers): ModuleList(
      (0): Linear(in_features=340, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=64, bias=True)
    )
    (out_pop_layer): Linear(in_features=64, out_features=90, bias=True)
  )
  (decoder): PopSpikeDecoder(
    (decoder): Conv1d(9, 9, kernel_size=(10,), stride=(1,), groups=9)
    (output_activation): ELU(alpha=1.0)
  )
)
Sequential(
  (0): Linear(in_features=34, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=1, bias=True)
)
################################################################################
                      [1m Learning iteration 0/4000 [0m

                       Computation: 2718 steps/s (collection: 0.719s, learning 2.295s)
               Value function loss: 3.1210
                    Surrogate loss: 0.0087
             Mean action noise std: 1.00
                       Mean reward: 4.82
               Mean episode length: 15.40
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 22.38
--------------------------------------------------------------------------------
                   Total timesteps: 8192
                    Iteration time: 3.01s
                        Total time: 3.01s
                               ETA: 12054.6s

################################################################################
                      [1m Learning iteration 1/4000 [0m

                       Computation: 3206 steps/s (collection: 0.455s, learning 2.100s)
               Value function loss: 3.6868
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 6.61
               Mean episode length: 23.39
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 13.98
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 2.56s
                        Total time: 5.57s
                               ETA: 11134.7s

################################################################################
                      [1m Learning iteration 2/4000 [0m

                       Computation: 3192 steps/s (collection: 0.482s, learning 2.084s)
               Value function loss: 3.9651
                    Surrogate loss: 0.0092
             Mean action noise std: 1.00
                       Mean reward: 6.67
               Mean episode length: 22.84
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 14.42
--------------------------------------------------------------------------------
                   Total timesteps: 24576
                    Iteration time: 2.57s
                        Total time: 8.13s
                               ETA: 10840.5s

################################################################################
                      [1m Learning iteration 3/4000 [0m

                       Computation: 3228 steps/s (collection: 0.469s, learning 2.069s)
               Value function loss: 4.0067
                    Surrogate loss: 0.0073
             Mean action noise std: 1.00
                       Mean reward: 7.27
               Mean episode length: 25.61
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 16.13
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 2.54s
                        Total time: 10.67s
                               ETA: 10664.0s

################################################################################
                      [1m Learning iteration 4/4000 [0m

                       Computation: 3253 steps/s (collection: 0.454s, learning 2.063s)
               Value function loss: 4.0719
                    Surrogate loss: 0.0073
             Mean action noise std: 1.00
                       Mean reward: 7.38
               Mean episode length: 26.54
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 15.97
--------------------------------------------------------------------------------
                   Total timesteps: 40960
                    Iteration time: 2.52s
                        Total time: 13.19s
                               ETA: 10541.1s

################################################################################
                      [1m Learning iteration 5/4000 [0m

                       Computation: 3003 steps/s (collection: 0.545s, learning 2.182s)
               Value function loss: 3.6348
                    Surrogate loss: 0.0082
             Mean action noise std: 1.00
                       Mean reward: 9.51
               Mean episode length: 33.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 18.08
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 2.73s
                        Total time: 15.92s
                               ETA: 10598.2s

################################################################################
                      [1m Learning iteration 6/4000 [0m

                       Computation: 3188 steps/s (collection: 0.495s, learning 2.074s)
               Value function loss: 3.8076
                    Surrogate loss: 0.0064
             Mean action noise std: 1.00
                       Mean reward: 8.20
               Mean episode length: 31.40
                 Mean success rate: 0.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 18.70
--------------------------------------------------------------------------------
                   Total timesteps: 57344
                    Iteration time: 2.57s
                        Total time: 18.49s
                               ETA: 10547.7s

################################################################################
                      [1m Learning iteration 7/4000 [0m

                       Computation: 3245 steps/s (collection: 0.456s, learning 2.068s)
               Value function loss: 2.8666
                    Surrogate loss: 0.0074
             Mean action noise std: 1.00
                       Mean reward: 8.66
               Mean episode length: 29.23
                 Mean success rate: 0.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 21.73
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 2.52s
                        Total time: 21.01s
                               ETA: 10486.7s

################################################################################
                      [1m Learning iteration 8/4000 [0m

                       Computation: 3221 steps/s (collection: 0.473s, learning 2.069s)
               Value function loss: 2.6061
                    Surrogate loss: 0.0066
             Mean action noise std: 1.00
                       Mean reward: 10.56
               Mean episode length: 38.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 23.81
--------------------------------------------------------------------------------
                   Total timesteps: 73728
                    Iteration time: 2.54s
                        Total time: 23.55s
                               ETA: 10447.0s

################################################################################
                      [1m Learning iteration 9/4000 [0m

                       Computation: 3124 steps/s (collection: 0.485s, learning 2.137s)
               Value function loss: 3.7281
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 11.31
               Mean episode length: 43.70
                 Mean success rate: 0.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 24.31
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 2.62s
                        Total time: 26.17s
                               ETA: 10446.2s

################################################################################
                      [1m Learning iteration 10/4000 [0m

                       Computation: 3127 steps/s (collection: 0.495s, learning 2.125s)
               Value function loss: 2.4565
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 11.99
               Mean episode length: 45.73
                 Mean success rate: 0.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 90112
                    Iteration time: 2.62s
                        Total time: 28.79s
                               ETA: 10444.4s

################################################################################
                      [1m Learning iteration 11/4000 [0m

                       Computation: 3099 steps/s (collection: 0.476s, learning 2.167s)
               Value function loss: 2.1462
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 12.91
               Mean episode length: 51.77
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 2.64s
                        Total time: 31.44s
                               ETA: 10450.1s

################################################################################
                      [1m Learning iteration 12/4000 [0m

                       Computation: 3156 steps/s (collection: 0.496s, learning 2.100s)
               Value function loss: 1.9442
                    Surrogate loss: 0.0093
             Mean action noise std: 1.00
                       Mean reward: 13.84
               Mean episode length: 57.35
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 106496
                    Iteration time: 2.60s
                        Total time: 34.03s
                               ETA: 10440.0s

################################################################################
                      [1m Learning iteration 13/4000 [0m

                       Computation: 3190 steps/s (collection: 0.457s, learning 2.110s)
               Value function loss: 3.6422
                    Surrogate loss: 0.0074
             Mean action noise std: 1.00
                       Mean reward: 16.59
               Mean episode length: 71.30
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 2.57s
                        Total time: 36.60s
                               ETA: 10423.0s

################################################################################
                      [1m Learning iteration 14/4000 [0m

                       Computation: 3216 steps/s (collection: 0.446s, learning 2.100s)
               Value function loss: 2.8422
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 19.37
               Mean episode length: 88.53
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 122880
                    Iteration time: 2.55s
                        Total time: 39.15s
                               ETA: 10402.5s

################################################################################
                      [1m Learning iteration 15/4000 [0m

                       Computation: 3239 steps/s (collection: 0.432s, learning 2.097s)
               Value function loss: 2.5039
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 20.98
               Mean episode length: 99.42
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 2.53s
                        Total time: 41.68s
                               ETA: 10379.7s

################################################################################
                      [1m Learning iteration 16/4000 [0m

                       Computation: 3103 steps/s (collection: 0.515s, learning 2.125s)
               Value function loss: 2.8604
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 23.96
               Mean episode length: 117.46
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 139264
                    Iteration time: 2.64s
                        Total time: 44.31s
                               ETA: 10385.3s

################################################################################
                      [1m Learning iteration 17/4000 [0m

                       Computation: 3227 steps/s (collection: 0.479s, learning 2.060s)
               Value function loss: 3.4890
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 27.17
               Mean episode length: 136.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 2.54s
                        Total time: 46.85s
                               ETA: 10367.5s

################################################################################
                      [1m Learning iteration 18/4000 [0m

                       Computation: 3240 steps/s (collection: 0.444s, learning 2.084s)
               Value function loss: 1.7259
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 27.69
               Mean episode length: 140.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 155648
                    Iteration time: 2.53s
                        Total time: 49.38s
                               ETA: 10349.2s

################################################################################
                      [1m Learning iteration 19/4000 [0m

                       Computation: 3178 steps/s (collection: 0.469s, learning 2.109s)
               Value function loss: 2.9956
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 31.89
               Mean episode length: 165.68
                 Mean success rate: 0.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 2.58s
                        Total time: 51.96s
                               ETA: 10342.3s

################################################################################
                      [1m Learning iteration 20/4000 [0m

                       Computation: 3249 steps/s (collection: 0.405s, learning 2.115s)
               Value function loss: 2.7829
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 35.45
               Mean episode length: 187.07
                 Mean success rate: 0.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 172032
                    Iteration time: 2.52s
                        Total time: 54.48s
                               ETA: 10325.1s

################################################################################
                      [1m Learning iteration 21/4000 [0m

                       Computation: 3232 steps/s (collection: 0.438s, learning 2.097s)
               Value function loss: 3.6173
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 40.57
               Mean episode length: 216.37
                 Mean success rate: 0.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 2.53s
                        Total time: 57.01s
                               ETA: 10311.7s

################################################################################
                      [1m Learning iteration 22/4000 [0m

                       Computation: 3131 steps/s (collection: 0.489s, learning 2.128s)
               Value function loss: 5.6167
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 43.82
               Mean episode length: 232.00
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 188416
                    Iteration time: 2.62s
                        Total time: 59.63s
                               ETA: 10313.4s

################################################################################
                      [1m Learning iteration 23/4000 [0m

                       Computation: 3136 steps/s (collection: 0.473s, learning 2.139s)
               Value function loss: 4.5944
                    Surrogate loss: 0.0031
             Mean action noise std: 1.00
                       Mean reward: 45.48
               Mean episode length: 243.76
                 Mean success rate: 0.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 2.61s
                        Total time: 62.24s
                               ETA: 10314.0s

################################################################################
                      [1m Learning iteration 24/4000 [0m

                       Computation: 3307 steps/s (collection: 0.458s, learning 2.019s)
               Value function loss: 7.9069
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 44.60
               Mean episode length: 237.18
                 Mean success rate: 0.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 204800
                    Iteration time: 2.48s
                        Total time: 64.72s
                               ETA: 10292.9s

################################################################################
                      [1m Learning iteration 25/4000 [0m

                       Computation: 3278 steps/s (collection: 0.473s, learning 2.026s)
               Value function loss: 12.6362
                    Surrogate loss: 0.0074
             Mean action noise std: 1.00
                       Mean reward: 36.24
               Mean episode length: 182.59
                 Mean success rate: 0.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 25.36
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 2.50s
                        Total time: 67.22s
                               ETA: 10276.6s

################################################################################
                      [1m Learning iteration 26/4000 [0m

                       Computation: 3234 steps/s (collection: 0.482s, learning 2.050s)
               Value function loss: 9.5032
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 34.07
               Mean episode length: 170.67
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 221184
                    Iteration time: 2.53s
                        Total time: 69.75s
                               ETA: 10266.2s

################################################################################
                      [1m Learning iteration 27/4000 [0m

                       Computation: 3238 steps/s (collection: 0.468s, learning 2.061s)
               Value function loss: 10.4443
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 30.71
               Mean episode length: 146.14
                 Mean success rate: 0.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 2.53s
                        Total time: 72.28s
                               ETA: 10256.0s

################################################################################
                      [1m Learning iteration 28/4000 [0m

                       Computation: 3254 steps/s (collection: 0.465s, learning 2.052s)
               Value function loss: 9.3100
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 31.46
               Mean episode length: 144.67
                 Mean success rate: 0.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 237568
                    Iteration time: 2.52s
                        Total time: 74.80s
                               ETA: 10244.6s

################################################################################
                      [1m Learning iteration 29/4000 [0m

                       Computation: 3247 steps/s (collection: 0.471s, learning 2.052s)
               Value function loss: 10.8540
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 37.39
               Mean episode length: 167.35
                 Mean success rate: 0.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 2.52s
                        Total time: 77.32s
                               ETA: 10234.6s

################################################################################
                      [1m Learning iteration 30/4000 [0m

                       Computation: 3280 steps/s (collection: 0.455s, learning 2.042s)
               Value function loss: 8.2519
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 41.60
               Mean episode length: 186.66
                 Mean success rate: 0.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 253952
                    Iteration time: 2.50s
                        Total time: 79.82s
                               ETA: 10221.8s

################################################################################
                      [1m Learning iteration 31/4000 [0m

                       Computation: 3227 steps/s (collection: 0.465s, learning 2.073s)
               Value function loss: 9.3753
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 45.82
               Mean episode length: 201.53
                 Mean success rate: 0.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 2.54s
                        Total time: 82.36s
                               ETA: 10214.7s

################################################################################
                      [1m Learning iteration 32/4000 [0m

                       Computation: 3345 steps/s (collection: 0.425s, learning 2.024s)
               Value function loss: 9.4969
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 54.36
               Mean episode length: 236.67
                 Mean success rate: 0.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 270336
                    Iteration time: 2.45s
                        Total time: 84.80s
                               ETA: 10197.1s

################################################################################
                      [1m Learning iteration 33/4000 [0m

                       Computation: 3326 steps/s (collection: 0.449s, learning 2.014s)
               Value function loss: 8.4732
                    Surrogate loss: 0.0042
             Mean action noise std: 1.00
                       Mean reward: 58.37
               Mean episode length: 248.81
                 Mean success rate: 0.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 2.46s
                        Total time: 87.27s
                               ETA: 10182.1s

################################################################################
                      [1m Learning iteration 34/4000 [0m

                       Computation: 3314 steps/s (collection: 0.456s, learning 2.016s)
               Value function loss: 13.3179
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 66.70
               Mean episode length: 267.96
                 Mean success rate: 0.00
                  Mean reward/step: 0.37
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 286720
                    Iteration time: 2.47s
                        Total time: 89.74s
                               ETA: 10168.7s

################################################################################
                      [1m Learning iteration 35/4000 [0m

                       Computation: 3333 steps/s (collection: 0.422s, learning 2.036s)
               Value function loss: 8.8660
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 72.93
               Mean episode length: 283.02
                 Mean success rate: 0.00
                  Mean reward/step: 0.37
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 2.46s
                        Total time: 92.20s
                               ETA: 10154.4s

################################################################################
                      [1m Learning iteration 36/4000 [0m

                       Computation: 3289 steps/s (collection: 0.434s, learning 2.056s)
               Value function loss: 7.4570
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 75.13
               Mean episode length: 284.31
                 Mean success rate: 0.00
                  Mean reward/step: 0.37
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 303104
                    Iteration time: 2.49s
                        Total time: 94.69s
                               ETA: 10144.3s

################################################################################
                      [1m Learning iteration 37/4000 [0m

                       Computation: 3246 steps/s (collection: 0.451s, learning 2.072s)
               Value function loss: 14.4433
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 76.48
               Mean episode length: 281.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.37
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 2.52s
                        Total time: 97.21s
                               ETA: 10138.0s

################################################################################
                      [1m Learning iteration 38/4000 [0m

                       Computation: 3303 steps/s (collection: 0.418s, learning 2.062s)
               Value function loss: 14.6646
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 78.30
               Mean episode length: 274.65
                 Mean success rate: 0.00
                  Mean reward/step: 0.37
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 319488
                    Iteration time: 2.48s
                        Total time: 99.69s
                               ETA: 10127.5s

################################################################################
                      [1m Learning iteration 39/4000 [0m

                       Computation: 3260 steps/s (collection: 0.458s, learning 2.054s)
               Value function loss: 18.7519
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 83.33
               Mean episode length: 280.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.38
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 2.51s
                        Total time: 102.20s
                               ETA: 10120.6s

################################################################################
                      [1m Learning iteration 40/4000 [0m

                       Computation: 3248 steps/s (collection: 0.454s, learning 2.067s)
               Value function loss: 15.9568
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 85.85
               Mean episode length: 278.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.38
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 335872
                    Iteration time: 2.52s
                        Total time: 104.72s
                               ETA: 10114.8s

################################################################################
                      [1m Learning iteration 41/4000 [0m

                       Computation: 3260 steps/s (collection: 0.457s, learning 2.055s)
               Value function loss: 15.8677
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 91.99
               Mean episode length: 285.18
                 Mean success rate: 0.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 2.51s
                        Total time: 107.24s
                               ETA: 10108.3s

################################################################################
                      [1m Learning iteration 42/4000 [0m

                       Computation: 3185 steps/s (collection: 0.514s, learning 2.058s)
               Value function loss: 20.6363
                    Surrogate loss: 0.0037
             Mean action noise std: 1.00
                       Mean reward: 97.36
               Mean episode length: 293.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.38
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 352256
                    Iteration time: 2.57s
                        Total time: 109.81s
                               ETA: 10107.5s

################################################################################
                      [1m Learning iteration 43/4000 [0m

                       Computation: 3336 steps/s (collection: 0.410s, learning 2.046s)
               Value function loss: 21.0950
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 107.44
               Mean episode length: 316.31
                 Mean success rate: 0.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 2.46s
                        Total time: 112.26s
                               ETA: 10096.1s

################################################################################
                      [1m Learning iteration 44/4000 [0m

                       Computation: 3250 steps/s (collection: 0.441s, learning 2.080s)
               Value function loss: 18.2554
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 113.09
               Mean episode length: 327.64
                 Mean success rate: 0.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 368640
                    Iteration time: 2.52s
                        Total time: 114.78s
                               ETA: 10090.8s

################################################################################
                      [1m Learning iteration 45/4000 [0m

                       Computation: 3206 steps/s (collection: 0.484s, learning 2.070s)
               Value function loss: 18.6624
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 123.11
               Mean episode length: 347.99
                 Mean success rate: 0.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 2.55s
                        Total time: 117.34s
                               ETA: 10088.6s

################################################################################
                      [1m Learning iteration 46/4000 [0m

                       Computation: 3252 steps/s (collection: 0.438s, learning 2.081s)
               Value function loss: 18.0881
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 132.63
               Mean episode length: 370.98
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 385024
                    Iteration time: 2.52s
                        Total time: 119.86s
                               ETA: 10083.4s

################################################################################
                      [1m Learning iteration 47/4000 [0m

                       Computation: 3234 steps/s (collection: 0.441s, learning 2.091s)
               Value function loss: 19.2982
                    Surrogate loss: 0.0018
             Mean action noise std: 1.00
                       Mean reward: 132.12
               Mean episode length: 362.76
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 2.53s
                        Total time: 122.39s
                               ETA: 10079.3s

################################################################################
                      [1m Learning iteration 48/4000 [0m

                       Computation: 3244 steps/s (collection: 0.462s, learning 2.063s)
               Value function loss: 16.8566
                    Surrogate loss: 0.0020
             Mean action noise std: 1.00
                       Mean reward: 139.52
               Mean episode length: 377.42
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 401408
                    Iteration time: 2.53s
                        Total time: 124.92s
                               ETA: 10074.8s

################################################################################
                      [1m Learning iteration 49/4000 [0m

                       Computation: 3185 steps/s (collection: 0.526s, learning 2.046s)
               Value function loss: 20.6880
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 136.59
               Mean episode length: 365.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.44
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 2.57s
                        Total time: 127.49s
                               ETA: 10074.0s

################################################################################
                      [1m Learning iteration 50/4000 [0m

                       Computation: 3109 steps/s (collection: 0.513s, learning 2.122s)
               Value function loss: 21.7590
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 137.27
               Mean episode length: 362.80
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 417792
                    Iteration time: 2.63s
                        Total time: 130.12s
                               ETA: 10078.1s

################################################################################
                      [1m Learning iteration 51/4000 [0m

                       Computation: 3284 steps/s (collection: 0.429s, learning 2.064s)
               Value function loss: 20.2091
                    Surrogate loss: 0.0039
             Mean action noise std: 1.00
                       Mean reward: 126.26
               Mean episode length: 334.39
                 Mean success rate: 0.00
                  Mean reward/step: 0.46
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 2.49s
                        Total time: 132.62s
                               ETA: 10071.2s

################################################################################
                      [1m Learning iteration 52/4000 [0m

                       Computation: 3233 steps/s (collection: 0.460s, learning 2.074s)
               Value function loss: 27.9458
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 115.87
               Mean episode length: 300.99
                 Mean success rate: 0.00
                  Mean reward/step: 0.46
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 434176
                    Iteration time: 2.53s
                        Total time: 135.15s
                               ETA: 10067.4s

################################################################################
                      [1m Learning iteration 53/4000 [0m

                       Computation: 3291 steps/s (collection: 0.452s, learning 2.037s)
               Value function loss: 22.6225
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 106.36
               Mean episode length: 270.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 2.49s
                        Total time: 137.64s
                               ETA: 10060.3s

################################################################################
                      [1m Learning iteration 54/4000 [0m

                       Computation: 3253 steps/s (collection: 0.466s, learning 2.052s)
               Value function loss: 23.5247
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 107.82
               Mean episode length: 269.74
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 450560
                    Iteration time: 2.52s
                        Total time: 140.16s
                               ETA: 10055.6s

################################################################################
                      [1m Learning iteration 55/4000 [0m

                       Computation: 3227 steps/s (collection: 0.468s, learning 2.071s)
               Value function loss: 24.7705
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 108.42
               Mean episode length: 270.37
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 2.54s
                        Total time: 142.69s
                               ETA: 10052.3s

################################################################################
                      [1m Learning iteration 56/4000 [0m

                       Computation: 3174 steps/s (collection: 0.506s, learning 2.074s)
               Value function loss: 41.3269
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 101.63
               Mean episode length: 246.87
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 466944
                    Iteration time: 2.58s
                        Total time: 145.27s
                               ETA: 10052.0s

################################################################################
                      [1m Learning iteration 57/4000 [0m

                       Computation: 3224 steps/s (collection: 0.483s, learning 2.058s)
               Value function loss: 32.0876
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 103.48
               Mean episode length: 244.87
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 2.54s
                        Total time: 147.82s
                               ETA: 10048.9s

################################################################################
                      [1m Learning iteration 58/4000 [0m

                       Computation: 3249 steps/s (collection: 0.469s, learning 2.052s)
               Value function loss: 36.9978
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 112.20
               Mean episode length: 255.60
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 483328
                    Iteration time: 2.52s
                        Total time: 150.34s
                               ETA: 10044.5s

################################################################################
                      [1m Learning iteration 59/4000 [0m

                       Computation: 3207 steps/s (collection: 0.474s, learning 2.080s)
               Value function loss: 34.0805
                    Surrogate loss: 0.0028
             Mean action noise std: 1.00
                       Mean reward: 118.38
               Mean episode length: 265.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 2.55s
                        Total time: 152.89s
                               ETA: 10042.3s

################################################################################
                      [1m Learning iteration 60/4000 [0m

                       Computation: 3189 steps/s (collection: 0.482s, learning 2.087s)
               Value function loss: 48.1883
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 115.78
               Mean episode length: 247.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 499712
                    Iteration time: 2.57s
                        Total time: 155.46s
                               ETA: 10041.1s

################################################################################
                      [1m Learning iteration 61/4000 [0m

                       Computation: 3249 steps/s (collection: 0.462s, learning 2.060s)
               Value function loss: 51.7854
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 116.78
               Mean episode length: 241.03
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 2.52s
                        Total time: 157.98s
                               ETA: 10036.8s

################################################################################
                      [1m Learning iteration 62/4000 [0m

                       Computation: 3262 steps/s (collection: 0.482s, learning 2.029s)
               Value function loss: 61.5562
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 97.21
               Mean episode length: 196.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 25.05
--------------------------------------------------------------------------------
                   Total timesteps: 516096
                    Iteration time: 2.51s
                        Total time: 160.49s
                               ETA: 10032.0s

################################################################################
                      [1m Learning iteration 63/4000 [0m

                       Computation: 3171 steps/s (collection: 0.517s, learning 2.067s)
               Value function loss: 57.2545
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 74.13
               Mean episode length: 147.29
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 24.98
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 2.58s
                        Total time: 163.07s
                               ETA: 10031.6s

################################################################################
                      [1m Learning iteration 64/4000 [0m

                       Computation: 3146 steps/s (collection: 0.498s, learning 2.106s)
               Value function loss: 27.0205
                    Surrogate loss: 0.0042
             Mean action noise std: 1.00
                       Mean reward: 71.90
               Mean episode length: 141.74
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 532480
                    Iteration time: 2.60s
                        Total time: 165.68s
                               ETA: 10032.4s

################################################################################
                      [1m Learning iteration 65/4000 [0m

                       Computation: 3196 steps/s (collection: 0.461s, learning 2.102s)
               Value function loss: 38.8767
                    Surrogate loss: 0.0074
             Mean action noise std: 1.00
                       Mean reward: 80.40
               Mean episode length: 154.19
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 2.56s
                        Total time: 168.24s
                               ETA: 10030.7s

################################################################################
                      [1m Learning iteration 66/4000 [0m

                       Computation: 3222 steps/s (collection: 0.485s, learning 2.057s)
               Value function loss: 50.9029
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 81.46
               Mean episode length: 154.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 25.84
--------------------------------------------------------------------------------
                   Total timesteps: 548864
                    Iteration time: 2.54s
                        Total time: 170.78s
                               ETA: 10027.7s

################################################################################
                      [1m Learning iteration 67/4000 [0m

                       Computation: 3234 steps/s (collection: 0.488s, learning 2.045s)
               Value function loss: 31.5221
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 87.60
               Mean episode length: 169.51
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 2.53s
                        Total time: 173.32s
                               ETA: 10024.2s

################################################################################
                      [1m Learning iteration 68/4000 [0m

                       Computation: 3182 steps/s (collection: 0.466s, learning 2.108s)
               Value function loss: 37.1152
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 92.43
               Mean episode length: 178.85
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 565248
                    Iteration time: 2.57s
                        Total time: 175.89s
                               ETA: 10023.1s

################################################################################
                      [1m Learning iteration 69/4000 [0m

                       Computation: 3207 steps/s (collection: 0.447s, learning 2.107s)
               Value function loss: 29.0488
                    Surrogate loss: 0.0064
             Mean action noise std: 1.00
                       Mean reward: 87.08
               Mean episode length: 169.93
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 2.55s
                        Total time: 178.44s
                               ETA: 10020.9s

################################################################################
                      [1m Learning iteration 70/4000 [0m

                       Computation: 3128 steps/s (collection: 0.501s, learning 2.118s)
               Value function loss: 22.8253
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 82.96
               Mean episode length: 163.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 581632
                    Iteration time: 2.62s
                        Total time: 181.06s
                               ETA: 10022.2s

################################################################################
                      [1m Learning iteration 71/4000 [0m

                       Computation: 3196 steps/s (collection: 0.502s, learning 2.061s)
               Value function loss: 12.1570
                    Surrogate loss: 0.0071
             Mean action noise std: 1.00
                       Mean reward: 87.91
               Mean episode length: 174.96
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 2.56s
                        Total time: 183.63s
                               ETA: 10020.3s

################################################################################
                      [1m Learning iteration 72/4000 [0m

                       Computation: 3281 steps/s (collection: 0.424s, learning 2.072s)
               Value function loss: 24.7861
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 94.04
               Mean episode length: 189.74
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 598016
                    Iteration time: 2.50s
                        Total time: 186.12s
                               ETA: 10014.9s

################################################################################
                      [1m Learning iteration 73/4000 [0m

                       Computation: 3175 steps/s (collection: 0.436s, learning 2.144s)
               Value function loss: 21.8758
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 105.12
               Mean episode length: 208.50
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 2.58s
                        Total time: 188.70s
                               ETA: 10013.9s

################################################################################
                      [1m Learning iteration 74/4000 [0m

                       Computation: 3217 steps/s (collection: 0.471s, learning 2.076s)
               Value function loss: 24.0715
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 108.59
               Mean episode length: 213.42
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 614400
                    Iteration time: 2.55s
                        Total time: 191.25s
                               ETA: 10011.2s

################################################################################
                      [1m Learning iteration 75/4000 [0m

                       Computation: 3290 steps/s (collection: 0.471s, learning 2.018s)
               Value function loss: 29.6965
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 114.55
               Mean episode length: 226.32
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 2.49s
                        Total time: 193.74s
                               ETA: 10005.5s

################################################################################
                      [1m Learning iteration 76/4000 [0m

                       Computation: 3217 steps/s (collection: 0.441s, learning 2.105s)
               Value function loss: 33.8533
                    Surrogate loss: 0.0038
             Mean action noise std: 1.00
                       Mean reward: 118.24
               Mean episode length: 236.59
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 630784
                    Iteration time: 2.55s
                        Total time: 196.28s
                               ETA: 10002.8s

################################################################################
                      [1m Learning iteration 77/4000 [0m

                       Computation: 3172 steps/s (collection: 0.518s, learning 2.064s)
               Value function loss: 36.1370
                    Surrogate loss: 0.0073
             Mean action noise std: 1.00
                       Mean reward: 127.56
               Mean episode length: 255.19
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 2.58s
                        Total time: 198.87s
                               ETA: 10001.9s

################################################################################
                      [1m Learning iteration 78/4000 [0m

                       Computation: 3324 steps/s (collection: 0.425s, learning 2.039s)
               Value function loss: 45.7052
                    Surrogate loss: 0.0066
             Mean action noise std: 1.00
                       Mean reward: 136.73
               Mean episode length: 272.08
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 647168
                    Iteration time: 2.46s
                        Total time: 201.33s
                               ETA: 9995.1s

################################################################################
                      [1m Learning iteration 79/4000 [0m

                       Computation: 3226 steps/s (collection: 0.477s, learning 2.062s)
               Value function loss: 53.5710
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 130.93
               Mean episode length: 260.37
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 2.54s
                        Total time: 203.87s
                               ETA: 9992.1s

################################################################################
                      [1m Learning iteration 80/4000 [0m

                       Computation: 3207 steps/s (collection: 0.480s, learning 2.074s)
               Value function loss: 37.8360
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 128.48
               Mean episode length: 253.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 663552
                    Iteration time: 2.55s
                        Total time: 206.42s
                               ETA: 9989.8s

################################################################################
                      [1m Learning iteration 81/4000 [0m

                       Computation: 3251 steps/s (collection: 0.474s, learning 2.046s)
               Value function loss: 40.5380
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 133.10
               Mean episode length: 255.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 2.52s
                        Total time: 208.94s
                               ETA: 9985.9s

################################################################################
                      [1m Learning iteration 82/4000 [0m

                       Computation: 3258 steps/s (collection: 0.445s, learning 2.069s)
               Value function loss: 37.7121
                    Surrogate loss: 0.0073
             Mean action noise std: 1.00
                       Mean reward: 130.58
               Mean episode length: 249.53
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 679936
                    Iteration time: 2.51s
                        Total time: 211.46s
                               ETA: 9981.7s

################################################################################
                      [1m Learning iteration 83/4000 [0m

                       Computation: 3236 steps/s (collection: 0.472s, learning 2.059s)
               Value function loss: 37.2747
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 133.64
               Mean episode length: 251.48
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 2.53s
                        Total time: 213.99s
                               ETA: 9978.4s

################################################################################
                      [1m Learning iteration 84/4000 [0m

                       Computation: 3250 steps/s (collection: 0.479s, learning 2.042s)
               Value function loss: 45.5029
                    Surrogate loss: 0.0080
             Mean action noise std: 1.00
                       Mean reward: 123.76
               Mean episode length: 231.80
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 696320
                    Iteration time: 2.52s
                        Total time: 216.51s
                               ETA: 9974.6s

################################################################################
                      [1m Learning iteration 85/4000 [0m

                       Computation: 3249 steps/s (collection: 0.474s, learning 2.047s)
               Value function loss: 33.8906
                    Surrogate loss: 0.0081
             Mean action noise std: 1.00
                       Mean reward: 127.54
               Mean episode length: 234.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 2.52s
                        Total time: 219.03s
                               ETA: 9970.9s

################################################################################
                      [1m Learning iteration 86/4000 [0m

                       Computation: 3241 steps/s (collection: 0.469s, learning 2.059s)
               Value function loss: 23.3570
                    Surrogate loss: 0.0079
             Mean action noise std: 1.00
                       Mean reward: 126.59
               Mean episode length: 232.85
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 712704
                    Iteration time: 2.53s
                        Total time: 221.56s
                               ETA: 9967.5s

################################################################################
                      [1m Learning iteration 87/4000 [0m

                       Computation: 3237 steps/s (collection: 0.460s, learning 2.070s)
               Value function loss: 37.6942
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 107.32
               Mean episode length: 199.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 2.53s
                        Total time: 224.09s
                               ETA: 9964.2s

################################################################################
                      [1m Learning iteration 88/4000 [0m

                       Computation: 3266 steps/s (collection: 0.453s, learning 2.055s)
               Value function loss: 31.5923
                    Surrogate loss: 0.0066
             Mean action noise std: 1.00
                       Mean reward: 105.28
               Mean episode length: 194.03
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 729088
                    Iteration time: 2.51s
                        Total time: 226.60s
                               ETA: 9960.0s

################################################################################
                      [1m Learning iteration 89/4000 [0m

                       Computation: 3198 steps/s (collection: 0.473s, learning 2.088s)
               Value function loss: 47.6168
                    Surrogate loss: 0.0071
             Mean action noise std: 1.00
                       Mean reward: 107.70
               Mean episode length: 195.08
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 2.56s
                        Total time: 229.16s
                               ETA: 9958.1s

################################################################################
                      [1m Learning iteration 90/4000 [0m

                       Computation: 3246 steps/s (collection: 0.473s, learning 2.050s)
               Value function loss: 50.7903
                    Surrogate loss: 0.0081
             Mean action noise std: 1.00
                       Mean reward: 108.22
               Mean episode length: 195.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 745472
                    Iteration time: 2.52s
                        Total time: 231.68s
                               ETA: 9954.6s

################################################################################
                      [1m Learning iteration 91/4000 [0m

                       Computation: 3298 steps/s (collection: 0.453s, learning 2.030s)
               Value function loss: 38.7413
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 112.11
               Mean episode length: 205.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 2.48s
                        Total time: 234.16s
                               ETA: 9949.4s

################################################################################
                      [1m Learning iteration 92/4000 [0m

                       Computation: 3202 steps/s (collection: 0.491s, learning 2.067s)
               Value function loss: 56.5482
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 125.01
               Mean episode length: 222.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 761856
                    Iteration time: 2.56s
                        Total time: 236.72s
                               ETA: 9947.4s

################################################################################
                      [1m Learning iteration 93/4000 [0m

                       Computation: 3143 steps/s (collection: 0.470s, learning 2.136s)
               Value function loss: 41.4822
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 127.00
               Mean episode length: 226.74
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 2.61s
                        Total time: 239.33s
                               ETA: 9947.3s

################################################################################
                      [1m Learning iteration 94/4000 [0m

                       Computation: 3246 steps/s (collection: 0.476s, learning 2.047s)
               Value function loss: 56.3242
                    Surrogate loss: 0.0073
             Mean action noise std: 1.00
                       Mean reward: 122.78
               Mean episode length: 223.30
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 778240
                    Iteration time: 2.52s
                        Total time: 241.85s
                               ETA: 9943.9s

################################################################################
                      [1m Learning iteration 95/4000 [0m

                       Computation: 3243 steps/s (collection: 0.462s, learning 2.064s)
               Value function loss: 38.4028
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 123.41
               Mean episode length: 227.00
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 2.53s
                        Total time: 244.38s
                               ETA: 9940.5s

################################################################################
                      [1m Learning iteration 96/4000 [0m

                       Computation: 3259 steps/s (collection: 0.464s, learning 2.049s)
               Value function loss: 40.4282
                    Surrogate loss: 0.0072
             Mean action noise std: 1.00
                       Mean reward: 118.25
               Mean episode length: 213.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 794624
                    Iteration time: 2.51s
                        Total time: 246.89s
                               ETA: 9936.6s

################################################################################
                      [1m Learning iteration 97/4000 [0m

                       Computation: 3193 steps/s (collection: 0.477s, learning 2.088s)
               Value function loss: 37.8125
                    Surrogate loss: 0.0078
             Mean action noise std: 1.00
                       Mean reward: 125.70
               Mean episode length: 228.24
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 2.57s
                        Total time: 249.45s
                               ETA: 9934.9s

################################################################################
                      [1m Learning iteration 98/4000 [0m

                       Computation: 3249 steps/s (collection: 0.463s, learning 2.058s)
               Value function loss: 40.4917
                    Surrogate loss: 0.0064
             Mean action noise std: 1.00
                       Mean reward: 115.42
               Mean episode length: 208.66
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 811008
                    Iteration time: 2.52s
                        Total time: 251.98s
                               ETA: 9931.4s

################################################################################
                      [1m Learning iteration 99/4000 [0m

                       Computation: 3269 steps/s (collection: 0.442s, learning 2.064s)
               Value function loss: 37.5687
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 120.13
               Mean episode length: 214.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 2.51s
                        Total time: 254.48s
                               ETA: 9927.3s

################################################################################
                     [1m Learning iteration 100/4000 [0m

                       Computation: 3182 steps/s (collection: 0.506s, learning 2.068s)
               Value function loss: 36.9008
                    Surrogate loss: 0.0061
             Mean action noise std: 1.00
                       Mean reward: 121.51
               Mean episode length: 216.31
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 827392
                    Iteration time: 2.57s
                        Total time: 257.06s
                               ETA: 9925.9s

################################################################################
                     [1m Learning iteration 101/4000 [0m

                       Computation: 3137 steps/s (collection: 0.555s, learning 2.056s)
               Value function loss: 58.2300
                    Surrogate loss: 0.0073
             Mean action noise std: 1.00
                       Mean reward: 116.75
               Mean episode length: 204.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 2.61s
                        Total time: 259.67s
                               ETA: 9925.9s

################################################################################
                     [1m Learning iteration 102/4000 [0m

                       Computation: 3234 steps/s (collection: 0.477s, learning 2.056s)
               Value function loss: 48.0173
                    Surrogate loss: 0.0072
             Mean action noise std: 1.00
                       Mean reward: 111.48
               Mean episode length: 190.84
                 Mean success rate: 0.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 843776
                    Iteration time: 2.53s
                        Total time: 262.20s
                               ETA: 9922.8s

################################################################################
                     [1m Learning iteration 103/4000 [0m

                       Computation: 3198 steps/s (collection: 0.527s, learning 2.035s)
               Value function loss: 54.4209
                    Surrogate loss: 0.0063
             Mean action noise std: 1.00
                       Mean reward: 109.95
               Mean episode length: 190.24
                 Mean success rate: 0.00
                  Mean reward/step: 0.62
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 2.56s
                        Total time: 264.76s
                               ETA: 9920.9s

################################################################################
                     [1m Learning iteration 104/4000 [0m

                       Computation: 3207 steps/s (collection: 0.486s, learning 2.069s)
               Value function loss: 48.8631
                    Surrogate loss: 0.0066
             Mean action noise std: 1.00
                       Mean reward: 102.90
               Mean episode length: 174.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 860160
                    Iteration time: 2.55s
                        Total time: 267.31s
                               ETA: 9918.7s

################################################################################
                     [1m Learning iteration 105/4000 [0m

                       Computation: 3208 steps/s (collection: 0.480s, learning 2.073s)
               Value function loss: 36.7672
                    Surrogate loss: 0.0061
             Mean action noise std: 1.00
                       Mean reward: 108.98
               Mean episode length: 184.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 2.55s
                        Total time: 269.87s
                               ETA: 9916.4s

################################################################################
                     [1m Learning iteration 106/4000 [0m

                       Computation: 3210 steps/s (collection: 0.507s, learning 2.044s)
               Value function loss: 57.9317
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 116.78
               Mean episode length: 197.22
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 876544
                    Iteration time: 2.55s
                        Total time: 272.42s
                               ETA: 9914.0s

################################################################################
                     [1m Learning iteration 107/4000 [0m

                       Computation: 3180 steps/s (collection: 0.493s, learning 2.083s)
               Value function loss: 37.7331
                    Surrogate loss: 0.0074
             Mean action noise std: 1.00
                       Mean reward: 117.00
               Mean episode length: 202.93
                 Mean success rate: 0.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 2.58s
                        Total time: 274.99s
                               ETA: 9912.5s

################################################################################
                     [1m Learning iteration 108/4000 [0m

                       Computation: 3229 steps/s (collection: 0.492s, learning 2.045s)
               Value function loss: 43.7118
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 120.57
               Mean episode length: 202.46
                 Mean success rate: 0.00
                  Mean reward/step: 0.64
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 892928
                    Iteration time: 2.54s
                        Total time: 277.53s
                               ETA: 9909.7s

################################################################################
                     [1m Learning iteration 109/4000 [0m

                       Computation: 3259 steps/s (collection: 0.461s, learning 2.052s)
               Value function loss: 57.6583
                    Surrogate loss: 0.0082
             Mean action noise std: 1.00
                       Mean reward: 118.99
               Mean episode length: 207.32
                 Mean success rate: 0.00
                  Mean reward/step: 0.63
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 2.51s
                        Total time: 280.04s
                               ETA: 9905.9s

################################################################################
                     [1m Learning iteration 110/4000 [0m

                       Computation: 3215 steps/s (collection: 0.473s, learning 2.075s)
               Value function loss: 59.3484
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 123.29
               Mean episode length: 214.47
                 Mean success rate: 0.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 909312
                    Iteration time: 2.55s
                        Total time: 282.59s
                               ETA: 9903.5s

################################################################################
                     [1m Learning iteration 111/4000 [0m

                       Computation: 3268 steps/s (collection: 0.472s, learning 2.034s)
               Value function loss: 45.2136
                    Surrogate loss: 0.0081
             Mean action noise std: 1.00
                       Mean reward: 121.39
               Mean episode length: 210.65
                 Mean success rate: 0.00
                  Mean reward/step: 0.70
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 2.51s
                        Total time: 285.10s
                               ETA: 9899.5s

################################################################################
                     [1m Learning iteration 112/4000 [0m

                       Computation: 3122 steps/s (collection: 0.508s, learning 2.115s)
               Value function loss: 59.9148
                    Surrogate loss: 0.0068
             Mean action noise std: 1.00
                       Mean reward: 125.72
               Mean episode length: 214.84
                 Mean success rate: 0.00
                  Mean reward/step: 0.68
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 925696
                    Iteration time: 2.62s
                        Total time: 287.72s
                               ETA: 9899.7s

################################################################################
                     [1m Learning iteration 113/4000 [0m

                       Computation: 3169 steps/s (collection: 0.522s, learning 2.063s)
               Value function loss: 54.1919
                    Surrogate loss: 0.0076
             Mean action noise std: 1.00
                       Mean reward: 118.80
               Mean episode length: 199.93
                 Mean success rate: 0.00
                  Mean reward/step: 0.70
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 2.58s
                        Total time: 290.31s
                               ETA: 9898.4s

################################################################################
                     [1m Learning iteration 114/4000 [0m

                       Computation: 3191 steps/s (collection: 0.507s, learning 2.060s)
               Value function loss: 74.5207
                    Surrogate loss: 0.0088
             Mean action noise std: 1.00
                       Mean reward: 112.41
               Mean episode length: 179.41
                 Mean success rate: 0.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 942080
                    Iteration time: 2.57s
                        Total time: 292.87s
                               ETA: 9896.6s

################################################################################
                     [1m Learning iteration 115/4000 [0m

                       Computation: 3131 steps/s (collection: 0.542s, learning 2.074s)
               Value function loss: 62.4346
                    Surrogate loss: 0.0078
             Mean action noise std: 1.00
                       Mean reward: 109.85
               Mean episode length: 168.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 25.84
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 2.62s
                        Total time: 295.49s
                               ETA: 9896.4s

################################################################################
                     [1m Learning iteration 116/4000 [0m

                       Computation: 3139 steps/s (collection: 0.498s, learning 2.111s)
               Value function loss: 94.7147
                    Surrogate loss: 0.0088
             Mean action noise std: 1.00
                       Mean reward: 108.01
               Mean episode length: 161.52
                 Mean success rate: 0.00
                  Mean reward/step: 0.74
       Mean episode length/episode: 24.98
--------------------------------------------------------------------------------
                   Total timesteps: 958464
                    Iteration time: 2.61s
                        Total time: 298.10s
                               ETA: 9895.9s

################################################################################
                     [1m Learning iteration 117/4000 [0m

                       Computation: 3165 steps/s (collection: 0.539s, learning 2.049s)
               Value function loss: 55.2489
                    Surrogate loss: 0.0091
             Mean action noise std: 1.00
                       Mean reward: 99.76
               Mean episode length: 151.27
                 Mean success rate: 0.00
                  Mean reward/step: 0.68
       Mean episode length/episode: 25.84
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 2.59s
                        Total time: 300.69s
                               ETA: 9894.6s

################################################################################
                     [1m Learning iteration 118/4000 [0m

                       Computation: 3219 steps/s (collection: 0.509s, learning 2.035s)
               Value function loss: 51.5467
                    Surrogate loss: 0.0085
             Mean action noise std: 1.00
                       Mean reward: 99.92
               Mean episode length: 149.81
                 Mean success rate: 0.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 974848
                    Iteration time: 2.54s
                        Total time: 303.23s
                               ETA: 9892.0s

################################################################################
                     [1m Learning iteration 119/4000 [0m

                       Computation: 3276 steps/s (collection: 0.471s, learning 2.029s)
               Value function loss: 67.1215
                    Surrogate loss: 0.0089
             Mean action noise std: 1.00
                       Mean reward: 94.91
               Mean episode length: 144.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 2.50s
                        Total time: 305.73s
                               ETA: 9887.9s

################################################################################
                     [1m Learning iteration 120/4000 [0m

                       Computation: 3145 steps/s (collection: 0.544s, learning 2.061s)
               Value function loss: 66.4655
                    Surrogate loss: 0.0073
             Mean action noise std: 1.00
                       Mean reward: 99.41
               Mean episode length: 148.45
                 Mean success rate: 0.00
                  Mean reward/step: 0.77
       Mean episode length/episode: 25.21
--------------------------------------------------------------------------------
                   Total timesteps: 991232
                    Iteration time: 2.60s
                        Total time: 308.34s
                               ETA: 9887.1s

################################################################################
                     [1m Learning iteration 121/4000 [0m

                       Computation: 3200 steps/s (collection: 0.503s, learning 2.056s)
               Value function loss: 57.2466
                    Surrogate loss: 0.0088
             Mean action noise std: 1.00
                       Mean reward: 95.56
               Mean episode length: 136.86
                 Mean success rate: 0.00
                  Mean reward/step: 0.77
       Mean episode length/episode: 25.92
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 2.56s
                        Total time: 310.90s
                               ETA: 9884.9s

################################################################################
                     [1m Learning iteration 122/4000 [0m

                       Computation: 3237 steps/s (collection: 0.487s, learning 2.043s)
               Value function loss: 55.8930
                    Surrogate loss: 0.0086
             Mean action noise std: 1.00
                       Mean reward: 103.33
               Mean episode length: 137.07
                 Mean success rate: 0.00
                  Mean reward/step: 0.77
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 1007616
                    Iteration time: 2.53s
                        Total time: 313.43s
                               ETA: 9881.8s

################################################################################
                     [1m Learning iteration 123/4000 [0m

                       Computation: 3209 steps/s (collection: 0.518s, learning 2.035s)
               Value function loss: 67.2799
                    Surrogate loss: 0.0087
             Mean action noise std: 1.00
                       Mean reward: 101.21
               Mean episode length: 132.53
                 Mean success rate: 0.00
                  Mean reward/step: 0.77
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 2.55s
                        Total time: 315.98s
                               ETA: 9879.4s

################################################################################
                     [1m Learning iteration 124/4000 [0m

                       Computation: 3279 steps/s (collection: 0.448s, learning 2.050s)
               Value function loss: 70.6339
                    Surrogate loss: 0.0096
             Mean action noise std: 1.00
                       Mean reward: 100.37
               Mean episode length: 131.18
                 Mean success rate: 0.00
                  Mean reward/step: 0.78
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 1024000
                    Iteration time: 2.50s
                        Total time: 318.48s
                               ETA: 9875.3s

################################################################################
                     [1m Learning iteration 125/4000 [0m

                       Computation: 3221 steps/s (collection: 0.486s, learning 2.056s)
               Value function loss: 61.8161
                    Surrogate loss: 0.0088
             Mean action noise std: 1.00
                       Mean reward: 103.93
               Mean episode length: 134.81
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 2.54s
                        Total time: 321.02s
                               ETA: 9872.6s

################################################################################
                     [1m Learning iteration 126/4000 [0m

                       Computation: 3212 steps/s (collection: 0.495s, learning 2.056s)
               Value function loss: 74.6078
                    Surrogate loss: 0.0086
             Mean action noise std: 1.00
                       Mean reward: 99.56
               Mean episode length: 133.47
                 Mean success rate: 0.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 1040384
                    Iteration time: 2.55s
                        Total time: 323.57s
                               ETA: 9870.1s

################################################################################
                     [1m Learning iteration 127/4000 [0m

                       Computation: 3284 steps/s (collection: 0.461s, learning 2.033s)
               Value function loss: 79.6473
                    Surrogate loss: 0.0079
             Mean action noise std: 1.00
                       Mean reward: 105.03
               Mean episode length: 139.80
                 Mean success rate: 0.00
                  Mean reward/step: 0.76
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 2.49s
                        Total time: 326.06s
                               ETA: 9865.9s

################################################################################
                     [1m Learning iteration 128/4000 [0m

                       Computation: 3218 steps/s (collection: 0.505s, learning 2.041s)
               Value function loss: 97.8764
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 117.73
               Mean episode length: 155.15
                 Mean success rate: 0.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 1056768
                    Iteration time: 2.55s
                        Total time: 328.61s
                               ETA: 9863.3s

################################################################################
                     [1m Learning iteration 129/4000 [0m

                       Computation: 3219 steps/s (collection: 0.500s, learning 2.045s)
               Value function loss: 56.2758
                    Surrogate loss: 0.0088
             Mean action noise std: 1.00
                       Mean reward: 119.29
               Mean episode length: 158.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.76
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 2.54s
                        Total time: 331.15s
                               ETA: 9860.7s

################################################################################
                     [1m Learning iteration 130/4000 [0m

                       Computation: 3225 steps/s (collection: 0.501s, learning 2.039s)
               Value function loss: 73.0188
                    Surrogate loss: 0.0095
             Mean action noise std: 1.00
                       Mean reward: 123.57
               Mean episode length: 166.58
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1073152
                    Iteration time: 2.54s
                        Total time: 333.69s
                               ETA: 9858.0s

################################################################################
                     [1m Learning iteration 131/4000 [0m

                       Computation: 3284 steps/s (collection: 0.447s, learning 2.048s)
               Value function loss: 61.6266
                    Surrogate loss: 0.0094
             Mean action noise std: 1.00
                       Mean reward: 122.81
               Mean episode length: 162.37
                 Mean success rate: 0.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 2.49s
                        Total time: 336.19s
                               ETA: 9853.8s

################################################################################
                     [1m Learning iteration 132/4000 [0m

                       Computation: 3200 steps/s (collection: 0.510s, learning 2.049s)
               Value function loss: 97.4850
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 130.86
               Mean episode length: 171.14
                 Mean success rate: 0.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1089536
                    Iteration time: 2.56s
                        Total time: 338.75s
                               ETA: 9851.7s

################################################################################
                     [1m Learning iteration 133/4000 [0m

                       Computation: 3158 steps/s (collection: 0.506s, learning 2.087s)
               Value function loss: 86.7707
                    Surrogate loss: 0.0091
             Mean action noise std: 1.00
                       Mean reward: 139.08
               Mean episode length: 177.63
                 Mean success rate: 0.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 2.59s
                        Total time: 341.34s
                               ETA: 9850.5s

################################################################################
                     [1m Learning iteration 134/4000 [0m

                       Computation: 3215 steps/s (collection: 0.478s, learning 2.069s)
               Value function loss: 85.6768
                    Surrogate loss: 0.0093
             Mean action noise std: 1.00
                       Mean reward: 144.31
               Mean episode length: 175.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1105920
                    Iteration time: 2.55s
                        Total time: 343.89s
                               ETA: 9847.9s

################################################################################
                     [1m Learning iteration 135/4000 [0m

                       Computation: 3210 steps/s (collection: 0.516s, learning 2.036s)
               Value function loss: 123.0613
                    Surrogate loss: 0.0084
             Mean action noise std: 1.00
                       Mean reward: 160.30
               Mean episode length: 191.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 2.55s
                        Total time: 346.44s
                               ETA: 9845.5s

################################################################################
                     [1m Learning iteration 136/4000 [0m

                       Computation: 3206 steps/s (collection: 0.476s, learning 2.078s)
               Value function loss: 103.5042
                    Surrogate loss: 0.0092
             Mean action noise std: 1.00
                       Mean reward: 167.61
               Mean episode length: 198.20
                 Mean success rate: 0.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1122304
                    Iteration time: 2.55s
                        Total time: 348.99s
                               ETA: 9843.1s

################################################################################
                     [1m Learning iteration 137/4000 [0m

                       Computation: 3282 steps/s (collection: 0.463s, learning 2.032s)
               Value function loss: 112.5930
                    Surrogate loss: 0.0099
             Mean action noise std: 1.00
                       Mean reward: 177.88
               Mean episode length: 211.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.96
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 2.50s
                        Total time: 351.49s
                               ETA: 9839.1s

################################################################################
                     [1m Learning iteration 138/4000 [0m

                       Computation: 3172 steps/s (collection: 0.555s, learning 2.027s)
               Value function loss: 103.2713
                    Surrogate loss: 0.0107
             Mean action noise std: 1.00
                       Mean reward: 182.89
               Mean episode length: 217.10
                 Mean success rate: 0.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1138688
                    Iteration time: 2.58s
                        Total time: 354.07s
                               ETA: 9837.6s

################################################################################
                     [1m Learning iteration 139/4000 [0m

                       Computation: 3250 steps/s (collection: 0.483s, learning 2.037s)
               Value function loss: 90.8766
                    Surrogate loss: 0.0083
             Mean action noise std: 1.00
                       Mean reward: 192.40
               Mean episode length: 227.84
                 Mean success rate: 0.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 2.52s
                        Total time: 356.59s
                               ETA: 9834.3s

################################################################################
                     [1m Learning iteration 140/4000 [0m

                       Computation: 3232 steps/s (collection: 0.451s, learning 2.084s)
               Value function loss: 103.4597
                    Surrogate loss: 0.0098
             Mean action noise std: 1.00
                       Mean reward: 200.55
               Mean episode length: 236.35
                 Mean success rate: 0.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1155072
                    Iteration time: 2.53s
                        Total time: 359.13s
                               ETA: 9831.4s

################################################################################
                     [1m Learning iteration 141/4000 [0m

                       Computation: 3239 steps/s (collection: 0.481s, learning 2.048s)
               Value function loss: 118.3286
                    Surrogate loss: 0.0090
             Mean action noise std: 1.00
                       Mean reward: 191.74
               Mean episode length: 226.26
                 Mean success rate: 0.00
                  Mean reward/step: 0.97
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 2.53s
                        Total time: 361.65s
                               ETA: 9828.3s

################################################################################
                     [1m Learning iteration 142/4000 [0m

                       Computation: 3256 steps/s (collection: 0.476s, learning 2.040s)
               Value function loss: 148.7351
                    Surrogate loss: 0.0069
             Mean action noise std: 1.00
                       Mean reward: 183.49
               Mean episode length: 214.93
                 Mean success rate: 0.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1171456
                    Iteration time: 2.52s
                        Total time: 364.17s
                               ETA: 9825.0s

################################################################################
                     [1m Learning iteration 143/4000 [0m

                       Computation: 3265 steps/s (collection: 0.451s, learning 2.058s)
               Value function loss: 153.6687
                    Surrogate loss: 0.0092
             Mean action noise std: 1.00
                       Mean reward: 180.35
               Mean episode length: 209.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 2.51s
                        Total time: 366.68s
                               ETA: 9821.4s

################################################################################
                     [1m Learning iteration 144/4000 [0m

                       Computation: 3271 steps/s (collection: 0.447s, learning 2.057s)
               Value function loss: 133.1953
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 174.69
               Mean episode length: 198.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1187840
                    Iteration time: 2.50s
                        Total time: 369.18s
                               ETA: 9817.7s

################################################################################
                     [1m Learning iteration 145/4000 [0m

                       Computation: 3276 steps/s (collection: 0.440s, learning 2.060s)
               Value function loss: 141.9077
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 173.04
               Mean episode length: 194.99
                 Mean success rate: 0.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 2.50s
                        Total time: 371.68s
                               ETA: 9814.0s

################################################################################
                     [1m Learning iteration 146/4000 [0m

                       Computation: 3275 steps/s (collection: 0.450s, learning 2.051s)
               Value function loss: 140.7315
                    Surrogate loss: 0.0095
             Mean action noise std: 1.00
                       Mean reward: 191.92
               Mean episode length: 208.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1204224
                    Iteration time: 2.50s
                        Total time: 374.18s
                               ETA: 9810.2s

################################################################################
                     [1m Learning iteration 147/4000 [0m

                       Computation: 3211 steps/s (collection: 0.498s, learning 2.053s)
               Value function loss: 152.7210
                    Surrogate loss: 0.0082
             Mean action noise std: 1.00
                       Mean reward: 200.89
               Mean episode length: 213.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 2.55s
                        Total time: 376.73s
                               ETA: 9807.8s

################################################################################
                     [1m Learning iteration 148/4000 [0m

                       Computation: 3303 steps/s (collection: 0.459s, learning 2.021s)
               Value function loss: 153.8279
                    Surrogate loss: 0.0087
             Mean action noise std: 1.00
                       Mean reward: 188.71
               Mean episode length: 196.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 1220608
                    Iteration time: 2.48s
                        Total time: 379.21s
                               ETA: 9803.6s

################################################################################
                     [1m Learning iteration 149/4000 [0m

                       Computation: 3248 steps/s (collection: 0.484s, learning 2.038s)
               Value function loss: 169.6539
                    Surrogate loss: 0.0093
             Mean action noise std: 1.00
                       Mean reward: 187.24
               Mean episode length: 185.04
                 Mean success rate: 0.00
                  Mean reward/step: 0.97
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 2.52s
                        Total time: 381.74s
                               ETA: 9800.4s

################################################################################
                     [1m Learning iteration 150/4000 [0m

                       Computation: 3238 steps/s (collection: 0.499s, learning 2.031s)
               Value function loss: 169.9078
                    Surrogate loss: 0.0082
             Mean action noise std: 1.00
                       Mean reward: 197.08
               Mean episode length: 194.96
                 Mean success rate: 0.00
                  Mean reward/step: 0.98
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1236992
                    Iteration time: 2.53s
                        Total time: 384.27s
                               ETA: 9797.5s

################################################################################
                     [1m Learning iteration 151/4000 [0m

                       Computation: 3248 steps/s (collection: 0.495s, learning 2.027s)
               Value function loss: 91.0003
                    Surrogate loss: 0.0086
             Mean action noise std: 1.00
                       Mean reward: 190.90
               Mean episode length: 190.47
                 Mean success rate: 0.00
                  Mean reward/step: 0.98
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 2.52s
                        Total time: 386.79s
                               ETA: 9794.4s

################################################################################
                     [1m Learning iteration 152/4000 [0m

                       Computation: 3259 steps/s (collection: 0.489s, learning 2.025s)
               Value function loss: 115.3298
                    Surrogate loss: 0.0104
             Mean action noise std: 1.00
                       Mean reward: 191.05
               Mean episode length: 192.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1253376
                    Iteration time: 2.51s
                        Total time: 389.30s
                               ETA: 9791.1s

################################################################################
                     [1m Learning iteration 153/4000 [0m

                       Computation: 3205 steps/s (collection: 0.523s, learning 2.033s)
               Value function loss: 142.0996
                    Surrogate loss: 0.0073
             Mean action noise std: 1.00
                       Mean reward: 196.00
               Mean episode length: 200.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 2.56s
                        Total time: 391.86s
                               ETA: 9788.8s

################################################################################
                     [1m Learning iteration 154/4000 [0m

                       Computation: 3154 steps/s (collection: 0.523s, learning 2.074s)
               Value function loss: 143.4966
                    Surrogate loss: 0.0092
             Mean action noise std: 1.00
                       Mean reward: 187.38
               Mean episode length: 193.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1269760
                    Iteration time: 2.60s
                        Total time: 394.45s
                               ETA: 9787.6s

################################################################################
                     [1m Learning iteration 155/4000 [0m

                       Computation: 3177 steps/s (collection: 0.446s, learning 2.132s)
               Value function loss: 144.0766
                    Surrogate loss: 0.0091
             Mean action noise std: 1.00
                       Mean reward: 192.11
               Mean episode length: 196.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 2.58s
                        Total time: 397.03s
                               ETA: 9785.8s

################################################################################
                     [1m Learning iteration 156/4000 [0m

                       Computation: 3148 steps/s (collection: 0.448s, learning 2.154s)
               Value function loss: 229.8169
                    Surrogate loss: 0.0084
             Mean action noise std: 1.00
                       Mean reward: 185.73
               Mean episode length: 190.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1286144
                    Iteration time: 2.60s
                        Total time: 399.63s
                               ETA: 9784.7s

################################################################################
                     [1m Learning iteration 157/4000 [0m

                       Computation: 3086 steps/s (collection: 0.492s, learning 2.163s)
               Value function loss: 150.6316
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 195.91
               Mean episode length: 195.85
                 Mean success rate: 0.50
                  Mean reward/step: 1.08
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 2.65s
                        Total time: 402.29s
                               ETA: 9784.8s

################################################################################
                     [1m Learning iteration 158/4000 [0m

                       Computation: 3106 steps/s (collection: 0.514s, learning 2.123s)
               Value function loss: 132.0775
                    Surrogate loss: 0.0083
             Mean action noise std: 1.00
                       Mean reward: 201.87
               Mean episode length: 198.08
                 Mean success rate: 0.50
                  Mean reward/step: 1.06
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1302528
                    Iteration time: 2.64s
                        Total time: 404.93s
                               ETA: 9784.4s

################################################################################
                     [1m Learning iteration 159/4000 [0m

                       Computation: 3191 steps/s (collection: 0.468s, learning 2.099s)
               Value function loss: 206.1170
                    Surrogate loss: 0.0100
             Mean action noise std: 1.00
                       Mean reward: 217.97
               Mean episode length: 208.81
                 Mean success rate: 0.50
                  Mean reward/step: 1.05
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 2.57s
                        Total time: 407.49s
                               ETA: 9782.4s

################################################################################
                     [1m Learning iteration 160/4000 [0m

                       Computation: 3138 steps/s (collection: 0.453s, learning 2.157s)
               Value function loss: 149.4906
                    Surrogate loss: 0.0079
             Mean action noise std: 1.00
                       Mean reward: 217.51
               Mean episode length: 209.00
                 Mean success rate: 0.50
                  Mean reward/step: 1.11
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1318912
                    Iteration time: 2.61s
                        Total time: 410.10s
                               ETA: 9781.3s

################################################################################
                     [1m Learning iteration 161/4000 [0m

                       Computation: 3141 steps/s (collection: 0.536s, learning 2.072s)
               Value function loss: 138.7819
                    Surrogate loss: 0.0102
             Mean action noise std: 1.00
                       Mean reward: 228.39
               Mean episode length: 218.45
                 Mean success rate: 0.50
                  Mean reward/step: 1.06
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 2.61s
                        Total time: 412.71s
                               ETA: 9780.2s

################################################################################
                     [1m Learning iteration 162/4000 [0m

                       Computation: 3078 steps/s (collection: 0.528s, learning 2.133s)
               Value function loss: 186.9373
                    Surrogate loss: 0.0106
             Mean action noise std: 1.00
                       Mean reward: 244.85
               Mean episode length: 230.13
                 Mean success rate: 0.50
                  Mean reward/step: 1.10
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1335296
                    Iteration time: 2.66s
                        Total time: 415.37s
                               ETA: 9780.4s

################################################################################
                     [1m Learning iteration 163/4000 [0m

                       Computation: 3147 steps/s (collection: 0.476s, learning 2.127s)
               Value function loss: 177.3545
                    Surrogate loss: 0.0095
             Mean action noise std: 1.00
                       Mean reward: 253.11
               Mean episode length: 241.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 2.60s
                        Total time: 417.97s
                               ETA: 9779.1s

################################################################################
                     [1m Learning iteration 164/4000 [0m

                       Computation: 3045 steps/s (collection: 0.518s, learning 2.172s)
               Value function loss: 188.4864
                    Surrogate loss: 0.0088
             Mean action noise std: 1.00
                       Mean reward: 259.76
               Mean episode length: 249.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1351680
                    Iteration time: 2.69s
                        Total time: 420.66s
                               ETA: 9779.8s

################################################################################
                     [1m Learning iteration 165/4000 [0m

                       Computation: 3261 steps/s (collection: 0.480s, learning 2.032s)
               Value function loss: 196.3539
                    Surrogate loss: 0.0099
             Mean action noise std: 1.00
                       Mean reward: 273.12
               Mean episode length: 261.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 2.51s
                        Total time: 423.18s
                               ETA: 9776.4s

################################################################################
                     [1m Learning iteration 166/4000 [0m

                       Computation: 3248 steps/s (collection: 0.498s, learning 2.024s)
               Value function loss: 212.6064
                    Surrogate loss: 0.0097
             Mean action noise std: 1.00
                       Mean reward: 279.96
               Mean episode length: 267.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1368064
                    Iteration time: 2.52s
                        Total time: 425.70s
                               ETA: 9773.2s

################################################################################
                     [1m Learning iteration 167/4000 [0m

                       Computation: 3348 steps/s (collection: 0.441s, learning 2.005s)
               Value function loss: 228.0630
                    Surrogate loss: 0.0096
             Mean action noise std: 1.00
                       Mean reward: 287.29
               Mean episode length: 270.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 2.45s
                        Total time: 428.15s
                               ETA: 9768.3s

################################################################################
                     [1m Learning iteration 168/4000 [0m

                       Computation: 3246 steps/s (collection: 0.498s, learning 2.026s)
               Value function loss: 179.1023
                    Surrogate loss: 0.0112
             Mean action noise std: 1.00
                       Mean reward: 279.62
               Mean episode length: 262.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1384448
                    Iteration time: 2.52s
                        Total time: 430.67s
                               ETA: 9765.2s

################################################################################
                     [1m Learning iteration 169/4000 [0m

                       Computation: 3224 steps/s (collection: 0.491s, learning 2.050s)
               Value function loss: 156.2833
                    Surrogate loss: 0.0106
             Mean action noise std: 1.00
                       Mean reward: 272.43
               Mean episode length: 251.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 2.54s
                        Total time: 433.21s
                               ETA: 9762.5s

################################################################################
                     [1m Learning iteration 170/4000 [0m

                       Computation: 3119 steps/s (collection: 0.563s, learning 2.063s)
               Value function loss: 193.3941
                    Surrogate loss: 0.0106
             Mean action noise std: 1.00
                       Mean reward: 257.23
               Mean episode length: 232.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1400832
                    Iteration time: 2.63s
                        Total time: 435.84s
                               ETA: 9761.7s

################################################################################
                     [1m Learning iteration 171/4000 [0m

                       Computation: 3205 steps/s (collection: 0.488s, learning 2.068s)
               Value function loss: 113.7250
                    Surrogate loss: 0.0109
             Mean action noise std: 1.00
                       Mean reward: 254.77
               Mean episode length: 229.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 2.56s
                        Total time: 438.39s
                               ETA: 9759.3s

################################################################################
                     [1m Learning iteration 172/4000 [0m

                       Computation: 3215 steps/s (collection: 0.477s, learning 2.071s)
               Value function loss: 150.6001
                    Surrogate loss: 0.0104
             Mean action noise std: 1.00
                       Mean reward: 248.17
               Mean episode length: 220.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1417216
                    Iteration time: 2.55s
                        Total time: 440.94s
                               ETA: 9756.7s

################################################################################
                     [1m Learning iteration 173/4000 [0m

                       Computation: 3197 steps/s (collection: 0.540s, learning 2.022s)
               Value function loss: 206.5596
                    Surrogate loss: 0.0094
             Mean action noise std: 1.00
                       Mean reward: 234.63
               Mean episode length: 208.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 2.56s
                        Total time: 443.50s
                               ETA: 9754.5s

################################################################################
                     [1m Learning iteration 174/4000 [0m

                       Computation: 3268 steps/s (collection: 0.461s, learning 2.045s)
               Value function loss: 226.4234
                    Surrogate loss: 0.0089
             Mean action noise std: 1.00
                       Mean reward: 237.85
               Mean episode length: 208.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1433600
                    Iteration time: 2.51s
                        Total time: 446.01s
                               ETA: 9751.0s

################################################################################
                     [1m Learning iteration 175/4000 [0m

                       Computation: 3250 steps/s (collection: 0.471s, learning 2.049s)
               Value function loss: 200.0159
                    Surrogate loss: 0.0108
             Mean action noise std: 1.00
                       Mean reward: 221.40
               Mean episode length: 193.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 2.52s
                        Total time: 448.53s
                               ETA: 9747.8s

################################################################################
                     [1m Learning iteration 176/4000 [0m

                       Computation: 3228 steps/s (collection: 0.480s, learning 2.057s)
               Value function loss: 163.9009
                    Surrogate loss: 0.0126
             Mean action noise std: 1.00
                       Mean reward: 227.72
               Mean episode length: 199.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1449984
                    Iteration time: 2.54s
                        Total time: 451.07s
                               ETA: 9745.1s

################################################################################
                     [1m Learning iteration 177/4000 [0m

                       Computation: 3151 steps/s (collection: 0.540s, learning 2.060s)
               Value function loss: 150.3424
                    Surrogate loss: 0.0103
             Mean action noise std: 1.00
                       Mean reward: 225.00
               Mean episode length: 198.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 2.60s
                        Total time: 453.67s
                               ETA: 9743.6s

################################################################################
                     [1m Learning iteration 178/4000 [0m

                       Computation: 3168 steps/s (collection: 0.480s, learning 2.106s)
               Value function loss: 216.9179
                    Surrogate loss: 0.0091
             Mean action noise std: 0.99
                       Mean reward: 231.43
               Mean episode length: 200.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1466368
                    Iteration time: 2.59s
                        Total time: 456.25s
                               ETA: 9741.8s

################################################################################
                     [1m Learning iteration 179/4000 [0m

                       Computation: 3143 steps/s (collection: 0.498s, learning 2.108s)
               Value function loss: 154.1106
                    Surrogate loss: 0.0130
             Mean action noise std: 0.99
                       Mean reward: 241.76
               Mean episode length: 211.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 2.61s
                        Total time: 458.86s
                               ETA: 9740.5s

################################################################################
                     [1m Learning iteration 180/4000 [0m

                       Computation: 3165 steps/s (collection: 0.489s, learning 2.098s)
               Value function loss: 180.6963
                    Surrogate loss: 0.0092
             Mean action noise std: 0.99
                       Mean reward: 250.50
               Mean episode length: 218.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1482752
                    Iteration time: 2.59s
                        Total time: 461.44s
                               ETA: 9738.8s

################################################################################
                     [1m Learning iteration 181/4000 [0m

                       Computation: 3047 steps/s (collection: 0.529s, learning 2.159s)
               Value function loss: 247.5465
                    Surrogate loss: 0.0102
             Mean action noise std: 0.99
                       Mean reward: 248.58
               Mean episode length: 215.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 2.69s
                        Total time: 464.13s
                               ETA: 9739.1s

################################################################################
                     [1m Learning iteration 182/4000 [0m

                       Computation: 3105 steps/s (collection: 0.532s, learning 2.106s)
               Value function loss: 191.2374
                    Surrogate loss: 0.0089
             Mean action noise std: 0.99
                       Mean reward: 250.50
               Mean episode length: 218.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1499136
                    Iteration time: 2.64s
                        Total time: 466.77s
                               ETA: 9738.4s

################################################################################
                     [1m Learning iteration 183/4000 [0m

                       Computation: 3220 steps/s (collection: 0.483s, learning 2.061s)
               Value function loss: 208.6324
                    Surrogate loss: 0.0091
             Mean action noise std: 0.99
                       Mean reward: 244.08
               Mean episode length: 210.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 2.54s
                        Total time: 469.31s
                               ETA: 9735.7s

################################################################################
                     [1m Learning iteration 184/4000 [0m

                       Computation: 3210 steps/s (collection: 0.489s, learning 2.063s)
               Value function loss: 142.9364
                    Surrogate loss: 0.0121
             Mean action noise std: 1.00
                       Mean reward: 225.93
               Mean episode length: 192.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1515520
                    Iteration time: 2.55s
                        Total time: 471.87s
                               ETA: 9733.2s

################################################################################
                     [1m Learning iteration 185/4000 [0m

                       Computation: 3211 steps/s (collection: 0.497s, learning 2.054s)
               Value function loss: 152.4623
                    Surrogate loss: 0.0128
             Mean action noise std: 1.00
                       Mean reward: 224.71
               Mean episode length: 190.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 2.55s
                        Total time: 474.42s
                               ETA: 9730.7s

################################################################################
                     [1m Learning iteration 186/4000 [0m

                       Computation: 3048 steps/s (collection: 0.572s, learning 2.115s)
               Value function loss: 191.0282
                    Surrogate loss: 0.0123
             Mean action noise std: 1.00
                       Mean reward: 227.85
               Mean episode length: 192.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1531904
                    Iteration time: 2.69s
                        Total time: 477.10s
                               ETA: 9730.9s

################################################################################
                     [1m Learning iteration 187/4000 [0m

                       Computation: 3153 steps/s (collection: 0.489s, learning 2.109s)
               Value function loss: 223.8303
                    Surrogate loss: 0.0125
             Mean action noise std: 1.00
                       Mean reward: 212.41
               Mean episode length: 177.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 2.60s
                        Total time: 479.70s
                               ETA: 9729.3s

################################################################################
                     [1m Learning iteration 188/4000 [0m

                       Computation: 3116 steps/s (collection: 0.538s, learning 2.090s)
               Value function loss: 238.2952
                    Surrogate loss: 0.0097
             Mean action noise std: 1.00
                       Mean reward: 214.18
               Mean episode length: 179.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1548288
                    Iteration time: 2.63s
                        Total time: 482.33s
                               ETA: 9728.3s

################################################################################
                     [1m Learning iteration 189/4000 [0m

                       Computation: 3161 steps/s (collection: 0.540s, learning 2.051s)
               Value function loss: 156.6682
                    Surrogate loss: 0.0128
             Mean action noise std: 0.99
                       Mean reward: 223.56
               Mean episode length: 185.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 2.59s
                        Total time: 484.92s
                               ETA: 9726.5s

################################################################################
                     [1m Learning iteration 190/4000 [0m

                       Computation: 3226 steps/s (collection: 0.485s, learning 2.054s)
               Value function loss: 157.5693
                    Surrogate loss: 0.0135
             Mean action noise std: 1.00
                       Mean reward: 214.62
               Mean episode length: 178.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1564672
                    Iteration time: 2.54s
                        Total time: 487.46s
                               ETA: 9723.7s

################################################################################
                     [1m Learning iteration 191/4000 [0m

                       Computation: 3233 steps/s (collection: 0.467s, learning 2.066s)
               Value function loss: 167.4644
                    Surrogate loss: 0.0123
             Mean action noise std: 1.00
                       Mean reward: 224.58
               Mean episode length: 186.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 2.53s
                        Total time: 489.99s
                               ETA: 9720.8s

################################################################################
                     [1m Learning iteration 192/4000 [0m

                       Computation: 3169 steps/s (collection: 0.478s, learning 2.107s)
               Value function loss: 181.1294
                    Surrogate loss: 0.0112
             Mean action noise std: 0.99
                       Mean reward: 223.87
               Mean episode length: 184.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1581056
                    Iteration time: 2.58s
                        Total time: 492.58s
                               ETA: 9718.9s

################################################################################
                     [1m Learning iteration 193/4000 [0m

                       Computation: 3244 steps/s (collection: 0.470s, learning 2.055s)
               Value function loss: 196.8375
                    Surrogate loss: 0.0104
             Mean action noise std: 0.99
                       Mean reward: 220.52
               Mean episode length: 182.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 2.52s
                        Total time: 495.10s
                               ETA: 9715.8s

################################################################################
                     [1m Learning iteration 194/4000 [0m

                       Computation: 3186 steps/s (collection: 0.491s, learning 2.080s)
               Value function loss: 232.1911
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 212.67
               Mean episode length: 176.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1597440
                    Iteration time: 2.57s
                        Total time: 497.67s
                               ETA: 9713.6s

################################################################################
                     [1m Learning iteration 195/4000 [0m

                       Computation: 3171 steps/s (collection: 0.488s, learning 2.095s)
               Value function loss: 193.0353
                    Surrogate loss: 0.0117
             Mean action noise std: 1.00
                       Mean reward: 211.83
               Mean episode length: 176.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 2.58s
                        Total time: 500.26s
                               ETA: 9711.6s

################################################################################
                     [1m Learning iteration 196/4000 [0m

                       Computation: 3178 steps/s (collection: 0.486s, learning 2.092s)
               Value function loss: 184.8052
                    Surrogate loss: 0.0106
             Mean action noise std: 1.00
                       Mean reward: 208.02
               Mean episode length: 170.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1613824
                    Iteration time: 2.58s
                        Total time: 502.83s
                               ETA: 9709.6s

################################################################################
                     [1m Learning iteration 197/4000 [0m

                       Computation: 3156 steps/s (collection: 0.534s, learning 2.061s)
               Value function loss: 179.6639
                    Surrogate loss: 0.0120
             Mean action noise std: 1.00
                       Mean reward: 218.90
               Mean episode length: 182.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 2.60s
                        Total time: 505.43s
                               ETA: 9707.8s

################################################################################
                     [1m Learning iteration 198/4000 [0m

                       Computation: 3206 steps/s (collection: 0.482s, learning 2.072s)
               Value function loss: 209.8806
                    Surrogate loss: 0.0122
             Mean action noise std: 1.00
                       Mean reward: 233.36
               Mean episode length: 194.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1630208
                    Iteration time: 2.55s
                        Total time: 507.98s
                               ETA: 9705.3s

################################################################################
                     [1m Learning iteration 199/4000 [0m

                       Computation: 3184 steps/s (collection: 0.493s, learning 2.080s)
               Value function loss: 290.0756
                    Surrogate loss: 0.0097
             Mean action noise std: 1.00
                       Mean reward: 227.58
               Mean episode length: 188.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 2.57s
                        Total time: 510.56s
                               ETA: 9703.1s

################################################################################
                     [1m Learning iteration 200/4000 [0m

                       Computation: 3193 steps/s (collection: 0.537s, learning 2.029s)
               Value function loss: 284.0880
                    Surrogate loss: 0.0093
             Mean action noise std: 1.00
                       Mean reward: 235.48
               Mean episode length: 191.88
                 Mean success rate: 1.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 1646592
                    Iteration time: 2.57s
                        Total time: 513.12s
                               ETA: 9700.8s

################################################################################
                     [1m Learning iteration 201/4000 [0m

                       Computation: 3185 steps/s (collection: 0.486s, learning 2.086s)
               Value function loss: 159.9914
                    Surrogate loss: 0.0119
             Mean action noise std: 0.99
                       Mean reward: 247.66
               Mean episode length: 202.53
                 Mean success rate: 1.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 2.57s
                        Total time: 515.69s
                               ETA: 9698.6s

################################################################################
                     [1m Learning iteration 202/4000 [0m

                       Computation: 3213 steps/s (collection: 0.501s, learning 2.048s)
               Value function loss: 226.4188
                    Surrogate loss: 0.0138
             Mean action noise std: 0.99
                       Mean reward: 268.69
               Mean episode length: 213.97
                 Mean success rate: 1.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1662976
                    Iteration time: 2.55s
                        Total time: 518.24s
                               ETA: 9696.0s

################################################################################
                     [1m Learning iteration 203/4000 [0m

                       Computation: 3153 steps/s (collection: 0.498s, learning 2.100s)
               Value function loss: 248.8535
                    Surrogate loss: 0.0114
             Mean action noise std: 0.99
                       Mean reward: 259.65
               Mean episode length: 204.00
                 Mean success rate: 1.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 2.60s
                        Total time: 520.84s
                               ETA: 9694.3s

################################################################################
                     [1m Learning iteration 204/4000 [0m

                       Computation: 3186 steps/s (collection: 0.469s, learning 2.102s)
               Value function loss: 238.0441
                    Surrogate loss: 0.0126
             Mean action noise std: 0.99
                       Mean reward: 263.90
               Mean episode length: 206.81
                 Mean success rate: 1.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1679360
                    Iteration time: 2.57s
                        Total time: 523.41s
                               ETA: 9692.1s

################################################################################
                     [1m Learning iteration 205/4000 [0m

                       Computation: 3119 steps/s (collection: 0.531s, learning 2.095s)
               Value function loss: 204.6083
                    Surrogate loss: 0.0140
             Mean action noise std: 0.99
                       Mean reward: 267.81
               Mean episode length: 212.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 2.63s
                        Total time: 526.04s
                               ETA: 9690.8s

################################################################################
                     [1m Learning iteration 206/4000 [0m

                       Computation: 3113 steps/s (collection: 0.545s, learning 2.086s)
               Value function loss: 204.7427
                    Surrogate loss: 0.0141
             Mean action noise std: 0.99
                       Mean reward: 246.09
               Mean episode length: 194.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 1695744
                    Iteration time: 2.63s
                        Total time: 528.67s
                               ETA: 9689.7s

################################################################################
                     [1m Learning iteration 207/4000 [0m

                       Computation: 3126 steps/s (collection: 0.518s, learning 2.102s)
               Value function loss: 149.4766
                    Surrogate loss: 0.0166
             Mean action noise std: 0.99
                       Mean reward: 243.16
               Mean episode length: 192.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 2.62s
                        Total time: 531.29s
                               ETA: 9688.3s

################################################################################
                     [1m Learning iteration 208/4000 [0m

                       Computation: 3169 steps/s (collection: 0.530s, learning 2.054s)
               Value function loss: 206.7196
                    Surrogate loss: 0.0122
             Mean action noise std: 0.99
                       Mean reward: 254.33
               Mean episode length: 202.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1712128
                    Iteration time: 2.58s
                        Total time: 533.87s
                               ETA: 9686.3s

################################################################################
                     [1m Learning iteration 209/4000 [0m

                       Computation: 3174 steps/s (collection: 0.500s, learning 2.081s)
               Value function loss: 202.1646
                    Surrogate loss: 0.0135
             Mean action noise std: 0.99
                       Mean reward: 263.54
               Mean episode length: 208.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 2.58s
                        Total time: 536.45s
                               ETA: 9684.3s

################################################################################
                     [1m Learning iteration 210/4000 [0m

                       Computation: 3185 steps/s (collection: 0.542s, learning 2.029s)
               Value function loss: 169.6903
                    Surrogate loss: 0.0141
             Mean action noise std: 0.99
                       Mean reward: 258.78
               Mean episode length: 201.36
                 Mean success rate: 0.50
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1728512
                    Iteration time: 2.57s
                        Total time: 539.03s
                               ETA: 9682.0s

################################################################################
                     [1m Learning iteration 211/4000 [0m

                       Computation: 3188 steps/s (collection: 0.491s, learning 2.077s)
               Value function loss: 168.0932
                    Surrogate loss: 0.0158
             Mean action noise std: 0.99
                       Mean reward: 266.00
               Mean episode length: 207.81
                 Mean success rate: 0.50
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 2.57s
                        Total time: 541.59s
                               ETA: 9679.7s

################################################################################
                     [1m Learning iteration 212/4000 [0m

                       Computation: 3213 steps/s (collection: 0.505s, learning 2.045s)
               Value function loss: 185.1433
                    Surrogate loss: 0.0128
             Mean action noise std: 0.99
                       Mean reward: 257.44
               Mean episode length: 200.42
                 Mean success rate: 0.50
                  Mean reward/step: 1.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1744896
                    Iteration time: 2.55s
                        Total time: 544.14s
                               ETA: 9677.1s

################################################################################
                     [1m Learning iteration 213/4000 [0m

                       Computation: 3229 steps/s (collection: 0.458s, learning 2.078s)
               Value function loss: 214.5393
                    Surrogate loss: 0.0125
             Mean action noise std: 0.99
                       Mean reward: 265.92
               Mean episode length: 203.65
                 Mean success rate: 0.50
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 2.54s
                        Total time: 546.68s
                               ETA: 9674.2s

################################################################################
                     [1m Learning iteration 214/4000 [0m

                       Computation: 3175 steps/s (collection: 0.506s, learning 2.074s)
               Value function loss: 218.4133
                    Surrogate loss: 0.0120
             Mean action noise std: 0.99
                       Mean reward: 280.12
               Mean episode length: 209.97
                 Mean success rate: 0.50
                  Mean reward/step: 1.29
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1761280
                    Iteration time: 2.58s
                        Total time: 549.26s
                               ETA: 9672.1s

################################################################################
                     [1m Learning iteration 215/4000 [0m

                       Computation: 3242 steps/s (collection: 0.457s, learning 2.069s)
               Value function loss: 178.3212
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 281.95
               Mean episode length: 212.99
                 Mean success rate: 0.50
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 2.53s
                        Total time: 551.79s
                               ETA: 9669.0s

################################################################################
                     [1m Learning iteration 216/4000 [0m

                       Computation: 3215 steps/s (collection: 0.456s, learning 2.092s)
               Value function loss: 248.9925
                    Surrogate loss: 0.0152
             Mean action noise std: 0.99
                       Mean reward: 288.07
               Mean episode length: 217.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1777664
                    Iteration time: 2.55s
                        Total time: 554.33s
                               ETA: 9666.4s

################################################################################
                     [1m Learning iteration 217/4000 [0m

                       Computation: 3213 steps/s (collection: 0.475s, learning 2.073s)
               Value function loss: 165.4174
                    Surrogate loss: 0.0163
             Mean action noise std: 0.99
                       Mean reward: 309.19
               Mean episode length: 228.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 2.55s
                        Total time: 556.88s
                               ETA: 9663.7s

################################################################################
                     [1m Learning iteration 218/4000 [0m

                       Computation: 3179 steps/s (collection: 0.497s, learning 2.080s)
               Value function loss: 182.3335
                    Surrogate loss: 0.0151
             Mean action noise std: 0.99
                       Mean reward: 324.67
               Mean episode length: 240.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1794048
                    Iteration time: 2.58s
                        Total time: 559.46s
                               ETA: 9661.5s

################################################################################
                     [1m Learning iteration 219/4000 [0m

                       Computation: 3134 steps/s (collection: 0.502s, learning 2.111s)
               Value function loss: 159.8364
                    Surrogate loss: 0.0153
             Mean action noise std: 0.99
                       Mean reward: 322.73
               Mean episode length: 239.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 2.61s
                        Total time: 562.07s
                               ETA: 9660.0s

################################################################################
                     [1m Learning iteration 220/4000 [0m

                       Computation: 3187 steps/s (collection: 0.507s, learning 2.063s)
               Value function loss: 186.2278
                    Surrogate loss: 0.0130
             Mean action noise std: 0.99
                       Mean reward: 328.63
               Mean episode length: 243.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1810432
                    Iteration time: 2.57s
                        Total time: 564.64s
                               ETA: 9657.7s

################################################################################
                     [1m Learning iteration 221/4000 [0m

                       Computation: 3179 steps/s (collection: 0.483s, learning 2.093s)
               Value function loss: 208.7887
                    Surrogate loss: 0.0116
             Mean action noise std: 0.99
                       Mean reward: 335.45
               Mean episode length: 245.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 2.58s
                        Total time: 567.22s
                               ETA: 9655.5s

################################################################################
                     [1m Learning iteration 222/4000 [0m

                       Computation: 3175 steps/s (collection: 0.497s, learning 2.083s)
               Value function loss: 184.2612
                    Surrogate loss: 0.0121
             Mean action noise std: 0.99
                       Mean reward: 351.68
               Mean episode length: 261.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1826816
                    Iteration time: 2.58s
                        Total time: 569.80s
                               ETA: 9653.4s

################################################################################
                     [1m Learning iteration 223/4000 [0m

                       Computation: 3171 steps/s (collection: 0.468s, learning 2.115s)
               Value function loss: 188.8397
                    Surrogate loss: 0.0113
             Mean action noise std: 0.99
                       Mean reward: 348.88
               Mean episode length: 258.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 2.58s
                        Total time: 572.38s
                               ETA: 9651.3s

################################################################################
                     [1m Learning iteration 224/4000 [0m

                       Computation: 3197 steps/s (collection: 0.513s, learning 2.050s)
               Value function loss: 318.5321
                    Surrogate loss: 0.0102
             Mean action noise std: 0.99
                       Mean reward: 350.45
               Mean episode length: 258.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1843200
                    Iteration time: 2.56s
                        Total time: 574.94s
                               ETA: 9648.8s

################################################################################
                     [1m Learning iteration 225/4000 [0m

                       Computation: 3169 steps/s (collection: 0.514s, learning 2.071s)
               Value function loss: 153.7433
                    Surrogate loss: 0.0131
             Mean action noise std: 0.99
                       Mean reward: 355.30
               Mean episode length: 264.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 2.58s
                        Total time: 577.53s
                               ETA: 9646.8s

################################################################################
                     [1m Learning iteration 226/4000 [0m

                       Computation: 3137 steps/s (collection: 0.534s, learning 2.076s)
               Value function loss: 177.8918
                    Surrogate loss: 0.0181
             Mean action noise std: 0.99
                       Mean reward: 366.07
               Mean episode length: 274.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1859584
                    Iteration time: 2.61s
                        Total time: 580.14s
                               ETA: 9645.1s

################################################################################
                     [1m Learning iteration 227/4000 [0m

                       Computation: 3183 steps/s (collection: 0.485s, learning 2.089s)
               Value function loss: 122.3350
                    Surrogate loss: 0.0124
             Mean action noise std: 0.99
                       Mean reward: 356.72
               Mean episode length: 267.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 2.57s
                        Total time: 582.71s
                               ETA: 9642.9s

################################################################################
                     [1m Learning iteration 228/4000 [0m

                       Computation: 3192 steps/s (collection: 0.477s, learning 2.089s)
               Value function loss: 221.4646
                    Surrogate loss: 0.0146
             Mean action noise std: 0.99
                       Mean reward: 363.71
               Mean episode length: 271.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1875968
                    Iteration time: 2.57s
                        Total time: 585.28s
                               ETA: 9640.5s

################################################################################
                     [1m Learning iteration 229/4000 [0m

                       Computation: 3188 steps/s (collection: 0.525s, learning 2.045s)
               Value function loss: 182.9404
                    Surrogate loss: 0.0162
             Mean action noise std: 0.99
                       Mean reward: 379.75
               Mean episode length: 283.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 2.57s
                        Total time: 587.85s
                               ETA: 9638.2s

################################################################################
                     [1m Learning iteration 230/4000 [0m

                       Computation: 3121 steps/s (collection: 0.536s, learning 2.088s)
               Value function loss: 168.1971
                    Surrogate loss: 0.0134
             Mean action noise std: 0.99
                       Mean reward: 368.56
               Mean episode length: 279.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1892352
                    Iteration time: 2.62s
                        Total time: 590.47s
                               ETA: 9636.7s

################################################################################
                     [1m Learning iteration 231/4000 [0m

                       Computation: 3224 steps/s (collection: 0.470s, learning 2.070s)
               Value function loss: 156.9360
                    Surrogate loss: 0.0167
             Mean action noise std: 0.99
                       Mean reward: 362.47
               Mean episode length: 278.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 2.54s
                        Total time: 593.01s
                               ETA: 9633.9s

################################################################################
                     [1m Learning iteration 232/4000 [0m

                       Computation: 3162 steps/s (collection: 0.525s, learning 2.066s)
               Value function loss: 235.1471
                    Surrogate loss: 0.0151
             Mean action noise std: 0.99
                       Mean reward: 368.15
               Mean episode length: 279.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1908736
                    Iteration time: 2.59s
                        Total time: 595.60s
                               ETA: 9631.9s

################################################################################
                     [1m Learning iteration 233/4000 [0m

                       Computation: 3155 steps/s (collection: 0.538s, learning 2.058s)
               Value function loss: 245.2978
                    Surrogate loss: 0.0117
             Mean action noise std: 0.99
                       Mean reward: 349.49
               Mean episode length: 266.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 2.60s
                        Total time: 598.20s
                               ETA: 9630.0s

################################################################################
                     [1m Learning iteration 234/4000 [0m

                       Computation: 3173 steps/s (collection: 0.491s, learning 2.091s)
               Value function loss: 140.0285
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 331.81
               Mean episode length: 253.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1925120
                    Iteration time: 2.58s
                        Total time: 600.78s
                               ETA: 9627.8s

################################################################################
                     [1m Learning iteration 235/4000 [0m

                       Computation: 3255 steps/s (collection: 0.467s, learning 2.050s)
               Value function loss: 199.4729
                    Surrogate loss: 0.0113
             Mean action noise std: 0.99
                       Mean reward: 314.27
               Mean episode length: 240.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 2.52s
                        Total time: 603.30s
                               ETA: 9624.7s

################################################################################
                     [1m Learning iteration 236/4000 [0m

                       Computation: 3200 steps/s (collection: 0.506s, learning 2.054s)
               Value function loss: 157.8169
                    Surrogate loss: 0.0141
             Mean action noise std: 0.99
                       Mean reward: 326.22
               Mean episode length: 248.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1941504
                    Iteration time: 2.56s
                        Total time: 605.86s
                               ETA: 9622.2s

################################################################################
                     [1m Learning iteration 237/4000 [0m

                       Computation: 3223 steps/s (collection: 0.490s, learning 2.051s)
               Value function loss: 143.6408
                    Surrogate loss: 0.0132
             Mean action noise std: 0.99
                       Mean reward: 323.73
               Mean episode length: 247.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 2.54s
                        Total time: 608.40s
                               ETA: 9619.4s

################################################################################
                     [1m Learning iteration 238/4000 [0m

                       Computation: 3250 steps/s (collection: 0.462s, learning 2.058s)
               Value function loss: 201.2567
                    Surrogate loss: 0.0128
             Mean action noise std: 0.99
                       Mean reward: 328.52
               Mean episode length: 252.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1957888
                    Iteration time: 2.52s
                        Total time: 610.92s
                               ETA: 9616.2s

################################################################################
                     [1m Learning iteration 239/4000 [0m

                       Computation: 3232 steps/s (collection: 0.463s, learning 2.071s)
               Value function loss: 192.2360
                    Surrogate loss: 0.0172
             Mean action noise std: 0.99
                       Mean reward: 306.39
               Mean episode length: 238.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 2.53s
                        Total time: 613.45s
                               ETA: 9613.3s

################################################################################
                     [1m Learning iteration 240/4000 [0m

                       Computation: 3204 steps/s (collection: 0.478s, learning 2.079s)
               Value function loss: 217.5045
                    Surrogate loss: 0.0127
             Mean action noise std: 0.99
                       Mean reward: 313.74
               Mean episode length: 243.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1974272
                    Iteration time: 2.56s
                        Total time: 616.01s
                               ETA: 9610.8s

################################################################################
                     [1m Learning iteration 241/4000 [0m

                       Computation: 3175 steps/s (collection: 0.491s, learning 2.088s)
               Value function loss: 180.0301
                    Surrogate loss: 0.0143
             Mean action noise std: 0.99
                       Mean reward: 307.61
               Mean episode length: 235.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 2.58s
                        Total time: 618.59s
                               ETA: 9608.6s

################################################################################
                     [1m Learning iteration 242/4000 [0m

                       Computation: 3112 steps/s (collection: 0.493s, learning 2.139s)
               Value function loss: 178.6092
                    Surrogate loss: 0.0112
             Mean action noise std: 0.99
                       Mean reward: 290.52
               Mean episode length: 222.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1990656
                    Iteration time: 2.63s
                        Total time: 621.22s
                               ETA: 9607.2s

################################################################################
                     [1m Learning iteration 243/4000 [0m

                       Computation: 3160 steps/s (collection: 0.529s, learning 2.063s)
               Value function loss: 123.5996
                    Surrogate loss: 0.0158
             Mean action noise std: 0.99
                       Mean reward: 294.46
               Mean episode length: 222.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 2.59s
                        Total time: 623.81s
                               ETA: 9605.2s

################################################################################
                     [1m Learning iteration 244/4000 [0m

                       Computation: 3161 steps/s (collection: 0.511s, learning 2.080s)
               Value function loss: 230.7480
                    Surrogate loss: 0.0162
             Mean action noise std: 0.99
                       Mean reward: 305.45
               Mean episode length: 229.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2007040
                    Iteration time: 2.59s
                        Total time: 626.41s
                               ETA: 9603.2s

################################################################################
                     [1m Learning iteration 245/4000 [0m

                       Computation: 3166 steps/s (collection: 0.525s, learning 2.062s)
               Value function loss: 185.2478
                    Surrogate loss: 0.0181
             Mean action noise std: 0.99
                       Mean reward: 314.19
               Mean episode length: 235.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 2.59s
                        Total time: 628.99s
                               ETA: 9601.1s

################################################################################
                     [1m Learning iteration 246/4000 [0m

                       Computation: 3133 steps/s (collection: 0.509s, learning 2.105s)
               Value function loss: 185.2664
                    Surrogate loss: 0.0148
             Mean action noise std: 0.99
                       Mean reward: 312.94
               Mean episode length: 230.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 2023424
                    Iteration time: 2.61s
                        Total time: 631.61s
                               ETA: 9599.4s

################################################################################
                     [1m Learning iteration 247/4000 [0m

                       Computation: 3205 steps/s (collection: 0.515s, learning 2.041s)
               Value function loss: 167.7231
                    Surrogate loss: 0.0166
             Mean action noise std: 0.99
                       Mean reward: 300.44
               Mean episode length: 219.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 2.56s
                        Total time: 634.16s
                               ETA: 9596.8s

################################################################################
                     [1m Learning iteration 248/4000 [0m

                       Computation: 3180 steps/s (collection: 0.491s, learning 2.084s)
               Value function loss: 177.7221
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 296.43
               Mean episode length: 218.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2039808
                    Iteration time: 2.58s
                        Total time: 636.74s
                               ETA: 9594.6s

################################################################################
                     [1m Learning iteration 249/4000 [0m

                       Computation: 3183 steps/s (collection: 0.502s, learning 2.072s)
               Value function loss: 215.1749
                    Surrogate loss: 0.0147
             Mean action noise std: 0.99
                       Mean reward: 297.02
               Mean episode length: 215.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 2.57s
                        Total time: 639.31s
                               ETA: 9592.2s

################################################################################
                     [1m Learning iteration 250/4000 [0m

                       Computation: 3202 steps/s (collection: 0.487s, learning 2.070s)
               Value function loss: 115.9824
                    Surrogate loss: 0.0222
             Mean action noise std: 0.99
                       Mean reward: 284.79
               Mean episode length: 204.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2056192
                    Iteration time: 2.56s
                        Total time: 641.87s
                               ETA: 9589.7s

################################################################################
                     [1m Learning iteration 251/4000 [0m

                       Computation: 3214 steps/s (collection: 0.459s, learning 2.089s)
               Value function loss: 169.2061
                    Surrogate loss: 0.0118
             Mean action noise std: 0.99
                       Mean reward: 290.78
               Mean episode length: 215.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 2.55s
                        Total time: 644.42s
                               ETA: 9587.0s

################################################################################
                     [1m Learning iteration 252/4000 [0m

                       Computation: 3218 steps/s (collection: 0.461s, learning 2.084s)
               Value function loss: 154.9893
                    Surrogate loss: 0.0160
             Mean action noise std: 0.99
                       Mean reward: 298.60
               Mean episode length: 220.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2072576
                    Iteration time: 2.55s
                        Total time: 646.96s
                               ETA: 9584.3s

################################################################################
                     [1m Learning iteration 253/4000 [0m

                       Computation: 3251 steps/s (collection: 0.485s, learning 2.034s)
               Value function loss: 212.1130
                    Surrogate loss: 0.0118
             Mean action noise std: 0.99
                       Mean reward: 307.43
               Mean episode length: 227.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 2.52s
                        Total time: 649.48s
                               ETA: 9581.2s

################################################################################
                     [1m Learning iteration 254/4000 [0m

                       Computation: 3198 steps/s (collection: 0.486s, learning 2.075s)
               Value function loss: 215.6411
                    Surrogate loss: 0.0123
             Mean action noise std: 0.99
                       Mean reward: 298.12
               Mean episode length: 217.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2088960
                    Iteration time: 2.56s
                        Total time: 652.04s
                               ETA: 9578.7s

################################################################################
                     [1m Learning iteration 255/4000 [0m

                       Computation: 3236 steps/s (collection: 0.476s, learning 2.055s)
               Value function loss: 186.3640
                    Surrogate loss: 0.0145
             Mean action noise std: 0.99
                       Mean reward: 292.25
               Mean episode length: 213.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 2.53s
                        Total time: 654.58s
                               ETA: 9575.7s

################################################################################
                     [1m Learning iteration 256/4000 [0m

                       Computation: 3268 steps/s (collection: 0.461s, learning 2.045s)
               Value function loss: 203.6874
                    Surrogate loss: 0.0124
             Mean action noise std: 0.99
                       Mean reward: 291.86
               Mean episode length: 209.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2105344
                    Iteration time: 2.51s
                        Total time: 657.08s
                               ETA: 9572.4s

################################################################################
                     [1m Learning iteration 257/4000 [0m

                       Computation: 3242 steps/s (collection: 0.469s, learning 2.057s)
               Value function loss: 176.8306
                    Surrogate loss: 0.0158
             Mean action noise std: 0.99
                       Mean reward: 296.02
               Mean episode length: 212.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 2.53s
                        Total time: 659.61s
                               ETA: 9569.4s

################################################################################
                     [1m Learning iteration 258/4000 [0m

                       Computation: 3220 steps/s (collection: 0.484s, learning 2.059s)
               Value function loss: 183.0005
                    Surrogate loss: 0.0139
             Mean action noise std: 0.99
                       Mean reward: 301.00
               Mean episode length: 214.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2121728
                    Iteration time: 2.54s
                        Total time: 662.15s
                               ETA: 9566.7s

################################################################################
                     [1m Learning iteration 259/4000 [0m

                       Computation: 3201 steps/s (collection: 0.517s, learning 2.042s)
               Value function loss: 173.1191
                    Surrogate loss: 0.0146
             Mean action noise std: 0.99
                       Mean reward: 303.28
               Mean episode length: 214.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 2.56s
                        Total time: 664.71s
                               ETA: 9564.2s

################################################################################
                     [1m Learning iteration 260/4000 [0m

                       Computation: 3196 steps/s (collection: 0.454s, learning 2.109s)
               Value function loss: 161.4988
                    Surrogate loss: 0.0161
             Mean action noise std: 0.99
                       Mean reward: 305.04
               Mean episode length: 216.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2138112
                    Iteration time: 2.56s
                        Total time: 667.27s
                               ETA: 9561.7s

################################################################################
                     [1m Learning iteration 261/4000 [0m

                       Computation: 3188 steps/s (collection: 0.521s, learning 2.049s)
               Value function loss: 139.0357
                    Surrogate loss: 0.0135
             Mean action noise std: 0.99
                       Mean reward: 314.64
               Mean episode length: 221.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 2.57s
                        Total time: 669.84s
                               ETA: 9559.3s

################################################################################
                     [1m Learning iteration 262/4000 [0m

                       Computation: 3220 steps/s (collection: 0.447s, learning 2.096s)
               Value function loss: 152.8290
                    Surrogate loss: 0.0131
             Mean action noise std: 0.99
                       Mean reward: 320.37
               Mean episode length: 224.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2154496
                    Iteration time: 2.54s
                        Total time: 672.39s
                               ETA: 9556.6s

################################################################################
                     [1m Learning iteration 263/4000 [0m

                       Computation: 3254 steps/s (collection: 0.468s, learning 2.049s)
               Value function loss: 194.0725
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 320.23
               Mean episode length: 223.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 2.52s
                        Total time: 674.90s
                               ETA: 9553.5s

################################################################################
                     [1m Learning iteration 264/4000 [0m

                       Computation: 3287 steps/s (collection: 0.461s, learning 2.031s)
               Value function loss: 167.7728
                    Surrogate loss: 0.0127
             Mean action noise std: 0.99
                       Mean reward: 326.01
               Mean episode length: 227.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2170880
                    Iteration time: 2.49s
                        Total time: 677.39s
                               ETA: 9550.0s

################################################################################
                     [1m Learning iteration 265/4000 [0m

                       Computation: 3221 steps/s (collection: 0.497s, learning 2.045s)
               Value function loss: 275.0002
                    Surrogate loss: 0.0115
             Mean action noise std: 0.99
                       Mean reward: 319.70
               Mean episode length: 225.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 2.54s
                        Total time: 679.94s
                               ETA: 9547.2s

################################################################################
                     [1m Learning iteration 266/4000 [0m

                       Computation: 3279 steps/s (collection: 0.458s, learning 2.040s)
               Value function loss: 209.3402
                    Surrogate loss: 0.0130
             Mean action noise std: 0.99
                       Mean reward: 287.96
               Mean episode length: 205.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2187264
                    Iteration time: 2.50s
                        Total time: 682.44s
                               ETA: 9543.9s

################################################################################
                     [1m Learning iteration 267/4000 [0m

                       Computation: 3207 steps/s (collection: 0.500s, learning 2.053s)
               Value function loss: 194.5805
                    Surrogate loss: 0.0151
             Mean action noise std: 0.99
                       Mean reward: 291.19
               Mean episode length: 211.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 2.55s
                        Total time: 684.99s
                               ETA: 9541.3s

################################################################################
                     [1m Learning iteration 268/4000 [0m

                       Computation: 3236 steps/s (collection: 0.450s, learning 2.081s)
               Value function loss: 169.5133
                    Surrogate loss: 0.0130
             Mean action noise std: 0.99
                       Mean reward: 276.95
               Mean episode length: 201.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2203648
                    Iteration time: 2.53s
                        Total time: 687.52s
                               ETA: 9538.4s

################################################################################
                     [1m Learning iteration 269/4000 [0m

                       Computation: 3271 steps/s (collection: 0.458s, learning 2.046s)
               Value function loss: 189.2266
                    Surrogate loss: 0.0146
             Mean action noise std: 0.99
                       Mean reward: 266.28
               Mean episode length: 198.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 2.50s
                        Total time: 690.02s
                               ETA: 9535.1s

################################################################################
                     [1m Learning iteration 270/4000 [0m

                       Computation: 3255 steps/s (collection: 0.450s, learning 2.067s)
               Value function loss: 156.7647
                    Surrogate loss: 0.0140
             Mean action noise std: 0.99
                       Mean reward: 261.38
               Mean episode length: 194.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2220032
                    Iteration time: 2.52s
                        Total time: 692.54s
                               ETA: 9532.0s

################################################################################
                     [1m Learning iteration 271/4000 [0m

                       Computation: 3249 steps/s (collection: 0.479s, learning 2.042s)
               Value function loss: 218.9597
                    Surrogate loss: 0.0139
             Mean action noise std: 0.99
                       Mean reward: 292.26
               Mean episode length: 215.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 2.52s
                        Total time: 695.06s
                               ETA: 9529.0s

################################################################################
                     [1m Learning iteration 272/4000 [0m

                       Computation: 3290 steps/s (collection: 0.453s, learning 2.037s)
               Value function loss: 187.8172
                    Surrogate loss: 0.0123
             Mean action noise std: 0.99
                       Mean reward: 299.82
               Mean episode length: 217.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2236416
                    Iteration time: 2.49s
                        Total time: 697.55s
                               ETA: 9525.5s

################################################################################
                     [1m Learning iteration 273/4000 [0m

                       Computation: 3265 steps/s (collection: 0.466s, learning 2.043s)
               Value function loss: 252.8379
                    Surrogate loss: 0.0128
             Mean action noise std: 0.99
                       Mean reward: 306.36
               Mean episode length: 222.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 2.51s
                        Total time: 700.06s
                               ETA: 9522.4s

################################################################################
                     [1m Learning iteration 274/4000 [0m

                       Computation: 3273 steps/s (collection: 0.457s, learning 2.046s)
               Value function loss: 177.7698
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 308.23
               Mean episode length: 220.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 2252800
                    Iteration time: 2.50s
                        Total time: 702.56s
                               ETA: 9519.1s

################################################################################
                     [1m Learning iteration 275/4000 [0m

                       Computation: 3207 steps/s (collection: 0.496s, learning 2.059s)
               Value function loss: 192.6150
                    Surrogate loss: 0.0144
             Mean action noise std: 0.99
                       Mean reward: 312.82
               Mean episode length: 218.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 2.55s
                        Total time: 705.12s
                               ETA: 9516.5s

################################################################################
                     [1m Learning iteration 276/4000 [0m

                       Computation: 3198 steps/s (collection: 0.476s, learning 2.086s)
               Value function loss: 126.8952
                    Surrogate loss: 0.0152
             Mean action noise std: 0.99
                       Mean reward: 312.67
               Mean episode length: 219.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2269184
                    Iteration time: 2.56s
                        Total time: 707.68s
                               ETA: 9514.1s

################################################################################
                     [1m Learning iteration 277/4000 [0m

                       Computation: 3275 steps/s (collection: 0.455s, learning 2.046s)
               Value function loss: 154.8887
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 323.89
               Mean episode length: 225.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 2.50s
                        Total time: 710.18s
                               ETA: 9510.8s

################################################################################
                     [1m Learning iteration 278/4000 [0m

                       Computation: 3273 steps/s (collection: 0.452s, learning 2.051s)
               Value function loss: 193.7528
                    Surrogate loss: 0.0153
             Mean action noise std: 0.99
                       Mean reward: 303.97
               Mean episode length: 214.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2285568
                    Iteration time: 2.50s
                        Total time: 712.68s
                               ETA: 9507.5s

################################################################################
                     [1m Learning iteration 279/4000 [0m

                       Computation: 3188 steps/s (collection: 0.530s, learning 2.039s)
               Value function loss: 295.1260
                    Surrogate loss: 0.0120
             Mean action noise std: 0.99
                       Mean reward: 308.69
               Mean episode length: 219.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 2.57s
                        Total time: 715.25s
                               ETA: 9505.2s

################################################################################
                     [1m Learning iteration 280/4000 [0m

                       Computation: 3241 steps/s (collection: 0.482s, learning 2.046s)
               Value function loss: 177.3595
                    Surrogate loss: 0.0144
             Mean action noise std: 0.99
                       Mean reward: 316.09
               Mean episode length: 222.24
                 Mean success rate: 0.50
                  Mean reward/step: 1.30
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2301952
                    Iteration time: 2.53s
                        Total time: 717.78s
                               ETA: 9502.3s

################################################################################
                     [1m Learning iteration 281/4000 [0m

                       Computation: 3286 steps/s (collection: 0.440s, learning 2.052s)
               Value function loss: 156.3270
                    Surrogate loss: 0.0144
             Mean action noise std: 0.99
                       Mean reward: 317.14
               Mean episode length: 223.53
                 Mean success rate: 0.50
                  Mean reward/step: 1.32
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 2.49s
                        Total time: 720.27s
                               ETA: 9498.9s

################################################################################
                     [1m Learning iteration 282/4000 [0m

                       Computation: 3287 steps/s (collection: 0.454s, learning 2.038s)
               Value function loss: 230.8760
                    Surrogate loss: 0.0149
             Mean action noise std: 0.99
                       Mean reward: 314.82
               Mean episode length: 223.09
                 Mean success rate: 0.50
                  Mean reward/step: 1.39
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2318336
                    Iteration time: 2.49s
                        Total time: 722.76s
                               ETA: 9495.5s

################################################################################
                     [1m Learning iteration 283/4000 [0m

                       Computation: 3216 steps/s (collection: 0.485s, learning 2.062s)
               Value function loss: 201.4353
                    Surrogate loss: 0.0149
             Mean action noise std: 0.99
                       Mean reward: 325.62
               Mean episode length: 229.63
                 Mean success rate: 0.50
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 2.55s
                        Total time: 725.31s
                               ETA: 9492.9s

################################################################################
                     [1m Learning iteration 284/4000 [0m

                       Computation: 3276 steps/s (collection: 0.434s, learning 2.066s)
               Value function loss: 151.8824
                    Surrogate loss: 0.0147
             Mean action noise std: 0.99
                       Mean reward: 320.07
               Mean episode length: 225.25
                 Mean success rate: 0.50
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2334720
                    Iteration time: 2.50s
                        Total time: 727.81s
                               ETA: 9489.6s

################################################################################
                     [1m Learning iteration 285/4000 [0m

                       Computation: 3170 steps/s (collection: 0.474s, learning 2.110s)
               Value function loss: 194.3857
                    Surrogate loss: 0.0145
             Mean action noise std: 0.99
                       Mean reward: 324.24
               Mean episode length: 231.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 2.58s
                        Total time: 730.39s
                               ETA: 9487.5s

################################################################################
                     [1m Learning iteration 286/4000 [0m

                       Computation: 3139 steps/s (collection: 0.454s, learning 2.155s)
               Value function loss: 181.3381
                    Surrogate loss: 0.0185
             Mean action noise std: 0.99
                       Mean reward: 327.90
               Mean episode length: 235.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2351104
                    Iteration time: 2.61s
                        Total time: 733.00s
                               ETA: 9485.6s

################################################################################
                     [1m Learning iteration 287/4000 [0m

                       Computation: 3241 steps/s (collection: 0.442s, learning 2.085s)
               Value function loss: 159.0243
                    Surrogate loss: 0.0183
             Mean action noise std: 0.99
                       Mean reward: 339.64
               Mean episode length: 242.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 2.53s
                        Total time: 735.53s
                               ETA: 9482.7s

################################################################################
                     [1m Learning iteration 288/4000 [0m

                       Computation: 3259 steps/s (collection: 0.451s, learning 2.062s)
               Value function loss: 226.6916
                    Surrogate loss: 0.0148
             Mean action noise std: 0.99
                       Mean reward: 328.03
               Mean episode length: 238.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2367488
                    Iteration time: 2.51s
                        Total time: 738.04s
                               ETA: 9479.6s

################################################################################
                     [1m Learning iteration 289/4000 [0m

                       Computation: 3172 steps/s (collection: 0.461s, learning 2.121s)
               Value function loss: 200.4347
                    Surrogate loss: 0.0131
             Mean action noise std: 0.99
                       Mean reward: 315.84
               Mean episode length: 230.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 2.58s
                        Total time: 740.63s
                               ETA: 9477.5s

################################################################################
                     [1m Learning iteration 290/4000 [0m

                       Computation: 3200 steps/s (collection: 0.511s, learning 2.049s)
               Value function loss: 129.4705
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 310.94
               Mean episode length: 225.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2383872
                    Iteration time: 2.56s
                        Total time: 743.19s
                               ETA: 9475.0s

################################################################################
                     [1m Learning iteration 291/4000 [0m

                       Computation: 3238 steps/s (collection: 0.510s, learning 2.019s)
               Value function loss: 267.5787
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 290.68
               Mean episode length: 210.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 2.53s
                        Total time: 745.71s
                               ETA: 9472.1s

################################################################################
                     [1m Learning iteration 292/4000 [0m

                       Computation: 3210 steps/s (collection: 0.472s, learning 2.079s)
               Value function loss: 220.8342
                    Surrogate loss: 0.0133
             Mean action noise std: 0.99
                       Mean reward: 260.66
               Mean episode length: 190.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 2400256
                    Iteration time: 2.55s
                        Total time: 748.27s
                               ETA: 9469.5s

################################################################################
                     [1m Learning iteration 293/4000 [0m

                       Computation: 3182 steps/s (collection: 0.465s, learning 2.109s)
               Value function loss: 133.2149
                    Surrogate loss: 0.0166
             Mean action noise std: 0.99
                       Mean reward: 264.35
               Mean episode length: 194.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 2.57s
                        Total time: 750.84s
                               ETA: 9467.2s

################################################################################
                     [1m Learning iteration 294/4000 [0m

                       Computation: 3225 steps/s (collection: 0.446s, learning 2.093s)
               Value function loss: 181.0347
                    Surrogate loss: 0.0161
             Mean action noise std: 0.99
                       Mean reward: 278.32
               Mean episode length: 201.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2416640
                    Iteration time: 2.54s
                        Total time: 753.38s
                               ETA: 9464.5s

################################################################################
                     [1m Learning iteration 295/4000 [0m

                       Computation: 3141 steps/s (collection: 0.504s, learning 2.104s)
               Value function loss: 238.9585
                    Surrogate loss: 0.0135
             Mean action noise std: 0.99
                       Mean reward: 289.97
               Mean episode length: 212.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 2.61s
                        Total time: 755.99s
                               ETA: 9462.6s

################################################################################
                     [1m Learning iteration 296/4000 [0m

                       Computation: 3049 steps/s (collection: 0.514s, learning 2.172s)
               Value function loss: 205.0239
                    Surrogate loss: 0.0132
             Mean action noise std: 0.99
                       Mean reward: 295.78
               Mean episode length: 214.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2433024
                    Iteration time: 2.69s
                        Total time: 758.67s
                               ETA: 9461.7s

################################################################################
                     [1m Learning iteration 297/4000 [0m

                       Computation: 3031 steps/s (collection: 0.560s, learning 2.142s)
               Value function loss: 181.5658
                    Surrogate loss: 0.0138
             Mean action noise std: 0.99
                       Mean reward: 309.43
               Mean episode length: 223.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 2.70s
                        Total time: 761.38s
                               ETA: 9461.0s

################################################################################
                     [1m Learning iteration 298/4000 [0m

                       Computation: 3211 steps/s (collection: 0.463s, learning 2.088s)
               Value function loss: 224.6202
                    Surrogate loss: 0.0138
             Mean action noise std: 0.99
                       Mean reward: 317.72
               Mean episode length: 228.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2449408
                    Iteration time: 2.55s
                        Total time: 763.93s
                               ETA: 9458.4s

################################################################################
                     [1m Learning iteration 299/4000 [0m

                       Computation: 3111 steps/s (collection: 0.485s, learning 2.148s)
               Value function loss: 278.7672
                    Surrogate loss: 0.0133
             Mean action noise std: 0.99
                       Mean reward: 289.50
               Mean episode length: 209.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 2.63s
                        Total time: 766.56s
                               ETA: 9456.8s

################################################################################
                     [1m Learning iteration 300/4000 [0m

                       Computation: 3103 steps/s (collection: 0.513s, learning 2.127s)
               Value function loss: 166.1723
                    Surrogate loss: 0.0132
             Mean action noise std: 0.99
                       Mean reward: 291.17
               Mean episode length: 207.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2465792
                    Iteration time: 2.64s
                        Total time: 769.20s
                               ETA: 9455.3s

################################################################################
                     [1m Learning iteration 301/4000 [0m

                       Computation: 3053 steps/s (collection: 0.546s, learning 2.137s)
               Value function loss: 192.5183
                    Surrogate loss: 0.0139
             Mean action noise std: 0.99
                       Mean reward: 285.40
               Mean episode length: 204.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 2.68s
                        Total time: 771.88s
                               ETA: 9454.3s

################################################################################
                     [1m Learning iteration 302/4000 [0m

                       Computation: 3145 steps/s (collection: 0.524s, learning 2.080s)
               Value function loss: 196.0699
                    Surrogate loss: 0.0149
             Mean action noise std: 0.99
                       Mean reward: 292.13
               Mean episode length: 212.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2482176
                    Iteration time: 2.60s
                        Total time: 774.49s
                               ETA: 9452.3s

################################################################################
                     [1m Learning iteration 303/4000 [0m

                       Computation: 3095 steps/s (collection: 0.522s, learning 2.124s)
               Value function loss: 206.4022
                    Surrogate loss: 0.0169
             Mean action noise std: 0.99
                       Mean reward: 296.90
               Mean episode length: 213.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 2.65s
                        Total time: 777.13s
                               ETA: 9450.9s

################################################################################
                     [1m Learning iteration 304/4000 [0m

                       Computation: 3024 steps/s (collection: 0.547s, learning 2.161s)
               Value function loss: 132.4080
                    Surrogate loss: 0.0184
             Mean action noise std: 0.99
                       Mean reward: 305.68
               Mean episode length: 218.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2498560
                    Iteration time: 2.71s
                        Total time: 779.84s
                               ETA: 9450.1s

################################################################################
                     [1m Learning iteration 305/4000 [0m

                       Computation: 3105 steps/s (collection: 0.491s, learning 2.147s)
               Value function loss: 172.3081
                    Surrogate loss: 0.0135
             Mean action noise std: 0.99
                       Mean reward: 316.54
               Mean episode length: 226.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 2.64s
                        Total time: 782.48s
                               ETA: 9448.6s

################################################################################
                     [1m Learning iteration 306/4000 [0m

                       Computation: 3214 steps/s (collection: 0.513s, learning 2.035s)
               Value function loss: 150.4298
                    Surrogate loss: 0.0202
             Mean action noise std: 1.00
                       Mean reward: 327.32
               Mean episode length: 231.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2514944
                    Iteration time: 2.55s
                        Total time: 785.03s
                               ETA: 9445.9s

################################################################################
                     [1m Learning iteration 307/4000 [0m

                       Computation: 3185 steps/s (collection: 0.524s, learning 2.048s)
               Value function loss: 236.3906
                    Surrogate loss: 0.0166
             Mean action noise std: 0.99
                       Mean reward: 334.13
               Mean episode length: 235.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 2.57s
                        Total time: 787.60s
                               ETA: 9443.5s

################################################################################
                     [1m Learning iteration 308/4000 [0m

                       Computation: 3183 steps/s (collection: 0.512s, learning 2.061s)
               Value function loss: 201.5876
                    Surrogate loss: 0.0168
             Mean action noise std: 1.00
                       Mean reward: 325.96
               Mean episode length: 228.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2531328
                    Iteration time: 2.57s
                        Total time: 790.17s
                               ETA: 9441.1s

################################################################################
                     [1m Learning iteration 309/4000 [0m

                       Computation: 3185 steps/s (collection: 0.501s, learning 2.070s)
               Value function loss: 161.8484
                    Surrogate loss: 0.0158
             Mean action noise std: 0.99
                       Mean reward: 317.02
               Mean episode length: 219.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 2.57s
                        Total time: 792.74s
                               ETA: 9438.8s

################################################################################
                     [1m Learning iteration 310/4000 [0m

                       Computation: 3228 steps/s (collection: 0.485s, learning 2.053s)
               Value function loss: 221.3889
                    Surrogate loss: 0.0150
             Mean action noise std: 0.99
                       Mean reward: 324.30
               Mean episode length: 221.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2547712
                    Iteration time: 2.54s
                        Total time: 795.28s
                               ETA: 9436.0s

################################################################################
                     [1m Learning iteration 311/4000 [0m

                       Computation: 3302 steps/s (collection: 0.463s, learning 2.017s)
               Value function loss: 220.5729
                    Surrogate loss: 0.0147
             Mean action noise std: 0.99
                       Mean reward: 310.35
               Mean episode length: 212.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 2.48s
                        Total time: 797.76s
                               ETA: 9432.5s

################################################################################
                     [1m Learning iteration 312/4000 [0m

                       Computation: 3184 steps/s (collection: 0.495s, learning 2.077s)
               Value function loss: 261.0950
                    Surrogate loss: 0.0141
             Mean action noise std: 0.99
                       Mean reward: 318.46
               Mean episode length: 218.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 2564096
                    Iteration time: 2.57s
                        Total time: 800.33s
                               ETA: 9430.1s

################################################################################
                     [1m Learning iteration 313/4000 [0m

                       Computation: 3214 steps/s (collection: 0.492s, learning 2.056s)
               Value function loss: 259.7843
                    Surrogate loss: 0.0149
             Mean action noise std: 0.99
                       Mean reward: 325.39
               Mean episode length: 222.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 2.55s
                        Total time: 802.88s
                               ETA: 9427.5s

################################################################################
                     [1m Learning iteration 314/4000 [0m

                       Computation: 3248 steps/s (collection: 0.492s, learning 2.030s)
               Value function loss: 243.0629
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 340.79
               Mean episode length: 233.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2580480
                    Iteration time: 2.52s
                        Total time: 805.40s
                               ETA: 9424.5s

################################################################################
                     [1m Learning iteration 315/4000 [0m

                       Computation: 3224 steps/s (collection: 0.473s, learning 2.068s)
               Value function loss: 232.7342
                    Surrogate loss: 0.0148
             Mean action noise std: 0.99
                       Mean reward: 325.95
               Mean episode length: 224.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 2.54s
                        Total time: 807.94s
                               ETA: 9421.7s

################################################################################
                     [1m Learning iteration 316/4000 [0m

                       Computation: 3210 steps/s (collection: 0.493s, learning 2.059s)
               Value function loss: 202.1009
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 316.37
               Mean episode length: 217.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2596864
                    Iteration time: 2.55s
                        Total time: 810.50s
                               ETA: 9419.1s

################################################################################
                     [1m Learning iteration 317/4000 [0m

                       Computation: 3141 steps/s (collection: 0.502s, learning 2.106s)
               Value function loss: 187.1822
                    Surrogate loss: 0.0167
             Mean action noise std: 0.99
                       Mean reward: 299.80
               Mean episode length: 204.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 2.61s
                        Total time: 813.10s
                               ETA: 9417.2s

################################################################################
                     [1m Learning iteration 318/4000 [0m

                       Computation: 3180 steps/s (collection: 0.520s, learning 2.056s)
               Value function loss: 220.7383
                    Surrogate loss: 0.0147
             Mean action noise std: 0.99
                       Mean reward: 285.69
               Mean episode length: 192.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2613248
                    Iteration time: 2.58s
                        Total time: 815.68s
                               ETA: 9414.8s

################################################################################
                     [1m Learning iteration 319/4000 [0m

                       Computation: 3102 steps/s (collection: 0.513s, learning 2.128s)
               Value function loss: 264.1556
                    Surrogate loss: 0.0119
             Mean action noise std: 0.99
                       Mean reward: 276.83
               Mean episode length: 183.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 2.64s
                        Total time: 818.32s
                               ETA: 9413.2s

################################################################################
                     [1m Learning iteration 320/4000 [0m

                       Computation: 2997 steps/s (collection: 0.572s, learning 2.161s)
               Value function loss: 194.1521
                    Surrogate loss: 0.0122
             Mean action noise std: 0.99
                       Mean reward: 298.77
               Mean episode length: 196.64
                 Mean success rate: 0.50
                  Mean reward/step: 1.45
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2629632
                    Iteration time: 2.73s
                        Total time: 821.05s
                               ETA: 9412.7s

################################################################################
                     [1m Learning iteration 321/4000 [0m

                       Computation: 3113 steps/s (collection: 0.522s, learning 2.109s)
               Value function loss: 178.3826
                    Surrogate loss: 0.0172
             Mean action noise std: 0.99
                       Mean reward: 271.43
               Mean episode length: 176.13
                 Mean success rate: 0.50
                  Mean reward/step: 1.51
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 2.63s
                        Total time: 823.68s
                               ETA: 9411.0s

################################################################################
                     [1m Learning iteration 322/4000 [0m

                       Computation: 3214 steps/s (collection: 0.480s, learning 2.068s)
               Value function loss: 188.9245
                    Surrogate loss: 0.0141
             Mean action noise std: 0.99
                       Mean reward: 263.64
               Mean episode length: 173.82
                 Mean success rate: 0.50
                  Mean reward/step: 1.41
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 2646016
                    Iteration time: 2.55s
                        Total time: 826.23s
                               ETA: 9408.3s

################################################################################
                     [1m Learning iteration 323/4000 [0m

                       Computation: 3176 steps/s (collection: 0.487s, learning 2.091s)
               Value function loss: 160.1225
                    Surrogate loss: 0.0165
             Mean action noise std: 0.99
                       Mean reward: 259.48
               Mean episode length: 172.87
                 Mean success rate: 0.50
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 2.58s
                        Total time: 828.81s
                               ETA: 9406.0s

################################################################################
                     [1m Learning iteration 324/4000 [0m

                       Computation: 3114 steps/s (collection: 0.527s, learning 2.104s)
               Value function loss: 181.6853
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 258.83
               Mean episode length: 170.51
                 Mean success rate: 0.50
                  Mean reward/step: 1.48
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2662400
                    Iteration time: 2.63s
                        Total time: 831.44s
                               ETA: 9404.2s

################################################################################
                     [1m Learning iteration 325/4000 [0m

                       Computation: 3138 steps/s (collection: 0.525s, learning 2.086s)
               Value function loss: 258.8163
                    Surrogate loss: 0.0113
             Mean action noise std: 0.99
                       Mean reward: 260.70
               Mean episode length: 177.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 2.61s
                        Total time: 834.05s
                               ETA: 9402.3s

################################################################################
                     [1m Learning iteration 326/4000 [0m

                       Computation: 3183 steps/s (collection: 0.501s, learning 2.073s)
               Value function loss: 205.9325
                    Surrogate loss: 0.0148
             Mean action noise std: 0.99
                       Mean reward: 269.90
               Mean episode length: 185.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 2678784
                    Iteration time: 2.57s
                        Total time: 836.62s
                               ETA: 9399.9s

################################################################################
                     [1m Learning iteration 327/4000 [0m

                       Computation: 3174 steps/s (collection: 0.474s, learning 2.106s)
               Value function loss: 171.8565
                    Surrogate loss: 0.0158
             Mean action noise std: 0.99
                       Mean reward: 277.14
               Mean episode length: 191.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 2.58s
                        Total time: 839.21s
                               ETA: 9397.6s

################################################################################
                     [1m Learning iteration 328/4000 [0m

                       Computation: 3061 steps/s (collection: 0.543s, learning 2.133s)
               Value function loss: 160.4698
                    Surrogate loss: 0.0168
             Mean action noise std: 0.99
                       Mean reward: 286.30
               Mean episode length: 200.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2695168
                    Iteration time: 2.68s
                        Total time: 841.88s
                               ETA: 9396.3s

################################################################################
                     [1m Learning iteration 329/4000 [0m

                       Computation: 3168 steps/s (collection: 0.473s, learning 2.112s)
               Value function loss: 167.0772
                    Surrogate loss: 0.0208
             Mean action noise std: 0.99
                       Mean reward: 302.60
               Mean episode length: 212.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 2.59s
                        Total time: 844.47s
                               ETA: 9394.0s

################################################################################
                     [1m Learning iteration 330/4000 [0m

                       Computation: 3234 steps/s (collection: 0.503s, learning 2.030s)
               Value function loss: 162.1369
                    Surrogate loss: 0.0151
             Mean action noise std: 0.99
                       Mean reward: 304.46
               Mean episode length: 213.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2711552
                    Iteration time: 2.53s
                        Total time: 847.00s
                               ETA: 9391.2s

################################################################################
                     [1m Learning iteration 331/4000 [0m

                       Computation: 3184 steps/s (collection: 0.518s, learning 2.055s)
               Value function loss: 148.9408
                    Surrogate loss: 0.0161
             Mean action noise std: 0.99
                       Mean reward: 305.43
               Mean episode length: 214.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 2.57s
                        Total time: 849.57s
                               ETA: 9388.8s

################################################################################
                     [1m Learning iteration 332/4000 [0m

                       Computation: 3179 steps/s (collection: 0.449s, learning 2.128s)
               Value function loss: 160.8605
                    Surrogate loss: 0.0174
             Mean action noise std: 0.99
                       Mean reward: 324.78
               Mean episode length: 226.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2727936
                    Iteration time: 2.58s
                        Total time: 852.15s
                               ETA: 9386.4s

################################################################################
                     [1m Learning iteration 333/4000 [0m

                       Computation: 3113 steps/s (collection: 0.566s, learning 2.065s)
               Value function loss: 191.4042
                    Surrogate loss: 0.0158
             Mean action noise std: 0.99
                       Mean reward: 323.56
               Mean episode length: 224.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 2.63s
                        Total time: 854.78s
                               ETA: 9384.7s

################################################################################
                     [1m Learning iteration 334/4000 [0m

                       Computation: 3161 steps/s (collection: 0.502s, learning 2.089s)
               Value function loss: 153.4064
                    Surrogate loss: 0.0159
             Mean action noise std: 0.99
                       Mean reward: 304.27
               Mean episode length: 212.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2744320
                    Iteration time: 2.59s
                        Total time: 857.37s
                               ETA: 9382.5s

################################################################################
                     [1m Learning iteration 335/4000 [0m

                       Computation: 3223 steps/s (collection: 0.482s, learning 2.059s)
               Value function loss: 142.6808
                    Surrogate loss: 0.0135
             Mean action noise std: 0.99
                       Mean reward: 316.03
               Mean episode length: 221.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 2.54s
                        Total time: 859.91s
                               ETA: 9379.7s

################################################################################
                     [1m Learning iteration 336/4000 [0m

                       Computation: 3138 steps/s (collection: 0.517s, learning 2.093s)
               Value function loss: 186.6848
                    Surrogate loss: 0.0138
             Mean action noise std: 0.99
                       Mean reward: 310.00
               Mean episode length: 215.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2760704
                    Iteration time: 2.61s
                        Total time: 862.52s
                               ETA: 9377.7s

################################################################################
                     [1m Learning iteration 337/4000 [0m

                       Computation: 3255 steps/s (collection: 0.484s, learning 2.032s)
               Value function loss: 250.3741
                    Surrogate loss: 0.0129
             Mean action noise std: 0.99
                       Mean reward: 329.04
               Mean episode length: 225.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 2.52s
                        Total time: 865.04s
                               ETA: 9374.7s

################################################################################
                     [1m Learning iteration 338/4000 [0m

                       Computation: 3299 steps/s (collection: 0.468s, learning 2.015s)
               Value function loss: 137.2973
                    Surrogate loss: 0.0158
             Mean action noise std: 0.99
                       Mean reward: 321.19
               Mean episode length: 221.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2777088
                    Iteration time: 2.48s
                        Total time: 867.52s
                               ETA: 9371.3s

################################################################################
                     [1m Learning iteration 339/4000 [0m

                       Computation: 3170 steps/s (collection: 0.496s, learning 2.088s)
               Value function loss: 220.3241
                    Surrogate loss: 0.0129
             Mean action noise std: 0.99
                       Mean reward: 338.16
               Mean episode length: 230.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 2.58s
                        Total time: 870.11s
                               ETA: 9369.0s

################################################################################
                     [1m Learning iteration 340/4000 [0m

                       Computation: 3125 steps/s (collection: 0.512s, learning 2.109s)
               Value function loss: 222.9823
                    Surrogate loss: 0.0148
             Mean action noise std: 0.99
                       Mean reward: 372.07
               Mean episode length: 250.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2793472
                    Iteration time: 2.62s
                        Total time: 872.73s
                               ETA: 9367.1s

################################################################################
                     [1m Learning iteration 341/4000 [0m

                       Computation: 3197 steps/s (collection: 0.527s, learning 2.034s)
               Value function loss: 214.6939
                    Surrogate loss: 0.0168
             Mean action noise std: 0.99
                       Mean reward: 366.36
               Mean episode length: 244.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 2.56s
                        Total time: 875.29s
                               ETA: 9364.6s

################################################################################
                     [1m Learning iteration 342/4000 [0m

                       Computation: 3129 steps/s (collection: 0.514s, learning 2.104s)
               Value function loss: 289.9315
                    Surrogate loss: 0.0173
             Mean action noise std: 0.99
                       Mean reward: 395.87
               Mean episode length: 263.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 2809856
                    Iteration time: 2.62s
                        Total time: 877.91s
                               ETA: 9362.6s

################################################################################
                     [1m Learning iteration 343/4000 [0m

                       Computation: 3136 steps/s (collection: 0.483s, learning 2.129s)
               Value function loss: 185.8196
                    Surrogate loss: 0.0149
             Mean action noise std: 0.99
                       Mean reward: 381.41
               Mean episode length: 255.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 2.61s
                        Total time: 880.52s
                               ETA: 9360.6s

################################################################################
                     [1m Learning iteration 344/4000 [0m

                       Computation: 3169 steps/s (collection: 0.486s, learning 2.098s)
               Value function loss: 195.1768
                    Surrogate loss: 0.0172
             Mean action noise std: 0.99
                       Mean reward: 365.11
               Mean episode length: 247.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2826240
                    Iteration time: 2.58s
                        Total time: 883.10s
                               ETA: 9358.3s

################################################################################
                     [1m Learning iteration 345/4000 [0m

                       Computation: 3146 steps/s (collection: 0.519s, learning 2.084s)
               Value function loss: 166.7464
                    Surrogate loss: 0.0199
             Mean action noise std: 0.99
                       Mean reward: 361.40
               Mean episode length: 246.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 2.60s
                        Total time: 885.71s
                               ETA: 9356.2s

################################################################################
                     [1m Learning iteration 346/4000 [0m

                       Computation: 3175 steps/s (collection: 0.490s, learning 2.090s)
               Value function loss: 242.4860
                    Surrogate loss: 0.0195
             Mean action noise std: 0.99
                       Mean reward: 344.70
               Mean episode length: 238.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 2842624
                    Iteration time: 2.58s
                        Total time: 888.29s
                               ETA: 9353.9s

################################################################################
                     [1m Learning iteration 347/4000 [0m

                       Computation: 3187 steps/s (collection: 0.491s, learning 2.079s)
               Value function loss: 173.1263
                    Surrogate loss: 0.0150
             Mean action noise std: 0.99
                       Mean reward: 350.07
               Mean episode length: 240.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 2.57s
                        Total time: 890.86s
                               ETA: 9351.4s

################################################################################
                     [1m Learning iteration 348/4000 [0m

                       Computation: 3143 steps/s (collection: 0.480s, learning 2.126s)
               Value function loss: 252.5236
                    Surrogate loss: 0.0148
             Mean action noise std: 0.99
                       Mean reward: 325.82
               Mean episode length: 224.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2859008
                    Iteration time: 2.61s
                        Total time: 893.46s
                               ETA: 9349.4s

################################################################################
                     [1m Learning iteration 349/4000 [0m

                       Computation: 3152 steps/s (collection: 0.545s, learning 2.053s)
               Value function loss: 195.7371
                    Surrogate loss: 0.0129
             Mean action noise std: 0.99
                       Mean reward: 342.13
               Mean episode length: 237.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 2.60s
                        Total time: 896.06s
                               ETA: 9347.2s

################################################################################
                     [1m Learning iteration 350/4000 [0m

                       Computation: 3131 steps/s (collection: 0.507s, learning 2.110s)
               Value function loss: 217.5370
                    Surrogate loss: 0.0131
             Mean action noise std: 0.99
                       Mean reward: 349.16
               Mean episode length: 238.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2875392
                    Iteration time: 2.62s
                        Total time: 898.68s
                               ETA: 9345.2s

################################################################################
                     [1m Learning iteration 351/4000 [0m

                       Computation: 3148 steps/s (collection: 0.504s, learning 2.098s)
               Value function loss: 156.0732
                    Surrogate loss: 0.0163
             Mean action noise std: 0.99
                       Mean reward: 350.83
               Mean episode length: 237.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 2.60s
                        Total time: 901.28s
                               ETA: 9343.1s

################################################################################
                     [1m Learning iteration 352/4000 [0m

                       Computation: 3239 steps/s (collection: 0.478s, learning 2.051s)
               Value function loss: 173.2017
                    Surrogate loss: 0.0159
             Mean action noise std: 0.99
                       Mean reward: 341.25
               Mean episode length: 228.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2891776
                    Iteration time: 2.53s
                        Total time: 903.81s
                               ETA: 9340.2s

################################################################################
                     [1m Learning iteration 353/4000 [0m

                       Computation: 3156 steps/s (collection: 0.545s, learning 2.051s)
               Value function loss: 235.7172
                    Surrogate loss: 0.0147
             Mean action noise std: 0.99
                       Mean reward: 353.25
               Mean episode length: 236.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 2.60s
                        Total time: 906.40s
                               ETA: 9338.0s

################################################################################
                     [1m Learning iteration 354/4000 [0m

                       Computation: 3153 steps/s (collection: 0.510s, learning 2.088s)
               Value function loss: 249.0087
                    Surrogate loss: 0.0150
             Mean action noise std: 0.99
                       Mean reward: 351.60
               Mean episode length: 235.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2908160
                    Iteration time: 2.60s
                        Total time: 909.00s
                               ETA: 9335.8s

################################################################################
                     [1m Learning iteration 355/4000 [0m

                       Computation: 3202 steps/s (collection: 0.498s, learning 2.060s)
               Value function loss: 192.3862
                    Surrogate loss: 0.0140
             Mean action noise std: 0.99
                       Mean reward: 347.81
               Mean episode length: 230.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 2.56s
                        Total time: 911.56s
                               ETA: 9333.2s

################################################################################
                     [1m Learning iteration 356/4000 [0m

                       Computation: 3186 steps/s (collection: 0.489s, learning 2.082s)
               Value function loss: 196.6910
                    Surrogate loss: 0.0140
             Mean action noise std: 0.99
                       Mean reward: 355.83
               Mean episode length: 239.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2924544
                    Iteration time: 2.57s
                        Total time: 914.13s
                               ETA: 9330.8s

################################################################################
                     [1m Learning iteration 357/4000 [0m

                       Computation: 3072 steps/s (collection: 0.541s, learning 2.125s)
               Value function loss: 269.6093
                    Surrogate loss: 0.0150
             Mean action noise std: 0.99
                       Mean reward: 362.84
               Mean episode length: 245.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 2.67s
                        Total time: 916.80s
                               ETA: 9329.3s

################################################################################
                     [1m Learning iteration 358/4000 [0m

                       Computation: 3094 steps/s (collection: 0.529s, learning 2.118s)
               Value function loss: 222.6248
                    Surrogate loss: 0.0161
             Mean action noise std: 0.99
                       Mean reward: 377.47
               Mean episode length: 256.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2940928
                    Iteration time: 2.65s
                        Total time: 919.44s
                               ETA: 9327.6s

################################################################################
                     [1m Learning iteration 359/4000 [0m

                       Computation: 3128 steps/s (collection: 0.491s, learning 2.127s)
               Value function loss: 221.4756
                    Surrogate loss: 0.0144
             Mean action noise std: 0.99
                       Mean reward: 376.87
               Mean episode length: 256.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 2.62s
                        Total time: 922.06s
                               ETA: 9325.6s

################################################################################
                     [1m Learning iteration 360/4000 [0m

                       Computation: 3177 steps/s (collection: 0.484s, learning 2.095s)
               Value function loss: 138.2613
                    Surrogate loss: 0.0148
             Mean action noise std: 0.99
                       Mean reward: 386.72
               Mean episode length: 265.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2957312
                    Iteration time: 2.58s
                        Total time: 924.64s
                               ETA: 9323.2s

################################################################################
                     [1m Learning iteration 361/4000 [0m

                       Computation: 3183 steps/s (collection: 0.503s, learning 2.070s)
               Value function loss: 209.6057
                    Surrogate loss: 0.0163
             Mean action noise std: 0.99
                       Mean reward: 372.47
               Mean episode length: 255.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 2.57s
                        Total time: 927.21s
                               ETA: 9320.8s

################################################################################
                     [1m Learning iteration 362/4000 [0m

                       Computation: 3205 steps/s (collection: 0.492s, learning 2.063s)
               Value function loss: 167.7080
                    Surrogate loss: 0.0129
             Mean action noise std: 0.99
                       Mean reward: 364.02
               Mean episode length: 250.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2973696
                    Iteration time: 2.56s
                        Total time: 929.77s
                               ETA: 9318.2s

################################################################################
                     [1m Learning iteration 363/4000 [0m

                       Computation: 3266 steps/s (collection: 0.462s, learning 2.046s)
               Value function loss: 165.6527
                    Surrogate loss: 0.0147
             Mean action noise std: 0.99
                       Mean reward: 361.05
               Mean episode length: 248.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 2.51s
                        Total time: 932.28s
                               ETA: 9315.1s

################################################################################
                     [1m Learning iteration 364/4000 [0m

                       Computation: 3273 steps/s (collection: 0.452s, learning 2.051s)
               Value function loss: 199.4316
                    Surrogate loss: 0.0125
             Mean action noise std: 0.99
                       Mean reward: 374.43
               Mean episode length: 255.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2990080
                    Iteration time: 2.50s
                        Total time: 934.78s
                               ETA: 9311.9s

################################################################################
                     [1m Learning iteration 365/4000 [0m

                       Computation: 3175 steps/s (collection: 0.510s, learning 2.069s)
               Value function loss: 179.9748
                    Surrogate loss: 0.0154
             Mean action noise std: 0.99
                       Mean reward: 385.81
               Mean episode length: 262.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 2.58s
                        Total time: 937.36s
                               ETA: 9309.6s

################################################################################
                     [1m Learning iteration 366/4000 [0m

                       Computation: 3145 steps/s (collection: 0.508s, learning 2.096s)
               Value function loss: 150.9373
                    Surrogate loss: 0.0206
             Mean action noise std: 0.99
                       Mean reward: 376.87
               Mean episode length: 256.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3006464
                    Iteration time: 2.60s
                        Total time: 939.96s
                               ETA: 9307.4s

################################################################################
                     [1m Learning iteration 367/4000 [0m

                       Computation: 3141 steps/s (collection: 0.513s, learning 2.095s)
               Value function loss: 197.5603
                    Surrogate loss: 0.0157
             Mean action noise std: 0.99
                       Mean reward: 399.60
               Mean episode length: 270.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 2.61s
                        Total time: 942.57s
                               ETA: 9305.3s

################################################################################
                     [1m Learning iteration 368/4000 [0m

                       Computation: 3157 steps/s (collection: 0.481s, learning 2.113s)
               Value function loss: 172.9654
                    Surrogate loss: 0.0159
             Mean action noise std: 0.99
                       Mean reward: 396.73
               Mean episode length: 267.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3022848
                    Iteration time: 2.59s
                        Total time: 945.17s
                               ETA: 9303.1s

################################################################################
                     [1m Learning iteration 369/4000 [0m

                       Computation: 3210 steps/s (collection: 0.507s, learning 2.045s)
               Value function loss: 208.6754
                    Surrogate loss: 0.0174
             Mean action noise std: 0.99
                       Mean reward: 393.48
               Mean episode length: 264.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 2.55s
                        Total time: 947.72s
                               ETA: 9300.4s

################################################################################
                     [1m Learning iteration 370/4000 [0m

                       Computation: 3228 steps/s (collection: 0.470s, learning 2.067s)
               Value function loss: 234.9361
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 383.41
               Mean episode length: 256.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3039232
                    Iteration time: 2.54s
                        Total time: 950.25s
                               ETA: 9297.6s

################################################################################
                     [1m Learning iteration 371/4000 [0m

                       Computation: 3206 steps/s (collection: 0.479s, learning 2.076s)
               Value function loss: 208.6035
                    Surrogate loss: 0.0147
             Mean action noise std: 0.99
                       Mean reward: 365.99
               Mean episode length: 243.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 2.55s
                        Total time: 952.81s
                               ETA: 9295.0s

################################################################################
                     [1m Learning iteration 372/4000 [0m

                       Computation: 3200 steps/s (collection: 0.508s, learning 2.051s)
               Value function loss: 166.0673
                    Surrogate loss: 0.0156
             Mean action noise std: 0.99
                       Mean reward: 386.94
               Mean episode length: 256.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3055616
                    Iteration time: 2.56s
                        Total time: 955.37s
                               ETA: 9292.4s

################################################################################
                     [1m Learning iteration 373/4000 [0m

                       Computation: 3167 steps/s (collection: 0.512s, learning 2.074s)
               Value function loss: 225.1969
                    Surrogate loss: 0.0117
             Mean action noise std: 0.99
                       Mean reward: 379.83
               Mean episode length: 254.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 2.59s
                        Total time: 957.95s
                               ETA: 9290.1s

################################################################################
                     [1m Learning iteration 374/4000 [0m

                       Computation: 3230 steps/s (collection: 0.463s, learning 2.073s)
               Value function loss: 186.4341
                    Surrogate loss: 0.0144
             Mean action noise std: 0.99
                       Mean reward: 389.46
               Mean episode length: 259.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3072000
                    Iteration time: 2.54s
                        Total time: 960.49s
                               ETA: 9287.3s

################################################################################
                     [1m Learning iteration 375/4000 [0m

                       Computation: 3342 steps/s (collection: 0.444s, learning 2.007s)
               Value function loss: 194.0738
                    Surrogate loss: 0.0167
             Mean action noise std: 0.99
                       Mean reward: 406.83
               Mean episode length: 268.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 2.45s
                        Total time: 962.94s
                               ETA: 9283.7s

################################################################################
                     [1m Learning iteration 376/4000 [0m

                       Computation: 3326 steps/s (collection: 0.452s, learning 2.011s)
               Value function loss: 228.9599
                    Surrogate loss: 0.0152
             Mean action noise std: 0.99
                       Mean reward: 429.63
               Mean episode length: 281.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3088384
                    Iteration time: 2.46s
                        Total time: 965.40s
                               ETA: 9280.2s

################################################################################
                     [1m Learning iteration 377/4000 [0m

                       Computation: 3273 steps/s (collection: 0.458s, learning 2.045s)
               Value function loss: 223.1442
                    Surrogate loss: 0.0159
             Mean action noise std: 0.99
                       Mean reward: 428.77
               Mean episode length: 281.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 2.50s
                        Total time: 967.91s
                               ETA: 9277.1s

################################################################################
                     [1m Learning iteration 378/4000 [0m

                       Computation: 3257 steps/s (collection: 0.472s, learning 2.043s)
               Value function loss: 337.6849
                    Surrogate loss: 0.0150
             Mean action noise std: 0.99
                       Mean reward: 434.78
               Mean episode length: 288.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3104768
                    Iteration time: 2.51s
                        Total time: 970.42s
                               ETA: 9274.1s

################################################################################
                     [1m Learning iteration 379/4000 [0m

                       Computation: 3232 steps/s (collection: 0.448s, learning 2.086s)
               Value function loss: 225.5141
                    Surrogate loss: 0.0157
             Mean action noise std: 0.99
                       Mean reward: 427.26
               Mean episode length: 283.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 2.53s
                        Total time: 972.96s
                               ETA: 9271.2s

################################################################################
                     [1m Learning iteration 380/4000 [0m

                       Computation: 3212 steps/s (collection: 0.498s, learning 2.053s)
               Value function loss: 225.5667
                    Surrogate loss: 0.0136
             Mean action noise std: 0.99
                       Mean reward: 422.27
               Mean episode length: 277.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3121152
                    Iteration time: 2.55s
                        Total time: 975.51s
                               ETA: 9268.6s

################################################################################
                     [1m Learning iteration 381/4000 [0m

                       Computation: 3183 steps/s (collection: 0.509s, learning 2.065s)
               Value function loss: 240.8590
                    Surrogate loss: 0.0160
             Mean action noise std: 0.99
                       Mean reward: 404.27
               Mean episode length: 267.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 2.57s
                        Total time: 978.08s
                               ETA: 9266.2s

################################################################################
                     [1m Learning iteration 382/4000 [0m

                       Computation: 3214 steps/s (collection: 0.507s, learning 2.041s)
               Value function loss: 245.5553
                    Surrogate loss: 0.0153
             Mean action noise std: 0.99
                       Mean reward: 379.49
               Mean episode length: 252.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 3137536
                    Iteration time: 2.55s
                        Total time: 980.63s
                               ETA: 9263.5s

################################################################################
                     [1m Learning iteration 383/4000 [0m

                       Computation: 3165 steps/s (collection: 0.478s, learning 2.110s)
               Value function loss: 232.1024
                    Surrogate loss: 0.0176
             Mean action noise std: 0.99
                       Mean reward: 376.94
               Mean episode length: 249.67
                 Mean success rate: 0.50
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 2.59s
                        Total time: 983.22s
                               ETA: 9261.2s

################################################################################
                     [1m Learning iteration 384/4000 [0m

                       Computation: 3179 steps/s (collection: 0.531s, learning 2.046s)
               Value function loss: 279.2244
                    Surrogate loss: 0.0140
             Mean action noise std: 0.99
                       Mean reward: 380.93
               Mean episode length: 249.10
                 Mean success rate: 0.50
                  Mean reward/step: 1.60
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3153920
                    Iteration time: 2.58s
                        Total time: 985.79s
                               ETA: 9258.8s

################################################################################
                     [1m Learning iteration 385/4000 [0m

                       Computation: 3218 steps/s (collection: 0.487s, learning 2.058s)
               Value function loss: 219.3647
                    Surrogate loss: 0.0135
             Mean action noise std: 0.99
                       Mean reward: 395.07
               Mean episode length: 254.91
                 Mean success rate: 0.50
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 2.55s
                        Total time: 988.34s
                               ETA: 9256.1s

################################################################################
                     [1m Learning iteration 386/4000 [0m

                       Computation: 3248 steps/s (collection: 0.464s, learning 2.058s)
               Value function loss: 227.0273
                    Surrogate loss: 0.0181
             Mean action noise std: 0.99
                       Mean reward: 390.40
               Mean episode length: 250.60
                 Mean success rate: 0.50
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3170304
                    Iteration time: 2.52s
                        Total time: 990.86s
                               ETA: 9253.1s

################################################################################
                     [1m Learning iteration 387/4000 [0m

                       Computation: 3153 steps/s (collection: 0.526s, learning 2.072s)
               Value function loss: 200.3962
                    Surrogate loss: 0.0159
             Mean action noise std: 0.99
                       Mean reward: 392.51
               Mean episode length: 251.17
                 Mean success rate: 0.50
                  Mean reward/step: 1.54
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 2.60s
                        Total time: 993.46s
                               ETA: 9250.9s

################################################################################
                     [1m Learning iteration 388/4000 [0m

                       Computation: 3255 steps/s (collection: 0.482s, learning 2.035s)
               Value function loss: 217.5271
                    Surrogate loss: 0.0161
             Mean action noise std: 0.99
                       Mean reward: 384.97
               Mean episode length: 241.36
                 Mean success rate: 0.50
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3186688
                    Iteration time: 2.52s
                        Total time: 995.97s
                               ETA: 9248.0s

################################################################################
                     [1m Learning iteration 389/4000 [0m

                       Computation: 3254 steps/s (collection: 0.452s, learning 2.065s)
               Value function loss: 202.9907
                    Surrogate loss: 0.0167
             Mean action noise std: 0.99
                       Mean reward: 381.39
               Mean episode length: 240.05
                 Mean success rate: 0.50
                  Mean reward/step: 1.67
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 2.52s
                        Total time: 998.49s
                               ETA: 9245.0s

################################################################################
                     [1m Learning iteration 390/4000 [0m

                       Computation: 3221 steps/s (collection: 0.490s, learning 2.053s)
               Value function loss: 278.8036
                    Surrogate loss: 0.0153
             Mean action noise std: 0.99
                       Mean reward: 370.87
               Mean episode length: 231.53
                 Mean success rate: 0.50
                  Mean reward/step: 1.77
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3203072
                    Iteration time: 2.54s
                        Total time: 1001.03s
                               ETA: 9242.3s

################################################################################
                     [1m Learning iteration 391/4000 [0m

                       Computation: 3292 steps/s (collection: 0.443s, learning 2.044s)
               Value function loss: 315.6332
                    Surrogate loss: 0.0177
             Mean action noise std: 0.99
                       Mean reward: 344.14
               Mean episode length: 216.10
                 Mean success rate: 0.50
                  Mean reward/step: 1.73
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 2.49s
                        Total time: 1003.52s
                               ETA: 9239.1s

################################################################################
                     [1m Learning iteration 392/4000 [0m

                       Computation: 3161 steps/s (collection: 0.511s, learning 2.080s)
               Value function loss: 210.2254
                    Surrogate loss: 0.0154
             Mean action noise std: 0.99
                       Mean reward: 348.74
               Mean episode length: 215.91
                 Mean success rate: 0.50
                  Mean reward/step: 1.61
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3219456
                    Iteration time: 2.59s
                        Total time: 1006.11s
                               ETA: 9236.8s

################################################################################
                     [1m Learning iteration 393/4000 [0m

                       Computation: 3127 steps/s (collection: 0.510s, learning 2.110s)
               Value function loss: 233.0384
                    Surrogate loss: 0.0176
             Mean action noise std: 0.99
                       Mean reward: 319.12
               Mean episode length: 198.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 2.62s
                        Total time: 1008.73s
                               ETA: 9234.8s

################################################################################
                     [1m Learning iteration 394/4000 [0m

                       Computation: 3170 steps/s (collection: 0.511s, learning 2.073s)
               Value function loss: 218.5420
                    Surrogate loss: 0.0179
             Mean action noise std: 0.99
                       Mean reward: 314.55
               Mean episode length: 194.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3235840
                    Iteration time: 2.58s
                        Total time: 1011.32s
                               ETA: 9232.4s

################################################################################
                     [1m Learning iteration 395/4000 [0m

                       Computation: 3261 steps/s (collection: 0.487s, learning 2.024s)
               Value function loss: 206.9908
                    Surrogate loss: 0.0171
             Mean action noise std: 0.99
                       Mean reward: 308.32
               Mean episode length: 188.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 2.51s
                        Total time: 1013.83s
                               ETA: 9229.4s

################################################################################
                     [1m Learning iteration 396/4000 [0m

                       Computation: 3177 steps/s (collection: 0.516s, learning 2.062s)
               Value function loss: 262.9865
                    Surrogate loss: 0.0165
             Mean action noise std: 0.99
                       Mean reward: 323.88
               Mean episode length: 195.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3252224
                    Iteration time: 2.58s
                        Total time: 1016.41s
                               ETA: 9227.0s

################################################################################
                     [1m Learning iteration 397/4000 [0m

                       Computation: 3221 steps/s (collection: 0.479s, learning 2.064s)
               Value function loss: 311.1772
                    Surrogate loss: 0.0131
             Mean action noise std: 0.99
                       Mean reward: 343.40
               Mean episode length: 205.85
                 Mean success rate: 0.50
                  Mean reward/step: 1.68
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 2.54s
                        Total time: 1018.95s
                               ETA: 9224.3s

################################################################################
                     [1m Learning iteration 398/4000 [0m

                       Computation: 3196 steps/s (collection: 0.492s, learning 2.072s)
               Value function loss: 317.0485
                    Surrogate loss: 0.0134
             Mean action noise std: 0.99
                       Mean reward: 358.26
               Mean episode length: 214.19
                 Mean success rate: 0.50
                  Mean reward/step: 1.65
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3268608
                    Iteration time: 2.56s
                        Total time: 1021.51s
                               ETA: 9221.8s

################################################################################
                     [1m Learning iteration 399/4000 [0m

                       Computation: 3289 steps/s (collection: 0.460s, learning 2.030s)
               Value function loss: 288.5797
                    Surrogate loss: 0.0164
             Mean action noise std: 0.99
                       Mean reward: 380.98
               Mean episode length: 230.15
                 Mean success rate: 0.50
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 2.49s
                        Total time: 1024.00s
                               ETA: 9218.6s

################################################################################
                     [1m Learning iteration 400/4000 [0m

                       Computation: 3239 steps/s (collection: 0.490s, learning 2.039s)
               Value function loss: 278.7316
                    Surrogate loss: 0.0154
             Mean action noise std: 0.99
                       Mean reward: 386.56
               Mean episode length: 235.72
                 Mean success rate: 0.50
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3284992
                    Iteration time: 2.53s
                        Total time: 1026.53s
                               ETA: 9215.7s

################################################################################
                     [1m Learning iteration 401/4000 [0m

                       Computation: 3243 steps/s (collection: 0.466s, learning 2.059s)
               Value function loss: 234.4363
                    Surrogate loss: 0.0155
             Mean action noise std: 0.99
                       Mean reward: 394.52
               Mean episode length: 241.89
                 Mean success rate: 0.50
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 2.53s
                        Total time: 1029.06s
                               ETA: 9212.9s

################################################################################
                     [1m Learning iteration 402/4000 [0m

                       Computation: 3199 steps/s (collection: 0.469s, learning 2.092s)
               Value function loss: 277.0809
                    Surrogate loss: 0.0149
             Mean action noise std: 0.99
                       Mean reward: 392.49
               Mean episode length: 241.54
                 Mean success rate: 0.50
                  Mean reward/step: 1.58
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 3301376
                    Iteration time: 2.56s
                        Total time: 1031.62s
                               ETA: 9210.3s

################################################################################
                     [1m Learning iteration 403/4000 [0m

                       Computation: 3293 steps/s (collection: 0.439s, learning 2.048s)
               Value function loss: 264.6413
                    Surrogate loss: 0.0159
             Mean action noise std: 0.99
                       Mean reward: 378.32
               Mean episode length: 234.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 2.49s
                        Total time: 1034.10s
                               ETA: 9207.1s

################################################################################
                     [1m Learning iteration 404/4000 [0m

                       Computation: 3255 steps/s (collection: 0.476s, learning 2.040s)
               Value function loss: 403.2547
                    Surrogate loss: 0.0155
             Mean action noise std: 0.99
                       Mean reward: 380.40
               Mean episode length: 234.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3317760
                    Iteration time: 2.52s
                        Total time: 1036.62s
                               ETA: 9204.2s

################################################################################
                     [1m Learning iteration 405/4000 [0m

                       Computation: 3240 steps/s (collection: 0.467s, learning 2.061s)
               Value function loss: 285.5985
                    Surrogate loss: 0.0158
             Mean action noise std: 0.99
                       Mean reward: 382.94
               Mean episode length: 234.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 2.53s
                        Total time: 1039.15s
                               ETA: 9201.3s

################################################################################
                     [1m Learning iteration 406/4000 [0m

                       Computation: 3214 steps/s (collection: 0.491s, learning 2.058s)
               Value function loss: 324.3169
                    Surrogate loss: 0.0187
             Mean action noise std: 0.99
                       Mean reward: 387.76
               Mean episode length: 238.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3334144
                    Iteration time: 2.55s
                        Total time: 1041.70s
                               ETA: 9198.7s

################################################################################
                     [1m Learning iteration 407/4000 [0m

                       Computation: 3233 steps/s (collection: 0.459s, learning 2.075s)
               Value function loss: 351.2245
                    Surrogate loss: 0.0169
             Mean action noise std: 0.99
                       Mean reward: 392.05
               Mean episode length: 237.00
                 Mean success rate: 0.50
                  Mean reward/step: 1.69
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 2.53s
                        Total time: 1044.23s
                               ETA: 9195.9s

################################################################################
                     [1m Learning iteration 408/4000 [0m

                       Computation: 3230 steps/s (collection: 0.487s, learning 2.049s)
               Value function loss: 433.7474
                    Surrogate loss: 0.0151
             Mean action noise std: 0.99
                       Mean reward: 424.20
               Mean episode length: 255.94
                 Mean success rate: 1.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3350528
                    Iteration time: 2.54s
                        Total time: 1046.77s
                               ETA: 9193.1s

################################################################################
                     [1m Learning iteration 409/4000 [0m

                       Computation: 3238 steps/s (collection: 0.463s, learning 2.067s)
               Value function loss: 313.3667
                    Surrogate loss: 0.0170
             Mean action noise std: 0.99
                       Mean reward: 425.06
               Mean episode length: 253.79
                 Mean success rate: 1.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 2.53s
                        Total time: 1049.30s
                               ETA: 9190.3s

################################################################################
                     [1m Learning iteration 410/4000 [0m

                       Computation: 3267 steps/s (collection: 0.461s, learning 2.046s)
               Value function loss: 231.9131
                    Surrogate loss: 0.0172
             Mean action noise std: 0.99
                       Mean reward: 437.32
               Mean episode length: 259.96
                 Mean success rate: 1.50
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3366912
                    Iteration time: 2.51s
                        Total time: 1051.80s
                               ETA: 9187.3s

################################################################################
                     [1m Learning iteration 411/4000 [0m

                       Computation: 3194 steps/s (collection: 0.442s, learning 2.123s)
               Value function loss: 278.3929
                    Surrogate loss: 0.0152
             Mean action noise std: 0.99
                       Mean reward: 450.96
               Mean episode length: 270.82
                 Mean success rate: 1.50
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 2.56s
                        Total time: 1054.37s
                               ETA: 9184.8s

################################################################################
                     [1m Learning iteration 412/4000 [0m

                       Computation: 3224 steps/s (collection: 0.482s, learning 2.059s)
               Value function loss: 146.7583
                    Surrogate loss: 0.0189
             Mean action noise std: 0.99
                       Mean reward: 451.82
               Mean episode length: 271.81
                 Mean success rate: 1.50
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3383296
                    Iteration time: 2.54s
                        Total time: 1056.91s
                               ETA: 9182.1s

################################################################################
                     [1m Learning iteration 413/4000 [0m

                       Computation: 3276 steps/s (collection: 0.475s, learning 2.025s)
               Value function loss: 413.5910
                    Surrogate loss: 0.0143
             Mean action noise std: 0.99
                       Mean reward: 465.97
               Mean episode length: 274.46
                 Mean success rate: 1.50
                  Mean reward/step: 1.66
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 2.50s
                        Total time: 1059.41s
                               ETA: 9179.0s

################################################################################
                     [1m Learning iteration 414/4000 [0m

                       Computation: 3149 steps/s (collection: 0.503s, learning 2.098s)
               Value function loss: 255.3707
                    Surrogate loss: 0.0153
             Mean action noise std: 0.99
                       Mean reward: 463.55
               Mean episode length: 271.85
                 Mean success rate: 1.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3399680
                    Iteration time: 2.60s
                        Total time: 1062.01s
                               ETA: 9176.8s

################################################################################
                     [1m Learning iteration 415/4000 [0m

                       Computation: 3304 steps/s (collection: 0.437s, learning 2.042s)
               Value function loss: 327.9880
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 465.26
               Mean episode length: 278.73
                 Mean success rate: 0.50
                  Mean reward/step: 1.73
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 2.48s
                        Total time: 1064.49s
                               ETA: 9173.5s

################################################################################
                     [1m Learning iteration 416/4000 [0m

                       Computation: 3249 steps/s (collection: 0.456s, learning 2.065s)
               Value function loss: 380.7360
                    Surrogate loss: 0.0126
             Mean action noise std: 0.99
                       Mean reward: 467.53
               Mean episode length: 282.23
                 Mean success rate: 0.50
                  Mean reward/step: 1.79
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3416064
                    Iteration time: 2.52s
                        Total time: 1067.01s
                               ETA: 9170.7s

################################################################################
                     [1m Learning iteration 417/4000 [0m

                       Computation: 3241 steps/s (collection: 0.433s, learning 2.094s)
               Value function loss: 437.7519
                    Surrogate loss: 0.0114
             Mean action noise std: 0.99
                       Mean reward: 459.53
               Mean episode length: 279.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 2.53s
                        Total time: 1069.54s
                               ETA: 9167.8s

################################################################################
                     [1m Learning iteration 418/4000 [0m

                       Computation: 3235 steps/s (collection: 0.457s, learning 2.075s)
               Value function loss: 446.6494
                    Surrogate loss: 0.0132
             Mean action noise std: 0.99
                       Mean reward: 466.13
               Mean episode length: 278.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3432448
                    Iteration time: 2.53s
                        Total time: 1072.07s
                               ETA: 9165.0s

################################################################################
                     [1m Learning iteration 419/4000 [0m

                       Computation: 3266 steps/s (collection: 0.458s, learning 2.050s)
               Value function loss: 316.6798
                    Surrogate loss: 0.0180
             Mean action noise std: 0.99
                       Mean reward: 466.67
               Mean episode length: 281.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 2.51s
                        Total time: 1074.58s
                               ETA: 9162.0s

################################################################################
                     [1m Learning iteration 420/4000 [0m

                       Computation: 3238 steps/s (collection: 0.494s, learning 2.036s)
               Value function loss: 394.7433
                    Surrogate loss: 0.0129
             Mean action noise std: 0.99
                       Mean reward: 491.80
               Mean episode length: 297.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3448832
                    Iteration time: 2.53s
                        Total time: 1077.11s
                               ETA: 9159.2s

################################################################################
                     [1m Learning iteration 421/4000 [0m

                       Computation: 3313 steps/s (collection: 0.441s, learning 2.031s)
               Value function loss: 322.6213
                    Surrogate loss: 0.0145
             Mean action noise std: 0.99
                       Mean reward: 475.87
               Mean episode length: 288.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 2.47s
                        Total time: 1079.58s
                               ETA: 9156.0s

################################################################################
                     [1m Learning iteration 422/4000 [0m

                       Computation: 3276 steps/s (collection: 0.451s, learning 2.049s)
               Value function loss: 419.7345
                    Surrogate loss: 0.0147
             Mean action noise std: 0.99
                       Mean reward: 458.06
               Mean episode length: 276.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3465216
                    Iteration time: 2.50s
                        Total time: 1082.08s
                               ETA: 9152.9s

################################################################################
                     [1m Learning iteration 423/4000 [0m

                       Computation: 3243 steps/s (collection: 0.477s, learning 2.049s)
               Value function loss: 445.7492
                    Surrogate loss: 0.0151
             Mean action noise std: 0.99
                       Mean reward: 453.63
               Mean episode length: 274.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 2.53s
                        Total time: 1084.60s
                               ETA: 9150.1s

################################################################################
                     [1m Learning iteration 424/4000 [0m

                       Computation: 3168 steps/s (collection: 0.519s, learning 2.066s)
               Value function loss: 428.8680
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 435.55
               Mean episode length: 265.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3481600
                    Iteration time: 2.59s
                        Total time: 1087.19s
                               ETA: 9147.7s

################################################################################
                     [1m Learning iteration 425/4000 [0m

                       Computation: 3195 steps/s (collection: 0.487s, learning 2.077s)
               Value function loss: 551.4295
                    Surrogate loss: 0.0161
             Mean action noise std: 0.99
                       Mean reward: 433.20
               Mean episode length: 262.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.87
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 2.56s
                        Total time: 1089.75s
                               ETA: 9145.2s

################################################################################
                     [1m Learning iteration 426/4000 [0m

                       Computation: 3130 steps/s (collection: 0.511s, learning 2.106s)
               Value function loss: 368.4825
                    Surrogate loss: 0.0154
             Mean action noise std: 0.99
                       Mean reward: 410.38
               Mean episode length: 244.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3497984
                    Iteration time: 2.62s
                        Total time: 1092.37s
                               ETA: 9143.2s

################################################################################
                     [1m Learning iteration 427/4000 [0m

                       Computation: 3133 steps/s (collection: 0.517s, learning 2.097s)
               Value function loss: 536.4881
                    Surrogate loss: 0.0169
             Mean action noise std: 0.98
                       Mean reward: 426.16
               Mean episode length: 250.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 2.61s
                        Total time: 1094.98s
                               ETA: 9141.1s

################################################################################
                     [1m Learning iteration 428/4000 [0m

                       Computation: 3207 steps/s (collection: 0.516s, learning 2.038s)
               Value function loss: 522.7142
                    Surrogate loss: 0.0125
             Mean action noise std: 0.98
                       Mean reward: 416.56
               Mean episode length: 237.06
                 Mean success rate: 0.50
                  Mean reward/step: 1.97
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3514368
                    Iteration time: 2.55s
                        Total time: 1097.54s
                               ETA: 9138.5s

################################################################################
                     [1m Learning iteration 429/4000 [0m

                       Computation: 3205 steps/s (collection: 0.489s, learning 2.067s)
               Value function loss: 703.7208
                    Surrogate loss: 0.0142
             Mean action noise std: 0.98
                       Mean reward: 449.41
               Mean episode length: 247.71
                 Mean success rate: 0.50
                  Mean reward/step: 2.02
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 2.56s
                        Total time: 1100.09s
                               ETA: 9135.9s

################################################################################
                     [1m Learning iteration 430/4000 [0m

                       Computation: 3227 steps/s (collection: 0.472s, learning 2.066s)
               Value function loss: 734.7838
                    Surrogate loss: 0.0129
             Mean action noise std: 0.98
                       Mean reward: 457.43
               Mean episode length: 251.22
                 Mean success rate: 0.50
                  Mean reward/step: 2.04
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3530752
                    Iteration time: 2.54s
                        Total time: 1102.63s
                               ETA: 9133.2s

################################################################################
                     [1m Learning iteration 431/4000 [0m

                       Computation: 3206 steps/s (collection: 0.475s, learning 2.080s)
               Value function loss: 434.6649
                    Surrogate loss: 0.0151
             Mean action noise std: 0.98
                       Mean reward: 477.27
               Mean episode length: 261.11
                 Mean success rate: 0.50
                  Mean reward/step: 1.88
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 2.55s
                        Total time: 1105.19s
                               ETA: 9130.6s

################################################################################
                     [1m Learning iteration 432/4000 [0m

                       Computation: 3222 steps/s (collection: 0.466s, learning 2.076s)
               Value function loss: 548.0729
                    Surrogate loss: 0.0129
             Mean action noise std: 0.98
                       Mean reward: 482.58
               Mean episode length: 261.22
                 Mean success rate: 0.50
                  Mean reward/step: 1.76
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3547136
                    Iteration time: 2.54s
                        Total time: 1107.73s
                               ETA: 9127.9s

################################################################################
                     [1m Learning iteration 433/4000 [0m

                       Computation: 3223 steps/s (collection: 0.474s, learning 2.068s)
               Value function loss: 545.6841
                    Surrogate loss: 0.0147
             Mean action noise std: 0.98
                       Mean reward: 479.13
               Mean episode length: 263.76
                 Mean success rate: 0.50
                  Mean reward/step: 1.83
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 2.54s
                        Total time: 1110.27s
                               ETA: 9125.2s

################################################################################
                     [1m Learning iteration 434/4000 [0m

                       Computation: 3169 steps/s (collection: 0.478s, learning 2.106s)
               Value function loss: 488.4043
                    Surrogate loss: 0.0144
             Mean action noise std: 0.98
                       Mean reward: 503.89
               Mean episode length: 277.05
                 Mean success rate: 0.50
                  Mean reward/step: 1.72
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3563520
                    Iteration time: 2.58s
                        Total time: 1112.85s
                               ETA: 9122.8s

################################################################################
                     [1m Learning iteration 435/4000 [0m

                       Computation: 3093 steps/s (collection: 0.509s, learning 2.138s)
               Value function loss: 435.2042
                    Surrogate loss: 0.0138
             Mean action noise std: 0.98
                       Mean reward: 483.33
               Mean episode length: 269.87
                 Mean success rate: 0.50
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 2.65s
                        Total time: 1115.50s
                               ETA: 9121.0s

################################################################################
                     [1m Learning iteration 436/4000 [0m

                       Computation: 3169 steps/s (collection: 0.474s, learning 2.111s)
               Value function loss: 394.8574
                    Surrogate loss: 0.0173
             Mean action noise std: 0.98
                       Mean reward: 449.67
               Mean episode length: 252.84
                 Mean success rate: 0.50
                  Mean reward/step: 1.76
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3579904
                    Iteration time: 2.58s
                        Total time: 1118.09s
                               ETA: 9118.7s

################################################################################
                     [1m Learning iteration 437/4000 [0m

                       Computation: 3209 steps/s (collection: 0.437s, learning 2.115s)
               Value function loss: 329.8451
                    Surrogate loss: 0.0152
             Mean action noise std: 0.98
                       Mean reward: 464.36
               Mean episode length: 256.47
                 Mean success rate: 0.50
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 2.55s
                        Total time: 1120.64s
                               ETA: 9116.1s

################################################################################
                     [1m Learning iteration 438/4000 [0m

                       Computation: 3079 steps/s (collection: 0.498s, learning 2.163s)
               Value function loss: 552.4232
                    Surrogate loss: 0.0128
             Mean action noise std: 0.98
                       Mean reward: 463.74
               Mean episode length: 255.26
                 Mean success rate: 0.50
                  Mean reward/step: 1.75
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3596288
                    Iteration time: 2.66s
                        Total time: 1123.30s
                               ETA: 9114.3s

################################################################################
                     [1m Learning iteration 439/4000 [0m

                       Computation: 3145 steps/s (collection: 0.492s, learning 2.113s)
               Value function loss: 415.7625
                    Surrogate loss: 0.0154
             Mean action noise std: 0.98
                       Mean reward: 472.62
               Mean episode length: 254.19
                 Mean success rate: 1.50
                  Mean reward/step: 1.78
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 2.60s
                        Total time: 1125.90s
                               ETA: 9112.1s

################################################################################
                     [1m Learning iteration 440/4000 [0m

                       Computation: 3161 steps/s (collection: 0.482s, learning 2.108s)
               Value function loss: 504.4883
                    Surrogate loss: 0.0160
             Mean action noise std: 0.98
                       Mean reward: 474.16
               Mean episode length: 257.34
                 Mean success rate: 1.50
                  Mean reward/step: 1.84
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3612672
                    Iteration time: 2.59s
                        Total time: 1128.49s
                               ETA: 9109.8s

################################################################################
                     [1m Learning iteration 441/4000 [0m

                       Computation: 3215 steps/s (collection: 0.445s, learning 2.102s)
               Value function loss: 528.3731
                    Surrogate loss: 0.0177
             Mean action noise std: 0.98
                       Mean reward: 467.44
               Mean episode length: 256.12
                 Mean success rate: 1.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 2.55s
                        Total time: 1131.04s
                               ETA: 9107.2s

################################################################################
                     [1m Learning iteration 442/4000 [0m

                       Computation: 3012 steps/s (collection: 0.568s, learning 2.151s)
               Value function loss: 606.3031
                    Surrogate loss: 0.0145
             Mean action noise std: 0.98
                       Mean reward: 491.51
               Mean episode length: 271.07
                 Mean success rate: 1.00
                  Mean reward/step: 1.93
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3629056
                    Iteration time: 2.72s
                        Total time: 1133.76s
                               ETA: 9105.9s

################################################################################
                     [1m Learning iteration 443/4000 [0m

                       Computation: 3158 steps/s (collection: 0.525s, learning 2.068s)
               Value function loss: 612.8548
                    Surrogate loss: 0.0151
             Mean action noise std: 0.98
                       Mean reward: 493.18
               Mean episode length: 273.29
                 Mean success rate: 1.00
                  Mean reward/step: 1.90
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 2.59s
                        Total time: 1136.36s
                               ETA: 9103.6s

################################################################################
                     [1m Learning iteration 444/4000 [0m

                       Computation: 3229 steps/s (collection: 0.486s, learning 2.051s)
               Value function loss: 793.5438
                    Surrogate loss: 0.0122
             Mean action noise std: 0.98
                       Mean reward: 483.63
               Mean episode length: 267.10
                 Mean success rate: 1.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3645440
                    Iteration time: 2.54s
                        Total time: 1138.89s
                               ETA: 9100.9s

################################################################################
                     [1m Learning iteration 445/4000 [0m

                       Computation: 3105 steps/s (collection: 0.516s, learning 2.122s)
               Value function loss: 721.9893
                    Surrogate loss: 0.0142
             Mean action noise std: 0.98
                       Mean reward: 460.02
               Mean episode length: 259.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.88
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 2.64s
                        Total time: 1141.53s
                               ETA: 9099.0s

################################################################################
                     [1m Learning iteration 446/4000 [0m

                       Computation: 3077 steps/s (collection: 0.561s, learning 2.101s)
               Value function loss: 816.7965
                    Surrogate loss: 0.0144
             Mean action noise std: 0.98
                       Mean reward: 451.65
               Mean episode length: 247.25
                 Mean success rate: 0.50
                  Mean reward/step: 1.89
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3661824
                    Iteration time: 2.66s
                        Total time: 1144.19s
                               ETA: 9097.2s

################################################################################
                     [1m Learning iteration 447/4000 [0m

                       Computation: 3239 steps/s (collection: 0.483s, learning 2.046s)
               Value function loss: 695.7619
                    Surrogate loss: 0.0130
             Mean action noise std: 0.98
                       Mean reward: 468.69
               Mean episode length: 251.38
                 Mean success rate: 1.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 2.53s
                        Total time: 1146.72s
                               ETA: 9094.4s

################################################################################
                     [1m Learning iteration 448/4000 [0m

                       Computation: 3281 steps/s (collection: 0.453s, learning 2.043s)
               Value function loss: 911.4085
                    Surrogate loss: 0.0133
             Mean action noise std: 0.98
                       Mean reward: 496.88
               Mean episode length: 262.83
                 Mean success rate: 1.50
                  Mean reward/step: 2.02
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3678208
                    Iteration time: 2.50s
                        Total time: 1149.22s
                               ETA: 9091.3s

################################################################################
                     [1m Learning iteration 449/4000 [0m

                       Computation: 3250 steps/s (collection: 0.482s, learning 2.038s)
               Value function loss: 914.3427
                    Surrogate loss: 0.0129
             Mean action noise std: 0.98
                       Mean reward: 501.87
               Mean episode length: 258.46
                 Mean success rate: 2.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 2.52s
                        Total time: 1151.74s
                               ETA: 9088.5s

################################################################################
                     [1m Learning iteration 450/4000 [0m

                       Computation: 3167 steps/s (collection: 0.499s, learning 2.088s)
               Value function loss: 1105.3139
                    Surrogate loss: 0.0122
             Mean action noise std: 0.98
                       Mean reward: 501.12
               Mean episode length: 254.44
                 Mean success rate: 2.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3694592
                    Iteration time: 2.59s
                        Total time: 1154.32s
                               ETA: 9086.1s

################################################################################
                     [1m Learning iteration 451/4000 [0m

                       Computation: 3260 steps/s (collection: 0.497s, learning 2.015s)
               Value function loss: 834.5321
                    Surrogate loss: 0.0164
             Mean action noise std: 0.98
                       Mean reward: 500.67
               Mean episode length: 257.67
                 Mean success rate: 2.00
                  Mean reward/step: 1.92
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 2.51s
                        Total time: 1156.84s
                               ETA: 9083.2s

################################################################################
                     [1m Learning iteration 452/4000 [0m

                       Computation: 3270 steps/s (collection: 0.448s, learning 2.057s)
               Value function loss: 763.3934
                    Surrogate loss: 0.0139
             Mean action noise std: 0.98
                       Mean reward: 489.44
               Mean episode length: 257.12
                 Mean success rate: 2.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3710976
                    Iteration time: 2.51s
                        Total time: 1159.34s
                               ETA: 9080.2s

################################################################################
                     [1m Learning iteration 453/4000 [0m

                       Computation: 3237 steps/s (collection: 0.444s, learning 2.087s)
               Value function loss: 830.3075
                    Surrogate loss: 0.0140
             Mean action noise std: 0.98
                       Mean reward: 519.31
               Mean episode length: 264.67
                 Mean success rate: 3.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 2.53s
                        Total time: 1161.87s
                               ETA: 9077.4s

################################################################################
                     [1m Learning iteration 454/4000 [0m

                       Computation: 3224 steps/s (collection: 0.496s, learning 2.044s)
               Value function loss: 1380.3262
                    Surrogate loss: 0.0127
             Mean action noise std: 0.98
                       Mean reward: 513.57
               Mean episode length: 260.69
                 Mean success rate: 3.50
                  Mean reward/step: 2.16
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3727360
                    Iteration time: 2.54s
                        Total time: 1164.41s
                               ETA: 9074.7s

################################################################################
                     [1m Learning iteration 455/4000 [0m

                       Computation: 3191 steps/s (collection: 0.522s, learning 2.045s)
               Value function loss: 1338.3079
                    Surrogate loss: 0.0133
             Mean action noise std: 0.98
                       Mean reward: 511.71
               Mean episode length: 257.45
                 Mean success rate: 4.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 2.57s
                        Total time: 1166.98s
                               ETA: 9072.2s

################################################################################
                     [1m Learning iteration 456/4000 [0m

                       Computation: 3253 steps/s (collection: 0.471s, learning 2.047s)
               Value function loss: 768.2258
                    Surrogate loss: 0.0193
             Mean action noise std: 0.98
                       Mean reward: 499.06
               Mean episode length: 256.96
                 Mean success rate: 4.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3743744
                    Iteration time: 2.52s
                        Total time: 1169.50s
                               ETA: 9069.4s

################################################################################
                     [1m Learning iteration 457/4000 [0m

                       Computation: 3264 steps/s (collection: 0.456s, learning 2.054s)
               Value function loss: 768.2652
                    Surrogate loss: 0.0172
             Mean action noise std: 0.98
                       Mean reward: 493.02
               Mean episode length: 251.81
                 Mean success rate: 4.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 2.51s
                        Total time: 1172.01s
                               ETA: 9066.4s

################################################################################
                     [1m Learning iteration 458/4000 [0m

                       Computation: 3196 steps/s (collection: 0.483s, learning 2.080s)
               Value function loss: 1330.8837
                    Surrogate loss: 0.0189
             Mean action noise std: 0.98
                       Mean reward: 534.55
               Mean episode length: 260.32
                 Mean success rate: 4.50
                  Mean reward/step: 2.21
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3760128
                    Iteration time: 2.56s
                        Total time: 1174.57s
                               ETA: 9063.9s

################################################################################
                     [1m Learning iteration 459/4000 [0m

                       Computation: 3213 steps/s (collection: 0.502s, learning 2.047s)
               Value function loss: 1228.0522
                    Surrogate loss: 0.0119
             Mean action noise std: 0.98
                       Mean reward: 532.64
               Mean episode length: 256.32
                 Mean success rate: 3.50
                  Mean reward/step: 2.23
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 2.55s
                        Total time: 1177.12s
                               ETA: 9061.2s

################################################################################
                     [1m Learning iteration 460/4000 [0m

                       Computation: 3196 steps/s (collection: 0.472s, learning 2.090s)
               Value function loss: 1018.2573
                    Surrogate loss: 0.0115
             Mean action noise std: 0.98
                       Mean reward: 502.84
               Mean episode length: 250.72
                 Mean success rate: 2.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3776512
                    Iteration time: 2.56s
                        Total time: 1179.68s
                               ETA: 9058.7s

################################################################################
                     [1m Learning iteration 461/4000 [0m

                       Computation: 3115 steps/s (collection: 0.490s, learning 2.139s)
               Value function loss: 1162.2853
                    Surrogate loss: 0.0126
             Mean action noise std: 0.98
                       Mean reward: 479.15
               Mean episode length: 239.81
                 Mean success rate: 1.50
                  Mean reward/step: 2.00
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 2.63s
                        Total time: 1182.31s
                               ETA: 9056.7s

################################################################################
                     [1m Learning iteration 462/4000 [0m

                       Computation: 3138 steps/s (collection: 0.543s, learning 2.067s)
               Value function loss: 1765.1436
                    Surrogate loss: 0.0123
             Mean action noise std: 0.98
                       Mean reward: 524.97
               Mean episode length: 248.74
                 Mean success rate: 2.50
                  Mean reward/step: 2.22
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 3792896
                    Iteration time: 2.61s
                        Total time: 1184.92s
                               ETA: 9054.5s

################################################################################
                     [1m Learning iteration 463/4000 [0m

                       Computation: 3235 steps/s (collection: 0.492s, learning 2.041s)
               Value function loss: 1499.9252
                    Surrogate loss: 0.0124
             Mean action noise std: 0.98
                       Mean reward: 492.91
               Mean episode length: 237.04
                 Mean success rate: 2.50
                  Mean reward/step: 2.19
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 2.53s
                        Total time: 1187.45s
                               ETA: 9051.8s

################################################################################
                     [1m Learning iteration 464/4000 [0m

                       Computation: 3177 steps/s (collection: 0.503s, learning 2.075s)
               Value function loss: 1455.6713
                    Surrogate loss: 0.0154
             Mean action noise std: 0.98
                       Mean reward: 508.72
               Mean episode length: 240.57
                 Mean success rate: 2.50
                  Mean reward/step: 2.11
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3809280
                    Iteration time: 2.58s
                        Total time: 1190.03s
                               ETA: 9049.3s

################################################################################
                     [1m Learning iteration 465/4000 [0m

                       Computation: 3191 steps/s (collection: 0.461s, learning 2.105s)
               Value function loss: 1708.3757
                    Surrogate loss: 0.0115
             Mean action noise std: 0.98
                       Mean reward: 541.88
               Mean episode length: 256.23
                 Mean success rate: 2.50
                  Mean reward/step: 2.13
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 2.57s
                        Total time: 1192.60s
                               ETA: 9046.8s

################################################################################
                     [1m Learning iteration 466/4000 [0m

                       Computation: 3007 steps/s (collection: 0.560s, learning 2.164s)
               Value function loss: 1573.3552
                    Surrogate loss: 0.0122
             Mean action noise std: 0.98
                       Mean reward: 523.69
               Mean episode length: 238.79
                 Mean success rate: 3.50
                  Mean reward/step: 2.12
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3825664
                    Iteration time: 2.72s
                        Total time: 1195.32s
                               ETA: 9045.5s

################################################################################
                     [1m Learning iteration 467/4000 [0m

                       Computation: 3142 steps/s (collection: 0.502s, learning 2.105s)
               Value function loss: 1586.1725
                    Surrogate loss: 0.0122
             Mean action noise std: 0.98
                       Mean reward: 521.54
               Mean episode length: 237.59
                 Mean success rate: 3.50
                  Mean reward/step: 2.32
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 2.61s
                        Total time: 1197.93s
                               ETA: 9043.3s

################################################################################
                     [1m Learning iteration 468/4000 [0m

                       Computation: 3154 steps/s (collection: 0.473s, learning 2.124s)
               Value function loss: 2019.9691
                    Surrogate loss: 0.0130
             Mean action noise std: 0.98
                       Mean reward: 528.77
               Mean episode length: 233.96
                 Mean success rate: 4.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3842048
                    Iteration time: 2.60s
                        Total time: 1200.52s
                               ETA: 9041.1s

################################################################################
                     [1m Learning iteration 469/4000 [0m

                       Computation: 3087 steps/s (collection: 0.498s, learning 2.155s)
               Value function loss: 1900.6333
                    Surrogate loss: 0.0162
             Mean action noise std: 0.98
                       Mean reward: 506.85
               Mean episode length: 232.38
                 Mean success rate: 3.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 2.65s
                        Total time: 1203.18s
                               ETA: 9039.2s

################################################################################
                     [1m Learning iteration 470/4000 [0m

                       Computation: 3235 steps/s (collection: 0.502s, learning 2.030s)
               Value function loss: 2115.5154
                    Surrogate loss: 0.0134
             Mean action noise std: 0.98
                       Mean reward: 515.00
               Mean episode length: 235.50
                 Mean success rate: 3.50
                  Mean reward/step: 2.39
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3858432
                    Iteration time: 2.53s
                        Total time: 1205.71s
                               ETA: 9036.4s

################################################################################
                     [1m Learning iteration 471/4000 [0m

                       Computation: 3168 steps/s (collection: 0.535s, learning 2.051s)
               Value function loss: 2401.9520
                    Surrogate loss: 0.0110
             Mean action noise std: 0.98
                       Mean reward: 468.09
               Mean episode length: 220.12
                 Mean success rate: 3.50
                  Mean reward/step: 2.25
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 2.59s
                        Total time: 1208.30s
                               ETA: 9034.1s

################################################################################
                     [1m Learning iteration 472/4000 [0m

                       Computation: 3237 steps/s (collection: 0.489s, learning 2.041s)
               Value function loss: 2250.5551
                    Surrogate loss: 0.0139
             Mean action noise std: 0.98
                       Mean reward: 455.90
               Mean episode length: 219.88
                 Mean success rate: 3.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3874816
                    Iteration time: 2.53s
                        Total time: 1210.83s
                               ETA: 9031.3s

################################################################################
                     [1m Learning iteration 473/4000 [0m

                       Computation: 3149 steps/s (collection: 0.521s, learning 2.080s)
               Value function loss: 3448.2638
                    Surrogate loss: 0.0128
             Mean action noise std: 0.98
                       Mean reward: 494.55
               Mean episode length: 227.62
                 Mean success rate: 4.00
                  Mean reward/step: 2.49
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 2.60s
                        Total time: 1213.43s
                               ETA: 9029.0s

################################################################################
                     [1m Learning iteration 474/4000 [0m

                       Computation: 3173 steps/s (collection: 0.518s, learning 2.063s)
               Value function loss: 2183.2282
                    Surrogate loss: 0.0153
             Mean action noise std: 0.98
                       Mean reward: 482.84
               Mean episode length: 219.28
                 Mean success rate: 5.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3891200
                    Iteration time: 2.58s
                        Total time: 1216.01s
                               ETA: 9026.6s

################################################################################
                     [1m Learning iteration 475/4000 [0m

                       Computation: 3172 steps/s (collection: 0.507s, learning 2.075s)
               Value function loss: 3159.4451
                    Surrogate loss: 0.0128
             Mean action noise std: 0.98
                       Mean reward: 514.27
               Mean episode length: 226.01
                 Mean success rate: 6.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 2.58s
                        Total time: 1218.59s
                               ETA: 9024.2s

################################################################################
                     [1m Learning iteration 476/4000 [0m

                       Computation: 3324 steps/s (collection: 0.434s, learning 2.030s)
               Value function loss: 2161.6818
                    Surrogate loss: 0.0133
             Mean action noise std: 0.98
                       Mean reward: 577.05
               Mean episode length: 244.89
                 Mean success rate: 7.50
                  Mean reward/step: 2.22
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3907584
                    Iteration time: 2.46s
                        Total time: 1221.05s
                               ETA: 9021.0s

################################################################################
                     [1m Learning iteration 477/4000 [0m

                       Computation: 3211 steps/s (collection: 0.480s, learning 2.070s)
               Value function loss: 2220.0858
                    Surrogate loss: 0.0144
             Mean action noise std: 0.98
                       Mean reward: 603.48
               Mean episode length: 250.58
                 Mean success rate: 8.50
                  Mean reward/step: 2.28
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 2.55s
                        Total time: 1223.61s
                               ETA: 9018.3s

################################################################################
                     [1m Learning iteration 478/4000 [0m

                       Computation: 3231 steps/s (collection: 0.468s, learning 2.067s)
               Value function loss: 4295.4701
                    Surrogate loss: 0.0106
             Mean action noise std: 0.98
                       Mean reward: 605.00
               Mean episode length: 253.63
                 Mean success rate: 8.50
                  Mean reward/step: 2.47
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3923968
                    Iteration time: 2.53s
                        Total time: 1226.14s
                               ETA: 9015.6s

################################################################################
                     [1m Learning iteration 479/4000 [0m

                       Computation: 3208 steps/s (collection: 0.513s, learning 2.041s)
               Value function loss: 3482.8981
                    Surrogate loss: 0.0110
             Mean action noise std: 0.98
                       Mean reward: 613.83
               Mean episode length: 259.07
                 Mean success rate: 8.00
                  Mean reward/step: 2.58
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 2.55s
                        Total time: 1228.69s
                               ETA: 9013.0s

################################################################################
                     [1m Learning iteration 480/4000 [0m

                       Computation: 3205 steps/s (collection: 0.486s, learning 2.070s)
               Value function loss: 2717.8233
                    Surrogate loss: 0.0155
             Mean action noise std: 0.98
                       Mean reward: 635.32
               Mean episode length: 270.38
                 Mean success rate: 7.50
                  Mean reward/step: 2.37
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3940352
                    Iteration time: 2.56s
                        Total time: 1231.25s
                               ETA: 9010.4s

################################################################################
                     [1m Learning iteration 481/4000 [0m

                       Computation: 3230 steps/s (collection: 0.482s, learning 2.054s)
               Value function loss: 3251.4017
                    Surrogate loss: 0.0135
             Mean action noise std: 0.98
                       Mean reward: 639.34
               Mean episode length: 268.38
                 Mean success rate: 8.00
                  Mean reward/step: 2.58
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 2.54s
                        Total time: 1233.79s
                               ETA: 9007.7s

################################################################################
                     [1m Learning iteration 482/4000 [0m

                       Computation: 3196 steps/s (collection: 0.504s, learning 2.059s)
               Value function loss: 3603.7032
                    Surrogate loss: 0.0147
             Mean action noise std: 0.98
                       Mean reward: 604.00
               Mean episode length: 265.29
                 Mean success rate: 7.50
                  Mean reward/step: 2.73
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3956736
                    Iteration time: 2.56s
                        Total time: 1236.35s
                               ETA: 9005.1s

################################################################################
                     [1m Learning iteration 483/4000 [0m

                       Computation: 3199 steps/s (collection: 0.495s, learning 2.065s)
               Value function loss: 4592.5212
                    Surrogate loss: 0.0108
             Mean action noise std: 0.98
                       Mean reward: 585.06
               Mean episode length: 256.42
                 Mean success rate: 7.50
                  Mean reward/step: 2.56
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 2.56s
                        Total time: 1238.91s
                               ETA: 9002.6s

################################################################################
                     [1m Learning iteration 484/4000 [0m

                       Computation: 3160 steps/s (collection: 0.490s, learning 2.103s)
               Value function loss: 2597.1764
                    Surrogate loss: 0.0119
             Mean action noise std: 0.98
                       Mean reward: 601.70
               Mean episode length: 261.05
                 Mean success rate: 8.00
                  Mean reward/step: 2.61
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3973120
                    Iteration time: 2.59s
                        Total time: 1241.50s
                               ETA: 9000.2s

################################################################################
                     [1m Learning iteration 485/4000 [0m

                       Computation: 3160 steps/s (collection: 0.500s, learning 2.093s)
               Value function loss: 4588.4352
                    Surrogate loss: 0.0104
             Mean action noise std: 0.98
                       Mean reward: 601.34
               Mean episode length: 243.34
                 Mean success rate: 10.50
                  Mean reward/step: 2.60
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 2.59s
                        Total time: 1244.09s
                               ETA: 8997.9s

################################################################################
                     [1m Learning iteration 486/4000 [0m

                       Computation: 3146 steps/s (collection: 0.520s, learning 2.084s)
               Value function loss: 3421.7617
                    Surrogate loss: 0.0117
             Mean action noise std: 0.98
                       Mean reward: 587.42
               Mean episode length: 236.75
                 Mean success rate: 10.50
                  Mean reward/step: 2.31
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3989504
                    Iteration time: 2.60s
                        Total time: 1246.70s
                               ETA: 8995.7s

################################################################################
                     [1m Learning iteration 487/4000 [0m

                       Computation: 3204 steps/s (collection: 0.499s, learning 2.058s)
               Value function loss: 3147.5012
                    Surrogate loss: 0.0119
             Mean action noise std: 0.98
                       Mean reward: 575.07
               Mean episode length: 229.34
                 Mean success rate: 10.50
                  Mean reward/step: 2.13
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 2.56s
                        Total time: 1249.25s
                               ETA: 8993.1s

################################################################################
                     [1m Learning iteration 488/4000 [0m

                       Computation: 3222 steps/s (collection: 0.478s, learning 2.064s)
               Value function loss: 3447.0425
                    Surrogate loss: 0.0126
             Mean action noise std: 0.98
                       Mean reward: 546.09
               Mean episode length: 229.72
                 Mean success rate: 8.00
                  Mean reward/step: 2.33
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4005888
                    Iteration time: 2.54s
                        Total time: 1251.80s
                               ETA: 8990.4s

################################################################################
                     [1m Learning iteration 489/4000 [0m

                       Computation: 3162 steps/s (collection: 0.472s, learning 2.118s)
               Value function loss: 5026.6486
                    Surrogate loss: 0.0109
             Mean action noise std: 0.98
                       Mean reward: 607.80
               Mean episode length: 230.28
                 Mean success rate: 11.00
                  Mean reward/step: 2.32
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 2.59s
                        Total time: 1254.39s
                               ETA: 8988.1s

################################################################################
                     [1m Learning iteration 490/4000 [0m

                       Computation: 3146 steps/s (collection: 0.504s, learning 2.100s)
               Value function loss: 2734.1691
                    Surrogate loss: 0.0144
             Mean action noise std: 0.98
                       Mean reward: 586.43
               Mean episode length: 231.26
                 Mean success rate: 10.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4022272
                    Iteration time: 2.60s
                        Total time: 1256.99s
                               ETA: 8985.8s

################################################################################
                     [1m Learning iteration 491/4000 [0m

                       Computation: 3218 steps/s (collection: 0.478s, learning 2.067s)
               Value function loss: 3804.6422
                    Surrogate loss: 0.0138
             Mean action noise std: 0.98
                       Mean reward: 571.91
               Mean episode length: 235.21
                 Mean success rate: 9.00
                  Mean reward/step: 2.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 2.55s
                        Total time: 1259.54s
                               ETA: 8983.2s

################################################################################
                     [1m Learning iteration 492/4000 [0m

                       Computation: 3165 steps/s (collection: 0.447s, learning 2.141s)
               Value function loss: 4100.3712
                    Surrogate loss: 0.0134
             Mean action noise std: 0.98
                       Mean reward: 586.13
               Mean episode length: 240.76
                 Mean success rate: 9.00
                  Mean reward/step: 2.51
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4038656
                    Iteration time: 2.59s
                        Total time: 1262.12s
                               ETA: 8980.8s

################################################################################
                     [1m Learning iteration 493/4000 [0m

                       Computation: 3185 steps/s (collection: 0.473s, learning 2.098s)
               Value function loss: 6508.5425
                    Surrogate loss: 0.0100
             Mean action noise std: 0.98
                       Mean reward: 590.56
               Mean episode length: 238.16
                 Mean success rate: 11.00
                  Mean reward/step: 2.86
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 2.57s
                        Total time: 1264.70s
                               ETA: 8978.3s

################################################################################
                     [1m Learning iteration 494/4000 [0m

                       Computation: 3199 steps/s (collection: 0.533s, learning 2.027s)
               Value function loss: 4363.0566
                    Surrogate loss: 0.0130
             Mean action noise std: 0.98
                       Mean reward: 533.05
               Mean episode length: 230.24
                 Mean success rate: 10.00
                  Mean reward/step: 2.64
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 4055040
                    Iteration time: 2.56s
                        Total time: 1267.26s
                               ETA: 8975.8s

################################################################################
                     [1m Learning iteration 495/4000 [0m

                       Computation: 3213 steps/s (collection: 0.474s, learning 2.076s)
               Value function loss: 3440.6999
                    Surrogate loss: 0.0122
             Mean action noise std: 0.98
                       Mean reward: 531.23
               Mean episode length: 229.76
                 Mean success rate: 9.50
                  Mean reward/step: 2.56
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 2.55s
                        Total time: 1269.80s
                               ETA: 8973.1s

################################################################################
                     [1m Learning iteration 496/4000 [0m

                       Computation: 3214 steps/s (collection: 0.470s, learning 2.078s)
               Value function loss: 3790.8223
                    Surrogate loss: 0.0142
             Mean action noise std: 0.98
                       Mean reward: 538.64
               Mean episode length: 218.71
                 Mean success rate: 12.00
                  Mean reward/step: 2.54
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4071424
                    Iteration time: 2.55s
                        Total time: 1272.35s
                               ETA: 8970.5s

################################################################################
                     [1m Learning iteration 497/4000 [0m

                       Computation: 3127 steps/s (collection: 0.486s, learning 2.134s)
               Value function loss: 3892.6201
                    Surrogate loss: 0.0120
             Mean action noise std: 0.98
                       Mean reward: 532.79
               Mean episode length: 217.45
                 Mean success rate: 12.50
                  Mean reward/step: 2.57
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 2.62s
                        Total time: 1274.97s
                               ETA: 8968.3s

################################################################################
                     [1m Learning iteration 498/4000 [0m

                       Computation: 3278 steps/s (collection: 0.470s, learning 2.029s)
               Value function loss: 3665.0132
                    Surrogate loss: 0.0102
             Mean action noise std: 0.98
                       Mean reward: 585.44
               Mean episode length: 225.22
                 Mean success rate: 13.50
                  Mean reward/step: 2.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4087808
                    Iteration time: 2.50s
                        Total time: 1277.47s
                               ETA: 8965.3s

################################################################################
                     [1m Learning iteration 499/4000 [0m

                       Computation: 3178 steps/s (collection: 0.512s, learning 2.065s)
               Value function loss: 4665.8219
                    Surrogate loss: 0.0111
             Mean action noise std: 0.98
                       Mean reward: 626.75
               Mean episode length: 220.07
                 Mean success rate: 15.50
                  Mean reward/step: 2.48
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 2.58s
                        Total time: 1280.05s
                               ETA: 8962.9s

################################################################################
                     [1m Learning iteration 500/4000 [0m

                       Computation: 3213 steps/s (collection: 0.452s, learning 2.097s)
               Value function loss: 3577.3133
                    Surrogate loss: 0.0142
             Mean action noise std: 0.98
                       Mean reward: 611.36
               Mean episode length: 214.22
                 Mean success rate: 14.50
                  Mean reward/step: 2.54
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4104192
                    Iteration time: 2.55s
                        Total time: 1282.60s
                               ETA: 8960.3s

################################################################################
                     [1m Learning iteration 501/4000 [0m

                       Computation: 3206 steps/s (collection: 0.472s, learning 2.083s)
               Value function loss: 3716.1000
                    Surrogate loss: 0.0156
             Mean action noise std: 0.98
                       Mean reward: 626.31
               Mean episode length: 228.10
                 Mean success rate: 12.50
                  Mean reward/step: 2.65
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 2.55s
                        Total time: 1285.15s
                               ETA: 8957.7s

################################################################################
                     [1m Learning iteration 502/4000 [0m

                       Computation: 3191 steps/s (collection: 0.494s, learning 2.073s)
               Value function loss: 4129.1487
                    Surrogate loss: 0.0150
             Mean action noise std: 0.98
                       Mean reward: 597.44
               Mean episode length: 214.90
                 Mean success rate: 11.50
                  Mean reward/step: 2.74
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 4120576
                    Iteration time: 2.57s
                        Total time: 1287.72s
                               ETA: 8955.2s

################################################################################
                     [1m Learning iteration 503/4000 [0m

                       Computation: 3169 steps/s (collection: 0.515s, learning 2.070s)
               Value function loss: 5185.5965
                    Surrogate loss: 0.0138
             Mean action noise std: 0.98
                       Mean reward: 533.59
               Mean episode length: 207.69
                 Mean success rate: 9.50
                  Mean reward/step: 2.81
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 2.59s
                        Total time: 1290.30s
                               ETA: 8952.8s

################################################################################
                     [1m Learning iteration 504/4000 [0m

                       Computation: 3206 steps/s (collection: 0.494s, learning 2.060s)
               Value function loss: 3905.5862
                    Surrogate loss: 0.0129
             Mean action noise std: 0.98
                       Mean reward: 488.42
               Mean episode length: 204.01
                 Mean success rate: 7.50
                  Mean reward/step: 2.93
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4136960
                    Iteration time: 2.55s
                        Total time: 1292.86s
                               ETA: 8950.2s

################################################################################
                     [1m Learning iteration 505/4000 [0m

                       Computation: 3236 steps/s (collection: 0.464s, learning 2.067s)
               Value function loss: 7080.6936
                    Surrogate loss: 0.0130
             Mean action noise std: 0.98
                       Mean reward: 540.79
               Mean episode length: 211.88
                 Mean success rate: 9.50
                  Mean reward/step: 3.02
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 2.53s
                        Total time: 1295.39s
                               ETA: 8947.4s

################################################################################
                     [1m Learning iteration 506/4000 [0m

                       Computation: 3240 steps/s (collection: 0.502s, learning 2.027s)
               Value function loss: 5980.9239
                    Surrogate loss: 0.0147
             Mean action noise std: 0.98
                       Mean reward: 555.99
               Mean episode length: 212.07
                 Mean success rate: 10.50
                  Mean reward/step: 2.94
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4153344
                    Iteration time: 2.53s
                        Total time: 1297.92s
                               ETA: 8944.6s

################################################################################
                     [1m Learning iteration 507/4000 [0m

                       Computation: 3262 steps/s (collection: 0.452s, learning 2.059s)
               Value function loss: 3852.2581
                    Surrogate loss: 0.0136
             Mean action noise std: 0.98
                       Mean reward: 550.12
               Mean episode length: 213.46
                 Mean success rate: 10.50
                  Mean reward/step: 2.61
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 2.51s
                        Total time: 1300.43s
                               ETA: 8941.7s

################################################################################
                     [1m Learning iteration 508/4000 [0m

                       Computation: 3263 steps/s (collection: 0.458s, learning 2.052s)
               Value function loss: 3291.3321
                    Surrogate loss: 0.0123
             Mean action noise std: 0.98
                       Mean reward: 563.32
               Mean episode length: 221.28
                 Mean success rate: 11.50
                  Mean reward/step: 2.32
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4169728
                    Iteration time: 2.51s
                        Total time: 1302.94s
                               ETA: 8938.8s

################################################################################
                     [1m Learning iteration 509/4000 [0m

                       Computation: 3251 steps/s (collection: 0.449s, learning 2.071s)
               Value function loss: 3069.5443
                    Surrogate loss: 0.0135
             Mean action noise std: 0.98
                       Mean reward: 606.05
               Mean episode length: 240.72
                 Mean success rate: 11.50
                  Mean reward/step: 2.34
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 2.52s
                        Total time: 1305.46s
                               ETA: 8936.0s

################################################################################
                     [1m Learning iteration 510/4000 [0m

                       Computation: 3231 steps/s (collection: 0.499s, learning 2.036s)
               Value function loss: 4757.8888
                    Surrogate loss: 0.0117
             Mean action noise std: 0.98
                       Mean reward: 639.70
               Mean episode length: 239.60
                 Mean success rate: 13.00
                  Mean reward/step: 2.44
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4186112
                    Iteration time: 2.54s
                        Total time: 1307.99s
                               ETA: 8933.3s

################################################################################
                     [1m Learning iteration 511/4000 [0m

                       Computation: 3225 steps/s (collection: 0.468s, learning 2.072s)
               Value function loss: 4416.6934
                    Surrogate loss: 0.0148
             Mean action noise std: 0.98
                       Mean reward: 582.44
               Mean episode length: 223.38
                 Mean success rate: 12.00
                  Mean reward/step: 2.49
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 2.54s
                        Total time: 1310.53s
                               ETA: 8930.6s

################################################################################
                     [1m Learning iteration 512/4000 [0m

                       Computation: 3196 steps/s (collection: 0.495s, learning 2.068s)
               Value function loss: 6913.0738
                    Surrogate loss: 0.0135
             Mean action noise std: 0.98
                       Mean reward: 535.29
               Mean episode length: 205.09
                 Mean success rate: 12.50
                  Mean reward/step: 2.68
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 4202496
                    Iteration time: 2.56s
                        Total time: 1313.10s
                               ETA: 8928.0s

################################################################################
                     [1m Learning iteration 513/4000 [0m

                       Computation: 3174 steps/s (collection: 0.478s, learning 2.102s)
               Value function loss: 4394.7383
                    Surrogate loss: 0.0137
             Mean action noise std: 0.98
                       Mean reward: 512.73
               Mean episode length: 180.89
                 Mean success rate: 12.00
                  Mean reward/step: 2.49
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 2.58s
                        Total time: 1315.68s
                               ETA: 8925.6s

################################################################################
                     [1m Learning iteration 514/4000 [0m

                       Computation: 3253 steps/s (collection: 0.473s, learning 2.045s)
               Value function loss: 2413.3977
                    Surrogate loss: 0.0181
             Mean action noise std: 0.98
                       Mean reward: 485.30
               Mean episode length: 174.00
                 Mean success rate: 12.50
                  Mean reward/step: 2.34
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4218880
                    Iteration time: 2.52s
                        Total time: 1318.19s
                               ETA: 8922.8s

################################################################################
                     [1m Learning iteration 515/4000 [0m

                       Computation: 3241 steps/s (collection: 0.475s, learning 2.052s)
               Value function loss: 2453.8242
                    Surrogate loss: 0.0182
             Mean action noise std: 0.98
                       Mean reward: 475.08
               Mean episode length: 181.56
                 Mean success rate: 11.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 2.53s
                        Total time: 1320.72s
                               ETA: 8920.0s

################################################################################
                     [1m Learning iteration 516/4000 [0m

                       Computation: 3206 steps/s (collection: 0.482s, learning 2.072s)
               Value function loss: 2670.8877
                    Surrogate loss: 0.0146
             Mean action noise std: 0.98
                       Mean reward: 495.29
               Mean episode length: 188.10
                 Mean success rate: 10.50
                  Mean reward/step: 2.26
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 4235264
                    Iteration time: 2.55s
                        Total time: 1323.28s
                               ETA: 8917.4s

################################################################################
                     [1m Learning iteration 517/4000 [0m

                       Computation: 3191 steps/s (collection: 0.459s, learning 2.108s)
               Value function loss: 3849.3674
                    Surrogate loss: 0.0133
             Mean action noise std: 0.98
                       Mean reward: 473.35
               Mean episode length: 207.02
                 Mean success rate: 8.50
                  Mean reward/step: 2.46
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 2.57s
                        Total time: 1325.84s
                               ETA: 8914.9s

################################################################################
                     [1m Learning iteration 518/4000 [0m

                       Computation: 3175 steps/s (collection: 0.520s, learning 2.060s)
               Value function loss: 4199.6741
                    Surrogate loss: 0.0119
             Mean action noise std: 0.98
                       Mean reward: 542.00
               Mean episode length: 225.38
                 Mean success rate: 8.50
                  Mean reward/step: 2.49
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 4251648
                    Iteration time: 2.58s
                        Total time: 1328.42s
                               ETA: 8912.5s

################################################################################
                     [1m Learning iteration 519/4000 [0m

                       Computation: 3188 steps/s (collection: 0.484s, learning 2.086s)
               Value function loss: 2911.2398
                    Surrogate loss: 0.0122
             Mean action noise std: 0.98
                       Mean reward: 505.21
               Mean episode length: 211.12
                 Mean success rate: 8.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 2.57s
                        Total time: 1330.99s
                               ETA: 8910.0s

################################################################################
                     [1m Learning iteration 520/4000 [0m

                       Computation: 3086 steps/s (collection: 0.500s, learning 2.154s)
               Value function loss: 2947.2426
                    Surrogate loss: 0.0134
             Mean action noise std: 0.98
                       Mean reward: 455.13
               Mean episode length: 192.19
                 Mean success rate: 7.00
                  Mean reward/step: 2.48
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 4268032
                    Iteration time: 2.65s
                        Total time: 1333.65s
                               ETA: 8908.0s

################################################################################
                     [1m Learning iteration 521/4000 [0m

                       Computation: 3269 steps/s (collection: 0.447s, learning 2.058s)
               Value function loss: 3919.9674
                    Surrogate loss: 0.0158
             Mean action noise std: 0.98
                       Mean reward: 464.64
               Mean episode length: 192.33
                 Mean success rate: 7.00
                  Mean reward/step: 2.89
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 2.51s
                        Total time: 1336.15s
                               ETA: 8905.1s

################################################################################
                     [1m Learning iteration 522/4000 [0m

                       Computation: 3183 steps/s (collection: 0.490s, learning 2.083s)
               Value function loss: 4167.8708
                    Surrogate loss: 0.0126
             Mean action noise std: 0.98
                       Mean reward: 421.51
               Mean episode length: 168.09
                 Mean success rate: 7.00
                  Mean reward/step: 2.97
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4284416
                    Iteration time: 2.57s
                        Total time: 1338.73s
                               ETA: 8902.7s

################################################################################
                     [1m Learning iteration 523/4000 [0m

                       Computation: 3231 steps/s (collection: 0.464s, learning 2.071s)
               Value function loss: 4372.1625
                    Surrogate loss: 0.0137
             Mean action noise std: 0.98
                       Mean reward: 365.78
               Mean episode length: 161.60
                 Mean success rate: 6.00
                  Mean reward/step: 2.95
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 2.53s
                        Total time: 1341.26s
                               ETA: 8899.9s

################################################################################
                     [1m Learning iteration 524/4000 [0m

                       Computation: 3227 steps/s (collection: 0.450s, learning 2.088s)
               Value function loss: 5894.9706
                    Surrogate loss: 0.0120
             Mean action noise std: 0.98
                       Mean reward: 428.93
               Mean episode length: 167.24
                 Mean success rate: 8.50
                  Mean reward/step: 2.98
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4300800
                    Iteration time: 2.54s
                        Total time: 1343.80s
                               ETA: 8897.2s

################################################################################
                     [1m Learning iteration 525/4000 [0m

                       Computation: 3154 steps/s (collection: 0.480s, learning 2.117s)
               Value function loss: 6483.3857
                    Surrogate loss: 0.0136
             Mean action noise std: 0.98
                       Mean reward: 467.32
               Mean episode length: 170.50
                 Mean success rate: 11.00
                  Mean reward/step: 2.94
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 2.60s
                        Total time: 1346.40s
                               ETA: 8894.9s

################################################################################
                     [1m Learning iteration 526/4000 [0m

                       Computation: 3266 steps/s (collection: 0.437s, learning 2.072s)
               Value function loss: 4753.3889
                    Surrogate loss: 0.0144
             Mean action noise std: 0.97
                       Mean reward: 478.47
               Mean episode length: 165.06
                 Mean success rate: 11.50
                  Mean reward/step: 2.78
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 4317184
                    Iteration time: 2.51s
                        Total time: 1348.90s
                               ETA: 8892.0s

################################################################################
                     [1m Learning iteration 527/4000 [0m

                       Computation: 3218 steps/s (collection: 0.459s, learning 2.087s)
               Value function loss: 5605.2062
                    Surrogate loss: 0.0129
             Mean action noise std: 0.98
                       Mean reward: 520.93
               Mean episode length: 183.08
                 Mean success rate: 13.00
                  Mean reward/step: 3.03
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 2.55s
                        Total time: 1351.45s
                               ETA: 8889.4s

################################################################################
                     [1m Learning iteration 528/4000 [0m

                       Computation: 3194 steps/s (collection: 0.468s, learning 2.097s)
               Value function loss: 5031.2539
                    Surrogate loss: 0.0148
             Mean action noise std: 0.98
                       Mean reward: 529.63
               Mean episode length: 195.24
                 Mean success rate: 11.00
                  Mean reward/step: 2.99
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 4333568
                    Iteration time: 2.56s
                        Total time: 1354.01s
                               ETA: 8886.8s

################################################################################
                     [1m Learning iteration 529/4000 [0m

                       Computation: 3166 steps/s (collection: 0.467s, learning 2.119s)
               Value function loss: 3555.3743
                    Surrogate loss: 0.0169
             Mean action noise std: 0.98
                       Mean reward: 508.56
               Mean episode length: 191.40
                 Mean success rate: 11.00
                  Mean reward/step: 3.05
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 2.59s
                        Total time: 1356.60s
                               ETA: 8884.5s

################################################################################
                     [1m Learning iteration 530/4000 [0m

                       Computation: 3216 steps/s (collection: 0.475s, learning 2.073s)
               Value function loss: 5295.7113
                    Surrogate loss: 0.0136
             Mean action noise std: 0.97
                       Mean reward: 548.92
               Mean episode length: 206.32
                 Mean success rate: 10.50
                  Mean reward/step: 3.11
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 4349952
                    Iteration time: 2.55s
                        Total time: 1359.15s
                               ETA: 8881.8s

################################################################################
                     [1m Learning iteration 531/4000 [0m

                       Computation: 3124 steps/s (collection: 0.519s, learning 2.103s)
               Value function loss: 4994.2662
                    Surrogate loss: 0.0148
             Mean action noise std: 0.97
                       Mean reward: 513.97
               Mean episode length: 190.26
                 Mean success rate: 10.50
                  Mean reward/step: 2.90
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 2.62s
                        Total time: 1361.77s
                               ETA: 8879.7s

################################################################################
                     [1m Learning iteration 532/4000 [0m

                       Computation: 3193 steps/s (collection: 0.486s, learning 2.079s)
               Value function loss: 4095.9042
                    Surrogate loss: 0.0172
             Mean action noise std: 0.97
                       Mean reward: 513.09
               Mean episode length: 188.84
                 Mean success rate: 11.00
                  Mean reward/step: 2.98
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4366336
                    Iteration time: 2.57s
                        Total time: 1364.33s
                               ETA: 8877.1s

################################################################################
                     [1m Learning iteration 533/4000 [0m

                       Computation: 3224 steps/s (collection: 0.475s, learning 2.066s)
               Value function loss: 4782.1910
                    Surrogate loss: 0.0125
             Mean action noise std: 0.97
                       Mean reward: 538.85
               Mean episode length: 184.10
                 Mean success rate: 12.50
                  Mean reward/step: 2.73
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 2.54s
                        Total time: 1366.88s
                               ETA: 8874.5s

################################################################################
                     [1m Learning iteration 534/4000 [0m

                       Computation: 3204 steps/s (collection: 0.477s, learning 2.079s)
               Value function loss: 2612.8685
                    Surrogate loss: 0.0181
             Mean action noise std: 0.97
                       Mean reward: 571.30
               Mean episode length: 197.65
                 Mean success rate: 14.50
                  Mean reward/step: 2.51
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4382720
                    Iteration time: 2.56s
                        Total time: 1369.43s
                               ETA: 8871.9s

################################################################################
                     [1m Learning iteration 535/4000 [0m

                       Computation: 3238 steps/s (collection: 0.471s, learning 2.059s)
               Value function loss: 4810.0500
                    Surrogate loss: 0.0148
             Mean action noise std: 0.97
                       Mean reward: 600.23
               Mean episode length: 200.84
                 Mean success rate: 16.50
                  Mean reward/step: 2.76
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 2.53s
                        Total time: 1371.96s
                               ETA: 8869.1s

################################################################################
                     [1m Learning iteration 536/4000 [0m

                       Computation: 3207 steps/s (collection: 0.491s, learning 2.064s)
               Value function loss: 4401.7331
                    Surrogate loss: 0.0168
             Mean action noise std: 0.97
                       Mean reward: 640.32
               Mean episode length: 204.47
                 Mean success rate: 16.50
                  Mean reward/step: 2.99
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4399104
                    Iteration time: 2.55s
                        Total time: 1374.52s
                               ETA: 8866.5s

################################################################################
                     [1m Learning iteration 537/4000 [0m

                       Computation: 3256 steps/s (collection: 0.448s, learning 2.067s)
               Value function loss: 4488.0793
                    Surrogate loss: 0.0131
             Mean action noise std: 0.97
                       Mean reward: 587.88
               Mean episode length: 193.44
                 Mean success rate: 14.50
                  Mean reward/step: 3.02
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 2.52s
                        Total time: 1377.03s
                               ETA: 8863.7s

################################################################################
                     [1m Learning iteration 538/4000 [0m

                       Computation: 3243 steps/s (collection: 0.447s, learning 2.078s)
               Value function loss: 5151.3945
                    Surrogate loss: 0.0150
             Mean action noise std: 0.97
                       Mean reward: 556.46
               Mean episode length: 193.05
                 Mean success rate: 13.50
                  Mean reward/step: 3.11
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4415488
                    Iteration time: 2.53s
                        Total time: 1379.56s
                               ETA: 8860.9s

################################################################################
                     [1m Learning iteration 539/4000 [0m

                       Computation: 3191 steps/s (collection: 0.494s, learning 2.072s)
               Value function loss: 6036.9221
                    Surrogate loss: 0.0150
             Mean action noise std: 0.97
                       Mean reward: 556.70
               Mean episode length: 189.44
                 Mean success rate: 13.50
                  Mean reward/step: 3.28
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 2.57s
                        Total time: 1382.12s
                               ETA: 8858.4s

################################################################################
                     [1m Learning iteration 540/4000 [0m

                       Computation: 3155 steps/s (collection: 0.516s, learning 2.080s)
               Value function loss: 5206.7218
                    Surrogate loss: 0.0167
             Mean action noise std: 0.97
                       Mean reward: 565.68
               Mean episode length: 194.97
                 Mean success rate: 12.50
                  Mean reward/step: 3.23
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4431872
                    Iteration time: 2.60s
                        Total time: 1384.72s
                               ETA: 8856.1s

################################################################################
                     [1m Learning iteration 541/4000 [0m

                       Computation: 3228 steps/s (collection: 0.456s, learning 2.081s)
               Value function loss: 4580.0162
                    Surrogate loss: 0.0165
             Mean action noise std: 0.97
                       Mean reward: 570.12
               Mean episode length: 199.43
                 Mean success rate: 12.50
                  Mean reward/step: 3.21
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 2.54s
                        Total time: 1387.26s
                               ETA: 8853.4s

################################################################################
                     [1m Learning iteration 542/4000 [0m

                       Computation: 3275 steps/s (collection: 0.434s, learning 2.068s)
               Value function loss: 6885.7125
                    Surrogate loss: 0.0149
             Mean action noise std: 0.97
                       Mean reward: 611.12
               Mean episode length: 215.38
                 Mean success rate: 14.00
                  Mean reward/step: 3.25
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4448256
                    Iteration time: 2.50s
                        Total time: 1389.76s
                               ETA: 8850.4s

################################################################################
                     [1m Learning iteration 543/4000 [0m

                       Computation: 3205 steps/s (collection: 0.469s, learning 2.086s)
               Value function loss: 7062.4047
                    Surrogate loss: 0.0136
             Mean action noise std: 0.97
                       Mean reward: 717.15
               Mean episode length: 232.10
                 Mean success rate: 17.00
                  Mean reward/step: 2.90
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 2.56s
                        Total time: 1392.31s
                               ETA: 8847.8s

################################################################################
                     [1m Learning iteration 544/4000 [0m

                       Computation: 3174 steps/s (collection: 0.484s, learning 2.096s)
               Value function loss: 5599.0354
                    Surrogate loss: 0.0149
             Mean action noise std: 0.97
                       Mean reward: 677.42
               Mean episode length: 220.94
                 Mean success rate: 16.00
                  Mean reward/step: 3.22
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4464640
                    Iteration time: 2.58s
                        Total time: 1394.89s
                               ETA: 8845.4s

################################################################################
                     [1m Learning iteration 545/4000 [0m

                       Computation: 3200 steps/s (collection: 0.492s, learning 2.068s)
               Value function loss: 6415.5141
                    Surrogate loss: 0.0128
             Mean action noise std: 0.97
                       Mean reward: 673.07
               Mean episode length: 220.69
                 Mean success rate: 16.00
                  Mean reward/step: 3.49
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 2.56s
                        Total time: 1397.45s
                               ETA: 8842.9s

################################################################################
                     [1m Learning iteration 546/4000 [0m

                       Computation: 3230 steps/s (collection: 0.482s, learning 2.054s)
               Value function loss: 7706.0757
                    Surrogate loss: 0.0115
             Mean action noise std: 0.97
                       Mean reward: 693.93
               Mean episode length: 220.18
                 Mean success rate: 17.00
                  Mean reward/step: 3.13
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 4481024
                    Iteration time: 2.54s
                        Total time: 1399.99s
                               ETA: 8840.1s

################################################################################
                     [1m Learning iteration 547/4000 [0m

                       Computation: 3176 steps/s (collection: 0.479s, learning 2.100s)
               Value function loss: 5374.6346
                    Surrogate loss: 0.0123
             Mean action noise std: 0.97
                       Mean reward: 618.22
               Mean episode length: 206.66
                 Mean success rate: 15.00
                  Mean reward/step: 2.91
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 2.58s
                        Total time: 1402.57s
                               ETA: 8837.7s

################################################################################
                     [1m Learning iteration 548/4000 [0m

                       Computation: 3279 steps/s (collection: 0.447s, learning 2.051s)
               Value function loss: 5027.4731
                    Surrogate loss: 0.0155
             Mean action noise std: 0.97
                       Mean reward: 654.56
               Mean episode length: 224.12
                 Mean success rate: 16.00
                  Mean reward/step: 2.80
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4497408
                    Iteration time: 2.50s
                        Total time: 1405.07s
                               ETA: 8834.8s

################################################################################
                     [1m Learning iteration 549/4000 [0m

                       Computation: 3164 steps/s (collection: 0.535s, learning 2.054s)
               Value function loss: 6988.3956
                    Surrogate loss: 0.0117
             Mean action noise std: 0.97
                       Mean reward: 677.49
               Mean episode length: 215.80
                 Mean success rate: 16.00
                  Mean reward/step: 2.80
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 2.59s
                        Total time: 1407.65s
                               ETA: 8832.4s

################################################################################
                     [1m Learning iteration 550/4000 [0m

                       Computation: 3221 steps/s (collection: 0.501s, learning 2.043s)
               Value function loss: 6120.1675
                    Surrogate loss: 0.0144
             Mean action noise std: 0.97
                       Mean reward: 719.12
               Mean episode length: 217.19
                 Mean success rate: 16.00
                  Mean reward/step: 2.90
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4513792
                    Iteration time: 2.54s
                        Total time: 1410.20s
                               ETA: 8829.7s

################################################################################
                     [1m Learning iteration 551/4000 [0m

                       Computation: 3167 steps/s (collection: 0.519s, learning 2.067s)
               Value function loss: 5473.4474
                    Surrogate loss: 0.0127
             Mean action noise std: 0.97
                       Mean reward: 704.17
               Mean episode length: 207.32
                 Mean success rate: 16.00
                  Mean reward/step: 2.64
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 2.59s
                        Total time: 1412.78s
                               ETA: 8827.3s

################################################################################
                     [1m Learning iteration 552/4000 [0m

                       Computation: 3296 steps/s (collection: 0.429s, learning 2.056s)
               Value function loss: 3549.9173
                    Surrogate loss: 0.0133
             Mean action noise std: 0.97
                       Mean reward: 666.11
               Mean episode length: 215.71
                 Mean success rate: 14.00
                  Mean reward/step: 2.62
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4530176
                    Iteration time: 2.48s
                        Total time: 1415.27s
                               ETA: 8824.3s

################################################################################
                     [1m Learning iteration 553/4000 [0m

                       Computation: 3206 steps/s (collection: 0.494s, learning 2.061s)
               Value function loss: 4264.3864
                    Surrogate loss: 0.0144
             Mean action noise std: 0.97
                       Mean reward: 588.28
               Mean episode length: 204.38
                 Mean success rate: 11.50
                  Mean reward/step: 2.79
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 2.56s
                        Total time: 1417.82s
                               ETA: 8821.7s

################################################################################
                     [1m Learning iteration 554/4000 [0m

                       Computation: 3255 steps/s (collection: 0.468s, learning 2.049s)
               Value function loss: 3835.9274
                    Surrogate loss: 0.0200
             Mean action noise std: 0.97
                       Mean reward: 542.27
               Mean episode length: 200.16
                 Mean success rate: 10.50
                  Mean reward/step: 2.86
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4546560
                    Iteration time: 2.52s
                        Total time: 1420.34s
                               ETA: 8818.9s

################################################################################
                     [1m Learning iteration 555/4000 [0m

                       Computation: 3270 steps/s (collection: 0.459s, learning 2.045s)
               Value function loss: 3981.7520
                    Surrogate loss: 0.0172
             Mean action noise std: 0.97
                       Mean reward: 528.26
               Mean episode length: 196.41
                 Mean success rate: 10.50
                  Mean reward/step: 3.06
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 2.50s
                        Total time: 1422.84s
                               ETA: 8816.0s

################################################################################
                     [1m Learning iteration 556/4000 [0m

                       Computation: 3273 steps/s (collection: 0.460s, learning 2.042s)
               Value function loss: 6806.9895
                    Surrogate loss: 0.0149
             Mean action noise std: 0.97
                       Mean reward: 487.17
               Mean episode length: 188.53
                 Mean success rate: 11.00
                  Mean reward/step: 3.02
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4562944
                    Iteration time: 2.50s
                        Total time: 1425.35s
                               ETA: 8813.1s

################################################################################
                     [1m Learning iteration 557/4000 [0m

                       Computation: 3280 steps/s (collection: 0.451s, learning 2.046s)
               Value function loss: 3962.6456
                    Surrogate loss: 0.0159
             Mean action noise std: 0.97
                       Mean reward: 545.47
               Mean episode length: 199.34
                 Mean success rate: 13.00
                  Mean reward/step: 2.97
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 2.50s
                        Total time: 1427.84s
                               ETA: 8810.2s

################################################################################
                     [1m Learning iteration 558/4000 [0m

                       Computation: 3284 steps/s (collection: 0.458s, learning 2.036s)
               Value function loss: 5353.2646
                    Surrogate loss: 0.0143
             Mean action noise std: 0.97
                       Mean reward: 605.42
               Mean episode length: 199.50
                 Mean success rate: 16.00
                  Mean reward/step: 3.04
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4579328
                    Iteration time: 2.49s
                        Total time: 1430.34s
                               ETA: 8807.2s

################################################################################
                     [1m Learning iteration 559/4000 [0m

                       Computation: 3250 steps/s (collection: 0.473s, learning 2.047s)
               Value function loss: 7226.0446
                    Surrogate loss: 0.0152
             Mean action noise std: 0.97
                       Mean reward: 665.42
               Mean episode length: 201.25
                 Mean success rate: 17.00
                  Mean reward/step: 3.15
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 2.52s
                        Total time: 1432.86s
                               ETA: 8804.4s

################################################################################
                     [1m Learning iteration 560/4000 [0m

                       Computation: 3219 steps/s (collection: 0.492s, learning 2.053s)
               Value function loss: 4826.0054
                    Surrogate loss: 0.0165
             Mean action noise std: 0.97
                       Mean reward: 660.03
               Mean episode length: 202.23
                 Mean success rate: 17.50
                  Mean reward/step: 3.12
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4595712
                    Iteration time: 2.54s
                        Total time: 1435.40s
                               ETA: 8801.8s

################################################################################
                     [1m Learning iteration 561/4000 [0m

                       Computation: 3258 steps/s (collection: 0.454s, learning 2.060s)
               Value function loss: 5839.1285
                    Surrogate loss: 0.0173
             Mean action noise std: 0.97
                       Mean reward: 645.91
               Mean episode length: 207.04
                 Mean success rate: 16.50
                  Mean reward/step: 3.08
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 2.51s
                        Total time: 1437.92s
                               ETA: 8798.9s

################################################################################
                     [1m Learning iteration 562/4000 [0m

                       Computation: 3244 steps/s (collection: 0.444s, learning 2.081s)
               Value function loss: 5120.4677
                    Surrogate loss: 0.0173
             Mean action noise std: 0.97
                       Mean reward: 608.62
               Mean episode length: 210.47
                 Mean success rate: 14.50
                  Mean reward/step: 3.12
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4612096
                    Iteration time: 2.52s
                        Total time: 1440.44s
                               ETA: 8796.2s

################################################################################
                     [1m Learning iteration 563/4000 [0m

                       Computation: 3208 steps/s (collection: 0.461s, learning 2.092s)
               Value function loss: 3992.5854
                    Surrogate loss: 0.0183
             Mean action noise std: 0.97
                       Mean reward: 617.87
               Mean episode length: 210.01
                 Mean success rate: 14.00
                  Mean reward/step: 3.10
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 2.55s
                        Total time: 1442.99s
                               ETA: 8793.6s

################################################################################
                     [1m Learning iteration 564/4000 [0m

                       Computation: 3127 steps/s (collection: 0.507s, learning 2.112s)
               Value function loss: 4585.6815
                    Surrogate loss: 0.0198
             Mean action noise std: 0.97
                       Mean reward: 557.80
               Mean episode length: 209.68
                 Mean success rate: 12.50
                  Mean reward/step: 2.97
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 4628480
                    Iteration time: 2.62s
                        Total time: 1445.61s
                               ETA: 8791.4s

################################################################################
                     [1m Learning iteration 565/4000 [0m

                       Computation: 3179 steps/s (collection: 0.479s, learning 2.097s)
               Value function loss: 4784.5134
                    Surrogate loss: 0.0127
             Mean action noise std: 0.97
                       Mean reward: 575.74
               Mean episode length: 207.93
                 Mean success rate: 13.00
                  Mean reward/step: 2.84
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 2.58s
                        Total time: 1448.19s
                               ETA: 8788.9s

################################################################################
                     [1m Learning iteration 566/4000 [0m

                       Computation: 3125 steps/s (collection: 0.512s, learning 2.110s)
               Value function loss: 2202.8791
                    Surrogate loss: 0.0166
             Mean action noise std: 0.97
                       Mean reward: 532.87
               Mean episode length: 193.93
                 Mean success rate: 12.00
                  Mean reward/step: 2.51
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4644864
                    Iteration time: 2.62s
                        Total time: 1450.81s
                               ETA: 8786.7s

################################################################################
                     [1m Learning iteration 567/4000 [0m

                       Computation: 3173 steps/s (collection: 0.504s, learning 2.078s)
               Value function loss: 3481.2772
                    Surrogate loss: 0.0114
             Mean action noise std: 0.97
                       Mean reward: 482.04
               Mean episode length: 185.88
                 Mean success rate: 11.50
                  Mean reward/step: 2.49
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 2.58s
                        Total time: 1453.39s
                               ETA: 8784.3s

################################################################################
                     [1m Learning iteration 568/4000 [0m

                       Computation: 3122 steps/s (collection: 0.503s, learning 2.120s)
               Value function loss: 3841.4717
                    Surrogate loss: 0.0185
             Mean action noise std: 0.97
                       Mean reward: 493.36
               Mean episode length: 177.38
                 Mean success rate: 12.50
                  Mean reward/step: 2.56
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 4661248
                    Iteration time: 2.62s
                        Total time: 1456.02s
                               ETA: 8782.2s

################################################################################
                     [1m Learning iteration 569/4000 [0m

                       Computation: 3129 steps/s (collection: 0.509s, learning 2.109s)
               Value function loss: 3430.9889
                    Surrogate loss: 0.0175
             Mean action noise std: 0.97
                       Mean reward: 448.25
               Mean episode length: 175.65
                 Mean success rate: 9.50
                  Mean reward/step: 2.46
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 2.62s
                        Total time: 1458.63s
                               ETA: 8779.9s

################################################################################
                     [1m Learning iteration 570/4000 [0m

                       Computation: 3232 steps/s (collection: 0.480s, learning 2.054s)
               Value function loss: 3185.8265
                    Surrogate loss: 0.0185
             Mean action noise std: 0.97
                       Mean reward: 516.19
               Mean episode length: 194.53
                 Mean success rate: 9.50
                  Mean reward/step: 2.64
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 4677632
                    Iteration time: 2.53s
                        Total time: 1461.17s
                               ETA: 8777.2s

################################################################################
                     [1m Learning iteration 571/4000 [0m

                       Computation: 3163 steps/s (collection: 0.482s, learning 2.108s)
               Value function loss: 3934.0453
                    Surrogate loss: 0.0138
             Mean action noise std: 0.97
                       Mean reward: 498.49
               Mean episode length: 185.40
                 Mean success rate: 9.00
                  Mean reward/step: 2.58
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 2.59s
                        Total time: 1463.76s
                               ETA: 8774.9s

################################################################################
                     [1m Learning iteration 572/4000 [0m

                       Computation: 3145 steps/s (collection: 0.526s, learning 2.078s)
               Value function loss: 4281.3711
                    Surrogate loss: 0.0176
             Mean action noise std: 0.97
                       Mean reward: 479.19
               Mean episode length: 181.97
                 Mean success rate: 7.50
                  Mean reward/step: 2.75
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 4694016
                    Iteration time: 2.60s
                        Total time: 1466.36s
                               ETA: 8772.6s

################################################################################
                     [1m Learning iteration 573/4000 [0m

                       Computation: 3178 steps/s (collection: 0.482s, learning 2.095s)
               Value function loss: 4314.5082
                    Surrogate loss: 0.0136
             Mean action noise std: 0.97
                       Mean reward: 511.81
               Mean episode length: 182.71
                 Mean success rate: 9.50
                  Mean reward/step: 2.79
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 2.58s
                        Total time: 1468.94s
                               ETA: 8770.1s

################################################################################
                     [1m Learning iteration 574/4000 [0m

                       Computation: 3203 steps/s (collection: 0.479s, learning 2.078s)
               Value function loss: 4126.6608
                    Surrogate loss: 0.0171
             Mean action noise std: 0.97
                       Mean reward: 467.51
               Mean episode length: 173.19
                 Mean success rate: 9.50
                  Mean reward/step: 3.15
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4710400
                    Iteration time: 2.56s
                        Total time: 1471.50s
                               ETA: 8767.6s

################################################################################
                     [1m Learning iteration 575/4000 [0m

                       Computation: 3073 steps/s (collection: 0.547s, learning 2.119s)
               Value function loss: 5869.9247
                    Surrogate loss: 0.0152
             Mean action noise std: 0.97
                       Mean reward: 450.49
               Mean episode length: 164.37
                 Mean success rate: 10.00
                  Mean reward/step: 3.39
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 2.67s
                        Total time: 1474.16s
                               ETA: 8765.6s

################################################################################
                     [1m Learning iteration 576/4000 [0m

                       Computation: 3177 steps/s (collection: 0.482s, learning 2.097s)
               Value function loss: 7286.8682
                    Surrogate loss: 0.0134
             Mean action noise std: 0.97
                       Mean reward: 491.76
               Mean episode length: 178.63
                 Mean success rate: 11.50
                  Mean reward/step: 3.70
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4726784
                    Iteration time: 2.58s
                        Total time: 1476.74s
                               ETA: 8763.2s

################################################################################
                     [1m Learning iteration 577/4000 [0m

                       Computation: 3092 steps/s (collection: 0.515s, learning 2.134s)
               Value function loss: 7908.7025
                    Surrogate loss: 0.0123
             Mean action noise std: 0.97
                       Mean reward: 533.96
               Mean episode length: 177.16
                 Mean success rate: 12.50
                  Mean reward/step: 3.39
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 2.65s
                        Total time: 1479.39s
                               ETA: 8761.2s

################################################################################
                     [1m Learning iteration 578/4000 [0m

                       Computation: 3082 steps/s (collection: 0.531s, learning 2.127s)
               Value function loss: 7013.0034
                    Surrogate loss: 0.0133
             Mean action noise std: 0.97
                       Mean reward: 537.19
               Mean episode length: 181.23
                 Mean success rate: 12.50
                  Mean reward/step: 3.67
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4743168
                    Iteration time: 2.66s
                        Total time: 1482.05s
                               ETA: 8759.2s

################################################################################
                     [1m Learning iteration 579/4000 [0m

                       Computation: 3128 steps/s (collection: 0.480s, learning 2.139s)
               Value function loss: 6706.4261
                    Surrogate loss: 0.0163
             Mean action noise std: 0.97
                       Mean reward: 539.30
               Mean episode length: 181.11
                 Mean success rate: 12.50
                  Mean reward/step: 3.95
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 2.62s
                        Total time: 1484.67s
                               ETA: 8757.0s

################################################################################
                     [1m Learning iteration 580/4000 [0m

                       Computation: 3211 steps/s (collection: 0.463s, learning 2.087s)
               Value function loss: 5904.3531
                    Surrogate loss: 0.0155
             Mean action noise std: 0.97
                       Mean reward: 534.14
               Mean episode length: 188.41
                 Mean success rate: 11.50
                  Mean reward/step: 4.14
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4759552
                    Iteration time: 2.55s
                        Total time: 1487.22s
                               ETA: 8754.4s

################################################################################
                     [1m Learning iteration 581/4000 [0m

                       Computation: 3081 steps/s (collection: 0.503s, learning 2.155s)
               Value function loss: 5479.1772
                    Surrogate loss: 0.0180
             Mean action noise std: 0.97
                       Mean reward: 543.28
               Mean episode length: 185.53
                 Mean success rate: 12.00
                  Mean reward/step: 3.89
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 2.66s
                        Total time: 1489.87s
                               ETA: 8752.4s

################################################################################
                     [1m Learning iteration 582/4000 [0m

                       Computation: 3039 steps/s (collection: 0.522s, learning 2.173s)
               Value function loss: 8492.0681
                    Surrogate loss: 0.0145
             Mean action noise std: 0.97
                       Mean reward: 607.79
               Mean episode length: 193.12
                 Mean success rate: 14.00
                  Mean reward/step: 4.00
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4775936
                    Iteration time: 2.70s
                        Total time: 1492.57s
                               ETA: 8750.6s

################################################################################
                     [1m Learning iteration 583/4000 [0m

                       Computation: 3061 steps/s (collection: 0.565s, learning 2.111s)
               Value function loss: 9686.4840
                    Surrogate loss: 0.0140
             Mean action noise std: 0.97
                       Mean reward: 635.60
               Mean episode length: 205.04
                 Mean success rate: 14.50
                  Mean reward/step: 4.49
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 2.68s
                        Total time: 1495.25s
                               ETA: 8748.7s

################################################################################
                     [1m Learning iteration 584/4000 [0m

                       Computation: 3154 steps/s (collection: 0.492s, learning 2.105s)
               Value function loss: 10336.8258
                    Surrogate loss: 0.0147
             Mean action noise std: 0.97
                       Mean reward: 698.58
               Mean episode length: 214.83
                 Mean success rate: 16.00
                  Mean reward/step: 4.56
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4792320
                    Iteration time: 2.60s
                        Total time: 1497.84s
                               ETA: 8746.4s

################################################################################
                     [1m Learning iteration 585/4000 [0m

                       Computation: 3178 steps/s (collection: 0.480s, learning 2.097s)
               Value function loss: 9254.7230
                    Surrogate loss: 0.0160
             Mean action noise std: 0.97
                       Mean reward: 807.76
               Mean episode length: 228.53
                 Mean success rate: 18.00
                  Mean reward/step: 4.59
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 2.58s
                        Total time: 1500.42s
                               ETA: 8743.9s

################################################################################
                     [1m Learning iteration 586/4000 [0m

                       Computation: 3076 steps/s (collection: 0.557s, learning 2.106s)
               Value function loss: 12808.8458
                    Surrogate loss: 0.0144
             Mean action noise std: 0.97
                       Mean reward: 979.38
               Mean episode length: 248.07
                 Mean success rate: 22.00
                  Mean reward/step: 4.91
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4808704
                    Iteration time: 2.66s
                        Total time: 1503.08s
                               ETA: 8741.9s

################################################################################
                     [1m Learning iteration 587/4000 [0m

                       Computation: 3194 steps/s (collection: 0.505s, learning 2.060s)
               Value function loss: 13571.7412
                    Surrogate loss: 0.0142
             Mean action noise std: 0.97
                       Mean reward: 926.00
               Mean episode length: 240.87
                 Mean success rate: 19.00
                  Mean reward/step: 5.19
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 2.56s
                        Total time: 1505.65s
                               ETA: 8739.4s

################################################################################
                     [1m Learning iteration 588/4000 [0m

                       Computation: 3195 steps/s (collection: 0.484s, learning 2.080s)
               Value function loss: 11108.0452
                    Surrogate loss: 0.0131
             Mean action noise std: 0.97
                       Mean reward: 976.03
               Mean episode length: 240.02
                 Mean success rate: 21.50
                  Mean reward/step: 5.15
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4825088
                    Iteration time: 2.56s
                        Total time: 1508.21s
                               ETA: 8736.9s

################################################################################
                     [1m Learning iteration 589/4000 [0m

                       Computation: 3290 steps/s (collection: 0.439s, learning 2.051s)
               Value function loss: 11560.3927
                    Surrogate loss: 0.0150
             Mean action noise std: 0.97
                       Mean reward: 982.81
               Mean episode length: 232.50
                 Mean success rate: 21.50
                  Mean reward/step: 4.89
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 2.49s
                        Total time: 1510.70s
                               ETA: 8733.9s

################################################################################
                     [1m Learning iteration 590/4000 [0m

                       Computation: 3158 steps/s (collection: 0.494s, learning 2.100s)
               Value function loss: 10881.8864
                    Surrogate loss: 0.0156
             Mean action noise std: 0.97
                       Mean reward: 932.37
               Mean episode length: 232.56
                 Mean success rate: 20.50
                  Mean reward/step: 4.79
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4841472
                    Iteration time: 2.59s
                        Total time: 1513.29s
                               ETA: 8731.5s

################################################################################
                     [1m Learning iteration 591/4000 [0m

                       Computation: 3253 steps/s (collection: 0.458s, learning 2.060s)
               Value function loss: 7398.9866
                    Surrogate loss: 0.0199
             Mean action noise std: 0.97
                       Mean reward: 843.36
               Mean episode length: 216.76
                 Mean success rate: 17.50
                  Mean reward/step: 4.76
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 2.52s
                        Total time: 1515.81s
                               ETA: 8728.7s

################################################################################
                     [1m Learning iteration 592/4000 [0m

                       Computation: 3222 steps/s (collection: 0.457s, learning 2.086s)
               Value function loss: 12509.6780
                    Surrogate loss: 0.0148
             Mean action noise std: 0.97
                       Mean reward: 896.50
               Mean episode length: 219.41
                 Mean success rate: 19.50
                  Mean reward/step: 4.94
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4857856
                    Iteration time: 2.54s
                        Total time: 1518.35s
                               ETA: 8726.1s

################################################################################
                     [1m Learning iteration 593/4000 [0m

                       Computation: 3221 steps/s (collection: 0.472s, learning 2.071s)
               Value function loss: 11448.2187
                    Surrogate loss: 0.0164
             Mean action noise std: 0.97
                       Mean reward: 977.78
               Mean episode length: 222.12
                 Mean success rate: 19.50
                  Mean reward/step: 4.99
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 2.54s
                        Total time: 1520.90s
                               ETA: 8723.4s

################################################################################
                     [1m Learning iteration 594/4000 [0m

                       Computation: 3191 steps/s (collection: 0.473s, learning 2.094s)
               Value function loss: 15066.7304
                    Surrogate loss: 0.0131
             Mean action noise std: 0.97
                       Mean reward: 1041.99
               Mean episode length: 224.73
                 Mean success rate: 19.50
                  Mean reward/step: 5.24
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4874240
                    Iteration time: 2.57s
                        Total time: 1523.46s
                               ETA: 8720.9s

################################################################################
                     [1m Learning iteration 595/4000 [0m

                       Computation: 3226 steps/s (collection: 0.483s, learning 2.055s)
               Value function loss: 10498.2618
                    Surrogate loss: 0.0132
             Mean action noise std: 0.97
                       Mean reward: 1077.08
               Mean episode length: 221.88
                 Mean success rate: 20.50
                  Mean reward/step: 5.39
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 2.54s
                        Total time: 1526.00s
                               ETA: 8718.2s

################################################################################
                     [1m Learning iteration 596/4000 [0m

                       Computation: 3193 steps/s (collection: 0.474s, learning 2.092s)
               Value function loss: 8595.3488
                    Surrogate loss: 0.0146
             Mean action noise std: 0.97
                       Mean reward: 1051.11
               Mean episode length: 214.32
                 Mean success rate: 21.50
                  Mean reward/step: 6.06
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4890624
                    Iteration time: 2.57s
                        Total time: 1528.57s
                               ETA: 8715.7s

################################################################################
                     [1m Learning iteration 597/4000 [0m

                       Computation: 3198 steps/s (collection: 0.500s, learning 2.060s)
               Value function loss: 10965.1911
                    Surrogate loss: 0.0155
             Mean action noise std: 0.97
                       Mean reward: 1022.09
               Mean episode length: 208.09
                 Mean success rate: 22.00
                  Mean reward/step: 5.94
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 2.56s
                        Total time: 1531.13s
                               ETA: 8713.1s

################################################################################
                     [1m Learning iteration 598/4000 [0m

                       Computation: 3136 steps/s (collection: 0.495s, learning 2.117s)
               Value function loss: 19345.3739
                    Surrogate loss: 0.0142
             Mean action noise std: 0.97
                       Mean reward: 1080.38
               Mean episode length: 213.35
                 Mean success rate: 22.00
                  Mean reward/step: 6.68
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4907008
                    Iteration time: 2.61s
                        Total time: 1533.74s
                               ETA: 8710.8s

################################################################################
                     [1m Learning iteration 599/4000 [0m

                       Computation: 3215 steps/s (collection: 0.470s, learning 2.078s)
               Value function loss: 20118.2736
                    Surrogate loss: 0.0129
             Mean action noise std: 0.97
                       Mean reward: 1124.06
               Mean episode length: 227.81
                 Mean success rate: 23.00
                  Mean reward/step: 6.76
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 2.55s
                        Total time: 1536.29s
                               ETA: 8708.2s

################################################################################
                     [1m Learning iteration 600/4000 [0m

                       Computation: 3222 steps/s (collection: 0.466s, learning 2.076s)
               Value function loss: 23621.7769
                    Surrogate loss: 0.0131
             Mean action noise std: 0.97
                       Mean reward: 1109.04
               Mean episode length: 225.86
                 Mean success rate: 22.00
                  Mean reward/step: 6.86
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4923392
                    Iteration time: 2.54s
                        Total time: 1538.83s
                               ETA: 8705.5s

################################################################################
                     [1m Learning iteration 601/4000 [0m

                       Computation: 3130 steps/s (collection: 0.493s, learning 2.124s)
               Value function loss: 19153.6629
                    Surrogate loss: 0.0153
             Mean action noise std: 0.97
                       Mean reward: 1144.38
               Mean episode length: 223.54
                 Mean success rate: 22.50
                  Mean reward/step: 6.60
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 2.62s
                        Total time: 1541.45s
                               ETA: 8703.3s

################################################################################
                     [1m Learning iteration 602/4000 [0m

                       Computation: 2997 steps/s (collection: 0.594s, learning 2.139s)
               Value function loss: 19047.1342
                    Surrogate loss: 0.0138
             Mean action noise std: 0.97
                       Mean reward: 1421.41
               Mean episode length: 249.93
                 Mean success rate: 27.00
                  Mean reward/step: 6.70
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4939776
                    Iteration time: 2.73s
                        Total time: 1544.18s
                               ETA: 8701.7s

################################################################################
                     [1m Learning iteration 603/4000 [0m

                       Computation: 3190 steps/s (collection: 0.472s, learning 2.096s)
               Value function loss: 19463.7003
                    Surrogate loss: 0.0163
             Mean action noise std: 0.97
                       Mean reward: 1521.42
               Mean episode length: 257.18
                 Mean success rate: 28.00
                  Mean reward/step: 5.94
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 2.57s
                        Total time: 1546.75s
                               ETA: 8699.2s

################################################################################
                     [1m Learning iteration 604/4000 [0m

                       Computation: 3144 steps/s (collection: 0.509s, learning 2.096s)
               Value function loss: 13837.0343
                    Surrogate loss: 0.0147
             Mean action noise std: 0.97
                       Mean reward: 1468.94
               Mean episode length: 248.99
                 Mean success rate: 27.50
                  Mean reward/step: 6.44
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4956160
                    Iteration time: 2.60s
                        Total time: 1549.35s
                               ETA: 8696.9s

################################################################################
                     [1m Learning iteration 605/4000 [0m

                       Computation: 3195 steps/s (collection: 0.469s, learning 2.094s)
               Value function loss: 19975.1792
                    Surrogate loss: 0.0119
             Mean action noise std: 0.97
                       Mean reward: 1433.28
               Mean episode length: 238.37
                 Mean success rate: 30.00
                  Mean reward/step: 6.61
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 2.56s
                        Total time: 1551.92s
                               ETA: 8694.3s

################################################################################
                     [1m Learning iteration 606/4000 [0m

                       Computation: 3099 steps/s (collection: 0.519s, learning 2.123s)
               Value function loss: 22360.1440
                    Surrogate loss: 0.0135
             Mean action noise std: 0.97
                       Mean reward: 1373.04
               Mean episode length: 221.93
                 Mean success rate: 28.50
                  Mean reward/step: 5.96
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 4972544
                    Iteration time: 2.64s
                        Total time: 1554.56s
                               ETA: 8692.2s

################################################################################
                     [1m Learning iteration 607/4000 [0m

                       Computation: 3076 steps/s (collection: 0.540s, learning 2.123s)
               Value function loss: 22280.8593
                    Surrogate loss: 0.0142
             Mean action noise std: 0.97
                       Mean reward: 1250.16
               Mean episode length: 216.53
                 Mean success rate: 28.00
                  Mean reward/step: 5.62
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 2.66s
                        Total time: 1557.22s
                               ETA: 8690.2s

################################################################################
                     [1m Learning iteration 608/4000 [0m

                       Computation: 3143 steps/s (collection: 0.480s, learning 2.126s)
               Value function loss: 14283.4017
                    Surrogate loss: 0.0178
             Mean action noise std: 0.97
                       Mean reward: 1278.53
               Mean episode length: 214.28
                 Mean success rate: 28.50
                  Mean reward/step: 5.29
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4988928
                    Iteration time: 2.61s
                        Total time: 1559.83s
                               ETA: 8687.9s

################################################################################
                     [1m Learning iteration 609/4000 [0m

                       Computation: 3145 steps/s (collection: 0.478s, learning 2.127s)
               Value function loss: 16750.0943
                    Surrogate loss: 0.0172
             Mean action noise std: 0.97
                       Mean reward: 1452.66
               Mean episode length: 221.51
                 Mean success rate: 30.50
                  Mean reward/step: 5.36
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 2.60s
                        Total time: 1562.43s
                               ETA: 8685.6s

################################################################################
                     [1m Learning iteration 610/4000 [0m

                       Computation: 3107 steps/s (collection: 0.529s, learning 2.108s)
               Value function loss: 21041.6857
                    Surrogate loss: 0.0174
             Mean action noise std: 0.97
                       Mean reward: 1635.65
               Mean episode length: 232.56
                 Mean success rate: 33.00
                  Mean reward/step: 5.80
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 5005312
                    Iteration time: 2.64s
                        Total time: 1565.07s
                               ETA: 8683.4s

################################################################################
                     [1m Learning iteration 611/4000 [0m

                       Computation: 3056 steps/s (collection: 0.553s, learning 2.127s)
               Value function loss: 14912.3768
                    Surrogate loss: 0.0134
             Mean action noise std: 0.97
                       Mean reward: 1619.11
               Mean episode length: 231.34
                 Mean success rate: 31.50
                  Mean reward/step: 5.82
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 2.68s
                        Total time: 1567.75s
                               ETA: 8681.5s

################################################################################
                     [1m Learning iteration 612/4000 [0m

                       Computation: 3199 steps/s (collection: 0.470s, learning 2.090s)
               Value function loss: 18302.7943
                    Surrogate loss: 0.0151
             Mean action noise std: 0.97
                       Mean reward: 1629.22
               Mean episode length: 233.03
                 Mean success rate: 30.00
                  Mean reward/step: 6.13
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5021696
                    Iteration time: 2.56s
                        Total time: 1570.31s
                               ETA: 8679.0s

################################################################################
                     [1m Learning iteration 613/4000 [0m

                       Computation: 3159 steps/s (collection: 0.501s, learning 2.092s)
               Value function loss: 13050.7535
                    Surrogate loss: 0.0144
             Mean action noise std: 0.97
                       Mean reward: 1590.23
               Mean episode length: 231.79
                 Mean success rate: 28.50
                  Mean reward/step: 6.03
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 2.59s
                        Total time: 1572.90s
                               ETA: 8676.6s

################################################################################
                     [1m Learning iteration 614/4000 [0m

                       Computation: 3146 steps/s (collection: 0.441s, learning 2.163s)
               Value function loss: 21447.6524
                    Surrogate loss: 0.0133
             Mean action noise std: 0.97
                       Mean reward: 1574.11
               Mean episode length: 228.39
                 Mean success rate: 29.00
                  Mean reward/step: 6.30
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5038080
                    Iteration time: 2.60s
                        Total time: 1575.51s
                               ETA: 8674.3s

################################################################################
                     [1m Learning iteration 615/4000 [0m

                       Computation: 3043 steps/s (collection: 0.542s, learning 2.150s)
               Value function loss: 14368.2132
                    Surrogate loss: 0.0182
             Mean action noise std: 0.97
                       Mean reward: 1384.44
               Mean episode length: 226.01
                 Mean success rate: 24.00
                  Mean reward/step: 6.95
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 2.69s
                        Total time: 1578.20s
                               ETA: 8672.4s

################################################################################
                     [1m Learning iteration 616/4000 [0m

                       Computation: 3245 steps/s (collection: 0.451s, learning 2.073s)
               Value function loss: 17169.6053
                    Surrogate loss: 0.0209
             Mean action noise std: 0.97
                       Mean reward: 1226.57
               Mean episode length: 214.43
                 Mean success rate: 20.50
                  Mean reward/step: 6.72
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5054464
                    Iteration time: 2.52s
                        Total time: 1580.72s
                               ETA: 8669.6s

################################################################################
                     [1m Learning iteration 617/4000 [0m

                       Computation: 3132 steps/s (collection: 0.491s, learning 2.124s)
               Value function loss: 14338.9563
                    Surrogate loss: 0.0214
             Mean action noise std: 0.97
                       Mean reward: 1180.97
               Mean episode length: 214.18
                 Mean success rate: 21.50
                  Mean reward/step: 6.61
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 2.62s
                        Total time: 1583.34s
                               ETA: 8667.4s

################################################################################
                     [1m Learning iteration 618/4000 [0m

                       Computation: 3136 steps/s (collection: 0.507s, learning 2.105s)
               Value function loss: 19857.5671
                    Surrogate loss: 0.0182
             Mean action noise std: 0.97
                       Mean reward: 1215.06
               Mean episode length: 223.00
                 Mean success rate: 23.00
                  Mean reward/step: 6.68
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5070848
                    Iteration time: 2.61s
                        Total time: 1585.95s
                               ETA: 8665.1s

################################################################################
                     [1m Learning iteration 619/4000 [0m

                       Computation: 3031 steps/s (collection: 0.581s, learning 2.121s)
               Value function loss: 24341.5664
                    Surrogate loss: 0.0134
             Mean action noise std: 0.97
                       Mean reward: 1204.89
               Mean episode length: 230.37
                 Mean success rate: 24.00
                  Mean reward/step: 6.71
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 2.70s
                        Total time: 1588.65s
                               ETA: 8663.3s

################################################################################
                     [1m Learning iteration 620/4000 [0m

                       Computation: 3170 steps/s (collection: 0.499s, learning 2.085s)
               Value function loss: 13753.2249
                    Surrogate loss: 0.0163
             Mean action noise std: 0.97
                       Mean reward: 1212.95
               Mean episode length: 234.53
                 Mean success rate: 24.50
                  Mean reward/step: 6.78
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5087232
                    Iteration time: 2.58s
                        Total time: 1591.24s
                               ETA: 8660.8s

################################################################################
                     [1m Learning iteration 621/4000 [0m

                       Computation: 3098 steps/s (collection: 0.518s, learning 2.125s)
               Value function loss: 31700.5010
                    Surrogate loss: 0.0137
             Mean action noise std: 0.97
                       Mean reward: 1416.38
               Mean episode length: 237.75
                 Mean success rate: 27.50
                  Mean reward/step: 6.55
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 2.64s
                        Total time: 1593.88s
                               ETA: 8658.7s

################################################################################
                     [1m Learning iteration 622/4000 [0m

                       Computation: 3133 steps/s (collection: 0.462s, learning 2.152s)
               Value function loss: 14592.9844
                    Surrogate loss: 0.0135
             Mean action noise std: 0.97
                       Mean reward: 1501.34
               Mean episode length: 249.31
                 Mean success rate: 30.00
                  Mean reward/step: 6.71
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 5103616
                    Iteration time: 2.61s
                        Total time: 1596.50s
                               ETA: 8656.4s

################################################################################
                     [1m Learning iteration 623/4000 [0m

                       Computation: 3145 steps/s (collection: 0.483s, learning 2.121s)
               Value function loss: 13941.9673
                    Surrogate loss: 0.0187
             Mean action noise std: 0.97
                       Mean reward: 1530.77
               Mean episode length: 245.13
                 Mean success rate: 29.00
                  Mean reward/step: 7.18
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 2.60s
                        Total time: 1599.10s
                               ETA: 8654.1s

################################################################################
                     [1m Learning iteration 624/4000 [0m

                       Computation: 3124 steps/s (collection: 0.499s, learning 2.123s)
               Value function loss: 25258.5381
                    Surrogate loss: 0.0136
             Mean action noise std: 0.97
                       Mean reward: 1596.98
               Mean episode length: 241.73
                 Mean success rate: 29.00
                  Mean reward/step: 7.35
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5120000
                    Iteration time: 2.62s
                        Total time: 1601.72s
                               ETA: 8651.9s

################################################################################
                     [1m Learning iteration 625/4000 [0m

                       Computation: 3111 steps/s (collection: 0.501s, learning 2.132s)
               Value function loss: 24123.2553
                    Surrogate loss: 0.0131
             Mean action noise std: 0.97
                       Mean reward: 1572.88
               Mean episode length: 240.77
                 Mean success rate: 28.00
                  Mean reward/step: 7.04
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 2.63s
                        Total time: 1604.35s
                               ETA: 8649.7s

################################################################################
                     [1m Learning iteration 626/4000 [0m

                       Computation: 3104 steps/s (collection: 0.555s, learning 2.083s)
               Value function loss: 19146.7811
                    Surrogate loss: 0.0152
             Mean action noise std: 0.97
                       Mean reward: 1728.74
               Mean episode length: 250.20
                 Mean success rate: 29.50
                  Mean reward/step: 7.15
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5136384
                    Iteration time: 2.64s
                        Total time: 1606.99s
                               ETA: 8647.5s

################################################################################
                     [1m Learning iteration 627/4000 [0m

                       Computation: 3172 steps/s (collection: 0.499s, learning 2.083s)
               Value function loss: 22179.6845
                    Surrogate loss: 0.0123
             Mean action noise std: 0.97
                       Mean reward: 1692.95
               Mean episode length: 245.86
                 Mean success rate: 29.50
                  Mean reward/step: 6.98
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 2.58s
                        Total time: 1609.57s
                               ETA: 8645.1s

################################################################################
                     [1m Learning iteration 628/4000 [0m

                       Computation: 3193 steps/s (collection: 0.474s, learning 2.091s)
               Value function loss: 19973.6747
                    Surrogate loss: 0.0133
             Mean action noise std: 0.97
                       Mean reward: 1776.21
               Mean episode length: 238.71
                 Mean success rate: 29.50
                  Mean reward/step: 7.00
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5152768
                    Iteration time: 2.56s
                        Total time: 1612.14s
                               ETA: 8642.5s

################################################################################
                     [1m Learning iteration 629/4000 [0m

                       Computation: 3200 steps/s (collection: 0.479s, learning 2.080s)
               Value function loss: 17462.1648
                    Surrogate loss: 0.0130
             Mean action noise std: 0.97
                       Mean reward: 1741.91
               Mean episode length: 236.20
                 Mean success rate: 27.00
                  Mean reward/step: 7.31
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 2.56s
                        Total time: 1614.70s
                               ETA: 8639.9s

################################################################################
                     [1m Learning iteration 630/4000 [0m

                       Computation: 3185 steps/s (collection: 0.493s, learning 2.078s)
               Value function loss: 22747.5041
                    Surrogate loss: 0.0137
             Mean action noise std: 0.97
                       Mean reward: 1677.94
               Mean episode length: 228.60
                 Mean success rate: 25.50
                  Mean reward/step: 7.48
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5169152
                    Iteration time: 2.57s
                        Total time: 1617.27s
                               ETA: 8637.4s

################################################################################
                     [1m Learning iteration 631/4000 [0m

                       Computation: 3214 steps/s (collection: 0.457s, learning 2.091s)
               Value function loss: 25531.7037
                    Surrogate loss: 0.0129
             Mean action noise std: 0.96
                       Mean reward: 1547.84
               Mean episode length: 232.62
                 Mean success rate: 23.50
                  Mean reward/step: 7.50
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 2.55s
                        Total time: 1619.82s
                               ETA: 8634.8s

################################################################################
                     [1m Learning iteration 632/4000 [0m

                       Computation: 3217 steps/s (collection: 0.479s, learning 2.067s)
               Value function loss: 32358.8457
                    Surrogate loss: 0.0149
             Mean action noise std: 0.96
                       Mean reward: 1663.39
               Mean episode length: 231.87
                 Mean success rate: 24.50
                  Mean reward/step: 7.47
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5185536
                    Iteration time: 2.55s
                        Total time: 1622.36s
                               ETA: 8632.1s

################################################################################
                     [1m Learning iteration 633/4000 [0m

                       Computation: 3239 steps/s (collection: 0.469s, learning 2.060s)
               Value function loss: 20779.6802
                    Surrogate loss: 0.0141
             Mean action noise std: 0.96
                       Mean reward: 1532.26
               Mean episode length: 228.79
                 Mean success rate: 23.50
                  Mean reward/step: 7.48
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 2.53s
                        Total time: 1624.89s
                               ETA: 8629.4s

################################################################################
                     [1m Learning iteration 634/4000 [0m

                       Computation: 3163 steps/s (collection: 0.501s, learning 2.088s)
               Value function loss: 30025.5590
                    Surrogate loss: 0.0126
             Mean action noise std: 0.96
                       Mean reward: 1577.03
               Mean episode length: 239.35
                 Mean success rate: 23.50
                  Mean reward/step: 7.22
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5201920
                    Iteration time: 2.59s
                        Total time: 1627.48s
                               ETA: 8626.9s

################################################################################
                     [1m Learning iteration 635/4000 [0m

                       Computation: 3249 steps/s (collection: 0.457s, learning 2.064s)
               Value function loss: 30908.0624
                    Surrogate loss: 0.0131
             Mean action noise std: 0.96
                       Mean reward: 1778.34
               Mean episode length: 241.54
                 Mean success rate: 28.00
                  Mean reward/step: 6.43
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 2.52s
                        Total time: 1630.00s
                               ETA: 8624.2s

################################################################################
                     [1m Learning iteration 636/4000 [0m

                       Computation: 3213 steps/s (collection: 0.462s, learning 2.087s)
               Value function loss: 24246.2889
                    Surrogate loss: 0.0152
             Mean action noise std: 0.96
                       Mean reward: 1806.96
               Mean episode length: 240.97
                 Mean success rate: 28.50
                  Mean reward/step: 6.41
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5218304
                    Iteration time: 2.55s
                        Total time: 1632.55s
                               ETA: 8621.5s

################################################################################
                     [1m Learning iteration 637/4000 [0m

                       Computation: 3145 steps/s (collection: 0.493s, learning 2.112s)
               Value function loss: 23629.2057
                    Surrogate loss: 0.0137
             Mean action noise std: 0.96
                       Mean reward: 1771.74
               Mean episode length: 232.41
                 Mean success rate: 27.50
                  Mean reward/step: 6.26
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 2.60s
                        Total time: 1635.16s
                               ETA: 8619.2s

################################################################################
                     [1m Learning iteration 638/4000 [0m

                       Computation: 3233 steps/s (collection: 0.468s, learning 2.065s)
               Value function loss: 29454.1809
                    Surrogate loss: 0.0137
             Mean action noise std: 0.96
                       Mean reward: 1726.01
               Mean episode length: 230.42
                 Mean success rate: 26.50
                  Mean reward/step: 6.22
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 5234688
                    Iteration time: 2.53s
                        Total time: 1637.69s
                               ETA: 8616.5s

################################################################################
                     [1m Learning iteration 639/4000 [0m

                       Computation: 3114 steps/s (collection: 0.472s, learning 2.158s)
               Value function loss: 31785.2127
                    Surrogate loss: 0.0137
             Mean action noise std: 0.96
                       Mean reward: 1666.64
               Mean episode length: 231.06
                 Mean success rate: 26.00
                  Mean reward/step: 5.84
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 2.63s
                        Total time: 1640.32s
                               ETA: 8614.2s

################################################################################
                     [1m Learning iteration 640/4000 [0m

                       Computation: 3144 steps/s (collection: 0.478s, learning 2.128s)
               Value function loss: 17638.4806
                    Surrogate loss: 0.0154
             Mean action noise std: 0.96
                       Mean reward: 1561.56
               Mean episode length: 228.22
                 Mean success rate: 24.00
                  Mean reward/step: 6.37
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5251072
                    Iteration time: 2.61s
                        Total time: 1642.93s
                               ETA: 8611.9s

################################################################################
                     [1m Learning iteration 641/4000 [0m

                       Computation: 3136 steps/s (collection: 0.461s, learning 2.151s)
               Value function loss: 12822.8711
                    Surrogate loss: 0.0163
             Mean action noise std: 0.96
                       Mean reward: 1401.83
               Mean episode length: 217.12
                 Mean success rate: 22.00
                  Mean reward/step: 6.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 2.61s
                        Total time: 1645.54s
                               ETA: 8609.6s

################################################################################
                     [1m Learning iteration 642/4000 [0m

                       Computation: 3142 steps/s (collection: 0.513s, learning 2.093s)
               Value function loss: 20377.6875
                    Surrogate loss: 0.0148
             Mean action noise std: 0.96
                       Mean reward: 1463.54
               Mean episode length: 216.23
                 Mean success rate: 23.50
                  Mean reward/step: 7.09
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5267456
                    Iteration time: 2.61s
                        Total time: 1648.14s
                               ETA: 8607.3s

################################################################################
                     [1m Learning iteration 643/4000 [0m

                       Computation: 3200 steps/s (collection: 0.489s, learning 2.071s)
               Value function loss: 25967.2747
                    Surrogate loss: 0.0147
             Mean action noise std: 0.96
                       Mean reward: 1346.46
               Mean episode length: 208.06
                 Mean success rate: 22.50
                  Mean reward/step: 7.28
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 2.56s
                        Total time: 1650.70s
                               ETA: 8604.7s

################################################################################
                     [1m Learning iteration 644/4000 [0m

                       Computation: 3211 steps/s (collection: 0.499s, learning 2.052s)
               Value function loss: 27811.1736
                    Surrogate loss: 0.0137
             Mean action noise std: 0.96
                       Mean reward: 1317.73
               Mean episode length: 204.85
                 Mean success rate: 21.50
                  Mean reward/step: 7.12
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5283840
                    Iteration time: 2.55s
                        Total time: 1653.26s
                               ETA: 8602.1s

################################################################################
                     [1m Learning iteration 645/4000 [0m

                       Computation: 3175 steps/s (collection: 0.507s, learning 2.073s)
               Value function loss: 30002.0014
                    Surrogate loss: 0.0129
             Mean action noise std: 0.96
                       Mean reward: 1452.58
               Mean episode length: 211.50
                 Mean success rate: 24.50
                  Mean reward/step: 6.76
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 2.58s
                        Total time: 1655.84s
                               ETA: 8599.6s

################################################################################
                     [1m Learning iteration 646/4000 [0m

                       Computation: 3203 steps/s (collection: 0.496s, learning 2.061s)
               Value function loss: 18490.8464
                    Surrogate loss: 0.0178
             Mean action noise std: 0.96
                       Mean reward: 1489.72
               Mean episode length: 214.14
                 Mean success rate: 24.00
                  Mean reward/step: 6.22
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5300224
                    Iteration time: 2.56s
                        Total time: 1658.39s
                               ETA: 8597.0s

################################################################################
                     [1m Learning iteration 647/4000 [0m

                       Computation: 3230 steps/s (collection: 0.475s, learning 2.061s)
               Value function loss: 12757.5190
                    Surrogate loss: 0.0221
             Mean action noise std: 0.96
                       Mean reward: 1574.97
               Mean episode length: 221.11
                 Mean success rate: 24.00
                  Mean reward/step: 6.19
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 2.54s
                        Total time: 1660.93s
                               ETA: 8594.3s

################################################################################
                     [1m Learning iteration 648/4000 [0m

                       Computation: 3237 steps/s (collection: 0.478s, learning 2.052s)
               Value function loss: 19844.4327
                    Surrogate loss: 0.0167
             Mean action noise std: 0.96
                       Mean reward: 1465.45
               Mean episode length: 215.24
                 Mean success rate: 22.00
                  Mean reward/step: 6.45
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 5316608
                    Iteration time: 2.53s
                        Total time: 1663.46s
                               ETA: 8591.5s

################################################################################
                     [1m Learning iteration 649/4000 [0m

                       Computation: 3220 steps/s (collection: 0.469s, learning 2.075s)
               Value function loss: 20766.0919
                    Surrogate loss: 0.0156
             Mean action noise std: 0.96
                       Mean reward: 1368.89
               Mean episode length: 213.22
                 Mean success rate: 22.50
                  Mean reward/step: 6.28
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 2.54s
                        Total time: 1666.00s
                               ETA: 8588.9s

################################################################################
                     [1m Learning iteration 650/4000 [0m

                       Computation: 3198 steps/s (collection: 0.503s, learning 2.059s)
               Value function loss: 17171.7624
                    Surrogate loss: 0.0156
             Mean action noise std: 0.96
                       Mean reward: 1299.35
               Mean episode length: 209.58
                 Mean success rate: 21.00
                  Mean reward/step: 5.99
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5332992
                    Iteration time: 2.56s
                        Total time: 1668.56s
                               ETA: 8586.3s

################################################################################
                     [1m Learning iteration 651/4000 [0m

                       Computation: 3235 steps/s (collection: 0.475s, learning 2.057s)
               Value function loss: 14160.8695
                    Surrogate loss: 0.0151
             Mean action noise std: 0.96
                       Mean reward: 1151.50
               Mean episode length: 205.66
                 Mean success rate: 19.50
                  Mean reward/step: 6.35
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 2.53s
                        Total time: 1671.09s
                               ETA: 8583.6s

################################################################################
                     [1m Learning iteration 652/4000 [0m

                       Computation: 3167 steps/s (collection: 0.490s, learning 2.097s)
               Value function loss: 29111.4933
                    Surrogate loss: 0.0164
             Mean action noise std: 0.96
                       Mean reward: 1236.67
               Mean episode length: 202.78
                 Mean success rate: 22.50
                  Mean reward/step: 6.48
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 5349376
                    Iteration time: 2.59s
                        Total time: 1673.68s
                               ETA: 8581.1s

################################################################################
                     [1m Learning iteration 653/4000 [0m

                       Computation: 3098 steps/s (collection: 0.530s, learning 2.114s)
               Value function loss: 23113.3670
                    Surrogate loss: 0.0157
             Mean action noise std: 0.96
                       Mean reward: 1310.28
               Mean episode length: 209.95
                 Mean success rate: 22.00
                  Mean reward/step: 6.50
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 2.64s
                        Total time: 1676.33s
                               ETA: 8579.0s

################################################################################
                     [1m Learning iteration 654/4000 [0m

                       Computation: 3274 steps/s (collection: 0.467s, learning 2.035s)
               Value function loss: 20475.5612
                    Surrogate loss: 0.0147
             Mean action noise std: 0.96
                       Mean reward: 1352.35
               Mean episode length: 220.47
                 Mean success rate: 21.50
                  Mean reward/step: 6.59
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5365760
                    Iteration time: 2.50s
                        Total time: 1678.83s
                               ETA: 8576.1s

################################################################################
                     [1m Learning iteration 655/4000 [0m

                       Computation: 3174 steps/s (collection: 0.496s, learning 2.085s)
               Value function loss: 31060.8246
                    Surrogate loss: 0.0167
             Mean action noise std: 0.96
                       Mean reward: 1484.23
               Mean episode length: 218.41
                 Mean success rate: 22.50
                  Mean reward/step: 6.50
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 2.58s
                        Total time: 1681.41s
                               ETA: 8573.6s

################################################################################
                     [1m Learning iteration 656/4000 [0m

                       Computation: 3253 steps/s (collection: 0.448s, learning 2.070s)
               Value function loss: 11404.6959
                    Surrogate loss: 0.0169
             Mean action noise std: 0.96
                       Mean reward: 1288.86
               Mean episode length: 211.65
                 Mean success rate: 19.00
                  Mean reward/step: 6.52
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5382144
                    Iteration time: 2.52s
                        Total time: 1683.93s
                               ETA: 8570.9s

################################################################################
                     [1m Learning iteration 657/4000 [0m

                       Computation: 3162 steps/s (collection: 0.472s, learning 2.119s)
               Value function loss: 23217.2104
                    Surrogate loss: 0.0163
             Mean action noise std: 0.96
                       Mean reward: 1306.23
               Mean episode length: 209.57
                 Mean success rate: 18.00
                  Mean reward/step: 7.07
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 2.59s
                        Total time: 1686.52s
                               ETA: 8568.4s

################################################################################
                     [1m Learning iteration 658/4000 [0m

                       Computation: 3124 steps/s (collection: 0.486s, learning 2.136s)
               Value function loss: 25997.7041
                    Surrogate loss: 0.0144
             Mean action noise std: 0.96
                       Mean reward: 1307.06
               Mean episode length: 203.67
                 Mean success rate: 18.50
                  Mean reward/step: 7.26
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5398528
                    Iteration time: 2.62s
                        Total time: 1689.14s
                               ETA: 8566.2s

################################################################################
                     [1m Learning iteration 659/4000 [0m

                       Computation: 3177 steps/s (collection: 0.494s, learning 2.084s)
               Value function loss: 20244.8754
                    Surrogate loss: 0.0178
             Mean action noise std: 0.96
                       Mean reward: 1154.00
               Mean episode length: 190.12
                 Mean success rate: 15.50
                  Mean reward/step: 6.93
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 2.58s
                        Total time: 1691.72s
                               ETA: 8563.7s

################################################################################
                     [1m Learning iteration 660/4000 [0m

                       Computation: 3299 steps/s (collection: 0.423s, learning 2.059s)
               Value function loss: 19438.7294
                    Surrogate loss: 0.0160
             Mean action noise std: 0.96
                       Mean reward: 1227.45
               Mean episode length: 197.94
                 Mean success rate: 17.00
                  Mean reward/step: 6.89
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5414912
                    Iteration time: 2.48s
                        Total time: 1694.20s
                               ETA: 8560.7s

################################################################################
                     [1m Learning iteration 661/4000 [0m

                       Computation: 3198 steps/s (collection: 0.497s, learning 2.065s)
               Value function loss: 28540.0367
                    Surrogate loss: 0.0131
             Mean action noise std: 0.96
                       Mean reward: 1250.45
               Mean episode length: 195.34
                 Mean success rate: 17.50
                  Mean reward/step: 7.35
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 2.56s
                        Total time: 1696.76s
                               ETA: 8558.1s

################################################################################
                     [1m Learning iteration 662/4000 [0m

                       Computation: 3249 steps/s (collection: 0.469s, learning 2.052s)
               Value function loss: 31761.4177
                    Surrogate loss: 0.0158
             Mean action noise std: 0.96
                       Mean reward: 1476.80
               Mean episode length: 207.78
                 Mean success rate: 22.00
                  Mean reward/step: 7.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5431296
                    Iteration time: 2.52s
                        Total time: 1699.28s
                               ETA: 8555.4s

################################################################################
                     [1m Learning iteration 663/4000 [0m

                       Computation: 3208 steps/s (collection: 0.500s, learning 2.053s)
               Value function loss: 16254.3748
                    Surrogate loss: 0.0164
             Mean action noise std: 0.96
                       Mean reward: 1372.67
               Mean episode length: 205.06
                 Mean success rate: 22.00
                  Mean reward/step: 7.93
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 2.55s
                        Total time: 1701.84s
                               ETA: 8552.7s

################################################################################
                     [1m Learning iteration 664/4000 [0m

                       Computation: 3187 steps/s (collection: 0.476s, learning 2.095s)
               Value function loss: 26272.3202
                    Surrogate loss: 0.0148
             Mean action noise std: 0.96
                       Mean reward: 1399.09
               Mean episode length: 206.68
                 Mean success rate: 22.50
                  Mean reward/step: 7.89
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5447680
                    Iteration time: 2.57s
                        Total time: 1704.41s
                               ETA: 8550.2s

################################################################################
                     [1m Learning iteration 665/4000 [0m

                       Computation: 3238 steps/s (collection: 0.459s, learning 2.070s)
               Value function loss: 30172.7879
                    Surrogate loss: 0.0157
             Mean action noise std: 0.96
                       Mean reward: 1470.15
               Mean episode length: 205.30
                 Mean success rate: 24.00
                  Mean reward/step: 8.11
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 2.53s
                        Total time: 1706.94s
                               ETA: 8547.5s

################################################################################
                     [1m Learning iteration 666/4000 [0m

                       Computation: 3238 steps/s (collection: 0.453s, learning 2.076s)
               Value function loss: 36862.1071
                    Surrogate loss: 0.0133
             Mean action noise std: 0.96
                       Mean reward: 1589.99
               Mean episode length: 211.75
                 Mean success rate: 25.00
                  Mean reward/step: 7.80
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5464064
                    Iteration time: 2.53s
                        Total time: 1709.46s
                               ETA: 8544.8s

################################################################################
                     [1m Learning iteration 667/4000 [0m

                       Computation: 3200 steps/s (collection: 0.454s, learning 2.106s)
               Value function loss: 41833.7696
                    Surrogate loss: 0.0115
             Mean action noise std: 0.96
                       Mean reward: 1576.80
               Mean episode length: 217.74
                 Mean success rate: 23.00
                  Mean reward/step: 8.40
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 2.56s
                        Total time: 1712.02s
                               ETA: 8542.2s

################################################################################
                     [1m Learning iteration 668/4000 [0m

                       Computation: 3165 steps/s (collection: 0.454s, learning 2.134s)
               Value function loss: 36188.2748
                    Surrogate loss: 0.0133
             Mean action noise std: 0.96
                       Mean reward: 1813.32
               Mean episode length: 231.04
                 Mean success rate: 25.50
                  Mean reward/step: 8.18
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5480448
                    Iteration time: 2.59s
                        Total time: 1714.61s
                               ETA: 8539.7s

################################################################################
                     [1m Learning iteration 669/4000 [0m

                       Computation: 3191 steps/s (collection: 0.465s, learning 2.102s)
               Value function loss: 36679.6404
                    Surrogate loss: 0.0142
             Mean action noise std: 0.96
                       Mean reward: 1962.74
               Mean episode length: 245.01
                 Mean success rate: 27.00
                  Mean reward/step: 7.94
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 2.57s
                        Total time: 1717.18s
                               ETA: 8537.2s

################################################################################
                     [1m Learning iteration 670/4000 [0m

                       Computation: 3206 steps/s (collection: 0.478s, learning 2.077s)
               Value function loss: 16614.0454
                    Surrogate loss: 0.0161
             Mean action noise std: 0.96
                       Mean reward: 1938.62
               Mean episode length: 249.81
                 Mean success rate: 27.00
                  Mean reward/step: 8.18
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5496832
                    Iteration time: 2.56s
                        Total time: 1719.73s
                               ETA: 8534.6s

################################################################################
                     [1m Learning iteration 671/4000 [0m

                       Computation: 3200 steps/s (collection: 0.460s, learning 2.100s)
               Value function loss: 39736.7110
                    Surrogate loss: 0.0141
             Mean action noise std: 0.96
                       Mean reward: 1833.38
               Mean episode length: 241.15
                 Mean success rate: 25.50
                  Mean reward/step: 8.57
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 2.56s
                        Total time: 1722.29s
                               ETA: 8532.0s

################################################################################
                     [1m Learning iteration 672/4000 [0m

                       Computation: 3207 steps/s (collection: 0.483s, learning 2.071s)
               Value function loss: 24442.9518
                    Surrogate loss: 0.0148
             Mean action noise std: 0.96
                       Mean reward: 1820.65
               Mean episode length: 241.51
                 Mean success rate: 27.00
                  Mean reward/step: 9.24
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5513216
                    Iteration time: 2.55s
                        Total time: 1724.85s
                               ETA: 8529.4s

################################################################################
                     [1m Learning iteration 673/4000 [0m

                       Computation: 3163 steps/s (collection: 0.463s, learning 2.127s)
               Value function loss: 36244.2128
                    Surrogate loss: 0.0129
             Mean action noise std: 0.96
                       Mean reward: 1763.28
               Mean episode length: 236.89
                 Mean success rate: 27.50
                  Mean reward/step: 9.24
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 2.59s
                        Total time: 1727.44s
                               ETA: 8527.0s

################################################################################
                     [1m Learning iteration 674/4000 [0m

                       Computation: 3192 steps/s (collection: 0.509s, learning 2.057s)
               Value function loss: 34311.5590
                    Surrogate loss: 0.0164
             Mean action noise std: 0.96
                       Mean reward: 1554.97
               Mean episode length: 216.03
                 Mean success rate: 24.50
                  Mean reward/step: 8.84
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 5529600
                    Iteration time: 2.57s
                        Total time: 1730.00s
                               ETA: 8524.4s

################################################################################
                     [1m Learning iteration 675/4000 [0m

                       Computation: 3253 steps/s (collection: 0.463s, learning 2.055s)
               Value function loss: 32017.3410
                    Surrogate loss: 0.0143
             Mean action noise std: 0.96
                       Mean reward: 1715.36
               Mean episode length: 215.40
                 Mean success rate: 25.50
                  Mean reward/step: 8.59
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 2.52s
                        Total time: 1732.52s
                               ETA: 8521.6s

################################################################################
                     [1m Learning iteration 676/4000 [0m

                       Computation: 3228 steps/s (collection: 0.458s, learning 2.079s)
               Value function loss: 29556.5776
                    Surrogate loss: 0.0150
             Mean action noise std: 0.96
                       Mean reward: 1733.05
               Mean episode length: 212.01
                 Mean success rate: 26.00
                  Mean reward/step: 8.70
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5545984
                    Iteration time: 2.54s
                        Total time: 1735.06s
                               ETA: 8519.0s

################################################################################
                     [1m Learning iteration 677/4000 [0m

                       Computation: 3269 steps/s (collection: 0.473s, learning 2.032s)
               Value function loss: 29763.1070
                    Surrogate loss: 0.0159
             Mean action noise std: 0.96
                       Mean reward: 1826.86
               Mean episode length: 219.22
                 Mean success rate: 27.00
                  Mean reward/step: 8.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 2.51s
                        Total time: 1737.56s
                               ETA: 8516.1s

################################################################################
                     [1m Learning iteration 678/4000 [0m

                       Computation: 3206 steps/s (collection: 0.463s, learning 2.092s)
               Value function loss: 31213.3420
                    Surrogate loss: 0.0175
             Mean action noise std: 0.96
                       Mean reward: 2026.12
               Mean episode length: 229.41
                 Mean success rate: 30.50
                  Mean reward/step: 8.96
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5562368
                    Iteration time: 2.55s
                        Total time: 1740.12s
                               ETA: 8513.5s

################################################################################
                     [1m Learning iteration 679/4000 [0m

                       Computation: 3250 steps/s (collection: 0.458s, learning 2.062s)
               Value function loss: 26696.0774
                    Surrogate loss: 0.0141
             Mean action noise std: 0.96
                       Mean reward: 1936.95
               Mean episode length: 220.44
                 Mean success rate: 30.50
                  Mean reward/step: 8.91
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 2.52s
                        Total time: 1742.64s
                               ETA: 8510.7s

################################################################################
                     [1m Learning iteration 680/4000 [0m

                       Computation: 3328 steps/s (collection: 0.431s, learning 2.030s)
               Value function loss: 37622.9101
                    Surrogate loss: 0.0111
             Mean action noise std: 0.96
                       Mean reward: 2120.38
               Mean episode length: 223.44
                 Mean success rate: 33.00
                  Mean reward/step: 8.76
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5578752
                    Iteration time: 2.46s
                        Total time: 1745.10s
                               ETA: 8507.7s

################################################################################
                     [1m Learning iteration 681/4000 [0m

                       Computation: 3234 steps/s (collection: 0.485s, learning 2.048s)
               Value function loss: 29777.5235
                    Surrogate loss: 0.0193
             Mean action noise std: 0.96
                       Mean reward: 2247.06
               Mean episode length: 238.01
                 Mean success rate: 36.00
                  Mean reward/step: 8.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 2.53s
                        Total time: 1747.63s
                               ETA: 8505.0s

################################################################################
                     [1m Learning iteration 682/4000 [0m

                       Computation: 3215 steps/s (collection: 0.433s, learning 2.115s)
               Value function loss: 33634.0572
                    Surrogate loss: 0.0189
             Mean action noise std: 0.96
                       Mean reward: 2377.04
               Mean episode length: 247.12
                 Mean success rate: 37.00
                  Mean reward/step: 8.11
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5595136
                    Iteration time: 2.55s
                        Total time: 1750.18s
                               ETA: 8502.3s

################################################################################
                     [1m Learning iteration 683/4000 [0m

                       Computation: 3225 steps/s (collection: 0.490s, learning 2.050s)
               Value function loss: 33099.6331
                    Surrogate loss: 0.0162
             Mean action noise std: 0.96
                       Mean reward: 2350.22
               Mean episode length: 240.78
                 Mean success rate: 37.50
                  Mean reward/step: 7.89
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 2.54s
                        Total time: 1752.72s
                               ETA: 8499.7s

################################################################################
                     [1m Learning iteration 684/4000 [0m

                       Computation: 3251 steps/s (collection: 0.469s, learning 2.051s)
               Value function loss: 27262.7068
                    Surrogate loss: 0.0148
             Mean action noise std: 0.96
                       Mean reward: 2598.29
               Mean episode length: 252.60
                 Mean success rate: 39.00
                  Mean reward/step: 7.77
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5611520
                    Iteration time: 2.52s
                        Total time: 1755.24s
                               ETA: 8496.9s

################################################################################
                     [1m Learning iteration 685/4000 [0m

                       Computation: 3251 steps/s (collection: 0.489s, learning 2.031s)
               Value function loss: 17961.3929
                    Surrogate loss: 0.0209
             Mean action noise std: 0.96
                       Mean reward: 2756.80
               Mean episode length: 266.10
                 Mean success rate: 41.00
                  Mean reward/step: 8.14
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 2.52s
                        Total time: 1757.76s
                               ETA: 8494.1s

################################################################################
                     [1m Learning iteration 686/4000 [0m

                       Computation: 3201 steps/s (collection: 0.485s, learning 2.073s)
               Value function loss: 32506.2728
                    Surrogate loss: 0.0131
             Mean action noise std: 0.96
                       Mean reward: 2755.29
               Mean episode length: 278.44
                 Mean success rate: 39.50
                  Mean reward/step: 8.41
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5627904
                    Iteration time: 2.56s
                        Total time: 1760.32s
                               ETA: 8491.6s

################################################################################
                     [1m Learning iteration 687/4000 [0m

                       Computation: 3247 steps/s (collection: 0.497s, learning 2.025s)
               Value function loss: 24760.0364
                    Surrogate loss: 0.0135
             Mean action noise std: 0.95
                       Mean reward: 2535.35
               Mean episode length: 276.57
                 Mean success rate: 36.50
                  Mean reward/step: 8.54
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 2.52s
                        Total time: 1762.84s
                               ETA: 8488.8s

################################################################################
                     [1m Learning iteration 688/4000 [0m

                       Computation: 3198 steps/s (collection: 0.471s, learning 2.090s)
               Value function loss: 36890.1012
                    Surrogate loss: 0.0114
             Mean action noise std: 0.95
                       Mean reward: 2511.88
               Mean episode length: 263.40
                 Mean success rate: 34.50
                  Mean reward/step: 8.60
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5644288
                    Iteration time: 2.56s
                        Total time: 1765.40s
                               ETA: 8486.2s

################################################################################
                     [1m Learning iteration 689/4000 [0m

                       Computation: 3295 steps/s (collection: 0.430s, learning 2.055s)
               Value function loss: 27859.0219
                    Surrogate loss: 0.0168
             Mean action noise std: 0.95
                       Mean reward: 2336.71
               Mean episode length: 264.71
                 Mean success rate: 34.00
                  Mean reward/step: 8.99
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 2.49s
                        Total time: 1767.89s
                               ETA: 8483.3s

################################################################################
                     [1m Learning iteration 690/4000 [0m

                       Computation: 3201 steps/s (collection: 0.496s, learning 2.063s)
               Value function loss: 41023.2993
                    Surrogate loss: 0.0154
             Mean action noise std: 0.95
                       Mean reward: 2335.97
               Mean episode length: 278.71
                 Mean success rate: 32.50
                  Mean reward/step: 8.51
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5660672
                    Iteration time: 2.56s
                        Total time: 1770.45s
                               ETA: 8480.7s

################################################################################
                     [1m Learning iteration 691/4000 [0m

                       Computation: 3238 steps/s (collection: 0.476s, learning 2.054s)
               Value function loss: 34925.8958
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 2211.04
               Mean episode length: 283.18
                 Mean success rate: 31.50
                  Mean reward/step: 8.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 2.53s
                        Total time: 1772.98s
                               ETA: 8478.0s

################################################################################
                     [1m Learning iteration 692/4000 [0m

                       Computation: 3217 steps/s (collection: 0.455s, learning 2.091s)
               Value function loss: 27989.3161
                    Surrogate loss: 0.0153
             Mean action noise std: 0.95
                       Mean reward: 2186.97
               Mean episode length: 283.06
                 Mean success rate: 32.00
                  Mean reward/step: 8.42
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5677056
                    Iteration time: 2.55s
                        Total time: 1775.52s
                               ETA: 8475.4s

################################################################################
                     [1m Learning iteration 693/4000 [0m

                       Computation: 3256 steps/s (collection: 0.426s, learning 2.089s)
               Value function loss: 15330.8181
                    Surrogate loss: 0.0171
             Mean action noise std: 0.95
                       Mean reward: 2296.65
               Mean episode length: 291.64
                 Mean success rate: 33.50
                  Mean reward/step: 8.78
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 2.52s
                        Total time: 1778.04s
                               ETA: 8472.6s

################################################################################
                     [1m Learning iteration 694/4000 [0m

                       Computation: 3251 steps/s (collection: 0.477s, learning 2.043s)
               Value function loss: 38614.7837
                    Surrogate loss: 0.0139
             Mean action noise std: 0.95
                       Mean reward: 2284.72
               Mean episode length: 284.93
                 Mean success rate: 32.50
                  Mean reward/step: 9.35
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5693440
                    Iteration time: 2.52s
                        Total time: 1780.56s
                               ETA: 8469.8s

################################################################################
                     [1m Learning iteration 695/4000 [0m

                       Computation: 3289 steps/s (collection: 0.445s, learning 2.045s)
               Value function loss: 35606.2344
                    Surrogate loss: 0.0136
             Mean action noise std: 0.95
                       Mean reward: 2376.94
               Mean episode length: 298.15
                 Mean success rate: 35.00
                  Mean reward/step: 9.52
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 2.49s
                        Total time: 1783.05s
                               ETA: 8466.9s

################################################################################
                     [1m Learning iteration 696/4000 [0m

                       Computation: 3259 steps/s (collection: 0.453s, learning 2.060s)
               Value function loss: 34099.9280
                    Surrogate loss: 0.0156
             Mean action noise std: 0.95
                       Mean reward: 2352.92
               Mean episode length: 296.94
                 Mean success rate: 35.00
                  Mean reward/step: 9.91
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5709824
                    Iteration time: 2.51s
                        Total time: 1785.56s
                               ETA: 8464.1s

################################################################################
                     [1m Learning iteration 697/4000 [0m

                       Computation: 3280 steps/s (collection: 0.464s, learning 2.033s)
               Value function loss: 33862.9052
                    Surrogate loss: 0.0151
             Mean action noise std: 0.95
                       Mean reward: 2353.93
               Mean episode length: 290.55
                 Mean success rate: 35.50
                  Mean reward/step: 10.79
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 2.50s
                        Total time: 1788.06s
                               ETA: 8461.3s

################################################################################
                     [1m Learning iteration 698/4000 [0m

                       Computation: 3299 steps/s (collection: 0.439s, learning 2.043s)
               Value function loss: 47096.7542
                    Surrogate loss: 0.0153
             Mean action noise std: 0.95
                       Mean reward: 2413.68
               Mean episode length: 285.53
                 Mean success rate: 35.50
                  Mean reward/step: 10.65
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5726208
                    Iteration time: 2.48s
                        Total time: 1790.54s
                               ETA: 8458.3s

################################################################################
                     [1m Learning iteration 699/4000 [0m

                       Computation: 3245 steps/s (collection: 0.469s, learning 2.056s)
               Value function loss: 39124.6226
                    Surrogate loss: 0.0147
             Mean action noise std: 0.95
                       Mean reward: 2479.64
               Mean episode length: 284.36
                 Mean success rate: 37.00
                  Mean reward/step: 10.16
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 2.52s
                        Total time: 1793.07s
                               ETA: 8455.6s

################################################################################
                     [1m Learning iteration 700/4000 [0m

                       Computation: 3328 steps/s (collection: 0.423s, learning 2.039s)
               Value function loss: 36868.2059
                    Surrogate loss: 0.0153
             Mean action noise std: 0.95
                       Mean reward: 2646.36
               Mean episode length: 289.46
                 Mean success rate: 37.50
                  Mean reward/step: 10.19
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5742592
                    Iteration time: 2.46s
                        Total time: 1795.53s
                               ETA: 8452.6s

################################################################################
                     [1m Learning iteration 701/4000 [0m

                       Computation: 3252 steps/s (collection: 0.473s, learning 2.045s)
               Value function loss: 34264.8142
                    Surrogate loss: 0.0168
             Mean action noise std: 0.95
                       Mean reward: 2595.27
               Mean episode length: 289.15
                 Mean success rate: 38.00
                  Mean reward/step: 10.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 2.52s
                        Total time: 1798.05s
                               ETA: 8449.8s

################################################################################
                     [1m Learning iteration 702/4000 [0m

                       Computation: 3255 steps/s (collection: 0.442s, learning 2.074s)
               Value function loss: 44743.2343
                    Surrogate loss: 0.0139
             Mean action noise std: 0.95
                       Mean reward: 2656.21
               Mean episode length: 291.92
                 Mean success rate: 39.00
                  Mean reward/step: 10.63
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5758976
                    Iteration time: 2.52s
                        Total time: 1800.56s
                               ETA: 8447.0s

################################################################################
                     [1m Learning iteration 703/4000 [0m

                       Computation: 3221 steps/s (collection: 0.452s, learning 2.091s)
               Value function loss: 44861.9611
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 2854.62
               Mean episode length: 287.62
                 Mean success rate: 40.50
                  Mean reward/step: 11.42
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 2.54s
                        Total time: 1803.10s
                               ETA: 8444.4s

################################################################################
                     [1m Learning iteration 704/4000 [0m

                       Computation: 3214 steps/s (collection: 0.467s, learning 2.082s)
               Value function loss: 31792.0811
                    Surrogate loss: 0.0128
             Mean action noise std: 0.95
                       Mean reward: 2794.79
               Mean episode length: 276.86
                 Mean success rate: 39.50
                  Mean reward/step: 10.92
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5775360
                    Iteration time: 2.55s
                        Total time: 1805.65s
                               ETA: 8441.8s

################################################################################
                     [1m Learning iteration 705/4000 [0m

                       Computation: 3154 steps/s (collection: 0.510s, learning 2.087s)
               Value function loss: 47562.3931
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 2768.56
               Mean episode length: 275.90
                 Mean success rate: 38.00
                  Mean reward/step: 11.05
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 2.60s
                        Total time: 1808.25s
                               ETA: 8439.4s

################################################################################
                     [1m Learning iteration 706/4000 [0m

                       Computation: 3142 steps/s (collection: 0.500s, learning 2.107s)
               Value function loss: 44041.5458
                    Surrogate loss: 0.0167
             Mean action noise std: 0.95
                       Mean reward: 2750.80
               Mean episode length: 281.50
                 Mean success rate: 38.50
                  Mean reward/step: 10.87
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5791744
                    Iteration time: 2.61s
                        Total time: 1810.86s
                               ETA: 8437.0s

################################################################################
                     [1m Learning iteration 707/4000 [0m

                       Computation: 3200 steps/s (collection: 0.445s, learning 2.115s)
               Value function loss: 42179.4399
                    Surrogate loss: 0.0144
             Mean action noise std: 0.95
                       Mean reward: 2873.67
               Mean episode length: 286.82
                 Mean success rate: 40.00
                  Mean reward/step: 11.27
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 2.56s
                        Total time: 1813.42s
                               ETA: 8434.4s

################################################################################
                     [1m Learning iteration 708/4000 [0m

                       Computation: 3187 steps/s (collection: 0.491s, learning 2.079s)
               Value function loss: 38192.2326
                    Surrogate loss: 0.0154
             Mean action noise std: 0.95
                       Mean reward: 2982.47
               Mean episode length: 280.61
                 Mean success rate: 41.00
                  Mean reward/step: 11.05
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5808128
                    Iteration time: 2.57s
                        Total time: 1815.99s
                               ETA: 8431.9s

################################################################################
                     [1m Learning iteration 709/4000 [0m

                       Computation: 3179 steps/s (collection: 0.450s, learning 2.126s)
               Value function loss: 35749.2539
                    Surrogate loss: 0.0184
             Mean action noise std: 0.95
                       Mean reward: 2814.22
               Mean episode length: 269.09
                 Mean success rate: 38.00
                  Mean reward/step: 11.21
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 2.58s
                        Total time: 1818.56s
                               ETA: 8429.4s

################################################################################
                     [1m Learning iteration 710/4000 [0m

                       Computation: 3200 steps/s (collection: 0.493s, learning 2.067s)
               Value function loss: 57872.2061
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 2830.76
               Mean episode length: 273.44
                 Mean success rate: 37.50
                  Mean reward/step: 10.74
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5824512
                    Iteration time: 2.56s
                        Total time: 1821.12s
                               ETA: 8426.9s

################################################################################
                     [1m Learning iteration 711/4000 [0m

                       Computation: 3215 steps/s (collection: 0.494s, learning 2.054s)
               Value function loss: 34413.5348
                    Surrogate loss: 0.0170
             Mean action noise std: 0.95
                       Mean reward: 2859.08
               Mean episode length: 274.23
                 Mean success rate: 38.50
                  Mean reward/step: 10.78
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 2.55s
                        Total time: 1823.67s
                               ETA: 8424.2s

################################################################################
                     [1m Learning iteration 712/4000 [0m

                       Computation: 3176 steps/s (collection: 0.473s, learning 2.106s)
               Value function loss: 44232.1506
                    Surrogate loss: 0.0169
             Mean action noise std: 0.95
                       Mean reward: 2764.79
               Mean episode length: 268.23
                 Mean success rate: 38.00
                  Mean reward/step: 11.12
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5840896
                    Iteration time: 2.58s
                        Total time: 1826.25s
                               ETA: 8421.8s

################################################################################
                     [1m Learning iteration 713/4000 [0m

                       Computation: 3202 steps/s (collection: 0.500s, learning 2.058s)
               Value function loss: 39993.7368
                    Surrogate loss: 0.0174
             Mean action noise std: 0.95
                       Mean reward: 2819.71
               Mean episode length: 262.44
                 Mean success rate: 37.00
                  Mean reward/step: 10.88
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 2.56s
                        Total time: 1828.81s
                               ETA: 8419.2s

################################################################################
                     [1m Learning iteration 714/4000 [0m

                       Computation: 3194 steps/s (collection: 0.486s, learning 2.078s)
               Value function loss: 63908.5073
                    Surrogate loss: 0.0149
             Mean action noise std: 0.95
                       Mean reward: 2898.95
               Mean episode length: 266.49
                 Mean success rate: 37.00
                  Mean reward/step: 10.40
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 5857280
                    Iteration time: 2.56s
                        Total time: 1831.37s
                               ETA: 8416.6s

################################################################################
                     [1m Learning iteration 715/4000 [0m

                       Computation: 3174 steps/s (collection: 0.462s, learning 2.119s)
               Value function loss: 36936.8207
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 2937.84
               Mean episode length: 276.28
                 Mean success rate: 37.00
                  Mean reward/step: 10.29
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 2.58s
                        Total time: 1833.95s
                               ETA: 8414.2s

################################################################################
                     [1m Learning iteration 716/4000 [0m

                       Computation: 3015 steps/s (collection: 0.534s, learning 2.183s)
               Value function loss: 33176.2075
                    Surrogate loss: 0.0148
             Mean action noise std: 0.95
                       Mean reward: 2844.13
               Mean episode length: 269.48
                 Mean success rate: 36.00
                  Mean reward/step: 9.62
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5873664
                    Iteration time: 2.72s
                        Total time: 1836.67s
                               ETA: 8412.3s

################################################################################
                     [1m Learning iteration 717/4000 [0m

                       Computation: 3016 steps/s (collection: 0.596s, learning 2.120s)
               Value function loss: 43889.0000
                    Surrogate loss: 0.0154
             Mean action noise std: 0.95
                       Mean reward: 2760.75
               Mean episode length: 263.54
                 Mean success rate: 36.50
                  Mean reward/step: 9.88
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 2.72s
                        Total time: 1839.39s
                               ETA: 8410.4s

################################################################################
                     [1m Learning iteration 718/4000 [0m

                       Computation: 3069 steps/s (collection: 0.537s, learning 2.131s)
               Value function loss: 40251.6535
                    Surrogate loss: 0.0136
             Mean action noise std: 0.95
                       Mean reward: 2628.86
               Mean episode length: 253.58
                 Mean success rate: 34.00
                  Mean reward/step: 10.74
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5890048
                    Iteration time: 2.67s
                        Total time: 1842.05s
                               ETA: 8408.4s

################################################################################
                     [1m Learning iteration 719/4000 [0m

                       Computation: 3208 steps/s (collection: 0.466s, learning 2.087s)
               Value function loss: 45535.3682
                    Surrogate loss: 0.0154
             Mean action noise std: 0.95
                       Mean reward: 2801.76
               Mean episode length: 261.61
                 Mean success rate: 36.00
                  Mean reward/step: 11.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 2.55s
                        Total time: 1844.61s
                               ETA: 8405.8s

################################################################################
                     [1m Learning iteration 720/4000 [0m

                       Computation: 3178 steps/s (collection: 0.458s, learning 2.119s)
               Value function loss: 46533.6527
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 2830.44
               Mean episode length: 264.11
                 Mean success rate: 37.50
                  Mean reward/step: 11.80
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5906432
                    Iteration time: 2.58s
                        Total time: 1847.18s
                               ETA: 8403.3s

################################################################################
                     [1m Learning iteration 721/4000 [0m

                       Computation: 3115 steps/s (collection: 0.497s, learning 2.132s)
               Value function loss: 39377.1216
                    Surrogate loss: 0.0156
             Mean action noise std: 0.95
                       Mean reward: 2788.22
               Mean episode length: 261.07
                 Mean success rate: 37.00
                  Mean reward/step: 12.33
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 2.63s
                        Total time: 1849.81s
                               ETA: 8401.0s

################################################################################
                     [1m Learning iteration 722/4000 [0m

                       Computation: 3090 steps/s (collection: 0.528s, learning 2.123s)
               Value function loss: 51443.5872
                    Surrogate loss: 0.0149
             Mean action noise std: 0.95
                       Mean reward: 2645.45
               Mean episode length: 244.79
                 Mean success rate: 35.00
                  Mean reward/step: 12.95
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5922816
                    Iteration time: 2.65s
                        Total time: 1852.47s
                               ETA: 8398.9s

################################################################################
                     [1m Learning iteration 723/4000 [0m

                       Computation: 3232 steps/s (collection: 0.438s, learning 2.097s)
               Value function loss: 46911.2376
                    Surrogate loss: 0.0146
             Mean action noise std: 0.95
                       Mean reward: 2889.36
               Mean episode length: 259.01
                 Mean success rate: 40.50
                  Mean reward/step: 12.81
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 2.53s
                        Total time: 1855.00s
                               ETA: 8396.2s

################################################################################
                     [1m Learning iteration 724/4000 [0m

                       Computation: 3185 steps/s (collection: 0.457s, learning 2.115s)
               Value function loss: 48687.1335
                    Surrogate loss: 0.0189
             Mean action noise std: 0.96
                       Mean reward: 2895.55
               Mean episode length: 255.34
                 Mean success rate: 39.00
                  Mean reward/step: 12.24
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 5939200
                    Iteration time: 2.57s
                        Total time: 1857.57s
                               ETA: 8393.7s

################################################################################
                     [1m Learning iteration 725/4000 [0m

                       Computation: 3094 steps/s (collection: 0.517s, learning 2.130s)
               Value function loss: 40712.4516
                    Surrogate loss: 0.0197
             Mean action noise std: 0.96
                       Mean reward: 3376.11
               Mean episode length: 275.51
                 Mean success rate: 44.50
                  Mean reward/step: 11.81
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 2.65s
                        Total time: 1860.22s
                               ETA: 8391.5s

################################################################################
                     [1m Learning iteration 726/4000 [0m

                       Computation: 3128 steps/s (collection: 0.548s, learning 2.071s)
               Value function loss: 54486.3307
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 3130.89
               Mean episode length: 273.75
                 Mean success rate: 42.00
                  Mean reward/step: 11.87
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5955584
                    Iteration time: 2.62s
                        Total time: 1862.84s
                               ETA: 8389.2s

################################################################################
                     [1m Learning iteration 727/4000 [0m

                       Computation: 3280 steps/s (collection: 0.457s, learning 2.040s)
               Value function loss: 40207.0755
                    Surrogate loss: 0.0178
             Mean action noise std: 0.95
                       Mean reward: 2876.44
               Mean episode length: 258.18
                 Mean success rate: 38.50
                  Mean reward/step: 11.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 2.50s
                        Total time: 1865.33s
                               ETA: 8386.3s

################################################################################
                     [1m Learning iteration 728/4000 [0m

                       Computation: 3230 steps/s (collection: 0.482s, learning 2.054s)
               Value function loss: 48499.0097
                    Surrogate loss: 0.0168
             Mean action noise std: 0.95
                       Mean reward: 3064.09
               Mean episode length: 270.77
                 Mean success rate: 41.50
                  Mean reward/step: 11.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5971968
                    Iteration time: 2.54s
                        Total time: 1867.87s
                               ETA: 8383.6s

################################################################################
                     [1m Learning iteration 729/4000 [0m

                       Computation: 3183 steps/s (collection: 0.522s, learning 2.052s)
               Value function loss: 48299.5199
                    Surrogate loss: 0.0219
             Mean action noise std: 0.95
                       Mean reward: 3001.90
               Mean episode length: 274.96
                 Mean success rate: 41.50
                  Mean reward/step: 11.04
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 2.57s
                        Total time: 1870.44s
                               ETA: 8381.1s

################################################################################
                     [1m Learning iteration 730/4000 [0m

                       Computation: 3225 steps/s (collection: 0.501s, learning 2.038s)
               Value function loss: 58136.9156
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 3281.98
               Mean episode length: 285.45
                 Mean success rate: 41.50
                  Mean reward/step: 10.71
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5988352
                    Iteration time: 2.54s
                        Total time: 1872.98s
                               ETA: 8378.5s

################################################################################
                     [1m Learning iteration 731/4000 [0m

                       Computation: 3259 steps/s (collection: 0.454s, learning 2.060s)
               Value function loss: 45541.6514
                    Surrogate loss: 0.0127
             Mean action noise std: 0.95
                       Mean reward: 3209.80
               Mean episode length: 287.69
                 Mean success rate: 41.50
                  Mean reward/step: 10.67
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 2.51s
                        Total time: 1875.50s
                               ETA: 8375.7s

################################################################################
                     [1m Learning iteration 732/4000 [0m

                       Computation: 3232 steps/s (collection: 0.473s, learning 2.061s)
               Value function loss: 58873.9396
                    Surrogate loss: 0.0151
             Mean action noise std: 0.95
                       Mean reward: 3323.93
               Mean episode length: 287.06
                 Mean success rate: 41.50
                  Mean reward/step: 10.96
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6004736
                    Iteration time: 2.53s
                        Total time: 1878.03s
                               ETA: 8373.0s

################################################################################
                     [1m Learning iteration 733/4000 [0m

                       Computation: 3214 steps/s (collection: 0.511s, learning 2.038s)
               Value function loss: 38688.1019
                    Surrogate loss: 0.0171
             Mean action noise std: 0.95
                       Mean reward: 3279.38
               Mean episode length: 279.15
                 Mean success rate: 40.00
                  Mean reward/step: 10.88
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 2.55s
                        Total time: 1880.58s
                               ETA: 8370.4s

################################################################################
                     [1m Learning iteration 734/4000 [0m

                       Computation: 3163 steps/s (collection: 0.488s, learning 2.102s)
               Value function loss: 37049.6683
                    Surrogate loss: 0.0177
             Mean action noise std: 0.95
                       Mean reward: 3216.30
               Mean episode length: 271.89
                 Mean success rate: 37.00
                  Mean reward/step: 11.17
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6021120
                    Iteration time: 2.59s
                        Total time: 1883.17s
                               ETA: 8367.9s

################################################################################
                     [1m Learning iteration 735/4000 [0m

                       Computation: 3228 steps/s (collection: 0.476s, learning 2.061s)
               Value function loss: 38786.4884
                    Surrogate loss: 0.0165
             Mean action noise std: 0.95
                       Mean reward: 3278.23
               Mean episode length: 272.43
                 Mean success rate: 37.50
                  Mean reward/step: 11.21
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 2.54s
                        Total time: 1885.71s
                               ETA: 8365.3s

################################################################################
                     [1m Learning iteration 736/4000 [0m

                       Computation: 3276 steps/s (collection: 0.465s, learning 2.035s)
               Value function loss: 29506.1098
                    Surrogate loss: 0.0158
             Mean action noise std: 0.95
                       Mean reward: 2783.05
               Mean episode length: 250.09
                 Mean success rate: 32.00
                  Mean reward/step: 11.45
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6037504
                    Iteration time: 2.50s
                        Total time: 1888.21s
                               ETA: 8362.4s

################################################################################
                     [1m Learning iteration 737/4000 [0m

                       Computation: 3209 steps/s (collection: 0.482s, learning 2.071s)
               Value function loss: 53657.9641
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 2764.85
               Mean episode length: 256.23
                 Mean success rate: 32.50
                  Mean reward/step: 11.57
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 2.55s
                        Total time: 1890.76s
                               ETA: 8359.8s

################################################################################
                     [1m Learning iteration 738/4000 [0m

                       Computation: 3249 steps/s (collection: 0.455s, learning 2.066s)
               Value function loss: 45149.5798
                    Surrogate loss: 0.0191
             Mean action noise std: 0.95
                       Mean reward: 2743.65
               Mean episode length: 248.53
                 Mean success rate: 32.00
                  Mean reward/step: 11.07
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6053888
                    Iteration time: 2.52s
                        Total time: 1893.28s
                               ETA: 8357.1s

################################################################################
                     [1m Learning iteration 739/4000 [0m

                       Computation: 3202 steps/s (collection: 0.472s, learning 2.086s)
               Value function loss: 37030.5760
                    Surrogate loss: 0.0161
             Mean action noise std: 0.95
                       Mean reward: 2521.45
               Mean episode length: 244.47
                 Mean success rate: 30.00
                  Mean reward/step: 11.15
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 2.56s
                        Total time: 1895.84s
                               ETA: 8354.5s

################################################################################
                     [1m Learning iteration 740/4000 [0m

                       Computation: 3144 steps/s (collection: 0.487s, learning 2.118s)
               Value function loss: 55546.8651
                    Surrogate loss: 0.0155
             Mean action noise std: 0.95
                       Mean reward: 2635.20
               Mean episode length: 256.43
                 Mean success rate: 32.00
                  Mean reward/step: 11.23
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6070272
                    Iteration time: 2.61s
                        Total time: 1898.44s
                               ETA: 8352.1s

################################################################################
                     [1m Learning iteration 741/4000 [0m

                       Computation: 3047 steps/s (collection: 0.557s, learning 2.131s)
               Value function loss: 36088.2900
                    Surrogate loss: 0.0166
             Mean action noise std: 0.95
                       Mean reward: 2835.67
               Mean episode length: 262.85
                 Mean success rate: 34.50
                  Mean reward/step: 11.43
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 2.69s
                        Total time: 1901.13s
                               ETA: 8350.1s

################################################################################
                     [1m Learning iteration 742/4000 [0m

                       Computation: 3131 steps/s (collection: 0.516s, learning 2.100s)
               Value function loss: 51905.6505
                    Surrogate loss: 0.0185
             Mean action noise std: 0.95
                       Mean reward: 2995.54
               Mean episode length: 269.61
                 Mean success rate: 35.00
                  Mean reward/step: 11.61
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6086656
                    Iteration time: 2.62s
                        Total time: 1903.75s
                               ETA: 8347.8s

################################################################################
                     [1m Learning iteration 743/4000 [0m

                       Computation: 3184 steps/s (collection: 0.497s, learning 2.075s)
               Value function loss: 53076.8150
                    Surrogate loss: 0.0176
             Mean action noise std: 0.95
                       Mean reward: 3002.92
               Mean episode length: 265.15
                 Mean success rate: 36.00
                  Mean reward/step: 11.27
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 2.57s
                        Total time: 1906.32s
                               ETA: 8345.3s

################################################################################
                     [1m Learning iteration 744/4000 [0m

                       Computation: 3150 steps/s (collection: 0.485s, learning 2.115s)
               Value function loss: 34695.7292
                    Surrogate loss: 0.0167
             Mean action noise std: 0.95
                       Mean reward: 3224.01
               Mean episode length: 273.83
                 Mean success rate: 37.50
                  Mean reward/step: 10.96
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6103040
                    Iteration time: 2.60s
                        Total time: 1908.92s
                               ETA: 8342.9s

################################################################################
                     [1m Learning iteration 745/4000 [0m

                       Computation: 3142 steps/s (collection: 0.479s, learning 2.128s)
               Value function loss: 37886.7108
                    Surrogate loss: 0.0172
             Mean action noise std: 0.95
                       Mean reward: 3428.99
               Mean episode length: 283.74
                 Mean success rate: 39.00
                  Mean reward/step: 12.00
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 2.61s
                        Total time: 1911.53s
                               ETA: 8340.5s

################################################################################
                     [1m Learning iteration 746/4000 [0m

                       Computation: 3105 steps/s (collection: 0.510s, learning 2.127s)
               Value function loss: 56749.4853
                    Surrogate loss: 0.0179
             Mean action noise std: 0.95
                       Mean reward: 3429.89
               Mean episode length: 289.26
                 Mean success rate: 39.50
                  Mean reward/step: 12.92
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6119424
                    Iteration time: 2.64s
                        Total time: 1914.17s
                               ETA: 8338.3s

################################################################################
                     [1m Learning iteration 747/4000 [0m

                       Computation: 3114 steps/s (collection: 0.532s, learning 2.098s)
               Value function loss: 51660.7350
                    Surrogate loss: 0.0144
             Mean action noise std: 0.95
                       Mean reward: 3417.17
               Mean episode length: 288.25
                 Mean success rate: 40.00
                  Mean reward/step: 12.86
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 2.63s
                        Total time: 1916.80s
                               ETA: 8336.0s

################################################################################
                     [1m Learning iteration 748/4000 [0m

                       Computation: 3128 steps/s (collection: 0.517s, learning 2.101s)
               Value function loss: 56754.5266
                    Surrogate loss: 0.0159
             Mean action noise std: 0.95
                       Mean reward: 3073.33
               Mean episode length: 278.41
                 Mean success rate: 38.00
                  Mean reward/step: 13.02
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6135808
                    Iteration time: 2.62s
                        Total time: 1919.41s
                               ETA: 8333.7s

################################################################################
                     [1m Learning iteration 749/4000 [0m

                       Computation: 3090 steps/s (collection: 0.521s, learning 2.130s)
               Value function loss: 40598.7052
                    Surrogate loss: 0.0204
             Mean action noise std: 0.95
                       Mean reward: 3168.91
               Mean episode length: 285.87
                 Mean success rate: 39.50
                  Mean reward/step: 13.33
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 2.65s
                        Total time: 1922.07s
                               ETA: 8331.5s

################################################################################
                     [1m Learning iteration 750/4000 [0m

                       Computation: 3006 steps/s (collection: 0.591s, learning 2.134s)
               Value function loss: 54357.1627
                    Surrogate loss: 0.0163
             Mean action noise std: 0.95
                       Mean reward: 3313.51
               Mean episode length: 292.44
                 Mean success rate: 41.00
                  Mean reward/step: 13.52
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6152192
                    Iteration time: 2.73s
                        Total time: 1924.79s
                               ETA: 8329.7s

################################################################################
                     [1m Learning iteration 751/4000 [0m

                       Computation: 3114 steps/s (collection: 0.531s, learning 2.100s)
               Value function loss: 41718.3330
                    Surrogate loss: 0.0141
             Mean action noise std: 0.95
                       Mean reward: 3122.17
               Mean episode length: 292.45
                 Mean success rate: 39.50
                  Mean reward/step: 14.42
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 2.63s
                        Total time: 1927.42s
                               ETA: 8327.4s

################################################################################
                     [1m Learning iteration 752/4000 [0m

                       Computation: 3160 steps/s (collection: 0.484s, learning 2.108s)
               Value function loss: 53297.8866
                    Surrogate loss: 0.0150
             Mean action noise std: 0.95
                       Mean reward: 2944.06
               Mean episode length: 274.45
                 Mean success rate: 37.00
                  Mean reward/step: 14.96
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6168576
                    Iteration time: 2.59s
                        Total time: 1930.01s
                               ETA: 8324.9s

################################################################################
                     [1m Learning iteration 753/4000 [0m

                       Computation: 3168 steps/s (collection: 0.482s, learning 2.103s)
               Value function loss: 60985.8201
                    Surrogate loss: 0.0139
             Mean action noise std: 0.95
                       Mean reward: 3227.77
               Mean episode length: 282.65
                 Mean success rate: 40.50
                  Mean reward/step: 15.09
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 2.59s
                        Total time: 1932.60s
                               ETA: 8322.5s

################################################################################
                     [1m Learning iteration 754/4000 [0m

                       Computation: 3204 steps/s (collection: 0.447s, learning 2.109s)
               Value function loss: 59584.4835
                    Surrogate loss: 0.0157
             Mean action noise std: 0.95
                       Mean reward: 3452.92
               Mean episode length: 285.99
                 Mean success rate: 41.00
                  Mean reward/step: 14.41
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6184960
                    Iteration time: 2.56s
                        Total time: 1935.15s
                               ETA: 8319.9s

################################################################################
                     [1m Learning iteration 755/4000 [0m

                       Computation: 3100 steps/s (collection: 0.531s, learning 2.111s)
               Value function loss: 57674.3062
                    Surrogate loss: 0.0182
             Mean action noise std: 0.95
                       Mean reward: 3585.74
               Mean episode length: 287.42
                 Mean success rate: 41.00
                  Mean reward/step: 13.98
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 2.64s
                        Total time: 1937.80s
                               ETA: 8317.7s

################################################################################
                     [1m Learning iteration 756/4000 [0m

                       Computation: 3119 steps/s (collection: 0.504s, learning 2.122s)
               Value function loss: 71272.8773
                    Surrogate loss: 0.0229
             Mean action noise std: 0.95
                       Mean reward: 3910.54
               Mean episode length: 288.74
                 Mean success rate: 44.00
                  Mean reward/step: 13.70
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6201344
                    Iteration time: 2.63s
                        Total time: 1940.42s
                               ETA: 8315.4s

################################################################################
                     [1m Learning iteration 757/4000 [0m

                       Computation: 3231 steps/s (collection: 0.455s, learning 2.079s)
               Value function loss: 31882.7611
                    Surrogate loss: 0.0169
             Mean action noise std: 0.95
                       Mean reward: 3690.40
               Mean episode length: 266.96
                 Mean success rate: 42.00
                  Mean reward/step: 13.38
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 2.53s
                        Total time: 1942.96s
                               ETA: 8312.7s

################################################################################
                     [1m Learning iteration 758/4000 [0m

                       Computation: 3206 steps/s (collection: 0.477s, learning 2.078s)
               Value function loss: 43125.4533
                    Surrogate loss: 0.0132
             Mean action noise std: 0.95
                       Mean reward: 3561.36
               Mean episode length: 261.47
                 Mean success rate: 39.50
                  Mean reward/step: 12.99
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 6217728
                    Iteration time: 2.55s
                        Total time: 1945.51s
                               ETA: 8310.1s

################################################################################
                     [1m Learning iteration 759/4000 [0m

                       Computation: 3154 steps/s (collection: 0.487s, learning 2.110s)
               Value function loss: 56310.6593
                    Surrogate loss: 0.0153
             Mean action noise std: 0.95
                       Mean reward: 3278.26
               Mean episode length: 244.30
                 Mean success rate: 37.50
                  Mean reward/step: 12.50
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 2.60s
                        Total time: 1948.11s
                               ETA: 8307.7s

################################################################################
                     [1m Learning iteration 760/4000 [0m

                       Computation: 3170 steps/s (collection: 0.467s, learning 2.117s)
               Value function loss: 42986.1754
                    Surrogate loss: 0.0150
             Mean action noise std: 0.95
                       Mean reward: 3199.63
               Mean episode length: 244.70
                 Mean success rate: 37.50
                  Mean reward/step: 12.60
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6234112
                    Iteration time: 2.58s
                        Total time: 1950.69s
                               ETA: 8305.2s

################################################################################
                     [1m Learning iteration 761/4000 [0m

                       Computation: 3178 steps/s (collection: 0.473s, learning 2.104s)
               Value function loss: 53063.8213
                    Surrogate loss: 0.0132
             Mean action noise std: 0.95
                       Mean reward: 3204.47
               Mean episode length: 241.81
                 Mean success rate: 37.50
                  Mean reward/step: 12.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 2.58s
                        Total time: 1953.27s
                               ETA: 8302.7s

################################################################################
                     [1m Learning iteration 762/4000 [0m

                       Computation: 3159 steps/s (collection: 0.477s, learning 2.116s)
               Value function loss: 55857.2417
                    Surrogate loss: 0.0199
             Mean action noise std: 0.95
                       Mean reward: 3115.76
               Mean episode length: 237.69
                 Mean success rate: 36.00
                  Mean reward/step: 12.70
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 6250496
                    Iteration time: 2.59s
                        Total time: 1955.86s
                               ETA: 8300.2s

################################################################################
                     [1m Learning iteration 763/4000 [0m

                       Computation: 3159 steps/s (collection: 0.502s, learning 2.091s)
               Value function loss: 44176.4715
                    Surrogate loss: 0.0112
             Mean action noise std: 0.95
                       Mean reward: 3395.48
               Mean episode length: 248.76
                 Mean success rate: 38.00
                  Mean reward/step: 11.89
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 2.59s
                        Total time: 1958.46s
                               ETA: 8297.8s

################################################################################
                     [1m Learning iteration 764/4000 [0m

                       Computation: 3142 steps/s (collection: 0.487s, learning 2.120s)
               Value function loss: 46811.8884
                    Surrogate loss: 0.0133
             Mean action noise std: 0.95
                       Mean reward: 3693.58
               Mean episode length: 257.01
                 Mean success rate: 41.50
                  Mean reward/step: 11.49
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6266880
                    Iteration time: 2.61s
                        Total time: 1961.06s
                               ETA: 8295.4s

################################################################################
                     [1m Learning iteration 765/4000 [0m

                       Computation: 3169 steps/s (collection: 0.500s, learning 2.085s)
               Value function loss: 43149.1707
                    Surrogate loss: 0.0155
             Mean action noise std: 0.95
                       Mean reward: 3790.64
               Mean episode length: 263.17
                 Mean success rate: 42.00
                  Mean reward/step: 11.33
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 2.58s
                        Total time: 1963.65s
                               ETA: 8292.9s

################################################################################
                     [1m Learning iteration 766/4000 [0m

                       Computation: 3225 steps/s (collection: 0.453s, learning 2.087s)
               Value function loss: 45062.0760
                    Surrogate loss: 0.0130
             Mean action noise std: 0.95
                       Mean reward: 3871.50
               Mean episode length: 264.94
                 Mean success rate: 42.50
                  Mean reward/step: 11.61
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6283264
                    Iteration time: 2.54s
                        Total time: 1966.19s
                               ETA: 8290.3s

################################################################################
                     [1m Learning iteration 767/4000 [0m

                       Computation: 3139 steps/s (collection: 0.467s, learning 2.142s)
               Value function loss: 41121.4530
                    Surrogate loss: 0.0166
             Mean action noise std: 0.95
                       Mean reward: 3750.82
               Mean episode length: 263.85
                 Mean success rate: 41.50
                  Mean reward/step: 11.76
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 2.61s
                        Total time: 1968.80s
                               ETA: 8287.9s

################################################################################
                     [1m Learning iteration 768/4000 [0m

                       Computation: 3191 steps/s (collection: 0.495s, learning 2.072s)
               Value function loss: 49202.8250
                    Surrogate loss: 0.0152
             Mean action noise std: 0.95
                       Mean reward: 3478.51
               Mean episode length: 258.58
                 Mean success rate: 38.50
                  Mean reward/step: 11.80
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 6299648
                    Iteration time: 2.57s
                        Total time: 1971.36s
                               ETA: 8285.4s

################################################################################
                     [1m Learning iteration 769/4000 [0m

                       Computation: 3175 steps/s (collection: 0.457s, learning 2.122s)
               Value function loss: 53810.8456
                    Surrogate loss: 0.0169
             Mean action noise std: 0.95
                       Mean reward: 3260.98
               Mean episode length: 249.75
                 Mean success rate: 34.00
                  Mean reward/step: 11.21
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 2.58s
                        Total time: 1973.94s
                               ETA: 8282.9s

################################################################################
                     [1m Learning iteration 770/4000 [0m

                       Computation: 3113 steps/s (collection: 0.491s, learning 2.140s)
               Value function loss: 39792.9301
                    Surrogate loss: 0.0167
             Mean action noise std: 0.95
                       Mean reward: 3079.46
               Mean episode length: 247.12
                 Mean success rate: 32.50
                  Mean reward/step: 10.48
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6316032
                    Iteration time: 2.63s
                        Total time: 1976.57s
                               ETA: 8280.6s

################################################################################
                     [1m Learning iteration 771/4000 [0m

                       Computation: 3181 steps/s (collection: 0.472s, learning 2.103s)
               Value function loss: 38076.6555
                    Surrogate loss: 0.0154
             Mean action noise std: 0.95
                       Mean reward: 2929.10
               Mean episode length: 249.18
                 Mean success rate: 31.50
                  Mean reward/step: 10.29
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 2.58s
                        Total time: 1979.15s
                               ETA: 8278.1s

################################################################################
                     [1m Learning iteration 772/4000 [0m

                       Computation: 3128 steps/s (collection: 0.509s, learning 2.109s)
               Value function loss: 35022.9828
                    Surrogate loss: 0.0141
             Mean action noise std: 0.95
                       Mean reward: 2691.18
               Mean episode length: 238.85
                 Mean success rate: 29.00
                  Mean reward/step: 10.48
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6332416
                    Iteration time: 2.62s
                        Total time: 1981.77s
                               ETA: 8275.7s

################################################################################
                     [1m Learning iteration 773/4000 [0m

                       Computation: 3177 steps/s (collection: 0.502s, learning 2.075s)
               Value function loss: 37070.8397
                    Surrogate loss: 0.0152
             Mean action noise std: 0.95
                       Mean reward: 2611.22
               Mean episode length: 236.16
                 Mean success rate: 28.50
                  Mean reward/step: 10.74
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 2.58s
                        Total time: 1984.35s
                               ETA: 8273.2s

################################################################################
                     [1m Learning iteration 774/4000 [0m

                       Computation: 3279 steps/s (collection: 0.469s, learning 2.029s)
               Value function loss: 70217.0927
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 2915.46
               Mean episode length: 246.19
                 Mean success rate: 33.50
                  Mean reward/step: 10.66
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6348800
                    Iteration time: 2.50s
                        Total time: 1986.84s
                               ETA: 8270.4s

################################################################################
                     [1m Learning iteration 775/4000 [0m

                       Computation: 3191 steps/s (collection: 0.468s, learning 2.099s)
               Value function loss: 34056.1321
                    Surrogate loss: 0.0165
             Mean action noise std: 0.95
                       Mean reward: 2396.99
               Mean episode length: 228.54
                 Mean success rate: 27.50
                  Mean reward/step: 10.15
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 2.57s
                        Total time: 1989.41s
                               ETA: 8267.8s

################################################################################
                     [1m Learning iteration 776/4000 [0m

                       Computation: 3209 steps/s (collection: 0.485s, learning 2.068s)
               Value function loss: 45008.2032
                    Surrogate loss: 0.0184
             Mean action noise std: 0.95
                       Mean reward: 2543.41
               Mean episode length: 232.32
                 Mean success rate: 28.50
                  Mean reward/step: 9.52
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 6365184
                    Iteration time: 2.55s
                        Total time: 1991.96s
                               ETA: 8265.2s

################################################################################
                     [1m Learning iteration 777/4000 [0m

                       Computation: 3157 steps/s (collection: 0.485s, learning 2.109s)
               Value function loss: 36521.9204
                    Surrogate loss: 0.0146
             Mean action noise std: 0.95
                       Mean reward: 2490.35
               Mean episode length: 234.50
                 Mean success rate: 29.00
                  Mean reward/step: 10.11
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 2.59s
                        Total time: 1994.56s
                               ETA: 8262.8s

################################################################################
                     [1m Learning iteration 778/4000 [0m

                       Computation: 3213 steps/s (collection: 0.467s, learning 2.082s)
               Value function loss: 48921.6452
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 2672.99
               Mean episode length: 246.69
                 Mean success rate: 32.50
                  Mean reward/step: 10.16
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6381568
                    Iteration time: 2.55s
                        Total time: 1997.11s
                               ETA: 8260.2s

################################################################################
                     [1m Learning iteration 779/4000 [0m

                       Computation: 3224 steps/s (collection: 0.469s, learning 2.071s)
               Value function loss: 42843.4333
                    Surrogate loss: 0.0128
             Mean action noise std: 0.95
                       Mean reward: 2758.69
               Mean episode length: 253.40
                 Mean success rate: 32.50
                  Mean reward/step: 10.10
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 2.54s
                        Total time: 1999.65s
                               ETA: 8257.5s

################################################################################
                     [1m Learning iteration 780/4000 [0m

                       Computation: 3172 steps/s (collection: 0.485s, learning 2.097s)
               Value function loss: 30399.3256
                    Surrogate loss: 0.0166
             Mean action noise std: 0.95
                       Mean reward: 2501.01
               Mean episode length: 235.59
                 Mean success rate: 29.00
                  Mean reward/step: 9.78
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6397952
                    Iteration time: 2.58s
                        Total time: 2002.23s
                               ETA: 8255.0s

################################################################################
                     [1m Learning iteration 781/4000 [0m

                       Computation: 3128 steps/s (collection: 0.503s, learning 2.115s)
               Value function loss: 37274.2642
                    Surrogate loss: 0.0118
             Mean action noise std: 0.95
                       Mean reward: 2692.30
               Mean episode length: 241.98
                 Mean success rate: 31.00
                  Mean reward/step: 10.09
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 2.62s
                        Total time: 2004.85s
                               ETA: 8252.7s

################################################################################
                     [1m Learning iteration 782/4000 [0m

                       Computation: 3187 steps/s (collection: 0.480s, learning 2.089s)
               Value function loss: 41558.3281
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 2555.72
               Mean episode length: 234.04
                 Mean success rate: 30.00
                  Mean reward/step: 10.00
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6414336
                    Iteration time: 2.57s
                        Total time: 2007.42s
                               ETA: 8250.1s

################################################################################
                     [1m Learning iteration 783/4000 [0m

                       Computation: 3204 steps/s (collection: 0.451s, learning 2.105s)
               Value function loss: 31918.6702
                    Surrogate loss: 0.0155
             Mean action noise std: 0.95
                       Mean reward: 2559.12
               Mean episode length: 231.70
                 Mean success rate: 29.50
                  Mean reward/step: 10.58
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 2.56s
                        Total time: 2009.97s
                               ETA: 8247.6s

################################################################################
                     [1m Learning iteration 784/4000 [0m

                       Computation: 3192 steps/s (collection: 0.464s, learning 2.102s)
               Value function loss: 36637.0677
                    Surrogate loss: 0.0159
             Mean action noise std: 0.95
                       Mean reward: 2426.52
               Mean episode length: 228.24
                 Mean success rate: 27.00
                  Mean reward/step: 11.02
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6430720
                    Iteration time: 2.57s
                        Total time: 2012.54s
                               ETA: 8245.0s

################################################################################
                     [1m Learning iteration 785/4000 [0m

                       Computation: 3175 steps/s (collection: 0.483s, learning 2.096s)
               Value function loss: 40461.6254
                    Surrogate loss: 0.0123
             Mean action noise std: 0.95
                       Mean reward: 2379.88
               Mean episode length: 224.12
                 Mean success rate: 27.50
                  Mean reward/step: 11.11
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 2.58s
                        Total time: 2015.12s
                               ETA: 8242.5s

################################################################################
                     [1m Learning iteration 786/4000 [0m

                       Computation: 3176 steps/s (collection: 0.481s, learning 2.098s)
               Value function loss: 42192.0679
                    Surrogate loss: 0.0131
             Mean action noise std: 0.95
                       Mean reward: 2269.62
               Mean episode length: 224.06
                 Mean success rate: 27.00
                  Mean reward/step: 11.18
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6447104
                    Iteration time: 2.58s
                        Total time: 2017.70s
                               ETA: 8240.0s

################################################################################
                     [1m Learning iteration 787/4000 [0m

                       Computation: 3194 steps/s (collection: 0.467s, learning 2.097s)
               Value function loss: 39008.2550
                    Surrogate loss: 0.0162
             Mean action noise std: 0.95
                       Mean reward: 2306.12
               Mean episode length: 228.80
                 Mean success rate: 28.00
                  Mean reward/step: 11.33
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 2.56s
                        Total time: 2020.26s
                               ETA: 8237.4s

################################################################################
                     [1m Learning iteration 788/4000 [0m

                       Computation: 3210 steps/s (collection: 0.485s, learning 2.067s)
               Value function loss: 41984.5437
                    Surrogate loss: 0.0162
             Mean action noise std: 0.95
                       Mean reward: 2358.34
               Mean episode length: 229.76
                 Mean success rate: 28.00
                  Mean reward/step: 11.16
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6463488
                    Iteration time: 2.55s
                        Total time: 2022.81s
                               ETA: 8234.8s

################################################################################
                     [1m Learning iteration 789/4000 [0m

                       Computation: 3164 steps/s (collection: 0.478s, learning 2.110s)
               Value function loss: 43402.3127
                    Surrogate loss: 0.0156
             Mean action noise std: 0.95
                       Mean reward: 2565.77
               Mean episode length: 239.94
                 Mean success rate: 29.50
                  Mean reward/step: 11.21
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 2.59s
                        Total time: 2025.40s
                               ETA: 8232.4s

################################################################################
                     [1m Learning iteration 790/4000 [0m

                       Computation: 3180 steps/s (collection: 0.492s, learning 2.084s)
               Value function loss: 51262.0156
                    Surrogate loss: 0.0134
             Mean action noise std: 0.95
                       Mean reward: 2613.81
               Mean episode length: 246.52
                 Mean success rate: 29.50
                  Mean reward/step: 11.00
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6479872
                    Iteration time: 2.58s
                        Total time: 2027.98s
                               ETA: 8229.8s

################################################################################
                     [1m Learning iteration 791/4000 [0m

                       Computation: 3150 steps/s (collection: 0.497s, learning 2.103s)
               Value function loss: 59017.6948
                    Surrogate loss: 0.0136
             Mean action noise std: 0.95
                       Mean reward: 2954.01
               Mean episode length: 263.98
                 Mean success rate: 33.50
                  Mean reward/step: 10.73
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 2.60s
                        Total time: 2030.58s
                               ETA: 8227.4s

################################################################################
                     [1m Learning iteration 792/4000 [0m

                       Computation: 3163 steps/s (collection: 0.494s, learning 2.095s)
               Value function loss: 34129.0696
                    Surrogate loss: 0.0190
             Mean action noise std: 0.95
                       Mean reward: 2771.71
               Mean episode length: 257.00
                 Mean success rate: 33.00
                  Mean reward/step: 10.13
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6496256
                    Iteration time: 2.59s
                        Total time: 2033.17s
                               ETA: 8225.0s

################################################################################
                     [1m Learning iteration 793/4000 [0m

                       Computation: 3210 steps/s (collection: 0.504s, learning 2.048s)
               Value function loss: 33708.0382
                    Surrogate loss: 0.0209
             Mean action noise std: 0.95
                       Mean reward: 2747.88
               Mean episode length: 257.62
                 Mean success rate: 33.00
                  Mean reward/step: 10.61
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 2.55s
                        Total time: 2035.72s
                               ETA: 8222.4s

################################################################################
                     [1m Learning iteration 794/4000 [0m

                       Computation: 3257 steps/s (collection: 0.496s, learning 2.019s)
               Value function loss: 48274.5032
                    Surrogate loss: 0.0114
             Mean action noise std: 0.95
                       Mean reward: 2832.98
               Mean episode length: 269.17
                 Mean success rate: 34.50
                  Mean reward/step: 9.99
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6512640
                    Iteration time: 2.52s
                        Total time: 2038.23s
                               ETA: 8219.6s

################################################################################
                     [1m Learning iteration 795/4000 [0m

                       Computation: 3239 steps/s (collection: 0.459s, learning 2.069s)
               Value function loss: 42301.0475
                    Surrogate loss: 0.0117
             Mean action noise std: 0.95
                       Mean reward: 2877.69
               Mean episode length: 267.83
                 Mean success rate: 34.50
                  Mean reward/step: 9.49
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 2.53s
                        Total time: 2040.76s
                               ETA: 8216.9s

################################################################################
                     [1m Learning iteration 796/4000 [0m

                       Computation: 3228 steps/s (collection: 0.464s, learning 2.073s)
               Value function loss: 33563.4393
                    Surrogate loss: 0.0103
             Mean action noise std: 0.95
                       Mean reward: 2861.16
               Mean episode length: 266.90
                 Mean success rate: 33.50
                  Mean reward/step: 10.05
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6529024
                    Iteration time: 2.54s
                        Total time: 2043.30s
                               ETA: 8214.2s

################################################################################
                     [1m Learning iteration 797/4000 [0m

                       Computation: 3261 steps/s (collection: 0.445s, learning 2.067s)
               Value function loss: 39888.6193
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 2988.74
               Mean episode length: 272.73
                 Mean success rate: 35.00
                  Mean reward/step: 10.97
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 2.51s
                        Total time: 2045.81s
                               ETA: 8211.4s

################################################################################
                     [1m Learning iteration 798/4000 [0m

                       Computation: 3225 steps/s (collection: 0.472s, learning 2.068s)
               Value function loss: 43957.0644
                    Surrogate loss: 0.0132
             Mean action noise std: 0.95
                       Mean reward: 3137.13
               Mean episode length: 279.73
                 Mean success rate: 36.00
                  Mean reward/step: 11.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6545408
                    Iteration time: 2.54s
                        Total time: 2048.35s
                               ETA: 8208.8s

################################################################################
                     [1m Learning iteration 799/4000 [0m

                       Computation: 3307 steps/s (collection: 0.423s, learning 2.054s)
               Value function loss: 32256.3936
                    Surrogate loss: 0.0146
             Mean action noise std: 0.95
                       Mean reward: 3029.62
               Mean episode length: 269.86
                 Mean success rate: 34.50
                  Mean reward/step: 12.28
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 2.48s
                        Total time: 2050.83s
                               ETA: 8205.9s

################################################################################
                     [1m Learning iteration 800/4000 [0m

                       Computation: 3170 steps/s (collection: 0.469s, learning 2.115s)
               Value function loss: 51193.9823
                    Surrogate loss: 0.0116
             Mean action noise std: 0.95
                       Mean reward: 3188.36
               Mean episode length: 277.43
                 Mean success rate: 36.00
                  Mean reward/step: 13.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6561792
                    Iteration time: 2.58s
                        Total time: 2053.41s
                               ETA: 8203.4s

################################################################################
                     [1m Learning iteration 801/4000 [0m

                       Computation: 3219 steps/s (collection: 0.459s, learning 2.086s)
               Value function loss: 42940.0541
                    Surrogate loss: 0.0107
             Mean action noise std: 0.95
                       Mean reward: 3447.00
               Mean episode length: 290.17
                 Mean success rate: 39.50
                  Mean reward/step: 12.47
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 2.54s
                        Total time: 2055.96s
                               ETA: 8200.8s

################################################################################
                     [1m Learning iteration 802/4000 [0m

                       Computation: 3156 steps/s (collection: 0.518s, learning 2.077s)
               Value function loss: 56190.1366
                    Surrogate loss: 0.0104
             Mean action noise std: 0.95
                       Mean reward: 3499.66
               Mean episode length: 294.14
                 Mean success rate: 40.00
                  Mean reward/step: 11.67
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6578176
                    Iteration time: 2.60s
                        Total time: 2058.55s
                               ETA: 8198.3s

################################################################################
                     [1m Learning iteration 803/4000 [0m

                       Computation: 3163 steps/s (collection: 0.478s, learning 2.112s)
               Value function loss: 52115.9902
                    Surrogate loss: 0.0104
             Mean action noise std: 0.95
                       Mean reward: 3588.48
               Mean episode length: 299.68
                 Mean success rate: 42.50
                  Mean reward/step: 11.03
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 2.59s
                        Total time: 2061.14s
                               ETA: 8195.9s

################################################################################
                     [1m Learning iteration 804/4000 [0m

                       Computation: 3150 steps/s (collection: 0.525s, learning 2.075s)
               Value function loss: 41414.0214
                    Surrogate loss: 0.0175
             Mean action noise std: 0.95
                       Mean reward: 3622.82
               Mean episode length: 310.18
                 Mean success rate: 43.50
                  Mean reward/step: 10.96
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6594560
                    Iteration time: 2.60s
                        Total time: 2063.74s
                               ETA: 8193.4s

################################################################################
                     [1m Learning iteration 805/4000 [0m

                       Computation: 3153 steps/s (collection: 0.544s, learning 2.054s)
               Value function loss: 57118.9379
                    Surrogate loss: 0.0241
             Mean action noise std: 0.95
                       Mean reward: 3766.94
               Mean episode length: 315.16
                 Mean success rate: 46.50
                  Mean reward/step: 10.89
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 2.60s
                        Total time: 2066.34s
                               ETA: 8191.0s

################################################################################
                     [1m Learning iteration 806/4000 [0m

                       Computation: 3218 steps/s (collection: 0.494s, learning 2.051s)
               Value function loss: 49899.3735
                    Surrogate loss: 0.0128
             Mean action noise std: 0.95
                       Mean reward: 3678.36
               Mean episode length: 320.08
                 Mean success rate: 46.50
                  Mean reward/step: 10.53
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6610944
                    Iteration time: 2.55s
                        Total time: 2068.88s
                               ETA: 8188.4s

################################################################################
                     [1m Learning iteration 807/4000 [0m

                       Computation: 3172 steps/s (collection: 0.510s, learning 2.072s)
               Value function loss: 50333.5128
                    Surrogate loss: 0.0109
             Mean action noise std: 0.95
                       Mean reward: 3395.58
               Mean episode length: 318.24
                 Mean success rate: 45.00
                  Mean reward/step: 10.00
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 2.58s
                        Total time: 2071.47s
                               ETA: 8185.9s

################################################################################
                     [1m Learning iteration 808/4000 [0m

                       Computation: 3198 steps/s (collection: 0.493s, learning 2.069s)
               Value function loss: 41367.3347
                    Surrogate loss: 0.0119
             Mean action noise std: 0.95
                       Mean reward: 3244.17
               Mean episode length: 310.40
                 Mean success rate: 42.50
                  Mean reward/step: 9.76
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6627328
                    Iteration time: 2.56s
                        Total time: 2074.03s
                               ETA: 8183.3s

################################################################################
                     [1m Learning iteration 809/4000 [0m

                       Computation: 3252 steps/s (collection: 0.454s, learning 2.065s)
               Value function loss: 49364.0793
                    Surrogate loss: 0.0130
             Mean action noise std: 0.95
                       Mean reward: 3287.10
               Mean episode length: 311.20
                 Mean success rate: 42.50
                  Mean reward/step: 9.55
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 2.52s
                        Total time: 2076.55s
                               ETA: 8180.6s

################################################################################
                     [1m Learning iteration 810/4000 [0m

                       Computation: 3197 steps/s (collection: 0.463s, learning 2.099s)
               Value function loss: 39080.7250
                    Surrogate loss: 0.0154
             Mean action noise std: 0.95
                       Mean reward: 3340.38
               Mean episode length: 311.04
                 Mean success rate: 43.00
                  Mean reward/step: 9.14
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6643712
                    Iteration time: 2.56s
                        Total time: 2079.11s
                               ETA: 8178.0s

################################################################################
                     [1m Learning iteration 811/4000 [0m

                       Computation: 3114 steps/s (collection: 0.508s, learning 2.122s)
               Value function loss: 43196.0450
                    Surrogate loss: 0.0112
             Mean action noise std: 0.95
                       Mean reward: 2986.94
               Mean episode length: 291.69
                 Mean success rate: 37.50
                  Mean reward/step: 9.13
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 2.63s
                        Total time: 2081.74s
                               ETA: 8175.7s

################################################################################
                     [1m Learning iteration 812/4000 [0m

                       Computation: 3207 steps/s (collection: 0.502s, learning 2.052s)
               Value function loss: 33038.7913
                    Surrogate loss: 0.0111
             Mean action noise std: 0.95
                       Mean reward: 2743.45
               Mean episode length: 270.60
                 Mean success rate: 34.00
                  Mean reward/step: 9.47
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6660096
                    Iteration time: 2.55s
                        Total time: 2084.29s
                               ETA: 8173.1s

################################################################################
                     [1m Learning iteration 813/4000 [0m

                       Computation: 3224 steps/s (collection: 0.477s, learning 2.063s)
               Value function loss: 35249.8036
                    Surrogate loss: 0.0127
             Mean action noise std: 0.95
                       Mean reward: 2655.54
               Mean episode length: 254.31
                 Mean success rate: 31.50
                  Mean reward/step: 10.44
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 2.54s
                        Total time: 2086.83s
                               ETA: 8170.4s

################################################################################
                     [1m Learning iteration 814/4000 [0m

                       Computation: 3240 steps/s (collection: 0.478s, learning 2.050s)
               Value function loss: 33125.7977
                    Surrogate loss: 0.0111
             Mean action noise std: 0.95
                       Mean reward: 2671.76
               Mean episode length: 258.58
                 Mean success rate: 33.00
                  Mean reward/step: 11.97
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6676480
                    Iteration time: 2.53s
                        Total time: 2089.36s
                               ETA: 8167.7s

################################################################################
                     [1m Learning iteration 815/4000 [0m

                       Computation: 3253 steps/s (collection: 0.464s, learning 2.054s)
               Value function loss: 44642.5957
                    Surrogate loss: 0.0112
             Mean action noise std: 0.95
                       Mean reward: 2697.76
               Mean episode length: 252.78
                 Mean success rate: 33.50
                  Mean reward/step: 12.23
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 2.52s
                        Total time: 2091.88s
                               ETA: 8165.0s

################################################################################
                     [1m Learning iteration 816/4000 [0m

                       Computation: 3121 steps/s (collection: 0.526s, learning 2.098s)
               Value function loss: 32431.2323
                    Surrogate loss: 0.0155
             Mean action noise std: 0.95
                       Mean reward: 2289.68
               Mean episode length: 234.72
                 Mean success rate: 28.50
                  Mean reward/step: 11.94
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6692864
                    Iteration time: 2.62s
                        Total time: 2094.50s
                               ETA: 8162.7s

################################################################################
                     [1m Learning iteration 817/4000 [0m

                       Computation: 3173 steps/s (collection: 0.518s, learning 2.064s)
               Value function loss: 37330.0845
                    Surrogate loss: 0.0136
             Mean action noise std: 0.95
                       Mean reward: 2358.09
               Mean episode length: 231.96
                 Mean success rate: 29.50
                  Mean reward/step: 12.32
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 2.58s
                        Total time: 2097.09s
                               ETA: 8160.2s

################################################################################
                     [1m Learning iteration 818/4000 [0m

                       Computation: 3301 steps/s (collection: 0.434s, learning 2.047s)
               Value function loss: 39116.8569
                    Surrogate loss: 0.0129
             Mean action noise std: 0.95
                       Mean reward: 2424.07
               Mean episode length: 230.34
                 Mean success rate: 30.00
                  Mean reward/step: 12.30
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6709248
                    Iteration time: 2.48s
                        Total time: 2099.57s
                               ETA: 8157.3s

################################################################################
                     [1m Learning iteration 819/4000 [0m

                       Computation: 3270 steps/s (collection: 0.433s, learning 2.072s)
               Value function loss: 47857.7221
                    Surrogate loss: 0.0124
             Mean action noise std: 0.95
                       Mean reward: 2621.84
               Mean episode length: 249.12
                 Mean success rate: 32.50
                  Mean reward/step: 12.26
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 2.50s
                        Total time: 2102.07s
                               ETA: 8154.5s

################################################################################
                     [1m Learning iteration 820/4000 [0m

                       Computation: 3258 steps/s (collection: 0.446s, learning 2.068s)
               Value function loss: 54234.1296
                    Surrogate loss: 0.0101
             Mean action noise std: 0.95
                       Mean reward: 2742.34
               Mean episode length: 264.12
                 Mean success rate: 36.00
                  Mean reward/step: 12.18
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6725632
                    Iteration time: 2.51s
                        Total time: 2104.59s
                               ETA: 8151.7s

################################################################################
                     [1m Learning iteration 821/4000 [0m

                       Computation: 3187 steps/s (collection: 0.495s, learning 2.075s)
               Value function loss: 61856.8599
                    Surrogate loss: 0.0130
             Mean action noise std: 0.95
                       Mean reward: 2985.89
               Mean episode length: 278.63
                 Mean success rate: 38.50
                  Mean reward/step: 12.09
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 2.57s
                        Total time: 2107.16s
                               ETA: 8149.2s

################################################################################
                     [1m Learning iteration 822/4000 [0m

                       Computation: 3216 steps/s (collection: 0.475s, learning 2.072s)
               Value function loss: 64407.9061
                    Surrogate loss: 0.0121
             Mean action noise std: 0.95
                       Mean reward: 3259.66
               Mean episode length: 293.96
                 Mean success rate: 41.00
                  Mean reward/step: 11.67
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6742016
                    Iteration time: 2.55s
                        Total time: 2109.70s
                               ETA: 8146.6s

################################################################################
                     [1m Learning iteration 823/4000 [0m

                       Computation: 3214 steps/s (collection: 0.493s, learning 2.055s)
               Value function loss: 48961.2193
                    Surrogate loss: 0.0181
             Mean action noise std: 0.95
                       Mean reward: 3399.48
               Mean episode length: 299.57
                 Mean success rate: 44.00
                  Mean reward/step: 11.31
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 2.55s
                        Total time: 2112.25s
                               ETA: 8144.0s

################################################################################
                     [1m Learning iteration 824/4000 [0m

                       Computation: 3241 steps/s (collection: 0.483s, learning 2.044s)
               Value function loss: 39171.3885
                    Surrogate loss: 0.0164
             Mean action noise std: 0.95
                       Mean reward: 3407.60
               Mean episode length: 301.80
                 Mean success rate: 44.50
                  Mean reward/step: 11.27
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6758400
                    Iteration time: 2.53s
                        Total time: 2114.78s
                               ETA: 8141.3s

################################################################################
                     [1m Learning iteration 825/4000 [0m

                       Computation: 3212 steps/s (collection: 0.479s, learning 2.070s)
               Value function loss: 51418.9313
                    Surrogate loss: 0.0165
             Mean action noise std: 0.95
                       Mean reward: 3357.20
               Mean episode length: 294.70
                 Mean success rate: 44.50
                  Mean reward/step: 11.30
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 2.55s
                        Total time: 2117.33s
                               ETA: 8138.6s

################################################################################
                     [1m Learning iteration 826/4000 [0m

                       Computation: 3313 steps/s (collection: 0.433s, learning 2.039s)
               Value function loss: 52828.5456
                    Surrogate loss: 0.0150
             Mean action noise std: 0.95
                       Mean reward: 3344.91
               Mean episode length: 292.14
                 Mean success rate: 44.50
                  Mean reward/step: 11.71
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6774784
                    Iteration time: 2.47s
                        Total time: 2119.80s
                               ETA: 8135.7s

################################################################################
                     [1m Learning iteration 827/4000 [0m

                       Computation: 3288 steps/s (collection: 0.459s, learning 2.032s)
               Value function loss: 40821.6476
                    Surrogate loss: 0.0201
             Mean action noise std: 0.95
                       Mean reward: 3442.57
               Mean episode length: 294.47
                 Mean success rate: 44.00
                  Mean reward/step: 11.90
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 2.49s
                        Total time: 2122.29s
                               ETA: 8132.9s

################################################################################
                     [1m Learning iteration 828/4000 [0m

                       Computation: 3220 steps/s (collection: 0.498s, learning 2.046s)
               Value function loss: 47285.3678
                    Surrogate loss: 0.0132
             Mean action noise std: 0.95
                       Mean reward: 3329.52
               Mean episode length: 286.43
                 Mean success rate: 42.50
                  Mean reward/step: 12.39
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6791168
                    Iteration time: 2.54s
                        Total time: 2124.84s
                               ETA: 8130.3s

################################################################################
                     [1m Learning iteration 829/4000 [0m

                       Computation: 3251 steps/s (collection: 0.449s, learning 2.071s)
               Value function loss: 52894.2736
                    Surrogate loss: 0.0151
             Mean action noise std: 0.95
                       Mean reward: 3001.50
               Mean episode length: 268.20
                 Mean success rate: 38.50
                  Mean reward/step: 12.43
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 2.52s
                        Total time: 2127.36s
                               ETA: 8127.5s

################################################################################
                     [1m Learning iteration 830/4000 [0m

                       Computation: 3271 steps/s (collection: 0.477s, learning 2.027s)
               Value function loss: 36679.4297
                    Surrogate loss: 0.0132
             Mean action noise std: 0.95
                       Mean reward: 2750.08
               Mean episode length: 253.47
                 Mean success rate: 33.50
                  Mean reward/step: 12.79
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6807552
                    Iteration time: 2.50s
                        Total time: 2129.86s
                               ETA: 8124.7s

################################################################################
                     [1m Learning iteration 831/4000 [0m

                       Computation: 3200 steps/s (collection: 0.474s, learning 2.086s)
               Value function loss: 50990.8570
                    Surrogate loss: 0.0153
             Mean action noise std: 0.95
                       Mean reward: 2864.37
               Mean episode length: 258.62
                 Mean success rate: 35.50
                  Mean reward/step: 13.30
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 2.56s
                        Total time: 2132.42s
                               ETA: 8122.2s

################################################################################
                     [1m Learning iteration 832/4000 [0m

                       Computation: 3253 steps/s (collection: 0.471s, learning 2.046s)
               Value function loss: 45446.6931
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 3056.11
               Mean episode length: 262.20
                 Mean success rate: 36.00
                  Mean reward/step: 13.39
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6823936
                    Iteration time: 2.52s
                        Total time: 2134.94s
                               ETA: 8119.4s

################################################################################
                     [1m Learning iteration 833/4000 [0m

                       Computation: 3213 steps/s (collection: 0.511s, learning 2.038s)
               Value function loss: 52190.4275
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 3251.17
               Mean episode length: 263.36
                 Mean success rate: 37.00
                  Mean reward/step: 13.37
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 2.55s
                        Total time: 2137.49s
                               ETA: 8116.8s

################################################################################
                     [1m Learning iteration 834/4000 [0m

                       Computation: 3251 steps/s (collection: 0.466s, learning 2.054s)
               Value function loss: 46568.5271
                    Surrogate loss: 0.0169
             Mean action noise std: 0.95
                       Mean reward: 3341.24
               Mean episode length: 264.20
                 Mean success rate: 37.50
                  Mean reward/step: 13.08
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6840320
                    Iteration time: 2.52s
                        Total time: 2140.01s
                               ETA: 8114.1s

################################################################################
                     [1m Learning iteration 835/4000 [0m

                       Computation: 3296 steps/s (collection: 0.461s, learning 2.023s)
               Value function loss: 49816.7214
                    Surrogate loss: 0.0122
             Mean action noise std: 0.95
                       Mean reward: 3433.12
               Mean episode length: 265.54
                 Mean success rate: 39.00
                  Mean reward/step: 14.14
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 2.48s
                        Total time: 2142.49s
                               ETA: 8111.2s

################################################################################
                     [1m Learning iteration 836/4000 [0m

                       Computation: 3214 steps/s (collection: 0.498s, learning 2.050s)
               Value function loss: 58413.5580
                    Surrogate loss: 0.0135
             Mean action noise std: 0.95
                       Mean reward: 3625.61
               Mean episode length: 284.13
                 Mean success rate: 43.00
                  Mean reward/step: 14.44
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6856704
                    Iteration time: 2.55s
                        Total time: 2145.04s
                               ETA: 8108.6s

################################################################################
                     [1m Learning iteration 837/4000 [0m

                       Computation: 3254 steps/s (collection: 0.486s, learning 2.031s)
               Value function loss: 51747.6066
                    Surrogate loss: 0.0125
             Mean action noise std: 0.95
                       Mean reward: 3747.61
               Mean episode length: 286.05
                 Mean success rate: 43.50
                  Mean reward/step: 14.21
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 2.52s
                        Total time: 2147.56s
                               ETA: 8105.9s

################################################################################
                     [1m Learning iteration 838/4000 [0m

                       Computation: 3256 steps/s (collection: 0.492s, learning 2.023s)
               Value function loss: 68880.7962
                    Surrogate loss: 0.0130
             Mean action noise std: 0.95
                       Mean reward: 3933.74
               Mean episode length: 296.00
                 Mean success rate: 46.00
                  Mean reward/step: 13.48
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6873088
                    Iteration time: 2.52s
                        Total time: 2150.07s
                               ETA: 8103.1s

################################################################################
                     [1m Learning iteration 839/4000 [0m

                       Computation: 3294 steps/s (collection: 0.443s, learning 2.044s)
               Value function loss: 46747.6373
                    Surrogate loss: 0.0136
             Mean action noise std: 0.95
                       Mean reward: 3939.17
               Mean episode length: 302.07
                 Mean success rate: 46.00
                  Mean reward/step: 12.84
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 2.49s
                        Total time: 2152.56s
                               ETA: 8100.3s

################################################################################
                     [1m Learning iteration 840/4000 [0m

                       Computation: 3271 steps/s (collection: 0.485s, learning 2.019s)
               Value function loss: 49212.7643
                    Surrogate loss: 0.0143
             Mean action noise std: 0.95
                       Mean reward: 3989.35
               Mean episode length: 309.82
                 Mean success rate: 46.50
                  Mean reward/step: 12.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6889472
                    Iteration time: 2.50s
                        Total time: 2155.06s
                               ETA: 8097.5s

################################################################################
                     [1m Learning iteration 841/4000 [0m

                       Computation: 3235 steps/s (collection: 0.488s, learning 2.043s)
               Value function loss: 55189.6710
                    Surrogate loss: 0.0143
             Mean action noise std: 0.95
                       Mean reward: 3990.75
               Mean episode length: 314.98
                 Mean success rate: 47.50
                  Mean reward/step: 12.67
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 2.53s
                        Total time: 2157.59s
                               ETA: 8094.8s

################################################################################
                     [1m Learning iteration 842/4000 [0m

                       Computation: 3344 steps/s (collection: 0.444s, learning 2.006s)
               Value function loss: 43085.7974
                    Surrogate loss: 0.0134
             Mean action noise std: 0.95
                       Mean reward: 3893.73
               Mean episode length: 311.91
                 Mean success rate: 44.50
                  Mean reward/step: 12.42
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6905856
                    Iteration time: 2.45s
                        Total time: 2160.04s
                               ETA: 8091.8s

################################################################################
                     [1m Learning iteration 843/4000 [0m

                       Computation: 3201 steps/s (collection: 0.481s, learning 2.078s)
               Value function loss: 62271.8255
                    Surrogate loss: 0.0120
             Mean action noise std: 0.95
                       Mean reward: 3891.45
               Mean episode length: 297.70
                 Mean success rate: 43.50
                  Mean reward/step: 12.44
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 2.56s
                        Total time: 2162.60s
                               ETA: 8089.3s

################################################################################
                     [1m Learning iteration 844/4000 [0m

                       Computation: 3239 steps/s (collection: 0.469s, learning 2.060s)
               Value function loss: 46615.6634
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 3719.46
               Mean episode length: 292.88
                 Mean success rate: 41.50
                  Mean reward/step: 12.37
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6922240
                    Iteration time: 2.53s
                        Total time: 2165.13s
                               ETA: 8086.6s

################################################################################
                     [1m Learning iteration 845/4000 [0m

                       Computation: 3256 steps/s (collection: 0.451s, learning 2.065s)
               Value function loss: 49604.5471
                    Surrogate loss: 0.0147
             Mean action noise std: 0.95
                       Mean reward: 3847.83
               Mean episode length: 290.59
                 Mean success rate: 42.50
                  Mean reward/step: 12.77
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 2.52s
                        Total time: 2167.65s
                               ETA: 8083.8s

################################################################################
                     [1m Learning iteration 846/4000 [0m

                       Computation: 3188 steps/s (collection: 0.463s, learning 2.106s)
               Value function loss: 58445.3689
                    Surrogate loss: 0.0159
             Mean action noise std: 0.95
                       Mean reward: 3796.21
               Mean episode length: 286.12
                 Mean success rate: 42.50
                  Mean reward/step: 13.43
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6938624
                    Iteration time: 2.57s
                        Total time: 2170.21s
                               ETA: 8081.3s

################################################################################
                     [1m Learning iteration 847/4000 [0m

                       Computation: 3104 steps/s (collection: 0.517s, learning 2.122s)
               Value function loss: 61473.8757
                    Surrogate loss: 0.0151
             Mean action noise std: 0.95
                       Mean reward: 3898.70
               Mean episode length: 284.81
                 Mean success rate: 43.50
                  Mean reward/step: 13.31
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 2.64s
                        Total time: 2172.85s
                               ETA: 8079.0s

################################################################################
                     [1m Learning iteration 848/4000 [0m

                       Computation: 3207 steps/s (collection: 0.441s, learning 2.114s)
               Value function loss: 40282.9026
                    Surrogate loss: 0.0148
             Mean action noise std: 0.95
                       Mean reward: 3729.75
               Mean episode length: 273.02
                 Mean success rate: 41.50
                  Mean reward/step: 13.38
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6955008
                    Iteration time: 2.55s
                        Total time: 2175.41s
                               ETA: 8076.4s

################################################################################
                     [1m Learning iteration 849/4000 [0m

                       Computation: 3151 steps/s (collection: 0.460s, learning 2.140s)
               Value function loss: 55787.5953
                    Surrogate loss: 0.0141
             Mean action noise std: 0.95
                       Mean reward: 3944.67
               Mean episode length: 283.12
                 Mean success rate: 44.00
                  Mean reward/step: 13.08
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 2.60s
                        Total time: 2178.01s
                               ETA: 8074.0s

################################################################################
                     [1m Learning iteration 850/4000 [0m

                       Computation: 3225 steps/s (collection: 0.476s, learning 2.064s)
               Value function loss: 37241.4051
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 3879.60
               Mean episode length: 292.36
                 Mean success rate: 44.50
                  Mean reward/step: 13.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6971392
                    Iteration time: 2.54s
                        Total time: 2180.55s
                               ETA: 8071.4s

################################################################################
                     [1m Learning iteration 851/4000 [0m

                       Computation: 3117 steps/s (collection: 0.475s, learning 2.153s)
               Value function loss: 46794.2832
                    Surrogate loss: 0.0157
             Mean action noise std: 0.95
                       Mean reward: 3623.06
               Mean episode length: 286.56
                 Mean success rate: 43.00
                  Mean reward/step: 14.46
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 2.63s
                        Total time: 2183.17s
                               ETA: 8069.0s

################################################################################
                     [1m Learning iteration 852/4000 [0m

                       Computation: 3197 steps/s (collection: 0.466s, learning 2.097s)
               Value function loss: 49565.0872
                    Surrogate loss: 0.0161
             Mean action noise std: 0.95
                       Mean reward: 3507.23
               Mean episode length: 282.83
                 Mean success rate: 42.50
                  Mean reward/step: 14.49
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6987776
                    Iteration time: 2.56s
                        Total time: 2185.74s
                               ETA: 8066.5s

################################################################################
                     [1m Learning iteration 853/4000 [0m

                       Computation: 3194 steps/s (collection: 0.454s, learning 2.110s)
               Value function loss: 41338.8939
                    Surrogate loss: 0.0149
             Mean action noise std: 0.95
                       Mean reward: 3523.34
               Mean episode length: 276.50
                 Mean success rate: 42.00
                  Mean reward/step: 14.89
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 2.56s
                        Total time: 2188.30s
                               ETA: 8063.9s

################################################################################
                     [1m Learning iteration 854/4000 [0m

                       Computation: 3214 steps/s (collection: 0.461s, learning 2.088s)
               Value function loss: 50697.5197
                    Surrogate loss: 0.0161
             Mean action noise std: 0.95
                       Mean reward: 3390.95
               Mean episode length: 282.15
                 Mean success rate: 42.50
                  Mean reward/step: 15.17
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7004160
                    Iteration time: 2.55s
                        Total time: 2190.85s
                               ETA: 8061.3s

################################################################################
                     [1m Learning iteration 855/4000 [0m

                       Computation: 3064 steps/s (collection: 0.521s, learning 2.152s)
               Value function loss: 72449.7605
                    Surrogate loss: 0.0133
             Mean action noise std: 0.95
                       Mean reward: 3309.54
               Mean episode length: 276.56
                 Mean success rate: 41.50
                  Mean reward/step: 15.39
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 2.67s
                        Total time: 2193.52s
                               ETA: 8059.1s

################################################################################
                     [1m Learning iteration 856/4000 [0m

                       Computation: 3153 steps/s (collection: 0.478s, learning 2.120s)
               Value function loss: 75179.6501
                    Surrogate loss: 0.0128
             Mean action noise std: 0.95
                       Mean reward: 3608.26
               Mean episode length: 281.77
                 Mean success rate: 44.50
                  Mean reward/step: 15.17
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7020544
                    Iteration time: 2.60s
                        Total time: 2196.12s
                               ETA: 8056.7s

################################################################################
                     [1m Learning iteration 857/4000 [0m

                       Computation: 3094 steps/s (collection: 0.458s, learning 2.189s)
               Value function loss: 54282.9307
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 3725.16
               Mean episode length: 273.29
                 Mean success rate: 44.50
                  Mean reward/step: 15.18
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 2.65s
                        Total time: 2198.77s
                               ETA: 8054.5s

################################################################################
                     [1m Learning iteration 858/4000 [0m

                       Computation: 3131 steps/s (collection: 0.469s, learning 2.147s)
               Value function loss: 72772.7374
                    Surrogate loss: 0.0125
             Mean action noise std: 0.95
                       Mean reward: 4219.45
               Mean episode length: 288.37
                 Mean success rate: 49.50
                  Mean reward/step: 14.54
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7036928
                    Iteration time: 2.62s
                        Total time: 2201.38s
                               ETA: 8052.1s

################################################################################
                     [1m Learning iteration 859/4000 [0m

                       Computation: 3112 steps/s (collection: 0.516s, learning 2.116s)
               Value function loss: 73055.0263
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 4372.34
               Mean episode length: 295.80
                 Mean success rate: 50.50
                  Mean reward/step: 14.34
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 2.63s
                        Total time: 2204.02s
                               ETA: 8049.8s

################################################################################
                     [1m Learning iteration 860/4000 [0m

                       Computation: 3117 steps/s (collection: 0.511s, learning 2.117s)
               Value function loss: 57344.8758
                    Surrogate loss: 0.0144
             Mean action noise std: 0.95
                       Mean reward: 3993.54
               Mean episode length: 271.46
                 Mean success rate: 46.00
                  Mean reward/step: 14.42
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 7053312
                    Iteration time: 2.63s
                        Total time: 2206.64s
                               ETA: 8047.5s

################################################################################
                     [1m Learning iteration 861/4000 [0m

                       Computation: 3210 steps/s (collection: 0.461s, learning 2.091s)
               Value function loss: 72303.2397
                    Surrogate loss: 0.0125
             Mean action noise std: 0.95
                       Mean reward: 4108.59
               Mean episode length: 269.26
                 Mean success rate: 45.50
                  Mean reward/step: 13.81
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 2.55s
                        Total time: 2209.19s
                               ETA: 8044.9s

################################################################################
                     [1m Learning iteration 862/4000 [0m

                       Computation: 3126 steps/s (collection: 0.484s, learning 2.136s)
               Value function loss: 58907.9525
                    Surrogate loss: 0.0151
             Mean action noise std: 0.95
                       Mean reward: 3976.74
               Mean episode length: 271.62
                 Mean success rate: 45.00
                  Mean reward/step: 14.08
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7069696
                    Iteration time: 2.62s
                        Total time: 2211.81s
                               ETA: 8042.5s

################################################################################
                     [1m Learning iteration 863/4000 [0m

                       Computation: 3033 steps/s (collection: 0.539s, learning 2.161s)
               Value function loss: 49742.6326
                    Surrogate loss: 0.0151
             Mean action noise std: 0.95
                       Mean reward: 3920.52
               Mean episode length: 270.62
                 Mean success rate: 44.00
                  Mean reward/step: 14.22
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 2.70s
                        Total time: 2214.51s
                               ETA: 8040.4s

################################################################################
                     [1m Learning iteration 864/4000 [0m

                       Computation: 3062 steps/s (collection: 0.539s, learning 2.136s)
               Value function loss: 61050.6899
                    Surrogate loss: 0.0174
             Mean action noise std: 0.95
                       Mean reward: 3929.16
               Mean episode length: 271.43
                 Mean success rate: 43.50
                  Mean reward/step: 14.38
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7086080
                    Iteration time: 2.67s
                        Total time: 2217.19s
                               ETA: 8038.3s

################################################################################
                     [1m Learning iteration 865/4000 [0m

                       Computation: 3189 steps/s (collection: 0.467s, learning 2.101s)
               Value function loss: 54416.5738
                    Surrogate loss: 0.0204
             Mean action noise std: 0.95
                       Mean reward: 3560.65
               Mean episode length: 256.93
                 Mean success rate: 39.00
                  Mean reward/step: 14.42
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 2.57s
                        Total time: 2219.76s
                               ETA: 8035.7s

################################################################################
                     [1m Learning iteration 866/4000 [0m

                       Computation: 3188 steps/s (collection: 0.502s, learning 2.067s)
               Value function loss: 58863.5007
                    Surrogate loss: 0.0133
             Mean action noise std: 0.95
                       Mean reward: 3774.61
               Mean episode length: 264.77
                 Mean success rate: 41.00
                  Mean reward/step: 14.05
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7102464
                    Iteration time: 2.57s
                        Total time: 2222.33s
                               ETA: 8033.2s

################################################################################
                     [1m Learning iteration 867/4000 [0m

                       Computation: 3253 steps/s (collection: 0.489s, learning 2.029s)
               Value function loss: 49621.0066
                    Surrogate loss: 0.0119
             Mean action noise std: 0.95
                       Mean reward: 3816.07
               Mean episode length: 270.87
                 Mean success rate: 41.50
                  Mean reward/step: 13.71
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 2.52s
                        Total time: 2224.84s
                               ETA: 8030.5s

################################################################################
                     [1m Learning iteration 868/4000 [0m

                       Computation: 3231 steps/s (collection: 0.494s, learning 2.041s)
               Value function loss: 67337.6744
                    Surrogate loss: 0.0131
             Mean action noise std: 0.95
                       Mean reward: 3979.64
               Mean episode length: 277.23
                 Mean success rate: 43.00
                  Mean reward/step: 13.24
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7118848
                    Iteration time: 2.54s
                        Total time: 2227.38s
                               ETA: 8027.8s

################################################################################
                     [1m Learning iteration 869/4000 [0m

                       Computation: 3278 steps/s (collection: 0.460s, learning 2.039s)
               Value function loss: 44466.7246
                    Surrogate loss: 0.0126
             Mean action noise std: 0.95
                       Mean reward: 3804.68
               Mean episode length: 263.86
                 Mean success rate: 41.50
                  Mean reward/step: 13.11
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 2.50s
                        Total time: 2229.88s
                               ETA: 8025.0s

################################################################################
                     [1m Learning iteration 870/4000 [0m

                       Computation: 3223 steps/s (collection: 0.457s, learning 2.084s)
               Value function loss: 40484.0720
                    Surrogate loss: 0.0134
             Mean action noise std: 0.95
                       Mean reward: 3747.03
               Mean episode length: 262.64
                 Mean success rate: 42.00
                  Mean reward/step: 12.98
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7135232
                    Iteration time: 2.54s
                        Total time: 2232.42s
                               ETA: 8022.4s

################################################################################
                     [1m Learning iteration 871/4000 [0m

                       Computation: 3197 steps/s (collection: 0.483s, learning 2.078s)
               Value function loss: 47137.3856
                    Surrogate loss: 0.0124
             Mean action noise std: 0.95
                       Mean reward: 3546.97
               Mean episode length: 251.41
                 Mean success rate: 39.00
                  Mean reward/step: 13.40
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 2.56s
                        Total time: 2234.98s
                               ETA: 8019.8s

################################################################################
                     [1m Learning iteration 872/4000 [0m

                       Computation: 3244 steps/s (collection: 0.475s, learning 2.050s)
               Value function loss: 45614.5083
                    Surrogate loss: 0.0115
             Mean action noise std: 0.95
                       Mean reward: 3420.29
               Mean episode length: 244.09
                 Mean success rate: 38.00
                  Mean reward/step: 13.69
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7151616
                    Iteration time: 2.52s
                        Total time: 2237.51s
                               ETA: 8017.1s

################################################################################
                     [1m Learning iteration 873/4000 [0m

                       Computation: 3142 steps/s (collection: 0.503s, learning 2.104s)
               Value function loss: 78355.4489
                    Surrogate loss: 0.0151
             Mean action noise std: 0.95
                       Mean reward: 3285.72
               Mean episode length: 243.33
                 Mean success rate: 37.00
                  Mean reward/step: 13.54
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 2.61s
                        Total time: 2240.11s
                               ETA: 8014.7s

################################################################################
                     [1m Learning iteration 874/4000 [0m

                       Computation: 3239 steps/s (collection: 0.485s, learning 2.043s)
               Value function loss: 58705.5666
                    Surrogate loss: 0.0136
             Mean action noise std: 0.95
                       Mean reward: 3240.26
               Mean episode length: 237.95
                 Mean success rate: 36.50
                  Mean reward/step: 12.66
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7168000
                    Iteration time: 2.53s
                        Total time: 2242.64s
                               ETA: 8012.0s

################################################################################
                     [1m Learning iteration 875/4000 [0m

                       Computation: 3246 steps/s (collection: 0.482s, learning 2.041s)
               Value function loss: 69901.7644
                    Surrogate loss: 0.0152
             Mean action noise std: 0.95
                       Mean reward: 3452.09
               Mean episode length: 255.81
                 Mean success rate: 38.00
                  Mean reward/step: 11.98
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 2.52s
                        Total time: 2245.17s
                               ETA: 8009.3s

################################################################################
                     [1m Learning iteration 876/4000 [0m

                       Computation: 3162 steps/s (collection: 0.531s, learning 2.059s)
               Value function loss: 46152.0593
                    Surrogate loss: 0.0149
             Mean action noise std: 0.95
                       Mean reward: 3573.55
               Mean episode length: 264.57
                 Mean success rate: 39.50
                  Mean reward/step: 11.39
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7184384
                    Iteration time: 2.59s
                        Total time: 2247.76s
                               ETA: 8006.8s

################################################################################
                     [1m Learning iteration 877/4000 [0m

                       Computation: 3206 steps/s (collection: 0.489s, learning 2.066s)
               Value function loss: 50922.5425
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 3786.82
               Mean episode length: 275.42
                 Mean success rate: 42.00
                  Mean reward/step: 11.72
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 2.55s
                        Total time: 2250.31s
                               ETA: 8004.2s

################################################################################
                     [1m Learning iteration 878/4000 [0m

                       Computation: 3154 steps/s (collection: 0.478s, learning 2.119s)
               Value function loss: 44782.8680
                    Surrogate loss: 0.0159
             Mean action noise std: 0.95
                       Mean reward: 3959.80
               Mean episode length: 282.30
                 Mean success rate: 43.50
                  Mean reward/step: 11.90
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7200768
                    Iteration time: 2.60s
                        Total time: 2252.91s
                               ETA: 8001.8s

################################################################################
                     [1m Learning iteration 879/4000 [0m

                       Computation: 3055 steps/s (collection: 0.538s, learning 2.143s)
               Value function loss: 36104.5250
                    Surrogate loss: 0.0143
             Mean action noise std: 0.95
                       Mean reward: 3605.89
               Mean episode length: 265.31
                 Mean success rate: 40.50
                  Mean reward/step: 12.13
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 2.68s
                        Total time: 2255.59s
                               ETA: 7999.6s

################################################################################
                     [1m Learning iteration 880/4000 [0m

                       Computation: 3144 steps/s (collection: 0.523s, learning 2.082s)
               Value function loss: 48576.8542
                    Surrogate loss: 0.0164
             Mean action noise std: 0.95
                       Mean reward: 3661.79
               Mean episode length: 264.69
                 Mean success rate: 41.00
                  Mean reward/step: 12.39
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7217152
                    Iteration time: 2.61s
                        Total time: 2258.19s
                               ETA: 7997.2s

################################################################################
                     [1m Learning iteration 881/4000 [0m

                       Computation: 3145 steps/s (collection: 0.492s, learning 2.112s)
               Value function loss: 53771.9889
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 3742.25
               Mean episode length: 269.85
                 Mean success rate: 42.50
                  Mean reward/step: 12.25
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 2.60s
                        Total time: 2260.80s
                               ETA: 7994.8s

################################################################################
                     [1m Learning iteration 882/4000 [0m

                       Computation: 3170 steps/s (collection: 0.470s, learning 2.114s)
               Value function loss: 42611.1316
                    Surrogate loss: 0.0192
             Mean action noise std: 0.95
                       Mean reward: 3421.01
               Mean episode length: 254.09
                 Mean success rate: 39.50
                  Mean reward/step: 12.86
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7233536
                    Iteration time: 2.58s
                        Total time: 2263.38s
                               ETA: 7992.3s

################################################################################
                     [1m Learning iteration 883/4000 [0m

                       Computation: 3120 steps/s (collection: 0.510s, learning 2.116s)
               Value function loss: 45681.7018
                    Surrogate loss: 0.0166
             Mean action noise std: 0.95
                       Mean reward: 3138.26
               Mean episode length: 245.46
                 Mean success rate: 37.50
                  Mean reward/step: 13.39
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 2.63s
                        Total time: 2266.01s
                               ETA: 7990.0s

################################################################################
                     [1m Learning iteration 884/4000 [0m

                       Computation: 3068 steps/s (collection: 0.552s, learning 2.117s)
               Value function loss: 54353.9649
                    Surrogate loss: 0.0155
             Mean action noise std: 0.95
                       Mean reward: 3093.94
               Mean episode length: 246.26
                 Mean success rate: 38.00
                  Mean reward/step: 13.18
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7249920
                    Iteration time: 2.67s
                        Total time: 2268.68s
                               ETA: 7987.8s

################################################################################
                     [1m Learning iteration 885/4000 [0m

                       Computation: 3187 steps/s (collection: 0.460s, learning 2.110s)
               Value function loss: 58866.5313
                    Surrogate loss: 0.0099
             Mean action noise std: 0.95
                       Mean reward: 3234.94
               Mean episode length: 255.59
                 Mean success rate: 39.00
                  Mean reward/step: 12.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 2.57s
                        Total time: 2271.25s
                               ETA: 7985.3s

################################################################################
                     [1m Learning iteration 886/4000 [0m

                       Computation: 3209 steps/s (collection: 0.449s, learning 2.104s)
               Value function loss: 51581.4464
                    Surrogate loss: 0.0114
             Mean action noise std: 0.95
                       Mean reward: 3497.39
               Mean episode length: 271.14
                 Mean success rate: 42.50
                  Mean reward/step: 13.03
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7266304
                    Iteration time: 2.55s
                        Total time: 2273.80s
                               ETA: 7982.7s

################################################################################
                     [1m Learning iteration 887/4000 [0m

                       Computation: 3044 steps/s (collection: 0.529s, learning 2.162s)
               Value function loss: 55488.8791
                    Surrogate loss: 0.0130
             Mean action noise std: 0.95
                       Mean reward: 3666.20
               Mean episode length: 287.12
                 Mean success rate: 45.50
                  Mean reward/step: 13.15
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 2.69s
                        Total time: 2276.49s
                               ETA: 7980.5s

################################################################################
                     [1m Learning iteration 888/4000 [0m

                       Computation: 3061 steps/s (collection: 0.556s, learning 2.120s)
               Value function loss: 56119.6736
                    Surrogate loss: 0.0158
             Mean action noise std: 0.95
                       Mean reward: 3696.33
               Mean episode length: 287.92
                 Mean success rate: 46.50
                  Mean reward/step: 12.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7282688
                    Iteration time: 2.68s
                        Total time: 2279.17s
                               ETA: 7978.4s

################################################################################
                     [1m Learning iteration 889/4000 [0m

                       Computation: 3206 steps/s (collection: 0.477s, learning 2.078s)
               Value function loss: 57818.3383
                    Surrogate loss: 0.0146
             Mean action noise std: 0.95
                       Mean reward: 3788.72
               Mean episode length: 301.15
                 Mean success rate: 48.00
                  Mean reward/step: 12.16
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 2.55s
                        Total time: 2281.72s
                               ETA: 7975.8s

################################################################################
                     [1m Learning iteration 890/4000 [0m

                       Computation: 3162 steps/s (collection: 0.487s, learning 2.103s)
               Value function loss: 65525.0841
                    Surrogate loss: 0.0116
             Mean action noise std: 0.95
                       Mean reward: 3978.71
               Mean episode length: 316.38
                 Mean success rate: 52.00
                  Mean reward/step: 12.24
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 7299072
                    Iteration time: 2.59s
                        Total time: 2284.31s
                               ETA: 7973.3s

################################################################################
                     [1m Learning iteration 891/4000 [0m

                       Computation: 3043 steps/s (collection: 0.521s, learning 2.171s)
               Value function loss: 37871.2434
                    Surrogate loss: 0.0167
             Mean action noise std: 0.95
                       Mean reward: 3880.03
               Mean episode length: 310.23
                 Mean success rate: 50.00
                  Mean reward/step: 11.65
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 2.69s
                        Total time: 2287.00s
                               ETA: 7971.2s

################################################################################
                     [1m Learning iteration 892/4000 [0m

                       Computation: 3079 steps/s (collection: 0.529s, learning 2.131s)
               Value function loss: 60962.4490
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 3709.98
               Mean episode length: 304.62
                 Mean success rate: 49.00
                  Mean reward/step: 11.72
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7315456
                    Iteration time: 2.66s
                        Total time: 2289.66s
                               ETA: 7968.9s

################################################################################
                     [1m Learning iteration 893/4000 [0m

                       Computation: 3182 steps/s (collection: 0.486s, learning 2.088s)
               Value function loss: 54552.1645
                    Surrogate loss: 0.0128
             Mean action noise std: 0.95
                       Mean reward: 3654.40
               Mean episode length: 308.24
                 Mean success rate: 47.50
                  Mean reward/step: 11.58
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 2.57s
                        Total time: 2292.24s
                               ETA: 7966.4s

################################################################################
                     [1m Learning iteration 894/4000 [0m

                       Computation: 3129 steps/s (collection: 0.529s, learning 2.088s)
               Value function loss: 55089.5965
                    Surrogate loss: 0.0156
             Mean action noise std: 0.94
                       Mean reward: 3700.41
               Mean episode length: 302.72
                 Mean success rate: 46.00
                  Mean reward/step: 11.72
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7331840
                    Iteration time: 2.62s
                        Total time: 2294.85s
                               ETA: 7964.0s

################################################################################
                     [1m Learning iteration 895/4000 [0m

                       Computation: 3152 steps/s (collection: 0.466s, learning 2.133s)
               Value function loss: 47499.2488
                    Surrogate loss: 0.0148
             Mean action noise std: 0.94
                       Mean reward: 3417.94
               Mean episode length: 283.13
                 Mean success rate: 43.00
                  Mean reward/step: 11.85
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 2.60s
                        Total time: 2297.45s
                               ETA: 7961.6s

################################################################################
                     [1m Learning iteration 896/4000 [0m

                       Computation: 3202 steps/s (collection: 0.458s, learning 2.100s)
               Value function loss: 43277.4265
                    Surrogate loss: 0.0180
             Mean action noise std: 0.94
                       Mean reward: 3315.53
               Mean episode length: 268.62
                 Mean success rate: 41.00
                  Mean reward/step: 12.36
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7348224
                    Iteration time: 2.56s
                        Total time: 2300.01s
                               ETA: 7959.0s

################################################################################
                     [1m Learning iteration 897/4000 [0m

                       Computation: 3063 steps/s (collection: 0.556s, learning 2.118s)
               Value function loss: 40642.7167
                    Surrogate loss: 0.0150
             Mean action noise std: 0.94
                       Mean reward: 3164.99
               Mean episode length: 266.71
                 Mean success rate: 40.00
                  Mean reward/step: 12.94
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 2.67s
                        Total time: 2302.68s
                               ETA: 7956.8s

################################################################################
                     [1m Learning iteration 898/4000 [0m

                       Computation: 3174 steps/s (collection: 0.480s, learning 2.101s)
               Value function loss: 40426.4249
                    Surrogate loss: 0.0159
             Mean action noise std: 0.94
                       Mean reward: 3282.41
               Mean episode length: 266.25
                 Mean success rate: 41.00
                  Mean reward/step: 13.85
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7364608
                    Iteration time: 2.58s
                        Total time: 2305.27s
                               ETA: 7954.3s

################################################################################
                     [1m Learning iteration 899/4000 [0m

                       Computation: 3135 steps/s (collection: 0.469s, learning 2.143s)
               Value function loss: 47443.6030
                    Surrogate loss: 0.0142
             Mean action noise std: 0.94
                       Mean reward: 3078.64
               Mean episode length: 259.89
                 Mean success rate: 40.00
                  Mean reward/step: 13.74
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 2.61s
                        Total time: 2307.88s
                               ETA: 7951.9s

################################################################################
                     [1m Learning iteration 900/4000 [0m

                       Computation: 3082 steps/s (collection: 0.473s, learning 2.185s)
               Value function loss: 56629.3452
                    Surrogate loss: 0.0115
             Mean action noise std: 0.94
                       Mean reward: 3013.43
               Mean episode length: 261.45
                 Mean success rate: 40.50
                  Mean reward/step: 13.48
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7380992
                    Iteration time: 2.66s
                        Total time: 2310.53s
                               ETA: 7949.7s

################################################################################
                     [1m Learning iteration 901/4000 [0m

                       Computation: 3141 steps/s (collection: 0.492s, learning 2.116s)
               Value function loss: 56126.4739
                    Surrogate loss: 0.0123
             Mean action noise std: 0.94
                       Mean reward: 2837.10
               Mean episode length: 250.35
                 Mean success rate: 37.50
                  Mean reward/step: 12.64
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 2.61s
                        Total time: 2313.14s
                               ETA: 7947.3s

################################################################################
                     [1m Learning iteration 902/4000 [0m

                       Computation: 3145 steps/s (collection: 0.485s, learning 2.119s)
               Value function loss: 62816.2636
                    Surrogate loss: 0.0140
             Mean action noise std: 0.94
                       Mean reward: 2885.26
               Mean episode length: 252.96
                 Mean success rate: 39.50
                  Mean reward/step: 12.49
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7397376
                    Iteration time: 2.60s
                        Total time: 2315.75s
                               ETA: 7944.8s

################################################################################
                     [1m Learning iteration 903/4000 [0m

                       Computation: 3160 steps/s (collection: 0.479s, learning 2.113s)
               Value function loss: 59640.6209
                    Surrogate loss: 0.0164
             Mean action noise std: 0.94
                       Mean reward: 2815.63
               Mean episode length: 240.54
                 Mean success rate: 37.00
                  Mean reward/step: 12.03
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 2.59s
                        Total time: 2318.34s
                               ETA: 7942.4s

################################################################################
                     [1m Learning iteration 904/4000 [0m

                       Computation: 3090 steps/s (collection: 0.502s, learning 2.149s)
               Value function loss: 46756.7568
                    Surrogate loss: 0.0112
             Mean action noise std: 0.94
                       Mean reward: 2930.77
               Mean episode length: 240.97
                 Mean success rate: 37.50
                  Mean reward/step: 11.88
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7413760
                    Iteration time: 2.65s
                        Total time: 2320.99s
                               ETA: 7940.1s

################################################################################
                     [1m Learning iteration 905/4000 [0m

                       Computation: 3152 steps/s (collection: 0.484s, learning 2.114s)
               Value function loss: 46606.5177
                    Surrogate loss: 0.0145
             Mean action noise std: 0.94
                       Mean reward: 2910.45
               Mean episode length: 230.99
                 Mean success rate: 36.50
                  Mean reward/step: 12.29
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 2.60s
                        Total time: 2323.59s
                               ETA: 7937.6s

################################################################################
                     [1m Learning iteration 906/4000 [0m

                       Computation: 3102 steps/s (collection: 0.486s, learning 2.154s)
               Value function loss: 54561.2999
                    Surrogate loss: 0.0161
             Mean action noise std: 0.94
                       Mean reward: 2897.64
               Mean episode length: 228.76
                 Mean success rate: 36.50
                  Mean reward/step: 11.39
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7430144
                    Iteration time: 2.64s
                        Total time: 2326.23s
                               ETA: 7935.3s

################################################################################
                     [1m Learning iteration 907/4000 [0m

                       Computation: 3123 steps/s (collection: 0.488s, learning 2.135s)
               Value function loss: 49530.5491
                    Surrogate loss: 0.0165
             Mean action noise std: 0.94
                       Mean reward: 2919.80
               Mean episode length: 226.21
                 Mean success rate: 35.50
                  Mean reward/step: 10.84
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 2.62s
                        Total time: 2328.85s
                               ETA: 7933.0s

################################################################################
                     [1m Learning iteration 908/4000 [0m

                       Computation: 3126 steps/s (collection: 0.499s, learning 2.122s)
               Value function loss: 52693.5252
                    Surrogate loss: 0.0175
             Mean action noise std: 0.94
                       Mean reward: 2715.17
               Mean episode length: 218.72
                 Mean success rate: 34.50
                  Mean reward/step: 11.03
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 7446528
                    Iteration time: 2.62s
                        Total time: 2331.47s
                               ETA: 7930.6s

################################################################################
                     [1m Learning iteration 909/4000 [0m

                       Computation: 3129 steps/s (collection: 0.482s, learning 2.136s)
               Value function loss: 39547.5311
                    Surrogate loss: 0.0143
             Mean action noise std: 0.94
                       Mean reward: 2429.07
               Mean episode length: 203.06
                 Mean success rate: 30.00
                  Mean reward/step: 11.17
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 2.62s
                        Total time: 2334.09s
                               ETA: 7928.2s

################################################################################
                     [1m Learning iteration 910/4000 [0m

                       Computation: 3179 steps/s (collection: 0.464s, learning 2.113s)
               Value function loss: 37557.0822
                    Surrogate loss: 0.0117
             Mean action noise std: 0.94
                       Mean reward: 2401.57
               Mean episode length: 201.31
                 Mean success rate: 30.00
                  Mean reward/step: 11.36
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7462912
                    Iteration time: 2.58s
                        Total time: 2336.67s
                               ETA: 7925.7s

################################################################################
                     [1m Learning iteration 911/4000 [0m

                       Computation: 3103 steps/s (collection: 0.494s, learning 2.145s)
               Value function loss: 64712.6472
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 2538.66
               Mean episode length: 216.18
                 Mean success rate: 30.50
                  Mean reward/step: 11.39
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 2.64s
                        Total time: 2339.31s
                               ETA: 7923.4s

################################################################################
                     [1m Learning iteration 912/4000 [0m

                       Computation: 3187 steps/s (collection: 0.500s, learning 2.069s)
               Value function loss: 33347.2719
                    Surrogate loss: 0.0122
             Mean action noise std: 0.94
                       Mean reward: 2204.24
               Mean episode length: 205.44
                 Mean success rate: 26.50
                  Mean reward/step: 11.97
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 7479296
                    Iteration time: 2.57s
                        Total time: 2341.88s
                               ETA: 7920.8s

################################################################################
                     [1m Learning iteration 913/4000 [0m

                       Computation: 3231 steps/s (collection: 0.481s, learning 2.054s)
               Value function loss: 47900.4426
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 2419.98
               Mean episode length: 213.57
                 Mean success rate: 28.00
                  Mean reward/step: 12.45
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 2.53s
                        Total time: 2344.41s
                               ETA: 7918.2s

################################################################################
                     [1m Learning iteration 914/4000 [0m

                       Computation: 3228 steps/s (collection: 0.444s, learning 2.093s)
               Value function loss: 47144.4356
                    Surrogate loss: 0.0112
             Mean action noise std: 0.94
                       Mean reward: 2427.58
               Mean episode length: 210.97
                 Mean success rate: 29.00
                  Mean reward/step: 12.02
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7495680
                    Iteration time: 2.54s
                        Total time: 2346.95s
                               ETA: 7915.5s

################################################################################
                     [1m Learning iteration 915/4000 [0m

                       Computation: 3166 steps/s (collection: 0.458s, learning 2.130s)
               Value function loss: 42510.8497
                    Surrogate loss: 0.0128
             Mean action noise std: 0.94
                       Mean reward: 2237.55
               Mean episode length: 198.95
                 Mean success rate: 26.00
                  Mean reward/step: 12.68
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 2.59s
                        Total time: 2349.54s
                               ETA: 7913.0s

################################################################################
                     [1m Learning iteration 916/4000 [0m

                       Computation: 3110 steps/s (collection: 0.530s, learning 2.103s)
               Value function loss: 57492.3570
                    Surrogate loss: 0.0122
             Mean action noise std: 0.94
                       Mean reward: 2234.56
               Mean episode length: 192.87
                 Mean success rate: 26.50
                  Mean reward/step: 12.38
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 7512064
                    Iteration time: 2.63s
                        Total time: 2352.17s
                               ETA: 7910.7s

################################################################################
                     [1m Learning iteration 917/4000 [0m

                       Computation: 3186 steps/s (collection: 0.468s, learning 2.103s)
               Value function loss: 33934.7542
                    Surrogate loss: 0.0155
             Mean action noise std: 0.94
                       Mean reward: 2311.02
               Mean episode length: 191.07
                 Mean success rate: 27.50
                  Mean reward/step: 12.32
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 2.57s
                        Total time: 2354.74s
                               ETA: 7908.1s

################################################################################
                     [1m Learning iteration 918/4000 [0m

                       Computation: 3208 steps/s (collection: 0.448s, learning 2.105s)
               Value function loss: 55732.1096
                    Surrogate loss: 0.0126
             Mean action noise std: 0.94
                       Mean reward: 2431.71
               Mean episode length: 191.31
                 Mean success rate: 28.00
                  Mean reward/step: 12.04
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 7528448
                    Iteration time: 2.55s
                        Total time: 2357.29s
                               ETA: 7905.5s

################################################################################
                     [1m Learning iteration 919/4000 [0m

                       Computation: 3170 steps/s (collection: 0.462s, learning 2.122s)
               Value function loss: 41389.4129
                    Surrogate loss: 0.0123
             Mean action noise std: 0.94
                       Mean reward: 2572.93
               Mean episode length: 200.16
                 Mean success rate: 29.50
                  Mean reward/step: 12.07
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 2.58s
                        Total time: 2359.88s
                               ETA: 7903.0s

################################################################################
                     [1m Learning iteration 920/4000 [0m

                       Computation: 3131 steps/s (collection: 0.496s, learning 2.119s)
               Value function loss: 57039.4088
                    Surrogate loss: 0.0121
             Mean action noise std: 0.94
                       Mean reward: 2668.80
               Mean episode length: 211.68
                 Mean success rate: 31.00
                  Mean reward/step: 12.10
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7544832
                    Iteration time: 2.62s
                        Total time: 2362.49s
                               ETA: 7900.6s

################################################################################
                     [1m Learning iteration 921/4000 [0m

                       Computation: 3190 steps/s (collection: 0.484s, learning 2.083s)
               Value function loss: 53274.5506
                    Surrogate loss: 0.0132
             Mean action noise std: 0.94
                       Mean reward: 2558.62
               Mean episode length: 213.06
                 Mean success rate: 31.00
                  Mean reward/step: 12.01
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 2.57s
                        Total time: 2365.06s
                               ETA: 7898.1s

################################################################################
                     [1m Learning iteration 922/4000 [0m

                       Computation: 3171 steps/s (collection: 0.505s, learning 2.078s)
               Value function loss: 71918.5244
                    Surrogate loss: 0.0125
             Mean action noise std: 0.94
                       Mean reward: 3027.57
               Mean episode length: 229.50
                 Mean success rate: 35.50
                  Mean reward/step: 12.13
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 7561216
                    Iteration time: 2.58s
                        Total time: 2367.64s
                               ETA: 7895.6s

################################################################################
                     [1m Learning iteration 923/4000 [0m

                       Computation: 3165 steps/s (collection: 0.479s, learning 2.108s)
               Value function loss: 43344.3349
                    Surrogate loss: 0.0159
             Mean action noise std: 0.94
                       Mean reward: 2792.52
               Mean episode length: 223.97
                 Mean success rate: 32.50
                  Mean reward/step: 11.81
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 2.59s
                        Total time: 2370.23s
                               ETA: 7893.1s

################################################################################
                     [1m Learning iteration 924/4000 [0m

                       Computation: 3206 steps/s (collection: 0.455s, learning 2.100s)
               Value function loss: 53167.0492
                    Surrogate loss: 0.0149
             Mean action noise std: 0.94
                       Mean reward: 2794.32
               Mean episode length: 224.73
                 Mean success rate: 32.50
                  Mean reward/step: 11.81
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7577600
                    Iteration time: 2.56s
                        Total time: 2372.79s
                               ETA: 7890.5s

################################################################################
                     [1m Learning iteration 925/4000 [0m

                       Computation: 3172 steps/s (collection: 0.488s, learning 2.095s)
               Value function loss: 56596.6893
                    Surrogate loss: 0.0138
             Mean action noise std: 0.94
                       Mean reward: 2714.02
               Mean episode length: 216.53
                 Mean success rate: 30.50
                  Mean reward/step: 11.69
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 2.58s
                        Total time: 2375.37s
                               ETA: 7888.0s

################################################################################
                     [1m Learning iteration 926/4000 [0m

                       Computation: 3160 steps/s (collection: 0.478s, learning 2.114s)
               Value function loss: 49292.1615
                    Surrogate loss: 0.0128
             Mean action noise std: 0.94
                       Mean reward: 2660.10
               Mean episode length: 213.52
                 Mean success rate: 29.50
                  Mean reward/step: 11.97
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7593984
                    Iteration time: 2.59s
                        Total time: 2377.96s
                               ETA: 7885.5s

################################################################################
                     [1m Learning iteration 927/4000 [0m

                       Computation: 3211 steps/s (collection: 0.494s, learning 2.057s)
               Value function loss: 56569.2100
                    Surrogate loss: 0.0100
             Mean action noise std: 0.94
                       Mean reward: 2803.78
               Mean episode length: 218.57
                 Mean success rate: 30.50
                  Mean reward/step: 11.90
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 2.55s
                        Total time: 2380.51s
                               ETA: 7882.9s

################################################################################
                     [1m Learning iteration 928/4000 [0m

                       Computation: 3305 steps/s (collection: 0.439s, learning 2.039s)
               Value function loss: 48091.0529
                    Surrogate loss: 0.0131
             Mean action noise std: 0.94
                       Mean reward: 2560.10
               Mean episode length: 211.69
                 Mean success rate: 28.50
                  Mean reward/step: 11.60
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 7610368
                    Iteration time: 2.48s
                        Total time: 2382.99s
                               ETA: 7880.0s

################################################################################
                     [1m Learning iteration 929/4000 [0m

                       Computation: 3167 steps/s (collection: 0.506s, learning 2.080s)
               Value function loss: 51460.6155
                    Surrogate loss: 0.0139
             Mean action noise std: 0.94
                       Mean reward: 2467.44
               Mean episode length: 214.52
                 Mean success rate: 29.00
                  Mean reward/step: 11.75
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 2.59s
                        Total time: 2385.58s
                               ETA: 7877.5s

################################################################################
                     [1m Learning iteration 930/4000 [0m

                       Computation: 3240 steps/s (collection: 0.464s, learning 2.064s)
               Value function loss: 47876.7851
                    Surrogate loss: 0.0141
             Mean action noise std: 0.94
                       Mean reward: 2583.04
               Mean episode length: 220.28
                 Mean success rate: 30.00
                  Mean reward/step: 12.15
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7626752
                    Iteration time: 2.53s
                        Total time: 2388.10s
                               ETA: 7874.8s

################################################################################
                     [1m Learning iteration 931/4000 [0m

                       Computation: 3171 steps/s (collection: 0.505s, learning 2.078s)
               Value function loss: 53440.5584
                    Surrogate loss: 0.0144
             Mean action noise std: 0.94
                       Mean reward: 2689.86
               Mean episode length: 229.93
                 Mean success rate: 32.50
                  Mean reward/step: 12.76
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 2.58s
                        Total time: 2390.69s
                               ETA: 7872.3s

################################################################################
                     [1m Learning iteration 932/4000 [0m

                       Computation: 3258 steps/s (collection: 0.475s, learning 2.039s)
               Value function loss: 54579.0093
                    Surrogate loss: 0.0127
             Mean action noise std: 0.94
                       Mean reward: 2540.24
               Mean episode length: 226.07
                 Mean success rate: 32.00
                  Mean reward/step: 12.95
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 7643136
                    Iteration time: 2.51s
                        Total time: 2393.20s
                               ETA: 7869.6s

################################################################################
                     [1m Learning iteration 933/4000 [0m

                       Computation: 3163 steps/s (collection: 0.501s, learning 2.089s)
               Value function loss: 42875.8143
                    Surrogate loss: 0.0143
             Mean action noise std: 0.94
                       Mean reward: 2247.76
               Mean episode length: 217.87
                 Mean success rate: 30.00
                  Mean reward/step: 12.72
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 2.59s
                        Total time: 2395.79s
                               ETA: 7867.1s

################################################################################
                     [1m Learning iteration 934/4000 [0m

                       Computation: 3216 steps/s (collection: 0.464s, learning 2.083s)
               Value function loss: 60464.7692
                    Surrogate loss: 0.0145
             Mean action noise std: 0.94
                       Mean reward: 2506.48
               Mean episode length: 219.32
                 Mean success rate: 32.50
                  Mean reward/step: 12.62
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 7659520
                    Iteration time: 2.55s
                        Total time: 2398.34s
                               ETA: 7864.5s

################################################################################
                     [1m Learning iteration 935/4000 [0m

                       Computation: 3217 steps/s (collection: 0.482s, learning 2.064s)
               Value function loss: 41526.7237
                    Surrogate loss: 0.0144
             Mean action noise std: 0.94
                       Mean reward: 2446.37
               Mean episode length: 211.35
                 Mean success rate: 31.00
                  Mean reward/step: 12.92
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 2.55s
                        Total time: 2400.88s
                               ETA: 7861.9s

################################################################################
                     [1m Learning iteration 936/4000 [0m

                       Computation: 3131 steps/s (collection: 0.541s, learning 2.074s)
               Value function loss: 30801.6352
                    Surrogate loss: 0.0168
             Mean action noise std: 0.94
                       Mean reward: 2193.26
               Mean episode length: 198.88
                 Mean success rate: 29.50
                  Mean reward/step: 13.59
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7675904
                    Iteration time: 2.62s
                        Total time: 2403.50s
                               ETA: 7859.5s

################################################################################
                     [1m Learning iteration 937/4000 [0m

                       Computation: 3164 steps/s (collection: 0.526s, learning 2.063s)
               Value function loss: 57571.0034
                    Surrogate loss: 0.0158
             Mean action noise std: 0.94
                       Mean reward: 2451.82
               Mean episode length: 204.57
                 Mean success rate: 31.50
                  Mean reward/step: 14.09
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 2.59s
                        Total time: 2406.09s
                               ETA: 7857.0s

################################################################################
                     [1m Learning iteration 938/4000 [0m

                       Computation: 3217 steps/s (collection: 0.491s, learning 2.055s)
               Value function loss: 62648.7521
                    Surrogate loss: 0.0155
             Mean action noise std: 0.94
                       Mean reward: 2747.94
               Mean episode length: 214.16
                 Mean success rate: 34.50
                  Mean reward/step: 14.02
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 7692288
                    Iteration time: 2.55s
                        Total time: 2408.63s
                               ETA: 7854.4s

################################################################################
                     [1m Learning iteration 939/4000 [0m

                       Computation: 3149 steps/s (collection: 0.500s, learning 2.101s)
               Value function loss: 35629.7191
                    Surrogate loss: 0.0153
             Mean action noise std: 0.94
                       Mean reward: 2674.98
               Mean episode length: 206.36
                 Mean success rate: 32.50
                  Mean reward/step: 14.47
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 2.60s
                        Total time: 2411.23s
                               ETA: 7851.9s

################################################################################
                     [1m Learning iteration 940/4000 [0m

                       Computation: 3200 steps/s (collection: 0.470s, learning 2.089s)
               Value function loss: 54525.2380
                    Surrogate loss: 0.0175
             Mean action noise std: 0.94
                       Mean reward: 2748.95
               Mean episode length: 214.96
                 Mean success rate: 33.50
                  Mean reward/step: 14.58
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7708672
                    Iteration time: 2.56s
                        Total time: 2413.79s
                               ETA: 7849.3s

################################################################################
                     [1m Learning iteration 941/4000 [0m

                       Computation: 3161 steps/s (collection: 0.527s, learning 2.064s)
               Value function loss: 46424.5402
                    Surrogate loss: 0.0180
             Mean action noise std: 0.94
                       Mean reward: 2946.67
               Mean episode length: 224.53
                 Mean success rate: 36.50
                  Mean reward/step: 14.60
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 2.59s
                        Total time: 2416.39s
                               ETA: 7846.8s

################################################################################
                     [1m Learning iteration 942/4000 [0m

                       Computation: 3278 steps/s (collection: 0.450s, learning 2.049s)
               Value function loss: 52256.3217
                    Surrogate loss: 0.0129
             Mean action noise std: 0.94
                       Mean reward: 3377.92
               Mean episode length: 237.64
                 Mean success rate: 39.50
                  Mean reward/step: 14.72
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7725056
                    Iteration time: 2.50s
                        Total time: 2418.88s
                               ETA: 7844.1s

################################################################################
                     [1m Learning iteration 943/4000 [0m

                       Computation: 3226 steps/s (collection: 0.488s, learning 2.051s)
               Value function loss: 29676.7173
                    Surrogate loss: 0.0138
             Mean action noise std: 0.94
                       Mean reward: 3058.93
               Mean episode length: 227.90
                 Mean success rate: 36.50
                  Mean reward/step: 14.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 2.54s
                        Total time: 2421.42s
                               ETA: 7841.4s

################################################################################
                     [1m Learning iteration 944/4000 [0m

                       Computation: 3283 steps/s (collection: 0.485s, learning 2.010s)
               Value function loss: 52403.5312
                    Surrogate loss: 0.0122
             Mean action noise std: 0.94
                       Mean reward: 3107.98
               Mean episode length: 226.64
                 Mean success rate: 36.00
                  Mean reward/step: 14.83
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7741440
                    Iteration time: 2.50s
                        Total time: 2423.92s
                               ETA: 7838.6s

################################################################################
                     [1m Learning iteration 945/4000 [0m

                       Computation: 3210 steps/s (collection: 0.514s, learning 2.037s)
               Value function loss: 46123.2696
                    Surrogate loss: 0.0118
             Mean action noise std: 0.94
                       Mean reward: 3211.40
               Mean episode length: 230.52
                 Mean success rate: 37.00
                  Mean reward/step: 15.09
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 2.55s
                        Total time: 2426.47s
                               ETA: 7836.0s

################################################################################
                     [1m Learning iteration 946/4000 [0m

                       Computation: 3247 steps/s (collection: 0.496s, learning 2.027s)
               Value function loss: 67861.3760
                    Surrogate loss: 0.0125
             Mean action noise std: 0.94
                       Mean reward: 3500.19
               Mean episode length: 246.15
                 Mean success rate: 40.50
                  Mean reward/step: 15.22
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7757824
                    Iteration time: 2.52s
                        Total time: 2428.99s
                               ETA: 7833.3s

################################################################################
                     [1m Learning iteration 947/4000 [0m

                       Computation: 3199 steps/s (collection: 0.505s, learning 2.056s)
               Value function loss: 77266.5220
                    Surrogate loss: 0.0124
             Mean action noise std: 0.94
                       Mean reward: 4085.14
               Mean episode length: 268.36
                 Mean success rate: 45.50
                  Mean reward/step: 14.87
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 2.56s
                        Total time: 2431.55s
                               ETA: 7830.7s

################################################################################
                     [1m Learning iteration 948/4000 [0m

                       Computation: 3253 steps/s (collection: 0.446s, learning 2.072s)
               Value function loss: 64004.4390
                    Surrogate loss: 0.0129
             Mean action noise std: 0.94
                       Mean reward: 4159.60
               Mean episode length: 271.49
                 Mean success rate: 46.50
                  Mean reward/step: 14.68
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7774208
                    Iteration time: 2.52s
                        Total time: 2434.07s
                               ETA: 7828.0s

################################################################################
                     [1m Learning iteration 949/4000 [0m

                       Computation: 3254 steps/s (collection: 0.467s, learning 2.050s)
               Value function loss: 61581.2530
                    Surrogate loss: 0.0112
             Mean action noise std: 0.94
                       Mean reward: 4433.50
               Mean episode length: 284.68
                 Mean success rate: 48.50
                  Mean reward/step: 14.21
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 2.52s
                        Total time: 2436.59s
                               ETA: 7825.3s

################################################################################
                     [1m Learning iteration 950/4000 [0m

                       Computation: 3251 steps/s (collection: 0.457s, learning 2.063s)
               Value function loss: 63953.8653
                    Surrogate loss: 0.0152
             Mean action noise std: 0.94
                       Mean reward: 4575.88
               Mean episode length: 295.20
                 Mean success rate: 48.50
                  Mean reward/step: 13.97
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7790592
                    Iteration time: 2.52s
                        Total time: 2439.11s
                               ETA: 7822.6s

################################################################################
                     [1m Learning iteration 951/4000 [0m

                       Computation: 3191 steps/s (collection: 0.493s, learning 2.074s)
               Value function loss: 64502.5218
                    Surrogate loss: 0.0203
             Mean action noise std: 0.94
                       Mean reward: 4615.37
               Mean episode length: 299.18
                 Mean success rate: 49.50
                  Mean reward/step: 13.86
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 2.57s
                        Total time: 2441.67s
                               ETA: 7820.0s

################################################################################
                     [1m Learning iteration 952/4000 [0m

                       Computation: 3190 steps/s (collection: 0.479s, learning 2.088s)
               Value function loss: 53117.8987
                    Surrogate loss: 0.0151
             Mean action noise std: 0.94
                       Mean reward: 4332.05
               Mean episode length: 289.39
                 Mean success rate: 46.50
                  Mean reward/step: 12.91
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7806976
                    Iteration time: 2.57s
                        Total time: 2444.24s
                               ETA: 7817.5s

################################################################################
                     [1m Learning iteration 953/4000 [0m

                       Computation: 3180 steps/s (collection: 0.494s, learning 2.082s)
               Value function loss: 57544.4612
                    Surrogate loss: 0.0129
             Mean action noise std: 0.94
                       Mean reward: 4184.59
               Mean episode length: 289.90
                 Mean success rate: 44.50
                  Mean reward/step: 12.50
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 2.58s
                        Total time: 2446.82s
                               ETA: 7814.9s

################################################################################
                     [1m Learning iteration 954/4000 [0m

                       Computation: 3179 steps/s (collection: 0.482s, learning 2.094s)
               Value function loss: 68542.0546
                    Surrogate loss: 0.0129
             Mean action noise std: 0.94
                       Mean reward: 4171.97
               Mean episode length: 292.87
                 Mean success rate: 44.50
                  Mean reward/step: 11.60
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7823360
                    Iteration time: 2.58s
                        Total time: 2449.39s
                               ETA: 7812.4s

################################################################################
                     [1m Learning iteration 955/4000 [0m

                       Computation: 3246 steps/s (collection: 0.461s, learning 2.062s)
               Value function loss: 58641.4375
                    Surrogate loss: 0.0132
             Mean action noise std: 0.94
                       Mean reward: 4061.79
               Mean episode length: 288.32
                 Mean success rate: 42.50
                  Mean reward/step: 11.68
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 2.52s
                        Total time: 2451.92s
                               ETA: 7809.7s

################################################################################
                     [1m Learning iteration 956/4000 [0m

                       Computation: 3130 steps/s (collection: 0.503s, learning 2.114s)
               Value function loss: 51070.6406
                    Surrogate loss: 0.0138
             Mean action noise std: 0.94
                       Mean reward: 3644.30
               Mean episode length: 266.08
                 Mean success rate: 39.00
                  Mean reward/step: 11.51
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7839744
                    Iteration time: 2.62s
                        Total time: 2454.53s
                               ETA: 7807.3s

################################################################################
                     [1m Learning iteration 957/4000 [0m

                       Computation: 3082 steps/s (collection: 0.535s, learning 2.123s)
               Value function loss: 47738.9049
                    Surrogate loss: 0.0127
             Mean action noise std: 0.94
                       Mean reward: 3699.20
               Mean episode length: 271.85
                 Mean success rate: 39.00
                  Mean reward/step: 11.25
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 2.66s
                        Total time: 2457.19s
                               ETA: 7805.0s

################################################################################
                     [1m Learning iteration 958/4000 [0m

                       Computation: 3208 steps/s (collection: 0.500s, learning 2.054s)
               Value function loss: 59168.6744
                    Surrogate loss: 0.0133
             Mean action noise std: 0.94
                       Mean reward: 3368.82
               Mean episode length: 262.00
                 Mean success rate: 37.00
                  Mean reward/step: 11.43
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 7856128
                    Iteration time: 2.55s
                        Total time: 2459.75s
                               ETA: 7802.4s

################################################################################
                     [1m Learning iteration 959/4000 [0m

                       Computation: 3221 steps/s (collection: 0.475s, learning 2.068s)
               Value function loss: 41867.1844
                    Surrogate loss: 0.0143
             Mean action noise std: 0.94
                       Mean reward: 3178.58
               Mean episode length: 249.20
                 Mean success rate: 35.00
                  Mean reward/step: 11.43
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 2.54s
                        Total time: 2462.29s
                               ETA: 7799.8s

################################################################################
                     [1m Learning iteration 960/4000 [0m

                       Computation: 3262 steps/s (collection: 0.460s, learning 2.051s)
               Value function loss: 47787.2755
                    Surrogate loss: 0.0155
             Mean action noise std: 0.94
                       Mean reward: 2520.19
               Mean episode length: 221.66
                 Mean success rate: 28.50
                  Mean reward/step: 12.15
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 7872512
                    Iteration time: 2.51s
                        Total time: 2464.80s
                               ETA: 7797.1s

################################################################################
                     [1m Learning iteration 961/4000 [0m

                       Computation: 3289 steps/s (collection: 0.465s, learning 2.026s)
               Value function loss: 37790.6960
                    Surrogate loss: 0.0170
             Mean action noise std: 0.94
                       Mean reward: 2414.00
               Mean episode length: 218.69
                 Mean success rate: 27.00
                  Mean reward/step: 12.85
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 2.49s
                        Total time: 2467.29s
                               ETA: 7794.3s

################################################################################
                     [1m Learning iteration 962/4000 [0m

                       Computation: 3243 steps/s (collection: 0.483s, learning 2.042s)
               Value function loss: 39980.0184
                    Surrogate loss: 0.0152
             Mean action noise std: 0.94
                       Mean reward: 2344.89
               Mean episode length: 215.96
                 Mean success rate: 26.50
                  Mean reward/step: 13.64
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7888896
                    Iteration time: 2.53s
                        Total time: 2469.82s
                               ETA: 7791.6s

################################################################################
                     [1m Learning iteration 963/4000 [0m

                       Computation: 3256 steps/s (collection: 0.466s, learning 2.049s)
               Value function loss: 42737.0750
                    Surrogate loss: 0.0129
             Mean action noise std: 0.93
                       Mean reward: 2539.88
               Mean episode length: 221.22
                 Mean success rate: 28.00
                  Mean reward/step: 14.90
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 2.52s
                        Total time: 2472.33s
                               ETA: 7788.9s

################################################################################
                     [1m Learning iteration 964/4000 [0m

                       Computation: 3309 steps/s (collection: 0.443s, learning 2.032s)
               Value function loss: 42550.9164
                    Surrogate loss: 0.0122
             Mean action noise std: 0.93
                       Mean reward: 2465.09
               Mean episode length: 217.66
                 Mean success rate: 25.50
                  Mean reward/step: 15.34
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7905280
                    Iteration time: 2.48s
                        Total time: 2474.81s
                               ETA: 7786.0s

################################################################################
                     [1m Learning iteration 965/4000 [0m

                       Computation: 3268 steps/s (collection: 0.456s, learning 2.051s)
               Value function loss: 62959.2925
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 2857.12
               Mean episode length: 237.75
                 Mean success rate: 30.00
                  Mean reward/step: 15.51
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 2.51s
                        Total time: 2477.31s
                               ETA: 7783.3s

################################################################################
                     [1m Learning iteration 966/4000 [0m

                       Computation: 3265 steps/s (collection: 0.450s, learning 2.058s)
               Value function loss: 65797.6305
                    Surrogate loss: 0.0125
             Mean action noise std: 0.93
                       Mean reward: 2873.12
               Mean episode length: 241.78
                 Mean success rate: 29.50
                  Mean reward/step: 15.42
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7921664
                    Iteration time: 2.51s
                        Total time: 2479.82s
                               ETA: 7780.5s

################################################################################
                     [1m Learning iteration 967/4000 [0m

                       Computation: 3240 steps/s (collection: 0.477s, learning 2.051s)
               Value function loss: 66638.1328
                    Surrogate loss: 0.0128
             Mean action noise std: 0.93
                       Mean reward: 3299.37
               Mean episode length: 256.90
                 Mean success rate: 35.00
                  Mean reward/step: 15.22
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 2.53s
                        Total time: 2482.35s
                               ETA: 7777.9s

################################################################################
                     [1m Learning iteration 968/4000 [0m

                       Computation: 3255 steps/s (collection: 0.488s, learning 2.029s)
               Value function loss: 59402.4854
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 3336.25
               Mean episode length: 254.31
                 Mean success rate: 36.50
                  Mean reward/step: 14.63
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7938048
                    Iteration time: 2.52s
                        Total time: 2484.87s
                               ETA: 7775.1s

################################################################################
                     [1m Learning iteration 969/4000 [0m

                       Computation: 3343 steps/s (collection: 0.432s, learning 2.017s)
               Value function loss: 63761.9624
                    Surrogate loss: 0.0120
             Mean action noise std: 0.93
                       Mean reward: 3490.03
               Mean episode length: 255.76
                 Mean success rate: 38.00
                  Mean reward/step: 14.14
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 2.45s
                        Total time: 2487.32s
                               ETA: 7772.2s

################################################################################
                     [1m Learning iteration 970/4000 [0m

                       Computation: 3355 steps/s (collection: 0.420s, learning 2.022s)
               Value function loss: 52461.8343
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 3414.67
               Mean episode length: 253.59
                 Mean success rate: 38.00
                  Mean reward/step: 14.77
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7954432
                    Iteration time: 2.44s
                        Total time: 2489.76s
                               ETA: 7769.3s

################################################################################
                     [1m Learning iteration 971/4000 [0m

                       Computation: 3337 steps/s (collection: 0.450s, learning 2.005s)
               Value function loss: 47709.0664
                    Surrogate loss: 0.0195
             Mean action noise std: 0.93
                       Mean reward: 3346.09
               Mean episode length: 255.43
                 Mean success rate: 37.50
                  Mean reward/step: 14.82
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 2.45s
                        Total time: 2492.21s
                               ETA: 7766.4s

################################################################################
                     [1m Learning iteration 972/4000 [0m

                       Computation: 3378 steps/s (collection: 0.433s, learning 1.992s)
               Value function loss: 76507.7090
                    Surrogate loss: 0.0132
             Mean action noise std: 0.93
                       Mean reward: 3261.52
               Mean episode length: 241.69
                 Mean success rate: 37.00
                  Mean reward/step: 14.71
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7970816
                    Iteration time: 2.43s
                        Total time: 2494.64s
                               ETA: 7763.4s

################################################################################
                     [1m Learning iteration 973/4000 [0m

                       Computation: 3330 steps/s (collection: 0.460s, learning 2.000s)
               Value function loss: 69744.5101
                    Surrogate loss: 0.0111
             Mean action noise std: 0.93
                       Mean reward: 3576.22
               Mean episode length: 250.65
                 Mean success rate: 38.50
                  Mean reward/step: 14.77
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 2.46s
                        Total time: 2497.10s
                               ETA: 7760.5s

################################################################################
                     [1m Learning iteration 974/4000 [0m

                       Computation: 3347 steps/s (collection: 0.454s, learning 1.993s)
               Value function loss: 67829.0581
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 3882.46
               Mean episode length: 263.94
                 Mean success rate: 42.00
                  Mean reward/step: 14.23
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7987200
                    Iteration time: 2.45s
                        Total time: 2499.54s
                               ETA: 7757.6s

################################################################################
                     [1m Learning iteration 975/4000 [0m

                       Computation: 3265 steps/s (collection: 0.509s, learning 2.000s)
               Value function loss: 67061.8080
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 3964.50
               Mean episode length: 266.56
                 Mean success rate: 43.00
                  Mean reward/step: 14.32
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 2.51s
                        Total time: 2502.05s
                               ETA: 7754.8s

################################################################################
                     [1m Learning iteration 976/4000 [0m

                       Computation: 3204 steps/s (collection: 0.554s, learning 2.002s)
               Value function loss: 63308.3010
                    Surrogate loss: 0.0128
             Mean action noise std: 0.93
                       Mean reward: 4147.19
               Mean episode length: 264.85
                 Mean success rate: 44.00
                  Mean reward/step: 13.98
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8003584
                    Iteration time: 2.56s
                        Total time: 2504.61s
                               ETA: 7752.2s

################################################################################
                     [1m Learning iteration 977/4000 [0m

                       Computation: 3405 steps/s (collection: 0.414s, learning 1.991s)
               Value function loss: 44610.9175
                    Surrogate loss: 0.0128
             Mean action noise std: 0.93
                       Mean reward: 4293.05
               Mean episode length: 270.86
                 Mean success rate: 45.50
                  Mean reward/step: 14.34
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 2.41s
                        Total time: 2507.01s
                               ETA: 7749.2s

################################################################################
                     [1m Learning iteration 978/4000 [0m

                       Computation: 3376 steps/s (collection: 0.427s, learning 1.999s)
               Value function loss: 58370.3016
                    Surrogate loss: 0.0128
             Mean action noise std: 0.93
                       Mean reward: 4358.61
               Mean episode length: 270.04
                 Mean success rate: 46.00
                  Mean reward/step: 14.70
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8019968
                    Iteration time: 2.43s
                        Total time: 2509.44s
                               ETA: 7746.2s

################################################################################
                     [1m Learning iteration 979/4000 [0m

                       Computation: 3406 steps/s (collection: 0.421s, learning 1.984s)
               Value function loss: 50814.8403
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 4063.56
               Mean episode length: 258.81
                 Mean success rate: 44.00
                  Mean reward/step: 15.64
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 2.40s
                        Total time: 2511.84s
                               ETA: 7743.1s

################################################################################
                     [1m Learning iteration 980/4000 [0m

                       Computation: 3390 steps/s (collection: 0.418s, learning 1.998s)
               Value function loss: 49953.4188
                    Surrogate loss: 0.0159
             Mean action noise std: 0.93
                       Mean reward: 3746.91
               Mean episode length: 244.75
                 Mean success rate: 41.00
                  Mean reward/step: 15.62
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8036352
                    Iteration time: 2.42s
                        Total time: 2514.26s
                               ETA: 7740.1s

################################################################################
                     [1m Learning iteration 981/4000 [0m

                       Computation: 3407 steps/s (collection: 0.417s, learning 1.987s)
               Value function loss: 45381.0491
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 3644.02
               Mean episode length: 242.56
                 Mean success rate: 39.50
                  Mean reward/step: 16.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 2.40s
                        Total time: 2516.66s
                               ETA: 7737.1s

################################################################################
                     [1m Learning iteration 982/4000 [0m

                       Computation: 3389 steps/s (collection: 0.422s, learning 1.995s)
               Value function loss: 79815.0744
                    Surrogate loss: 0.0140
             Mean action noise std: 0.93
                       Mean reward: 4034.66
               Mean episode length: 257.00
                 Mean success rate: 42.00
                  Mean reward/step: 16.15
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8052736
                    Iteration time: 2.42s
                        Total time: 2519.08s
                               ETA: 7734.1s

################################################################################
                     [1m Learning iteration 983/4000 [0m

                       Computation: 3387 steps/s (collection: 0.417s, learning 2.001s)
               Value function loss: 63370.9705
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 3769.10
               Mean episode length: 256.17
                 Mean success rate: 40.00
                  Mean reward/step: 16.42
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 2.42s
                        Total time: 2521.50s
                               ETA: 7731.1s

################################################################################
                     [1m Learning iteration 984/4000 [0m

                       Computation: 3238 steps/s (collection: 0.486s, learning 2.043s)
               Value function loss: 53256.2458
                    Surrogate loss: 0.0133
             Mean action noise std: 0.93
                       Mean reward: 3486.02
               Mean episode length: 246.88
                 Mean success rate: 38.00
                  Mean reward/step: 15.97
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8069120
                    Iteration time: 2.53s
                        Total time: 2524.03s
                               ETA: 7728.4s

################################################################################
                     [1m Learning iteration 985/4000 [0m

                       Computation: 3216 steps/s (collection: 0.469s, learning 2.078s)
               Value function loss: 70224.1778
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 3654.82
               Mean episode length: 253.66
                 Mean success rate: 40.00
                  Mean reward/step: 16.02
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 2.55s
                        Total time: 2526.58s
                               ETA: 7725.8s

################################################################################
                     [1m Learning iteration 986/4000 [0m

                       Computation: 3272 steps/s (collection: 0.457s, learning 2.046s)
               Value function loss: 63306.5080
                    Surrogate loss: 0.0129
             Mean action noise std: 0.93
                       Mean reward: 3930.93
               Mean episode length: 265.86
                 Mean success rate: 43.00
                  Mean reward/step: 15.66
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8085504
                    Iteration time: 2.50s
                        Total time: 2529.08s
                               ETA: 7723.0s

################################################################################
                     [1m Learning iteration 987/4000 [0m

                       Computation: 3288 steps/s (collection: 0.473s, learning 2.018s)
               Value function loss: 72549.2772
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 4117.21
               Mean episode length: 271.84
                 Mean success rate: 45.00
                  Mean reward/step: 15.72
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 2.49s
                        Total time: 2531.57s
                               ETA: 7720.3s

################################################################################
                     [1m Learning iteration 988/4000 [0m

                       Computation: 3219 steps/s (collection: 0.479s, learning 2.066s)
               Value function loss: 71772.3115
                    Surrogate loss: 0.0135
             Mean action noise std: 0.93
                       Mean reward: 4030.90
               Mean episode length: 262.60
                 Mean success rate: 44.50
                  Mean reward/step: 15.29
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8101888
                    Iteration time: 2.54s
                        Total time: 2534.12s
                               ETA: 7717.6s

################################################################################
                     [1m Learning iteration 989/4000 [0m

                       Computation: 3290 steps/s (collection: 0.444s, learning 2.046s)
               Value function loss: 62424.0860
                    Surrogate loss: 0.0131
             Mean action noise std: 0.93
                       Mean reward: 4085.24
               Mean episode length: 265.28
                 Mean success rate: 46.50
                  Mean reward/step: 15.10
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 2.49s
                        Total time: 2536.61s
                               ETA: 7714.9s

################################################################################
                     [1m Learning iteration 990/4000 [0m

                       Computation: 3162 steps/s (collection: 0.545s, learning 2.045s)
               Value function loss: 45434.3716
                    Surrogate loss: 0.0159
             Mean action noise std: 0.93
                       Mean reward: 3878.73
               Mean episode length: 257.98
                 Mean success rate: 45.00
                  Mean reward/step: 15.57
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8118272
                    Iteration time: 2.59s
                        Total time: 2539.20s
                               ETA: 7712.4s

################################################################################
                     [1m Learning iteration 991/4000 [0m

                       Computation: 3235 steps/s (collection: 0.490s, learning 2.042s)
               Value function loss: 65262.6280
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 4137.44
               Mean episode length: 264.63
                 Mean success rate: 47.00
                  Mean reward/step: 15.94
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 2.53s
                        Total time: 2541.73s
                               ETA: 7709.7s

################################################################################
                     [1m Learning iteration 992/4000 [0m

                       Computation: 3273 steps/s (collection: 0.456s, learning 2.047s)
               Value function loss: 67130.7737
                    Surrogate loss: 0.0152
             Mean action noise std: 0.93
                       Mean reward: 4158.29
               Mean episode length: 257.77
                 Mean success rate: 46.00
                  Mean reward/step: 16.17
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8134656
                    Iteration time: 2.50s
                        Total time: 2544.23s
                               ETA: 7707.0s

################################################################################
                     [1m Learning iteration 993/4000 [0m

                       Computation: 3309 steps/s (collection: 0.436s, learning 2.039s)
               Value function loss: 50791.8196
                    Surrogate loss: 0.0135
             Mean action noise std: 0.93
                       Mean reward: 4159.21
               Mean episode length: 260.25
                 Mean success rate: 46.00
                  Mean reward/step: 16.66
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 2.48s
                        Total time: 2546.71s
                               ETA: 7704.2s

################################################################################
                     [1m Learning iteration 994/4000 [0m

                       Computation: 3217 steps/s (collection: 0.493s, learning 2.053s)
               Value function loss: 44085.0318
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 3825.79
               Mean episode length: 244.13
                 Mean success rate: 42.00
                  Mean reward/step: 16.48
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8151040
                    Iteration time: 2.55s
                        Total time: 2549.25s
                               ETA: 7701.6s

################################################################################
                     [1m Learning iteration 995/4000 [0m

                       Computation: 3139 steps/s (collection: 0.490s, learning 2.119s)
               Value function loss: 63819.0961
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 3916.73
               Mean episode length: 251.32
                 Mean success rate: 43.00
                  Mean reward/step: 16.40
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 2.61s
                        Total time: 2551.86s
                               ETA: 7699.1s

################################################################################
                     [1m Learning iteration 996/4000 [0m

                       Computation: 3146 steps/s (collection: 0.432s, learning 2.171s)
               Value function loss: 58439.5728
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 3981.30
               Mean episode length: 250.75
                 Mean success rate: 42.50
                  Mean reward/step: 17.00
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8167424
                    Iteration time: 2.60s
                        Total time: 2554.46s
                               ETA: 7696.7s

################################################################################
                     [1m Learning iteration 997/4000 [0m

                       Computation: 3087 steps/s (collection: 0.431s, learning 2.223s)
               Value function loss: 58507.1502
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 4348.52
               Mean episode length: 263.88
                 Mean success rate: 45.00
                  Mean reward/step: 17.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 2.65s
                        Total time: 2557.12s
                               ETA: 7694.4s

################################################################################
                     [1m Learning iteration 998/4000 [0m

                       Computation: 3048 steps/s (collection: 0.496s, learning 2.191s)
               Value function loss: 75925.5854
                    Surrogate loss: 0.0144
             Mean action noise std: 0.93
                       Mean reward: 4304.27
               Mean episode length: 264.66
                 Mean success rate: 44.00
                  Mean reward/step: 17.30
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8183808
                    Iteration time: 2.69s
                        Total time: 2559.80s
                               ETA: 7692.2s

################################################################################
                     [1m Learning iteration 999/4000 [0m

                       Computation: 3142 steps/s (collection: 0.467s, learning 2.140s)
               Value function loss: 100194.6624
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 4490.31
               Mean episode length: 276.21
                 Mean success rate: 46.00
                  Mean reward/step: 16.11
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 2.61s
                        Total time: 2562.41s
                               ETA: 7689.8s

################################################################################
                     [1m Learning iteration 1000/4000 [0m

                       Computation: 3203 steps/s (collection: 0.470s, learning 2.088s)
               Value function loss: 70135.0257
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 4583.98
               Mean episode length: 281.69
                 Mean success rate: 47.50
                  Mean reward/step: 14.98
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8200192
                    Iteration time: 2.56s
                        Total time: 2564.97s
                               ETA: 7687.2s

################################################################################
                     [1m Learning iteration 1001/4000 [0m

                       Computation: 3201 steps/s (collection: 0.451s, learning 2.108s)
               Value function loss: 33750.1370
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 4503.99
               Mean episode length: 281.44
                 Mean success rate: 47.00
                  Mean reward/step: 15.32
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 2.56s
                        Total time: 2567.53s
                               ETA: 7684.6s

################################################################################
                     [1m Learning iteration 1002/4000 [0m

                       Computation: 3045 steps/s (collection: 0.514s, learning 2.176s)
               Value function loss: 78931.5389
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 4628.18
               Mean episode length: 284.31
                 Mean success rate: 47.50
                  Mean reward/step: 16.20
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8216576
                    Iteration time: 2.69s
                        Total time: 2570.22s
                               ETA: 7682.5s

################################################################################
                     [1m Learning iteration 1003/4000 [0m

                       Computation: 3120 steps/s (collection: 0.539s, learning 2.086s)
               Value function loss: 79655.0806
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 4485.30
               Mean episode length: 281.48
                 Mean success rate: 47.50
                  Mean reward/step: 15.89
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 2.63s
                        Total time: 2572.84s
                               ETA: 7680.1s

################################################################################
                     [1m Learning iteration 1004/4000 [0m

                       Computation: 3123 steps/s (collection: 0.484s, learning 2.138s)
               Value function loss: 57986.2931
                    Surrogate loss: 0.0159
             Mean action noise std: 0.93
                       Mean reward: 4437.97
               Mean episode length: 280.76
                 Mean success rate: 47.00
                  Mean reward/step: 15.25
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8232960
                    Iteration time: 2.62s
                        Total time: 2575.47s
                               ETA: 7677.7s

################################################################################
                     [1m Learning iteration 1005/4000 [0m

                       Computation: 3131 steps/s (collection: 0.518s, learning 2.099s)
               Value function loss: 69713.8147
                    Surrogate loss: 0.0161
             Mean action noise std: 0.93
                       Mean reward: 4321.47
               Mean episode length: 272.63
                 Mean success rate: 46.00
                  Mean reward/step: 15.12
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 2.62s
                        Total time: 2578.08s
                               ETA: 7675.3s

################################################################################
                     [1m Learning iteration 1006/4000 [0m

                       Computation: 3194 steps/s (collection: 0.471s, learning 2.094s)
               Value function loss: 63453.3241
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 4337.64
               Mean episode length: 268.32
                 Mean success rate: 45.50
                  Mean reward/step: 14.95
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8249344
                    Iteration time: 2.56s
                        Total time: 2580.65s
                               ETA: 7672.7s

################################################################################
                     [1m Learning iteration 1007/4000 [0m

                       Computation: 3240 steps/s (collection: 0.458s, learning 2.070s)
               Value function loss: 80549.6150
                    Surrogate loss: 0.0199
             Mean action noise std: 0.93
                       Mean reward: 4491.48
               Mean episode length: 264.82
                 Mean success rate: 46.00
                  Mean reward/step: 15.89
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 2.53s
                        Total time: 2583.17s
                               ETA: 7670.1s

################################################################################
                     [1m Learning iteration 1008/4000 [0m

                       Computation: 3241 steps/s (collection: 0.474s, learning 2.053s)
               Value function loss: 49371.1287
                    Surrogate loss: 0.0251
             Mean action noise std: 0.93
                       Mean reward: 4302.84
               Mean episode length: 261.54
                 Mean success rate: 44.50
                  Mean reward/step: 15.81
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8265728
                    Iteration time: 2.53s
                        Total time: 2585.70s
                               ETA: 7667.4s

################################################################################
                     [1m Learning iteration 1009/4000 [0m

                       Computation: 3227 steps/s (collection: 0.475s, learning 2.063s)
               Value function loss: 79964.3236
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 4431.13
               Mean episode length: 265.45
                 Mean success rate: 47.00
                  Mean reward/step: 15.74
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 2.54s
                        Total time: 2588.24s
                               ETA: 7664.8s

################################################################################
                     [1m Learning iteration 1010/4000 [0m

                       Computation: 3214 steps/s (collection: 0.475s, learning 2.074s)
               Value function loss: 65409.3471
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 4501.41
               Mean episode length: 271.53
                 Mean success rate: 48.00
                  Mean reward/step: 15.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8282112
                    Iteration time: 2.55s
                        Total time: 2590.79s
                               ETA: 7662.2s

################################################################################
                     [1m Learning iteration 1011/4000 [0m

                       Computation: 3225 steps/s (collection: 0.489s, learning 2.051s)
               Value function loss: 56073.6278
                    Surrogate loss: 0.0170
             Mean action noise std: 0.93
                       Mean reward: 4419.84
               Mean episode length: 266.69
                 Mean success rate: 47.50
                  Mean reward/step: 15.90
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 2.54s
                        Total time: 2593.33s
                               ETA: 7659.5s

################################################################################
                     [1m Learning iteration 1012/4000 [0m

                       Computation: 3236 steps/s (collection: 0.441s, learning 2.090s)
               Value function loss: 54695.6955
                    Surrogate loss: 0.0176
             Mean action noise std: 0.93
                       Mean reward: 4431.45
               Mean episode length: 276.38
                 Mean success rate: 48.50
                  Mean reward/step: 16.51
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8298496
                    Iteration time: 2.53s
                        Total time: 2595.86s
                               ETA: 7656.9s

################################################################################
                     [1m Learning iteration 1013/4000 [0m

                       Computation: 3286 steps/s (collection: 0.449s, learning 2.043s)
               Value function loss: 64839.2934
                    Surrogate loss: 0.0115
             Mean action noise std: 0.93
                       Mean reward: 4314.74
               Mean episode length: 280.63
                 Mean success rate: 49.00
                  Mean reward/step: 16.57
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 2.49s
                        Total time: 2598.35s
                               ETA: 7654.1s

################################################################################
                     [1m Learning iteration 1014/4000 [0m

                       Computation: 3186 steps/s (collection: 0.464s, learning 2.107s)
               Value function loss: 62131.2411
                    Surrogate loss: 0.0118
             Mean action noise std: 0.93
                       Mean reward: 3923.65
               Mean episode length: 270.99
                 Mean success rate: 44.50
                  Mean reward/step: 16.08
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8314880
                    Iteration time: 2.57s
                        Total time: 2600.92s
                               ETA: 7651.6s

################################################################################
                     [1m Learning iteration 1015/4000 [0m

                       Computation: 3264 steps/s (collection: 0.441s, learning 2.069s)
               Value function loss: 79894.7475
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 4384.24
               Mean episode length: 285.41
                 Mean success rate: 48.50
                  Mean reward/step: 16.10
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 2.51s
                        Total time: 2603.43s
                               ETA: 7648.9s

################################################################################
                     [1m Learning iteration 1016/4000 [0m

                       Computation: 3283 steps/s (collection: 0.461s, learning 2.033s)
               Value function loss: 65592.9269
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 4447.11
               Mean episode length: 284.45
                 Mean success rate: 48.00
                  Mean reward/step: 15.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8331264
                    Iteration time: 2.49s
                        Total time: 2605.93s
                               ETA: 7646.1s

################################################################################
                     [1m Learning iteration 1017/4000 [0m

                       Computation: 3245 steps/s (collection: 0.470s, learning 2.054s)
               Value function loss: 29303.7605
                    Surrogate loss: 0.0133
             Mean action noise std: 0.93
                       Mean reward: 4166.14
               Mean episode length: 275.57
                 Mean success rate: 45.50
                  Mean reward/step: 16.23
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 2.52s
                        Total time: 2608.45s
                               ETA: 7643.4s

################################################################################
                     [1m Learning iteration 1018/4000 [0m

                       Computation: 3268 steps/s (collection: 0.459s, learning 2.047s)
               Value function loss: 98360.1186
                    Surrogate loss: 0.0131
             Mean action noise std: 0.93
                       Mean reward: 4426.75
               Mean episode length: 287.68
                 Mean success rate: 48.00
                  Mean reward/step: 16.75
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8347648
                    Iteration time: 2.51s
                        Total time: 2610.96s
                               ETA: 7640.7s

################################################################################
                     [1m Learning iteration 1019/4000 [0m

                       Computation: 3076 steps/s (collection: 0.545s, learning 2.117s)
               Value function loss: 73174.0915
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 4757.91
               Mean episode length: 295.27
                 Mean success rate: 50.50
                  Mean reward/step: 15.93
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 2.66s
                        Total time: 2613.62s
                               ETA: 7638.4s

################################################################################
                     [1m Learning iteration 1020/4000 [0m

                       Computation: 3209 steps/s (collection: 0.478s, learning 2.075s)
               Value function loss: 65170.5144
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 4963.35
               Mean episode length: 300.40
                 Mean success rate: 51.50
                  Mean reward/step: 15.37
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8364032
                    Iteration time: 2.55s
                        Total time: 2616.17s
                               ETA: 7635.8s

################################################################################
                     [1m Learning iteration 1021/4000 [0m

                       Computation: 3205 steps/s (collection: 0.479s, learning 2.076s)
               Value function loss: 78873.5029
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 5245.24
               Mean episode length: 311.75
                 Mean success rate: 55.50
                  Mean reward/step: 15.55
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 2.56s
                        Total time: 2618.73s
                               ETA: 7633.3s

################################################################################
                     [1m Learning iteration 1022/4000 [0m

                       Computation: 3002 steps/s (collection: 0.593s, learning 2.136s)
               Value function loss: 52536.1784
                    Surrogate loss: 0.0159
             Mean action noise std: 0.93
                       Mean reward: 5124.38
               Mean episode length: 310.03
                 Mean success rate: 55.00
                  Mean reward/step: 15.60
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8380416
                    Iteration time: 2.73s
                        Total time: 2621.46s
                               ETA: 7631.2s

################################################################################
                     [1m Learning iteration 1023/4000 [0m

                       Computation: 3103 steps/s (collection: 0.547s, learning 2.093s)
               Value function loss: 58445.7745
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 5045.23
               Mean episode length: 305.53
                 Mean success rate: 53.50
                  Mean reward/step: 16.04
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 2.64s
                        Total time: 2624.10s
                               ETA: 7628.8s

################################################################################
                     [1m Learning iteration 1024/4000 [0m

                       Computation: 3162 steps/s (collection: 0.468s, learning 2.122s)
               Value function loss: 73003.4286
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 5415.25
               Mean episode length: 323.17
                 Mean success rate: 57.50
                  Mean reward/step: 16.46
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8396800
                    Iteration time: 2.59s
                        Total time: 2626.69s
                               ETA: 7626.4s

################################################################################
                     [1m Learning iteration 1025/4000 [0m

                       Computation: 3231 steps/s (collection: 0.463s, learning 2.072s)
               Value function loss: 80418.0939
                    Surrogate loss: 0.0133
             Mean action noise std: 0.93
                       Mean reward: 5458.26
               Mean episode length: 327.69
                 Mean success rate: 57.50
                  Mean reward/step: 16.88
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 2.53s
                        Total time: 2629.22s
                               ETA: 7623.7s

################################################################################
                     [1m Learning iteration 1026/4000 [0m

                       Computation: 3074 steps/s (collection: 0.526s, learning 2.138s)
               Value function loss: 63805.7960
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 5193.26
               Mean episode length: 317.89
                 Mean success rate: 56.00
                  Mean reward/step: 16.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8413184
                    Iteration time: 2.66s
                        Total time: 2631.89s
                               ETA: 7621.5s

################################################################################
                     [1m Learning iteration 1027/4000 [0m

                       Computation: 3184 steps/s (collection: 0.482s, learning 2.091s)
               Value function loss: 52628.3957
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 5367.50
               Mean episode length: 326.13
                 Mean success rate: 59.00
                  Mean reward/step: 16.65
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 2.57s
                        Total time: 2634.46s
                               ETA: 7618.9s

################################################################################
                     [1m Learning iteration 1028/4000 [0m

                       Computation: 3202 steps/s (collection: 0.488s, learning 2.070s)
               Value function loss: 76698.2938
                    Surrogate loss: 0.0132
             Mean action noise std: 0.93
                       Mean reward: 5093.59
               Mean episode length: 316.88
                 Mean success rate: 57.00
                  Mean reward/step: 16.33
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8429568
                    Iteration time: 2.56s
                        Total time: 2637.02s
                               ETA: 7616.3s

################################################################################
                     [1m Learning iteration 1029/4000 [0m

                       Computation: 3204 steps/s (collection: 0.458s, learning 2.098s)
               Value function loss: 69765.2396
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 5184.13
               Mean episode length: 323.51
                 Mean success rate: 59.00
                  Mean reward/step: 16.41
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 2.56s
                        Total time: 2639.57s
                               ETA: 7613.8s

################################################################################
                     [1m Learning iteration 1030/4000 [0m

                       Computation: 3216 steps/s (collection: 0.449s, learning 2.099s)
               Value function loss: 61946.7120
                    Surrogate loss: 0.0172
             Mean action noise std: 0.93
                       Mean reward: 5133.26
               Mean episode length: 321.69
                 Mean success rate: 58.50
                  Mean reward/step: 16.25
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8445952
                    Iteration time: 2.55s
                        Total time: 2642.12s
                               ETA: 7611.2s

################################################################################
                     [1m Learning iteration 1031/4000 [0m

                       Computation: 3221 steps/s (collection: 0.474s, learning 2.068s)
               Value function loss: 52997.0705
                    Surrogate loss: 0.0131
             Mean action noise std: 0.93
                       Mean reward: 4924.97
               Mean episode length: 309.75
                 Mean success rate: 56.50
                  Mean reward/step: 16.00
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 2.54s
                        Total time: 2644.66s
                               ETA: 7608.5s

################################################################################
                     [1m Learning iteration 1032/4000 [0m

                       Computation: 3156 steps/s (collection: 0.507s, learning 2.088s)
               Value function loss: 73251.5500
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 4931.06
               Mean episode length: 307.36
                 Mean success rate: 57.00
                  Mean reward/step: 15.71
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8462336
                    Iteration time: 2.60s
                        Total time: 2647.26s
                               ETA: 7606.1s

################################################################################
                     [1m Learning iteration 1033/4000 [0m

                       Computation: 3186 steps/s (collection: 0.475s, learning 2.096s)
               Value function loss: 72167.2422
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 4692.97
               Mean episode length: 298.17
                 Mean success rate: 56.00
                  Mean reward/step: 15.36
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 2.57s
                        Total time: 2649.83s
                               ETA: 7603.5s

################################################################################
                     [1m Learning iteration 1034/4000 [0m

                       Computation: 3168 steps/s (collection: 0.505s, learning 2.080s)
               Value function loss: 82755.4302
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 4799.23
               Mean episode length: 298.00
                 Mean success rate: 55.50
                  Mean reward/step: 15.06
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8478720
                    Iteration time: 2.59s
                        Total time: 2652.42s
                               ETA: 7601.0s

################################################################################
                     [1m Learning iteration 1035/4000 [0m

                       Computation: 3223 steps/s (collection: 0.455s, learning 2.086s)
               Value function loss: 78850.3923
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 5158.19
               Mean episode length: 316.69
                 Mean success rate: 59.50
                  Mean reward/step: 15.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 2.54s
                        Total time: 2654.96s
                               ETA: 7598.4s

################################################################################
                     [1m Learning iteration 1036/4000 [0m

                       Computation: 3248 steps/s (collection: 0.445s, learning 2.077s)
               Value function loss: 66461.3426
                    Surrogate loss: 0.0165
             Mean action noise std: 0.93
                       Mean reward: 4943.02
               Mean episode length: 305.94
                 Mean success rate: 57.00
                  Mean reward/step: 15.66
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8495104
                    Iteration time: 2.52s
                        Total time: 2657.48s
                               ETA: 7595.7s

################################################################################
                     [1m Learning iteration 1037/4000 [0m

                       Computation: 3189 steps/s (collection: 0.482s, learning 2.086s)
               Value function loss: 59878.3949
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 5132.84
               Mean episode length: 312.54
                 Mean success rate: 58.50
                  Mean reward/step: 15.34
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 2.57s
                        Total time: 2660.05s
                               ETA: 7593.2s

################################################################################
                     [1m Learning iteration 1038/4000 [0m

                       Computation: 3176 steps/s (collection: 0.503s, learning 2.076s)
               Value function loss: 66147.2291
                    Surrogate loss: 0.0123
             Mean action noise std: 0.93
                       Mean reward: 5119.58
               Mean episode length: 314.80
                 Mean success rate: 58.50
                  Mean reward/step: 15.61
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8511488
                    Iteration time: 2.58s
                        Total time: 2662.62s
                               ETA: 7590.7s

################################################################################
                     [1m Learning iteration 1039/4000 [0m

                       Computation: 3183 steps/s (collection: 0.492s, learning 2.081s)
               Value function loss: 68856.0317
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 5374.17
               Mean episode length: 330.25
                 Mean success rate: 62.00
                  Mean reward/step: 15.06
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 2.57s
                        Total time: 2665.20s
                               ETA: 7588.1s

################################################################################
                     [1m Learning iteration 1040/4000 [0m

                       Computation: 3236 steps/s (collection: 0.457s, learning 2.074s)
               Value function loss: 55665.3423
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 5186.07
               Mean episode length: 324.33
                 Mean success rate: 59.50
                  Mean reward/step: 15.02
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8527872
                    Iteration time: 2.53s
                        Total time: 2667.73s
                               ETA: 7585.5s

################################################################################
                     [1m Learning iteration 1041/4000 [0m

                       Computation: 3240 steps/s (collection: 0.483s, learning 2.044s)
               Value function loss: 79231.6092
                    Surrogate loss: 0.0133
             Mean action noise std: 0.93
                       Mean reward: 4464.54
               Mean episode length: 296.56
                 Mean success rate: 54.50
                  Mean reward/step: 15.30
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 2.53s
                        Total time: 2670.26s
                               ETA: 7582.8s

################################################################################
                     [1m Learning iteration 1042/4000 [0m

                       Computation: 3137 steps/s (collection: 0.483s, learning 2.127s)
               Value function loss: 52261.3236
                    Surrogate loss: 0.0160
             Mean action noise std: 0.93
                       Mean reward: 4407.77
               Mean episode length: 293.28
                 Mean success rate: 53.00
                  Mean reward/step: 15.46
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8544256
                    Iteration time: 2.61s
                        Total time: 2672.87s
                               ETA: 7580.4s

################################################################################
                     [1m Learning iteration 1043/4000 [0m

                       Computation: 3151 steps/s (collection: 0.497s, learning 2.102s)
               Value function loss: 45150.6783
                    Surrogate loss: 0.0188
             Mean action noise std: 0.93
                       Mean reward: 4204.82
               Mean episode length: 288.62
                 Mean success rate: 51.00
                  Mean reward/step: 15.92
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 2.60s
                        Total time: 2675.47s
                               ETA: 7577.9s

################################################################################
                     [1m Learning iteration 1044/4000 [0m

                       Computation: 3213 steps/s (collection: 0.457s, learning 2.093s)
               Value function loss: 70062.0422
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 4115.23
               Mean episode length: 286.21
                 Mean success rate: 49.50
                  Mean reward/step: 16.11
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8560640
                    Iteration time: 2.55s
                        Total time: 2678.02s
                               ETA: 7575.3s

################################################################################
                     [1m Learning iteration 1045/4000 [0m

                       Computation: 3190 steps/s (collection: 0.504s, learning 2.063s)
               Value function loss: 60731.8466
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 4100.34
               Mean episode length: 278.43
                 Mean success rate: 49.50
                  Mean reward/step: 16.05
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 2.57s
                        Total time: 2680.58s
                               ETA: 7572.8s

################################################################################
                     [1m Learning iteration 1046/4000 [0m

                       Computation: 3089 steps/s (collection: 0.508s, learning 2.144s)
               Value function loss: 74420.5989
                    Surrogate loss: 0.0112
             Mean action noise std: 0.93
                       Mean reward: 4135.45
               Mean episode length: 278.10
                 Mean success rate: 49.50
                  Mean reward/step: 16.51
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8577024
                    Iteration time: 2.65s
                        Total time: 2683.24s
                               ETA: 7570.5s

################################################################################
                     [1m Learning iteration 1047/4000 [0m

                       Computation: 3180 steps/s (collection: 0.483s, learning 2.092s)
               Value function loss: 76009.5771
                    Surrogate loss: 0.0120
             Mean action noise std: 0.93
                       Mean reward: 4340.97
               Mean episode length: 283.74
                 Mean success rate: 51.00
                  Mean reward/step: 16.50
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 2.58s
                        Total time: 2685.81s
                               ETA: 7567.9s

################################################################################
                     [1m Learning iteration 1048/4000 [0m

                       Computation: 3162 steps/s (collection: 0.469s, learning 2.121s)
               Value function loss: 57926.7259
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 4499.30
               Mean episode length: 289.48
                 Mean success rate: 51.00
                  Mean reward/step: 16.16
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8593408
                    Iteration time: 2.59s
                        Total time: 2688.40s
                               ETA: 7565.5s

################################################################################
                     [1m Learning iteration 1049/4000 [0m

                       Computation: 3159 steps/s (collection: 0.470s, learning 2.123s)
               Value function loss: 66222.8499
                    Surrogate loss: 0.0167
             Mean action noise std: 0.93
                       Mean reward: 4530.15
               Mean episode length: 291.92
                 Mean success rate: 50.00
                  Mean reward/step: 16.39
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 2.59s
                        Total time: 2691.00s
                               ETA: 7563.0s

################################################################################
                     [1m Learning iteration 1050/4000 [0m

                       Computation: 3137 steps/s (collection: 0.503s, learning 2.108s)
               Value function loss: 57957.9062
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 4457.38
               Mean episode length: 283.69
                 Mean success rate: 50.00
                  Mean reward/step: 17.08
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8609792
                    Iteration time: 2.61s
                        Total time: 2693.61s
                               ETA: 7560.6s

################################################################################
                     [1m Learning iteration 1051/4000 [0m

                       Computation: 3105 steps/s (collection: 0.529s, learning 2.108s)
               Value function loss: 63421.0364
                    Surrogate loss: 0.0152
             Mean action noise std: 0.93
                       Mean reward: 4678.05
               Mean episode length: 290.15
                 Mean success rate: 51.50
                  Mean reward/step: 17.22
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 2.64s
                        Total time: 2696.24s
                               ETA: 7558.2s

################################################################################
                     [1m Learning iteration 1052/4000 [0m

                       Computation: 3219 steps/s (collection: 0.472s, learning 2.072s)
               Value function loss: 42699.5688
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 4527.53
               Mean episode length: 290.60
                 Mean success rate: 50.50
                  Mean reward/step: 17.15
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8626176
                    Iteration time: 2.54s
                        Total time: 2698.79s
                               ETA: 7555.6s

################################################################################
                     [1m Learning iteration 1053/4000 [0m

                       Computation: 3167 steps/s (collection: 0.497s, learning 2.089s)
               Value function loss: 80645.6970
                    Surrogate loss: 0.0180
             Mean action noise std: 0.93
                       Mean reward: 4659.45
               Mean episode length: 294.24
                 Mean success rate: 49.50
                  Mean reward/step: 16.94
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 2.59s
                        Total time: 2701.37s
                               ETA: 7553.1s

################################################################################
                     [1m Learning iteration 1054/4000 [0m

                       Computation: 3176 steps/s (collection: 0.500s, learning 2.078s)
               Value function loss: 78984.8499
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 4755.69
               Mean episode length: 295.28
                 Mean success rate: 50.00
                  Mean reward/step: 16.74
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8642560
                    Iteration time: 2.58s
                        Total time: 2703.95s
                               ETA: 7550.6s

################################################################################
                     [1m Learning iteration 1055/4000 [0m

                       Computation: 3224 steps/s (collection: 0.481s, learning 2.060s)
               Value function loss: 62052.8254
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 4918.98
               Mean episode length: 305.57
                 Mean success rate: 52.00
                  Mean reward/step: 15.87
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 2.54s
                        Total time: 2706.49s
                               ETA: 7547.9s

################################################################################
                     [1m Learning iteration 1056/4000 [0m

                       Computation: 3198 steps/s (collection: 0.461s, learning 2.100s)
               Value function loss: 72645.8277
                    Surrogate loss: 0.0114
             Mean action noise std: 0.93
                       Mean reward: 4815.44
               Mean episode length: 301.04
                 Mean success rate: 52.00
                  Mean reward/step: 15.47
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8658944
                    Iteration time: 2.56s
                        Total time: 2709.05s
                               ETA: 7545.4s

################################################################################
                     [1m Learning iteration 1057/4000 [0m

                       Computation: 3149 steps/s (collection: 0.506s, learning 2.095s)
               Value function loss: 72880.0084
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 5141.96
               Mean episode length: 309.93
                 Mean success rate: 55.00
                  Mean reward/step: 15.04
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 2.60s
                        Total time: 2711.65s
                               ETA: 7542.9s

################################################################################
                     [1m Learning iteration 1058/4000 [0m

                       Computation: 3141 steps/s (collection: 0.534s, learning 2.074s)
               Value function loss: 58563.6438
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 4846.92
               Mean episode length: 293.11
                 Mean success rate: 52.50
                  Mean reward/step: 15.05
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8675328
                    Iteration time: 2.61s
                        Total time: 2714.26s
                               ETA: 7540.5s

################################################################################
                     [1m Learning iteration 1059/4000 [0m

                       Computation: 3128 steps/s (collection: 0.481s, learning 2.137s)
               Value function loss: 69983.4341
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 4868.15
               Mean episode length: 293.92
                 Mean success rate: 52.50
                  Mean reward/step: 15.86
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 2.62s
                        Total time: 2716.88s
                               ETA: 7538.1s

################################################################################
                     [1m Learning iteration 1060/4000 [0m

                       Computation: 3174 steps/s (collection: 0.489s, learning 2.092s)
               Value function loss: 63925.7098
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 4782.14
               Mean episode length: 289.33
                 Mean success rate: 51.50
                  Mean reward/step: 16.55
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8691712
                    Iteration time: 2.58s
                        Total time: 2719.46s
                               ETA: 7535.5s

################################################################################
                     [1m Learning iteration 1061/4000 [0m

                       Computation: 3133 steps/s (collection: 0.486s, learning 2.128s)
               Value function loss: 74824.5618
                    Surrogate loss: 0.0168
             Mean action noise std: 0.93
                       Mean reward: 4669.62
               Mean episode length: 283.98
                 Mean success rate: 49.50
                  Mean reward/step: 16.46
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 2.61s
                        Total time: 2722.08s
                               ETA: 7533.1s

################################################################################
                     [1m Learning iteration 1062/4000 [0m

                       Computation: 3182 steps/s (collection: 0.500s, learning 2.074s)
               Value function loss: 82408.4797
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 4681.97
               Mean episode length: 277.68
                 Mean success rate: 47.50
                  Mean reward/step: 15.91
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 8708096
                    Iteration time: 2.57s
                        Total time: 2724.65s
                               ETA: 7530.6s

################################################################################
                     [1m Learning iteration 1063/4000 [0m

                       Computation: 3177 steps/s (collection: 0.472s, learning 2.106s)
               Value function loss: 72568.7474
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 4285.38
               Mean episode length: 263.18
                 Mean success rate: 43.50
                  Mean reward/step: 15.31
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 2.58s
                        Total time: 2727.23s
                               ETA: 7528.1s

################################################################################
                     [1m Learning iteration 1064/4000 [0m

                       Computation: 3106 steps/s (collection: 0.480s, learning 2.157s)
               Value function loss: 54861.2224
                    Surrogate loss: 0.0169
             Mean action noise std: 0.93
                       Mean reward: 4349.95
               Mean episode length: 269.50
                 Mean success rate: 44.00
                  Mean reward/step: 15.41
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8724480
                    Iteration time: 2.64s
                        Total time: 2729.87s
                               ETA: 7525.7s

################################################################################
                     [1m Learning iteration 1065/4000 [0m

                       Computation: 3240 steps/s (collection: 0.485s, learning 2.043s)
               Value function loss: 65629.9969
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 4465.85
               Mean episode length: 276.99
                 Mean success rate: 45.00
                  Mean reward/step: 15.77
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 2.53s
                        Total time: 2732.39s
                               ETA: 7523.1s

################################################################################
                     [1m Learning iteration 1066/4000 [0m

                       Computation: 3163 steps/s (collection: 0.503s, learning 2.087s)
               Value function loss: 50725.1516
                    Surrogate loss: 0.0178
             Mean action noise std: 0.93
                       Mean reward: 4327.34
               Mean episode length: 269.57
                 Mean success rate: 44.00
                  Mean reward/step: 16.37
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8740864
                    Iteration time: 2.59s
                        Total time: 2734.98s
                               ETA: 7520.6s

################################################################################
                     [1m Learning iteration 1067/4000 [0m

                       Computation: 3198 steps/s (collection: 0.498s, learning 2.063s)
               Value function loss: 56819.6141
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 4243.98
               Mean episode length: 262.89
                 Mean success rate: 43.00
                  Mean reward/step: 16.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 2.56s
                        Total time: 2737.54s
                               ETA: 7518.0s

################################################################################
                     [1m Learning iteration 1068/4000 [0m

                       Computation: 3246 steps/s (collection: 0.472s, learning 2.052s)
               Value function loss: 54940.6102
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 4435.30
               Mean episode length: 271.80
                 Mean success rate: 44.50
                  Mean reward/step: 17.14
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8757248
                    Iteration time: 2.52s
                        Total time: 2740.07s
                               ETA: 7515.3s

################################################################################
                     [1m Learning iteration 1069/4000 [0m

                       Computation: 3243 steps/s (collection: 0.455s, learning 2.071s)
               Value function loss: 54059.8321
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 4218.74
               Mean episode length: 273.62
                 Mean success rate: 43.00
                  Mean reward/step: 17.59
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 2.53s
                        Total time: 2742.59s
                               ETA: 7512.7s

################################################################################
                     [1m Learning iteration 1070/4000 [0m

                       Computation: 3177 steps/s (collection: 0.486s, learning 2.092s)
               Value function loss: 75383.5964
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 4373.91
               Mean episode length: 282.78
                 Mean success rate: 45.50
                  Mean reward/step: 17.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8773632
                    Iteration time: 2.58s
                        Total time: 2745.17s
                               ETA: 7510.1s

################################################################################
                     [1m Learning iteration 1071/4000 [0m

                       Computation: 3206 steps/s (collection: 0.495s, learning 2.060s)
               Value function loss: 72021.6608
                    Surrogate loss: 0.0162
             Mean action noise std: 0.93
                       Mean reward: 4481.60
               Mean episode length: 285.06
                 Mean success rate: 47.00
                  Mean reward/step: 17.47
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 2.55s
                        Total time: 2747.73s
                               ETA: 7507.5s

################################################################################
                     [1m Learning iteration 1072/4000 [0m

                       Computation: 3270 steps/s (collection: 0.450s, learning 2.055s)
               Value function loss: 76596.7136
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 4809.03
               Mean episode length: 289.19
                 Mean success rate: 48.50
                  Mean reward/step: 17.23
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8790016
                    Iteration time: 2.50s
                        Total time: 2750.23s
                               ETA: 7504.8s

################################################################################
                     [1m Learning iteration 1073/4000 [0m

                       Computation: 3182 steps/s (collection: 0.474s, learning 2.100s)
               Value function loss: 78496.7590
                    Surrogate loss: 0.0164
             Mean action noise std: 0.93
                       Mean reward: 4747.45
               Mean episode length: 284.93
                 Mean success rate: 48.50
                  Mean reward/step: 17.10
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 2.57s
                        Total time: 2752.81s
                               ETA: 7502.3s

################################################################################
                     [1m Learning iteration 1074/4000 [0m

                       Computation: 3201 steps/s (collection: 0.482s, learning 2.078s)
               Value function loss: 46111.7304
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 4725.40
               Mean episode length: 281.38
                 Mean success rate: 48.00
                  Mean reward/step: 16.96
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8806400
                    Iteration time: 2.56s
                        Total time: 2755.36s
                               ETA: 7499.7s

################################################################################
                     [1m Learning iteration 1075/4000 [0m

                       Computation: 3237 steps/s (collection: 0.477s, learning 2.054s)
               Value function loss: 67811.7700
                    Surrogate loss: 0.0117
             Mean action noise std: 0.93
                       Mean reward: 4237.80
               Mean episode length: 256.35
                 Mean success rate: 45.00
                  Mean reward/step: 17.53
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 2.53s
                        Total time: 2757.89s
                               ETA: 7497.1s

################################################################################
                     [1m Learning iteration 1076/4000 [0m

                       Computation: 3229 steps/s (collection: 0.441s, learning 2.096s)
               Value function loss: 68764.7012
                    Surrogate loss: 0.0144
             Mean action noise std: 0.93
                       Mean reward: 4448.93
               Mean episode length: 267.08
                 Mean success rate: 47.00
                  Mean reward/step: 17.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8822784
                    Iteration time: 2.54s
                        Total time: 2760.43s
                               ETA: 7494.4s

################################################################################
                     [1m Learning iteration 1077/4000 [0m

                       Computation: 3167 steps/s (collection: 0.464s, learning 2.123s)
               Value function loss: 72230.4849
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 4744.23
               Mean episode length: 276.12
                 Mean success rate: 48.50
                  Mean reward/step: 17.32
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 2.59s
                        Total time: 2763.02s
                               ETA: 7491.9s

################################################################################
                     [1m Learning iteration 1078/4000 [0m

                       Computation: 3264 steps/s (collection: 0.435s, learning 2.074s)
               Value function loss: 88912.7077
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 4825.33
               Mean episode length: 279.95
                 Mean success rate: 50.50
                  Mean reward/step: 16.70
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8839168
                    Iteration time: 2.51s
                        Total time: 2765.53s
                               ETA: 7489.2s

################################################################################
                     [1m Learning iteration 1079/4000 [0m

                       Computation: 3022 steps/s (collection: 0.571s, learning 2.139s)
               Value function loss: 61161.3920
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 4654.65
               Mean episode length: 275.26
                 Mean success rate: 49.00
                  Mean reward/step: 16.25
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 2.71s
                        Total time: 2768.24s
                               ETA: 7487.1s

################################################################################
                     [1m Learning iteration 1080/4000 [0m

                       Computation: 3198 steps/s (collection: 0.460s, learning 2.101s)
               Value function loss: 69992.3982
                    Surrogate loss: 0.0124
             Mean action noise std: 0.93
                       Mean reward: 4839.18
               Mean episode length: 288.36
                 Mean success rate: 52.00
                  Mean reward/step: 16.34
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8855552
                    Iteration time: 2.56s
                        Total time: 2770.80s
                               ETA: 7484.5s

################################################################################
                     [1m Learning iteration 1081/4000 [0m

                       Computation: 3262 steps/s (collection: 0.459s, learning 2.052s)
               Value function loss: 71408.6295
                    Surrogate loss: 0.0120
             Mean action noise std: 0.93
                       Mean reward: 5166.45
               Mean episode length: 305.90
                 Mean success rate: 55.50
                  Mean reward/step: 16.19
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 2.51s
                        Total time: 2773.31s
                               ETA: 7481.8s

################################################################################
                     [1m Learning iteration 1082/4000 [0m

                       Computation: 3170 steps/s (collection: 0.498s, learning 2.086s)
               Value function loss: 54119.8544
                    Surrogate loss: 0.0114
             Mean action noise std: 0.93
                       Mean reward: 5196.30
               Mean episode length: 305.06
                 Mean success rate: 54.00
                  Mean reward/step: 15.76
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8871936
                    Iteration time: 2.58s
                        Total time: 2775.89s
                               ETA: 7479.3s

################################################################################
                     [1m Learning iteration 1083/4000 [0m

                       Computation: 3288 steps/s (collection: 0.481s, learning 2.009s)
               Value function loss: 61457.2387
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 5127.07
               Mean episode length: 299.75
                 Mean success rate: 54.00
                  Mean reward/step: 15.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 2.49s
                        Total time: 2778.38s
                               ETA: 7476.5s

################################################################################
                     [1m Learning iteration 1084/4000 [0m

                       Computation: 3277 steps/s (collection: 0.470s, learning 2.030s)
               Value function loss: 57626.6221
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 4616.56
               Mean episode length: 280.92
                 Mean success rate: 48.50
                  Mean reward/step: 15.81
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8888320
                    Iteration time: 2.50s
                        Total time: 2780.88s
                               ETA: 7473.8s

################################################################################
                     [1m Learning iteration 1085/4000 [0m

                       Computation: 3194 steps/s (collection: 0.491s, learning 2.074s)
               Value function loss: 65508.2329
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 4738.17
               Mean episode length: 286.92
                 Mean success rate: 49.50
                  Mean reward/step: 15.18
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 2.56s
                        Total time: 2783.45s
                               ETA: 7471.2s

################################################################################
                     [1m Learning iteration 1086/4000 [0m

                       Computation: 3184 steps/s (collection: 0.501s, learning 2.071s)
               Value function loss: 65212.3254
                    Surrogate loss: 0.0159
             Mean action noise std: 0.93
                       Mean reward: 4660.31
               Mean episode length: 285.94
                 Mean success rate: 49.50
                  Mean reward/step: 15.42
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8904704
                    Iteration time: 2.57s
                        Total time: 2786.02s
                               ETA: 7468.7s

################################################################################
                     [1m Learning iteration 1087/4000 [0m

                       Computation: 3188 steps/s (collection: 0.482s, learning 2.087s)
               Value function loss: 78463.1885
                    Surrogate loss: 0.0114
             Mean action noise std: 0.93
                       Mean reward: 4123.05
               Mean episode length: 259.92
                 Mean success rate: 44.00
                  Mean reward/step: 16.10
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 2.57s
                        Total time: 2788.59s
                               ETA: 7466.1s

################################################################################
                     [1m Learning iteration 1088/4000 [0m

                       Computation: 3102 steps/s (collection: 0.480s, learning 2.160s)
               Value function loss: 70563.7715
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 4359.46
               Mean episode length: 270.79
                 Mean success rate: 46.50
                  Mean reward/step: 16.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8921088
                    Iteration time: 2.64s
                        Total time: 2791.23s
                               ETA: 7463.8s

################################################################################
                     [1m Learning iteration 1089/4000 [0m

                       Computation: 3202 steps/s (collection: 0.468s, learning 2.090s)
               Value function loss: 56179.6489
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 4441.87
               Mean episode length: 278.75
                 Mean success rate: 47.00
                  Mean reward/step: 17.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 2.56s
                        Total time: 2793.79s
                               ETA: 7461.2s

################################################################################
                     [1m Learning iteration 1090/4000 [0m

                       Computation: 3180 steps/s (collection: 0.499s, learning 2.077s)
               Value function loss: 56048.6820
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 4507.32
               Mean episode length: 276.81
                 Mean success rate: 46.50
                  Mean reward/step: 17.01
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8937472
                    Iteration time: 2.58s
                        Total time: 2796.36s
                               ETA: 7458.7s

################################################################################
                     [1m Learning iteration 1091/4000 [0m

                       Computation: 3152 steps/s (collection: 0.538s, learning 2.060s)
               Value function loss: 75444.8585
                    Surrogate loss: 0.0118
             Mean action noise std: 0.93
                       Mean reward: 4930.44
               Mean episode length: 294.74
                 Mean success rate: 50.00
                  Mean reward/step: 16.98
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 2.60s
                        Total time: 2798.96s
                               ETA: 7456.2s

################################################################################
                     [1m Learning iteration 1092/4000 [0m

                       Computation: 3241 steps/s (collection: 0.488s, learning 2.039s)
               Value function loss: 66625.9144
                    Surrogate loss: 0.0133
             Mean action noise std: 0.93
                       Mean reward: 4655.81
               Mean episode length: 281.00
                 Mean success rate: 46.50
                  Mean reward/step: 16.91
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8953856
                    Iteration time: 2.53s
                        Total time: 2801.49s
                               ETA: 7453.6s

################################################################################
                     [1m Learning iteration 1093/4000 [0m

                       Computation: 3142 steps/s (collection: 0.492s, learning 2.115s)
               Value function loss: 58677.4253
                    Surrogate loss: 0.0164
             Mean action noise std: 0.93
                       Mean reward: 4708.34
               Mean episode length: 282.39
                 Mean success rate: 47.00
                  Mean reward/step: 17.38
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 2.61s
                        Total time: 2804.10s
                               ETA: 7451.1s

################################################################################
                     [1m Learning iteration 1094/4000 [0m

                       Computation: 3215 steps/s (collection: 0.479s, learning 2.068s)
               Value function loss: 79441.3423
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 4601.36
               Mean episode length: 279.86
                 Mean success rate: 46.50
                  Mean reward/step: 17.41
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 8970240
                    Iteration time: 2.55s
                        Total time: 2806.64s
                               ETA: 7448.5s

################################################################################
                     [1m Learning iteration 1095/4000 [0m

                       Computation: 3206 steps/s (collection: 0.502s, learning 2.053s)
               Value function loss: 68672.1361
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 4106.49
               Mean episode length: 259.83
                 Mean success rate: 43.00
                  Mean reward/step: 16.74
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 2.55s
                        Total time: 2809.20s
                               ETA: 7445.9s

################################################################################
                     [1m Learning iteration 1096/4000 [0m

                       Computation: 3152 steps/s (collection: 0.484s, learning 2.115s)
               Value function loss: 77416.6333
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 4325.28
               Mean episode length: 264.06
                 Mean success rate: 46.00
                  Mean reward/step: 16.22
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8986624
                    Iteration time: 2.60s
                        Total time: 2811.80s
                               ETA: 7443.4s

################################################################################
                     [1m Learning iteration 1097/4000 [0m

                       Computation: 3187 steps/s (collection: 0.496s, learning 2.075s)
               Value function loss: 107440.2053
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 4244.25
               Mean episode length: 265.19
                 Mean success rate: 46.00
                  Mean reward/step: 16.47
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 2.57s
                        Total time: 2814.37s
                               ETA: 7440.9s

################################################################################
                     [1m Learning iteration 1098/4000 [0m

                       Computation: 3231 steps/s (collection: 0.464s, learning 2.072s)
               Value function loss: 64921.6225
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 4462.95
               Mean episode length: 274.02
                 Mean success rate: 48.50
                  Mean reward/step: 16.54
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9003008
                    Iteration time: 2.54s
                        Total time: 2816.90s
                               ETA: 7438.3s

################################################################################
                     [1m Learning iteration 1099/4000 [0m

                       Computation: 3234 steps/s (collection: 0.472s, learning 2.061s)
               Value function loss: 81650.5940
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 4686.47
               Mean episode length: 284.75
                 Mean success rate: 51.00
                  Mean reward/step: 16.88
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 2.53s
                        Total time: 2819.44s
                               ETA: 7435.6s

################################################################################
                     [1m Learning iteration 1100/4000 [0m

                       Computation: 3256 steps/s (collection: 0.482s, learning 2.033s)
               Value function loss: 57385.1154
                    Surrogate loss: 0.0160
             Mean action noise std: 0.93
                       Mean reward: 4730.03
               Mean episode length: 287.18
                 Mean success rate: 50.50
                  Mean reward/step: 16.91
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9019392
                    Iteration time: 2.52s
                        Total time: 2821.95s
                               ETA: 7432.9s

################################################################################
                     [1m Learning iteration 1101/4000 [0m

                       Computation: 3230 steps/s (collection: 0.498s, learning 2.037s)
               Value function loss: 52032.6876
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 4818.36
               Mean episode length: 292.82
                 Mean success rate: 50.00
                  Mean reward/step: 16.95
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 2.54s
                        Total time: 2824.49s
                               ETA: 7430.3s

################################################################################
                     [1m Learning iteration 1102/4000 [0m

                       Computation: 3260 steps/s (collection: 0.455s, learning 2.058s)
               Value function loss: 75838.8878
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 4894.79
               Mean episode length: 294.69
                 Mean success rate: 50.00
                  Mean reward/step: 17.17
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9035776
                    Iteration time: 2.51s
                        Total time: 2827.00s
                               ETA: 7427.6s

################################################################################
                     [1m Learning iteration 1103/4000 [0m

                       Computation: 3221 steps/s (collection: 0.501s, learning 2.041s)
               Value function loss: 76194.4232
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 4918.76
               Mean episode length: 294.73
                 Mean success rate: 50.00
                  Mean reward/step: 17.12
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 2.54s
                        Total time: 2829.54s
                               ETA: 7425.0s

################################################################################
                     [1m Learning iteration 1104/4000 [0m

                       Computation: 3226 steps/s (collection: 0.469s, learning 2.070s)
               Value function loss: 60213.9959
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 4940.81
               Mean episode length: 296.53
                 Mean success rate: 51.50
                  Mean reward/step: 16.97
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9052160
                    Iteration time: 2.54s
                        Total time: 2832.08s
                               ETA: 7422.4s

################################################################################
                     [1m Learning iteration 1105/4000 [0m

                       Computation: 3198 steps/s (collection: 0.507s, learning 2.055s)
               Value function loss: 65187.6090
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 4766.83
               Mean episode length: 287.18
                 Mean success rate: 49.00
                  Mean reward/step: 16.96
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 2.56s
                        Total time: 2834.64s
                               ETA: 7419.8s

################################################################################
                     [1m Learning iteration 1106/4000 [0m

                       Computation: 3219 steps/s (collection: 0.465s, learning 2.080s)
               Value function loss: 72673.9845
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 4201.85
               Mean episode length: 260.82
                 Mean success rate: 44.00
                  Mean reward/step: 17.27
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9068544
                    Iteration time: 2.54s
                        Total time: 2837.19s
                               ETA: 7417.2s

################################################################################
                     [1m Learning iteration 1107/4000 [0m

                       Computation: 3302 steps/s (collection: 0.445s, learning 2.035s)
               Value function loss: 60252.7090
                    Surrogate loss: 0.0119
             Mean action noise std: 0.93
                       Mean reward: 4351.03
               Mean episode length: 260.68
                 Mean success rate: 44.50
                  Mean reward/step: 17.75
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 2.48s
                        Total time: 2839.67s
                               ETA: 7414.4s

################################################################################
                     [1m Learning iteration 1108/4000 [0m

                       Computation: 3201 steps/s (collection: 0.496s, learning 2.063s)
               Value function loss: 65690.9789
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 4519.41
               Mean episode length: 262.95
                 Mean success rate: 47.00
                  Mean reward/step: 17.56
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9084928
                    Iteration time: 2.56s
                        Total time: 2842.23s
                               ETA: 7411.8s

################################################################################
                     [1m Learning iteration 1109/4000 [0m

                       Computation: 3283 steps/s (collection: 0.439s, learning 2.057s)
               Value function loss: 78385.7236
                    Surrogate loss: 0.0133
             Mean action noise std: 0.93
                       Mean reward: 4443.25
               Mean episode length: 258.61
                 Mean success rate: 45.50
                  Mean reward/step: 17.58
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 2.50s
                        Total time: 2844.72s
                               ETA: 7409.1s

################################################################################
                     [1m Learning iteration 1110/4000 [0m

                       Computation: 3222 steps/s (collection: 0.495s, learning 2.047s)
               Value function loss: 102220.3559
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 4460.20
               Mean episode length: 258.75
                 Mean success rate: 44.50
                  Mean reward/step: 17.23
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 9101312
                    Iteration time: 2.54s
                        Total time: 2847.27s
                               ETA: 7406.5s

################################################################################
                     [1m Learning iteration 1111/4000 [0m

                       Computation: 3247 steps/s (collection: 0.472s, learning 2.051s)
               Value function loss: 87878.4052
                    Surrogate loss: 0.0122
             Mean action noise std: 0.93
                       Mean reward: 4519.82
               Mean episode length: 260.86
                 Mean success rate: 45.00
                  Mean reward/step: 15.79
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 2.52s
                        Total time: 2849.79s
                               ETA: 7403.8s

################################################################################
                     [1m Learning iteration 1112/4000 [0m

                       Computation: 3266 steps/s (collection: 0.487s, learning 2.021s)
               Value function loss: 64510.0989
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 4695.43
               Mean episode length: 268.56
                 Mean success rate: 46.00
                  Mean reward/step: 15.38
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9117696
                    Iteration time: 2.51s
                        Total time: 2852.29s
                               ETA: 7401.1s

################################################################################
                     [1m Learning iteration 1113/4000 [0m

                       Computation: 3206 steps/s (collection: 0.473s, learning 2.082s)
               Value function loss: 88049.7843
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 4807.58
               Mean episode length: 279.76
                 Mean success rate: 47.50
                  Mean reward/step: 14.79
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 2.55s
                        Total time: 2854.85s
                               ETA: 7398.5s

################################################################################
                     [1m Learning iteration 1114/4000 [0m

                       Computation: 3215 steps/s (collection: 0.481s, learning 2.066s)
               Value function loss: 36491.8734
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 4633.48
               Mean episode length: 274.54
                 Mean success rate: 46.50
                  Mean reward/step: 14.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9134080
                    Iteration time: 2.55s
                        Total time: 2857.40s
                               ETA: 7395.9s

################################################################################
                     [1m Learning iteration 1115/4000 [0m

                       Computation: 3284 steps/s (collection: 0.464s, learning 2.030s)
               Value function loss: 64047.7254
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 4857.02
               Mean episode length: 284.92
                 Mean success rate: 48.50
                  Mean reward/step: 15.86
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 2.49s
                        Total time: 2859.89s
                               ETA: 7393.2s

################################################################################
                     [1m Learning iteration 1116/4000 [0m

                       Computation: 3260 steps/s (collection: 0.481s, learning 2.032s)
               Value function loss: 75007.5779
                    Surrogate loss: 0.0131
             Mean action noise std: 0.93
                       Mean reward: 4563.04
               Mean episode length: 278.68
                 Mean success rate: 46.50
                  Mean reward/step: 16.73
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 9150464
                    Iteration time: 2.51s
                        Total time: 2862.40s
                               ETA: 7390.5s

################################################################################
                     [1m Learning iteration 1117/4000 [0m

                       Computation: 3275 steps/s (collection: 0.451s, learning 2.050s)
               Value function loss: 68141.8842
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 4722.66
               Mean episode length: 280.25
                 Mean success rate: 48.50
                  Mean reward/step: 16.58
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 2.50s
                        Total time: 2864.90s
                               ETA: 7387.8s

################################################################################
                     [1m Learning iteration 1118/4000 [0m

                       Computation: 3207 steps/s (collection: 0.496s, learning 2.058s)
               Value function loss: 62014.6191
                    Surrogate loss: 0.0157
             Mean action noise std: 0.92
                       Mean reward: 4704.34
               Mean episode length: 277.11
                 Mean success rate: 48.50
                  Mean reward/step: 16.50
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9166848
                    Iteration time: 2.55s
                        Total time: 2867.46s
                               ETA: 7385.2s

################################################################################
                     [1m Learning iteration 1119/4000 [0m

                       Computation: 3245 steps/s (collection: 0.473s, learning 2.051s)
               Value function loss: 86293.9678
                    Surrogate loss: 0.0141
             Mean action noise std: 0.92
                       Mean reward: 4853.41
               Mean episode length: 285.10
                 Mean success rate: 51.00
                  Mean reward/step: 16.31
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 2.52s
                        Total time: 2869.98s
                               ETA: 7382.5s

################################################################################
                     [1m Learning iteration 1120/4000 [0m

                       Computation: 3270 steps/s (collection: 0.461s, learning 2.044s)
               Value function loss: 82484.0727
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 4444.23
               Mean episode length: 267.94
                 Mean success rate: 47.00
                  Mean reward/step: 15.65
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 9183232
                    Iteration time: 2.50s
                        Total time: 2872.49s
                               ETA: 7379.8s

################################################################################
                     [1m Learning iteration 1121/4000 [0m

                       Computation: 3287 steps/s (collection: 0.462s, learning 2.030s)
               Value function loss: 56750.1850
                    Surrogate loss: 0.0155
             Mean action noise std: 0.92
                       Mean reward: 4347.64
               Mean episode length: 259.82
                 Mean success rate: 46.00
                  Mean reward/step: 15.53
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 2.49s
                        Total time: 2874.98s
                               ETA: 7377.1s

################################################################################
                     [1m Learning iteration 1122/4000 [0m

                       Computation: 3234 steps/s (collection: 0.441s, learning 2.091s)
               Value function loss: 50758.6471
                    Surrogate loss: 0.0174
             Mean action noise std: 0.92
                       Mean reward: 4347.33
               Mean episode length: 264.55
                 Mean success rate: 47.50
                  Mean reward/step: 16.08
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9199616
                    Iteration time: 2.53s
                        Total time: 2877.51s
                               ETA: 7374.4s

################################################################################
                     [1m Learning iteration 1123/4000 [0m

                       Computation: 3189 steps/s (collection: 0.472s, learning 2.096s)
               Value function loss: 48103.7076
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 4505.56
               Mean episode length: 272.54
                 Mean success rate: 48.50
                  Mean reward/step: 16.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 2.57s
                        Total time: 2880.08s
                               ETA: 7371.9s

################################################################################
                     [1m Learning iteration 1124/4000 [0m

                       Computation: 3235 steps/s (collection: 0.483s, learning 2.048s)
               Value function loss: 53656.7852
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 4514.36
               Mean episode length: 273.88
                 Mean success rate: 49.00
                  Mean reward/step: 16.86
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9216000
                    Iteration time: 2.53s
                        Total time: 2882.61s
                               ETA: 7369.2s

################################################################################
                     [1m Learning iteration 1125/4000 [0m

                       Computation: 3131 steps/s (collection: 0.503s, learning 2.113s)
               Value function loss: 76171.1551
                    Surrogate loss: 0.0170
             Mean action noise std: 0.92
                       Mean reward: 4596.62
               Mean episode length: 283.33
                 Mean success rate: 49.00
                  Mean reward/step: 17.07
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 2.62s
                        Total time: 2885.23s
                               ETA: 7366.8s

################################################################################
                     [1m Learning iteration 1126/4000 [0m

                       Computation: 3178 steps/s (collection: 0.503s, learning 2.075s)
               Value function loss: 112341.6814
                    Surrogate loss: 0.0094
             Mean action noise std: 0.92
                       Mean reward: 4888.50
               Mean episode length: 304.31
                 Mean success rate: 52.50
                  Mean reward/step: 16.05
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9232384
                    Iteration time: 2.58s
                        Total time: 2887.81s
                               ETA: 7364.3s

################################################################################
                     [1m Learning iteration 1127/4000 [0m

                       Computation: 3191 steps/s (collection: 0.453s, learning 2.114s)
               Value function loss: 74949.2483
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 4974.31
               Mean episode length: 307.68
                 Mean success rate: 53.50
                  Mean reward/step: 14.91
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 2.57s
                        Total time: 2890.37s
                               ETA: 7361.7s

################################################################################
                     [1m Learning iteration 1128/4000 [0m

                       Computation: 3228 steps/s (collection: 0.466s, learning 2.071s)
               Value function loss: 71157.1688
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 5071.04
               Mean episode length: 314.75
                 Mean success rate: 54.00
                  Mean reward/step: 15.50
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9248768
                    Iteration time: 2.54s
                        Total time: 2892.91s
                               ETA: 7359.1s

################################################################################
                     [1m Learning iteration 1129/4000 [0m

                       Computation: 3197 steps/s (collection: 0.444s, learning 2.118s)
               Value function loss: 89225.2383
                    Surrogate loss: 0.0156
             Mean action noise std: 0.92
                       Mean reward: 5252.60
               Mean episode length: 317.98
                 Mean success rate: 54.50
                  Mean reward/step: 15.82
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 2.56s
                        Total time: 2895.47s
                               ETA: 7356.6s

################################################################################
                     [1m Learning iteration 1130/4000 [0m

                       Computation: 3235 steps/s (collection: 0.471s, learning 2.061s)
               Value function loss: 35444.6679
                    Surrogate loss: 0.0172
             Mean action noise std: 0.92
                       Mean reward: 4830.70
               Mean episode length: 297.60
                 Mean success rate: 50.50
                  Mean reward/step: 16.42
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9265152
                    Iteration time: 2.53s
                        Total time: 2898.00s
                               ETA: 7353.9s

################################################################################
                     [1m Learning iteration 1131/4000 [0m

                       Computation: 3173 steps/s (collection: 0.494s, learning 2.087s)
               Value function loss: 43000.9444
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 4546.45
               Mean episode length: 289.17
                 Mean success rate: 48.00
                  Mean reward/step: 17.75
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 2.58s
                        Total time: 2900.59s
                               ETA: 7351.4s

################################################################################
                     [1m Learning iteration 1132/4000 [0m

                       Computation: 3112 steps/s (collection: 0.483s, learning 2.149s)
               Value function loss: 61330.8471
                    Surrogate loss: 0.0133
             Mean action noise std: 0.93
                       Mean reward: 4485.29
               Mean episode length: 279.15
                 Mean success rate: 47.00
                  Mean reward/step: 18.88
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9281536
                    Iteration time: 2.63s
                        Total time: 2903.22s
                               ETA: 7349.0s

################################################################################
                     [1m Learning iteration 1133/4000 [0m

                       Computation: 3191 steps/s (collection: 0.476s, learning 2.091s)
               Value function loss: 82245.1137
                    Surrogate loss: 0.0156
             Mean action noise std: 0.92
                       Mean reward: 4190.28
               Mean episode length: 268.29
                 Mean success rate: 45.50
                  Mean reward/step: 18.27
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 2.57s
                        Total time: 2905.78s
                               ETA: 7346.5s

################################################################################
                     [1m Learning iteration 1134/4000 [0m

                       Computation: 3078 steps/s (collection: 0.512s, learning 2.149s)
               Value function loss: 96841.3943
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 4313.70
               Mean episode length: 271.09
                 Mean success rate: 47.00
                  Mean reward/step: 17.64
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9297920
                    Iteration time: 2.66s
                        Total time: 2908.45s
                               ETA: 7344.1s

################################################################################
                     [1m Learning iteration 1135/4000 [0m

                       Computation: 3141 steps/s (collection: 0.484s, learning 2.124s)
               Value function loss: 80056.2765
                    Surrogate loss: 0.0204
             Mean action noise std: 0.92
                       Mean reward: 4596.07
               Mean episode length: 285.46
                 Mean success rate: 49.50
                  Mean reward/step: 16.81
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 2.61s
                        Total time: 2911.05s
                               ETA: 7341.7s

################################################################################
                     [1m Learning iteration 1136/4000 [0m

                       Computation: 3208 steps/s (collection: 0.442s, learning 2.112s)
               Value function loss: 78887.2387
                    Surrogate loss: 0.0211
             Mean action noise std: 0.93
                       Mean reward: 4382.43
               Mean episode length: 268.73
                 Mean success rate: 48.00
                  Mean reward/step: 15.93
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 9314304
                    Iteration time: 2.55s
                        Total time: 2913.61s
                               ETA: 7339.1s

################################################################################
                     [1m Learning iteration 1137/4000 [0m

                       Computation: 3064 steps/s (collection: 0.499s, learning 2.175s)
               Value function loss: 63121.0208
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 4635.22
               Mean episode length: 279.48
                 Mean success rate: 50.50
                  Mean reward/step: 16.23
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 2.67s
                        Total time: 2916.28s
                               ETA: 7336.8s

################################################################################
                     [1m Learning iteration 1138/4000 [0m

                       Computation: 3082 steps/s (collection: 0.505s, learning 2.152s)
               Value function loss: 40932.6577
                    Surrogate loss: 0.0166
             Mean action noise std: 0.93
                       Mean reward: 4839.09
               Mean episode length: 286.08
                 Mean success rate: 53.00
                  Mean reward/step: 16.73
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9330688
                    Iteration time: 2.66s
                        Total time: 2918.94s
                               ETA: 7334.5s

################################################################################
                     [1m Learning iteration 1139/4000 [0m

                       Computation: 3145 steps/s (collection: 0.505s, learning 2.100s)
               Value function loss: 41977.5997
                    Surrogate loss: 0.0167
             Mean action noise std: 0.92
                       Mean reward: 4454.48
               Mean episode length: 273.90
                 Mean success rate: 51.00
                  Mean reward/step: 17.93
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 2.60s
                        Total time: 2921.54s
                               ETA: 7332.0s

################################################################################
                     [1m Learning iteration 1140/4000 [0m

                       Computation: 3185 steps/s (collection: 0.452s, learning 2.119s)
               Value function loss: 51606.5666
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 4394.74
               Mean episode length: 268.60
                 Mean success rate: 50.00
                  Mean reward/step: 18.41
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9347072
                    Iteration time: 2.57s
                        Total time: 2924.11s
                               ETA: 7329.5s

################################################################################
                     [1m Learning iteration 1141/4000 [0m

                       Computation: 3073 steps/s (collection: 0.520s, learning 2.145s)
               Value function loss: 94997.7949
                    Surrogate loss: 0.0169
             Mean action noise std: 0.92
                       Mean reward: 4348.20
               Mean episode length: 269.48
                 Mean success rate: 50.00
                  Mean reward/step: 18.80
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 2.67s
                        Total time: 2926.78s
                               ETA: 7327.2s

################################################################################
                     [1m Learning iteration 1142/4000 [0m

                       Computation: 3081 steps/s (collection: 0.489s, learning 2.169s)
               Value function loss: 97183.3053
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 4426.00
               Mean episode length: 268.32
                 Mean success rate: 50.00
                  Mean reward/step: 18.17
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9363456
                    Iteration time: 2.66s
                        Total time: 2929.44s
                               ETA: 7324.9s

################################################################################
                     [1m Learning iteration 1143/4000 [0m

                       Computation: 3129 steps/s (collection: 0.508s, learning 2.110s)
               Value function loss: 73739.3861
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 4775.34
               Mean episode length: 284.83
                 Mean success rate: 53.00
                  Mean reward/step: 17.56
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 2.62s
                        Total time: 2932.06s
                               ETA: 7322.4s

################################################################################
                     [1m Learning iteration 1144/4000 [0m

                       Computation: 3183 steps/s (collection: 0.476s, learning 2.097s)
               Value function loss: 86705.3007
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 4938.67
               Mean episode length: 291.25
                 Mean success rate: 54.00
                  Mean reward/step: 17.37
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9379840
                    Iteration time: 2.57s
                        Total time: 2934.63s
                               ETA: 7319.9s

################################################################################
                     [1m Learning iteration 1145/4000 [0m

                       Computation: 3125 steps/s (collection: 0.464s, learning 2.157s)
               Value function loss: 102689.4437
                    Surrogate loss: 0.0167
             Mean action noise std: 0.92
                       Mean reward: 5478.21
               Mean episode length: 310.64
                 Mean success rate: 59.00
                  Mean reward/step: 17.46
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 2.62s
                        Total time: 2937.25s
                               ETA: 7317.5s

################################################################################
                     [1m Learning iteration 1146/4000 [0m

                       Computation: 3076 steps/s (collection: 0.475s, learning 2.188s)
               Value function loss: 58979.1446
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 5687.31
               Mean episode length: 319.54
                 Mean success rate: 59.50
                  Mean reward/step: 17.17
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9396224
                    Iteration time: 2.66s
                        Total time: 2939.91s
                               ETA: 7315.2s

################################################################################
                     [1m Learning iteration 1147/4000 [0m

                       Computation: 3254 steps/s (collection: 0.502s, learning 2.016s)
               Value function loss: 66415.8164
                    Surrogate loss: 0.0169
             Mean action noise std: 0.92
                       Mean reward: 5624.65
               Mean episode length: 315.09
                 Mean success rate: 57.00
                  Mean reward/step: 17.31
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 2.52s
                        Total time: 2942.43s
                               ETA: 7312.5s

################################################################################
                     [1m Learning iteration 1148/4000 [0m

                       Computation: 3264 steps/s (collection: 0.460s, learning 2.049s)
               Value function loss: 60314.5058
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 5673.85
               Mean episode length: 311.49
                 Mean success rate: 57.00
                  Mean reward/step: 17.04
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 9412608
                    Iteration time: 2.51s
                        Total time: 2944.94s
                               ETA: 7309.8s

################################################################################
                     [1m Learning iteration 1149/4000 [0m

                       Computation: 3307 steps/s (collection: 0.424s, learning 2.054s)
               Value function loss: 57029.6506
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 5534.09
               Mean episode length: 302.60
                 Mean success rate: 56.50
                  Mean reward/step: 17.42
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 2.48s
                        Total time: 2947.42s
                               ETA: 7307.0s

################################################################################
                     [1m Learning iteration 1150/4000 [0m

                       Computation: 3284 steps/s (collection: 0.474s, learning 2.020s)
               Value function loss: 83105.7631
                    Surrogate loss: 0.0128
             Mean action noise std: 0.92
                       Mean reward: 5274.53
               Mean episode length: 295.02
                 Mean success rate: 55.00
                  Mean reward/step: 17.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9428992
                    Iteration time: 2.49s
                        Total time: 2949.91s
                               ETA: 7304.3s

################################################################################
                     [1m Learning iteration 1151/4000 [0m

                       Computation: 3285 steps/s (collection: 0.444s, learning 2.050s)
               Value function loss: 82541.3865
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 5730.24
               Mean episode length: 311.89
                 Mean success rate: 58.50
                  Mean reward/step: 16.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 2.49s
                        Total time: 2952.40s
                               ETA: 7301.6s

################################################################################
                     [1m Learning iteration 1152/4000 [0m

                       Computation: 3252 steps/s (collection: 0.464s, learning 2.055s)
               Value function loss: 71441.5014
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 5189.31
               Mean episode length: 288.33
                 Mean success rate: 53.50
                  Mean reward/step: 16.16
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9445376
                    Iteration time: 2.52s
                        Total time: 2954.92s
                               ETA: 7298.9s

################################################################################
                     [1m Learning iteration 1153/4000 [0m

                       Computation: 3216 steps/s (collection: 0.460s, learning 2.087s)
               Value function loss: 83595.0242
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 5078.47
               Mean episode length: 288.23
                 Mean success rate: 53.00
                  Mean reward/step: 16.09
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 2.55s
                        Total time: 2957.47s
                               ETA: 7296.3s

################################################################################
                     [1m Learning iteration 1154/4000 [0m

                       Computation: 3239 steps/s (collection: 0.471s, learning 2.058s)
               Value function loss: 74367.2101
                    Surrogate loss: 0.0177
             Mean action noise std: 0.92
                       Mean reward: 5493.47
               Mean episode length: 305.06
                 Mean success rate: 56.00
                  Mean reward/step: 15.83
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9461760
                    Iteration time: 2.53s
                        Total time: 2960.00s
                               ETA: 7293.6s

################################################################################
                     [1m Learning iteration 1155/4000 [0m

                       Computation: 3294 steps/s (collection: 0.435s, learning 2.052s)
               Value function loss: 63142.2550
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 5424.83
               Mean episode length: 305.47
                 Mean success rate: 56.00
                  Mean reward/step: 16.34
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 2.49s
                        Total time: 2962.49s
                               ETA: 7290.9s

################################################################################
                     [1m Learning iteration 1156/4000 [0m

                       Computation: 3231 steps/s (collection: 0.478s, learning 2.058s)
               Value function loss: 65082.8755
                    Surrogate loss: 0.0177
             Mean action noise std: 0.92
                       Mean reward: 5179.20
               Mean episode length: 301.88
                 Mean success rate: 53.50
                  Mean reward/step: 16.60
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9478144
                    Iteration time: 2.54s
                        Total time: 2965.02s
                               ETA: 7288.3s

################################################################################
                     [1m Learning iteration 1157/4000 [0m

                       Computation: 3254 steps/s (collection: 0.456s, learning 2.061s)
               Value function loss: 78523.5147
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 5483.94
               Mean episode length: 313.80
                 Mean success rate: 55.00
                  Mean reward/step: 16.05
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 2.52s
                        Total time: 2967.54s
                               ETA: 7285.6s

################################################################################
                     [1m Learning iteration 1158/4000 [0m

                       Computation: 3220 steps/s (collection: 0.451s, learning 2.093s)
               Value function loss: 96492.5437
                    Surrogate loss: 0.0131
             Mean action noise std: 0.92
                       Mean reward: 5200.11
               Mean episode length: 302.29
                 Mean success rate: 52.50
                  Mean reward/step: 15.31
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9494528
                    Iteration time: 2.54s
                        Total time: 2970.08s
                               ETA: 7283.0s

################################################################################
                     [1m Learning iteration 1159/4000 [0m

                       Computation: 3175 steps/s (collection: 0.495s, learning 2.085s)
               Value function loss: 66269.1409
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 5118.09
               Mean episode length: 300.54
                 Mean success rate: 52.00
                  Mean reward/step: 14.44
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 2.58s
                        Total time: 2972.66s
                               ETA: 7280.5s

################################################################################
                     [1m Learning iteration 1160/4000 [0m

                       Computation: 3124 steps/s (collection: 0.517s, learning 2.105s)
               Value function loss: 50771.9970
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 4571.05
               Mean episode length: 279.40
                 Mean success rate: 47.50
                  Mean reward/step: 14.77
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 9510912
                    Iteration time: 2.62s
                        Total time: 2975.28s
                               ETA: 7278.0s

################################################################################
                     [1m Learning iteration 1161/4000 [0m

                       Computation: 3129 steps/s (collection: 0.517s, learning 2.101s)
               Value function loss: 64322.4485
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 4727.23
               Mean episode length: 279.39
                 Mean success rate: 50.50
                  Mean reward/step: 15.27
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 2.62s
                        Total time: 2977.90s
                               ETA: 7275.6s

################################################################################
                     [1m Learning iteration 1162/4000 [0m

                       Computation: 3175 steps/s (collection: 0.466s, learning 2.114s)
               Value function loss: 63138.4892
                    Surrogate loss: 0.0121
             Mean action noise std: 0.92
                       Mean reward: 4686.83
               Mean episode length: 284.73
                 Mean success rate: 51.50
                  Mean reward/step: 15.77
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9527296
                    Iteration time: 2.58s
                        Total time: 2980.48s
                               ETA: 7273.1s

################################################################################
                     [1m Learning iteration 1163/4000 [0m

                       Computation: 3119 steps/s (collection: 0.498s, learning 2.129s)
               Value function loss: 77107.6752
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 4284.88
               Mean episode length: 268.33
                 Mean success rate: 48.00
                  Mean reward/step: 15.41
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 2.63s
                        Total time: 2983.11s
                               ETA: 7270.7s

################################################################################
                     [1m Learning iteration 1164/4000 [0m

                       Computation: 3022 steps/s (collection: 0.552s, learning 2.159s)
               Value function loss: 74733.3762
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 3940.78
               Mean episode length: 252.26
                 Mean success rate: 44.00
                  Mean reward/step: 15.73
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9543680
                    Iteration time: 2.71s
                        Total time: 2985.82s
                               ETA: 7268.5s

################################################################################
                     [1m Learning iteration 1165/4000 [0m

                       Computation: 3208 steps/s (collection: 0.470s, learning 2.084s)
               Value function loss: 49722.1275
                    Surrogate loss: 0.0189
             Mean action noise std: 0.92
                       Mean reward: 3754.62
               Mean episode length: 245.60
                 Mean success rate: 42.00
                  Mean reward/step: 15.87
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 2.55s
                        Total time: 2988.37s
                               ETA: 7265.9s

################################################################################
                     [1m Learning iteration 1166/4000 [0m

                       Computation: 3179 steps/s (collection: 0.488s, learning 2.088s)
               Value function loss: 61571.7128
                    Surrogate loss: 0.0174
             Mean action noise std: 0.92
                       Mean reward: 3821.58
               Mean episode length: 247.88
                 Mean success rate: 42.00
                  Mean reward/step: 15.97
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9560064
                    Iteration time: 2.58s
                        Total time: 2990.95s
                               ETA: 7263.4s

################################################################################
                     [1m Learning iteration 1167/4000 [0m

                       Computation: 3108 steps/s (collection: 0.470s, learning 2.165s)
               Value function loss: 57181.8992
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 3882.47
               Mean episode length: 254.84
                 Mean success rate: 41.50
                  Mean reward/step: 15.33
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 2.64s
                        Total time: 2993.58s
                               ETA: 7261.0s

################################################################################
                     [1m Learning iteration 1168/4000 [0m

                       Computation: 3148 steps/s (collection: 0.468s, learning 2.134s)
               Value function loss: 59619.6022
                    Surrogate loss: 0.0148
             Mean action noise std: 0.92
                       Mean reward: 3839.36
               Mean episode length: 256.25
                 Mean success rate: 41.00
                  Mean reward/step: 15.19
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 9576448
                    Iteration time: 2.60s
                        Total time: 2996.19s
                               ETA: 7258.5s

################################################################################
                     [1m Learning iteration 1169/4000 [0m

                       Computation: 3096 steps/s (collection: 0.523s, learning 2.122s)
               Value function loss: 77455.3749
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 3751.76
               Mean episode length: 250.04
                 Mean success rate: 40.00
                  Mean reward/step: 14.60
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 2.65s
                        Total time: 2998.83s
                               ETA: 7256.1s

################################################################################
                     [1m Learning iteration 1170/4000 [0m

                       Computation: 3184 steps/s (collection: 0.500s, learning 2.073s)
               Value function loss: 53674.0792
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 3430.01
               Mean episode length: 238.88
                 Mean success rate: 39.00
                  Mean reward/step: 15.60
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 9592832
                    Iteration time: 2.57s
                        Total time: 3001.40s
                               ETA: 7253.6s

################################################################################
                     [1m Learning iteration 1171/4000 [0m

                       Computation: 3143 steps/s (collection: 0.484s, learning 2.122s)
               Value function loss: 65143.3789
                    Surrogate loss: 0.0148
             Mean action noise std: 0.92
                       Mean reward: 3557.42
               Mean episode length: 249.34
                 Mean success rate: 40.50
                  Mean reward/step: 17.04
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 2.61s
                        Total time: 3004.01s
                               ETA: 7251.1s

################################################################################
                     [1m Learning iteration 1172/4000 [0m

                       Computation: 3118 steps/s (collection: 0.521s, learning 2.106s)
               Value function loss: 61085.9320
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 3839.82
               Mean episode length: 257.38
                 Mean success rate: 42.50
                  Mean reward/step: 17.86
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9609216
                    Iteration time: 2.63s
                        Total time: 3006.64s
                               ETA: 7248.7s

################################################################################
                     [1m Learning iteration 1173/4000 [0m

                       Computation: 3096 steps/s (collection: 0.507s, learning 2.139s)
               Value function loss: 71406.8399
                    Surrogate loss: 0.0168
             Mean action noise std: 0.92
                       Mean reward: 3971.51
               Mean episode length: 259.73
                 Mean success rate: 44.00
                  Mean reward/step: 19.11
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 2.65s
                        Total time: 3009.28s
                               ETA: 7246.4s

################################################################################
                     [1m Learning iteration 1174/4000 [0m

                       Computation: 3194 steps/s (collection: 0.478s, learning 2.087s)
               Value function loss: 79147.0320
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 4193.30
               Mean episode length: 267.83
                 Mean success rate: 45.50
                  Mean reward/step: 19.68
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9625600
                    Iteration time: 2.56s
                        Total time: 3011.85s
                               ETA: 7243.8s

################################################################################
                     [1m Learning iteration 1175/4000 [0m

                       Computation: 3175 steps/s (collection: 0.496s, learning 2.084s)
               Value function loss: 82326.9939
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 4515.90
               Mean episode length: 280.68
                 Mean success rate: 47.50
                  Mean reward/step: 19.33
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 2.58s
                        Total time: 3014.43s
                               ETA: 7241.3s

################################################################################
                     [1m Learning iteration 1176/4000 [0m

                       Computation: 3196 steps/s (collection: 0.483s, learning 2.079s)
               Value function loss: 83187.7719
                    Surrogate loss: 0.0148
             Mean action noise std: 0.92
                       Mean reward: 4879.02
               Mean episode length: 292.75
                 Mean success rate: 51.00
                  Mean reward/step: 18.93
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9641984
                    Iteration time: 2.56s
                        Total time: 3016.99s
                               ETA: 7238.7s

################################################################################
                     [1m Learning iteration 1177/4000 [0m

                       Computation: 3198 steps/s (collection: 0.446s, learning 2.115s)
               Value function loss: 52396.3814
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 5140.93
               Mean episode length: 297.21
                 Mean success rate: 53.00
                  Mean reward/step: 18.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 2.56s
                        Total time: 3019.55s
                               ETA: 7236.2s

################################################################################
                     [1m Learning iteration 1178/4000 [0m

                       Computation: 3135 steps/s (collection: 0.496s, learning 2.117s)
               Value function loss: 94992.4657
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 5639.45
               Mean episode length: 315.08
                 Mean success rate: 56.50
                  Mean reward/step: 18.83
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 9658368
                    Iteration time: 2.61s
                        Total time: 3022.16s
                               ETA: 7233.7s

################################################################################
                     [1m Learning iteration 1179/4000 [0m

                       Computation: 3241 steps/s (collection: 0.447s, learning 2.081s)
               Value function loss: 60384.8190
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 5924.75
               Mean episode length: 325.75
                 Mean success rate: 59.50
                  Mean reward/step: 18.32
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 2.53s
                        Total time: 3024.69s
                               ETA: 7231.1s

################################################################################
                     [1m Learning iteration 1180/4000 [0m

                       Computation: 3227 steps/s (collection: 0.463s, learning 2.075s)
               Value function loss: 72344.3564
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 5814.11
               Mean episode length: 322.98
                 Mean success rate: 59.00
                  Mean reward/step: 18.82
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9674752
                    Iteration time: 2.54s
                        Total time: 3027.23s
                               ETA: 7228.4s

################################################################################
                     [1m Learning iteration 1181/4000 [0m

                       Computation: 3191 steps/s (collection: 0.458s, learning 2.108s)
               Value function loss: 67796.5585
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 5819.41
               Mean episode length: 325.69
                 Mean success rate: 58.50
                  Mean reward/step: 18.87
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 2.57s
                        Total time: 3029.79s
                               ETA: 7225.9s

################################################################################
                     [1m Learning iteration 1182/4000 [0m

                       Computation: 3156 steps/s (collection: 0.518s, learning 2.077s)
               Value function loss: 74902.4875
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 5603.87
               Mean episode length: 311.94
                 Mean success rate: 55.50
                  Mean reward/step: 18.98
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9691136
                    Iteration time: 2.60s
                        Total time: 3032.39s
                               ETA: 7223.4s

################################################################################
                     [1m Learning iteration 1183/4000 [0m

                       Computation: 3234 steps/s (collection: 0.439s, learning 2.093s)
               Value function loss: 69987.9000
                    Surrogate loss: 0.0172
             Mean action noise std: 0.92
                       Mean reward: 5698.63
               Mean episode length: 316.52
                 Mean success rate: 57.00
                  Mean reward/step: 18.80
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 2.53s
                        Total time: 3034.92s
                               ETA: 7220.8s

################################################################################
                     [1m Learning iteration 1184/4000 [0m

                       Computation: 3161 steps/s (collection: 0.488s, learning 2.103s)
               Value function loss: 118231.5947
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 5733.25
               Mean episode length: 325.99
                 Mean success rate: 58.50
                  Mean reward/step: 18.67
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9707520
                    Iteration time: 2.59s
                        Total time: 3037.51s
                               ETA: 7218.3s

################################################################################
                     [1m Learning iteration 1185/4000 [0m

                       Computation: 3138 steps/s (collection: 0.522s, learning 2.088s)
               Value function loss: 84882.9352
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 5755.36
               Mean episode length: 327.04
                 Mean success rate: 58.00
                  Mean reward/step: 18.05
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 2.61s
                        Total time: 3040.12s
                               ETA: 7215.8s

################################################################################
                     [1m Learning iteration 1186/4000 [0m

                       Computation: 3205 steps/s (collection: 0.475s, learning 2.081s)
               Value function loss: 84018.8409
                    Surrogate loss: 0.0126
             Mean action noise std: 0.92
                       Mean reward: 5521.61
               Mean episode length: 316.42
                 Mean success rate: 55.50
                  Mean reward/step: 17.82
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9723904
                    Iteration time: 2.56s
                        Total time: 3042.68s
                               ETA: 7213.2s

################################################################################
                     [1m Learning iteration 1187/4000 [0m

                       Computation: 3139 steps/s (collection: 0.492s, learning 2.117s)
               Value function loss: 82138.5996
                    Surrogate loss: 0.0141
             Mean action noise std: 0.92
                       Mean reward: 5686.40
               Mean episode length: 311.61
                 Mean success rate: 56.00
                  Mean reward/step: 17.63
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 2.61s
                        Total time: 3045.29s
                               ETA: 7210.8s

################################################################################
                     [1m Learning iteration 1188/4000 [0m

                       Computation: 3160 steps/s (collection: 0.465s, learning 2.127s)
               Value function loss: 50765.1862
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 5835.35
               Mean episode length: 312.30
                 Mean success rate: 57.00
                  Mean reward/step: 17.78
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9740288
                    Iteration time: 2.59s
                        Total time: 3047.88s
                               ETA: 7208.3s

################################################################################
                     [1m Learning iteration 1189/4000 [0m

                       Computation: 3179 steps/s (collection: 0.490s, learning 2.086s)
               Value function loss: 77363.2672
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 5942.06
               Mean episode length: 313.72
                 Mean success rate: 58.50
                  Mean reward/step: 18.28
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 2.58s
                        Total time: 3050.46s
                               ETA: 7205.7s

################################################################################
                     [1m Learning iteration 1190/4000 [0m

                       Computation: 3099 steps/s (collection: 0.517s, learning 2.126s)
               Value function loss: 61337.0154
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 5663.63
               Mean episode length: 303.14
                 Mean success rate: 55.00
                  Mean reward/step: 19.03
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9756672
                    Iteration time: 2.64s
                        Total time: 3053.10s
                               ETA: 7203.4s

################################################################################
                     [1m Learning iteration 1191/4000 [0m

                       Computation: 3223 steps/s (collection: 0.478s, learning 2.063s)
               Value function loss: 90890.0951
                    Surrogate loss: 0.0124
             Mean action noise std: 0.92
                       Mean reward: 5800.16
               Mean episode length: 304.02
                 Mean success rate: 55.00
                  Mean reward/step: 19.83
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 2.54s
                        Total time: 3055.64s
                               ETA: 7200.7s

################################################################################
                     [1m Learning iteration 1192/4000 [0m

                       Computation: 3209 steps/s (collection: 0.475s, learning 2.077s)
               Value function loss: 83378.8858
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 5388.05
               Mean episode length: 289.24
                 Mean success rate: 52.50
                  Mean reward/step: 20.21
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 9773056
                    Iteration time: 2.55s
                        Total time: 3058.19s
                               ETA: 7198.2s

################################################################################
                     [1m Learning iteration 1193/4000 [0m

                       Computation: 3187 steps/s (collection: 0.474s, learning 2.096s)
               Value function loss: 61626.1700
                    Surrogate loss: 0.0173
             Mean action noise std: 0.92
                       Mean reward: 5290.00
               Mean episode length: 287.49
                 Mean success rate: 52.00
                  Mean reward/step: 19.97
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 2.57s
                        Total time: 3060.76s
                               ETA: 7195.6s

################################################################################
                     [1m Learning iteration 1194/4000 [0m

                       Computation: 3120 steps/s (collection: 0.483s, learning 2.142s)
               Value function loss: 104480.8361
                    Surrogate loss: 0.0155
             Mean action noise std: 0.92
                       Mean reward: 5671.63
               Mean episode length: 302.90
                 Mean success rate: 55.00
                  Mean reward/step: 19.67
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9789440
                    Iteration time: 2.62s
                        Total time: 3063.39s
                               ETA: 7193.2s

################################################################################
                     [1m Learning iteration 1195/4000 [0m

                       Computation: 3179 steps/s (collection: 0.507s, learning 2.069s)
               Value function loss: 52795.8359
                    Surrogate loss: 0.0174
             Mean action noise std: 0.92
                       Mean reward: 5432.29
               Mean episode length: 296.52
                 Mean success rate: 53.00
                  Mean reward/step: 18.97
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 2.58s
                        Total time: 3065.96s
                               ETA: 7190.7s

################################################################################
                     [1m Learning iteration 1196/4000 [0m

                       Computation: 3231 steps/s (collection: 0.462s, learning 2.073s)
               Value function loss: 50318.4170
                    Surrogate loss: 0.0196
             Mean action noise std: 0.92
                       Mean reward: 5503.12
               Mean episode length: 303.27
                 Mean success rate: 54.00
                  Mean reward/step: 19.19
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9805824
                    Iteration time: 2.54s
                        Total time: 3068.50s
                               ETA: 7188.0s

################################################################################
                     [1m Learning iteration 1197/4000 [0m

                       Computation: 3133 steps/s (collection: 0.469s, learning 2.145s)
               Value function loss: 71106.5991
                    Surrogate loss: 0.0171
             Mean action noise std: 0.92
                       Mean reward: 5415.75
               Mean episode length: 296.87
                 Mean success rate: 53.00
                  Mean reward/step: 19.06
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 2.61s
                        Total time: 3071.11s
                               ETA: 7185.6s

################################################################################
                     [1m Learning iteration 1198/4000 [0m

                       Computation: 3157 steps/s (collection: 0.491s, learning 2.103s)
               Value function loss: 86363.3158
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 5640.41
               Mean episode length: 305.60
                 Mean success rate: 55.00
                  Mean reward/step: 19.08
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9822208
                    Iteration time: 2.59s
                        Total time: 3073.71s
                               ETA: 7183.1s

################################################################################
                     [1m Learning iteration 1199/4000 [0m

                       Computation: 3222 steps/s (collection: 0.463s, learning 2.079s)
               Value function loss: 58909.9279
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 5958.67
               Mean episode length: 318.09
                 Mean success rate: 58.00
                  Mean reward/step: 18.74
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 2.54s
                        Total time: 3076.25s
                               ETA: 7180.5s

################################################################################
                     [1m Learning iteration 1200/4000 [0m

                       Computation: 3290 steps/s (collection: 0.444s, learning 2.046s)
               Value function loss: 96189.2999
                    Surrogate loss: 0.0152
             Mean action noise std: 0.92
                       Mean reward: 6392.69
               Mean episode length: 333.57
                 Mean success rate: 61.50
                  Mean reward/step: 18.36
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9838592
                    Iteration time: 2.49s
                        Total time: 3078.74s
                               ETA: 7177.7s

################################################################################
                     [1m Learning iteration 1201/4000 [0m

                       Computation: 3206 steps/s (collection: 0.468s, learning 2.087s)
               Value function loss: 76112.3598
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 6812.91
               Mean episode length: 351.80
                 Mean success rate: 65.00
                  Mean reward/step: 17.47
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 2.56s
                        Total time: 3081.30s
                               ETA: 7175.2s

################################################################################
                     [1m Learning iteration 1202/4000 [0m

                       Computation: 3156 steps/s (collection: 0.460s, learning 2.136s)
               Value function loss: 96629.4328
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 7214.41
               Mean episode length: 364.38
                 Mean success rate: 69.00
                  Mean reward/step: 16.50
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9854976
                    Iteration time: 2.60s
                        Total time: 3083.89s
                               ETA: 7172.7s

################################################################################
                     [1m Learning iteration 1203/4000 [0m

                       Computation: 3112 steps/s (collection: 0.478s, learning 2.154s)
               Value function loss: 87472.2481
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 7349.13
               Mean episode length: 370.13
                 Mean success rate: 69.50
                  Mean reward/step: 15.18
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 2.63s
                        Total time: 3086.52s
                               ETA: 7170.3s

################################################################################
                     [1m Learning iteration 1204/4000 [0m

                       Computation: 3187 steps/s (collection: 0.499s, learning 2.072s)
               Value function loss: 69571.0061
                    Surrogate loss: 0.0200
             Mean action noise std: 0.92
                       Mean reward: 7458.39
               Mean episode length: 376.01
                 Mean success rate: 71.50
                  Mean reward/step: 15.16
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9871360
                    Iteration time: 2.57s
                        Total time: 3089.09s
                               ETA: 7167.7s

################################################################################
                     [1m Learning iteration 1205/4000 [0m

                       Computation: 3236 steps/s (collection: 0.473s, learning 2.058s)
               Value function loss: 62031.0543
                    Surrogate loss: 0.0203
             Mean action noise std: 0.92
                       Mean reward: 7270.41
               Mean episode length: 369.07
                 Mean success rate: 70.00
                  Mean reward/step: 15.95
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 2.53s
                        Total time: 3091.62s
                               ETA: 7165.1s

################################################################################
                     [1m Learning iteration 1206/4000 [0m

                       Computation: 3212 steps/s (collection: 0.450s, learning 2.100s)
               Value function loss: 67028.7942
                    Surrogate loss: 0.0184
             Mean action noise std: 0.92
                       Mean reward: 7059.77
               Mean episode length: 366.44
                 Mean success rate: 70.50
                  Mean reward/step: 16.26
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9887744
                    Iteration time: 2.55s
                        Total time: 3094.17s
                               ETA: 7162.5s

################################################################################
                     [1m Learning iteration 1207/4000 [0m

                       Computation: 3198 steps/s (collection: 0.470s, learning 2.091s)
               Value function loss: 69846.1020
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 6877.77
               Mean episode length: 362.16
                 Mean success rate: 69.00
                  Mean reward/step: 16.07
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 2.56s
                        Total time: 3096.74s
                               ETA: 7159.9s

################################################################################
                     [1m Learning iteration 1208/4000 [0m

                       Computation: 3290 steps/s (collection: 0.441s, learning 2.048s)
               Value function loss: 70086.6062
                    Surrogate loss: 0.0181
             Mean action noise std: 0.92
                       Mean reward: 6096.29
               Mean episode length: 335.93
                 Mean success rate: 63.00
                  Mean reward/step: 16.64
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9904128
                    Iteration time: 2.49s
                        Total time: 3099.22s
                               ETA: 7157.2s

################################################################################
                     [1m Learning iteration 1209/4000 [0m

                       Computation: 3211 steps/s (collection: 0.457s, learning 2.093s)
               Value function loss: 73188.1817
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 6027.22
               Mean episode length: 337.31
                 Mean success rate: 63.00
                  Mean reward/step: 17.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 2.55s
                        Total time: 3101.78s
                               ETA: 7154.6s

################################################################################
                     [1m Learning iteration 1210/4000 [0m

                       Computation: 3177 steps/s (collection: 0.481s, learning 2.097s)
               Value function loss: 67705.6862
                    Surrogate loss: 0.0168
             Mean action noise std: 0.92
                       Mean reward: 5764.04
               Mean episode length: 333.63
                 Mean success rate: 60.50
                  Mean reward/step: 17.88
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9920512
                    Iteration time: 2.58s
                        Total time: 3104.35s
                               ETA: 7152.1s

################################################################################
                     [1m Learning iteration 1211/4000 [0m

                       Computation: 3236 steps/s (collection: 0.475s, learning 2.056s)
               Value function loss: 64941.2887
                    Surrogate loss: 0.0207
             Mean action noise std: 0.92
                       Mean reward: 5099.07
               Mean episode length: 315.21
                 Mean success rate: 56.50
                  Mean reward/step: 18.25
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 2.53s
                        Total time: 3106.88s
                               ETA: 7149.4s

################################################################################
                     [1m Learning iteration 1212/4000 [0m

                       Computation: 3236 steps/s (collection: 0.447s, learning 2.084s)
               Value function loss: 67763.9752
                    Surrogate loss: 0.0191
             Mean action noise std: 0.92
                       Mean reward: 5313.02
               Mean episode length: 323.32
                 Mean success rate: 57.00
                  Mean reward/step: 18.80
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9936896
                    Iteration time: 2.53s
                        Total time: 3109.42s
                               ETA: 7146.8s

################################################################################
                     [1m Learning iteration 1213/4000 [0m

                       Computation: 3137 steps/s (collection: 0.491s, learning 2.119s)
               Value function loss: 62148.7011
                    Surrogate loss: 0.0180
             Mean action noise std: 0.92
                       Mean reward: 5311.16
               Mean episode length: 325.67
                 Mean success rate: 56.50
                  Mean reward/step: 19.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 2.61s
                        Total time: 3112.03s
                               ETA: 7144.3s

################################################################################
                     [1m Learning iteration 1214/4000 [0m

                       Computation: 3124 steps/s (collection: 0.507s, learning 2.114s)
               Value function loss: 67403.5167
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 5514.55
               Mean episode length: 331.37
                 Mean success rate: 58.00
                  Mean reward/step: 19.10
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9953280
                    Iteration time: 2.62s
                        Total time: 3114.65s
                               ETA: 7141.9s

################################################################################
                     [1m Learning iteration 1215/4000 [0m

                       Computation: 3212 steps/s (collection: 0.469s, learning 2.082s)
               Value function loss: 69967.2252
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 5338.32
               Mean episode length: 321.93
                 Mean success rate: 55.50
                  Mean reward/step: 19.79
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 2.55s
                        Total time: 3117.20s
                               ETA: 7139.3s

################################################################################
                     [1m Learning iteration 1216/4000 [0m

                       Computation: 3213 steps/s (collection: 0.449s, learning 2.100s)
               Value function loss: 81024.2637
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 5522.55
               Mean episode length: 327.37
                 Mean success rate: 57.00
                  Mean reward/step: 20.25
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9969664
                    Iteration time: 2.55s
                        Total time: 3119.75s
                               ETA: 7136.7s

################################################################################
                     [1m Learning iteration 1217/4000 [0m

                       Computation: 3202 steps/s (collection: 0.476s, learning 2.082s)
               Value function loss: 48857.6401
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 5266.87
               Mean episode length: 311.75
                 Mean success rate: 54.00
                  Mean reward/step: 19.36
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 2.56s
                        Total time: 3122.31s
                               ETA: 7134.1s

################################################################################
                     [1m Learning iteration 1218/4000 [0m

                       Computation: 3237 steps/s (collection: 0.489s, learning 2.041s)
               Value function loss: 87575.3024
                    Surrogate loss: 0.0173
             Mean action noise std: 0.92
                       Mean reward: 5159.69
               Mean episode length: 306.82
                 Mean success rate: 52.50
                  Mean reward/step: 18.88
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9986048
                    Iteration time: 2.53s
                        Total time: 3124.84s
                               ETA: 7131.5s

################################################################################
                     [1m Learning iteration 1219/4000 [0m

                       Computation: 3243 steps/s (collection: 0.444s, learning 2.082s)
               Value function loss: 65091.6123
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 5410.25
               Mean episode length: 313.17
                 Mean success rate: 53.00
                  Mean reward/step: 19.07
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 2.53s
                        Total time: 3127.36s
                               ETA: 7128.8s

################################################################################
                     [1m Learning iteration 1220/4000 [0m

                       Computation: 3123 steps/s (collection: 0.517s, learning 2.106s)
               Value function loss: 83131.5045
                    Surrogate loss: 0.0182
             Mean action noise std: 0.92
                       Mean reward: 5101.61
               Mean episode length: 302.46
                 Mean success rate: 52.00
                  Mean reward/step: 19.73
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10002432
                    Iteration time: 2.62s
                        Total time: 3129.99s
                               ETA: 7126.4s

################################################################################
                     [1m Learning iteration 1221/4000 [0m

                       Computation: 3192 steps/s (collection: 0.466s, learning 2.100s)
               Value function loss: 89141.1693
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 5110.39
               Mean episode length: 299.72
                 Mean success rate: 51.50
                  Mean reward/step: 19.25
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 2.57s
                        Total time: 3132.55s
                               ETA: 7123.9s

################################################################################
                     [1m Learning iteration 1222/4000 [0m

                       Computation: 3095 steps/s (collection: 0.559s, learning 2.088s)
               Value function loss: 94016.3875
                    Surrogate loss: 0.0169
             Mean action noise std: 0.92
                       Mean reward: 5448.77
               Mean episode length: 304.68
                 Mean success rate: 53.50
                  Mean reward/step: 18.61
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10018816
                    Iteration time: 2.65s
                        Total time: 3135.20s
                               ETA: 7121.5s

################################################################################
                     [1m Learning iteration 1223/4000 [0m

                       Computation: 3296 steps/s (collection: 0.423s, learning 2.062s)
               Value function loss: 48315.4684
                    Surrogate loss: 0.0184
             Mean action noise std: 0.92
                       Mean reward: 5438.58
               Mean episode length: 301.48
                 Mean success rate: 52.50
                  Mean reward/step: 18.71
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 2.49s
                        Total time: 3137.68s
                               ETA: 7118.7s

################################################################################
                     [1m Learning iteration 1224/4000 [0m

                       Computation: 3232 steps/s (collection: 0.452s, learning 2.082s)
               Value function loss: 69893.1082
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 5721.92
               Mean episode length: 308.83
                 Mean success rate: 54.50
                  Mean reward/step: 19.78
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10035200
                    Iteration time: 2.53s
                        Total time: 3140.22s
                               ETA: 7116.1s

################################################################################
                     [1m Learning iteration 1225/4000 [0m

                       Computation: 3206 steps/s (collection: 0.490s, learning 2.065s)
               Value function loss: 101500.7614
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 6121.95
               Mean episode length: 320.45
                 Mean success rate: 57.50
                  Mean reward/step: 19.95
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 2.56s
                        Total time: 3142.77s
                               ETA: 7113.5s

################################################################################
                     [1m Learning iteration 1226/4000 [0m

                       Computation: 3266 steps/s (collection: 0.469s, learning 2.039s)
               Value function loss: 67261.2508
                    Surrogate loss: 0.0217
             Mean action noise std: 0.92
                       Mean reward: 6081.17
               Mean episode length: 318.18
                 Mean success rate: 57.00
                  Mean reward/step: 19.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10051584
                    Iteration time: 2.51s
                        Total time: 3145.28s
                               ETA: 7110.8s

################################################################################
                     [1m Learning iteration 1227/4000 [0m

                       Computation: 3253 steps/s (collection: 0.449s, learning 2.069s)
               Value function loss: 74613.1108
                    Surrogate loss: 0.0179
             Mean action noise std: 0.92
                       Mean reward: 6322.39
               Mean episode length: 321.04
                 Mean success rate: 59.00
                  Mean reward/step: 20.13
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 2.52s
                        Total time: 3147.80s
                               ETA: 7108.2s

################################################################################
                     [1m Learning iteration 1228/4000 [0m

                       Computation: 3211 steps/s (collection: 0.463s, learning 2.088s)
               Value function loss: 87734.3485
                    Surrogate loss: 0.0168
             Mean action noise std: 0.92
                       Mean reward: 6771.70
               Mean episode length: 334.55
                 Mean success rate: 61.50
                  Mean reward/step: 20.11
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10067968
                    Iteration time: 2.55s
                        Total time: 3150.35s
                               ETA: 7105.6s

################################################################################
                     [1m Learning iteration 1229/4000 [0m

                       Computation: 3246 steps/s (collection: 0.447s, learning 2.076s)
               Value function loss: 94248.2150
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 6802.50
               Mean episode length: 332.55
                 Mean success rate: 61.50
                  Mean reward/step: 20.16
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 2.52s
                        Total time: 3152.87s
                               ETA: 7102.9s

################################################################################
                     [1m Learning iteration 1230/4000 [0m

                       Computation: 3158 steps/s (collection: 0.462s, learning 2.132s)
               Value function loss: 42778.9436
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 6844.34
               Mean episode length: 333.28
                 Mean success rate: 62.00
                  Mean reward/step: 20.49
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10084352
                    Iteration time: 2.59s
                        Total time: 3155.47s
                               ETA: 7100.4s

################################################################################
                     [1m Learning iteration 1231/4000 [0m

                       Computation: 3219 steps/s (collection: 0.443s, learning 2.101s)
               Value function loss: 88797.8996
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 6708.57
               Mean episode length: 339.63
                 Mean success rate: 63.00
                  Mean reward/step: 20.95
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 2.54s
                        Total time: 3158.01s
                               ETA: 7097.8s

################################################################################
                     [1m Learning iteration 1232/4000 [0m

                       Computation: 3239 steps/s (collection: 0.442s, learning 2.087s)
               Value function loss: 107360.2119
                    Surrogate loss: 0.0134
             Mean action noise std: 0.92
                       Mean reward: 6631.79
               Mean episode length: 334.84
                 Mean success rate: 62.00
                  Mean reward/step: 20.16
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 10100736
                    Iteration time: 2.53s
                        Total time: 3160.54s
                               ETA: 7095.2s

################################################################################
                     [1m Learning iteration 1233/4000 [0m

                       Computation: 3252 steps/s (collection: 0.458s, learning 2.060s)
               Value function loss: 83580.6330
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 6580.13
               Mean episode length: 332.90
                 Mean success rate: 62.00
                  Mean reward/step: 19.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 2.52s
                        Total time: 3163.06s
                               ETA: 7092.5s

################################################################################
                     [1m Learning iteration 1234/4000 [0m

                       Computation: 3228 steps/s (collection: 0.500s, learning 2.038s)
               Value function loss: 77959.4614
                    Surrogate loss: 0.0175
             Mean action noise std: 0.92
                       Mean reward: 6779.71
               Mean episode length: 337.85
                 Mean success rate: 63.00
                  Mean reward/step: 19.88
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10117120
                    Iteration time: 2.54s
                        Total time: 3165.60s
                               ETA: 7089.9s

################################################################################
                     [1m Learning iteration 1235/4000 [0m

                       Computation: 3210 steps/s (collection: 0.461s, learning 2.091s)
               Value function loss: 68564.1665
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 6693.28
               Mean episode length: 332.23
                 Mean success rate: 62.00
                  Mean reward/step: 20.67
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 2.55s
                        Total time: 3168.15s
                               ETA: 7087.3s

################################################################################
                     [1m Learning iteration 1236/4000 [0m

                       Computation: 3215 steps/s (collection: 0.494s, learning 2.053s)
               Value function loss: 107121.1325
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 6824.30
               Mean episode length: 339.21
                 Mean success rate: 64.00
                  Mean reward/step: 20.25
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10133504
                    Iteration time: 2.55s
                        Total time: 3170.69s
                               ETA: 7084.7s

################################################################################
                     [1m Learning iteration 1237/4000 [0m

                       Computation: 3206 steps/s (collection: 0.456s, learning 2.099s)
               Value function loss: 106401.2252
                    Surrogate loss: 0.0166
             Mean action noise std: 0.92
                       Mean reward: 6726.47
               Mean episode length: 341.00
                 Mean success rate: 64.00
                  Mean reward/step: 19.67
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 2.56s
                        Total time: 3173.25s
                               ETA: 7082.1s

################################################################################
                     [1m Learning iteration 1238/4000 [0m

                       Computation: 3252 steps/s (collection: 0.466s, learning 2.052s)
               Value function loss: 75378.4883
                    Surrogate loss: 0.0163
             Mean action noise std: 0.92
                       Mean reward: 7031.60
               Mean episode length: 348.42
                 Mean success rate: 65.50
                  Mean reward/step: 18.92
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10149888
                    Iteration time: 2.52s
                        Total time: 3175.77s
                               ETA: 7079.5s

################################################################################
                     [1m Learning iteration 1239/4000 [0m

                       Computation: 3189 steps/s (collection: 0.461s, learning 2.107s)
               Value function loss: 76972.7312
                    Surrogate loss: 0.0167
             Mean action noise std: 0.92
                       Mean reward: 7167.30
               Mean episode length: 348.38
                 Mean success rate: 66.00
                  Mean reward/step: 18.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 2.57s
                        Total time: 3178.34s
                               ETA: 7076.9s

################################################################################
                     [1m Learning iteration 1240/4000 [0m

                       Computation: 3201 steps/s (collection: 0.507s, learning 2.052s)
               Value function loss: 83324.0258
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 7190.46
               Mean episode length: 352.10
                 Mean success rate: 66.00
                  Mean reward/step: 19.31
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10166272
                    Iteration time: 2.56s
                        Total time: 3180.90s
                               ETA: 7074.4s

################################################################################
                     [1m Learning iteration 1241/4000 [0m

                       Computation: 3194 steps/s (collection: 0.514s, learning 2.050s)
               Value function loss: 75766.9009
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 7099.14
               Mean episode length: 354.92
                 Mean success rate: 66.00
                  Mean reward/step: 19.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 2.56s
                        Total time: 3183.46s
                               ETA: 7071.8s

################################################################################
                     [1m Learning iteration 1242/4000 [0m

                       Computation: 3185 steps/s (collection: 0.471s, learning 2.101s)
               Value function loss: 83154.9103
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 7037.89
               Mean episode length: 350.19
                 Mean success rate: 65.00
                  Mean reward/step: 19.35
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10182656
                    Iteration time: 2.57s
                        Total time: 3186.03s
                               ETA: 7069.2s

################################################################################
                     [1m Learning iteration 1243/4000 [0m

                       Computation: 3259 steps/s (collection: 0.473s, learning 2.040s)
               Value function loss: 65237.9683
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 7042.16
               Mean episode length: 350.50
                 Mean success rate: 64.50
                  Mean reward/step: 19.23
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 2.51s
                        Total time: 3188.54s
                               ETA: 7066.6s

################################################################################
                     [1m Learning iteration 1244/4000 [0m

                       Computation: 3267 steps/s (collection: 0.487s, learning 2.020s)
               Value function loss: 83222.4070
                    Surrogate loss: 0.0170
             Mean action noise std: 0.92
                       Mean reward: 6860.97
               Mean episode length: 348.75
                 Mean success rate: 63.50
                  Mean reward/step: 19.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10199040
                    Iteration time: 2.51s
                        Total time: 3191.05s
                               ETA: 7063.9s

################################################################################
                     [1m Learning iteration 1245/4000 [0m

                       Computation: 3228 steps/s (collection: 0.493s, learning 2.045s)
               Value function loss: 79114.6624
                    Surrogate loss: 0.0185
             Mean action noise std: 0.92
                       Mean reward: 6862.94
               Mean episode length: 351.99
                 Mean success rate: 64.50
                  Mean reward/step: 19.77
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 2.54s
                        Total time: 3193.59s
                               ETA: 7061.3s

################################################################################
                     [1m Learning iteration 1246/4000 [0m

                       Computation: 3245 steps/s (collection: 0.483s, learning 2.042s)
               Value function loss: 51227.3718
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 6690.92
               Mean episode length: 345.03
                 Mean success rate: 62.00
                  Mean reward/step: 20.19
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10215424
                    Iteration time: 2.52s
                        Total time: 3196.11s
                               ETA: 7058.6s

################################################################################
                     [1m Learning iteration 1247/4000 [0m

                       Computation: 3211 steps/s (collection: 0.481s, learning 2.070s)
               Value function loss: 119023.0777
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 6183.49
               Mean episode length: 328.51
                 Mean success rate: 57.50
                  Mean reward/step: 20.40
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 2.55s
                        Total time: 3198.66s
                               ETA: 7056.0s

################################################################################
                     [1m Learning iteration 1248/4000 [0m

                       Computation: 3214 steps/s (collection: 0.477s, learning 2.072s)
               Value function loss: 89968.2392
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 6513.53
               Mean episode length: 338.80
                 Mean success rate: 60.50
                  Mean reward/step: 19.45
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10231808
                    Iteration time: 2.55s
                        Total time: 3201.21s
                               ETA: 7053.4s

################################################################################
                     [1m Learning iteration 1249/4000 [0m

                       Computation: 3255 steps/s (collection: 0.472s, learning 2.045s)
               Value function loss: 64532.8261
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 6341.17
               Mean episode length: 329.95
                 Mean success rate: 59.00
                  Mean reward/step: 19.64
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 2.52s
                        Total time: 3203.73s
                               ETA: 7050.8s

################################################################################
                     [1m Learning iteration 1250/4000 [0m

                       Computation: 3117 steps/s (collection: 0.515s, learning 2.113s)
               Value function loss: 64722.6131
                    Surrogate loss: 0.0202
             Mean action noise std: 0.92
                       Mean reward: 6199.56
               Mean episode length: 321.44
                 Mean success rate: 57.50
                  Mean reward/step: 19.79
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10248192
                    Iteration time: 2.63s
                        Total time: 3206.36s
                               ETA: 7048.3s

################################################################################
                     [1m Learning iteration 1251/4000 [0m

                       Computation: 3194 steps/s (collection: 0.477s, learning 2.087s)
               Value function loss: 97283.0935
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 6233.50
               Mean episode length: 323.62
                 Mean success rate: 58.50
                  Mean reward/step: 19.71
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 2.56s
                        Total time: 3208.92s
                               ETA: 7045.8s

################################################################################
                     [1m Learning iteration 1252/4000 [0m

                       Computation: 3226 steps/s (collection: 0.492s, learning 2.047s)
               Value function loss: 91334.3275
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 6013.94
               Mean episode length: 307.00
                 Mean success rate: 55.50
                  Mean reward/step: 18.87
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 10264576
                    Iteration time: 2.54s
                        Total time: 3211.46s
                               ETA: 7043.2s

################################################################################
                     [1m Learning iteration 1253/4000 [0m

                       Computation: 3210 steps/s (collection: 0.490s, learning 2.062s)
               Value function loss: 119572.5500
                    Surrogate loss: 0.0114
             Mean action noise std: 0.92
                       Mean reward: 6443.00
               Mean episode length: 319.56
                 Mean success rate: 60.00
                  Mean reward/step: 18.43
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 2.55s
                        Total time: 3214.01s
                               ETA: 7040.6s

################################################################################
                     [1m Learning iteration 1254/4000 [0m

                       Computation: 3254 steps/s (collection: 0.482s, learning 2.035s)
               Value function loss: 71167.3068
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 6404.70
               Mean episode length: 318.69
                 Mean success rate: 60.50
                  Mean reward/step: 18.27
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10280960
                    Iteration time: 2.52s
                        Total time: 3216.53s
                               ETA: 7037.9s

################################################################################
                     [1m Learning iteration 1255/4000 [0m

                       Computation: 3227 steps/s (collection: 0.467s, learning 2.072s)
               Value function loss: 59852.5205
                    Surrogate loss: 0.0169
             Mean action noise std: 0.92
                       Mean reward: 5906.80
               Mean episode length: 300.24
                 Mean success rate: 56.00
                  Mean reward/step: 19.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 2.54s
                        Total time: 3219.07s
                               ETA: 7035.3s

################################################################################
                     [1m Learning iteration 1256/4000 [0m

                       Computation: 3217 steps/s (collection: 0.474s, learning 2.072s)
               Value function loss: 100507.4654
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 5823.76
               Mean episode length: 299.55
                 Mean success rate: 56.00
                  Mean reward/step: 19.45
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 10297344
                    Iteration time: 2.55s
                        Total time: 3221.61s
                               ETA: 7032.7s

################################################################################
                     [1m Learning iteration 1257/4000 [0m

                       Computation: 3207 steps/s (collection: 0.485s, learning 2.068s)
               Value function loss: 81823.3868
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 5981.57
               Mean episode length: 304.29
                 Mean success rate: 56.50
                  Mean reward/step: 19.36
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 2.55s
                        Total time: 3224.17s
                               ETA: 7030.1s

################################################################################
                     [1m Learning iteration 1258/4000 [0m

                       Computation: 3283 steps/s (collection: 0.447s, learning 2.048s)
               Value function loss: 47692.4354
                    Surrogate loss: 0.0170
             Mean action noise std: 0.92
                       Mean reward: 5978.07
               Mean episode length: 308.65
                 Mean success rate: 56.50
                  Mean reward/step: 19.99
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10313728
                    Iteration time: 2.49s
                        Total time: 3226.66s
                               ETA: 7027.4s

################################################################################
                     [1m Learning iteration 1259/4000 [0m

                       Computation: 3294 steps/s (collection: 0.455s, learning 2.032s)
               Value function loss: 74034.1521
                    Surrogate loss: 0.0155
             Mean action noise std: 0.92
                       Mean reward: 6163.88
               Mean episode length: 314.00
                 Mean success rate: 58.00
                  Mean reward/step: 20.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 2.49s
                        Total time: 3229.15s
                               ETA: 7024.7s

################################################################################
                     [1m Learning iteration 1260/4000 [0m

                       Computation: 3219 steps/s (collection: 0.491s, learning 2.053s)
               Value function loss: 71891.4133
                    Surrogate loss: 0.0132
             Mean action noise std: 0.92
                       Mean reward: 5943.44
               Mean episode length: 309.03
                 Mean success rate: 55.00
                  Mean reward/step: 20.98
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10330112
                    Iteration time: 2.54s
                        Total time: 3231.69s
                               ETA: 7022.1s

################################################################################
                     [1m Learning iteration 1261/4000 [0m

                       Computation: 3226 steps/s (collection: 0.481s, learning 2.059s)
               Value function loss: 56874.9940
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 5820.35
               Mean episode length: 306.67
                 Mean success rate: 53.50
                  Mean reward/step: 20.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 2.54s
                        Total time: 3234.23s
                               ETA: 7019.5s

################################################################################
                     [1m Learning iteration 1262/4000 [0m

                       Computation: 3307 steps/s (collection: 0.437s, learning 2.040s)
               Value function loss: 63757.3811
                    Surrogate loss: 0.0166
             Mean action noise std: 0.92
                       Mean reward: 5673.77
               Mean episode length: 302.42
                 Mean success rate: 53.00
                  Mean reward/step: 21.18
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10346496
                    Iteration time: 2.48s
                        Total time: 3236.71s
                               ETA: 7016.7s

################################################################################
                     [1m Learning iteration 1263/4000 [0m

                       Computation: 3218 steps/s (collection: 0.462s, learning 2.083s)
               Value function loss: 130998.8432
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 6389.03
               Mean episode length: 323.17
                 Mean success rate: 59.50
                  Mean reward/step: 20.24
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 2.55s
                        Total time: 3239.25s
                               ETA: 7014.1s

################################################################################
                     [1m Learning iteration 1264/4000 [0m

                       Computation: 3191 steps/s (collection: 0.473s, learning 2.093s)
               Value function loss: 67920.6642
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 6594.90
               Mean episode length: 329.18
                 Mean success rate: 61.00
                  Mean reward/step: 19.95
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10362880
                    Iteration time: 2.57s
                        Total time: 3241.82s
                               ETA: 7011.6s

################################################################################
                     [1m Learning iteration 1265/4000 [0m

                       Computation: 3239 steps/s (collection: 0.463s, learning 2.066s)
               Value function loss: 70799.4349
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 6637.58
               Mean episode length: 329.87
                 Mean success rate: 61.00
                  Mean reward/step: 21.43
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 2.53s
                        Total time: 3244.35s
                               ETA: 7008.9s

################################################################################
                     [1m Learning iteration 1266/4000 [0m

                       Computation: 3148 steps/s (collection: 0.488s, learning 2.114s)
               Value function loss: 91322.3609
                    Surrogate loss: 0.0259
             Mean action noise std: 0.92
                       Mean reward: 6602.37
               Mean episode length: 331.93
                 Mean success rate: 61.50
                  Mean reward/step: 21.71
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10379264
                    Iteration time: 2.60s
                        Total time: 3246.95s
                               ETA: 7006.4s

################################################################################
                     [1m Learning iteration 1267/4000 [0m

                       Computation: 3220 steps/s (collection: 0.479s, learning 2.065s)
               Value function loss: 84314.6138
                    Surrogate loss: 0.0183
             Mean action noise std: 0.92
                       Mean reward: 6432.08
               Mean episode length: 321.49
                 Mean success rate: 59.50
                  Mean reward/step: 21.04
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 2.54s
                        Total time: 3249.50s
                               ETA: 7003.8s

################################################################################
                     [1m Learning iteration 1268/4000 [0m

                       Computation: 3260 steps/s (collection: 0.419s, learning 2.093s)
               Value function loss: 107927.1324
                    Surrogate loss: 0.0116
             Mean action noise std: 0.92
                       Mean reward: 6546.00
               Mean episode length: 322.02
                 Mean success rate: 61.00
                  Mean reward/step: 20.56
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10395648
                    Iteration time: 2.51s
                        Total time: 3252.01s
                               ETA: 7001.2s

################################################################################
                     [1m Learning iteration 1269/4000 [0m

                       Computation: 3170 steps/s (collection: 0.473s, learning 2.111s)
               Value function loss: 96529.2312
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 6846.41
               Mean episode length: 333.99
                 Mean success rate: 64.00
                  Mean reward/step: 19.00
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 2.58s
                        Total time: 3254.59s
                               ETA: 6998.7s

################################################################################
                     [1m Learning iteration 1270/4000 [0m

                       Computation: 3195 steps/s (collection: 0.464s, learning 2.100s)
               Value function loss: 54525.7143
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 6648.26
               Mean episode length: 327.29
                 Mean success rate: 61.50
                  Mean reward/step: 18.56
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10412032
                    Iteration time: 2.56s
                        Total time: 3257.16s
                               ETA: 6996.1s

################################################################################
                     [1m Learning iteration 1271/4000 [0m

                       Computation: 3159 steps/s (collection: 0.468s, learning 2.125s)
               Value function loss: 107041.0082
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 6144.61
               Mean episode length: 311.38
                 Mean success rate: 57.50
                  Mean reward/step: 18.84
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 2.59s
                        Total time: 3259.75s
                               ETA: 6993.6s

################################################################################
                     [1m Learning iteration 1272/4000 [0m

                       Computation: 3207 steps/s (collection: 0.482s, learning 2.072s)
               Value function loss: 64382.0701
                    Surrogate loss: 0.0173
             Mean action noise std: 0.92
                       Mean reward: 6005.61
               Mean episode length: 303.60
                 Mean success rate: 56.50
                  Mean reward/step: 18.26
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10428416
                    Iteration time: 2.55s
                        Total time: 3262.30s
                               ETA: 6991.0s

################################################################################
                     [1m Learning iteration 1273/4000 [0m

                       Computation: 3170 steps/s (collection: 0.496s, learning 2.088s)
               Value function loss: 88694.9539
                    Surrogate loss: 0.0148
             Mean action noise std: 0.92
                       Mean reward: 5785.19
               Mean episode length: 296.15
                 Mean success rate: 54.00
                  Mean reward/step: 18.53
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 2.58s
                        Total time: 3264.89s
                               ETA: 6988.5s

################################################################################
                     [1m Learning iteration 1274/4000 [0m

                       Computation: 3246 steps/s (collection: 0.436s, learning 2.087s)
               Value function loss: 77041.5625
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 5523.57
               Mean episode length: 286.78
                 Mean success rate: 51.00
                  Mean reward/step: 18.50
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10444800
                    Iteration time: 2.52s
                        Total time: 3267.41s
                               ETA: 6985.9s

################################################################################
                     [1m Learning iteration 1275/4000 [0m

                       Computation: 3186 steps/s (collection: 0.483s, learning 2.087s)
               Value function loss: 71849.1000
                    Surrogate loss: 0.0179
             Mean action noise std: 0.92
                       Mean reward: 5174.38
               Mean episode length: 274.30
                 Mean success rate: 48.00
                  Mean reward/step: 18.61
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 2.57s
                        Total time: 3269.98s
                               ETA: 6983.3s

################################################################################
                     [1m Learning iteration 1276/4000 [0m

                       Computation: 3071 steps/s (collection: 0.523s, learning 2.145s)
               Value function loss: 114019.2055
                    Surrogate loss: 0.0157
             Mean action noise std: 0.92
                       Mean reward: 5268.65
               Mean episode length: 279.26
                 Mean success rate: 49.00
                  Mean reward/step: 18.88
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10461184
                    Iteration time: 2.67s
                        Total time: 3272.65s
                               ETA: 6981.0s

################################################################################
                     [1m Learning iteration 1277/4000 [0m

                       Computation: 3031 steps/s (collection: 0.517s, learning 2.185s)
               Value function loss: 42364.4084
                    Surrogate loss: 0.0209
             Mean action noise std: 0.92
                       Mean reward: 5385.91
               Mean episode length: 283.18
                 Mean success rate: 50.00
                  Mean reward/step: 19.17
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 2.70s
                        Total time: 3275.35s
                               ETA: 6978.7s

################################################################################
                     [1m Learning iteration 1278/4000 [0m

                       Computation: 3218 steps/s (collection: 0.448s, learning 2.097s)
               Value function loss: 100216.3262
                    Surrogate loss: 0.0181
             Mean action noise std: 0.92
                       Mean reward: 5824.93
               Mean episode length: 296.43
                 Mean success rate: 52.50
                  Mean reward/step: 19.48
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10477568
                    Iteration time: 2.55s
                        Total time: 3277.90s
                               ETA: 6976.1s

################################################################################
                     [1m Learning iteration 1279/4000 [0m

                       Computation: 3117 steps/s (collection: 0.481s, learning 2.147s)
               Value function loss: 78312.9173
                    Surrogate loss: 0.0155
             Mean action noise std: 0.92
                       Mean reward: 5869.15
               Mean episode length: 298.94
                 Mean success rate: 53.50
                  Mean reward/step: 18.64
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 2.63s
                        Total time: 3280.52s
                               ETA: 6973.7s

################################################################################
                     [1m Learning iteration 1280/4000 [0m

                       Computation: 3099 steps/s (collection: 0.494s, learning 2.149s)
               Value function loss: 60568.1897
                    Surrogate loss: 0.0201
             Mean action noise std: 0.92
                       Mean reward: 5935.38
               Mean episode length: 301.53
                 Mean success rate: 54.00
                  Mean reward/step: 18.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10493952
                    Iteration time: 2.64s
                        Total time: 3283.17s
                               ETA: 6971.3s

################################################################################
                     [1m Learning iteration 1281/4000 [0m

                       Computation: 3141 steps/s (collection: 0.486s, learning 2.121s)
               Value function loss: 59528.2876
                    Surrogate loss: 0.0183
             Mean action noise std: 0.92
                       Mean reward: 6199.22
               Mean episode length: 312.45
                 Mean success rate: 57.50
                  Mean reward/step: 19.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 2.61s
                        Total time: 3285.77s
                               ETA: 6968.8s

################################################################################
                     [1m Learning iteration 1282/4000 [0m

                       Computation: 3094 steps/s (collection: 0.504s, learning 2.143s)
               Value function loss: 78134.0184
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 5735.91
               Mean episode length: 299.69
                 Mean success rate: 55.00
                  Mean reward/step: 19.21
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 10510336
                    Iteration time: 2.65s
                        Total time: 3288.42s
                               ETA: 6966.4s

################################################################################
                     [1m Learning iteration 1283/4000 [0m

                       Computation: 3143 steps/s (collection: 0.498s, learning 2.109s)
               Value function loss: 64672.8021
                    Surrogate loss: 0.0131
             Mean action noise std: 0.92
                       Mean reward: 5810.00
               Mean episode length: 300.18
                 Mean success rate: 55.50
                  Mean reward/step: 19.30
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 2.61s
                        Total time: 3291.03s
                               ETA: 6964.0s

################################################################################
                     [1m Learning iteration 1284/4000 [0m

                       Computation: 3087 steps/s (collection: 0.514s, learning 2.139s)
               Value function loss: 90484.1387
                    Surrogate loss: 0.0155
             Mean action noise std: 0.92
                       Mean reward: 5992.35
               Mean episode length: 307.24
                 Mean success rate: 57.00
                  Mean reward/step: 18.44
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10526720
                    Iteration time: 2.65s
                        Total time: 3293.68s
                               ETA: 6961.6s

################################################################################
                     [1m Learning iteration 1285/4000 [0m

                       Computation: 3135 steps/s (collection: 0.497s, learning 2.116s)
               Value function loss: 89408.7570
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 5510.04
               Mean episode length: 298.71
                 Mean success rate: 54.50
                  Mean reward/step: 17.84
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 2.61s
                        Total time: 3296.29s
                               ETA: 6959.1s

################################################################################
                     [1m Learning iteration 1286/4000 [0m

                       Computation: 3187 steps/s (collection: 0.451s, learning 2.120s)
               Value function loss: 60362.9966
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 5487.78
               Mean episode length: 297.16
                 Mean success rate: 54.00
                  Mean reward/step: 18.62
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 10543104
                    Iteration time: 2.57s
                        Total time: 3298.86s
                               ETA: 6956.6s

################################################################################
                     [1m Learning iteration 1287/4000 [0m

                       Computation: 3168 steps/s (collection: 0.496s, learning 2.089s)
               Value function loss: 86460.9427
                    Surrogate loss: 0.0126
             Mean action noise std: 0.92
                       Mean reward: 5654.13
               Mean episode length: 300.56
                 Mean success rate: 55.50
                  Mean reward/step: 19.88
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 2.59s
                        Total time: 3301.45s
                               ETA: 6954.1s

################################################################################
                     [1m Learning iteration 1288/4000 [0m

                       Computation: 3210 steps/s (collection: 0.473s, learning 2.078s)
               Value function loss: 95108.9969
                    Surrogate loss: 0.0137
             Mean action noise std: 0.92
                       Mean reward: 5750.52
               Mean episode length: 305.44
                 Mean success rate: 55.50
                  Mean reward/step: 20.92
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10559488
                    Iteration time: 2.55s
                        Total time: 3304.00s
                               ETA: 6951.5s

################################################################################
                     [1m Learning iteration 1289/4000 [0m

                       Computation: 3233 steps/s (collection: 0.480s, learning 2.054s)
               Value function loss: 88990.8161
                    Surrogate loss: 0.0115
             Mean action noise std: 0.92
                       Mean reward: 6267.41
               Mean episode length: 318.86
                 Mean success rate: 57.50
                  Mean reward/step: 19.54
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 2.53s
                        Total time: 3306.53s
                               ETA: 6948.8s

################################################################################
                     [1m Learning iteration 1290/4000 [0m

                       Computation: 3223 steps/s (collection: 0.472s, learning 2.068s)
               Value function loss: 82642.3513
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 6366.68
               Mean episode length: 320.60
                 Mean success rate: 58.00
                  Mean reward/step: 19.37
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10575872
                    Iteration time: 2.54s
                        Total time: 3309.07s
                               ETA: 6946.2s

################################################################################
                     [1m Learning iteration 1291/4000 [0m

                       Computation: 3255 steps/s (collection: 0.454s, learning 2.062s)
               Value function loss: 65483.8337
                    Surrogate loss: 0.0156
             Mean action noise std: 0.92
                       Mean reward: 6616.24
               Mean episode length: 328.32
                 Mean success rate: 60.50
                  Mean reward/step: 20.05
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 2.52s
                        Total time: 3311.59s
                               ETA: 6943.6s

################################################################################
                     [1m Learning iteration 1292/4000 [0m

                       Computation: 3245 steps/s (collection: 0.422s, learning 2.103s)
               Value function loss: 59523.1049
                    Surrogate loss: 0.0168
             Mean action noise std: 0.92
                       Mean reward: 6577.06
               Mean episode length: 329.04
                 Mean success rate: 60.00
                  Mean reward/step: 20.92
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10592256
                    Iteration time: 2.52s
                        Total time: 3314.12s
                               ETA: 6940.9s

################################################################################
                     [1m Learning iteration 1293/4000 [0m

                       Computation: 3259 steps/s (collection: 0.472s, learning 2.041s)
               Value function loss: 52731.3837
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 6477.06
               Mean episode length: 326.85
                 Mean success rate: 59.50
                  Mean reward/step: 21.90
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 2.51s
                        Total time: 3316.63s
                               ETA: 6938.3s

################################################################################
                     [1m Learning iteration 1294/4000 [0m

                       Computation: 3234 steps/s (collection: 0.485s, learning 2.047s)
               Value function loss: 119022.6930
                    Surrogate loss: 0.0141
             Mean action noise std: 0.92
                       Mean reward: 6571.37
               Mean episode length: 330.13
                 Mean success rate: 60.50
                  Mean reward/step: 21.67
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 10608640
                    Iteration time: 2.53s
                        Total time: 3319.16s
                               ETA: 6935.6s

################################################################################
                     [1m Learning iteration 1295/4000 [0m

                       Computation: 3205 steps/s (collection: 0.491s, learning 2.065s)
               Value function loss: 63884.4173
                    Surrogate loss: 0.0163
             Mean action noise std: 0.92
                       Mean reward: 6463.97
               Mean episode length: 327.81
                 Mean success rate: 59.50
                  Mean reward/step: 21.07
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 2.56s
                        Total time: 3321.72s
                               ETA: 6933.1s

################################################################################
                     [1m Learning iteration 1296/4000 [0m

                       Computation: 3198 steps/s (collection: 0.498s, learning 2.064s)
               Value function loss: 66328.4899
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 6383.82
               Mean episode length: 320.75
                 Mean success rate: 58.00
                  Mean reward/step: 21.01
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10625024
                    Iteration time: 2.56s
                        Total time: 3324.28s
                               ETA: 6930.5s

################################################################################
                     [1m Learning iteration 1297/4000 [0m

                       Computation: 3189 steps/s (collection: 0.464s, learning 2.105s)
               Value function loss: 65342.7113
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 5932.05
               Mean episode length: 309.26
                 Mean success rate: 54.00
                  Mean reward/step: 21.37
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 2.57s
                        Total time: 3326.85s
                               ETA: 6927.9s

################################################################################
                     [1m Learning iteration 1298/4000 [0m

                       Computation: 3256 steps/s (collection: 0.462s, learning 2.053s)
               Value function loss: 115890.9230
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 6190.35
               Mean episode length: 316.45
                 Mean success rate: 57.00
                  Mean reward/step: 20.90
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 10641408
                    Iteration time: 2.52s
                        Total time: 3329.36s
                               ETA: 6925.3s

################################################################################
                     [1m Learning iteration 1299/4000 [0m

                       Computation: 3173 steps/s (collection: 0.469s, learning 2.112s)
               Value function loss: 79894.0331
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 6140.13
               Mean episode length: 312.92
                 Mean success rate: 55.50
                  Mean reward/step: 19.83
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 2.58s
                        Total time: 3331.94s
                               ETA: 6922.8s

################################################################################
                     [1m Learning iteration 1300/4000 [0m

                       Computation: 3117 steps/s (collection: 0.542s, learning 2.086s)
               Value function loss: 121107.0975
                    Surrogate loss: 0.0159
             Mean action noise std: 0.91
                       Mean reward: 6270.17
               Mean episode length: 311.93
                 Mean success rate: 56.00
                  Mean reward/step: 19.41
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10657792
                    Iteration time: 2.63s
                        Total time: 3334.57s
                               ETA: 6920.3s

################################################################################
                     [1m Learning iteration 1301/4000 [0m

                       Computation: 3114 steps/s (collection: 0.532s, learning 2.098s)
               Value function loss: 61331.5658
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 6496.76
               Mean episode length: 316.68
                 Mean success rate: 56.50
                  Mean reward/step: 18.34
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 2.63s
                        Total time: 3337.20s
                               ETA: 6917.9s

################################################################################
                     [1m Learning iteration 1302/4000 [0m

                       Computation: 3132 steps/s (collection: 0.500s, learning 2.115s)
               Value function loss: 70996.0197
                    Surrogate loss: 0.0166
             Mean action noise std: 0.91
                       Mean reward: 6228.52
               Mean episode length: 306.75
                 Mean success rate: 55.00
                  Mean reward/step: 19.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10674176
                    Iteration time: 2.62s
                        Total time: 3339.82s
                               ETA: 6915.4s

################################################################################
                     [1m Learning iteration 1303/4000 [0m

                       Computation: 3165 steps/s (collection: 0.483s, learning 2.105s)
               Value function loss: 94868.4609
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 6338.17
               Mean episode length: 317.23
                 Mean success rate: 56.50
                  Mean reward/step: 19.72
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 2.59s
                        Total time: 3342.41s
                               ETA: 6912.9s

################################################################################
                     [1m Learning iteration 1304/4000 [0m

                       Computation: 3062 steps/s (collection: 0.531s, learning 2.144s)
               Value function loss: 106692.0088
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 6851.30
               Mean episode length: 330.11
                 Mean success rate: 60.50
                  Mean reward/step: 19.44
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10690560
                    Iteration time: 2.68s
                        Total time: 3345.08s
                               ETA: 6910.6s

################################################################################
                     [1m Learning iteration 1305/4000 [0m

                       Computation: 3033 steps/s (collection: 0.562s, learning 2.138s)
               Value function loss: 85840.9294
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 7127.89
               Mean episode length: 336.61
                 Mean success rate: 61.50
                  Mean reward/step: 18.25
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 2.70s
                        Total time: 3347.78s
                               ETA: 6908.3s

################################################################################
                     [1m Learning iteration 1306/4000 [0m

                       Computation: 3171 steps/s (collection: 0.447s, learning 2.137s)
               Value function loss: 49879.4460
                    Surrogate loss: 0.0175
             Mean action noise std: 0.91
                       Mean reward: 7153.23
               Mean episode length: 340.05
                 Mean success rate: 62.00
                  Mean reward/step: 18.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10706944
                    Iteration time: 2.58s
                        Total time: 3350.37s
                               ETA: 6905.8s

################################################################################
                     [1m Learning iteration 1307/4000 [0m

                       Computation: 3187 steps/s (collection: 0.450s, learning 2.120s)
               Value function loss: 79474.3044
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 6788.85
               Mean episode length: 326.80
                 Mean success rate: 60.00
                  Mean reward/step: 19.10
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 2.57s
                        Total time: 3352.94s
                               ETA: 6903.3s

################################################################################
                     [1m Learning iteration 1308/4000 [0m

                       Computation: 3049 steps/s (collection: 0.551s, learning 2.136s)
               Value function loss: 45225.4511
                    Surrogate loss: 0.0180
             Mean action noise std: 0.91
                       Mean reward: 6699.98
               Mean episode length: 327.06
                 Mean success rate: 59.50
                  Mean reward/step: 18.74
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10723328
                    Iteration time: 2.69s
                        Total time: 3355.62s
                               ETA: 6900.9s

################################################################################
                     [1m Learning iteration 1309/4000 [0m

                       Computation: 3143 steps/s (collection: 0.494s, learning 2.112s)
               Value function loss: 68774.6899
                    Surrogate loss: 0.0148
             Mean action noise std: 0.91
                       Mean reward: 6291.69
               Mean episode length: 314.24
                 Mean success rate: 56.00
                  Mean reward/step: 19.19
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 2.61s
                        Total time: 3358.23s
                               ETA: 6898.5s

################################################################################
                     [1m Learning iteration 1310/4000 [0m

                       Computation: 3232 steps/s (collection: 0.466s, learning 2.068s)
               Value function loss: 76846.7764
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 6398.01
               Mean episode length: 318.70
                 Mean success rate: 57.50
                  Mean reward/step: 19.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10739712
                    Iteration time: 2.53s
                        Total time: 3360.76s
                               ETA: 6895.8s

################################################################################
                     [1m Learning iteration 1311/4000 [0m

                       Computation: 3308 steps/s (collection: 0.440s, learning 2.036s)
               Value function loss: 71465.0851
                    Surrogate loss: 0.0169
             Mean action noise std: 0.91
                       Mean reward: 6312.62
               Mean episode length: 311.20
                 Mean success rate: 56.50
                  Mean reward/step: 20.75
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 2.48s
                        Total time: 3363.24s
                               ETA: 6893.1s

################################################################################
                     [1m Learning iteration 1312/4000 [0m

                       Computation: 3209 steps/s (collection: 0.462s, learning 2.090s)
               Value function loss: 66452.5782
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 6002.17
               Mean episode length: 305.68
                 Mean success rate: 55.50
                  Mean reward/step: 20.17
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10756096
                    Iteration time: 2.55s
                        Total time: 3365.79s
                               ETA: 6890.5s

################################################################################
                     [1m Learning iteration 1313/4000 [0m

                       Computation: 3168 steps/s (collection: 0.491s, learning 2.094s)
               Value function loss: 110255.7482
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 5403.75
               Mean episode length: 287.44
                 Mean success rate: 52.50
                  Mean reward/step: 20.13
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 2.59s
                        Total time: 3368.38s
                               ETA: 6888.0s

################################################################################
                     [1m Learning iteration 1314/4000 [0m

                       Computation: 3221 steps/s (collection: 0.473s, learning 2.070s)
               Value function loss: 103841.9469
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 5759.21
               Mean episode length: 300.61
                 Mean success rate: 55.50
                  Mean reward/step: 19.13
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10772480
                    Iteration time: 2.54s
                        Total time: 3370.92s
                               ETA: 6885.4s

################################################################################
                     [1m Learning iteration 1315/4000 [0m

                       Computation: 3127 steps/s (collection: 0.472s, learning 2.148s)
               Value function loss: 78725.5702
                    Surrogate loss: 0.0164
             Mean action noise std: 0.91
                       Mean reward: 5614.87
               Mean episode length: 299.99
                 Mean success rate: 54.50
                  Mean reward/step: 18.98
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 2.62s
                        Total time: 3373.54s
                               ETA: 6882.9s

################################################################################
                     [1m Learning iteration 1316/4000 [0m

                       Computation: 3225 steps/s (collection: 0.472s, learning 2.068s)
               Value function loss: 124493.3793
                    Surrogate loss: 0.0159
             Mean action noise std: 0.91
                       Mean reward: 6502.98
               Mean episode length: 330.35
                 Mean success rate: 62.00
                  Mean reward/step: 18.54
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10788864
                    Iteration time: 2.54s
                        Total time: 3376.08s
                               ETA: 6880.3s

################################################################################
                     [1m Learning iteration 1317/4000 [0m

                       Computation: 3236 steps/s (collection: 0.511s, learning 2.020s)
               Value function loss: 52619.0063
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 6653.83
               Mean episode length: 332.02
                 Mean success rate: 63.00
                  Mean reward/step: 18.97
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 2.53s
                        Total time: 3378.61s
                               ETA: 6877.7s

################################################################################
                     [1m Learning iteration 1318/4000 [0m

                       Computation: 3225 steps/s (collection: 0.467s, learning 2.073s)
               Value function loss: 72331.4317
                    Surrogate loss: 0.0173
             Mean action noise std: 0.91
                       Mean reward: 6401.68
               Mean episode length: 322.33
                 Mean success rate: 60.50
                  Mean reward/step: 19.30
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10805248
                    Iteration time: 2.54s
                        Total time: 3381.15s
                               ETA: 6875.1s

################################################################################
                     [1m Learning iteration 1319/4000 [0m

                       Computation: 3186 steps/s (collection: 0.484s, learning 2.087s)
               Value function loss: 77934.3052
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 6616.46
               Mean episode length: 334.19
                 Mean success rate: 63.50
                  Mean reward/step: 18.81
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 2.57s
                        Total time: 3383.72s
                               ETA: 6872.5s

################################################################################
                     [1m Learning iteration 1320/4000 [0m

                       Computation: 3189 steps/s (collection: 0.490s, learning 2.079s)
               Value function loss: 72430.8361
                    Surrogate loss: 0.0170
             Mean action noise std: 0.91
                       Mean reward: 6747.81
               Mean episode length: 342.73
                 Mean success rate: 64.50
                  Mean reward/step: 18.05
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10821632
                    Iteration time: 2.57s
                        Total time: 3386.29s
                               ETA: 6870.0s

################################################################################
                     [1m Learning iteration 1321/4000 [0m

                       Computation: 3223 steps/s (collection: 0.471s, learning 2.070s)
               Value function loss: 73025.1375
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 6223.36
               Mean episode length: 322.12
                 Mean success rate: 59.50
                  Mean reward/step: 17.74
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 2.54s
                        Total time: 3388.83s
                               ETA: 6867.4s

################################################################################
                     [1m Learning iteration 1322/4000 [0m

                       Computation: 3164 steps/s (collection: 0.471s, learning 2.118s)
               Value function loss: 77883.0842
                    Surrogate loss: 0.0171
             Mean action noise std: 0.91
                       Mean reward: 5879.92
               Mean episode length: 302.98
                 Mean success rate: 55.00
                  Mean reward/step: 18.49
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10838016
                    Iteration time: 2.59s
                        Total time: 3391.42s
                               ETA: 6864.9s

################################################################################
                     [1m Learning iteration 1323/4000 [0m

                       Computation: 3176 steps/s (collection: 0.495s, learning 2.084s)
               Value function loss: 82483.8424
                    Surrogate loss: 0.0160
             Mean action noise std: 0.91
                       Mean reward: 5083.88
               Mean episode length: 279.57
                 Mean success rate: 50.00
                  Mean reward/step: 18.51
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 2.58s
                        Total time: 3394.00s
                               ETA: 6862.3s

################################################################################
                     [1m Learning iteration 1324/4000 [0m

                       Computation: 3230 steps/s (collection: 0.466s, learning 2.070s)
               Value function loss: 63871.9986
                    Surrogate loss: 0.0188
             Mean action noise std: 0.91
                       Mean reward: 4888.28
               Mean episode length: 271.36
                 Mean success rate: 49.00
                  Mean reward/step: 18.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10854400
                    Iteration time: 2.54s
                        Total time: 3396.53s
                               ETA: 6859.7s

################################################################################
                     [1m Learning iteration 1325/4000 [0m

                       Computation: 3117 steps/s (collection: 0.515s, learning 2.112s)
               Value function loss: 93495.0690
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 4950.22
               Mean episode length: 273.53
                 Mean success rate: 48.50
                  Mean reward/step: 19.25
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 2.63s
                        Total time: 3399.16s
                               ETA: 6857.3s

################################################################################
                     [1m Learning iteration 1326/4000 [0m

                       Computation: 3212 steps/s (collection: 0.458s, learning 2.092s)
               Value function loss: 45322.9090
                    Surrogate loss: 0.0160
             Mean action noise std: 0.91
                       Mean reward: 4620.37
               Mean episode length: 260.34
                 Mean success rate: 45.50
                  Mean reward/step: 19.72
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10870784
                    Iteration time: 2.55s
                        Total time: 3401.71s
                               ETA: 6854.7s

################################################################################
                     [1m Learning iteration 1327/4000 [0m

                       Computation: 3224 steps/s (collection: 0.461s, learning 2.080s)
               Value function loss: 100132.7881
                    Surrogate loss: 0.0163
             Mean action noise std: 0.91
                       Mean reward: 4744.01
               Mean episode length: 263.64
                 Mean success rate: 46.00
                  Mean reward/step: 20.05
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 2.54s
                        Total time: 3404.25s
                               ETA: 6852.1s

################################################################################
                     [1m Learning iteration 1328/4000 [0m

                       Computation: 3232 steps/s (collection: 0.442s, learning 2.093s)
               Value function loss: 86320.7470
                    Surrogate loss: 0.0159
             Mean action noise std: 0.91
                       Mean reward: 5072.11
               Mean episode length: 276.80
                 Mean success rate: 49.00
                  Mean reward/step: 19.23
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10887168
                    Iteration time: 2.53s
                        Total time: 3406.79s
                               ETA: 6849.5s

################################################################################
                     [1m Learning iteration 1329/4000 [0m

                       Computation: 3124 steps/s (collection: 0.523s, learning 2.099s)
               Value function loss: 98196.3699
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 5325.45
               Mean episode length: 289.30
                 Mean success rate: 52.50
                  Mean reward/step: 18.91
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 2.62s
                        Total time: 3409.41s
                               ETA: 6847.0s

################################################################################
                     [1m Learning iteration 1330/4000 [0m

                       Computation: 3136 steps/s (collection: 0.471s, learning 2.141s)
               Value function loss: 75488.5005
                    Surrogate loss: 0.0161
             Mean action noise std: 0.91
                       Mean reward: 5644.43
               Mean episode length: 296.78
                 Mean success rate: 54.00
                  Mean reward/step: 18.22
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10903552
                    Iteration time: 2.61s
                        Total time: 3412.02s
                               ETA: 6844.6s

################################################################################
                     [1m Learning iteration 1331/4000 [0m

                       Computation: 3139 steps/s (collection: 0.495s, learning 2.115s)
               Value function loss: 59952.0271
                    Surrogate loss: 0.0195
             Mean action noise std: 0.91
                       Mean reward: 5719.29
               Mean episode length: 303.43
                 Mean success rate: 55.00
                  Mean reward/step: 18.25
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 2.61s
                        Total time: 3414.63s
                               ETA: 6842.1s

################################################################################
                     [1m Learning iteration 1332/4000 [0m

                       Computation: 3165 steps/s (collection: 0.461s, learning 2.126s)
               Value function loss: 72333.7332
                    Surrogate loss: 0.0186
             Mean action noise std: 0.91
                       Mean reward: 5599.71
               Mean episode length: 300.59
                 Mean success rate: 55.00
                  Mean reward/step: 18.38
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10919936
                    Iteration time: 2.59s
                        Total time: 3417.22s
                               ETA: 6839.6s

################################################################################
                     [1m Learning iteration 1333/4000 [0m

                       Computation: 3209 steps/s (collection: 0.449s, learning 2.103s)
               Value function loss: 64318.3220
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 5449.82
               Mean episode length: 292.60
                 Mean success rate: 54.00
                  Mean reward/step: 18.81
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 2.55s
                        Total time: 3419.77s
                               ETA: 6837.0s

################################################################################
                     [1m Learning iteration 1334/4000 [0m

                       Computation: 3143 steps/s (collection: 0.503s, learning 2.102s)
               Value function loss: 81805.3063
                    Surrogate loss: 0.0169
             Mean action noise std: 0.91
                       Mean reward: 5611.29
               Mean episode length: 295.80
                 Mean success rate: 55.50
                  Mean reward/step: 18.87
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10936320
                    Iteration time: 2.61s
                        Total time: 3422.38s
                               ETA: 6834.5s

################################################################################
                     [1m Learning iteration 1335/4000 [0m

                       Computation: 3227 steps/s (collection: 0.454s, learning 2.084s)
               Value function loss: 82510.2222
                    Surrogate loss: 0.0179
             Mean action noise std: 0.91
                       Mean reward: 5252.09
               Mean episode length: 281.58
                 Mean success rate: 51.50
                  Mean reward/step: 17.75
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 2.54s
                        Total time: 3424.92s
                               ETA: 6831.9s

################################################################################
                     [1m Learning iteration 1336/4000 [0m

                       Computation: 3129 steps/s (collection: 0.510s, learning 2.107s)
               Value function loss: 73495.2789
                    Surrogate loss: 0.0161
             Mean action noise std: 0.91
                       Mean reward: 5184.05
               Mean episode length: 279.14
                 Mean success rate: 51.50
                  Mean reward/step: 17.47
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 10952704
                    Iteration time: 2.62s
                        Total time: 3427.53s
                               ETA: 6829.4s

################################################################################
                     [1m Learning iteration 1337/4000 [0m

                       Computation: 3208 steps/s (collection: 0.470s, learning 2.083s)
               Value function loss: 79275.2800
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 4882.97
               Mean episode length: 274.95
                 Mean success rate: 49.00
                  Mean reward/step: 17.26
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 2.55s
                        Total time: 3430.09s
                               ETA: 6826.8s

################################################################################
                     [1m Learning iteration 1338/4000 [0m

                       Computation: 3145 steps/s (collection: 0.489s, learning 2.115s)
               Value function loss: 105862.7606
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 5094.34
               Mean episode length: 280.84
                 Mean success rate: 51.00
                  Mean reward/step: 17.92
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10969088
                    Iteration time: 2.60s
                        Total time: 3432.69s
                               ETA: 6824.4s

################################################################################
                     [1m Learning iteration 1339/4000 [0m

                       Computation: 3148 steps/s (collection: 0.498s, learning 2.104s)
               Value function loss: 41733.3794
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 5093.91
               Mean episode length: 278.62
                 Mean success rate: 50.00
                  Mean reward/step: 18.25
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 2.60s
                        Total time: 3435.29s
                               ETA: 6821.9s

################################################################################
                     [1m Learning iteration 1340/4000 [0m

                       Computation: 3150 steps/s (collection: 0.473s, learning 2.127s)
               Value function loss: 78462.8054
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 5372.79
               Mean episode length: 292.62
                 Mean success rate: 52.50
                  Mean reward/step: 19.14
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10985472
                    Iteration time: 2.60s
                        Total time: 3437.89s
                               ETA: 6819.4s

################################################################################
                     [1m Learning iteration 1341/4000 [0m

                       Computation: 3226 steps/s (collection: 0.468s, learning 2.070s)
               Value function loss: 82613.0479
                    Surrogate loss: 0.0156
             Mean action noise std: 0.91
                       Mean reward: 5590.78
               Mean episode length: 296.44
                 Mean success rate: 54.50
                  Mean reward/step: 18.64
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 2.54s
                        Total time: 3440.43s
                               ETA: 6816.8s

################################################################################
                     [1m Learning iteration 1342/4000 [0m

                       Computation: 3188 steps/s (collection: 0.484s, learning 2.085s)
               Value function loss: 75368.7759
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 5713.93
               Mean episode length: 303.52
                 Mean success rate: 55.50
                  Mean reward/step: 18.93
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 11001856
                    Iteration time: 2.57s
                        Total time: 3443.00s
                               ETA: 6814.2s

################################################################################
                     [1m Learning iteration 1343/4000 [0m

                       Computation: 3208 steps/s (collection: 0.465s, learning 2.088s)
               Value function loss: 72087.6479
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 5833.08
               Mean episode length: 307.47
                 Mean success rate: 56.00
                  Mean reward/step: 18.36
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 2.55s
                        Total time: 3445.55s
                               ETA: 6811.6s

################################################################################
                     [1m Learning iteration 1344/4000 [0m

                       Computation: 3190 steps/s (collection: 0.481s, learning 2.087s)
               Value function loss: 102006.6174
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 5915.31
               Mean episode length: 307.75
                 Mean success rate: 57.50
                  Mean reward/step: 18.44
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11018240
                    Iteration time: 2.57s
                        Total time: 3448.12s
                               ETA: 6809.1s

################################################################################
                     [1m Learning iteration 1345/4000 [0m

                       Computation: 3241 steps/s (collection: 0.472s, learning 2.055s)
               Value function loss: 46516.1473
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 5473.95
               Mean episode length: 290.62
                 Mean success rate: 53.00
                  Mean reward/step: 17.95
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 2.53s
                        Total time: 3450.65s
                               ETA: 6806.4s

################################################################################
                     [1m Learning iteration 1346/4000 [0m

                       Computation: 3203 steps/s (collection: 0.487s, learning 2.070s)
               Value function loss: 71924.6296
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 5094.24
               Mean episode length: 273.82
                 Mean success rate: 49.50
                  Mean reward/step: 17.85
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 11034624
                    Iteration time: 2.56s
                        Total time: 3453.20s
                               ETA: 6803.9s

################################################################################
                     [1m Learning iteration 1347/4000 [0m

                       Computation: 3213 steps/s (collection: 0.477s, learning 2.073s)
               Value function loss: 62811.0167
                    Surrogate loss: 0.0165
             Mean action noise std: 0.91
                       Mean reward: 4681.89
               Mean episode length: 257.21
                 Mean success rate: 46.50
                  Mean reward/step: 16.88
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 2.55s
                        Total time: 3455.75s
                               ETA: 6801.3s

################################################################################
                     [1m Learning iteration 1348/4000 [0m

                       Computation: 3191 steps/s (collection: 0.472s, learning 2.095s)
               Value function loss: 67849.2700
                    Surrogate loss: 0.0171
             Mean action noise std: 0.91
                       Mean reward: 4852.57
               Mean episode length: 259.50
                 Mean success rate: 47.50
                  Mean reward/step: 16.70
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11051008
                    Iteration time: 2.57s
                        Total time: 3458.32s
                               ETA: 6798.7s

################################################################################
                     [1m Learning iteration 1349/4000 [0m

                       Computation: 3210 steps/s (collection: 0.512s, learning 2.039s)
               Value function loss: 46529.5256
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 4244.09
               Mean episode length: 239.11
                 Mean success rate: 42.50
                  Mean reward/step: 16.86
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 2.55s
                        Total time: 3460.87s
                               ETA: 6796.1s

################################################################################
                     [1m Learning iteration 1350/4000 [0m

                       Computation: 3233 steps/s (collection: 0.483s, learning 2.051s)
               Value function loss: 95213.8523
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 4080.19
               Mean episode length: 233.49
                 Mean success rate: 40.00
                  Mean reward/step: 17.28
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 11067392
                    Iteration time: 2.53s
                        Total time: 3463.40s
                               ETA: 6793.5s

################################################################################
                     [1m Learning iteration 1351/4000 [0m

                       Computation: 3191 steps/s (collection: 0.478s, learning 2.088s)
               Value function loss: 97454.1787
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 4491.97
               Mean episode length: 254.87
                 Mean success rate: 44.00
                  Mean reward/step: 16.81
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 2.57s
                        Total time: 3465.97s
                               ETA: 6790.9s

################################################################################
                     [1m Learning iteration 1352/4000 [0m

                       Computation: 3218 steps/s (collection: 0.480s, learning 2.065s)
               Value function loss: 75258.7748
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 4561.64
               Mean episode length: 260.18
                 Mean success rate: 45.00
                  Mean reward/step: 16.57
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 11083776
                    Iteration time: 2.55s
                        Total time: 3468.52s
                               ETA: 6788.3s

################################################################################
                     [1m Learning iteration 1353/4000 [0m

                       Computation: 3249 steps/s (collection: 0.482s, learning 2.038s)
               Value function loss: 65947.0379
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 4609.04
               Mean episode length: 268.13
                 Mean success rate: 45.50
                  Mean reward/step: 16.45
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 2.52s
                        Total time: 3471.04s
                               ETA: 6785.7s

################################################################################
                     [1m Learning iteration 1354/4000 [0m

                       Computation: 3151 steps/s (collection: 0.504s, learning 2.096s)
               Value function loss: 70699.4522
                    Surrogate loss: 0.0173
             Mean action noise std: 0.91
                       Mean reward: 4914.81
               Mean episode length: 280.07
                 Mean success rate: 48.00
                  Mean reward/step: 16.31
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11100160
                    Iteration time: 2.60s
                        Total time: 3473.64s
                               ETA: 6783.2s

################################################################################
                     [1m Learning iteration 1355/4000 [0m

                       Computation: 3219 steps/s (collection: 0.474s, learning 2.071s)
               Value function loss: 56460.0437
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 4818.37
               Mean episode length: 274.64
                 Mean success rate: 47.50
                  Mean reward/step: 16.25
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 2.54s
                        Total time: 3476.18s
                               ETA: 6780.6s

################################################################################
                     [1m Learning iteration 1356/4000 [0m

                       Computation: 3263 steps/s (collection: 0.448s, learning 2.063s)
               Value function loss: 82550.0756
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 4923.98
               Mean episode length: 275.94
                 Mean success rate: 48.50
                  Mean reward/step: 17.08
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11116544
                    Iteration time: 2.51s
                        Total time: 3478.69s
                               ETA: 6777.9s

################################################################################
                     [1m Learning iteration 1357/4000 [0m

                       Computation: 3130 steps/s (collection: 0.518s, learning 2.099s)
               Value function loss: 65470.1014
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 4808.89
               Mean episode length: 269.45
                 Mean success rate: 47.00
                  Mean reward/step: 17.04
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 2.62s
                        Total time: 3481.31s
                               ETA: 6775.5s

################################################################################
                     [1m Learning iteration 1358/4000 [0m

                       Computation: 3189 steps/s (collection: 0.490s, learning 2.078s)
               Value function loss: 92742.4386
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 4415.41
               Mean episode length: 252.69
                 Mean success rate: 43.50
                  Mean reward/step: 16.82
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 11132928
                    Iteration time: 2.57s
                        Total time: 3483.88s
                               ETA: 6772.9s

################################################################################
                     [1m Learning iteration 1359/4000 [0m

                       Computation: 3241 steps/s (collection: 0.475s, learning 2.053s)
               Value function loss: 67319.1516
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 4345.27
               Mean episode length: 245.96
                 Mean success rate: 42.00
                  Mean reward/step: 17.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 2.53s
                        Total time: 3486.40s
                               ETA: 6770.3s

################################################################################
                     [1m Learning iteration 1360/4000 [0m

                       Computation: 3172 steps/s (collection: 0.480s, learning 2.103s)
               Value function loss: 64125.5775
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 4623.11
               Mean episode length: 258.47
                 Mean success rate: 45.00
                  Mean reward/step: 18.67
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11149312
                    Iteration time: 2.58s
                        Total time: 3488.99s
                               ETA: 6767.8s

################################################################################
                     [1m Learning iteration 1361/4000 [0m

                       Computation: 3268 steps/s (collection: 0.433s, learning 2.073s)
               Value function loss: 65179.6550
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 4402.72
               Mean episode length: 256.02
                 Mean success rate: 44.50
                  Mean reward/step: 19.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 2.51s
                        Total time: 3491.49s
                               ETA: 6765.1s

################################################################################
                     [1m Learning iteration 1362/4000 [0m

                       Computation: 3142 steps/s (collection: 0.484s, learning 2.122s)
               Value function loss: 70335.7657
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 4704.06
               Mean episode length: 269.35
                 Mean success rate: 48.00
                  Mean reward/step: 18.96
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11165696
                    Iteration time: 2.61s
                        Total time: 3494.10s
                               ETA: 6762.6s

################################################################################
                     [1m Learning iteration 1363/4000 [0m

                       Computation: 3185 steps/s (collection: 0.488s, learning 2.084s)
               Value function loss: 85512.8003
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 4702.71
               Mean episode length: 267.30
                 Mean success rate: 48.00
                  Mean reward/step: 18.41
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 2.57s
                        Total time: 3496.67s
                               ETA: 6760.1s

################################################################################
                     [1m Learning iteration 1364/4000 [0m

                       Computation: 3196 steps/s (collection: 0.478s, learning 2.084s)
               Value function loss: 65160.4187
                    Surrogate loss: 0.0173
             Mean action noise std: 0.91
                       Mean reward: 4867.50
               Mean episode length: 276.56
                 Mean success rate: 49.50
                  Mean reward/step: 18.59
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11182080
                    Iteration time: 2.56s
                        Total time: 3499.23s
                               ETA: 6757.5s

################################################################################
                     [1m Learning iteration 1365/4000 [0m

                       Computation: 3141 steps/s (collection: 0.494s, learning 2.113s)
               Value function loss: 85115.7965
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 5126.35
               Mean episode length: 281.82
                 Mean success rate: 52.50
                  Mean reward/step: 18.39
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 2.61s
                        Total time: 3501.84s
                               ETA: 6755.0s

################################################################################
                     [1m Learning iteration 1366/4000 [0m

                       Computation: 3171 steps/s (collection: 0.489s, learning 2.094s)
               Value function loss: 79608.7830
                    Surrogate loss: 0.0164
             Mean action noise std: 0.91
                       Mean reward: 5239.25
               Mean episode length: 296.18
                 Mean success rate: 54.50
                  Mean reward/step: 18.32
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11198464
                    Iteration time: 2.58s
                        Total time: 3504.42s
                               ETA: 6752.5s

################################################################################
                     [1m Learning iteration 1367/4000 [0m

                       Computation: 3192 steps/s (collection: 0.487s, learning 2.079s)
               Value function loss: 76767.5642
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 5147.12
               Mean episode length: 293.51
                 Mean success rate: 54.00
                  Mean reward/step: 17.93
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 2.57s
                        Total time: 3506.99s
                               ETA: 6749.9s

################################################################################
                     [1m Learning iteration 1368/4000 [0m

                       Computation: 3167 steps/s (collection: 0.503s, learning 2.083s)
               Value function loss: 73763.9375
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 5105.02
               Mean episode length: 289.98
                 Mean success rate: 53.00
                  Mean reward/step: 17.34
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11214848
                    Iteration time: 2.59s
                        Total time: 3509.58s
                               ETA: 6747.4s

################################################################################
                     [1m Learning iteration 1369/4000 [0m

                       Computation: 3246 steps/s (collection: 0.469s, learning 2.054s)
               Value function loss: 75911.7181
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 5292.46
               Mean episode length: 293.60
                 Mean success rate: 54.50
                  Mean reward/step: 17.53
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 2.52s
                        Total time: 3512.10s
                               ETA: 6744.8s

################################################################################
                     [1m Learning iteration 1370/4000 [0m

                       Computation: 3253 steps/s (collection: 0.485s, learning 2.033s)
               Value function loss: 105011.1903
                    Surrogate loss: 0.0166
             Mean action noise std: 0.91
                       Mean reward: 5415.93
               Mean episode length: 299.77
                 Mean success rate: 55.50
                  Mean reward/step: 18.15
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11231232
                    Iteration time: 2.52s
                        Total time: 3514.62s
                               ETA: 6742.1s

################################################################################
                     [1m Learning iteration 1371/4000 [0m

                       Computation: 3116 steps/s (collection: 0.497s, learning 2.131s)
               Value function loss: 71551.8760
                    Surrogate loss: 0.0161
             Mean action noise std: 0.91
                       Mean reward: 5393.34
               Mean episode length: 294.75
                 Mean success rate: 54.50
                  Mean reward/step: 18.50
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 2.63s
                        Total time: 3517.25s
                               ETA: 6739.7s

################################################################################
                     [1m Learning iteration 1372/4000 [0m

                       Computation: 3213 steps/s (collection: 0.467s, learning 2.082s)
               Value function loss: 92008.4913
                    Surrogate loss: 0.0163
             Mean action noise std: 0.91
                       Mean reward: 5637.80
               Mean episode length: 306.25
                 Mean success rate: 57.00
                  Mean reward/step: 18.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11247616
                    Iteration time: 2.55s
                        Total time: 3519.80s
                               ETA: 6737.1s

################################################################################
                     [1m Learning iteration 1373/4000 [0m

                       Computation: 3196 steps/s (collection: 0.446s, learning 2.117s)
               Value function loss: 77727.6309
                    Surrogate loss: 0.0148
             Mean action noise std: 0.91
                       Mean reward: 5659.05
               Mean episode length: 308.49
                 Mean success rate: 55.50
                  Mean reward/step: 18.68
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 2.56s
                        Total time: 3522.36s
                               ETA: 6734.5s

################################################################################
                     [1m Learning iteration 1374/4000 [0m

                       Computation: 3209 steps/s (collection: 0.478s, learning 2.075s)
               Value function loss: 51766.0584
                    Surrogate loss: 0.0166
             Mean action noise std: 0.91
                       Mean reward: 5561.74
               Mean episode length: 299.60
                 Mean success rate: 55.00
                  Mean reward/step: 18.96
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11264000
                    Iteration time: 2.55s
                        Total time: 3524.91s
                               ETA: 6731.9s

################################################################################
                     [1m Learning iteration 1375/4000 [0m

                       Computation: 3123 steps/s (collection: 0.488s, learning 2.135s)
               Value function loss: 67823.6194
                    Surrogate loss: 0.0156
             Mean action noise std: 0.91
                       Mean reward: 5501.83
               Mean episode length: 297.38
                 Mean success rate: 54.50
                  Mean reward/step: 19.24
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 2.62s
                        Total time: 3527.53s
                               ETA: 6729.5s

################################################################################
                     [1m Learning iteration 1376/4000 [0m

                       Computation: 3204 steps/s (collection: 0.503s, learning 2.053s)
               Value function loss: 68635.1544
                    Surrogate loss: 0.0179
             Mean action noise std: 0.91
                       Mean reward: 5390.48
               Mean episode length: 294.99
                 Mean success rate: 54.00
                  Mean reward/step: 18.97
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11280384
                    Iteration time: 2.56s
                        Total time: 3530.09s
                               ETA: 6726.9s

################################################################################
                     [1m Learning iteration 1377/4000 [0m

                       Computation: 3195 steps/s (collection: 0.485s, learning 2.078s)
               Value function loss: 64291.4898
                    Surrogate loss: 0.0163
             Mean action noise std: 0.91
                       Mean reward: 5125.90
               Mean episode length: 283.43
                 Mean success rate: 52.00
                  Mean reward/step: 19.23
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 2.56s
                        Total time: 3532.65s
                               ETA: 6724.3s

################################################################################
                     [1m Learning iteration 1378/4000 [0m

                       Computation: 3256 steps/s (collection: 0.482s, learning 2.034s)
               Value function loss: 108508.2035
                    Surrogate loss: 0.0126
             Mean action noise std: 0.91
                       Mean reward: 5618.23
               Mean episode length: 305.84
                 Mean success rate: 57.00
                  Mean reward/step: 19.07
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11296768
                    Iteration time: 2.52s
                        Total time: 3535.17s
                               ETA: 6721.7s

################################################################################
                     [1m Learning iteration 1379/4000 [0m

                       Computation: 3219 steps/s (collection: 0.468s, learning 2.077s)
               Value function loss: 77042.9802
                    Surrogate loss: 0.0236
             Mean action noise std: 0.91
                       Mean reward: 5392.17
               Mean episode length: 300.62
                 Mean success rate: 56.00
                  Mean reward/step: 18.61
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 2.54s
                        Total time: 3537.71s
                               ETA: 6719.1s

################################################################################
                     [1m Learning iteration 1380/4000 [0m

                       Computation: 3206 steps/s (collection: 0.500s, learning 2.054s)
               Value function loss: 77196.3305
                    Surrogate loss: 0.0191
             Mean action noise std: 0.91
                       Mean reward: 5327.39
               Mean episode length: 288.29
                 Mean success rate: 56.00
                  Mean reward/step: 18.46
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 11313152
                    Iteration time: 2.55s
                        Total time: 3540.27s
                               ETA: 6716.5s

################################################################################
                     [1m Learning iteration 1381/4000 [0m

                       Computation: 3247 steps/s (collection: 0.442s, learning 2.081s)
               Value function loss: 53267.6634
                    Surrogate loss: 0.0167
             Mean action noise std: 0.91
                       Mean reward: 5254.93
               Mean episode length: 287.70
                 Mean success rate: 55.50
                  Mean reward/step: 18.29
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 2.52s
                        Total time: 3542.79s
                               ETA: 6713.9s

################################################################################
                     [1m Learning iteration 1382/4000 [0m

                       Computation: 3270 steps/s (collection: 0.454s, learning 2.051s)
               Value function loss: 94306.9246
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 5795.18
               Mean episode length: 307.36
                 Mean success rate: 59.50
                  Mean reward/step: 18.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11329536
                    Iteration time: 2.50s
                        Total time: 3545.30s
                               ETA: 6711.2s

################################################################################
                     [1m Learning iteration 1383/4000 [0m

                       Computation: 3218 steps/s (collection: 0.486s, learning 2.060s)
               Value function loss: 55185.4531
                    Surrogate loss: 0.0174
             Mean action noise std: 0.91
                       Mean reward: 5508.46
               Mean episode length: 300.01
                 Mean success rate: 57.50
                  Mean reward/step: 18.98
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 2.55s
                        Total time: 3547.84s
                               ETA: 6708.6s

################################################################################
                     [1m Learning iteration 1384/4000 [0m

                       Computation: 3259 steps/s (collection: 0.490s, learning 2.023s)
               Value function loss: 74812.4809
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 5647.34
               Mean episode length: 310.51
                 Mean success rate: 58.00
                  Mean reward/step: 20.01
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11345920
                    Iteration time: 2.51s
                        Total time: 3550.36s
                               ETA: 6705.9s

################################################################################
                     [1m Learning iteration 1385/4000 [0m

                       Computation: 3203 steps/s (collection: 0.488s, learning 2.069s)
               Value function loss: 77441.4096
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 5535.25
               Mean episode length: 302.82
                 Mean success rate: 56.50
                  Mean reward/step: 20.02
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 2.56s
                        Total time: 3552.91s
                               ETA: 6703.4s

################################################################################
                     [1m Learning iteration 1386/4000 [0m

                       Computation: 3207 steps/s (collection: 0.456s, learning 2.098s)
               Value function loss: 86622.0483
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 5290.27
               Mean episode length: 292.24
                 Mean success rate: 53.50
                  Mean reward/step: 19.74
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11362304
                    Iteration time: 2.55s
                        Total time: 3555.47s
                               ETA: 6700.8s

################################################################################
                     [1m Learning iteration 1387/4000 [0m

                       Computation: 3231 steps/s (collection: 0.479s, learning 2.056s)
               Value function loss: 54003.1947
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 5255.11
               Mean episode length: 290.51
                 Mean success rate: 52.00
                  Mean reward/step: 19.47
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 2.54s
                        Total time: 3558.00s
                               ETA: 6698.2s

################################################################################
                     [1m Learning iteration 1388/4000 [0m

                       Computation: 3281 steps/s (collection: 0.432s, learning 2.065s)
               Value function loss: 101069.9279
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 5395.21
               Mean episode length: 289.77
                 Mean success rate: 51.00
                  Mean reward/step: 19.37
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 11378688
                    Iteration time: 2.50s
                        Total time: 3560.50s
                               ETA: 6695.5s

################################################################################
                     [1m Learning iteration 1389/4000 [0m

                       Computation: 3206 steps/s (collection: 0.492s, learning 2.063s)
               Value function loss: 88828.0236
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 5412.26
               Mean episode length: 290.85
                 Mean success rate: 50.50
                  Mean reward/step: 19.32
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 2.56s
                        Total time: 3563.05s
                               ETA: 6692.9s

################################################################################
                     [1m Learning iteration 1390/4000 [0m

                       Computation: 3234 steps/s (collection: 0.455s, learning 2.078s)
               Value function loss: 54293.8126
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 5317.65
               Mean episode length: 287.20
                 Mean success rate: 51.00
                  Mean reward/step: 19.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11395072
                    Iteration time: 2.53s
                        Total time: 3565.59s
                               ETA: 6690.3s

################################################################################
                     [1m Learning iteration 1391/4000 [0m

                       Computation: 3196 steps/s (collection: 0.504s, learning 2.059s)
               Value function loss: 96909.4594
                    Surrogate loss: 0.0156
             Mean action noise std: 0.91
                       Mean reward: 5454.91
               Mean episode length: 288.30
                 Mean success rate: 52.50
                  Mean reward/step: 19.81
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 2.56s
                        Total time: 3568.15s
                               ETA: 6687.7s

################################################################################
                     [1m Learning iteration 1392/4000 [0m

                       Computation: 3271 steps/s (collection: 0.440s, learning 2.064s)
               Value function loss: 88491.7279
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 5462.77
               Mean episode length: 284.65
                 Mean success rate: 52.00
                  Mean reward/step: 20.31
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11411456
                    Iteration time: 2.50s
                        Total time: 3570.65s
                               ETA: 6685.0s

################################################################################
                     [1m Learning iteration 1393/4000 [0m

                       Computation: 3285 steps/s (collection: 0.446s, learning 2.047s)
               Value function loss: 106560.8067
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 5517.92
               Mean episode length: 286.80
                 Mean success rate: 53.00
                  Mean reward/step: 20.11
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 2.49s
                        Total time: 3573.15s
                               ETA: 6682.3s

################################################################################
                     [1m Learning iteration 1394/4000 [0m

                       Computation: 3229 steps/s (collection: 0.491s, learning 2.046s)
               Value function loss: 109455.3172
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 5872.66
               Mean episode length: 294.80
                 Mean success rate: 56.50
                  Mean reward/step: 19.69
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 11427840
                    Iteration time: 2.54s
                        Total time: 3575.68s
                               ETA: 6679.7s

################################################################################
                     [1m Learning iteration 1395/4000 [0m

                       Computation: 3320 steps/s (collection: 0.443s, learning 2.024s)
               Value function loss: 69379.5768
                    Surrogate loss: 0.0234
             Mean action noise std: 0.91
                       Mean reward: 5840.85
               Mean episode length: 294.31
                 Mean success rate: 56.00
                  Mean reward/step: 18.94
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 2.47s
                        Total time: 3578.15s
                               ETA: 6677.0s

################################################################################
                     [1m Learning iteration 1396/4000 [0m

                       Computation: 3246 steps/s (collection: 0.461s, learning 2.062s)
               Value function loss: 91153.2394
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 5923.71
               Mean episode length: 296.31
                 Mean success rate: 57.00
                  Mean reward/step: 18.43
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11444224
                    Iteration time: 2.52s
                        Total time: 3580.67s
                               ETA: 6674.4s

################################################################################
                     [1m Learning iteration 1397/4000 [0m

                       Computation: 3305 steps/s (collection: 0.434s, learning 2.045s)
               Value function loss: 68030.0471
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 5900.49
               Mean episode length: 294.19
                 Mean success rate: 57.00
                  Mean reward/step: 19.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 2.48s
                        Total time: 3583.15s
                               ETA: 6671.6s

################################################################################
                     [1m Learning iteration 1398/4000 [0m

                       Computation: 3263 steps/s (collection: 0.476s, learning 2.034s)
               Value function loss: 56306.8429
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 5730.45
               Mean episode length: 285.55
                 Mean success rate: 54.50
                  Mean reward/step: 19.65
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 11460608
                    Iteration time: 2.51s
                        Total time: 3585.66s
                               ETA: 6669.0s

################################################################################
                     [1m Learning iteration 1399/4000 [0m

                       Computation: 3214 steps/s (collection: 0.466s, learning 2.082s)
               Value function loss: 53042.7118
                    Surrogate loss: 0.0181
             Mean action noise std: 0.91
                       Mean reward: 5482.09
               Mean episode length: 276.08
                 Mean success rate: 52.50
                  Mean reward/step: 20.92
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 2.55s
                        Total time: 3588.21s
                               ETA: 6666.4s

################################################################################
                     [1m Learning iteration 1400/4000 [0m

                       Computation: 3276 steps/s (collection: 0.466s, learning 2.034s)
               Value function loss: 83880.1935
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 5290.24
               Mean episode length: 271.56
                 Mean success rate: 51.00
                  Mean reward/step: 20.87
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11476992
                    Iteration time: 2.50s
                        Total time: 3590.71s
                               ETA: 6663.7s

################################################################################
                     [1m Learning iteration 1401/4000 [0m

                       Computation: 3274 steps/s (collection: 0.461s, learning 2.041s)
               Value function loss: 52250.6492
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 5339.95
               Mean episode length: 277.07
                 Mean success rate: 51.50
                  Mean reward/step: 21.27
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 2.50s
                        Total time: 3593.21s
                               ETA: 6661.0s

################################################################################
                     [1m Learning iteration 1402/4000 [0m

                       Computation: 3252 steps/s (collection: 0.449s, learning 2.070s)
               Value function loss: 103926.7742
                    Surrogate loss: 0.0165
             Mean action noise std: 0.91
                       Mean reward: 5355.44
               Mean episode length: 278.96
                 Mean success rate: 51.50
                  Mean reward/step: 21.28
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 11493376
                    Iteration time: 2.52s
                        Total time: 3595.73s
                               ETA: 6658.4s

################################################################################
                     [1m Learning iteration 1403/4000 [0m

                       Computation: 3226 steps/s (collection: 0.495s, learning 2.044s)
               Value function loss: 84061.2086
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 5356.81
               Mean episode length: 275.51
                 Mean success rate: 51.00
                  Mean reward/step: 20.83
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 2.54s
                        Total time: 3598.27s
                               ETA: 6655.8s

################################################################################
                     [1m Learning iteration 1404/4000 [0m

                       Computation: 3200 steps/s (collection: 0.472s, learning 2.087s)
               Value function loss: 99081.9027
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 5852.25
               Mean episode length: 295.23
                 Mean success rate: 54.50
                  Mean reward/step: 20.26
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11509760
                    Iteration time: 2.56s
                        Total time: 3600.83s
                               ETA: 6653.2s

################################################################################
                     [1m Learning iteration 1405/4000 [0m

                       Computation: 3168 steps/s (collection: 0.479s, learning 2.107s)
               Value function loss: 57441.7419
                    Surrogate loss: 0.0182
             Mean action noise std: 0.91
                       Mean reward: 5768.76
               Mean episode length: 294.18
                 Mean success rate: 53.50
                  Mean reward/step: 20.27
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 2.59s
                        Total time: 3603.42s
                               ETA: 6650.7s

################################################################################
                     [1m Learning iteration 1406/4000 [0m

                       Computation: 3233 steps/s (collection: 0.426s, learning 2.108s)
               Value function loss: 69039.2007
                    Surrogate loss: 0.0168
             Mean action noise std: 0.91
                       Mean reward: 6007.63
               Mean episode length: 300.24
                 Mean success rate: 56.00
                  Mean reward/step: 20.48
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11526144
                    Iteration time: 2.53s
                        Total time: 3605.95s
                               ETA: 6648.1s

################################################################################
                     [1m Learning iteration 1407/4000 [0m

                       Computation: 3187 steps/s (collection: 0.445s, learning 2.125s)
               Value function loss: 82476.6266
                    Surrogate loss: 0.0167
             Mean action noise std: 0.91
                       Mean reward: 6019.64
               Mean episode length: 300.30
                 Mean success rate: 56.50
                  Mean reward/step: 20.20
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 2.57s
                        Total time: 3608.52s
                               ETA: 6645.5s

################################################################################
                     [1m Learning iteration 1408/4000 [0m

                       Computation: 3264 steps/s (collection: 0.445s, learning 2.064s)
               Value function loss: 87166.8645
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 6100.71
               Mean episode length: 299.53
                 Mean success rate: 56.50
                  Mean reward/step: 19.84
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11542528
                    Iteration time: 2.51s
                        Total time: 3611.03s
                               ETA: 6642.9s

################################################################################
                     [1m Learning iteration 1409/4000 [0m

                       Computation: 3190 steps/s (collection: 0.483s, learning 2.085s)
               Value function loss: 96153.7737
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 5775.47
               Mean episode length: 289.19
                 Mean success rate: 54.50
                  Mean reward/step: 19.61
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 2.57s
                        Total time: 3613.60s
                               ETA: 6640.3s

################################################################################
                     [1m Learning iteration 1410/4000 [0m

                       Computation: 3183 steps/s (collection: 0.457s, learning 2.116s)
               Value function loss: 94432.5577
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 5236.76
               Mean episode length: 268.42
                 Mean success rate: 49.50
                  Mean reward/step: 18.73
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 11558912
                    Iteration time: 2.57s
                        Total time: 3616.17s
                               ETA: 6637.8s

################################################################################
                     [1m Learning iteration 1411/4000 [0m

                       Computation: 3195 steps/s (collection: 0.455s, learning 2.109s)
               Value function loss: 61332.2083
                    Surrogate loss: 0.0169
             Mean action noise std: 0.91
                       Mean reward: 5152.75
               Mean episode length: 264.31
                 Mean success rate: 49.50
                  Mean reward/step: 18.86
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 2.56s
                        Total time: 3618.73s
                               ETA: 6635.2s

################################################################################
                     [1m Learning iteration 1412/4000 [0m

                       Computation: 3172 steps/s (collection: 0.452s, learning 2.131s)
               Value function loss: 72846.6374
                    Surrogate loss: 0.0171
             Mean action noise std: 0.91
                       Mean reward: 5262.88
               Mean episode length: 271.71
                 Mean success rate: 50.50
                  Mean reward/step: 18.94
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11575296
                    Iteration time: 2.58s
                        Total time: 3621.32s
                               ETA: 6632.7s

################################################################################
                     [1m Learning iteration 1413/4000 [0m

                       Computation: 3227 steps/s (collection: 0.462s, learning 2.077s)
               Value function loss: 90364.4122
                    Surrogate loss: 0.0163
             Mean action noise std: 0.91
                       Mean reward: 5172.35
               Mean episode length: 267.09
                 Mean success rate: 50.00
                  Mean reward/step: 19.41
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 2.54s
                        Total time: 3623.86s
                               ETA: 6630.1s

################################################################################
                     [1m Learning iteration 1414/4000 [0m

                       Computation: 3202 steps/s (collection: 0.458s, learning 2.100s)
               Value function loss: 64417.1353
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 5258.15
               Mean episode length: 267.13
                 Mean success rate: 50.50
                  Mean reward/step: 19.64
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11591680
                    Iteration time: 2.56s
                        Total time: 3626.41s
                               ETA: 6627.5s

################################################################################
                     [1m Learning iteration 1415/4000 [0m

                       Computation: 3117 steps/s (collection: 0.454s, learning 2.174s)
               Value function loss: 75992.9887
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 5565.48
               Mean episode length: 278.19
                 Mean success rate: 52.50
                  Mean reward/step: 20.05
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 2.63s
                        Total time: 3629.04s
                               ETA: 6625.0s

################################################################################
                     [1m Learning iteration 1416/4000 [0m

                       Computation: 3134 steps/s (collection: 0.485s, learning 2.128s)
               Value function loss: 82777.6595
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 5527.23
               Mean episode length: 276.57
                 Mean success rate: 51.50
                  Mean reward/step: 20.52
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11608064
                    Iteration time: 2.61s
                        Total time: 3631.65s
                               ETA: 6622.6s

################################################################################
                     [1m Learning iteration 1417/4000 [0m

                       Computation: 3077 steps/s (collection: 0.529s, learning 2.133s)
               Value function loss: 85112.4508
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 5579.22
               Mean episode length: 276.50
                 Mean success rate: 51.00
                  Mean reward/step: 19.76
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 2.66s
                        Total time: 3634.32s
                               ETA: 6620.2s

################################################################################
                     [1m Learning iteration 1418/4000 [0m

                       Computation: 3147 steps/s (collection: 0.497s, learning 2.106s)
               Value function loss: 71458.2366
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 5879.46
               Mean episode length: 289.40
                 Mean success rate: 53.00
                  Mean reward/step: 19.48
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11624448
                    Iteration time: 2.60s
                        Total time: 3636.92s
                               ETA: 6617.7s

################################################################################
                     [1m Learning iteration 1419/4000 [0m

                       Computation: 3140 steps/s (collection: 0.493s, learning 2.116s)
               Value function loss: 79550.5729
                    Surrogate loss: 0.0176
             Mean action noise std: 0.91
                       Mean reward: 5609.11
               Mean episode length: 278.63
                 Mean success rate: 50.50
                  Mean reward/step: 20.41
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 2.61s
                        Total time: 3639.53s
                               ETA: 6615.2s

################################################################################
                     [1m Learning iteration 1420/4000 [0m

                       Computation: 3131 steps/s (collection: 0.491s, learning 2.125s)
               Value function loss: 84503.6609
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 5849.91
               Mean episode length: 286.38
                 Mean success rate: 52.50
                  Mean reward/step: 20.27
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11640832
                    Iteration time: 2.62s
                        Total time: 3642.14s
                               ETA: 6612.8s

################################################################################
                     [1m Learning iteration 1421/4000 [0m

                       Computation: 3150 steps/s (collection: 0.484s, learning 2.116s)
               Value function loss: 84182.3095
                    Surrogate loss: 0.0166
             Mean action noise std: 0.91
                       Mean reward: 6102.49
               Mean episode length: 299.37
                 Mean success rate: 54.50
                  Mean reward/step: 19.43
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 2.60s
                        Total time: 3644.74s
                               ETA: 6610.3s

################################################################################
                     [1m Learning iteration 1422/4000 [0m

                       Computation: 3151 steps/s (collection: 0.479s, learning 2.121s)
               Value function loss: 71905.5572
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 5997.89
               Mean episode length: 294.41
                 Mean success rate: 52.50
                  Mean reward/step: 19.29
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11657216
                    Iteration time: 2.60s
                        Total time: 3647.34s
                               ETA: 6607.8s

################################################################################
                     [1m Learning iteration 1423/4000 [0m

                       Computation: 3170 steps/s (collection: 0.490s, learning 2.094s)
               Value function loss: 88442.6193
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 5539.92
               Mean episode length: 279.54
                 Mean success rate: 50.00
                  Mean reward/step: 19.37
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 2.58s
                        Total time: 3649.93s
                               ETA: 6605.2s

################################################################################
                     [1m Learning iteration 1424/4000 [0m

                       Computation: 3152 steps/s (collection: 0.477s, learning 2.122s)
               Value function loss: 80344.5085
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 5785.97
               Mean episode length: 288.18
                 Mean success rate: 51.50
                  Mean reward/step: 19.41
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11673600
                    Iteration time: 2.60s
                        Total time: 3652.53s
                               ETA: 6602.7s

################################################################################
                     [1m Learning iteration 1425/4000 [0m

                       Computation: 3089 steps/s (collection: 0.514s, learning 2.138s)
               Value function loss: 97863.4470
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 5667.22
               Mean episode length: 291.74
                 Mean success rate: 52.00
                  Mean reward/step: 18.35
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 2.65s
                        Total time: 3655.18s
                               ETA: 6600.3s

################################################################################
                     [1m Learning iteration 1426/4000 [0m

                       Computation: 3117 steps/s (collection: 0.535s, learning 2.093s)
               Value function loss: 110347.1377
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 6316.01
               Mean episode length: 314.00
                 Mean success rate: 57.00
                  Mean reward/step: 17.97
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 11689984
                    Iteration time: 2.63s
                        Total time: 3657.81s
                               ETA: 6597.9s

################################################################################
                     [1m Learning iteration 1427/4000 [0m

                       Computation: 3160 steps/s (collection: 0.514s, learning 2.078s)
               Value function loss: 77061.4589
                    Surrogate loss: 0.0182
             Mean action noise std: 0.91
                       Mean reward: 6029.31
               Mean episode length: 306.56
                 Mean success rate: 54.50
                  Mean reward/step: 17.68
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 2.59s
                        Total time: 3660.40s
                               ETA: 6595.4s

################################################################################
                     [1m Learning iteration 1428/4000 [0m

                       Computation: 3256 steps/s (collection: 0.470s, learning 2.045s)
               Value function loss: 56716.1237
                    Surrogate loss: 0.0252
             Mean action noise std: 0.91
                       Mean reward: 5818.91
               Mean episode length: 301.13
                 Mean success rate: 53.50
                  Mean reward/step: 17.32
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11706368
                    Iteration time: 2.52s
                        Total time: 3662.91s
                               ETA: 6592.7s

################################################################################
                     [1m Learning iteration 1429/4000 [0m

                       Computation: 3180 steps/s (collection: 0.489s, learning 2.087s)
               Value function loss: 63770.1691
                    Surrogate loss: 0.0172
             Mean action noise std: 0.91
                       Mean reward: 5745.16
               Mean episode length: 300.81
                 Mean success rate: 53.50
                  Mean reward/step: 18.23
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 2.58s
                        Total time: 3665.49s
                               ETA: 6590.2s

################################################################################
                     [1m Learning iteration 1430/4000 [0m

                       Computation: 3236 steps/s (collection: 0.485s, learning 2.046s)
               Value function loss: 60493.5693
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 5907.50
               Mean episode length: 313.36
                 Mean success rate: 55.50
                  Mean reward/step: 18.64
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11722752
                    Iteration time: 2.53s
                        Total time: 3668.02s
                               ETA: 6587.6s

################################################################################
                     [1m Learning iteration 1431/4000 [0m

                       Computation: 3271 steps/s (collection: 0.460s, learning 2.044s)
               Value function loss: 61761.9380
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 5516.19
               Mean episode length: 295.17
                 Mean success rate: 52.00
                  Mean reward/step: 20.18
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 2.50s
                        Total time: 3670.53s
                               ETA: 6584.9s

################################################################################
                     [1m Learning iteration 1432/4000 [0m

                       Computation: 3245 steps/s (collection: 0.463s, learning 2.061s)
               Value function loss: 83039.8300
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 5435.28
               Mean episode length: 288.73
                 Mean success rate: 50.50
                  Mean reward/step: 19.95
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11739136
                    Iteration time: 2.52s
                        Total time: 3673.05s
                               ETA: 6582.3s

################################################################################
                     [1m Learning iteration 1433/4000 [0m

                       Computation: 3264 steps/s (collection: 0.472s, learning 2.038s)
               Value function loss: 44668.6592
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 5032.75
               Mean episode length: 274.02
                 Mean success rate: 47.00
                  Mean reward/step: 20.13
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 2.51s
                        Total time: 3675.56s
                               ETA: 6579.6s

################################################################################
                     [1m Learning iteration 1434/4000 [0m

                       Computation: 3163 steps/s (collection: 0.483s, learning 2.107s)
               Value function loss: 68309.7118
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 5001.09
               Mean episode length: 273.35
                 Mean success rate: 46.50
                  Mean reward/step: 20.25
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11755520
                    Iteration time: 2.59s
                        Total time: 3678.15s
                               ETA: 6577.1s

################################################################################
                     [1m Learning iteration 1435/4000 [0m

                       Computation: 3284 steps/s (collection: 0.459s, learning 2.036s)
               Value function loss: 80787.9235
                    Surrogate loss: 0.0111
             Mean action noise std: 0.91
                       Mean reward: 5205.28
               Mean episode length: 283.38
                 Mean success rate: 48.50
                  Mean reward/step: 20.44
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 2.49s
                        Total time: 3680.64s
                               ETA: 6574.4s

################################################################################
                     [1m Learning iteration 1436/4000 [0m

                       Computation: 3242 steps/s (collection: 0.478s, learning 2.048s)
               Value function loss: 66464.2536
                    Surrogate loss: 0.0108
             Mean action noise std: 0.91
                       Mean reward: 5501.85
               Mean episode length: 294.33
                 Mean success rate: 51.00
                  Mean reward/step: 19.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11771904
                    Iteration time: 2.53s
                        Total time: 3683.17s
                               ETA: 6571.8s

################################################################################
                     [1m Learning iteration 1437/4000 [0m

                       Computation: 3240 steps/s (collection: 0.489s, learning 2.039s)
               Value function loss: 68643.3296
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 5678.37
               Mean episode length: 301.25
                 Mean success rate: 52.00
                  Mean reward/step: 20.20
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 2.53s
                        Total time: 3685.70s
                               ETA: 6569.2s

################################################################################
                     [1m Learning iteration 1438/4000 [0m

                       Computation: 3207 steps/s (collection: 0.509s, learning 2.045s)
               Value function loss: 110965.0420
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 5790.06
               Mean episode length: 301.70
                 Mean success rate: 52.50
                  Mean reward/step: 19.66
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 11788288
                    Iteration time: 2.55s
                        Total time: 3688.25s
                               ETA: 6566.6s

################################################################################
                     [1m Learning iteration 1439/4000 [0m

                       Computation: 3213 steps/s (collection: 0.467s, learning 2.083s)
               Value function loss: 89195.5058
                    Surrogate loss: 0.0165
             Mean action noise std: 0.91
                       Mean reward: 5940.80
               Mean episode length: 304.78
                 Mean success rate: 54.00
                  Mean reward/step: 18.78
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 2.55s
                        Total time: 3690.80s
                               ETA: 6564.0s

################################################################################
                     [1m Learning iteration 1440/4000 [0m

                       Computation: 3178 steps/s (collection: 0.465s, learning 2.112s)
               Value function loss: 91534.8443
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 6202.49
               Mean episode length: 317.40
                 Mean success rate: 57.00
                  Mean reward/step: 19.09
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11804672
                    Iteration time: 2.58s
                        Total time: 3693.38s
                               ETA: 6561.4s

################################################################################
                     [1m Learning iteration 1441/4000 [0m

                       Computation: 3122 steps/s (collection: 0.534s, learning 2.089s)
               Value function loss: 101678.1361
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 6542.06
               Mean episode length: 327.64
                 Mean success rate: 61.00
                  Mean reward/step: 18.44
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 2.62s
                        Total time: 3696.00s
                               ETA: 6559.0s

################################################################################
                     [1m Learning iteration 1442/4000 [0m

                       Computation: 3049 steps/s (collection: 0.547s, learning 2.140s)
               Value function loss: 85838.2134
                    Surrogate loss: 0.0219
             Mean action noise std: 0.91
                       Mean reward: 6314.30
               Mean episode length: 315.79
                 Mean success rate: 59.00
                  Mean reward/step: 18.00
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11821056
                    Iteration time: 2.69s
                        Total time: 3698.69s
                               ETA: 6556.6s

################################################################################
                     [1m Learning iteration 1443/4000 [0m

                       Computation: 3118 steps/s (collection: 0.519s, learning 2.108s)
               Value function loss: 82658.3416
                    Surrogate loss: 0.0165
             Mean action noise std: 0.91
                       Mean reward: 6175.77
               Mean episode length: 311.18
                 Mean success rate: 59.00
                  Mean reward/step: 18.13
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 2.63s
                        Total time: 3701.31s
                               ETA: 6554.2s

################################################################################
                     [1m Learning iteration 1444/4000 [0m

                       Computation: 3147 steps/s (collection: 0.513s, learning 2.090s)
               Value function loss: 68948.3188
                    Surrogate loss: 0.0166
             Mean action noise std: 0.91
                       Mean reward: 6054.98
               Mean episode length: 305.82
                 Mean success rate: 58.00
                  Mean reward/step: 19.30
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11837440
                    Iteration time: 2.60s
                        Total time: 3703.92s
                               ETA: 6551.7s

################################################################################
                     [1m Learning iteration 1445/4000 [0m

                       Computation: 3081 steps/s (collection: 0.510s, learning 2.148s)
               Value function loss: 81677.1442
                    Surrogate loss: 0.0164
             Mean action noise std: 0.91
                       Mean reward: 6533.62
               Mean episode length: 322.89
                 Mean success rate: 61.50
                  Mean reward/step: 18.59
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 2.66s
                        Total time: 3706.57s
                               ETA: 6549.3s

################################################################################
                     [1m Learning iteration 1446/4000 [0m

                       Computation: 3160 steps/s (collection: 0.456s, learning 2.136s)
               Value function loss: 42422.3333
                    Surrogate loss: 0.0201
             Mean action noise std: 0.91
                       Mean reward: 6224.34
               Mean episode length: 313.54
                 Mean success rate: 59.50
                  Mean reward/step: 19.08
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11853824
                    Iteration time: 2.59s
                        Total time: 3709.17s
                               ETA: 6546.8s

################################################################################
                     [1m Learning iteration 1447/4000 [0m

                       Computation: 3182 steps/s (collection: 0.480s, learning 2.094s)
               Value function loss: 62704.9761
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 5818.88
               Mean episode length: 300.18
                 Mean success rate: 56.50
                  Mean reward/step: 19.85
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 2.57s
                        Total time: 3711.74s
                               ETA: 6544.3s

################################################################################
                     [1m Learning iteration 1448/4000 [0m

                       Computation: 3150 steps/s (collection: 0.484s, learning 2.116s)
               Value function loss: 66373.8360
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 5594.94
               Mean episode length: 293.48
                 Mean success rate: 55.00
                  Mean reward/step: 19.35
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11870208
                    Iteration time: 2.60s
                        Total time: 3714.34s
                               ETA: 6541.8s

################################################################################
                     [1m Learning iteration 1449/4000 [0m

                       Computation: 3095 steps/s (collection: 0.490s, learning 2.156s)
               Value function loss: 54554.5880
                    Surrogate loss: 0.0170
             Mean action noise std: 0.91
                       Mean reward: 5422.61
               Mean episode length: 290.75
                 Mean success rate: 53.00
                  Mean reward/step: 19.69
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 2.65s
                        Total time: 3716.99s
                               ETA: 6539.3s

################################################################################
                     [1m Learning iteration 1450/4000 [0m

                       Computation: 3071 steps/s (collection: 0.548s, learning 2.120s)
               Value function loss: 65617.3486
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 5294.88
               Mean episode length: 291.52
                 Mean success rate: 53.00
                  Mean reward/step: 19.70
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11886592
                    Iteration time: 2.67s
                        Total time: 3719.65s
                               ETA: 6537.0s

################################################################################
                     [1m Learning iteration 1451/4000 [0m

                       Computation: 3066 steps/s (collection: 0.550s, learning 2.121s)
               Value function loss: 58186.1581
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 5346.21
               Mean episode length: 294.89
                 Mean success rate: 53.50
                  Mean reward/step: 19.30
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 2.67s
                        Total time: 3722.33s
                               ETA: 6534.6s

################################################################################
                     [1m Learning iteration 1452/4000 [0m

                       Computation: 3155 steps/s (collection: 0.483s, learning 2.113s)
               Value function loss: 77045.2783
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 5542.07
               Mean episode length: 303.21
                 Mean success rate: 55.00
                  Mean reward/step: 20.05
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11902976
                    Iteration time: 2.60s
                        Total time: 3724.92s
                               ETA: 6532.1s

################################################################################
                     [1m Learning iteration 1453/4000 [0m

                       Computation: 3141 steps/s (collection: 0.495s, learning 2.112s)
               Value function loss: 56725.1564
                    Surrogate loss: 0.0164
             Mean action noise std: 0.91
                       Mean reward: 5261.67
               Mean episode length: 290.78
                 Mean success rate: 53.00
                  Mean reward/step: 19.78
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 2.61s
                        Total time: 3727.53s
                               ETA: 6529.6s

################################################################################
                     [1m Learning iteration 1454/4000 [0m

                       Computation: 3146 steps/s (collection: 0.481s, learning 2.122s)
               Value function loss: 91935.1259
                    Surrogate loss: 0.0174
             Mean action noise std: 0.91
                       Mean reward: 5735.83
               Mean episode length: 306.96
                 Mean success rate: 56.00
                  Mean reward/step: 19.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11919360
                    Iteration time: 2.60s
                        Total time: 3730.13s
                               ETA: 6527.1s

################################################################################
                     [1m Learning iteration 1455/4000 [0m

                       Computation: 3141 steps/s (collection: 0.487s, learning 2.121s)
               Value function loss: 68586.2169
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 5599.04
               Mean episode length: 303.43
                 Mean success rate: 54.00
                  Mean reward/step: 19.63
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 2.61s
                        Total time: 3732.74s
                               ETA: 6524.6s

################################################################################
                     [1m Learning iteration 1456/4000 [0m

                       Computation: 3151 steps/s (collection: 0.490s, learning 2.109s)
               Value function loss: 103597.0012
                    Surrogate loss: 0.0160
             Mean action noise std: 0.91
                       Mean reward: 6274.35
               Mean episode length: 324.40
                 Mean success rate: 59.00
                  Mean reward/step: 19.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11935744
                    Iteration time: 2.60s
                        Total time: 3735.34s
                               ETA: 6522.1s

################################################################################
                     [1m Learning iteration 1457/4000 [0m

                       Computation: 3213 steps/s (collection: 0.490s, learning 2.059s)
               Value function loss: 91774.1372
                    Surrogate loss: 0.0166
             Mean action noise std: 0.91
                       Mean reward: 6227.42
               Mean episode length: 317.98
                 Mean success rate: 58.50
                  Mean reward/step: 18.49
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 2.55s
                        Total time: 3737.89s
                               ETA: 6519.5s

################################################################################
                     [1m Learning iteration 1458/4000 [0m

                       Computation: 3056 steps/s (collection: 0.574s, learning 2.106s)
               Value function loss: 83238.4526
                    Surrogate loss: 0.0171
             Mean action noise std: 0.91
                       Mean reward: 6301.09
               Mean episode length: 312.64
                 Mean success rate: 58.50
                  Mean reward/step: 17.78
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11952128
                    Iteration time: 2.68s
                        Total time: 3740.57s
                               ETA: 6517.2s

################################################################################
                     [1m Learning iteration 1459/4000 [0m

                       Computation: 3195 steps/s (collection: 0.459s, learning 2.105s)
               Value function loss: 73276.5771
                    Surrogate loss: 0.0201
             Mean action noise std: 0.91
                       Mean reward: 6402.29
               Mean episode length: 320.21
                 Mean success rate: 60.00
                  Mean reward/step: 17.86
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 2.56s
                        Total time: 3743.13s
                               ETA: 6514.6s

################################################################################
                     [1m Learning iteration 1460/4000 [0m

                       Computation: 3164 steps/s (collection: 0.510s, learning 2.079s)
               Value function loss: 109310.5102
                    Surrogate loss: 0.0182
             Mean action noise std: 0.91
                       Mean reward: 6550.04
               Mean episode length: 323.34
                 Mean success rate: 61.50
                  Mean reward/step: 18.43
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11968512
                    Iteration time: 2.59s
                        Total time: 3745.72s
                               ETA: 6512.1s

################################################################################
                     [1m Learning iteration 1461/4000 [0m

                       Computation: 3057 steps/s (collection: 0.521s, learning 2.159s)
               Value function loss: 70492.9704
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 6184.42
               Mean episode length: 307.28
                 Mean success rate: 59.50
                  Mean reward/step: 18.06
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 2.68s
                        Total time: 3748.40s
                               ETA: 6509.7s

################################################################################
                     [1m Learning iteration 1462/4000 [0m

                       Computation: 3100 steps/s (collection: 0.539s, learning 2.104s)
               Value function loss: 64249.3146
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 5702.80
               Mean episode length: 294.69
                 Mean success rate: 55.50
                  Mean reward/step: 18.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11984896
                    Iteration time: 2.64s
                        Total time: 3751.04s
                               ETA: 6507.3s

################################################################################
                     [1m Learning iteration 1463/4000 [0m

                       Computation: 3216 steps/s (collection: 0.472s, learning 2.075s)
               Value function loss: 67501.9099
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 5729.86
               Mean episode length: 297.36
                 Mean success rate: 56.00
                  Mean reward/step: 19.41
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 2.55s
                        Total time: 3753.59s
                               ETA: 6504.7s

################################################################################
                     [1m Learning iteration 1464/4000 [0m

                       Computation: 3147 steps/s (collection: 0.487s, learning 2.116s)
               Value function loss: 68340.0067
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 5832.48
               Mean episode length: 300.44
                 Mean success rate: 56.50
                  Mean reward/step: 20.07
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12001280
                    Iteration time: 2.60s
                        Total time: 3756.19s
                               ETA: 6502.2s

################################################################################
                     [1m Learning iteration 1465/4000 [0m

                       Computation: 3117 steps/s (collection: 0.488s, learning 2.140s)
               Value function loss: 84483.8702
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 5763.73
               Mean episode length: 303.80
                 Mean success rate: 56.00
                  Mean reward/step: 20.66
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 2.63s
                        Total time: 3758.82s
                               ETA: 6499.7s

################################################################################
                     [1m Learning iteration 1466/4000 [0m

                       Computation: 3165 steps/s (collection: 0.489s, learning 2.098s)
               Value function loss: 62293.0698
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 5852.42
               Mean episode length: 308.49
                 Mean success rate: 57.50
                  Mean reward/step: 20.24
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12017664
                    Iteration time: 2.59s
                        Total time: 3761.41s
                               ETA: 6497.2s

################################################################################
                     [1m Learning iteration 1467/4000 [0m

                       Computation: 3214 steps/s (collection: 0.452s, learning 2.096s)
               Value function loss: 78286.0610
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 5730.47
               Mean episode length: 302.49
                 Mean success rate: 57.50
                  Mean reward/step: 19.98
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 2.55s
                        Total time: 3763.96s
                               ETA: 6494.6s

################################################################################
                     [1m Learning iteration 1468/4000 [0m

                       Computation: 3194 steps/s (collection: 0.476s, learning 2.089s)
               Value function loss: 73526.9550
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 5486.94
               Mean episode length: 293.72
                 Mean success rate: 55.00
                  Mean reward/step: 19.47
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12034048
                    Iteration time: 2.56s
                        Total time: 3766.52s
                               ETA: 6492.1s

################################################################################
                     [1m Learning iteration 1469/4000 [0m

                       Computation: 3192 steps/s (collection: 0.485s, learning 2.081s)
               Value function loss: 54223.0679
                    Surrogate loss: 0.0164
             Mean action noise std: 0.91
                       Mean reward: 5252.10
               Mean episode length: 287.38
                 Mean success rate: 52.00
                  Mean reward/step: 19.09
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 2.57s
                        Total time: 3769.09s
                               ETA: 6489.5s

################################################################################
                     [1m Learning iteration 1470/4000 [0m

                       Computation: 3125 steps/s (collection: 0.522s, learning 2.099s)
               Value function loss: 76569.6908
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 5517.89
               Mean episode length: 295.06
                 Mean success rate: 55.00
                  Mean reward/step: 19.11
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12050432
                    Iteration time: 2.62s
                        Total time: 3771.71s
                               ETA: 6487.0s

################################################################################
                     [1m Learning iteration 1471/4000 [0m

                       Computation: 3196 steps/s (collection: 0.463s, learning 2.100s)
               Value function loss: 83290.2398
                    Surrogate loss: 0.0104
             Mean action noise std: 0.91
                       Mean reward: 5495.06
               Mean episode length: 295.49
                 Mean success rate: 55.50
                  Mean reward/step: 19.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 2.56s
                        Total time: 3774.27s
                               ETA: 6484.5s

################################################################################
                     [1m Learning iteration 1472/4000 [0m

                       Computation: 3182 steps/s (collection: 0.481s, learning 2.093s)
               Value function loss: 115438.2196
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 5892.73
               Mean episode length: 312.08
                 Mean success rate: 58.50
                  Mean reward/step: 18.82
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 12066816
                    Iteration time: 2.57s
                        Total time: 3776.85s
                               ETA: 6481.9s

################################################################################
                     [1m Learning iteration 1473/4000 [0m

                       Computation: 3196 steps/s (collection: 0.472s, learning 2.090s)
               Value function loss: 88957.8083
                    Surrogate loss: 0.0116
             Mean action noise std: 0.91
                       Mean reward: 5815.43
               Mean episode length: 307.82
                 Mean success rate: 58.00
                  Mean reward/step: 18.91
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 2.56s
                        Total time: 3779.41s
                               ETA: 6479.4s

################################################################################
                     [1m Learning iteration 1474/4000 [0m

                       Computation: 3173 steps/s (collection: 0.470s, learning 2.112s)
               Value function loss: 99139.0731
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 6199.66
               Mean episode length: 321.12
                 Mean success rate: 60.00
                  Mean reward/step: 18.94
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12083200
                    Iteration time: 2.58s
                        Total time: 3781.99s
                               ETA: 6476.8s

################################################################################
                     [1m Learning iteration 1475/4000 [0m

                       Computation: 3202 steps/s (collection: 0.473s, learning 2.085s)
               Value function loss: 57602.3344
                    Surrogate loss: 0.0177
             Mean action noise std: 0.91
                       Mean reward: 6411.36
               Mean episode length: 325.01
                 Mean success rate: 61.00
                  Mean reward/step: 19.37
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 2.56s
                        Total time: 3784.55s
                               ETA: 6474.2s

################################################################################
                     [1m Learning iteration 1476/4000 [0m

                       Computation: 3225 steps/s (collection: 0.466s, learning 2.073s)
               Value function loss: 104402.6644
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 6913.54
               Mean episode length: 346.70
                 Mean success rate: 65.00
                  Mean reward/step: 20.50
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12099584
                    Iteration time: 2.54s
                        Total time: 3787.09s
                               ETA: 6471.6s

################################################################################
                     [1m Learning iteration 1477/4000 [0m

                       Computation: 3170 steps/s (collection: 0.492s, learning 2.092s)
               Value function loss: 79367.6828
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 7227.77
               Mean episode length: 356.48
                 Mean success rate: 68.00
                  Mean reward/step: 20.48
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 2.58s
                        Total time: 3789.67s
                               ETA: 6469.1s

################################################################################
                     [1m Learning iteration 1478/4000 [0m

                       Computation: 3239 steps/s (collection: 0.461s, learning 2.068s)
               Value function loss: 75460.8730
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 7339.97
               Mean episode length: 364.97
                 Mean success rate: 69.00
                  Mean reward/step: 20.62
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12115968
                    Iteration time: 2.53s
                        Total time: 3792.20s
                               ETA: 6466.5s

################################################################################
                     [1m Learning iteration 1479/4000 [0m

                       Computation: 3115 steps/s (collection: 0.526s, learning 2.103s)
               Value function loss: 78613.3391
                    Surrogate loss: 0.0171
             Mean action noise std: 0.91
                       Mean reward: 7429.89
               Mean episode length: 365.62
                 Mean success rate: 70.00
                  Mean reward/step: 21.34
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 2.63s
                        Total time: 3794.83s
                               ETA: 6464.0s

################################################################################
                     [1m Learning iteration 1480/4000 [0m

                       Computation: 3125 steps/s (collection: 0.477s, learning 2.144s)
               Value function loss: 67107.8127
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 7178.56
               Mean episode length: 353.82
                 Mean success rate: 69.00
                  Mean reward/step: 20.76
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12132352
                    Iteration time: 2.62s
                        Total time: 3797.45s
                               ETA: 6461.6s

################################################################################
                     [1m Learning iteration 1481/4000 [0m

                       Computation: 3214 steps/s (collection: 0.488s, learning 2.061s)
               Value function loss: 64095.1905
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 6849.28
               Mean episode length: 340.33
                 Mean success rate: 65.00
                  Mean reward/step: 21.12
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 2.55s
                        Total time: 3800.00s
                               ETA: 6459.0s

################################################################################
                     [1m Learning iteration 1482/4000 [0m

                       Computation: 3109 steps/s (collection: 0.512s, learning 2.123s)
               Value function loss: 50527.1714
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 7029.45
               Mean episode length: 350.60
                 Mean success rate: 66.50
                  Mean reward/step: 21.30
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12148736
                    Iteration time: 2.63s
                        Total time: 3802.64s
                               ETA: 6456.5s

################################################################################
                     [1m Learning iteration 1483/4000 [0m

                       Computation: 3164 steps/s (collection: 0.464s, learning 2.125s)
               Value function loss: 82288.1538
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 6613.16
               Mean episode length: 338.52
                 Mean success rate: 63.50
                  Mean reward/step: 21.53
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 2.59s
                        Total time: 3805.22s
                               ETA: 6454.0s

################################################################################
                     [1m Learning iteration 1484/4000 [0m

                       Computation: 3175 steps/s (collection: 0.487s, learning 2.093s)
               Value function loss: 61521.9522
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 6408.16
               Mean episode length: 328.61
                 Mean success rate: 62.00
                  Mean reward/step: 21.38
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12165120
                    Iteration time: 2.58s
                        Total time: 3807.80s
                               ETA: 6451.5s

################################################################################
                     [1m Learning iteration 1485/4000 [0m

                       Computation: 3251 steps/s (collection: 0.474s, learning 2.046s)
               Value function loss: 101678.1348
                    Surrogate loss: 0.0161
             Mean action noise std: 0.91
                       Mean reward: 6216.12
               Mean episode length: 324.79
                 Mean success rate: 61.50
                  Mean reward/step: 21.32
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 2.52s
                        Total time: 3810.32s
                               ETA: 6448.8s

################################################################################
                     [1m Learning iteration 1486/4000 [0m

                       Computation: 3148 steps/s (collection: 0.466s, learning 2.136s)
               Value function loss: 95623.5723
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 6276.14
               Mean episode length: 324.78
                 Mean success rate: 61.50
                  Mean reward/step: 20.99
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12181504
                    Iteration time: 2.60s
                        Total time: 3812.93s
                               ETA: 6446.3s

################################################################################
                     [1m Learning iteration 1487/4000 [0m

                       Computation: 3137 steps/s (collection: 0.491s, learning 2.119s)
               Value function loss: 62265.1201
                    Surrogate loss: 0.0168
             Mean action noise std: 0.91
                       Mean reward: 6383.72
               Mean episode length: 324.05
                 Mean success rate: 61.00
                  Mean reward/step: 20.21
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 2.61s
                        Total time: 3815.54s
                               ETA: 6443.8s

################################################################################
                     [1m Learning iteration 1488/4000 [0m

                       Computation: 3223 steps/s (collection: 0.480s, learning 2.062s)
               Value function loss: 122050.4057
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 6769.16
               Mean episode length: 333.40
                 Mean success rate: 62.50
                  Mean reward/step: 19.39
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 12197888
                    Iteration time: 2.54s
                        Total time: 3818.08s
                               ETA: 6441.2s

################################################################################
                     [1m Learning iteration 1489/4000 [0m

                       Computation: 3248 steps/s (collection: 0.475s, learning 2.047s)
               Value function loss: 92715.9156
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 6994.73
               Mean episode length: 342.88
                 Mean success rate: 65.50
                  Mean reward/step: 18.41
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 2.52s
                        Total time: 3820.60s
                               ETA: 6438.6s

################################################################################
                     [1m Learning iteration 1490/4000 [0m

                       Computation: 3171 steps/s (collection: 0.487s, learning 2.096s)
               Value function loss: 76905.9701
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 6705.00
               Mean episode length: 326.95
                 Mean success rate: 62.50
                  Mean reward/step: 18.38
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 12214272
                    Iteration time: 2.58s
                        Total time: 3823.18s
                               ETA: 6436.1s

################################################################################
                     [1m Learning iteration 1491/4000 [0m

                       Computation: 3214 steps/s (collection: 0.474s, learning 2.074s)
               Value function loss: 65999.9279
                    Surrogate loss: 0.0191
             Mean action noise std: 0.91
                       Mean reward: 6865.58
               Mean episode length: 333.59
                 Mean success rate: 63.50
                  Mean reward/step: 19.59
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 2.55s
                        Total time: 3825.73s
                               ETA: 6433.5s

################################################################################
                     [1m Learning iteration 1492/4000 [0m

                       Computation: 3244 steps/s (collection: 0.482s, learning 2.043s)
               Value function loss: 83087.3286
                    Surrogate loss: 0.0171
             Mean action noise std: 0.91
                       Mean reward: 7220.87
               Mean episode length: 341.06
                 Mean success rate: 65.00
                  Mean reward/step: 19.34
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12230656
                    Iteration time: 2.53s
                        Total time: 3828.26s
                               ETA: 6430.9s

################################################################################
                     [1m Learning iteration 1493/4000 [0m

                       Computation: 3295 steps/s (collection: 0.428s, learning 2.058s)
               Value function loss: 85118.8330
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 7266.79
               Mean episode length: 340.43
                 Mean success rate: 65.50
                  Mean reward/step: 18.80
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 2.49s
                        Total time: 3830.74s
                               ETA: 6428.2s

################################################################################
                     [1m Learning iteration 1494/4000 [0m

                       Computation: 3223 steps/s (collection: 0.469s, learning 2.073s)
               Value function loss: 77861.0528
                    Surrogate loss: 0.0186
             Mean action noise std: 0.91
                       Mean reward: 7180.84
               Mean episode length: 340.62
                 Mean success rate: 65.00
                  Mean reward/step: 18.32
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12247040
                    Iteration time: 2.54s
                        Total time: 3833.28s
                               ETA: 6425.6s

################################################################################
                     [1m Learning iteration 1495/4000 [0m

                       Computation: 3169 steps/s (collection: 0.497s, learning 2.088s)
               Value function loss: 68369.1027
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 6724.29
               Mean episode length: 328.61
                 Mean success rate: 62.50
                  Mean reward/step: 18.94
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 2.58s
                        Total time: 3835.87s
                               ETA: 6423.0s

################################################################################
                     [1m Learning iteration 1496/4000 [0m

                       Computation: 3171 steps/s (collection: 0.497s, learning 2.086s)
               Value function loss: 65937.4382
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 6670.26
               Mean episode length: 329.51
                 Mean success rate: 62.00
                  Mean reward/step: 19.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12263424
                    Iteration time: 2.58s
                        Total time: 3838.45s
                               ETA: 6420.5s

################################################################################
                     [1m Learning iteration 1497/4000 [0m

                       Computation: 3232 steps/s (collection: 0.455s, learning 2.079s)
               Value function loss: 71402.5240
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 6444.60
               Mean episode length: 319.94
                 Mean success rate: 60.00
                  Mean reward/step: 19.88
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 2.53s
                        Total time: 3840.99s
                               ETA: 6417.9s

################################################################################
                     [1m Learning iteration 1498/4000 [0m

                       Computation: 3208 steps/s (collection: 0.495s, learning 2.058s)
               Value function loss: 56041.8773
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 6618.21
               Mean episode length: 327.19
                 Mean success rate: 61.50
                  Mean reward/step: 20.03
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12279808
                    Iteration time: 2.55s
                        Total time: 3843.54s
                               ETA: 6415.3s

################################################################################
                     [1m Learning iteration 1499/4000 [0m

                       Computation: 3225 steps/s (collection: 0.458s, learning 2.082s)
               Value function loss: 88951.6636
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 6587.01
               Mean episode length: 324.12
                 Mean success rate: 62.00
                  Mean reward/step: 19.98
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 2.54s
                        Total time: 3846.08s
                               ETA: 6412.7s

################################################################################
                     [1m Learning iteration 1500/4000 [0m

                       Computation: 3184 steps/s (collection: 0.499s, learning 2.074s)
               Value function loss: 61226.6358
                    Surrogate loss: 0.0163
             Mean action noise std: 0.91
                       Mean reward: 6618.76
               Mean episode length: 331.05
                 Mean success rate: 63.00
                  Mean reward/step: 19.62
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12296192
                    Iteration time: 2.57s
                        Total time: 3848.65s
                               ETA: 6410.1s

################################################################################
                     [1m Learning iteration 1501/4000 [0m

                       Computation: 3303 steps/s (collection: 0.442s, learning 2.037s)
               Value function loss: 77415.2668
                    Surrogate loss: 0.0169
             Mean action noise std: 0.91
                       Mean reward: 6458.07
               Mean episode length: 330.89
                 Mean success rate: 62.00
                  Mean reward/step: 19.28
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 2.48s
                        Total time: 3851.13s
                               ETA: 6407.4s

################################################################################
                     [1m Learning iteration 1502/4000 [0m

                       Computation: 3155 steps/s (collection: 0.491s, learning 2.105s)
               Value function loss: 62362.5574
                    Surrogate loss: 0.0168
             Mean action noise std: 0.91
                       Mean reward: 5991.90
               Mean episode length: 317.79
                 Mean success rate: 58.50
                  Mean reward/step: 18.72
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12312576
                    Iteration time: 2.60s
                        Total time: 3853.73s
                               ETA: 6404.9s

################################################################################
                     [1m Learning iteration 1503/4000 [0m

                       Computation: 3256 steps/s (collection: 0.471s, learning 2.045s)
               Value function loss: 87435.1664
                    Surrogate loss: 0.0161
             Mean action noise std: 0.91
                       Mean reward: 6407.66
               Mean episode length: 332.23
                 Mean success rate: 62.50
                  Mean reward/step: 18.77
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 2.52s
                        Total time: 3856.24s
                               ETA: 6402.3s

################################################################################
                     [1m Learning iteration 1504/4000 [0m

                       Computation: 3193 steps/s (collection: 0.480s, learning 2.085s)
               Value function loss: 98316.7357
                    Surrogate loss: 0.0169
             Mean action noise std: 0.91
                       Mean reward: 6776.41
               Mean episode length: 348.02
                 Mean success rate: 66.50
                  Mean reward/step: 18.51
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12328960
                    Iteration time: 2.56s
                        Total time: 3858.81s
                               ETA: 6399.7s

################################################################################
                     [1m Learning iteration 1505/4000 [0m

                       Computation: 3120 steps/s (collection: 0.478s, learning 2.147s)
               Value function loss: 68203.8187
                    Surrogate loss: 0.0191
             Mean action noise std: 0.91
                       Mean reward: 6325.91
               Mean episode length: 332.82
                 Mean success rate: 62.00
                  Mean reward/step: 18.33
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 2.63s
                        Total time: 3861.43s
                               ETA: 6397.3s

################################################################################
                     [1m Learning iteration 1506/4000 [0m

                       Computation: 3199 steps/s (collection: 0.476s, learning 2.084s)
               Value function loss: 45258.3909
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 6051.38
               Mean episode length: 326.18
                 Mean success rate: 60.00
                  Mean reward/step: 18.84
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12345344
                    Iteration time: 2.56s
                        Total time: 3863.99s
                               ETA: 6394.7s

################################################################################
                     [1m Learning iteration 1507/4000 [0m

                       Computation: 3142 steps/s (collection: 0.522s, learning 2.085s)
               Value function loss: 88825.2045
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 5906.68
               Mean episode length: 317.15
                 Mean success rate: 57.50
                  Mean reward/step: 19.36
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 2.61s
                        Total time: 3866.60s
                               ETA: 6392.2s

################################################################################
                     [1m Learning iteration 1508/4000 [0m

                       Computation: 3236 steps/s (collection: 0.438s, learning 2.094s)
               Value function loss: 81538.6658
                    Surrogate loss: 0.0181
             Mean action noise std: 0.91
                       Mean reward: 5902.97
               Mean episode length: 316.19
                 Mean success rate: 57.50
                  Mean reward/step: 19.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12361728
                    Iteration time: 2.53s
                        Total time: 3869.13s
                               ETA: 6389.6s

################################################################################
                     [1m Learning iteration 1509/4000 [0m

                       Computation: 3173 steps/s (collection: 0.490s, learning 2.092s)
               Value function loss: 72654.6228
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 5950.71
               Mean episode length: 313.00
                 Mean success rate: 57.50
                  Mean reward/step: 19.77
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 2.58s
                        Total time: 3871.71s
                               ETA: 6387.0s

################################################################################
                     [1m Learning iteration 1510/4000 [0m

                       Computation: 3131 steps/s (collection: 0.499s, learning 2.117s)
               Value function loss: 99046.3537
                    Surrogate loss: 0.0170
             Mean action noise std: 0.91
                       Mean reward: 6140.24
               Mean episode length: 321.90
                 Mean success rate: 59.00
                  Mean reward/step: 19.62
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12378112
                    Iteration time: 2.62s
                        Total time: 3874.33s
                               ETA: 6384.6s

################################################################################
                     [1m Learning iteration 1511/4000 [0m

                       Computation: 3226 steps/s (collection: 0.484s, learning 2.055s)
               Value function loss: 53796.3697
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 6178.76
               Mean episode length: 323.39
                 Mean success rate: 59.00
                  Mean reward/step: 20.05
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 2.54s
                        Total time: 3876.87s
                               ETA: 6382.0s

################################################################################
                     [1m Learning iteration 1512/4000 [0m

                       Computation: 3279 steps/s (collection: 0.470s, learning 2.028s)
               Value function loss: 79457.4978
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 5929.67
               Mean episode length: 316.60
                 Mean success rate: 57.50
                  Mean reward/step: 20.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12394496
                    Iteration time: 2.50s
                        Total time: 3879.37s
                               ETA: 6379.3s

################################################################################
                     [1m Learning iteration 1513/4000 [0m

                       Computation: 3201 steps/s (collection: 0.458s, learning 2.100s)
               Value function loss: 70509.8522
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 6198.60
               Mean episode length: 323.32
                 Mean success rate: 59.00
                  Mean reward/step: 21.04
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 2.56s
                        Total time: 3881.92s
                               ETA: 6376.7s

################################################################################
                     [1m Learning iteration 1514/4000 [0m

                       Computation: 3198 steps/s (collection: 0.479s, learning 2.083s)
               Value function loss: 67939.4258
                    Surrogate loss: 0.0164
             Mean action noise std: 0.91
                       Mean reward: 5991.95
               Mean episode length: 313.19
                 Mean success rate: 56.50
                  Mean reward/step: 21.18
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12410880
                    Iteration time: 2.56s
                        Total time: 3884.49s
                               ETA: 6374.1s

################################################################################
                     [1m Learning iteration 1515/4000 [0m

                       Computation: 3209 steps/s (collection: 0.495s, learning 2.058s)
               Value function loss: 82530.1341
                    Surrogate loss: 0.0180
             Mean action noise std: 0.91
                       Mean reward: 6229.75
               Mean episode length: 322.39
                 Mean success rate: 60.00
                  Mean reward/step: 21.20
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 2.55s
                        Total time: 3887.04s
                               ETA: 6371.6s

################################################################################
                     [1m Learning iteration 1516/4000 [0m

                       Computation: 3259 steps/s (collection: 0.457s, learning 2.056s)
               Value function loss: 89286.0302
                    Surrogate loss: 0.0164
             Mean action noise std: 0.91
                       Mean reward: 6497.08
               Mean episode length: 331.02
                 Mean success rate: 62.00
                  Mean reward/step: 21.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12427264
                    Iteration time: 2.51s
                        Total time: 3889.55s
                               ETA: 6368.9s

################################################################################
                     [1m Learning iteration 1517/4000 [0m

                       Computation: 3198 steps/s (collection: 0.490s, learning 2.071s)
               Value function loss: 49328.7770
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 6465.56
               Mean episode length: 327.83
                 Mean success rate: 61.50
                  Mean reward/step: 21.43
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 2.56s
                        Total time: 3892.11s
                               ETA: 6366.3s

################################################################################
                     [1m Learning iteration 1518/4000 [0m

                       Computation: 3293 steps/s (collection: 0.425s, learning 2.063s)
               Value function loss: 88452.3422
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 6742.80
               Mean episode length: 338.41
                 Mean success rate: 63.50
                  Mean reward/step: 22.03
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12443648
                    Iteration time: 2.49s
                        Total time: 3894.60s
                               ETA: 6363.7s

################################################################################
                     [1m Learning iteration 1519/4000 [0m

                       Computation: 3232 steps/s (collection: 0.477s, learning 2.058s)
               Value function loss: 100363.2516
                    Surrogate loss: 0.0170
             Mean action noise std: 0.91
                       Mean reward: 7010.71
               Mean episode length: 345.45
                 Mean success rate: 65.50
                  Mean reward/step: 22.02
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 2.53s
                        Total time: 3897.13s
                               ETA: 6361.0s

################################################################################
                     [1m Learning iteration 1520/4000 [0m

                       Computation: 3219 steps/s (collection: 0.482s, learning 2.063s)
               Value function loss: 106248.8492
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 7174.49
               Mean episode length: 350.58
                 Mean success rate: 66.50
                  Mean reward/step: 21.60
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12460032
                    Iteration time: 2.54s
                        Total time: 3899.68s
                               ETA: 6358.5s

################################################################################
                     [1m Learning iteration 1521/4000 [0m

                       Computation: 3206 steps/s (collection: 0.508s, learning 2.047s)
               Value function loss: 90907.4062
                    Surrogate loss: 0.0156
             Mean action noise std: 0.91
                       Mean reward: 7535.06
               Mean episode length: 364.60
                 Mean success rate: 70.00
                  Mean reward/step: 20.97
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 2.56s
                        Total time: 3902.23s
                               ETA: 6355.9s

################################################################################
                     [1m Learning iteration 1522/4000 [0m

                       Computation: 3184 steps/s (collection: 0.466s, learning 2.107s)
               Value function loss: 66379.3029
                    Surrogate loss: 0.0150
             Mean action noise std: 0.90
                       Mean reward: 7531.69
               Mean episode length: 362.61
                 Mean success rate: 69.50
                  Mean reward/step: 20.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12476416
                    Iteration time: 2.57s
                        Total time: 3904.81s
                               ETA: 6353.3s

################################################################################
                     [1m Learning iteration 1523/4000 [0m

                       Computation: 3249 steps/s (collection: 0.470s, learning 2.051s)
               Value function loss: 72464.6681
                    Surrogate loss: 0.0173
             Mean action noise std: 0.90
                       Mean reward: 7792.43
               Mean episode length: 371.40
                 Mean success rate: 71.50
                  Mean reward/step: 20.38
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 2.52s
                        Total time: 3907.33s
                               ETA: 6350.7s

################################################################################
                     [1m Learning iteration 1524/4000 [0m

                       Computation: 3292 steps/s (collection: 0.435s, learning 2.053s)
               Value function loss: 96970.3138
                    Surrogate loss: 0.0142
             Mean action noise std: 0.90
                       Mean reward: 7733.65
               Mean episode length: 368.15
                 Mean success rate: 70.00
                  Mean reward/step: 20.87
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 12492800
                    Iteration time: 2.49s
                        Total time: 3909.82s
                               ETA: 6348.0s

################################################################################
                     [1m Learning iteration 1525/4000 [0m

                       Computation: 3272 steps/s (collection: 0.436s, learning 2.068s)
               Value function loss: 71549.9254
                    Surrogate loss: 0.0160
             Mean action noise std: 0.90
                       Mean reward: 7720.85
               Mean episode length: 372.04
                 Mean success rate: 71.00
                  Mean reward/step: 20.12
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 2.50s
                        Total time: 3912.32s
                               ETA: 6345.3s

################################################################################
                     [1m Learning iteration 1526/4000 [0m

                       Computation: 3081 steps/s (collection: 0.545s, learning 2.113s)
               Value function loss: 98435.6915
                    Surrogate loss: 0.0208
             Mean action noise std: 0.90
                       Mean reward: 7898.27
               Mean episode length: 376.38
                 Mean success rate: 72.50
                  Mean reward/step: 19.61
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12509184
                    Iteration time: 2.66s
                        Total time: 3914.98s
                               ETA: 6342.9s

################################################################################
                     [1m Learning iteration 1527/4000 [0m

                       Computation: 3208 steps/s (collection: 0.504s, learning 2.049s)
               Value function loss: 49753.5889
                    Surrogate loss: 0.0180
             Mean action noise std: 0.90
                       Mean reward: 7659.00
               Mean episode length: 366.85
                 Mean success rate: 70.50
                  Mean reward/step: 20.35
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 2.55s
                        Total time: 3917.53s
                               ETA: 6340.3s

################################################################################
                     [1m Learning iteration 1528/4000 [0m

                       Computation: 3233 steps/s (collection: 0.493s, learning 2.041s)
               Value function loss: 81111.7669
                    Surrogate loss: 0.0149
             Mean action noise std: 0.90
                       Mean reward: 7536.11
               Mean episode length: 360.40
                 Mean success rate: 69.50
                  Mean reward/step: 20.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12525568
                    Iteration time: 2.53s
                        Total time: 3920.06s
                               ETA: 6337.7s

################################################################################
                     [1m Learning iteration 1529/4000 [0m

                       Computation: 3242 steps/s (collection: 0.479s, learning 2.047s)
               Value function loss: 87736.5741
                    Surrogate loss: 0.0190
             Mean action noise std: 0.90
                       Mean reward: 7232.55
               Mean episode length: 344.00
                 Mean success rate: 66.00
                  Mean reward/step: 20.09
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 2.53s
                        Total time: 3922.59s
                               ETA: 6335.1s

################################################################################
                     [1m Learning iteration 1530/4000 [0m

                       Computation: 3195 steps/s (collection: 0.522s, learning 2.042s)
               Value function loss: 92637.4117
                    Surrogate loss: 0.0191
             Mean action noise std: 0.90
                       Mean reward: 7288.05
               Mean episode length: 348.88
                 Mean success rate: 66.50
                  Mean reward/step: 19.46
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12541952
                    Iteration time: 2.56s
                        Total time: 3925.16s
                               ETA: 6332.5s

################################################################################
                     [1m Learning iteration 1531/4000 [0m

                       Computation: 3156 steps/s (collection: 0.507s, learning 2.088s)
               Value function loss: 65156.1209
                    Surrogate loss: 0.0184
             Mean action noise std: 0.90
                       Mean reward: 7047.95
               Mean episode length: 340.43
                 Mean success rate: 65.00
                  Mean reward/step: 19.62
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 2.60s
                        Total time: 3927.75s
                               ETA: 6330.0s

################################################################################
                     [1m Learning iteration 1532/4000 [0m

                       Computation: 3177 steps/s (collection: 0.495s, learning 2.083s)
               Value function loss: 86468.4524
                    Surrogate loss: 0.0175
             Mean action noise std: 0.90
                       Mean reward: 7207.74
               Mean episode length: 345.55
                 Mean success rate: 66.50
                  Mean reward/step: 20.87
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12558336
                    Iteration time: 2.58s
                        Total time: 3930.33s
                               ETA: 6327.5s

################################################################################
                     [1m Learning iteration 1533/4000 [0m

                       Computation: 3311 steps/s (collection: 0.456s, learning 2.018s)
               Value function loss: 72507.8905
                    Surrogate loss: 0.0169
             Mean action noise std: 0.90
                       Mean reward: 7185.38
               Mean episode length: 345.81
                 Mean success rate: 66.50
                  Mean reward/step: 21.34
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 2.47s
                        Total time: 3932.80s
                               ETA: 6324.8s

################################################################################
                     [1m Learning iteration 1534/4000 [0m

                       Computation: 3227 steps/s (collection: 0.489s, learning 2.050s)
               Value function loss: 61349.8053
                    Surrogate loss: 0.0202
             Mean action noise std: 0.90
                       Mean reward: 7018.69
               Mean episode length: 337.73
                 Mean success rate: 64.50
                  Mean reward/step: 21.08
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12574720
                    Iteration time: 2.54s
                        Total time: 3935.34s
                               ETA: 6322.2s

################################################################################
                     [1m Learning iteration 1535/4000 [0m

                       Computation: 3189 steps/s (collection: 0.525s, learning 2.044s)
               Value function loss: 91827.5368
                    Surrogate loss: 0.0209
             Mean action noise std: 0.90
                       Mean reward: 7056.93
               Mean episode length: 343.75
                 Mean success rate: 64.00
                  Mean reward/step: 21.07
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 2.57s
                        Total time: 3937.91s
                               ETA: 6319.6s

################################################################################
                     [1m Learning iteration 1536/4000 [0m

                       Computation: 3252 steps/s (collection: 0.447s, learning 2.072s)
               Value function loss: 112332.3484
                    Surrogate loss: 0.0184
             Mean action noise std: 0.91
                       Mean reward: 7049.31
               Mean episode length: 345.04
                 Mean success rate: 64.00
                  Mean reward/step: 19.97
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 12591104
                    Iteration time: 2.52s
                        Total time: 3940.43s
                               ETA: 6317.0s

################################################################################
                     [1m Learning iteration 1537/4000 [0m

                       Computation: 3215 steps/s (collection: 0.492s, learning 2.055s)
               Value function loss: 66511.8465
                    Surrogate loss: 0.0186
             Mean action noise std: 0.91
                       Mean reward: 7178.94
               Mean episode length: 350.60
                 Mean success rate: 65.00
                  Mean reward/step: 19.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 2.55s
                        Total time: 3942.98s
                               ETA: 6314.4s

################################################################################
                     [1m Learning iteration 1538/4000 [0m

                       Computation: 3223 steps/s (collection: 0.482s, learning 2.060s)
               Value function loss: 84998.3288
                    Surrogate loss: 0.0210
             Mean action noise std: 0.91
                       Mean reward: 7267.88
               Mean episode length: 352.67
                 Mean success rate: 66.50
                  Mean reward/step: 20.11
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12607488
                    Iteration time: 2.54s
                        Total time: 3945.52s
                               ETA: 6311.8s

################################################################################
                     [1m Learning iteration 1539/4000 [0m

                       Computation: 3170 steps/s (collection: 0.547s, learning 2.037s)
               Value function loss: 71149.8979
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 6655.27
               Mean episode length: 331.61
                 Mean success rate: 61.50
                  Mean reward/step: 20.19
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 2.58s
                        Total time: 3948.10s
                               ETA: 6309.3s

################################################################################
                     [1m Learning iteration 1540/4000 [0m

                       Computation: 3250 steps/s (collection: 0.457s, learning 2.064s)
               Value function loss: 91651.3953
                    Surrogate loss: 0.0168
             Mean action noise std: 0.91
                       Mean reward: 6759.30
               Mean episode length: 337.31
                 Mean success rate: 63.00
                  Mean reward/step: 20.10
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12623872
                    Iteration time: 2.52s
                        Total time: 3950.62s
                               ETA: 6306.6s

################################################################################
                     [1m Learning iteration 1541/4000 [0m

                       Computation: 3290 steps/s (collection: 0.435s, learning 2.054s)
               Value function loss: 60328.9663
                    Surrogate loss: 0.0166
             Mean action noise std: 0.91
                       Mean reward: 6629.16
               Mean episode length: 332.40
                 Mean success rate: 61.50
                  Mean reward/step: 19.92
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 2.49s
                        Total time: 3953.11s
                               ETA: 6304.0s

################################################################################
                     [1m Learning iteration 1542/4000 [0m

                       Computation: 3198 steps/s (collection: 0.505s, learning 2.056s)
               Value function loss: 99328.2572
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 6870.89
               Mean episode length: 335.69
                 Mean success rate: 63.00
                  Mean reward/step: 19.99
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12640256
                    Iteration time: 2.56s
                        Total time: 3955.67s
                               ETA: 6301.4s

################################################################################
                     [1m Learning iteration 1543/4000 [0m

                       Computation: 3228 steps/s (collection: 0.489s, learning 2.048s)
               Value function loss: 60838.6355
                    Surrogate loss: 0.0192
             Mean action noise std: 0.91
                       Mean reward: 6645.51
               Mean episode length: 327.80
                 Mean success rate: 61.50
                  Mean reward/step: 20.44
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 2.54s
                        Total time: 3958.21s
                               ETA: 6298.8s

################################################################################
                     [1m Learning iteration 1544/4000 [0m

                       Computation: 3153 steps/s (collection: 0.489s, learning 2.109s)
               Value function loss: 101633.8315
                    Surrogate loss: 0.0164
             Mean action noise std: 0.91
                       Mean reward: 6401.18
               Mean episode length: 316.81
                 Mean success rate: 59.00
                  Mean reward/step: 19.78
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 12656640
                    Iteration time: 2.60s
                        Total time: 3960.81s
                               ETA: 6296.3s

################################################################################
                     [1m Learning iteration 1545/4000 [0m

                       Computation: 3209 steps/s (collection: 0.463s, learning 2.089s)
               Value function loss: 86557.8965
                    Surrogate loss: 0.0176
             Mean action noise std: 0.91
                       Mean reward: 6634.62
               Mean episode length: 325.79
                 Mean success rate: 61.00
                  Mean reward/step: 20.07
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 2.55s
                        Total time: 3963.36s
                               ETA: 6293.7s

################################################################################
                     [1m Learning iteration 1546/4000 [0m

                       Computation: 3207 steps/s (collection: 0.479s, learning 2.075s)
               Value function loss: 77310.5511
                    Surrogate loss: 0.0201
             Mean action noise std: 0.91
                       Mean reward: 6550.96
               Mean episode length: 325.56
                 Mean success rate: 61.00
                  Mean reward/step: 20.07
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12673024
                    Iteration time: 2.55s
                        Total time: 3965.91s
                               ETA: 6291.1s

################################################################################
                     [1m Learning iteration 1547/4000 [0m

                       Computation: 3210 steps/s (collection: 0.421s, learning 2.131s)
               Value function loss: 77838.0850
                    Surrogate loss: 0.0172
             Mean action noise std: 0.91
                       Mean reward: 6969.61
               Mean episode length: 337.61
                 Mean success rate: 64.00
                  Mean reward/step: 19.57
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 2.55s
                        Total time: 3968.47s
                               ETA: 6288.5s

################################################################################
                     [1m Learning iteration 1548/4000 [0m

                       Computation: 3207 steps/s (collection: 0.458s, learning 2.096s)
               Value function loss: 76404.7492
                    Surrogate loss: 0.0233
             Mean action noise std: 0.91
                       Mean reward: 6997.70
               Mean episode length: 340.63
                 Mean success rate: 64.00
                  Mean reward/step: 20.09
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12689408
                    Iteration time: 2.55s
                        Total time: 3971.02s
                               ETA: 6286.0s

################################################################################
                     [1m Learning iteration 1549/4000 [0m

                       Computation: 3229 steps/s (collection: 0.466s, learning 2.071s)
               Value function loss: 93746.5617
                    Surrogate loss: 0.0199
             Mean action noise std: 0.91
                       Mean reward: 7129.40
               Mean episode length: 342.43
                 Mean success rate: 64.00
                  Mean reward/step: 20.34
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 2.54s
                        Total time: 3973.56s
                               ETA: 6283.3s

################################################################################
                     [1m Learning iteration 1550/4000 [0m

                       Computation: 3186 steps/s (collection: 0.461s, learning 2.109s)
               Value function loss: 61369.1250
                    Surrogate loss: 0.0192
             Mean action noise std: 0.91
                       Mean reward: 7101.34
               Mean episode length: 345.13
                 Mean success rate: 65.00
                  Mean reward/step: 21.07
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12705792
                    Iteration time: 2.57s
                        Total time: 3976.13s
                               ETA: 6280.8s

################################################################################
                     [1m Learning iteration 1551/4000 [0m

                       Computation: 3138 steps/s (collection: 0.470s, learning 2.140s)
               Value function loss: 70040.9437
                    Surrogate loss: 0.0167
             Mean action noise std: 0.91
                       Mean reward: 7133.85
               Mean episode length: 347.07
                 Mean success rate: 65.50
                  Mean reward/step: 21.89
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 2.61s
                        Total time: 3978.74s
                               ETA: 6278.3s

################################################################################
                     [1m Learning iteration 1552/4000 [0m

                       Computation: 3165 steps/s (collection: 0.516s, learning 2.072s)
               Value function loss: 85937.3773
                    Surrogate loss: 0.0160
             Mean action noise std: 0.91
                       Mean reward: 7264.28
               Mean episode length: 356.77
                 Mean success rate: 68.00
                  Mean reward/step: 22.34
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12722176
                    Iteration time: 2.59s
                        Total time: 3981.33s
                               ETA: 6275.8s

################################################################################
                     [1m Learning iteration 1553/4000 [0m

                       Computation: 3287 steps/s (collection: 0.421s, learning 2.071s)
               Value function loss: 69012.3788
                    Surrogate loss: 0.0163
             Mean action noise std: 0.91
                       Mean reward: 7198.54
               Mean episode length: 356.20
                 Mean success rate: 67.50
                  Mean reward/step: 21.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 2.49s
                        Total time: 3983.82s
                               ETA: 6273.1s

################################################################################
                     [1m Learning iteration 1554/4000 [0m

                       Computation: 3163 steps/s (collection: 0.503s, learning 2.087s)
               Value function loss: 65404.0044
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 6817.78
               Mean episode length: 340.31
                 Mean success rate: 63.50
                  Mean reward/step: 21.77
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12738560
                    Iteration time: 2.59s
                        Total time: 3986.41s
                               ETA: 6270.6s

################################################################################
                     [1m Learning iteration 1555/4000 [0m

                       Computation: 3224 steps/s (collection: 0.451s, learning 2.089s)
               Value function loss: 98997.4564
                    Surrogate loss: 0.0168
             Mean action noise std: 0.91
                       Mean reward: 7194.06
               Mean episode length: 352.74
                 Mean success rate: 66.50
                  Mean reward/step: 21.63
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 2.54s
                        Total time: 3988.95s
                               ETA: 6268.0s

################################################################################
                     [1m Learning iteration 1556/4000 [0m

                       Computation: 3182 steps/s (collection: 0.432s, learning 2.142s)
               Value function loss: 75641.6616
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 7160.76
               Mean episode length: 353.47
                 Mean success rate: 66.00
                  Mean reward/step: 21.41
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12754944
                    Iteration time: 2.57s
                        Total time: 3991.52s
                               ETA: 6265.4s

################################################################################
                     [1m Learning iteration 1557/4000 [0m

                       Computation: 3111 steps/s (collection: 0.497s, learning 2.135s)
               Value function loss: 91476.9687
                    Surrogate loss: 0.0200
             Mean action noise std: 0.90
                       Mean reward: 7076.58
               Mean episode length: 349.03
                 Mean success rate: 65.50
                  Mean reward/step: 21.10
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 2.63s
                        Total time: 3994.15s
                               ETA: 6263.0s

################################################################################
                     [1m Learning iteration 1558/4000 [0m

                       Computation: 3202 steps/s (collection: 0.454s, learning 2.104s)
               Value function loss: 94347.6355
                    Surrogate loss: 0.0164
             Mean action noise std: 0.90
                       Mean reward: 7283.08
               Mean episode length: 355.66
                 Mean success rate: 67.50
                  Mean reward/step: 20.29
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12771328
                    Iteration time: 2.56s
                        Total time: 3996.71s
                               ETA: 6260.4s

################################################################################
                     [1m Learning iteration 1559/4000 [0m

                       Computation: 3173 steps/s (collection: 0.460s, learning 2.121s)
               Value function loss: 88775.4688
                    Surrogate loss: 0.0156
             Mean action noise std: 0.90
                       Mean reward: 7425.20
               Mean episode length: 361.91
                 Mean success rate: 69.50
                  Mean reward/step: 20.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 2.58s
                        Total time: 3999.29s
                               ETA: 6257.9s

################################################################################
                     [1m Learning iteration 1560/4000 [0m

                       Computation: 3098 steps/s (collection: 0.497s, learning 2.146s)
               Value function loss: 100341.2141
                    Surrogate loss: 0.0192
             Mean action noise std: 0.90
                       Mean reward: 7483.73
               Mean episode length: 363.16
                 Mean success rate: 69.00
                  Mean reward/step: 20.35
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12787712
                    Iteration time: 2.64s
                        Total time: 4001.94s
                               ETA: 6255.4s

################################################################################
                     [1m Learning iteration 1561/4000 [0m

                       Computation: 3093 steps/s (collection: 0.500s, learning 2.148s)
               Value function loss: 80618.3203
                    Surrogate loss: 0.0170
             Mean action noise std: 0.90
                       Mean reward: 7813.74
               Mean episode length: 365.78
                 Mean success rate: 70.00
                  Mean reward/step: 19.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 2.65s
                        Total time: 4004.59s
                               ETA: 6253.0s

################################################################################
                     [1m Learning iteration 1562/4000 [0m

                       Computation: 3141 steps/s (collection: 0.494s, learning 2.114s)
               Value function loss: 87159.6199
                    Surrogate loss: 0.0168
             Mean action noise std: 0.90
                       Mean reward: 8005.50
               Mean episode length: 374.28
                 Mean success rate: 71.50
                  Mean reward/step: 19.63
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12804096
                    Iteration time: 2.61s
                        Total time: 4007.19s
                               ETA: 6250.5s

################################################################################
                     [1m Learning iteration 1563/4000 [0m

                       Computation: 3172 steps/s (collection: 0.465s, learning 2.117s)
               Value function loss: 89390.8808
                    Surrogate loss: 0.0159
             Mean action noise std: 0.90
                       Mean reward: 8364.14
               Mean episode length: 389.80
                 Mean success rate: 75.00
                  Mean reward/step: 19.85
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 2.58s
                        Total time: 4009.78s
                               ETA: 6248.0s

################################################################################
                     [1m Learning iteration 1564/4000 [0m

                       Computation: 3184 steps/s (collection: 0.497s, learning 2.076s)
               Value function loss: 56002.5469
                    Surrogate loss: 0.0138
             Mean action noise std: 0.90
                       Mean reward: 7810.42
               Mean episode length: 370.28
                 Mean success rate: 70.50
                  Mean reward/step: 20.47
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12820480
                    Iteration time: 2.57s
                        Total time: 4012.35s
                               ETA: 6245.4s

################################################################################
                     [1m Learning iteration 1565/4000 [0m

                       Computation: 3100 steps/s (collection: 0.519s, learning 2.124s)
               Value function loss: 86035.0034
                    Surrogate loss: 0.0146
             Mean action noise std: 0.90
                       Mean reward: 8007.30
               Mean episode length: 373.61
                 Mean success rate: 73.00
                  Mean reward/step: 20.59
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 2.64s
                        Total time: 4014.99s
                               ETA: 6243.0s

################################################################################
                     [1m Learning iteration 1566/4000 [0m

                       Computation: 3189 steps/s (collection: 0.470s, learning 2.099s)
               Value function loss: 72963.6094
                    Surrogate loss: 0.0152
             Mean action noise std: 0.90
                       Mean reward: 7893.93
               Mean episode length: 374.33
                 Mean success rate: 72.50
                  Mean reward/step: 20.59
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12836864
                    Iteration time: 2.57s
                        Total time: 4017.56s
                               ETA: 6240.4s

################################################################################
                     [1m Learning iteration 1567/4000 [0m

                       Computation: 3233 steps/s (collection: 0.451s, learning 2.083s)
               Value function loss: 73559.5450
                    Surrogate loss: 0.0171
             Mean action noise std: 0.90
                       Mean reward: 7628.16
               Mean episode length: 367.02
                 Mean success rate: 71.50
                  Mean reward/step: 20.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 2.53s
                        Total time: 4020.09s
                               ETA: 6237.8s

################################################################################
                     [1m Learning iteration 1568/4000 [0m

                       Computation: 3245 steps/s (collection: 0.468s, learning 2.056s)
               Value function loss: 97522.6210
                    Surrogate loss: 0.0128
             Mean action noise std: 0.90
                       Mean reward: 7593.67
               Mean episode length: 360.63
                 Mean success rate: 69.50
                  Mean reward/step: 18.81
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 12853248
                    Iteration time: 2.52s
                        Total time: 4022.62s
                               ETA: 6235.2s

################################################################################
                     [1m Learning iteration 1569/4000 [0m

                       Computation: 3230 steps/s (collection: 0.470s, learning 2.065s)
               Value function loss: 58808.0296
                    Surrogate loss: 0.0142
             Mean action noise std: 0.90
                       Mean reward: 7526.92
               Mean episode length: 359.99
                 Mean success rate: 70.50
                  Mean reward/step: 18.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 2.54s
                        Total time: 4025.15s
                               ETA: 6232.6s

################################################################################
                     [1m Learning iteration 1570/4000 [0m

                       Computation: 3234 steps/s (collection: 0.471s, learning 2.061s)
               Value function loss: 96488.8592
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 7063.86
               Mean episode length: 348.86
                 Mean success rate: 68.00
                  Mean reward/step: 19.28
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12869632
                    Iteration time: 2.53s
                        Total time: 4027.68s
                               ETA: 6230.0s

################################################################################
                     [1m Learning iteration 1571/4000 [0m

                       Computation: 3227 steps/s (collection: 0.468s, learning 2.070s)
               Value function loss: 71989.3735
                    Surrogate loss: 0.0129
             Mean action noise std: 0.90
                       Mean reward: 7125.33
               Mean episode length: 357.06
                 Mean success rate: 70.00
                  Mean reward/step: 19.51
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 2.54s
                        Total time: 4030.22s
                               ETA: 6227.4s

################################################################################
                     [1m Learning iteration 1572/4000 [0m

                       Computation: 3282 steps/s (collection: 0.451s, learning 2.044s)
               Value function loss: 65363.5405
                    Surrogate loss: 0.0149
             Mean action noise std: 0.90
                       Mean reward: 7050.00
               Mean episode length: 352.69
                 Mean success rate: 69.50
                  Mean reward/step: 19.78
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12886016
                    Iteration time: 2.50s
                        Total time: 4032.72s
                               ETA: 6224.7s

################################################################################
                     [1m Learning iteration 1573/4000 [0m

                       Computation: 3219 steps/s (collection: 0.454s, learning 2.090s)
               Value function loss: 68646.8807
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 7240.02
               Mean episode length: 365.15
                 Mean success rate: 72.50
                  Mean reward/step: 19.92
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 2.54s
                        Total time: 4035.26s
                               ETA: 6222.1s

################################################################################
                     [1m Learning iteration 1574/4000 [0m

                       Computation: 3275 steps/s (collection: 0.431s, learning 2.071s)
               Value function loss: 46081.6600
                    Surrogate loss: 0.0143
             Mean action noise std: 0.90
                       Mean reward: 7038.91
               Mean episode length: 360.56
                 Mean success rate: 70.00
                  Mean reward/step: 20.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12902400
                    Iteration time: 2.50s
                        Total time: 4037.76s
                               ETA: 6219.4s

################################################################################
                     [1m Learning iteration 1575/4000 [0m

                       Computation: 3188 steps/s (collection: 0.492s, learning 2.077s)
               Value function loss: 95713.3078
                    Surrogate loss: 0.0138
             Mean action noise std: 0.90
                       Mean reward: 7204.22
               Mean episode length: 363.73
                 Mean success rate: 71.50
                  Mean reward/step: 20.97
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 2.57s
                        Total time: 4040.33s
                               ETA: 6216.9s

################################################################################
                     [1m Learning iteration 1576/4000 [0m

                       Computation: 3200 steps/s (collection: 0.466s, learning 2.093s)
               Value function loss: 71497.3481
                    Surrogate loss: 0.0123
             Mean action noise std: 0.90
                       Mean reward: 7002.51
               Mean episode length: 360.85
                 Mean success rate: 70.00
                  Mean reward/step: 20.94
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12918784
                    Iteration time: 2.56s
                        Total time: 4042.89s
                               ETA: 6214.3s

################################################################################
                     [1m Learning iteration 1577/4000 [0m

                       Computation: 3314 steps/s (collection: 0.429s, learning 2.042s)
               Value function loss: 84517.9956
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 6952.32
               Mean episode length: 358.75
                 Mean success rate: 69.50
                  Mean reward/step: 20.98
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 2.47s
                        Total time: 4045.36s
                               ETA: 6211.6s

################################################################################
                     [1m Learning iteration 1578/4000 [0m

                       Computation: 3268 steps/s (collection: 0.445s, learning 2.061s)
               Value function loss: 81743.4535
                    Surrogate loss: 0.0128
             Mean action noise std: 0.90
                       Mean reward: 7135.38
               Mean episode length: 365.30
                 Mean success rate: 70.50
                  Mean reward/step: 20.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12935168
                    Iteration time: 2.51s
                        Total time: 4047.87s
                               ETA: 6209.0s

################################################################################
                     [1m Learning iteration 1579/4000 [0m

                       Computation: 3177 steps/s (collection: 0.459s, learning 2.119s)
               Value function loss: 84375.3961
                    Surrogate loss: 0.0130
             Mean action noise std: 0.90
                       Mean reward: 7243.85
               Mean episode length: 367.37
                 Mean success rate: 71.50
                  Mean reward/step: 20.04
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 2.58s
                        Total time: 4050.45s
                               ETA: 6206.4s

################################################################################
                     [1m Learning iteration 1580/4000 [0m

                       Computation: 3193 steps/s (collection: 0.471s, learning 2.094s)
               Value function loss: 99681.3244
                    Surrogate loss: 0.0152
             Mean action noise std: 0.90
                       Mean reward: 6895.77
               Mean episode length: 349.69
                 Mean success rate: 67.00
                  Mean reward/step: 20.05
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 12951552
                    Iteration time: 2.56s
                        Total time: 4053.01s
                               ETA: 6203.9s

################################################################################
                     [1m Learning iteration 1581/4000 [0m

                       Computation: 3137 steps/s (collection: 0.484s, learning 2.127s)
               Value function loss: 69605.8637
                    Surrogate loss: 0.0153
             Mean action noise std: 0.90
                       Mean reward: 6832.75
               Mean episode length: 346.37
                 Mean success rate: 66.50
                  Mean reward/step: 19.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 2.61s
                        Total time: 4055.62s
                               ETA: 6201.4s

################################################################################
                     [1m Learning iteration 1582/4000 [0m

                       Computation: 3115 steps/s (collection: 0.499s, learning 2.131s)
               Value function loss: 80779.8693
                    Surrogate loss: 0.0117
             Mean action noise std: 0.90
                       Mean reward: 6923.24
               Mean episode length: 348.35
                 Mean success rate: 68.00
                  Mean reward/step: 19.10
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12967936
                    Iteration time: 2.63s
                        Total time: 4058.25s
                               ETA: 6198.9s

################################################################################
                     [1m Learning iteration 1583/4000 [0m

                       Computation: 3072 steps/s (collection: 0.490s, learning 2.177s)
               Value function loss: 84966.3053
                    Surrogate loss: 0.0140
             Mean action noise std: 0.90
                       Mean reward: 6881.58
               Mean episode length: 346.00
                 Mean success rate: 67.00
                  Mean reward/step: 19.21
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 2.67s
                        Total time: 4060.92s
                               ETA: 6196.5s

################################################################################
                     [1m Learning iteration 1584/4000 [0m

                       Computation: 3209 steps/s (collection: 0.459s, learning 2.093s)
               Value function loss: 67327.7092
                    Surrogate loss: 0.0152
             Mean action noise std: 0.90
                       Mean reward: 6970.30
               Mean episode length: 343.06
                 Mean success rate: 67.00
                  Mean reward/step: 19.29
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12984320
                    Iteration time: 2.55s
                        Total time: 4063.47s
                               ETA: 6193.9s

################################################################################
                     [1m Learning iteration 1585/4000 [0m

                       Computation: 3164 steps/s (collection: 0.471s, learning 2.118s)
               Value function loss: 83621.6029
                    Surrogate loss: 0.0120
             Mean action noise std: 0.90
                       Mean reward: 6896.34
               Mean episode length: 339.04
                 Mean success rate: 66.00
                  Mean reward/step: 19.24
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 2.59s
                        Total time: 4066.06s
                               ETA: 6191.4s

################################################################################
                     [1m Learning iteration 1586/4000 [0m

                       Computation: 3138 steps/s (collection: 0.487s, learning 2.123s)
               Value function loss: 62417.0475
                    Surrogate loss: 0.0151
             Mean action noise std: 0.90
                       Mean reward: 6845.77
               Mean episode length: 337.89
                 Mean success rate: 66.00
                  Mean reward/step: 18.57
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13000704
                    Iteration time: 2.61s
                        Total time: 4068.67s
                               ETA: 6188.9s

################################################################################
                     [1m Learning iteration 1587/4000 [0m

                       Computation: 3114 steps/s (collection: 0.513s, learning 2.118s)
               Value function loss: 64488.8860
                    Surrogate loss: 0.0133
             Mean action noise std: 0.90
                       Mean reward: 6597.18
               Mean episode length: 326.31
                 Mean success rate: 63.50
                  Mean reward/step: 18.41
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 2.63s
                        Total time: 4071.30s
                               ETA: 6186.4s

################################################################################
                     [1m Learning iteration 1588/4000 [0m

                       Computation: 3112 steps/s (collection: 0.528s, learning 2.104s)
               Value function loss: 69914.0907
                    Surrogate loss: 0.0164
             Mean action noise std: 0.90
                       Mean reward: 6573.42
               Mean episode length: 327.28
                 Mean success rate: 63.50
                  Mean reward/step: 18.65
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13017088
                    Iteration time: 2.63s
                        Total time: 4073.93s
                               ETA: 6184.0s

################################################################################
                     [1m Learning iteration 1589/4000 [0m

                       Computation: 3125 steps/s (collection: 0.496s, learning 2.125s)
               Value function loss: 63511.4209
                    Surrogate loss: 0.0154
             Mean action noise std: 0.90
                       Mean reward: 6514.40
               Mean episode length: 329.95
                 Mean success rate: 63.00
                  Mean reward/step: 18.16
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 2.62s
                        Total time: 4076.55s
                               ETA: 6181.5s

################################################################################
                     [1m Learning iteration 1590/4000 [0m

                       Computation: 3188 steps/s (collection: 0.453s, learning 2.117s)
               Value function loss: 75846.8750
                    Surrogate loss: 0.0128
             Mean action noise std: 0.90
                       Mean reward: 6388.59
               Mean episode length: 321.18
                 Mean success rate: 61.00
                  Mean reward/step: 18.26
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13033472
                    Iteration time: 2.57s
                        Total time: 4079.12s
                               ETA: 6178.9s

################################################################################
                     [1m Learning iteration 1591/4000 [0m

                       Computation: 3143 steps/s (collection: 0.487s, learning 2.119s)
               Value function loss: 87071.3229
                    Surrogate loss: 0.0153
             Mean action noise std: 0.90
                       Mean reward: 5849.37
               Mean episode length: 303.56
                 Mean success rate: 56.50
                  Mean reward/step: 18.99
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 2.61s
                        Total time: 4081.73s
                               ETA: 6176.4s

################################################################################
                     [1m Learning iteration 1592/4000 [0m

                       Computation: 3193 steps/s (collection: 0.458s, learning 2.107s)
               Value function loss: 89627.2130
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 6001.51
               Mean episode length: 312.47
                 Mean success rate: 58.00
                  Mean reward/step: 18.63
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13049856
                    Iteration time: 2.56s
                        Total time: 4084.29s
                               ETA: 6173.9s

################################################################################
                     [1m Learning iteration 1593/4000 [0m

                       Computation: 3133 steps/s (collection: 0.512s, learning 2.102s)
               Value function loss: 87589.0741
                    Surrogate loss: 0.0145
             Mean action noise std: 0.90
                       Mean reward: 5893.13
               Mean episode length: 309.02
                 Mean success rate: 57.00
                  Mean reward/step: 18.28
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 2.61s
                        Total time: 4086.91s
                               ETA: 6171.4s

################################################################################
                     [1m Learning iteration 1594/4000 [0m

                       Computation: 3194 steps/s (collection: 0.482s, learning 2.082s)
               Value function loss: 66938.8441
                    Surrogate loss: 0.0146
             Mean action noise std: 0.90
                       Mean reward: 5723.59
               Mean episode length: 309.60
                 Mean success rate: 56.50
                  Mean reward/step: 19.07
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13066240
                    Iteration time: 2.56s
                        Total time: 4089.47s
                               ETA: 6168.8s

################################################################################
                     [1m Learning iteration 1595/4000 [0m

                       Computation: 3110 steps/s (collection: 0.504s, learning 2.130s)
               Value function loss: 67214.2773
                    Surrogate loss: 0.0171
             Mean action noise std: 0.90
                       Mean reward: 5893.19
               Mean episode length: 315.36
                 Mean success rate: 58.00
                  Mean reward/step: 19.07
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 2.63s
                        Total time: 4092.11s
                               ETA: 6166.4s

################################################################################
                     [1m Learning iteration 1596/4000 [0m

                       Computation: 3089 steps/s (collection: 0.567s, learning 2.084s)
               Value function loss: 92452.0026
                    Surrogate loss: 0.0155
             Mean action noise std: 0.90
                       Mean reward: 5850.03
               Mean episode length: 307.30
                 Mean success rate: 58.00
                  Mean reward/step: 18.28
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 13082624
                    Iteration time: 2.65s
                        Total time: 4094.76s
                               ETA: 6163.9s

################################################################################
                     [1m Learning iteration 1597/4000 [0m

                       Computation: 3195 steps/s (collection: 0.483s, learning 2.081s)
               Value function loss: 47384.9402
                    Surrogate loss: 0.0146
             Mean action noise std: 0.90
                       Mean reward: 5923.27
               Mean episode length: 312.88
                 Mean success rate: 59.50
                  Mean reward/step: 19.38
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 2.56s
                        Total time: 4097.32s
                               ETA: 6161.4s

################################################################################
                     [1m Learning iteration 1598/4000 [0m

                       Computation: 3183 steps/s (collection: 0.466s, learning 2.107s)
               Value function loss: 93812.7543
                    Surrogate loss: 0.0157
             Mean action noise std: 0.90
                       Mean reward: 6279.40
               Mean episode length: 322.70
                 Mean success rate: 61.50
                  Mean reward/step: 20.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13099008
                    Iteration time: 2.57s
                        Total time: 4099.89s
                               ETA: 6158.8s

################################################################################
                     [1m Learning iteration 1599/4000 [0m

                       Computation: 3207 steps/s (collection: 0.473s, learning 2.081s)
               Value function loss: 95218.2854
                    Surrogate loss: 0.0164
             Mean action noise std: 0.90
                       Mean reward: 6506.77
               Mean episode length: 333.60
                 Mean success rate: 64.00
                  Mean reward/step: 20.26
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 2.55s
                        Total time: 4102.45s
                               ETA: 6156.2s

################################################################################
                     [1m Learning iteration 1600/4000 [0m

                       Computation: 3079 steps/s (collection: 0.580s, learning 2.080s)
               Value function loss: 65544.7252
                    Surrogate loss: 0.0174
             Mean action noise std: 0.90
                       Mean reward: 5873.33
               Mean episode length: 312.88
                 Mean success rate: 60.00
                  Mean reward/step: 20.04
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13115392
                    Iteration time: 2.66s
                        Total time: 4105.11s
                               ETA: 6153.8s

################################################################################
                     [1m Learning iteration 1601/4000 [0m

                       Computation: 3198 steps/s (collection: 0.464s, learning 2.097s)
               Value function loss: 80140.9493
                    Surrogate loss: 0.0153
             Mean action noise std: 0.90
                       Mean reward: 6161.65
               Mean episode length: 326.88
                 Mean success rate: 64.00
                  Mean reward/step: 20.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 2.56s
                        Total time: 4107.67s
                               ETA: 6151.2s

################################################################################
                     [1m Learning iteration 1602/4000 [0m

                       Computation: 3209 steps/s (collection: 0.474s, learning 2.078s)
               Value function loss: 76488.8272
                    Surrogate loss: 0.0132
             Mean action noise std: 0.90
                       Mean reward: 6132.68
               Mean episode length: 319.18
                 Mean success rate: 62.50
                  Mean reward/step: 21.47
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13131776
                    Iteration time: 2.55s
                        Total time: 4110.22s
                               ETA: 6148.7s

################################################################################
                     [1m Learning iteration 1603/4000 [0m

                       Computation: 3206 steps/s (collection: 0.478s, learning 2.077s)
               Value function loss: 70932.0222
                    Surrogate loss: 0.0179
             Mean action noise std: 0.90
                       Mean reward: 6178.58
               Mean episode length: 320.51
                 Mean success rate: 62.50
                  Mean reward/step: 21.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 2.55s
                        Total time: 4112.78s
                               ETA: 6146.1s

################################################################################
                     [1m Learning iteration 1604/4000 [0m

                       Computation: 3166 steps/s (collection: 0.508s, learning 2.080s)
               Value function loss: 106776.6281
                    Surrogate loss: 0.0169
             Mean action noise std: 0.90
                       Mean reward: 6376.14
               Mean episode length: 327.75
                 Mean success rate: 64.00
                  Mean reward/step: 21.95
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13148160
                    Iteration time: 2.59s
                        Total time: 4115.36s
                               ETA: 6143.6s

################################################################################
                     [1m Learning iteration 1605/4000 [0m

                       Computation: 3137 steps/s (collection: 0.483s, learning 2.128s)
               Value function loss: 66601.1703
                    Surrogate loss: 0.0163
             Mean action noise std: 0.90
                       Mean reward: 6475.35
               Mean episode length: 330.90
                 Mean success rate: 64.50
                  Mean reward/step: 21.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 2.61s
                        Total time: 4117.97s
                               ETA: 6141.1s

################################################################################
                     [1m Learning iteration 1606/4000 [0m

                       Computation: 3219 steps/s (collection: 0.445s, learning 2.100s)
               Value function loss: 78361.9955
                    Surrogate loss: 0.0182
             Mean action noise std: 0.90
                       Mean reward: 6202.79
               Mean episode length: 324.45
                 Mean success rate: 64.00
                  Mean reward/step: 21.90
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13164544
                    Iteration time: 2.54s
                        Total time: 4120.52s
                               ETA: 6138.5s

################################################################################
                     [1m Learning iteration 1607/4000 [0m

                       Computation: 3124 steps/s (collection: 0.463s, learning 2.159s)
               Value function loss: 94999.5942
                    Surrogate loss: 0.0144
             Mean action noise std: 0.90
                       Mean reward: 6373.20
               Mean episode length: 326.18
                 Mean success rate: 65.00
                  Mean reward/step: 20.76
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 2.62s
                        Total time: 4123.14s
                               ETA: 6136.0s

################################################################################
                     [1m Learning iteration 1608/4000 [0m

                       Computation: 3132 steps/s (collection: 0.511s, learning 2.104s)
               Value function loss: 54168.5544
                    Surrogate loss: 0.0166
             Mean action noise std: 0.90
                       Mean reward: 6404.34
               Mean episode length: 324.09
                 Mean success rate: 65.00
                  Mean reward/step: 20.99
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13180928
                    Iteration time: 2.62s
                        Total time: 4125.76s
                               ETA: 6133.5s

################################################################################
                     [1m Learning iteration 1609/4000 [0m

                       Computation: 3153 steps/s (collection: 0.499s, learning 2.098s)
               Value function loss: 102638.3674
                    Surrogate loss: 0.0158
             Mean action noise std: 0.90
                       Mean reward: 6819.17
               Mean episode length: 336.69
                 Mean success rate: 67.00
                  Mean reward/step: 20.68
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 2.60s
                        Total time: 4128.35s
                               ETA: 6131.0s

################################################################################
                     [1m Learning iteration 1610/4000 [0m

                       Computation: 3149 steps/s (collection: 0.473s, learning 2.128s)
               Value function loss: 67947.5886
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 6855.32
               Mean episode length: 340.26
                 Mean success rate: 68.00
                  Mean reward/step: 20.34
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13197312
                    Iteration time: 2.60s
                        Total time: 4130.95s
                               ETA: 6128.5s

################################################################################
                     [1m Learning iteration 1611/4000 [0m

                       Computation: 3148 steps/s (collection: 0.498s, learning 2.103s)
               Value function loss: 94530.0240
                    Surrogate loss: 0.0144
             Mean action noise std: 0.90
                       Mean reward: 6780.75
               Mean episode length: 335.96
                 Mean success rate: 67.00
                  Mean reward/step: 20.86
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 2.60s
                        Total time: 4133.56s
                               ETA: 6126.0s

################################################################################
                     [1m Learning iteration 1612/4000 [0m

                       Computation: 3139 steps/s (collection: 0.510s, learning 2.099s)
               Value function loss: 72344.1822
                    Surrogate loss: 0.0156
             Mean action noise std: 0.90
                       Mean reward: 7072.25
               Mean episode length: 346.77
                 Mean success rate: 68.50
                  Mean reward/step: 20.08
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13213696
                    Iteration time: 2.61s
                        Total time: 4136.17s
                               ETA: 6123.5s

################################################################################
                     [1m Learning iteration 1613/4000 [0m

                       Computation: 3137 steps/s (collection: 0.480s, learning 2.131s)
               Value function loss: 89233.9098
                    Surrogate loss: 0.0185
             Mean action noise std: 0.90
                       Mean reward: 6959.61
               Mean episode length: 337.41
                 Mean success rate: 66.50
                  Mean reward/step: 20.58
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 2.61s
                        Total time: 4138.78s
                               ETA: 6121.0s

################################################################################
                     [1m Learning iteration 1614/4000 [0m

                       Computation: 3208 steps/s (collection: 0.459s, learning 2.095s)
               Value function loss: 87463.8260
                    Surrogate loss: 0.0185
             Mean action noise std: 0.90
                       Mean reward: 7071.85
               Mean episode length: 337.23
                 Mean success rate: 65.50
                  Mean reward/step: 21.02
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13230080
                    Iteration time: 2.55s
                        Total time: 4141.33s
                               ETA: 6118.4s

################################################################################
                     [1m Learning iteration 1615/4000 [0m

                       Computation: 3220 steps/s (collection: 0.485s, learning 2.059s)
               Value function loss: 72903.0838
                    Surrogate loss: 0.0175
             Mean action noise std: 0.90
                       Mean reward: 6763.66
               Mean episode length: 325.37
                 Mean success rate: 63.00
                  Mean reward/step: 20.92
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 2.54s
                        Total time: 4143.87s
                               ETA: 6115.8s

################################################################################
                     [1m Learning iteration 1616/4000 [0m

                       Computation: 3206 steps/s (collection: 0.477s, learning 2.077s)
               Value function loss: 83398.6534
                    Surrogate loss: 0.0179
             Mean action noise std: 0.90
                       Mean reward: 6583.96
               Mean episode length: 314.32
                 Mean success rate: 60.50
                  Mean reward/step: 20.45
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13246464
                    Iteration time: 2.55s
                        Total time: 4146.43s
                               ETA: 6113.2s

################################################################################
                     [1m Learning iteration 1617/4000 [0m

                       Computation: 3122 steps/s (collection: 0.481s, learning 2.143s)
               Value function loss: 97977.8479
                    Surrogate loss: 0.0164
             Mean action noise std: 0.91
                       Mean reward: 7102.61
               Mean episode length: 331.76
                 Mean success rate: 63.00
                  Mean reward/step: 20.52
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 2.62s
                        Total time: 4149.05s
                               ETA: 6110.7s

################################################################################
                     [1m Learning iteration 1618/4000 [0m

                       Computation: 3171 steps/s (collection: 0.476s, learning 2.107s)
               Value function loss: 42713.8282
                    Surrogate loss: 0.0221
             Mean action noise std: 0.90
                       Mean reward: 6929.36
               Mean episode length: 323.76
                 Mean success rate: 61.00
                  Mean reward/step: 20.50
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13262848
                    Iteration time: 2.58s
                        Total time: 4151.64s
                               ETA: 6108.2s

################################################################################
                     [1m Learning iteration 1619/4000 [0m

                       Computation: 3242 steps/s (collection: 0.451s, learning 2.076s)
               Value function loss: 98081.1027
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 7677.31
               Mean episode length: 348.11
                 Mean success rate: 66.50
                  Mean reward/step: 21.32
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 2.53s
                        Total time: 4154.16s
                               ETA: 6105.6s

################################################################################
                     [1m Learning iteration 1620/4000 [0m

                       Computation: 3133 steps/s (collection: 0.480s, learning 2.134s)
               Value function loss: 78555.9517
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 7281.79
               Mean episode length: 336.67
                 Mean success rate: 63.50
                  Mean reward/step: 21.83
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13279232
                    Iteration time: 2.61s
                        Total time: 4156.78s
                               ETA: 6103.1s

################################################################################
                     [1m Learning iteration 1621/4000 [0m

                       Computation: 3176 steps/s (collection: 0.502s, learning 2.076s)
               Value function loss: 60934.3829
                    Surrogate loss: 0.0151
             Mean action noise std: 0.90
                       Mean reward: 7078.01
               Mean episode length: 334.29
                 Mean success rate: 62.00
                  Mean reward/step: 22.05
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 2.58s
                        Total time: 4159.36s
                               ETA: 6100.6s

################################################################################
                     [1m Learning iteration 1622/4000 [0m

                       Computation: 3168 steps/s (collection: 0.496s, learning 2.089s)
               Value function loss: 81133.7174
                    Surrogate loss: 0.0146
             Mean action noise std: 0.90
                       Mean reward: 6898.50
               Mean episode length: 330.09
                 Mean success rate: 61.00
                  Mean reward/step: 22.49
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13295616
                    Iteration time: 2.59s
                        Total time: 4161.94s
                               ETA: 6098.0s

################################################################################
                     [1m Learning iteration 1623/4000 [0m

                       Computation: 3215 steps/s (collection: 0.446s, learning 2.101s)
               Value function loss: 89206.2259
                    Surrogate loss: 0.0138
             Mean action noise std: 0.90
                       Mean reward: 7210.40
               Mean episode length: 341.52
                 Mean success rate: 63.00
                  Mean reward/step: 21.66
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 2.55s
                        Total time: 4164.49s
                               ETA: 6095.4s

################################################################################
                     [1m Learning iteration 1624/4000 [0m

                       Computation: 3178 steps/s (collection: 0.462s, learning 2.115s)
               Value function loss: 95041.0070
                    Surrogate loss: 0.0145
             Mean action noise std: 0.90
                       Mean reward: 7424.54
               Mean episode length: 351.27
                 Mean success rate: 66.00
                  Mean reward/step: 21.08
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13312000
                    Iteration time: 2.58s
                        Total time: 4167.07s
                               ETA: 6092.9s

################################################################################
                     [1m Learning iteration 1625/4000 [0m

                       Computation: 3140 steps/s (collection: 0.518s, learning 2.090s)
               Value function loss: 95904.4889
                    Surrogate loss: 0.0128
             Mean action noise std: 0.90
                       Mean reward: 7521.47
               Mean episode length: 359.29
                 Mean success rate: 67.50
                  Mean reward/step: 20.62
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 2.61s
                        Total time: 4169.67s
                               ETA: 6090.4s

################################################################################
                     [1m Learning iteration 1626/4000 [0m

                       Computation: 3224 steps/s (collection: 0.483s, learning 2.058s)
               Value function loss: 92671.3271
                    Surrogate loss: 0.0149
             Mean action noise std: 0.90
                       Mean reward: 7085.38
               Mean episode length: 346.39
                 Mean success rate: 64.50
                  Mean reward/step: 20.76
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13328384
                    Iteration time: 2.54s
                        Total time: 4172.21s
                               ETA: 6087.8s

################################################################################
                     [1m Learning iteration 1627/4000 [0m

                       Computation: 3155 steps/s (collection: 0.492s, learning 2.104s)
               Value function loss: 119946.0744
                    Surrogate loss: 0.0147
             Mean action noise std: 0.90
                       Mean reward: 7206.32
               Mean episode length: 350.31
                 Mean success rate: 66.50
                  Mean reward/step: 20.44
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 2.60s
                        Total time: 4174.81s
                               ETA: 6085.3s

################################################################################
                     [1m Learning iteration 1628/4000 [0m

                       Computation: 3190 steps/s (collection: 0.498s, learning 2.070s)
               Value function loss: 55816.1850
                    Surrogate loss: 0.0146
             Mean action noise std: 0.90
                       Mean reward: 6984.05
               Mean episode length: 338.56
                 Mean success rate: 64.50
                  Mean reward/step: 19.90
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13344768
                    Iteration time: 2.57s
                        Total time: 4177.38s
                               ETA: 6082.7s

################################################################################
                     [1m Learning iteration 1629/4000 [0m

                       Computation: 3204 steps/s (collection: 0.505s, learning 2.052s)
               Value function loss: 91306.9630
                    Surrogate loss: 0.0184
             Mean action noise std: 0.90
                       Mean reward: 7263.55
               Mean episode length: 344.20
                 Mean success rate: 66.50
                  Mean reward/step: 20.72
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 2.56s
                        Total time: 4179.93s
                               ETA: 6080.1s

################################################################################
                     [1m Learning iteration 1630/4000 [0m

                       Computation: 3136 steps/s (collection: 0.506s, learning 2.105s)
               Value function loss: 67149.3091
                    Surrogate loss: 0.0173
             Mean action noise std: 0.90
                       Mean reward: 6768.63
               Mean episode length: 327.80
                 Mean success rate: 63.50
                  Mean reward/step: 20.35
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 13361152
                    Iteration time: 2.61s
                        Total time: 4182.55s
                               ETA: 6077.6s

################################################################################
                     [1m Learning iteration 1631/4000 [0m

                       Computation: 3224 steps/s (collection: 0.475s, learning 2.066s)
               Value function loss: 98437.0395
                    Surrogate loss: 0.0151
             Mean action noise std: 0.90
                       Mean reward: 6947.37
               Mean episode length: 330.30
                 Mean success rate: 64.00
                  Mean reward/step: 20.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 2.54s
                        Total time: 4185.09s
                               ETA: 6075.0s

################################################################################
                     [1m Learning iteration 1632/4000 [0m

                       Computation: 3214 steps/s (collection: 0.477s, learning 2.072s)
               Value function loss: 102810.9043
                    Surrogate loss: 0.0150
             Mean action noise std: 0.90
                       Mean reward: 6988.04
               Mean episode length: 329.66
                 Mean success rate: 63.00
                  Mean reward/step: 20.74
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13377536
                    Iteration time: 2.55s
                        Total time: 4187.64s
                               ETA: 6072.5s

################################################################################
                     [1m Learning iteration 1633/4000 [0m

                       Computation: 3237 steps/s (collection: 0.480s, learning 2.050s)
               Value function loss: 60257.0260
                    Surrogate loss: 0.0169
             Mean action noise std: 0.90
                       Mean reward: 6981.31
               Mean episode length: 329.48
                 Mean success rate: 62.50
                  Mean reward/step: 20.45
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 2.53s
                        Total time: 4190.17s
                               ETA: 6069.8s

################################################################################
                     [1m Learning iteration 1634/4000 [0m

                       Computation: 3174 steps/s (collection: 0.486s, learning 2.095s)
               Value function loss: 45851.3261
                    Surrogate loss: 0.0174
             Mean action noise std: 0.90
                       Mean reward: 6922.92
               Mean episode length: 328.19
                 Mean success rate: 61.50
                  Mean reward/step: 21.45
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13393920
                    Iteration time: 2.58s
                        Total time: 4192.75s
                               ETA: 6067.3s

################################################################################
                     [1m Learning iteration 1635/4000 [0m

                       Computation: 3219 steps/s (collection: 0.479s, learning 2.066s)
               Value function loss: 94991.0799
                    Surrogate loss: 0.0197
             Mean action noise std: 0.90
                       Mean reward: 6747.25
               Mean episode length: 318.81
                 Mean success rate: 60.00
                  Mean reward/step: 22.54
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 2.54s
                        Total time: 4195.29s
                               ETA: 6064.7s

################################################################################
                     [1m Learning iteration 1636/4000 [0m

                       Computation: 3227 steps/s (collection: 0.477s, learning 2.062s)
               Value function loss: 87862.0906
                    Surrogate loss: 0.0188
             Mean action noise std: 0.90
                       Mean reward: 6846.41
               Mean episode length: 322.60
                 Mean success rate: 60.50
                  Mean reward/step: 22.31
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13410304
                    Iteration time: 2.54s
                        Total time: 4197.83s
                               ETA: 6062.1s

################################################################################
                     [1m Learning iteration 1637/4000 [0m

                       Computation: 3210 steps/s (collection: 0.496s, learning 2.056s)
               Value function loss: 77276.0639
                    Surrogate loss: 0.0156
             Mean action noise std: 0.90
                       Mean reward: 6873.57
               Mean episode length: 324.75
                 Mean success rate: 60.50
                  Mean reward/step: 22.22
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 2.55s
                        Total time: 4200.38s
                               ETA: 6059.5s

################################################################################
                     [1m Learning iteration 1638/4000 [0m

                       Computation: 3148 steps/s (collection: 0.507s, learning 2.095s)
               Value function loss: 97384.6383
                    Surrogate loss: 0.0139
             Mean action noise std: 0.90
                       Mean reward: 7340.93
               Mean episode length: 339.04
                 Mean success rate: 64.00
                  Mean reward/step: 21.80
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13426688
                    Iteration time: 2.60s
                        Total time: 4202.98s
                               ETA: 6057.0s

################################################################################
                     [1m Learning iteration 1639/4000 [0m

                       Computation: 3189 steps/s (collection: 0.498s, learning 2.070s)
               Value function loss: 64943.9377
                    Surrogate loss: 0.0153
             Mean action noise std: 0.90
                       Mean reward: 7154.34
               Mean episode length: 332.76
                 Mean success rate: 62.50
                  Mean reward/step: 21.27
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 2.57s
                        Total time: 4205.55s
                               ETA: 6054.5s

################################################################################
                     [1m Learning iteration 1640/4000 [0m

                       Computation: 3224 steps/s (collection: 0.507s, learning 2.034s)
               Value function loss: 116916.5070
                    Surrogate loss: 0.0178
             Mean action noise std: 0.90
                       Mean reward: 6825.36
               Mean episode length: 330.11
                 Mean success rate: 62.00
                  Mean reward/step: 20.71
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 13443072
                    Iteration time: 2.54s
                        Total time: 4208.09s
                               ETA: 6051.9s

################################################################################
                     [1m Learning iteration 1641/4000 [0m

                       Computation: 3161 steps/s (collection: 0.521s, learning 2.071s)
               Value function loss: 81873.2547
                    Surrogate loss: 0.0160
             Mean action noise std: 0.90
                       Mean reward: 6910.40
               Mean episode length: 330.94
                 Mean success rate: 62.00
                  Mean reward/step: 19.95
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 2.59s
                        Total time: 4210.68s
                               ETA: 6049.3s

################################################################################
                     [1m Learning iteration 1642/4000 [0m

                       Computation: 3131 steps/s (collection: 0.469s, learning 2.147s)
               Value function loss: 102794.2865
                    Surrogate loss: 0.0182
             Mean action noise std: 0.91
                       Mean reward: 7027.56
               Mean episode length: 332.28
                 Mean success rate: 63.50
                  Mean reward/step: 20.19
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13459456
                    Iteration time: 2.62s
                        Total time: 4213.30s
                               ETA: 6046.8s

################################################################################
                     [1m Learning iteration 1643/4000 [0m

                       Computation: 3178 steps/s (collection: 0.488s, learning 2.089s)
               Value function loss: 113963.1770
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 7070.00
               Mean episode length: 337.52
                 Mean success rate: 64.00
                  Mean reward/step: 20.22
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 2.58s
                        Total time: 4215.88s
                               ETA: 6044.3s

################################################################################
                     [1m Learning iteration 1644/4000 [0m

                       Computation: 3206 steps/s (collection: 0.477s, learning 2.078s)
               Value function loss: 53232.1795
                    Surrogate loss: 0.0156
             Mean action noise std: 0.90
                       Mean reward: 6634.41
               Mean episode length: 324.89
                 Mean success rate: 61.50
                  Mean reward/step: 20.40
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13475840
                    Iteration time: 2.56s
                        Total time: 4218.43s
                               ETA: 6041.7s

################################################################################
                     [1m Learning iteration 1645/4000 [0m

                       Computation: 3182 steps/s (collection: 0.483s, learning 2.091s)
               Value function loss: 105708.9872
                    Surrogate loss: 0.0181
             Mean action noise std: 0.90
                       Mean reward: 6332.32
               Mean episode length: 311.55
                 Mean success rate: 58.50
                  Mean reward/step: 20.03
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 2.57s
                        Total time: 4221.01s
                               ETA: 6039.2s

################################################################################
                     [1m Learning iteration 1646/4000 [0m

                       Computation: 3188 steps/s (collection: 0.482s, learning 2.088s)
               Value function loss: 90556.0221
                    Surrogate loss: 0.0172
             Mean action noise std: 0.90
                       Mean reward: 6309.88
               Mean episode length: 305.52
                 Mean success rate: 57.00
                  Mean reward/step: 19.51
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13492224
                    Iteration time: 2.57s
                        Total time: 4223.58s
                               ETA: 6036.6s

################################################################################
                     [1m Learning iteration 1647/4000 [0m

                       Computation: 3157 steps/s (collection: 0.507s, learning 2.088s)
               Value function loss: 90425.2181
                    Surrogate loss: 0.0144
             Mean action noise std: 0.90
                       Mean reward: 6640.19
               Mean episode length: 309.18
                 Mean success rate: 58.50
                  Mean reward/step: 19.66
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 2.59s
                        Total time: 4226.17s
                               ETA: 6034.1s

################################################################################
                     [1m Learning iteration 1648/4000 [0m

                       Computation: 3302 steps/s (collection: 0.447s, learning 2.034s)
               Value function loss: 95442.5698
                    Surrogate loss: 0.0157
             Mean action noise std: 0.90
                       Mean reward: 6606.09
               Mean episode length: 309.11
                 Mean success rate: 59.00
                  Mean reward/step: 19.67
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13508608
                    Iteration time: 2.48s
                        Total time: 4228.65s
                               ETA: 6031.4s

################################################################################
                     [1m Learning iteration 1649/4000 [0m

                       Computation: 3241 steps/s (collection: 0.497s, learning 2.030s)
               Value function loss: 54738.2262
                    Surrogate loss: 0.0158
             Mean action noise std: 0.90
                       Mean reward: 6385.38
               Mean episode length: 301.29
                 Mean success rate: 57.00
                  Mean reward/step: 20.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 2.53s
                        Total time: 4231.18s
                               ETA: 6028.8s

################################################################################
                     [1m Learning iteration 1650/4000 [0m

                       Computation: 3179 steps/s (collection: 0.502s, learning 2.074s)
               Value function loss: 65548.2201
                    Surrogate loss: 0.0149
             Mean action noise std: 0.90
                       Mean reward: 6544.95
               Mean episode length: 307.17
                 Mean success rate: 58.00
                  Mean reward/step: 21.55
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13524992
                    Iteration time: 2.58s
                        Total time: 4233.75s
                               ETA: 6026.2s

################################################################################
                     [1m Learning iteration 1651/4000 [0m

                       Computation: 3182 steps/s (collection: 0.484s, learning 2.090s)
               Value function loss: 97493.9033
                    Surrogate loss: 0.0148
             Mean action noise std: 0.90
                       Mean reward: 6321.98
               Mean episode length: 298.04
                 Mean success rate: 55.50
                  Mean reward/step: 21.71
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 2.57s
                        Total time: 4236.33s
                               ETA: 6023.7s

################################################################################
                     [1m Learning iteration 1652/4000 [0m

                       Computation: 3208 steps/s (collection: 0.494s, learning 2.060s)
               Value function loss: 92504.4500
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 6205.13
               Mean episode length: 296.44
                 Mean success rate: 56.00
                  Mean reward/step: 21.68
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 13541376
                    Iteration time: 2.55s
                        Total time: 4238.88s
                               ETA: 6021.1s

################################################################################
                     [1m Learning iteration 1653/4000 [0m

                       Computation: 3204 steps/s (collection: 0.499s, learning 2.057s)
               Value function loss: 81493.0645
                    Surrogate loss: 0.0134
             Mean action noise std: 0.90
                       Mean reward: 6151.47
               Mean episode length: 297.18
                 Mean success rate: 55.50
                  Mean reward/step: 21.57
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 2.56s
                        Total time: 4241.44s
                               ETA: 6018.5s

################################################################################
                     [1m Learning iteration 1654/4000 [0m

                       Computation: 3245 steps/s (collection: 0.462s, learning 2.062s)
               Value function loss: 51928.7943
                    Surrogate loss: 0.0155
             Mean action noise std: 0.90
                       Mean reward: 6099.97
               Mean episode length: 297.42
                 Mean success rate: 55.00
                  Mean reward/step: 21.56
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13557760
                    Iteration time: 2.52s
                        Total time: 4243.96s
                               ETA: 6015.9s

################################################################################
                     [1m Learning iteration 1655/4000 [0m

                       Computation: 3197 steps/s (collection: 0.501s, learning 2.062s)
               Value function loss: 79452.7482
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 5973.64
               Mean episode length: 298.92
                 Mean success rate: 55.00
                  Mean reward/step: 22.09
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 2.56s
                        Total time: 4246.52s
                               ETA: 6013.3s

################################################################################
                     [1m Learning iteration 1656/4000 [0m

                       Computation: 3217 steps/s (collection: 0.504s, learning 2.042s)
               Value function loss: 127322.9441
                    Surrogate loss: 0.0152
             Mean action noise std: 0.90
                       Mean reward: 6074.50
               Mean episode length: 304.92
                 Mean success rate: 57.50
                  Mean reward/step: 21.68
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13574144
                    Iteration time: 2.55s
                        Total time: 4249.07s
                               ETA: 6010.8s

################################################################################
                     [1m Learning iteration 1657/4000 [0m

                       Computation: 3270 steps/s (collection: 0.450s, learning 2.055s)
               Value function loss: 75217.4145
                    Surrogate loss: 0.0139
             Mean action noise std: 0.90
                       Mean reward: 6028.84
               Mean episode length: 304.86
                 Mean success rate: 58.50
                  Mean reward/step: 20.89
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 2.51s
                        Total time: 4251.58s
                               ETA: 6008.1s

################################################################################
                     [1m Learning iteration 1658/4000 [0m

                       Computation: 3242 steps/s (collection: 0.478s, learning 2.049s)
               Value function loss: 83444.2205
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 6063.01
               Mean episode length: 307.92
                 Mean success rate: 59.50
                  Mean reward/step: 21.77
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13590528
                    Iteration time: 2.53s
                        Total time: 4254.10s
                               ETA: 6005.5s

################################################################################
                     [1m Learning iteration 1659/4000 [0m

                       Computation: 3164 steps/s (collection: 0.526s, learning 2.062s)
               Value function loss: 102629.3966
                    Surrogate loss: 0.0154
             Mean action noise std: 0.90
                       Mean reward: 6669.05
               Mean episode length: 326.05
                 Mean success rate: 64.00
                  Mean reward/step: 21.97
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 2.59s
                        Total time: 4256.69s
                               ETA: 6003.0s

################################################################################
                     [1m Learning iteration 1660/4000 [0m

                       Computation: 3192 steps/s (collection: 0.507s, learning 2.059s)
               Value function loss: 88834.0809
                    Surrogate loss: 0.0157
             Mean action noise std: 0.90
                       Mean reward: 7178.56
               Mean episode length: 337.92
                 Mean success rate: 66.50
                  Mean reward/step: 22.16
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13606912
                    Iteration time: 2.57s
                        Total time: 4259.26s
                               ETA: 6000.4s

################################################################################
                     [1m Learning iteration 1661/4000 [0m

                       Computation: 3201 steps/s (collection: 0.519s, learning 2.039s)
               Value function loss: 97240.7642
                    Surrogate loss: 0.0171
             Mean action noise std: 0.90
                       Mean reward: 7509.39
               Mean episode length: 350.56
                 Mean success rate: 69.50
                  Mean reward/step: 22.15
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 2.56s
                        Total time: 4261.82s
                               ETA: 5997.8s

################################################################################
                     [1m Learning iteration 1662/4000 [0m

                       Computation: 3265 steps/s (collection: 0.477s, learning 2.031s)
               Value function loss: 82273.8722
                    Surrogate loss: 0.0148
             Mean action noise std: 0.90
                       Mean reward: 7627.58
               Mean episode length: 352.40
                 Mean success rate: 71.50
                  Mean reward/step: 22.32
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13623296
                    Iteration time: 2.51s
                        Total time: 4264.32s
                               ETA: 5995.2s

################################################################################
                     [1m Learning iteration 1663/4000 [0m

                       Computation: 3138 steps/s (collection: 0.516s, learning 2.094s)
               Value function loss: 107311.7450
                    Surrogate loss: 0.0143
             Mean action noise std: 0.90
                       Mean reward: 7940.60
               Mean episode length: 355.60
                 Mean success rate: 71.50
                  Mean reward/step: 22.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 2.61s
                        Total time: 4266.93s
                               ETA: 5992.7s

################################################################################
                     [1m Learning iteration 1664/4000 [0m

                       Computation: 3291 steps/s (collection: 0.434s, learning 2.055s)
               Value function loss: 60807.3095
                    Surrogate loss: 0.0164
             Mean action noise std: 0.90
                       Mean reward: 8133.46
               Mean episode length: 361.49
                 Mean success rate: 72.50
                  Mean reward/step: 22.89
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13639680
                    Iteration time: 2.49s
                        Total time: 4269.42s
                               ETA: 5990.0s

################################################################################
                     [1m Learning iteration 1665/4000 [0m

                       Computation: 3234 steps/s (collection: 0.476s, learning 2.056s)
               Value function loss: 54588.9124
                    Surrogate loss: 0.0174
             Mean action noise std: 0.90
                       Mean reward: 7496.50
               Mean episode length: 339.57
                 Mean success rate: 67.00
                  Mean reward/step: 23.54
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 2.53s
                        Total time: 4271.96s
                               ETA: 5987.4s

################################################################################
                     [1m Learning iteration 1666/4000 [0m

                       Computation: 3151 steps/s (collection: 0.528s, learning 2.071s)
               Value function loss: 65334.4867
                    Surrogate loss: 0.0150
             Mean action noise std: 0.90
                       Mean reward: 7216.98
               Mean episode length: 327.47
                 Mean success rate: 63.50
                  Mean reward/step: 23.99
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13656064
                    Iteration time: 2.60s
                        Total time: 4274.55s
                               ETA: 5984.9s

################################################################################
                     [1m Learning iteration 1667/4000 [0m

                       Computation: 3156 steps/s (collection: 0.522s, learning 2.073s)
               Value function loss: 100277.4896
                    Surrogate loss: 0.0172
             Mean action noise std: 0.90
                       Mean reward: 7407.90
               Mean episode length: 335.85
                 Mean success rate: 64.00
                  Mean reward/step: 23.55
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 2.59s
                        Total time: 4277.15s
                               ETA: 5982.4s

################################################################################
                     [1m Learning iteration 1668/4000 [0m

                       Computation: 3197 steps/s (collection: 0.493s, learning 2.069s)
               Value function loss: 91811.0444
                    Surrogate loss: 0.0176
             Mean action noise std: 0.90
                       Mean reward: 7406.23
               Mean episode length: 340.23
                 Mean success rate: 64.00
                  Mean reward/step: 22.77
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13672448
                    Iteration time: 2.56s
                        Total time: 4279.71s
                               ETA: 5979.8s

################################################################################
                     [1m Learning iteration 1669/4000 [0m

                       Computation: 3165 steps/s (collection: 0.493s, learning 2.095s)
               Value function loss: 79886.4554
                    Surrogate loss: 0.0150
             Mean action noise std: 0.90
                       Mean reward: 7468.28
               Mean episode length: 338.41
                 Mean success rate: 63.50
                  Mean reward/step: 22.38
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 2.59s
                        Total time: 4282.30s
                               ETA: 5977.3s

################################################################################
                     [1m Learning iteration 1670/4000 [0m

                       Computation: 3208 steps/s (collection: 0.510s, learning 2.043s)
               Value function loss: 78821.4018
                    Surrogate loss: 0.0164
             Mean action noise std: 0.90
                       Mean reward: 7449.38
               Mean episode length: 335.28
                 Mean success rate: 63.00
                  Mean reward/step: 23.02
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13688832
                    Iteration time: 2.55s
                        Total time: 4284.85s
                               ETA: 5974.7s

################################################################################
                     [1m Learning iteration 1671/4000 [0m

                       Computation: 3169 steps/s (collection: 0.532s, learning 2.053s)
               Value function loss: 93557.6751
                    Surrogate loss: 0.0157
             Mean action noise std: 0.90
                       Mean reward: 7290.15
               Mean episode length: 327.85
                 Mean success rate: 60.00
                  Mean reward/step: 23.59
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 2.58s
                        Total time: 4287.44s
                               ETA: 5972.2s

################################################################################
                     [1m Learning iteration 1672/4000 [0m

                       Computation: 3276 steps/s (collection: 0.444s, learning 2.057s)
               Value function loss: 105210.1623
                    Surrogate loss: 0.0158
             Mean action noise std: 0.90
                       Mean reward: 7350.53
               Mean episode length: 330.94
                 Mean success rate: 61.00
                  Mean reward/step: 23.14
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13705216
                    Iteration time: 2.50s
                        Total time: 4289.94s
                               ETA: 5969.5s

################################################################################
                     [1m Learning iteration 1673/4000 [0m

                       Computation: 3236 steps/s (collection: 0.479s, learning 2.052s)
               Value function loss: 97221.2516
                    Surrogate loss: 0.0144
             Mean action noise std: 0.90
                       Mean reward: 7820.97
               Mean episode length: 345.49
                 Mean success rate: 64.50
                  Mean reward/step: 22.67
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 2.53s
                        Total time: 4292.47s
                               ETA: 5966.9s

################################################################################
                     [1m Learning iteration 1674/4000 [0m

                       Computation: 3220 steps/s (collection: 0.477s, learning 2.067s)
               Value function loss: 104046.5137
                    Surrogate loss: 0.0149
             Mean action noise std: 0.90
                       Mean reward: 8018.54
               Mean episode length: 352.25
                 Mean success rate: 65.50
                  Mean reward/step: 22.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13721600
                    Iteration time: 2.54s
                        Total time: 4295.01s
                               ETA: 5964.3s

################################################################################
                     [1m Learning iteration 1675/4000 [0m

                       Computation: 3195 steps/s (collection: 0.492s, learning 2.071s)
               Value function loss: 95087.2395
                    Surrogate loss: 0.0146
             Mean action noise std: 0.90
                       Mean reward: 7947.47
               Mean episode length: 348.02
                 Mean success rate: 65.00
                  Mean reward/step: 21.90
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 2.56s
                        Total time: 4297.58s
                               ETA: 5961.7s

################################################################################
                     [1m Learning iteration 1676/4000 [0m

                       Computation: 3254 steps/s (collection: 0.469s, learning 2.048s)
               Value function loss: 101973.6402
                    Surrogate loss: 0.0155
             Mean action noise std: 0.90
                       Mean reward: 7581.07
               Mean episode length: 331.62
                 Mean success rate: 63.00
                  Mean reward/step: 21.94
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 13737984
                    Iteration time: 2.52s
                        Total time: 4300.09s
                               ETA: 5959.1s

################################################################################
                     [1m Learning iteration 1677/4000 [0m

                       Computation: 3294 steps/s (collection: 0.451s, learning 2.036s)
               Value function loss: 117647.6811
                    Surrogate loss: 0.0162
             Mean action noise std: 0.90
                       Mean reward: 7650.29
               Mean episode length: 335.99
                 Mean success rate: 63.50
                  Mean reward/step: 21.89
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 2.49s
                        Total time: 4302.58s
                               ETA: 5956.4s

################################################################################
                     [1m Learning iteration 1678/4000 [0m

                       Computation: 3225 steps/s (collection: 0.484s, learning 2.056s)
               Value function loss: 95446.2565
                    Surrogate loss: 0.0158
             Mean action noise std: 0.90
                       Mean reward: 7731.90
               Mean episode length: 341.50
                 Mean success rate: 64.50
                  Mean reward/step: 21.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13754368
                    Iteration time: 2.54s
                        Total time: 4305.12s
                               ETA: 5953.8s

################################################################################
                     [1m Learning iteration 1679/4000 [0m

                       Computation: 3231 steps/s (collection: 0.467s, learning 2.067s)
               Value function loss: 74211.6130
                    Surrogate loss: 0.0155
             Mean action noise std: 0.90
                       Mean reward: 7860.29
               Mean episode length: 345.56
                 Mean success rate: 65.50
                  Mean reward/step: 21.77
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 2.53s
                        Total time: 4307.66s
                               ETA: 5951.2s

################################################################################
                     [1m Learning iteration 1680/4000 [0m

                       Computation: 3221 steps/s (collection: 0.498s, learning 2.044s)
               Value function loss: 73016.4854
                    Surrogate loss: 0.0156
             Mean action noise std: 0.90
                       Mean reward: 7702.38
               Mean episode length: 337.46
                 Mean success rate: 64.00
                  Mean reward/step: 22.46
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13770752
                    Iteration time: 2.54s
                        Total time: 4310.20s
                               ETA: 5948.6s

################################################################################
                     [1m Learning iteration 1681/4000 [0m

                       Computation: 3201 steps/s (collection: 0.486s, learning 2.073s)
               Value function loss: 81293.9281
                    Surrogate loss: 0.0176
             Mean action noise std: 0.90
                       Mean reward: 7358.30
               Mean episode length: 324.47
                 Mean success rate: 62.00
                  Mean reward/step: 22.48
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 2.56s
                        Total time: 4312.76s
                               ETA: 5946.1s

################################################################################
                     [1m Learning iteration 1682/4000 [0m

                       Computation: 3160 steps/s (collection: 0.518s, learning 2.073s)
               Value function loss: 99484.6484
                    Surrogate loss: 0.0123
             Mean action noise std: 0.90
                       Mean reward: 7296.66
               Mean episode length: 325.94
                 Mean success rate: 62.50
                  Mean reward/step: 22.31
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13787136
                    Iteration time: 2.59s
                        Total time: 4315.35s
                               ETA: 5943.5s

################################################################################
                     [1m Learning iteration 1683/4000 [0m

                       Computation: 3215 steps/s (collection: 0.480s, learning 2.067s)
               Value function loss: 98065.2381
                    Surrogate loss: 0.0135
             Mean action noise std: 0.90
                       Mean reward: 7728.14
               Mean episode length: 337.61
                 Mean success rate: 65.50
                  Mean reward/step: 21.97
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 2.55s
                        Total time: 4317.90s
                               ETA: 5941.0s

################################################################################
                     [1m Learning iteration 1684/4000 [0m

                       Computation: 3191 steps/s (collection: 0.492s, learning 2.075s)
               Value function loss: 75630.9771
                    Surrogate loss: 0.0146
             Mean action noise std: 0.90
                       Mean reward: 7705.59
               Mean episode length: 338.51
                 Mean success rate: 65.00
                  Mean reward/step: 22.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13803520
                    Iteration time: 2.57s
                        Total time: 4320.46s
                               ETA: 5938.4s

################################################################################
                     [1m Learning iteration 1685/4000 [0m

                       Computation: 3194 steps/s (collection: 0.453s, learning 2.111s)
               Value function loss: 76674.0960
                    Surrogate loss: 0.0154
             Mean action noise std: 0.90
                       Mean reward: 8167.45
               Mean episode length: 354.34
                 Mean success rate: 68.50
                  Mean reward/step: 22.83
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 2.56s
                        Total time: 4323.03s
                               ETA: 5935.8s

################################################################################
                     [1m Learning iteration 1686/4000 [0m

                       Computation: 3252 steps/s (collection: 0.433s, learning 2.086s)
               Value function loss: 81604.7692
                    Surrogate loss: 0.0148
             Mean action noise std: 0.90
                       Mean reward: 8256.23
               Mean episode length: 354.32
                 Mean success rate: 68.00
                  Mean reward/step: 22.50
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13819904
                    Iteration time: 2.52s
                        Total time: 4325.55s
                               ETA: 5933.2s

################################################################################
                     [1m Learning iteration 1687/4000 [0m

                       Computation: 3194 steps/s (collection: 0.486s, learning 2.079s)
               Value function loss: 139451.3334
                    Surrogate loss: 0.0148
             Mean action noise std: 0.90
                       Mean reward: 8107.64
               Mean episode length: 354.82
                 Mean success rate: 68.50
                  Mean reward/step: 21.51
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 2.56s
                        Total time: 4328.11s
                               ETA: 5930.6s

################################################################################
                     [1m Learning iteration 1688/4000 [0m

                       Computation: 3205 steps/s (collection: 0.469s, learning 2.087s)
               Value function loss: 76617.4940
                    Surrogate loss: 0.0151
             Mean action noise std: 0.90
                       Mean reward: 7944.71
               Mean episode length: 350.12
                 Mean success rate: 68.00
                  Mean reward/step: 19.87
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13836288
                    Iteration time: 2.56s
                        Total time: 4330.67s
                               ETA: 5928.1s

################################################################################
                     [1m Learning iteration 1689/4000 [0m

                       Computation: 3182 steps/s (collection: 0.471s, learning 2.102s)
               Value function loss: 70657.0618
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 8161.17
               Mean episode length: 359.57
                 Mean success rate: 70.00
                  Mean reward/step: 19.91
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 2.57s
                        Total time: 4333.24s
                               ETA: 5925.5s

################################################################################
                     [1m Learning iteration 1690/4000 [0m

                       Computation: 3193 steps/s (collection: 0.447s, learning 2.118s)
               Value function loss: 96621.5419
                    Surrogate loss: 0.0128
             Mean action noise std: 0.90
                       Mean reward: 7926.39
               Mean episode length: 358.79
                 Mean success rate: 68.50
                  Mean reward/step: 20.41
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 13852672
                    Iteration time: 2.57s
                        Total time: 4335.80s
                               ETA: 5923.0s

################################################################################
                     [1m Learning iteration 1691/4000 [0m

                       Computation: 3137 steps/s (collection: 0.507s, learning 2.104s)
               Value function loss: 90683.0545
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 7733.90
               Mean episode length: 351.48
                 Mean success rate: 67.00
                  Mean reward/step: 21.13
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 2.61s
                        Total time: 4338.42s
                               ETA: 5920.5s

################################################################################
                     [1m Learning iteration 1692/4000 [0m

                       Computation: 3191 steps/s (collection: 0.470s, learning 2.097s)
               Value function loss: 147280.5820
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 7775.93
               Mean episode length: 355.48
                 Mean success rate: 68.00
                  Mean reward/step: 21.09
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13869056
                    Iteration time: 2.57s
                        Total time: 4340.98s
                               ETA: 5917.9s

################################################################################
                     [1m Learning iteration 1693/4000 [0m

                       Computation: 3187 steps/s (collection: 0.473s, learning 2.097s)
               Value function loss: 63180.2450
                    Surrogate loss: 0.0132
             Mean action noise std: 0.90
                       Mean reward: 7358.79
               Mean episode length: 340.82
                 Mean success rate: 65.00
                  Mean reward/step: 20.63
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 2.57s
                        Total time: 4343.55s
                               ETA: 5915.3s

################################################################################
                     [1m Learning iteration 1694/4000 [0m

                       Computation: 3164 steps/s (collection: 0.480s, learning 2.109s)
               Value function loss: 116743.3971
                    Surrogate loss: 0.0146
             Mean action noise std: 0.90
                       Mean reward: 7639.07
               Mean episode length: 351.05
                 Mean success rate: 67.00
                  Mean reward/step: 20.86
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13885440
                    Iteration time: 2.59s
                        Total time: 4346.14s
                               ETA: 5912.8s

################################################################################
                     [1m Learning iteration 1695/4000 [0m

                       Computation: 3217 steps/s (collection: 0.463s, learning 2.083s)
               Value function loss: 70683.4717
                    Surrogate loss: 0.0140
             Mean action noise std: 0.90
                       Mean reward: 6986.87
               Mean episode length: 328.09
                 Mean success rate: 61.50
                  Mean reward/step: 21.32
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 2.55s
                        Total time: 4348.69s
                               ETA: 5910.2s

################################################################################
                     [1m Learning iteration 1696/4000 [0m

                       Computation: 3101 steps/s (collection: 0.520s, learning 2.121s)
               Value function loss: 106564.9946
                    Surrogate loss: 0.0133
             Mean action noise std: 0.90
                       Mean reward: 6997.66
               Mean episode length: 326.31
                 Mean success rate: 61.00
                  Mean reward/step: 21.38
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 13901824
                    Iteration time: 2.64s
                        Total time: 4351.33s
                               ETA: 5907.8s

################################################################################
                     [1m Learning iteration 1697/4000 [0m

                       Computation: 3111 steps/s (collection: 0.502s, learning 2.131s)
               Value function loss: 83598.1809
                    Surrogate loss: 0.0157
             Mean action noise std: 0.90
                       Mean reward: 7018.66
               Mean episode length: 324.69
                 Mean success rate: 61.00
                  Mean reward/step: 20.98
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 2.63s
                        Total time: 4353.96s
                               ETA: 5905.3s

################################################################################
                     [1m Learning iteration 1698/4000 [0m

                       Computation: 3083 steps/s (collection: 0.513s, learning 2.143s)
               Value function loss: 97900.9179
                    Surrogate loss: 0.0149
             Mean action noise std: 0.90
                       Mean reward: 7165.59
               Mean episode length: 333.00
                 Mean success rate: 62.50
                  Mean reward/step: 21.05
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13918208
                    Iteration time: 2.66s
                        Total time: 4356.62s
                               ETA: 5902.8s

################################################################################
                     [1m Learning iteration 1699/4000 [0m

                       Computation: 3182 steps/s (collection: 0.471s, learning 2.104s)
               Value function loss: 86928.8708
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 7066.45
               Mean episode length: 332.69
                 Mean success rate: 62.00
                  Mean reward/step: 21.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 2.57s
                        Total time: 4359.19s
                               ETA: 5900.3s

################################################################################
                     [1m Learning iteration 1700/4000 [0m

                       Computation: 3096 steps/s (collection: 0.528s, learning 2.117s)
               Value function loss: 62948.7430
                    Surrogate loss: 0.0172
             Mean action noise std: 0.90
                       Mean reward: 6856.82
               Mean episode length: 330.31
                 Mean success rate: 61.50
                  Mean reward/step: 21.75
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13934592
                    Iteration time: 2.65s
                        Total time: 4361.84s
                               ETA: 5897.8s

################################################################################
                     [1m Learning iteration 1701/4000 [0m

                       Computation: 3165 steps/s (collection: 0.496s, learning 2.092s)
               Value function loss: 54316.1583
                    Surrogate loss: 0.0149
             Mean action noise std: 0.90
                       Mean reward: 6436.26
               Mean episode length: 313.40
                 Mean success rate: 57.50
                  Mean reward/step: 22.07
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 2.59s
                        Total time: 4364.42s
                               ETA: 5895.3s

################################################################################
                     [1m Learning iteration 1702/4000 [0m

                       Computation: 3162 steps/s (collection: 0.473s, learning 2.117s)
               Value function loss: 70210.5776
                    Surrogate loss: 0.0142
             Mean action noise std: 0.90
                       Mean reward: 6504.54
               Mean episode length: 318.30
                 Mean success rate: 59.00
                  Mean reward/step: 22.63
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13950976
                    Iteration time: 2.59s
                        Total time: 4367.02s
                               ETA: 5892.8s

################################################################################
                     [1m Learning iteration 1703/4000 [0m

                       Computation: 3170 steps/s (collection: 0.479s, learning 2.105s)
               Value function loss: 111541.6546
                    Surrogate loss: 0.0130
             Mean action noise std: 0.90
                       Mean reward: 6570.80
               Mean episode length: 323.80
                 Mean success rate: 60.50
                  Mean reward/step: 21.89
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 2.58s
                        Total time: 4369.60s
                               ETA: 5890.2s

################################################################################
                     [1m Learning iteration 1704/4000 [0m

                       Computation: 3169 steps/s (collection: 0.481s, learning 2.103s)
               Value function loss: 84449.2454
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 6665.44
               Mean episode length: 331.00
                 Mean success rate: 62.50
                  Mean reward/step: 21.52
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13967360
                    Iteration time: 2.58s
                        Total time: 4372.18s
                               ETA: 5887.7s

################################################################################
                     [1m Learning iteration 1705/4000 [0m

                       Computation: 3160 steps/s (collection: 0.491s, learning 2.101s)
               Value function loss: 76539.2018
                    Surrogate loss: 0.0162
             Mean action noise std: 0.90
                       Mean reward: 6800.87
               Mean episode length: 334.31
                 Mean success rate: 63.50
                  Mean reward/step: 20.98
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 2.59s
                        Total time: 4374.78s
                               ETA: 5885.2s

################################################################################
                     [1m Learning iteration 1706/4000 [0m

                       Computation: 3077 steps/s (collection: 0.530s, learning 2.132s)
               Value function loss: 93284.1743
                    Surrogate loss: 0.0128
             Mean action noise std: 0.90
                       Mean reward: 6817.96
               Mean episode length: 334.02
                 Mean success rate: 64.00
                  Mean reward/step: 20.56
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13983744
                    Iteration time: 2.66s
                        Total time: 4377.44s
                               ETA: 5882.7s

################################################################################
                     [1m Learning iteration 1707/4000 [0m

                       Computation: 3112 steps/s (collection: 0.521s, learning 2.111s)
               Value function loss: 92292.6101
                    Surrogate loss: 0.0151
             Mean action noise std: 0.90
                       Mean reward: 6659.57
               Mean episode length: 323.50
                 Mean success rate: 62.00
                  Mean reward/step: 20.83
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 2.63s
                        Total time: 4380.07s
                               ETA: 5880.3s

################################################################################
                     [1m Learning iteration 1708/4000 [0m

                       Computation: 3249 steps/s (collection: 0.464s, learning 2.057s)
               Value function loss: 103054.7069
                    Surrogate loss: 0.0140
             Mean action noise std: 0.90
                       Mean reward: 7194.23
               Mean episode length: 337.38
                 Mean success rate: 65.00
                  Mean reward/step: 20.55
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 14000128
                    Iteration time: 2.52s
                        Total time: 4382.59s
                               ETA: 5877.6s

################################################################################
                     [1m Learning iteration 1709/4000 [0m

                       Computation: 3260 steps/s (collection: 0.444s, learning 2.068s)
               Value function loss: 83012.2934
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 7145.68
               Mean episode length: 333.73
                 Mean success rate: 64.00
                  Mean reward/step: 20.48
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 2.51s
                        Total time: 4385.10s
                               ETA: 5875.0s

################################################################################
                     [1m Learning iteration 1710/4000 [0m

                       Computation: 3201 steps/s (collection: 0.449s, learning 2.110s)
               Value function loss: 66452.7782
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 7162.68
               Mean episode length: 329.56
                 Mean success rate: 63.00
                  Mean reward/step: 20.92
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14016512
                    Iteration time: 2.56s
                        Total time: 4387.66s
                               ETA: 5872.4s

################################################################################
                     [1m Learning iteration 1711/4000 [0m

                       Computation: 3295 steps/s (collection: 0.440s, learning 2.047s)
               Value function loss: 97881.7194
                    Surrogate loss: 0.0157
             Mean action noise std: 0.90
                       Mean reward: 7057.57
               Mean episode length: 324.10
                 Mean success rate: 61.50
                  Mean reward/step: 21.29
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 2.49s
                        Total time: 4390.15s
                               ETA: 5869.8s

################################################################################
                     [1m Learning iteration 1712/4000 [0m

                       Computation: 3243 steps/s (collection: 0.459s, learning 2.067s)
               Value function loss: 65431.6830
                    Surrogate loss: 0.0120
             Mean action noise std: 0.90
                       Mean reward: 7275.51
               Mean episode length: 332.83
                 Mean success rate: 63.50
                  Mean reward/step: 20.96
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14032896
                    Iteration time: 2.53s
                        Total time: 4392.67s
                               ETA: 5867.2s

################################################################################
                     [1m Learning iteration 1713/4000 [0m

                       Computation: 3295 steps/s (collection: 0.448s, learning 2.037s)
               Value function loss: 85404.2437
                    Surrogate loss: 0.0156
             Mean action noise std: 0.90
                       Mean reward: 7158.94
               Mean episode length: 329.85
                 Mean success rate: 63.00
                  Mean reward/step: 21.48
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 2.49s
                        Total time: 4395.16s
                               ETA: 5864.5s

################################################################################
                     [1m Learning iteration 1714/4000 [0m

                       Computation: 3235 steps/s (collection: 0.475s, learning 2.056s)
               Value function loss: 89254.8135
                    Surrogate loss: 0.0178
             Mean action noise std: 0.90
                       Mean reward: 7152.11
               Mean episode length: 328.87
                 Mean success rate: 62.00
                  Mean reward/step: 21.50
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14049280
                    Iteration time: 2.53s
                        Total time: 4397.69s
                               ETA: 5861.9s

################################################################################
                     [1m Learning iteration 1715/4000 [0m

                       Computation: 3247 steps/s (collection: 0.466s, learning 2.057s)
               Value function loss: 67792.0691
                    Surrogate loss: 0.0154
             Mean action noise std: 0.90
                       Mean reward: 7038.31
               Mean episode length: 326.00
                 Mean success rate: 62.00
                  Mean reward/step: 21.07
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 2.52s
                        Total time: 4400.21s
                               ETA: 5859.3s

################################################################################
                     [1m Learning iteration 1716/4000 [0m

                       Computation: 3215 steps/s (collection: 0.461s, learning 2.087s)
               Value function loss: 96832.5457
                    Surrogate loss: 0.0132
             Mean action noise std: 0.90
                       Mean reward: 6729.33
               Mean episode length: 317.44
                 Mean success rate: 61.00
                  Mean reward/step: 21.50
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14065664
                    Iteration time: 2.55s
                        Total time: 4402.76s
                               ETA: 5856.7s

################################################################################
                     [1m Learning iteration 1717/4000 [0m

                       Computation: 3304 steps/s (collection: 0.443s, learning 2.036s)
               Value function loss: 65403.6532
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 6806.83
               Mean episode length: 320.82
                 Mean success rate: 62.50
                  Mean reward/step: 21.18
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 2.48s
                        Total time: 4405.24s
                               ETA: 5854.0s

################################################################################
                     [1m Learning iteration 1718/4000 [0m

                       Computation: 3241 steps/s (collection: 0.472s, learning 2.055s)
               Value function loss: 117852.0725
                    Surrogate loss: 0.0125
             Mean action noise std: 0.90
                       Mean reward: 6680.32
               Mean episode length: 317.72
                 Mean success rate: 61.00
                  Mean reward/step: 21.27
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14082048
                    Iteration time: 2.53s
                        Total time: 4407.77s
                               ETA: 5851.4s

################################################################################
                     [1m Learning iteration 1719/4000 [0m

                       Computation: 3183 steps/s (collection: 0.458s, learning 2.115s)
               Value function loss: 78342.8246
                    Surrogate loss: 0.0151
             Mean action noise std: 0.90
                       Mean reward: 6815.07
               Mean episode length: 324.17
                 Mean success rate: 62.00
                  Mean reward/step: 21.52
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 2.57s
                        Total time: 4410.34s
                               ETA: 5848.8s

################################################################################
                     [1m Learning iteration 1720/4000 [0m

                       Computation: 3101 steps/s (collection: 0.503s, learning 2.138s)
               Value function loss: 67937.1280
                    Surrogate loss: 0.0155
             Mean action noise std: 0.90
                       Mean reward: 6384.66
               Mean episode length: 316.04
                 Mean success rate: 58.50
                  Mean reward/step: 21.71
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14098432
                    Iteration time: 2.64s
                        Total time: 4412.98s
                               ETA: 5846.4s

################################################################################
                     [1m Learning iteration 1721/4000 [0m

                       Computation: 3095 steps/s (collection: 0.515s, learning 2.132s)
               Value function loss: 88409.4007
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 6324.38
               Mean episode length: 312.48
                 Mean success rate: 58.50
                  Mean reward/step: 21.96
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 2.65s
                        Total time: 4415.63s
                               ETA: 5843.9s

################################################################################
                     [1m Learning iteration 1722/4000 [0m

                       Computation: 3098 steps/s (collection: 0.543s, learning 2.101s)
               Value function loss: 102472.7557
                    Surrogate loss: 0.0147
             Mean action noise std: 0.90
                       Mean reward: 6357.26
               Mean episode length: 312.23
                 Mean success rate: 59.00
                  Mean reward/step: 21.87
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14114816
                    Iteration time: 2.64s
                        Total time: 4418.27s
                               ETA: 5841.5s

################################################################################
                     [1m Learning iteration 1723/4000 [0m

                       Computation: 3124 steps/s (collection: 0.500s, learning 2.121s)
               Value function loss: 108887.7650
                    Surrogate loss: 0.0132
             Mean action noise std: 0.90
                       Mean reward: 7033.20
               Mean episode length: 332.67
                 Mean success rate: 63.00
                  Mean reward/step: 21.02
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 2.62s
                        Total time: 4420.89s
                               ETA: 5839.0s

################################################################################
                     [1m Learning iteration 1724/4000 [0m

                       Computation: 3179 steps/s (collection: 0.468s, learning 2.109s)
               Value function loss: 81346.2745
                    Surrogate loss: 0.0138
             Mean action noise std: 0.90
                       Mean reward: 7046.08
               Mean episode length: 327.81
                 Mean success rate: 62.00
                  Mean reward/step: 20.08
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14131200
                    Iteration time: 2.58s
                        Total time: 4423.47s
                               ETA: 5836.4s

################################################################################
                     [1m Learning iteration 1725/4000 [0m

                       Computation: 3082 steps/s (collection: 0.511s, learning 2.146s)
               Value function loss: 84085.5723
                    Surrogate loss: 0.0142
             Mean action noise std: 0.90
                       Mean reward: 7096.27
               Mean episode length: 335.02
                 Mean success rate: 63.00
                  Mean reward/step: 20.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 2.66s
                        Total time: 4426.13s
                               ETA: 5834.0s

################################################################################
                     [1m Learning iteration 1726/4000 [0m

                       Computation: 3160 steps/s (collection: 0.487s, learning 2.104s)
               Value function loss: 68449.0363
                    Surrogate loss: 0.0123
             Mean action noise std: 0.90
                       Mean reward: 6857.93
               Mean episode length: 324.74
                 Mean success rate: 61.00
                  Mean reward/step: 20.63
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14147584
                    Iteration time: 2.59s
                        Total time: 4428.72s
                               ETA: 5831.4s

################################################################################
                     [1m Learning iteration 1727/4000 [0m

                       Computation: 3170 steps/s (collection: 0.495s, learning 2.089s)
               Value function loss: 96402.6416
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 6862.20
               Mean episode length: 322.46
                 Mean success rate: 61.00
                  Mean reward/step: 21.19
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 2.58s
                        Total time: 4431.30s
                               ETA: 5828.9s

################################################################################
                     [1m Learning iteration 1728/4000 [0m

                       Computation: 3122 steps/s (collection: 0.482s, learning 2.141s)
               Value function loss: 66863.9635
                    Surrogate loss: 0.0131
             Mean action noise std: 0.90
                       Mean reward: 7181.97
               Mean episode length: 330.92
                 Mean success rate: 63.50
                  Mean reward/step: 21.81
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 14163968
                    Iteration time: 2.62s
                        Total time: 4433.93s
                               ETA: 5826.4s

################################################################################
                     [1m Learning iteration 1729/4000 [0m

                       Computation: 3118 steps/s (collection: 0.470s, learning 2.157s)
               Value function loss: 58967.7198
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 6924.13
               Mean episode length: 322.43
                 Mean success rate: 61.50
                  Mean reward/step: 22.09
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 2.63s
                        Total time: 4436.55s
                               ETA: 5823.9s

################################################################################
                     [1m Learning iteration 1730/4000 [0m

                       Computation: 3182 steps/s (collection: 0.465s, learning 2.109s)
               Value function loss: 105889.1758
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 7273.84
               Mean episode length: 337.15
                 Mean success rate: 64.00
                  Mean reward/step: 23.61
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14180352
                    Iteration time: 2.57s
                        Total time: 4439.13s
                               ETA: 5821.4s

################################################################################
                     [1m Learning iteration 1731/4000 [0m

                       Computation: 3166 steps/s (collection: 0.488s, learning 2.099s)
               Value function loss: 69096.9086
                    Surrogate loss: 0.0099
             Mean action noise std: 0.90
                       Mean reward: 6863.37
               Mean episode length: 325.56
                 Mean success rate: 61.00
                  Mean reward/step: 23.21
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 2.59s
                        Total time: 4441.71s
                               ETA: 5818.9s

################################################################################
                     [1m Learning iteration 1732/4000 [0m

                       Computation: 3113 steps/s (collection: 0.509s, learning 2.122s)
               Value function loss: 122466.9458
                    Surrogate loss: 0.0122
             Mean action noise std: 0.90
                       Mean reward: 7100.92
               Mean episode length: 331.44
                 Mean success rate: 62.50
                  Mean reward/step: 21.96
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14196736
                    Iteration time: 2.63s
                        Total time: 4444.35s
                               ETA: 5816.4s

################################################################################
                     [1m Learning iteration 1733/4000 [0m

                       Computation: 3166 steps/s (collection: 0.474s, learning 2.113s)
               Value function loss: 71288.6950
                    Surrogate loss: 0.0180
             Mean action noise std: 0.90
                       Mean reward: 7215.30
               Mean episode length: 339.19
                 Mean success rate: 64.00
                  Mean reward/step: 21.05
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 2.59s
                        Total time: 4446.93s
                               ETA: 5813.8s

################################################################################
                     [1m Learning iteration 1734/4000 [0m

                       Computation: 3177 steps/s (collection: 0.475s, learning 2.103s)
               Value function loss: 98785.2467
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 7493.40
               Mean episode length: 344.62
                 Mean success rate: 65.50
                  Mean reward/step: 21.06
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14213120
                    Iteration time: 2.58s
                        Total time: 4449.51s
                               ETA: 5811.3s

################################################################################
                     [1m Learning iteration 1735/4000 [0m

                       Computation: 3132 steps/s (collection: 0.483s, learning 2.132s)
               Value function loss: 79816.4663
                    Surrogate loss: 0.0146
             Mean action noise std: 0.90
                       Mean reward: 7509.91
               Mean episode length: 344.74
                 Mean success rate: 65.00
                  Mean reward/step: 21.25
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 2.62s
                        Total time: 4452.13s
                               ETA: 5808.8s

################################################################################
                     [1m Learning iteration 1736/4000 [0m

                       Computation: 3139 steps/s (collection: 0.482s, learning 2.127s)
               Value function loss: 81544.9356
                    Surrogate loss: 0.0140
             Mean action noise std: 0.90
                       Mean reward: 7799.71
               Mean episode length: 354.23
                 Mean success rate: 67.50
                  Mean reward/step: 21.44
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14229504
                    Iteration time: 2.61s
                        Total time: 4454.74s
                               ETA: 5806.3s

################################################################################
                     [1m Learning iteration 1737/4000 [0m

                       Computation: 3199 steps/s (collection: 0.451s, learning 2.109s)
               Value function loss: 79683.4815
                    Surrogate loss: 0.0154
             Mean action noise std: 0.90
                       Mean reward: 7630.58
               Mean episode length: 350.48
                 Mean success rate: 66.00
                  Mean reward/step: 21.30
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 2.56s
                        Total time: 4457.30s
                               ETA: 5803.7s

################################################################################
                     [1m Learning iteration 1738/4000 [0m

                       Computation: 3219 steps/s (collection: 0.467s, learning 2.078s)
               Value function loss: 77864.8511
                    Surrogate loss: 0.0132
             Mean action noise std: 0.90
                       Mean reward: 7691.66
               Mean episode length: 353.20
                 Mean success rate: 67.00
                  Mean reward/step: 21.41
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14245888
                    Iteration time: 2.54s
                        Total time: 4459.84s
                               ETA: 5801.1s

################################################################################
                     [1m Learning iteration 1739/4000 [0m

                       Computation: 3129 steps/s (collection: 0.497s, learning 2.121s)
               Value function loss: 94669.6824
                    Surrogate loss: 0.0120
             Mean action noise std: 0.90
                       Mean reward: 7945.65
               Mean episode length: 358.41
                 Mean success rate: 69.00
                  Mean reward/step: 21.01
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 2.62s
                        Total time: 4462.46s
                               ETA: 5798.6s

################################################################################
                     [1m Learning iteration 1740/4000 [0m

                       Computation: 3138 steps/s (collection: 0.512s, learning 2.098s)
               Value function loss: 77682.0094
                    Surrogate loss: 0.0147
             Mean action noise std: 0.90
                       Mean reward: 8184.58
               Mean episode length: 373.54
                 Mean success rate: 72.50
                  Mean reward/step: 21.08
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14262272
                    Iteration time: 2.61s
                        Total time: 4465.07s
                               ETA: 5796.1s

################################################################################
                     [1m Learning iteration 1741/4000 [0m

                       Computation: 3163 steps/s (collection: 0.477s, learning 2.112s)
               Value function loss: 83129.2325
                    Surrogate loss: 0.0155
             Mean action noise std: 0.90
                       Mean reward: 7949.06
               Mean episode length: 369.30
                 Mean success rate: 71.00
                  Mean reward/step: 20.61
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 2.59s
                        Total time: 4467.66s
                               ETA: 5793.6s

################################################################################
                     [1m Learning iteration 1742/4000 [0m

                       Computation: 3181 steps/s (collection: 0.473s, learning 2.102s)
               Value function loss: 80234.7494
                    Surrogate loss: 0.0147
             Mean action noise std: 0.90
                       Mean reward: 8050.61
               Mean episode length: 370.94
                 Mean success rate: 72.50
                  Mean reward/step: 20.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14278656
                    Iteration time: 2.58s
                        Total time: 4470.23s
                               ETA: 5791.0s

################################################################################
                     [1m Learning iteration 1743/4000 [0m

                       Computation: 3140 steps/s (collection: 0.500s, learning 2.108s)
               Value function loss: 88382.5261
                    Surrogate loss: 0.0151
             Mean action noise std: 0.90
                       Mean reward: 8023.98
               Mean episode length: 374.23
                 Mean success rate: 73.00
                  Mean reward/step: 20.68
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 2.61s
                        Total time: 4472.84s
                               ETA: 5788.5s

################################################################################
                     [1m Learning iteration 1744/4000 [0m

                       Computation: 3225 steps/s (collection: 0.458s, learning 2.082s)
               Value function loss: 74644.2110
                    Surrogate loss: 0.0162
             Mean action noise std: 0.90
                       Mean reward: 8238.97
               Mean episode length: 379.49
                 Mean success rate: 74.50
                  Mean reward/step: 21.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14295040
                    Iteration time: 2.54s
                        Total time: 4475.38s
                               ETA: 5785.9s

################################################################################
                     [1m Learning iteration 1745/4000 [0m

                       Computation: 3114 steps/s (collection: 0.510s, learning 2.120s)
               Value function loss: 38598.7237
                    Surrogate loss: 0.0148
             Mean action noise std: 0.90
                       Mean reward: 8112.90
               Mean episode length: 373.88
                 Mean success rate: 73.50
                  Mean reward/step: 21.74
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 2.63s
                        Total time: 4478.01s
                               ETA: 5783.5s

################################################################################
                     [1m Learning iteration 1746/4000 [0m

                       Computation: 3197 steps/s (collection: 0.475s, learning 2.087s)
               Value function loss: 76196.1968
                    Surrogate loss: 0.0143
             Mean action noise std: 0.90
                       Mean reward: 7817.10
               Mean episode length: 366.94
                 Mean success rate: 71.50
                  Mean reward/step: 22.55
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14311424
                    Iteration time: 2.56s
                        Total time: 4480.57s
                               ETA: 5780.9s

################################################################################
                     [1m Learning iteration 1747/4000 [0m

                       Computation: 3215 steps/s (collection: 0.479s, learning 2.069s)
               Value function loss: 114495.5737
                    Surrogate loss: 0.0166
             Mean action noise std: 0.90
                       Mean reward: 7972.38
               Mean episode length: 372.31
                 Mean success rate: 72.50
                  Mean reward/step: 21.92
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 2.55s
                        Total time: 4483.12s
                               ETA: 5778.3s

################################################################################
                     [1m Learning iteration 1748/4000 [0m

                       Computation: 3197 steps/s (collection: 0.486s, learning 2.076s)
               Value function loss: 98852.8632
                    Surrogate loss: 0.0146
             Mean action noise std: 0.90
                       Mean reward: 7757.19
               Mean episode length: 367.02
                 Mean success rate: 71.00
                  Mean reward/step: 21.34
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14327808
                    Iteration time: 2.56s
                        Total time: 4485.68s
                               ETA: 5775.7s

################################################################################
                     [1m Learning iteration 1749/4000 [0m

                       Computation: 3197 steps/s (collection: 0.480s, learning 2.083s)
               Value function loss: 64478.3430
                    Surrogate loss: 0.0165
             Mean action noise std: 0.90
                       Mean reward: 7685.53
               Mean episode length: 362.26
                 Mean success rate: 69.50
                  Mean reward/step: 21.76
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 2.56s
                        Total time: 4488.25s
                               ETA: 5773.2s

################################################################################
                     [1m Learning iteration 1750/4000 [0m

                       Computation: 3173 steps/s (collection: 0.508s, learning 2.074s)
               Value function loss: 124311.8957
                    Surrogate loss: 0.0169
             Mean action noise std: 0.90
                       Mean reward: 7991.59
               Mean episode length: 371.40
                 Mean success rate: 71.50
                  Mean reward/step: 21.67
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14344192
                    Iteration time: 2.58s
                        Total time: 4490.83s
                               ETA: 5770.6s

################################################################################
                     [1m Learning iteration 1751/4000 [0m

                       Computation: 3218 steps/s (collection: 0.470s, learning 2.076s)
               Value function loss: 85773.1214
                    Surrogate loss: 0.0184
             Mean action noise std: 0.90
                       Mean reward: 7904.57
               Mean episode length: 370.29
                 Mean success rate: 71.00
                  Mean reward/step: 21.24
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 2.55s
                        Total time: 4493.37s
                               ETA: 5768.0s

################################################################################
                     [1m Learning iteration 1752/4000 [0m

                       Computation: 3137 steps/s (collection: 0.456s, learning 2.154s)
               Value function loss: 103748.9061
                    Surrogate loss: 0.0144
             Mean action noise std: 0.90
                       Mean reward: 7780.80
               Mean episode length: 365.23
                 Mean success rate: 70.50
                  Mean reward/step: 20.76
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14360576
                    Iteration time: 2.61s
                        Total time: 4495.98s
                               ETA: 5765.5s

################################################################################
                     [1m Learning iteration 1753/4000 [0m

                       Computation: 3173 steps/s (collection: 0.519s, learning 2.062s)
               Value function loss: 73159.7149
                    Surrogate loss: 0.0157
             Mean action noise std: 0.90
                       Mean reward: 7804.40
               Mean episode length: 364.08
                 Mean success rate: 70.50
                  Mean reward/step: 20.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 2.58s
                        Total time: 4498.57s
                               ETA: 5763.0s

################################################################################
                     [1m Learning iteration 1754/4000 [0m

                       Computation: 3200 steps/s (collection: 0.462s, learning 2.098s)
               Value function loss: 110669.0184
                    Surrogate loss: 0.0134
             Mean action noise std: 0.90
                       Mean reward: 8222.31
               Mean episode length: 375.50
                 Mean success rate: 73.50
                  Mean reward/step: 21.31
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14376960
                    Iteration time: 2.56s
                        Total time: 4501.13s
                               ETA: 5760.4s

################################################################################
                     [1m Learning iteration 1755/4000 [0m

                       Computation: 3275 steps/s (collection: 0.456s, learning 2.044s)
               Value function loss: 63844.0919
                    Surrogate loss: 0.0193
             Mean action noise std: 0.90
                       Mean reward: 7940.23
               Mean episode length: 367.31
                 Mean success rate: 72.00
                  Mean reward/step: 20.85
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 2.50s
                        Total time: 4503.63s
                               ETA: 5757.8s

################################################################################
                     [1m Learning iteration 1756/4000 [0m

                       Computation: 3205 steps/s (collection: 0.473s, learning 2.082s)
               Value function loss: 63945.0666
                    Surrogate loss: 0.0150
             Mean action noise std: 0.90
                       Mean reward: 7661.39
               Mean episode length: 357.97
                 Mean success rate: 70.50
                  Mean reward/step: 21.03
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 14393344
                    Iteration time: 2.56s
                        Total time: 4506.18s
                               ETA: 5755.2s

################################################################################
                     [1m Learning iteration 1757/4000 [0m

                       Computation: 3154 steps/s (collection: 0.508s, learning 2.090s)
               Value function loss: 73844.5112
                    Surrogate loss: 0.0138
             Mean action noise std: 0.90
                       Mean reward: 7785.91
               Mean episode length: 362.40
                 Mean success rate: 71.50
                  Mean reward/step: 21.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 2.60s
                        Total time: 4508.78s
                               ETA: 5752.7s

################################################################################
                     [1m Learning iteration 1758/4000 [0m

                       Computation: 3206 steps/s (collection: 0.474s, learning 2.080s)
               Value function loss: 63618.2972
                    Surrogate loss: 0.0144
             Mean action noise std: 0.90
                       Mean reward: 7117.42
               Mean episode length: 338.14
                 Mean success rate: 66.50
                  Mean reward/step: 22.40
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 14409728
                    Iteration time: 2.55s
                        Total time: 4511.33s
                               ETA: 5750.1s

################################################################################
                     [1m Learning iteration 1759/4000 [0m

                       Computation: 3205 steps/s (collection: 0.489s, learning 2.067s)
               Value function loss: 88697.9733
                    Surrogate loss: 0.0127
             Mean action noise std: 0.90
                       Mean reward: 6857.74
               Mean episode length: 324.39
                 Mean success rate: 63.50
                  Mean reward/step: 22.06
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 2.56s
                        Total time: 4513.89s
                               ETA: 5747.5s

################################################################################
                     [1m Learning iteration 1760/4000 [0m

                       Computation: 3218 steps/s (collection: 0.474s, learning 2.071s)
               Value function loss: 96769.2386
                    Surrogate loss: 0.0145
             Mean action noise std: 0.90
                       Mean reward: 7076.36
               Mean episode length: 332.69
                 Mean success rate: 65.00
                  Mean reward/step: 22.08
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14426112
                    Iteration time: 2.55s
                        Total time: 4516.43s
                               ETA: 5744.9s

################################################################################
                     [1m Learning iteration 1761/4000 [0m

                       Computation: 3178 steps/s (collection: 0.468s, learning 2.109s)
               Value function loss: 91687.0452
                    Surrogate loss: 0.0133
             Mean action noise std: 0.90
                       Mean reward: 6803.72
               Mean episode length: 323.93
                 Mean success rate: 63.50
                  Mean reward/step: 22.29
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 2.58s
                        Total time: 4519.01s
                               ETA: 5742.4s

################################################################################
                     [1m Learning iteration 1762/4000 [0m

                       Computation: 3057 steps/s (collection: 0.546s, learning 2.134s)
               Value function loss: 86507.8731
                    Surrogate loss: 0.0135
             Mean action noise std: 0.90
                       Mean reward: 7223.53
               Mean episode length: 335.29
                 Mean success rate: 66.00
                  Mean reward/step: 21.66
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14442496
                    Iteration time: 2.68s
                        Total time: 4521.69s
                               ETA: 5740.0s

################################################################################
                     [1m Learning iteration 1763/4000 [0m

                       Computation: 3188 steps/s (collection: 0.449s, learning 2.120s)
               Value function loss: 98376.5807
                    Surrogate loss: 0.0123
             Mean action noise std: 0.90
                       Mean reward: 7266.54
               Mean episode length: 339.29
                 Mean success rate: 66.00
                  Mean reward/step: 21.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 2.57s
                        Total time: 4524.26s
                               ETA: 5737.4s

################################################################################
                     [1m Learning iteration 1764/4000 [0m

                       Computation: 3229 steps/s (collection: 0.461s, learning 2.076s)
               Value function loss: 74378.9523
                    Surrogate loss: 0.0164
             Mean action noise std: 0.90
                       Mean reward: 7272.38
               Mean episode length: 340.29
                 Mean success rate: 65.50
                  Mean reward/step: 20.98
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14458880
                    Iteration time: 2.54s
                        Total time: 4526.80s
                               ETA: 5734.8s

################################################################################
                     [1m Learning iteration 1765/4000 [0m

                       Computation: 3107 steps/s (collection: 0.491s, learning 2.146s)
               Value function loss: 100183.3713
                    Surrogate loss: 0.0150
             Mean action noise std: 0.90
                       Mean reward: 7747.97
               Mean episode length: 353.45
                 Mean success rate: 68.50
                  Mean reward/step: 21.69
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 2.64s
                        Total time: 4529.43s
                               ETA: 5732.3s

################################################################################
                     [1m Learning iteration 1766/4000 [0m

                       Computation: 3240 steps/s (collection: 0.464s, learning 2.064s)
               Value function loss: 78929.3023
                    Surrogate loss: 0.0168
             Mean action noise std: 0.90
                       Mean reward: 7842.18
               Mean episode length: 354.69
                 Mean success rate: 68.50
                  Mean reward/step: 22.08
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14475264
                    Iteration time: 2.53s
                        Total time: 4531.96s
                               ETA: 5729.7s

################################################################################
                     [1m Learning iteration 1767/4000 [0m

                       Computation: 3207 steps/s (collection: 0.476s, learning 2.078s)
               Value function loss: 70422.4252
                    Surrogate loss: 0.0144
             Mean action noise std: 0.90
                       Mean reward: 8239.81
               Mean episode length: 366.39
                 Mean success rate: 71.00
                  Mean reward/step: 22.06
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 2.55s
                        Total time: 4534.52s
                               ETA: 5727.1s

################################################################################
                     [1m Learning iteration 1768/4000 [0m

                       Computation: 3239 steps/s (collection: 0.449s, learning 2.079s)
               Value function loss: 83650.3447
                    Surrogate loss: 0.0155
             Mean action noise std: 0.90
                       Mean reward: 7758.60
               Mean episode length: 350.44
                 Mean success rate: 68.00
                  Mean reward/step: 21.94
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 14491648
                    Iteration time: 2.53s
                        Total time: 4537.04s
                               ETA: 5724.5s

################################################################################
                     [1m Learning iteration 1769/4000 [0m

                       Computation: 3262 steps/s (collection: 0.470s, learning 2.041s)
               Value function loss: 69289.4630
                    Surrogate loss: 0.0144
             Mean action noise std: 0.90
                       Mean reward: 7721.53
               Mean episode length: 350.12
                 Mean success rate: 67.50
                  Mean reward/step: 22.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 2.51s
                        Total time: 4539.56s
                               ETA: 5721.9s

################################################################################
                     [1m Learning iteration 1770/4000 [0m

                       Computation: 3113 steps/s (collection: 0.501s, learning 2.129s)
               Value function loss: 110611.4812
                    Surrogate loss: 0.0151
             Mean action noise std: 0.90
                       Mean reward: 7738.69
               Mean episode length: 358.49
                 Mean success rate: 69.00
                  Mean reward/step: 21.90
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14508032
                    Iteration time: 2.63s
                        Total time: 4542.19s
                               ETA: 5719.4s

################################################################################
                     [1m Learning iteration 1771/4000 [0m

                       Computation: 3199 steps/s (collection: 0.494s, learning 2.066s)
               Value function loss: 88832.1889
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 7805.06
               Mean episode length: 360.54
                 Mean success rate: 69.50
                  Mean reward/step: 21.79
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 2.56s
                        Total time: 4544.75s
                               ETA: 5716.8s

################################################################################
                     [1m Learning iteration 1772/4000 [0m

                       Computation: 3253 steps/s (collection: 0.468s, learning 2.049s)
               Value function loss: 78971.9256
                    Surrogate loss: 0.0152
             Mean action noise std: 0.90
                       Mean reward: 7615.61
               Mean episode length: 352.64
                 Mean success rate: 67.50
                  Mean reward/step: 21.28
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14524416
                    Iteration time: 2.52s
                        Total time: 4547.26s
                               ETA: 5714.2s

################################################################################
                     [1m Learning iteration 1773/4000 [0m

                       Computation: 3222 steps/s (collection: 0.467s, learning 2.076s)
               Value function loss: 66332.4561
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 7583.62
               Mean episode length: 348.98
                 Mean success rate: 66.50
                  Mean reward/step: 21.66
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 2.54s
                        Total time: 4549.81s
                               ETA: 5711.6s

################################################################################
                     [1m Learning iteration 1774/4000 [0m

                       Computation: 3143 steps/s (collection: 0.515s, learning 2.091s)
               Value function loss: 107813.3764
                    Surrogate loss: 0.0149
             Mean action noise std: 0.90
                       Mean reward: 7407.83
               Mean episode length: 346.43
                 Mean success rate: 66.50
                  Mean reward/step: 21.53
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 14540800
                    Iteration time: 2.61s
                        Total time: 4552.41s
                               ETA: 5709.1s

################################################################################
                     [1m Learning iteration 1775/4000 [0m

                       Computation: 3200 steps/s (collection: 0.472s, learning 2.088s)
               Value function loss: 79161.0604
                    Surrogate loss: 0.0148
             Mean action noise std: 0.90
                       Mean reward: 7297.17
               Mean episode length: 344.06
                 Mean success rate: 66.00
                  Mean reward/step: 21.33
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 2.56s
                        Total time: 4554.97s
                               ETA: 5706.5s

################################################################################
                     [1m Learning iteration 1776/4000 [0m

                       Computation: 3147 steps/s (collection: 0.493s, learning 2.110s)
               Value function loss: 78646.8877
                    Surrogate loss: 0.0139
             Mean action noise std: 0.90
                       Mean reward: 7266.84
               Mean episode length: 342.17
                 Mean success rate: 65.00
                  Mean reward/step: 21.15
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14557184
                    Iteration time: 2.60s
                        Total time: 4557.57s
                               ETA: 5704.0s

################################################################################
                     [1m Learning iteration 1777/4000 [0m

                       Computation: 3181 steps/s (collection: 0.508s, learning 2.067s)
               Value function loss: 128108.1255
                    Surrogate loss: 0.0121
             Mean action noise std: 0.90
                       Mean reward: 7289.55
               Mean episode length: 343.11
                 Mean success rate: 64.50
                  Mean reward/step: 20.87
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 2.58s
                        Total time: 4560.15s
                               ETA: 5701.5s

################################################################################
                     [1m Learning iteration 1778/4000 [0m

                       Computation: 3259 steps/s (collection: 0.468s, learning 2.045s)
               Value function loss: 73039.9737
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 7427.28
               Mean episode length: 340.31
                 Mean success rate: 63.50
                  Mean reward/step: 20.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14573568
                    Iteration time: 2.51s
                        Total time: 4562.66s
                               ETA: 5698.8s

################################################################################
                     [1m Learning iteration 1779/4000 [0m

                       Computation: 3206 steps/s (collection: 0.482s, learning 2.073s)
               Value function loss: 95911.0535
                    Surrogate loss: 0.0126
             Mean action noise std: 0.90
                       Mean reward: 6894.72
               Mean episode length: 321.05
                 Mean success rate: 59.50
                  Mean reward/step: 20.20
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 2.55s
                        Total time: 4565.22s
                               ETA: 5696.3s

################################################################################
                     [1m Learning iteration 1780/4000 [0m

                       Computation: 3208 steps/s (collection: 0.491s, learning 2.062s)
               Value function loss: 58605.4755
                    Surrogate loss: 0.0153
             Mean action noise std: 0.90
                       Mean reward: 6924.97
               Mean episode length: 322.75
                 Mean success rate: 60.00
                  Mean reward/step: 20.11
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14589952
                    Iteration time: 2.55s
                        Total time: 4567.77s
                               ETA: 5693.7s

################################################################################
                     [1m Learning iteration 1781/4000 [0m

                       Computation: 3213 steps/s (collection: 0.492s, learning 2.057s)
               Value function loss: 75038.3579
                    Surrogate loss: 0.0184
             Mean action noise std: 0.90
                       Mean reward: 6844.01
               Mean episode length: 321.55
                 Mean success rate: 60.00
                  Mean reward/step: 21.37
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 2.55s
                        Total time: 4570.32s
                               ETA: 5691.1s

################################################################################
                     [1m Learning iteration 1782/4000 [0m

                       Computation: 3187 steps/s (collection: 0.492s, learning 2.078s)
               Value function loss: 75138.6741
                    Surrogate loss: 0.0135
             Mean action noise std: 0.90
                       Mean reward: 6884.48
               Mean episode length: 323.92
                 Mean success rate: 59.50
                  Mean reward/step: 21.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14606336
                    Iteration time: 2.57s
                        Total time: 4572.89s
                               ETA: 5688.5s

################################################################################
                     [1m Learning iteration 1783/4000 [0m

                       Computation: 3232 steps/s (collection: 0.462s, learning 2.072s)
               Value function loss: 73546.4094
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 7221.63
               Mean episode length: 334.88
                 Mean success rate: 62.00
                  Mean reward/step: 22.18
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 2.53s
                        Total time: 4575.42s
                               ETA: 5685.9s

################################################################################
                     [1m Learning iteration 1784/4000 [0m

                       Computation: 3202 steps/s (collection: 0.479s, learning 2.079s)
               Value function loss: 104880.9290
                    Surrogate loss: 0.0135
             Mean action noise std: 0.90
                       Mean reward: 7417.65
               Mean episode length: 342.17
                 Mean success rate: 64.00
                  Mean reward/step: 21.86
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14622720
                    Iteration time: 2.56s
                        Total time: 4577.98s
                               ETA: 5683.4s

################################################################################
                     [1m Learning iteration 1785/4000 [0m

                       Computation: 3248 steps/s (collection: 0.442s, learning 2.080s)
               Value function loss: 73158.9533
                    Surrogate loss: 0.0154
             Mean action noise std: 0.90
                       Mean reward: 7691.99
               Mean episode length: 348.35
                 Mean success rate: 65.50
                  Mean reward/step: 21.97
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 2.52s
                        Total time: 4580.50s
                               ETA: 5680.7s

################################################################################
                     [1m Learning iteration 1786/4000 [0m

                       Computation: 3198 steps/s (collection: 0.486s, learning 2.075s)
               Value function loss: 75747.0758
                    Surrogate loss: 0.0151
             Mean action noise std: 0.90
                       Mean reward: 7698.81
               Mean episode length: 348.00
                 Mean success rate: 65.50
                  Mean reward/step: 21.23
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14639104
                    Iteration time: 2.56s
                        Total time: 4583.07s
                               ETA: 5678.2s

################################################################################
                     [1m Learning iteration 1787/4000 [0m

                       Computation: 3217 steps/s (collection: 0.461s, learning 2.085s)
               Value function loss: 80938.5110
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 7998.39
               Mean episode length: 362.25
                 Mean success rate: 69.00
                  Mean reward/step: 21.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 2.55s
                        Total time: 4585.61s
                               ETA: 5675.6s

################################################################################
                     [1m Learning iteration 1788/4000 [0m

                       Computation: 3296 steps/s (collection: 0.442s, learning 2.043s)
               Value function loss: 71003.8885
                    Surrogate loss: 0.0176
             Mean action noise std: 0.90
                       Mean reward: 8019.38
               Mean episode length: 364.74
                 Mean success rate: 69.50
                  Mean reward/step: 21.01
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14655488
                    Iteration time: 2.49s
                        Total time: 4588.10s
                               ETA: 5672.9s

################################################################################
                     [1m Learning iteration 1789/4000 [0m

                       Computation: 3210 steps/s (collection: 0.471s, learning 2.081s)
               Value function loss: 72959.8001
                    Surrogate loss: 0.0130
             Mean action noise std: 0.90
                       Mean reward: 8045.87
               Mean episode length: 371.19
                 Mean success rate: 71.00
                  Mean reward/step: 20.91
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 2.55s
                        Total time: 4590.65s
                               ETA: 5670.3s

################################################################################
                     [1m Learning iteration 1790/4000 [0m

                       Computation: 3169 steps/s (collection: 0.478s, learning 2.106s)
               Value function loss: 65304.8914
                    Surrogate loss: 0.0139
             Mean action noise std: 0.90
                       Mean reward: 8205.53
               Mean episode length: 378.81
                 Mean success rate: 71.50
                  Mean reward/step: 20.83
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14671872
                    Iteration time: 2.58s
                        Total time: 4593.23s
                               ETA: 5667.8s

################################################################################
                     [1m Learning iteration 1791/4000 [0m

                       Computation: 3145 steps/s (collection: 0.491s, learning 2.113s)
               Value function loss: 78317.6222
                    Surrogate loss: 0.0140
             Mean action noise std: 0.90
                       Mean reward: 8403.13
               Mean episode length: 387.66
                 Mean success rate: 73.50
                  Mean reward/step: 21.74
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 2.60s
                        Total time: 4595.84s
                               ETA: 5665.3s

################################################################################
                     [1m Learning iteration 1792/4000 [0m

                       Computation: 3166 steps/s (collection: 0.492s, learning 2.095s)
               Value function loss: 94288.8011
                    Surrogate loss: 0.0127
             Mean action noise std: 0.90
                       Mean reward: 8597.08
               Mean episode length: 398.35
                 Mean success rate: 76.50
                  Mean reward/step: 22.05
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14688256
                    Iteration time: 2.59s
                        Total time: 4598.42s
                               ETA: 5662.8s

################################################################################
                     [1m Learning iteration 1793/4000 [0m

                       Computation: 3229 steps/s (collection: 0.461s, learning 2.076s)
               Value function loss: 95291.1226
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 8589.03
               Mean episode length: 398.31
                 Mean success rate: 76.50
                  Mean reward/step: 21.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 2.54s
                        Total time: 4600.96s
                               ETA: 5660.2s

################################################################################
                     [1m Learning iteration 1794/4000 [0m

                       Computation: 3155 steps/s (collection: 0.487s, learning 2.109s)
               Value function loss: 116504.2861
                    Surrogate loss: 0.0129
             Mean action noise std: 0.90
                       Mean reward: 8574.07
               Mean episode length: 401.04
                 Mean success rate: 77.00
                  Mean reward/step: 20.40
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 14704640
                    Iteration time: 2.60s
                        Total time: 4603.56s
                               ETA: 5657.6s

################################################################################
                     [1m Learning iteration 1795/4000 [0m

                       Computation: 3131 steps/s (collection: 0.563s, learning 2.053s)
               Value function loss: 96030.7479
                    Surrogate loss: 0.0135
             Mean action noise std: 0.90
                       Mean reward: 8264.87
               Mean episode length: 393.65
                 Mean success rate: 75.50
                  Mean reward/step: 19.14
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 2.62s
                        Total time: 4606.17s
                               ETA: 5655.1s

################################################################################
                     [1m Learning iteration 1796/4000 [0m

                       Computation: 3250 steps/s (collection: 0.439s, learning 2.081s)
               Value function loss: 76984.2712
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 7961.72
               Mean episode length: 380.25
                 Mean success rate: 73.50
                  Mean reward/step: 19.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14721024
                    Iteration time: 2.52s
                        Total time: 4608.69s
                               ETA: 5652.5s

################################################################################
                     [1m Learning iteration 1797/4000 [0m

                       Computation: 3228 steps/s (collection: 0.460s, learning 2.078s)
               Value function loss: 103750.5838
                    Surrogate loss: 0.0126
             Mean action noise std: 0.90
                       Mean reward: 8225.98
               Mean episode length: 388.52
                 Mean success rate: 75.00
                  Mean reward/step: 19.94
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 2.54s
                        Total time: 4611.23s
                               ETA: 5649.9s

################################################################################
                     [1m Learning iteration 1798/4000 [0m

                       Computation: 3205 steps/s (collection: 0.498s, learning 2.057s)
               Value function loss: 61339.5820
                    Surrogate loss: 0.0144
             Mean action noise std: 0.90
                       Mean reward: 7934.21
               Mean episode length: 376.73
                 Mean success rate: 73.00
                  Mean reward/step: 19.60
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14737408
                    Iteration time: 2.56s
                        Total time: 4613.78s
                               ETA: 5647.3s

################################################################################
                     [1m Learning iteration 1799/4000 [0m

                       Computation: 3178 steps/s (collection: 0.497s, learning 2.080s)
               Value function loss: 70549.8534
                    Surrogate loss: 0.0145
             Mean action noise std: 0.90
                       Mean reward: 7491.25
               Mean episode length: 360.64
                 Mean success rate: 69.50
                  Mean reward/step: 19.95
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 2.58s
                        Total time: 4616.36s
                               ETA: 5644.8s

################################################################################
                     [1m Learning iteration 1800/4000 [0m

                       Computation: 3297 steps/s (collection: 0.455s, learning 2.030s)
               Value function loss: 66727.9697
                    Surrogate loss: 0.0142
             Mean action noise std: 0.90
                       Mean reward: 7458.52
               Mean episode length: 359.61
                 Mean success rate: 69.50
                  Mean reward/step: 20.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14753792
                    Iteration time: 2.48s
                        Total time: 4618.85s
                               ETA: 5642.1s

################################################################################
                     [1m Learning iteration 1801/4000 [0m

                       Computation: 3295 steps/s (collection: 0.450s, learning 2.036s)
               Value function loss: 71710.4161
                    Surrogate loss: 0.0154
             Mean action noise std: 0.90
                       Mean reward: 7343.66
               Mean episode length: 356.50
                 Mean success rate: 68.50
                  Mean reward/step: 21.17
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 2.49s
                        Total time: 4621.33s
                               ETA: 5639.5s

################################################################################
                     [1m Learning iteration 1802/4000 [0m

                       Computation: 3284 steps/s (collection: 0.429s, learning 2.065s)
               Value function loss: 71383.5945
                    Surrogate loss: 0.0157
             Mean action noise std: 0.90
                       Mean reward: 7230.60
               Mean episode length: 354.45
                 Mean success rate: 68.00
                  Mean reward/step: 21.15
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14770176
                    Iteration time: 2.49s
                        Total time: 4623.83s
                               ETA: 5636.8s

################################################################################
                     [1m Learning iteration 1803/4000 [0m

                       Computation: 3291 steps/s (collection: 0.422s, learning 2.067s)
               Value function loss: 81623.9139
                    Surrogate loss: 0.0152
             Mean action noise std: 0.90
                       Mean reward: 7264.26
               Mean episode length: 357.04
                 Mean success rate: 68.50
                  Mean reward/step: 20.88
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 2.49s
                        Total time: 4626.31s
                               ETA: 5634.2s

################################################################################
                     [1m Learning iteration 1804/4000 [0m

                       Computation: 3259 steps/s (collection: 0.459s, learning 2.054s)
               Value function loss: 110746.7753
                    Surrogate loss: 0.0161
             Mean action noise std: 0.90
                       Mean reward: 7182.43
               Mean episode length: 354.41
                 Mean success rate: 68.50
                  Mean reward/step: 20.53
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14786560
                    Iteration time: 2.51s
                        Total time: 4628.83s
                               ETA: 5631.5s

################################################################################
                     [1m Learning iteration 1805/4000 [0m

                       Computation: 3220 steps/s (collection: 0.453s, learning 2.090s)
               Value function loss: 84513.8896
                    Surrogate loss: 0.0177
             Mean action noise std: 0.90
                       Mean reward: 6912.21
               Mean episode length: 347.38
                 Mean success rate: 66.50
                  Mean reward/step: 19.83
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 2.54s
                        Total time: 4631.37s
                               ETA: 5628.9s

################################################################################
                     [1m Learning iteration 1806/4000 [0m

                       Computation: 3185 steps/s (collection: 0.482s, learning 2.090s)
               Value function loss: 70556.8962
                    Surrogate loss: 0.0175
             Mean action noise std: 0.90
                       Mean reward: 6799.28
               Mean episode length: 346.24
                 Mean success rate: 66.50
                  Mean reward/step: 19.78
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14802944
                    Iteration time: 2.57s
                        Total time: 4633.94s
                               ETA: 5626.4s

################################################################################
                     [1m Learning iteration 1807/4000 [0m

                       Computation: 3273 steps/s (collection: 0.473s, learning 2.029s)
               Value function loss: 71290.7276
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 6707.70
               Mean episode length: 341.10
                 Mean success rate: 65.50
                  Mean reward/step: 20.64
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 2.50s
                        Total time: 4636.45s
                               ETA: 5623.7s

################################################################################
                     [1m Learning iteration 1808/4000 [0m

                       Computation: 3236 steps/s (collection: 0.469s, learning 2.063s)
               Value function loss: 104829.0150
                    Surrogate loss: 0.0149
             Mean action noise std: 0.90
                       Mean reward: 6878.27
               Mean episode length: 341.06
                 Mean success rate: 66.00
                  Mean reward/step: 20.93
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14819328
                    Iteration time: 2.53s
                        Total time: 4638.98s
                               ETA: 5621.1s

################################################################################
                     [1m Learning iteration 1809/4000 [0m

                       Computation: 3207 steps/s (collection: 0.459s, learning 2.095s)
               Value function loss: 85229.8663
                    Surrogate loss: 0.0152
             Mean action noise std: 0.90
                       Mean reward: 6910.61
               Mean episode length: 339.12
                 Mean success rate: 65.50
                  Mean reward/step: 20.58
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 2.55s
                        Total time: 4641.53s
                               ETA: 5618.6s

################################################################################
                     [1m Learning iteration 1810/4000 [0m

                       Computation: 3225 steps/s (collection: 0.480s, learning 2.060s)
               Value function loss: 97840.0934
                    Surrogate loss: 0.0138
             Mean action noise std: 0.90
                       Mean reward: 6834.70
               Mean episode length: 334.55
                 Mean success rate: 65.00
                  Mean reward/step: 20.87
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14835712
                    Iteration time: 2.54s
                        Total time: 4644.07s
                               ETA: 5616.0s

################################################################################
                     [1m Learning iteration 1811/4000 [0m

                       Computation: 3228 steps/s (collection: 0.485s, learning 2.052s)
               Value function loss: 80379.9078
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 6439.88
               Mean episode length: 318.62
                 Mean success rate: 61.00
                  Mean reward/step: 20.85
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 2.54s
                        Total time: 4646.61s
                               ETA: 5613.4s

################################################################################
                     [1m Learning iteration 1812/4000 [0m

                       Computation: 3251 steps/s (collection: 0.459s, learning 2.060s)
               Value function loss: 63095.4048
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 6710.23
               Mean episode length: 329.00
                 Mean success rate: 64.50
                  Mean reward/step: 20.77
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14852096
                    Iteration time: 2.52s
                        Total time: 4649.13s
                               ETA: 5610.8s

################################################################################
                     [1m Learning iteration 1813/4000 [0m

                       Computation: 3262 steps/s (collection: 0.460s, learning 2.051s)
               Value function loss: 77696.1776
                    Surrogate loss: 0.0143
             Mean action noise std: 0.90
                       Mean reward: 6547.18
               Mean episode length: 321.19
                 Mean success rate: 62.00
                  Mean reward/step: 20.93
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 2.51s
                        Total time: 4651.64s
                               ETA: 5608.1s

################################################################################
                     [1m Learning iteration 1814/4000 [0m

                       Computation: 3257 steps/s (collection: 0.460s, learning 2.054s)
               Value function loss: 64959.8386
                    Surrogate loss: 0.0127
             Mean action noise std: 0.90
                       Mean reward: 6689.68
               Mean episode length: 323.08
                 Mean success rate: 62.50
                  Mean reward/step: 20.90
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14868480
                    Iteration time: 2.51s
                        Total time: 4654.15s
                               ETA: 5605.5s

################################################################################
                     [1m Learning iteration 1815/4000 [0m

                       Computation: 3300 steps/s (collection: 0.459s, learning 2.024s)
               Value function loss: 91093.5360
                    Surrogate loss: 0.0156
             Mean action noise std: 0.90
                       Mean reward: 7073.68
               Mean episode length: 337.97
                 Mean success rate: 65.50
                  Mean reward/step: 21.07
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 2.48s
                        Total time: 4656.64s
                               ETA: 5602.8s

################################################################################
                     [1m Learning iteration 1816/4000 [0m

                       Computation: 3218 steps/s (collection: 0.470s, learning 2.075s)
               Value function loss: 65683.2427
                    Surrogate loss: 0.0129
             Mean action noise std: 0.90
                       Mean reward: 6989.42
               Mean episode length: 334.76
                 Mean success rate: 65.50
                  Mean reward/step: 20.59
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14884864
                    Iteration time: 2.55s
                        Total time: 4659.18s
                               ETA: 5600.2s

################################################################################
                     [1m Learning iteration 1817/4000 [0m

                       Computation: 3300 steps/s (collection: 0.434s, learning 2.048s)
               Value function loss: 75790.8408
                    Surrogate loss: 0.0140
             Mean action noise std: 0.90
                       Mean reward: 7015.43
               Mean episode length: 334.24
                 Mean success rate: 66.00
                  Mean reward/step: 21.46
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 2.48s
                        Total time: 4661.66s
                               ETA: 5597.6s

################################################################################
                     [1m Learning iteration 1818/4000 [0m

                       Computation: 3252 steps/s (collection: 0.473s, learning 2.045s)
               Value function loss: 75543.4504
                    Surrogate loss: 0.0139
             Mean action noise std: 0.90
                       Mean reward: 6837.80
               Mean episode length: 328.40
                 Mean success rate: 65.00
                  Mean reward/step: 21.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14901248
                    Iteration time: 2.52s
                        Total time: 4664.18s
                               ETA: 5595.0s

################################################################################
                     [1m Learning iteration 1819/4000 [0m

                       Computation: 3181 steps/s (collection: 0.502s, learning 2.073s)
               Value function loss: 77320.7600
                    Surrogate loss: 0.0146
             Mean action noise std: 0.90
                       Mean reward: 6666.71
               Mean episode length: 316.69
                 Mean success rate: 63.00
                  Mean reward/step: 21.97
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 2.57s
                        Total time: 4666.76s
                               ETA: 5592.4s

################################################################################
                     [1m Learning iteration 1820/4000 [0m

                       Computation: 3297 steps/s (collection: 0.437s, learning 2.047s)
               Value function loss: 101420.6331
                    Surrogate loss: 0.0129
             Mean action noise std: 0.90
                       Mean reward: 7173.27
               Mean episode length: 332.13
                 Mean success rate: 66.00
                  Mean reward/step: 22.20
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14917632
                    Iteration time: 2.48s
                        Total time: 4669.24s
                               ETA: 5589.8s

################################################################################
                     [1m Learning iteration 1821/4000 [0m

                       Computation: 3230 steps/s (collection: 0.467s, learning 2.069s)
               Value function loss: 55293.3516
                    Surrogate loss: 0.0167
             Mean action noise std: 0.90
                       Mean reward: 7143.90
               Mean episode length: 330.98
                 Mean success rate: 66.00
                  Mean reward/step: 21.44
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 2.54s
                        Total time: 4671.78s
                               ETA: 5587.2s

################################################################################
                     [1m Learning iteration 1822/4000 [0m

                       Computation: 3204 steps/s (collection: 0.466s, learning 2.090s)
               Value function loss: 71338.0740
                    Surrogate loss: 0.0149
             Mean action noise std: 0.90
                       Mean reward: 7247.26
               Mean episode length: 338.83
                 Mean success rate: 68.00
                  Mean reward/step: 21.85
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14934016
                    Iteration time: 2.56s
                        Total time: 4674.33s
                               ETA: 5584.6s

################################################################################
                     [1m Learning iteration 1823/4000 [0m

                       Computation: 3225 steps/s (collection: 0.485s, learning 2.055s)
               Value function loss: 82271.6839
                    Surrogate loss: 0.0146
             Mean action noise std: 0.90
                       Mean reward: 7485.92
               Mean episode length: 352.21
                 Mean success rate: 69.00
                  Mean reward/step: 21.98
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.54s
                        Total time: 4676.87s
                               ETA: 5582.0s

################################################################################
                     [1m Learning iteration 1824/4000 [0m

                       Computation: 3235 steps/s (collection: 0.442s, learning 2.090s)
               Value function loss: 104651.2896
                    Surrogate loss: 0.0142
             Mean action noise std: 0.90
                       Mean reward: 7293.21
               Mean episode length: 344.15
                 Mean success rate: 67.00
                  Mean reward/step: 21.25
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14950400
                    Iteration time: 2.53s
                        Total time: 4679.40s
                               ETA: 5579.4s

################################################################################
                     [1m Learning iteration 1825/4000 [0m

                       Computation: 3213 steps/s (collection: 0.451s, learning 2.098s)
               Value function loss: 98817.3097
                    Surrogate loss: 0.0152
             Mean action noise std: 0.90
                       Mean reward: 7762.12
               Mean episode length: 359.66
                 Mean success rate: 69.50
                  Mean reward/step: 20.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 2.55s
                        Total time: 4681.95s
                               ETA: 5576.8s

################################################################################
                     [1m Learning iteration 1826/4000 [0m

                       Computation: 3278 steps/s (collection: 0.433s, learning 2.066s)
               Value function loss: 90707.6005
                    Surrogate loss: 0.0142
             Mean action noise std: 0.90
                       Mean reward: 7809.48
               Mean episode length: 360.45
                 Mean success rate: 69.50
                  Mean reward/step: 19.93
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14966784
                    Iteration time: 2.50s
                        Total time: 4684.45s
                               ETA: 5574.2s

################################################################################
                     [1m Learning iteration 1827/4000 [0m

                       Computation: 3142 steps/s (collection: 0.486s, learning 2.121s)
               Value function loss: 75911.3145
                    Surrogate loss: 0.0192
             Mean action noise std: 0.90
                       Mean reward: 7937.93
               Mean episode length: 368.27
                 Mean success rate: 70.50
                  Mean reward/step: 20.10
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 2.61s
                        Total time: 4687.06s
                               ETA: 5571.7s

################################################################################
                     [1m Learning iteration 1828/4000 [0m

                       Computation: 3086 steps/s (collection: 0.499s, learning 2.155s)
               Value function loss: 81001.4313
                    Surrogate loss: 0.0139
             Mean action noise std: 0.90
                       Mean reward: 7249.90
               Mean episode length: 347.61
                 Mean success rate: 64.50
                  Mean reward/step: 20.45
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 14983168
                    Iteration time: 2.65s
                        Total time: 4689.71s
                               ETA: 5569.2s

################################################################################
                     [1m Learning iteration 1829/4000 [0m

                       Computation: 3186 steps/s (collection: 0.472s, learning 2.099s)
               Value function loss: 81798.1207
                    Surrogate loss: 0.0161
             Mean action noise std: 0.90
                       Mean reward: 7416.56
               Mean episode length: 357.60
                 Mean success rate: 64.50
                  Mean reward/step: 19.84
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 2.57s
                        Total time: 4692.28s
                               ETA: 5566.6s

################################################################################
                     [1m Learning iteration 1830/4000 [0m

                       Computation: 3213 steps/s (collection: 0.428s, learning 2.121s)
               Value function loss: 61746.7517
                    Surrogate loss: 0.0147
             Mean action noise std: 0.90
                       Mean reward: 7399.20
               Mean episode length: 354.54
                 Mean success rate: 63.50
                  Mean reward/step: 20.13
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14999552
                    Iteration time: 2.55s
                        Total time: 4694.83s
                               ETA: 5564.1s

################################################################################
                     [1m Learning iteration 1831/4000 [0m

                       Computation: 3200 steps/s (collection: 0.479s, learning 2.081s)
               Value function loss: 86981.0488
                    Surrogate loss: 0.0152
             Mean action noise std: 0.90
                       Mean reward: 7477.85
               Mean episode length: 350.36
                 Mean success rate: 64.00
                  Mean reward/step: 20.38
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 2.56s
                        Total time: 4697.39s
                               ETA: 5561.5s

################################################################################
                     [1m Learning iteration 1832/4000 [0m

                       Computation: 3213 steps/s (collection: 0.461s, learning 2.088s)
               Value function loss: 60357.9545
                    Surrogate loss: 0.0166
             Mean action noise std: 0.90
                       Mean reward: 7175.70
               Mean episode length: 341.78
                 Mean success rate: 62.50
                  Mean reward/step: 21.19
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15015936
                    Iteration time: 2.55s
                        Total time: 4699.94s
                               ETA: 5558.9s

################################################################################
                     [1m Learning iteration 1833/4000 [0m

                       Computation: 3211 steps/s (collection: 0.463s, learning 2.088s)
               Value function loss: 61005.1262
                    Surrogate loss: 0.0145
             Mean action noise std: 0.90
                       Mean reward: 6767.93
               Mean episode length: 325.98
                 Mean success rate: 59.00
                  Mean reward/step: 21.80
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 2.55s
                        Total time: 4702.49s
                               ETA: 5556.3s

################################################################################
                     [1m Learning iteration 1834/4000 [0m

                       Computation: 3171 steps/s (collection: 0.448s, learning 2.135s)
               Value function loss: 79051.7067
                    Surrogate loss: 0.0147
             Mean action noise std: 0.90
                       Mean reward: 6496.41
               Mean episode length: 321.38
                 Mean success rate: 57.50
                  Mean reward/step: 22.42
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15032320
                    Iteration time: 2.58s
                        Total time: 4705.08s
                               ETA: 5553.8s

################################################################################
                     [1m Learning iteration 1835/4000 [0m

                       Computation: 3217 steps/s (collection: 0.461s, learning 2.085s)
               Value function loss: 91882.8609
                    Surrogate loss: 0.0128
             Mean action noise std: 0.90
                       Mean reward: 6927.58
               Mean episode length: 335.99
                 Mean success rate: 60.50
                  Mean reward/step: 22.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 2.55s
                        Total time: 4707.62s
                               ETA: 5551.2s

################################################################################
                     [1m Learning iteration 1836/4000 [0m

                       Computation: 3176 steps/s (collection: 0.479s, learning 2.100s)
               Value function loss: 114057.4838
                    Surrogate loss: 0.0158
             Mean action noise std: 0.90
                       Mean reward: 7270.12
               Mean episode length: 346.80
                 Mean success rate: 64.50
                  Mean reward/step: 22.19
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15048704
                    Iteration time: 2.58s
                        Total time: 4710.20s
                               ETA: 5548.7s

################################################################################
                     [1m Learning iteration 1837/4000 [0m

                       Computation: 3188 steps/s (collection: 0.458s, learning 2.111s)
               Value function loss: 80785.5656
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 7236.40
               Mean episode length: 347.85
                 Mean success rate: 65.00
                  Mean reward/step: 21.95
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 2.57s
                        Total time: 4712.77s
                               ETA: 5546.1s

################################################################################
                     [1m Learning iteration 1838/4000 [0m

                       Computation: 3022 steps/s (collection: 0.539s, learning 2.171s)
               Value function loss: 58562.8912
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 7054.83
               Mean episode length: 340.75
                 Mean success rate: 65.50
                  Mean reward/step: 22.24
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15065088
                    Iteration time: 2.71s
                        Total time: 4715.48s
                               ETA: 5543.7s

################################################################################
                     [1m Learning iteration 1839/4000 [0m

                       Computation: 3085 steps/s (collection: 0.531s, learning 2.125s)
               Value function loss: 105033.1060
                    Surrogate loss: 0.0152
             Mean action noise std: 0.90
                       Mean reward: 6820.66
               Mean episode length: 330.36
                 Mean success rate: 63.50
                  Mean reward/step: 22.61
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 2.66s
                        Total time: 4718.14s
                               ETA: 5541.2s

################################################################################
                     [1m Learning iteration 1840/4000 [0m

                       Computation: 3192 steps/s (collection: 0.455s, learning 2.111s)
               Value function loss: 75452.1953
                    Surrogate loss: 0.0156
             Mean action noise std: 0.90
                       Mean reward: 7191.95
               Mean episode length: 340.49
                 Mean success rate: 66.00
                  Mean reward/step: 22.14
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15081472
                    Iteration time: 2.57s
                        Total time: 4720.70s
                               ETA: 5538.7s

################################################################################
                     [1m Learning iteration 1841/4000 [0m

                       Computation: 3174 steps/s (collection: 0.473s, learning 2.107s)
               Value function loss: 79559.0878
                    Surrogate loss: 0.0153
             Mean action noise std: 0.90
                       Mean reward: 7280.47
               Mean episode length: 344.95
                 Mean success rate: 66.50
                  Mean reward/step: 22.49
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 2.58s
                        Total time: 4723.28s
                               ETA: 5536.1s

################################################################################
                     [1m Learning iteration 1842/4000 [0m

                       Computation: 3118 steps/s (collection: 0.474s, learning 2.154s)
               Value function loss: 83634.8920
                    Surrogate loss: 0.0172
             Mean action noise std: 0.90
                       Mean reward: 7277.61
               Mean episode length: 341.49
                 Mean success rate: 66.50
                  Mean reward/step: 22.90
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15097856
                    Iteration time: 2.63s
                        Total time: 4725.91s
                               ETA: 5533.6s

################################################################################
                     [1m Learning iteration 1843/4000 [0m

                       Computation: 3047 steps/s (collection: 0.532s, learning 2.156s)
               Value function loss: 98026.4311
                    Surrogate loss: 0.0168
             Mean action noise std: 0.90
                       Mean reward: 7159.01
               Mean episode length: 336.81
                 Mean success rate: 65.50
                  Mean reward/step: 22.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 2.69s
                        Total time: 4728.60s
                               ETA: 5531.2s

################################################################################
                     [1m Learning iteration 1844/4000 [0m

                       Computation: 3119 steps/s (collection: 0.533s, learning 2.093s)
               Value function loss: 113336.7718
                    Surrogate loss: 0.0146
             Mean action noise std: 0.90
                       Mean reward: 7153.21
               Mean episode length: 335.58
                 Mean success rate: 65.50
                  Mean reward/step: 22.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15114240
                    Iteration time: 2.63s
                        Total time: 4731.22s
                               ETA: 5528.7s

################################################################################
                     [1m Learning iteration 1845/4000 [0m

                       Computation: 3190 steps/s (collection: 0.456s, learning 2.112s)
               Value function loss: 88157.2818
                    Surrogate loss: 0.0140
             Mean action noise std: 0.90
                       Mean reward: 7185.80
               Mean episode length: 327.49
                 Mean success rate: 64.00
                  Mean reward/step: 21.70
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 2.57s
                        Total time: 4733.79s
                               ETA: 5526.2s

################################################################################
                     [1m Learning iteration 1846/4000 [0m

                       Computation: 3144 steps/s (collection: 0.483s, learning 2.122s)
               Value function loss: 84233.7209
                    Surrogate loss: 0.0191
             Mean action noise std: 0.90
                       Mean reward: 7331.70
               Mean episode length: 332.70
                 Mean success rate: 65.00
                  Mean reward/step: 21.31
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15130624
                    Iteration time: 2.60s
                        Total time: 4736.40s
                               ETA: 5523.7s

################################################################################
                     [1m Learning iteration 1847/4000 [0m

                       Computation: 3121 steps/s (collection: 0.528s, learning 2.096s)
               Value function loss: 88667.0892
                    Surrogate loss: 0.0156
             Mean action noise std: 0.90
                       Mean reward: 7306.26
               Mean episode length: 329.82
                 Mean success rate: 64.50
                  Mean reward/step: 21.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 2.62s
                        Total time: 4739.02s
                               ETA: 5521.2s

################################################################################
                     [1m Learning iteration 1848/4000 [0m

                       Computation: 3294 steps/s (collection: 0.461s, learning 2.026s)
               Value function loss: 66008.2094
                    Surrogate loss: 0.0130
             Mean action noise std: 0.90
                       Mean reward: 7272.29
               Mean episode length: 328.65
                 Mean success rate: 64.50
                  Mean reward/step: 21.44
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15147008
                    Iteration time: 2.49s
                        Total time: 4741.51s
                               ETA: 5518.5s

################################################################################
                     [1m Learning iteration 1849/4000 [0m

                       Computation: 3248 steps/s (collection: 0.460s, learning 2.062s)
               Value function loss: 72278.2236
                    Surrogate loss: 0.0123
             Mean action noise std: 0.90
                       Mean reward: 6976.87
               Mean episode length: 315.06
                 Mean success rate: 63.00
                  Mean reward/step: 21.51
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 2.52s
                        Total time: 4744.03s
                               ETA: 5515.9s

################################################################################
                     [1m Learning iteration 1850/4000 [0m

                       Computation: 3214 steps/s (collection: 0.474s, learning 2.074s)
               Value function loss: 93990.1252
                    Surrogate loss: 0.0151
             Mean action noise std: 0.90
                       Mean reward: 6819.69
               Mean episode length: 310.67
                 Mean success rate: 62.50
                  Mean reward/step: 21.69
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15163392
                    Iteration time: 2.55s
                        Total time: 4746.58s
                               ETA: 5513.3s

################################################################################
                     [1m Learning iteration 1851/4000 [0m

                       Computation: 3236 steps/s (collection: 0.473s, learning 2.058s)
               Value function loss: 120827.9766
                    Surrogate loss: 0.0148
             Mean action noise std: 0.90
                       Mean reward: 6866.53
               Mean episode length: 308.85
                 Mean success rate: 62.00
                  Mean reward/step: 21.77
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 2.53s
                        Total time: 4749.11s
                               ETA: 5510.7s

################################################################################
                     [1m Learning iteration 1852/4000 [0m

                       Computation: 3311 steps/s (collection: 0.425s, learning 2.049s)
               Value function loss: 87976.7003
                    Surrogate loss: 0.0169
             Mean action noise std: 0.90
                       Mean reward: 7202.78
               Mean episode length: 323.88
                 Mean success rate: 65.00
                  Mean reward/step: 21.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15179776
                    Iteration time: 2.47s
                        Total time: 4751.58s
                               ETA: 5508.0s

################################################################################
                     [1m Learning iteration 1853/4000 [0m

                       Computation: 3228 steps/s (collection: 0.480s, learning 2.057s)
               Value function loss: 63822.2218
                    Surrogate loss: 0.0157
             Mean action noise std: 0.90
                       Mean reward: 6998.77
               Mean episode length: 317.87
                 Mean success rate: 63.50
                  Mean reward/step: 22.13
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 2.54s
                        Total time: 4754.12s
                               ETA: 5505.4s

################################################################################
                     [1m Learning iteration 1854/4000 [0m

                       Computation: 3223 steps/s (collection: 0.470s, learning 2.071s)
               Value function loss: 109394.1627
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 6901.74
               Mean episode length: 313.11
                 Mean success rate: 61.50
                  Mean reward/step: 21.89
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 15196160
                    Iteration time: 2.54s
                        Total time: 4756.66s
                               ETA: 5502.9s

################################################################################
                     [1m Learning iteration 1855/4000 [0m

                       Computation: 3225 steps/s (collection: 0.469s, learning 2.071s)
               Value function loss: 89559.7252
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 7022.65
               Mean episode length: 316.85
                 Mean success rate: 62.50
                  Mean reward/step: 21.28
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 2.54s
                        Total time: 4759.20s
                               ETA: 5500.3s

################################################################################
                     [1m Learning iteration 1856/4000 [0m

                       Computation: 3309 steps/s (collection: 0.423s, learning 2.052s)
               Value function loss: 65858.9505
                    Surrogate loss: 0.0174
             Mean action noise std: 0.90
                       Mean reward: 6998.04
               Mean episode length: 320.48
                 Mean success rate: 62.00
                  Mean reward/step: 21.28
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15212544
                    Iteration time: 2.48s
                        Total time: 4761.67s
                               ETA: 5497.6s

################################################################################
                     [1m Learning iteration 1857/4000 [0m

                       Computation: 3237 steps/s (collection: 0.478s, learning 2.053s)
               Value function loss: 72743.5846
                    Surrogate loss: 0.0180
             Mean action noise std: 0.90
                       Mean reward: 7397.28
               Mean episode length: 332.93
                 Mean success rate: 65.00
                  Mean reward/step: 21.83
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 2.53s
                        Total time: 4764.20s
                               ETA: 5495.0s

################################################################################
                     [1m Learning iteration 1858/4000 [0m

                       Computation: 3241 steps/s (collection: 0.474s, learning 2.053s)
               Value function loss: 70973.5983
                    Surrogate loss: 0.0140
             Mean action noise std: 0.90
                       Mean reward: 7088.37
               Mean episode length: 320.80
                 Mean success rate: 62.00
                  Mean reward/step: 22.06
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 15228928
                    Iteration time: 2.53s
                        Total time: 4766.73s
                               ETA: 5492.4s

################################################################################
                     [1m Learning iteration 1859/4000 [0m

                       Computation: 3239 steps/s (collection: 0.453s, learning 2.076s)
               Value function loss: 65455.3497
                    Surrogate loss: 0.0140
             Mean action noise std: 0.90
                       Mean reward: 6458.97
               Mean episode length: 307.00
                 Mean success rate: 58.00
                  Mean reward/step: 22.75
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 2.53s
                        Total time: 4769.26s
                               ETA: 5489.8s

################################################################################
                     [1m Learning iteration 1860/4000 [0m

                       Computation: 3188 steps/s (collection: 0.486s, learning 2.083s)
               Value function loss: 101420.5039
                    Surrogate loss: 0.0156
             Mean action noise std: 0.90
                       Mean reward: 6549.94
               Mean episode length: 308.33
                 Mean success rate: 58.00
                  Mean reward/step: 22.97
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15245312
                    Iteration time: 2.57s
                        Total time: 4771.83s
                               ETA: 5487.2s

################################################################################
                     [1m Learning iteration 1861/4000 [0m

                       Computation: 3133 steps/s (collection: 0.492s, learning 2.122s)
               Value function loss: 87071.1553
                    Surrogate loss: 0.0126
             Mean action noise std: 0.90
                       Mean reward: 6939.61
               Mean episode length: 322.79
                 Mean success rate: 60.50
                  Mean reward/step: 22.25
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 2.61s
                        Total time: 4774.44s
                               ETA: 5484.7s

################################################################################
                     [1m Learning iteration 1862/4000 [0m

                       Computation: 3125 steps/s (collection: 0.501s, learning 2.120s)
               Value function loss: 86318.0512
                    Surrogate loss: 0.0153
             Mean action noise std: 0.90
                       Mean reward: 6890.32
               Mean episode length: 321.19
                 Mean success rate: 60.00
                  Mean reward/step: 22.10
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15261696
                    Iteration time: 2.62s
                        Total time: 4777.07s
                               ETA: 5482.2s

################################################################################
                     [1m Learning iteration 1863/4000 [0m

                       Computation: 3128 steps/s (collection: 0.541s, learning 2.078s)
               Value function loss: 97974.6988
                    Surrogate loss: 0.0144
             Mean action noise std: 0.90
                       Mean reward: 6865.55
               Mean episode length: 319.01
                 Mean success rate: 59.50
                  Mean reward/step: 21.92
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 2.62s
                        Total time: 4779.68s
                               ETA: 5479.7s

################################################################################
                     [1m Learning iteration 1864/4000 [0m

                       Computation: 3174 steps/s (collection: 0.487s, learning 2.093s)
               Value function loss: 86448.9582
                    Surrogate loss: 0.0133
             Mean action noise std: 0.90
                       Mean reward: 7032.26
               Mean episode length: 322.22
                 Mean success rate: 60.00
                  Mean reward/step: 21.69
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15278080
                    Iteration time: 2.58s
                        Total time: 4782.26s
                               ETA: 5477.2s

################################################################################
                     [1m Learning iteration 1865/4000 [0m

                       Computation: 3226 steps/s (collection: 0.457s, learning 2.082s)
               Value function loss: 86064.5149
                    Surrogate loss: 0.0162
             Mean action noise std: 0.90
                       Mean reward: 7271.31
               Mean episode length: 331.39
                 Mean success rate: 61.50
                  Mean reward/step: 21.94
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 2.54s
                        Total time: 4784.80s
                               ETA: 5474.6s

################################################################################
                     [1m Learning iteration 1866/4000 [0m

                       Computation: 3210 steps/s (collection: 0.464s, learning 2.088s)
               Value function loss: 74512.6742
                    Surrogate loss: 0.0165
             Mean action noise std: 0.90
                       Mean reward: 7609.46
               Mean episode length: 342.21
                 Mean success rate: 63.50
                  Mean reward/step: 22.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15294464
                    Iteration time: 2.55s
                        Total time: 4787.36s
                               ETA: 5472.0s

################################################################################
                     [1m Learning iteration 1867/4000 [0m

                       Computation: 3131 steps/s (collection: 0.510s, learning 2.105s)
               Value function loss: 123031.2520
                    Surrogate loss: 0.0148
             Mean action noise std: 0.90
                       Mean reward: 8167.85
               Mean episode length: 362.69
                 Mean success rate: 68.50
                  Mean reward/step: 22.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 2.62s
                        Total time: 4789.97s
                               ETA: 5469.5s

################################################################################
                     [1m Learning iteration 1868/4000 [0m

                       Computation: 3216 steps/s (collection: 0.453s, learning 2.094s)
               Value function loss: 55076.6813
                    Surrogate loss: 0.0152
             Mean action noise std: 0.90
                       Mean reward: 8127.78
               Mean episode length: 359.72
                 Mean success rate: 69.00
                  Mean reward/step: 23.03
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15310848
                    Iteration time: 2.55s
                        Total time: 4792.52s
                               ETA: 5466.9s

################################################################################
                     [1m Learning iteration 1869/4000 [0m

                       Computation: 3190 steps/s (collection: 0.476s, learning 2.092s)
               Value function loss: 83409.6722
                    Surrogate loss: 0.0182
             Mean action noise std: 0.90
                       Mean reward: 7609.49
               Mean episode length: 341.33
                 Mean success rate: 66.00
                  Mean reward/step: 24.25
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 2.57s
                        Total time: 4795.09s
                               ETA: 5464.3s

################################################################################
                     [1m Learning iteration 1870/4000 [0m

                       Computation: 3095 steps/s (collection: 0.505s, learning 2.141s)
               Value function loss: 95392.8920
                    Surrogate loss: 0.0140
             Mean action noise std: 0.90
                       Mean reward: 7964.63
               Mean episode length: 353.88
                 Mean success rate: 69.00
                  Mean reward/step: 23.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15327232
                    Iteration time: 2.65s
                        Total time: 4797.73s
                               ETA: 5461.9s

################################################################################
                     [1m Learning iteration 1871/4000 [0m

                       Computation: 3232 steps/s (collection: 0.480s, learning 2.054s)
               Value function loss: 73480.0019
                    Surrogate loss: 0.0130
             Mean action noise std: 0.90
                       Mean reward: 7892.14
               Mean episode length: 355.14
                 Mean success rate: 69.50
                  Mean reward/step: 23.03
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 2.53s
                        Total time: 4800.27s
                               ETA: 5459.3s

################################################################################
                     [1m Learning iteration 1872/4000 [0m

                       Computation: 3196 steps/s (collection: 0.464s, learning 2.099s)
               Value function loss: 75136.0708
                    Surrogate loss: 0.0153
             Mean action noise std: 0.90
                       Mean reward: 7940.45
               Mean episode length: 357.17
                 Mean success rate: 70.50
                  Mean reward/step: 23.19
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15343616
                    Iteration time: 2.56s
                        Total time: 4802.83s
                               ETA: 5456.7s

################################################################################
                     [1m Learning iteration 1873/4000 [0m

                       Computation: 3217 steps/s (collection: 0.463s, learning 2.083s)
               Value function loss: 70889.0368
                    Surrogate loss: 0.0177
             Mean action noise std: 0.90
                       Mean reward: 8151.91
               Mean episode length: 364.54
                 Mean success rate: 72.50
                  Mean reward/step: 23.26
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 2.55s
                        Total time: 4805.37s
                               ETA: 5454.1s

################################################################################
                     [1m Learning iteration 1874/4000 [0m

                       Computation: 3249 steps/s (collection: 0.479s, learning 2.042s)
               Value function loss: 73164.0032
                    Surrogate loss: 0.0166
             Mean action noise std: 0.90
                       Mean reward: 7868.58
               Mean episode length: 354.10
                 Mean success rate: 69.50
                  Mean reward/step: 22.96
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15360000
                    Iteration time: 2.52s
                        Total time: 4807.90s
                               ETA: 5451.5s

################################################################################
                     [1m Learning iteration 1875/4000 [0m

                       Computation: 3154 steps/s (collection: 0.494s, learning 2.103s)
               Value function loss: 95196.1077
                    Surrogate loss: 0.0151
             Mean action noise std: 0.90
                       Mean reward: 7524.81
               Mean episode length: 343.73
                 Mean success rate: 68.00
                  Mean reward/step: 22.03
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 2.60s
                        Total time: 4810.49s
                               ETA: 5449.0s

################################################################################
                     [1m Learning iteration 1876/4000 [0m

                       Computation: 3228 steps/s (collection: 0.439s, learning 2.098s)
               Value function loss: 82353.7576
                    Surrogate loss: 0.0161
             Mean action noise std: 0.90
                       Mean reward: 7438.78
               Mean episode length: 341.53
                 Mean success rate: 67.50
                  Mean reward/step: 21.46
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15376384
                    Iteration time: 2.54s
                        Total time: 4813.03s
                               ETA: 5446.4s

################################################################################
                     [1m Learning iteration 1877/4000 [0m

                       Computation: 3198 steps/s (collection: 0.481s, learning 2.080s)
               Value function loss: 90658.7418
                    Surrogate loss: 0.0127
             Mean action noise std: 0.90
                       Mean reward: 7676.06
               Mean episode length: 346.11
                 Mean success rate: 68.00
                  Mean reward/step: 21.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 2.56s
                        Total time: 4815.59s
                               ETA: 5443.8s

################################################################################
                     [1m Learning iteration 1878/4000 [0m

                       Computation: 3179 steps/s (collection: 0.481s, learning 2.096s)
               Value function loss: 89245.7220
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 7836.31
               Mean episode length: 349.21
                 Mean success rate: 69.00
                  Mean reward/step: 21.44
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15392768
                    Iteration time: 2.58s
                        Total time: 4818.17s
                               ETA: 5441.3s

################################################################################
                     [1m Learning iteration 1879/4000 [0m

                       Computation: 3222 steps/s (collection: 0.469s, learning 2.073s)
               Value function loss: 95582.5576
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 7905.30
               Mean episode length: 351.22
                 Mean success rate: 69.00
                  Mean reward/step: 21.30
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 2.54s
                        Total time: 4820.71s
                               ETA: 5438.7s

################################################################################
                     [1m Learning iteration 1880/4000 [0m

                       Computation: 3171 steps/s (collection: 0.491s, learning 2.092s)
               Value function loss: 72502.7626
                    Surrogate loss: 0.0166
             Mean action noise std: 0.90
                       Mean reward: 7984.72
               Mean episode length: 351.61
                 Mean success rate: 68.50
                  Mean reward/step: 21.61
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15409152
                    Iteration time: 2.58s
                        Total time: 4823.29s
                               ETA: 5436.1s

################################################################################
                     [1m Learning iteration 1881/4000 [0m

                       Computation: 3233 steps/s (collection: 0.458s, learning 2.075s)
               Value function loss: 84819.2424
                    Surrogate loss: 0.0181
             Mean action noise std: 0.90
                       Mean reward: 8130.42
               Mean episode length: 358.57
                 Mean success rate: 69.50
                  Mean reward/step: 21.91
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 2.53s
                        Total time: 4825.83s
                               ETA: 5433.5s

################################################################################
                     [1m Learning iteration 1882/4000 [0m

                       Computation: 3181 steps/s (collection: 0.500s, learning 2.075s)
               Value function loss: 96886.0562
                    Surrogate loss: 0.0133
             Mean action noise std: 0.90
                       Mean reward: 8133.14
               Mean episode length: 357.31
                 Mean success rate: 69.00
                  Mean reward/step: 21.98
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15425536
                    Iteration time: 2.57s
                        Total time: 4828.40s
                               ETA: 5431.0s

################################################################################
                     [1m Learning iteration 1883/4000 [0m

                       Computation: 3298 steps/s (collection: 0.425s, learning 2.058s)
               Value function loss: 75190.9616
                    Surrogate loss: 0.0122
             Mean action noise std: 0.90
                       Mean reward: 8206.41
               Mean episode length: 356.58
                 Mean success rate: 69.00
                  Mean reward/step: 21.25
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 2.48s
                        Total time: 4830.88s
                               ETA: 5428.3s

################################################################################
                     [1m Learning iteration 1884/4000 [0m

                       Computation: 3224 steps/s (collection: 0.458s, learning 2.082s)
               Value function loss: 77380.3464
                    Surrogate loss: 0.0123
             Mean action noise std: 0.90
                       Mean reward: 8016.39
               Mean episode length: 347.89
                 Mean success rate: 67.00
                  Mean reward/step: 21.06
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15441920
                    Iteration time: 2.54s
                        Total time: 4833.42s
                               ETA: 5425.7s

################################################################################
                     [1m Learning iteration 1885/4000 [0m

                       Computation: 3041 steps/s (collection: 0.535s, learning 2.158s)
               Value function loss: 103850.5464
                    Surrogate loss: 0.0119
             Mean action noise std: 0.90
                       Mean reward: 8073.57
               Mean episode length: 347.68
                 Mean success rate: 67.50
                  Mean reward/step: 21.58
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 2.69s
                        Total time: 4836.12s
                               ETA: 5423.3s

################################################################################
                     [1m Learning iteration 1886/4000 [0m

                       Computation: 3177 steps/s (collection: 0.476s, learning 2.102s)
               Value function loss: 89248.7690
                    Surrogate loss: 0.0147
             Mean action noise std: 0.90
                       Mean reward: 7933.60
               Mean episode length: 349.29
                 Mean success rate: 67.00
                  Mean reward/step: 21.98
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15458304
                    Iteration time: 2.58s
                        Total time: 4838.70s
                               ETA: 5420.8s

################################################################################
                     [1m Learning iteration 1887/4000 [0m

                       Computation: 3205 steps/s (collection: 0.463s, learning 2.093s)
               Value function loss: 86185.4910
                    Surrogate loss: 0.0163
             Mean action noise std: 0.90
                       Mean reward: 7661.96
               Mean episode length: 343.45
                 Mean success rate: 65.50
                  Mean reward/step: 21.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 2.56s
                        Total time: 4841.25s
                               ETA: 5418.2s

################################################################################
                     [1m Learning iteration 1888/4000 [0m

                       Computation: 3154 steps/s (collection: 0.475s, learning 2.122s)
               Value function loss: 67555.4847
                    Surrogate loss: 0.0163
             Mean action noise std: 0.90
                       Mean reward: 7685.01
               Mean episode length: 344.64
                 Mean success rate: 66.00
                  Mean reward/step: 21.47
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15474688
                    Iteration time: 2.60s
                        Total time: 4843.85s
                               ETA: 5415.7s

################################################################################
                     [1m Learning iteration 1889/4000 [0m

                       Computation: 3176 steps/s (collection: 0.475s, learning 2.105s)
               Value function loss: 86870.8521
                    Surrogate loss: 0.0166
             Mean action noise std: 0.90
                       Mean reward: 7809.52
               Mean episode length: 350.93
                 Mean success rate: 67.00
                  Mean reward/step: 22.19
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 2.58s
                        Total time: 4846.43s
                               ETA: 5413.1s

################################################################################
                     [1m Learning iteration 1890/4000 [0m

                       Computation: 3189 steps/s (collection: 0.459s, learning 2.109s)
               Value function loss: 79217.6110
                    Surrogate loss: 0.0174
             Mean action noise std: 0.90
                       Mean reward: 7445.68
               Mean episode length: 339.64
                 Mean success rate: 64.00
                  Mean reward/step: 22.80
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15491072
                    Iteration time: 2.57s
                        Total time: 4849.00s
                               ETA: 5410.6s

################################################################################
                     [1m Learning iteration 1891/4000 [0m

                       Computation: 3188 steps/s (collection: 0.470s, learning 2.100s)
               Value function loss: 83663.5531
                    Surrogate loss: 0.0123
             Mean action noise std: 0.90
                       Mean reward: 7819.08
               Mean episode length: 351.92
                 Mean success rate: 67.00
                  Mean reward/step: 23.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 2.57s
                        Total time: 4851.57s
                               ETA: 5408.0s

################################################################################
                     [1m Learning iteration 1892/4000 [0m

                       Computation: 3193 steps/s (collection: 0.461s, learning 2.105s)
               Value function loss: 77629.6270
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 7565.62
               Mean episode length: 346.36
                 Mean success rate: 65.50
                  Mean reward/step: 23.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15507456
                    Iteration time: 2.57s
                        Total time: 4854.13s
                               ETA: 5405.4s

################################################################################
                     [1m Learning iteration 1893/4000 [0m

                       Computation: 3151 steps/s (collection: 0.455s, learning 2.144s)
               Value function loss: 89912.9943
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 7874.03
               Mean episode length: 355.47
                 Mean success rate: 67.00
                  Mean reward/step: 22.70
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 2.60s
                        Total time: 4856.73s
                               ETA: 5402.9s

################################################################################
                     [1m Learning iteration 1894/4000 [0m

                       Computation: 3133 steps/s (collection: 0.528s, learning 2.086s)
               Value function loss: 96926.9174
                    Surrogate loss: 0.0161
             Mean action noise std: 0.90
                       Mean reward: 8090.27
               Mean episode length: 366.70
                 Mean success rate: 69.00
                  Mean reward/step: 22.43
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15523840
                    Iteration time: 2.61s
                        Total time: 4859.34s
                               ETA: 5400.4s

################################################################################
                     [1m Learning iteration 1895/4000 [0m

                       Computation: 3198 steps/s (collection: 0.461s, learning 2.100s)
               Value function loss: 131085.8668
                    Surrogate loss: 0.0183
             Mean action noise std: 0.90
                       Mean reward: 8409.71
               Mean episode length: 378.31
                 Mean success rate: 71.50
                  Mean reward/step: 21.90
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 2.56s
                        Total time: 4861.91s
                               ETA: 5397.8s

################################################################################
                     [1m Learning iteration 1896/4000 [0m

                       Computation: 3186 steps/s (collection: 0.506s, learning 2.064s)
               Value function loss: 95672.0046
                    Surrogate loss: 0.0175
             Mean action noise std: 0.90
                       Mean reward: 8056.87
               Mean episode length: 359.23
                 Mean success rate: 69.00
                  Mean reward/step: 20.97
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 15540224
                    Iteration time: 2.57s
                        Total time: 4864.48s
                               ETA: 5395.3s

################################################################################
                     [1m Learning iteration 1897/4000 [0m

                       Computation: 3130 steps/s (collection: 0.479s, learning 2.138s)
               Value function loss: 80365.1319
                    Surrogate loss: 0.0183
             Mean action noise std: 0.90
                       Mean reward: 8184.61
               Mean episode length: 360.50
                 Mean success rate: 69.50
                  Mean reward/step: 20.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 2.62s
                        Total time: 4867.09s
                               ETA: 5392.8s

################################################################################
                     [1m Learning iteration 1898/4000 [0m

                       Computation: 3178 steps/s (collection: 0.480s, learning 2.097s)
               Value function loss: 88922.6377
                    Surrogate loss: 0.0161
             Mean action noise std: 0.90
                       Mean reward: 7928.35
               Mean episode length: 351.26
                 Mean success rate: 68.50
                  Mean reward/step: 19.46
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15556608
                    Iteration time: 2.58s
                        Total time: 4869.67s
                               ETA: 5390.2s

################################################################################
                     [1m Learning iteration 1899/4000 [0m

                       Computation: 3147 steps/s (collection: 0.486s, learning 2.117s)
               Value function loss: 84221.5624
                    Surrogate loss: 0.0150
             Mean action noise std: 0.90
                       Mean reward: 7782.99
               Mean episode length: 351.74
                 Mean success rate: 69.00
                  Mean reward/step: 19.44
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 2.60s
                        Total time: 4872.27s
                               ETA: 5387.7s

################################################################################
                     [1m Learning iteration 1900/4000 [0m

                       Computation: 3195 steps/s (collection: 0.473s, learning 2.091s)
               Value function loss: 69835.0518
                    Surrogate loss: 0.0157
             Mean action noise std: 0.90
                       Mean reward: 7468.56
               Mean episode length: 345.33
                 Mean success rate: 68.00
                  Mean reward/step: 20.52
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15572992
                    Iteration time: 2.56s
                        Total time: 4874.84s
                               ETA: 5385.1s

################################################################################
                     [1m Learning iteration 1901/4000 [0m

                       Computation: 3155 steps/s (collection: 0.497s, learning 2.099s)
               Value function loss: 99719.1087
                    Surrogate loss: 0.0162
             Mean action noise std: 0.90
                       Mean reward: 7241.60
               Mean episode length: 341.25
                 Mean success rate: 66.50
                  Mean reward/step: 21.05
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 2.60s
                        Total time: 4877.43s
                               ETA: 5382.6s

################################################################################
                     [1m Learning iteration 1902/4000 [0m

                       Computation: 3210 steps/s (collection: 0.457s, learning 2.095s)
               Value function loss: 72124.1148
                    Surrogate loss: 0.0149
             Mean action noise std: 0.90
                       Mean reward: 6443.65
               Mean episode length: 314.00
                 Mean success rate: 60.00
                  Mean reward/step: 20.96
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15589376
                    Iteration time: 2.55s
                        Total time: 4879.99s
                               ETA: 5380.0s

################################################################################
                     [1m Learning iteration 1903/4000 [0m

                       Computation: 3187 steps/s (collection: 0.482s, learning 2.089s)
               Value function loss: 85898.1347
                    Surrogate loss: 0.0153
             Mean action noise std: 0.90
                       Mean reward: 6543.07
               Mean episode length: 316.86
                 Mean success rate: 59.50
                  Mean reward/step: 21.22
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 2.57s
                        Total time: 4882.56s
                               ETA: 5377.5s

################################################################################
                     [1m Learning iteration 1904/4000 [0m

                       Computation: 3220 steps/s (collection: 0.467s, learning 2.077s)
               Value function loss: 81045.6507
                    Surrogate loss: 0.0134
             Mean action noise std: 0.90
                       Mean reward: 6450.04
               Mean episode length: 312.44
                 Mean success rate: 57.50
                  Mean reward/step: 21.41
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15605760
                    Iteration time: 2.54s
                        Total time: 4885.10s
                               ETA: 5374.9s

################################################################################
                     [1m Learning iteration 1905/4000 [0m

                       Computation: 3220 steps/s (collection: 0.471s, learning 2.073s)
               Value function loss: 73942.7906
                    Surrogate loss: 0.0177
             Mean action noise std: 0.90
                       Mean reward: 6381.25
               Mean episode length: 312.04
                 Mean success rate: 57.00
                  Mean reward/step: 21.81
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 2.54s
                        Total time: 4887.64s
                               ETA: 5372.3s

################################################################################
                     [1m Learning iteration 1906/4000 [0m

                       Computation: 3255 steps/s (collection: 0.475s, learning 2.041s)
               Value function loss: 110866.9899
                    Surrogate loss: 0.0162
             Mean action noise std: 0.90
                       Mean reward: 6548.50
               Mean episode length: 311.81
                 Mean success rate: 57.00
                  Mean reward/step: 21.96
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15622144
                    Iteration time: 2.52s
                        Total time: 4890.16s
                               ETA: 5369.7s

################################################################################
                     [1m Learning iteration 1907/4000 [0m

                       Computation: 3217 steps/s (collection: 0.475s, learning 2.072s)
               Value function loss: 83854.5754
                    Surrogate loss: 0.0142
             Mean action noise std: 0.90
                       Mean reward: 6496.46
               Mean episode length: 307.59
                 Mean success rate: 55.50
                  Mean reward/step: 22.17
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 2.55s
                        Total time: 4892.71s
                               ETA: 5367.1s

################################################################################
                     [1m Learning iteration 1908/4000 [0m

                       Computation: 3201 steps/s (collection: 0.479s, learning 2.080s)
               Value function loss: 81155.0641
                    Surrogate loss: 0.0165
             Mean action noise std: 0.90
                       Mean reward: 6866.70
               Mean episode length: 316.26
                 Mean success rate: 58.00
                  Mean reward/step: 22.38
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15638528
                    Iteration time: 2.56s
                        Total time: 4895.27s
                               ETA: 5364.5s

################################################################################
                     [1m Learning iteration 1909/4000 [0m

                       Computation: 3263 steps/s (collection: 0.457s, learning 2.054s)
               Value function loss: 93521.0049
                    Surrogate loss: 0.0174
             Mean action noise std: 0.90
                       Mean reward: 7077.00
               Mean episode length: 324.44
                 Mean success rate: 60.00
                  Mean reward/step: 22.48
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 2.51s
                        Total time: 4897.78s
                               ETA: 5361.9s

################################################################################
                     [1m Learning iteration 1910/4000 [0m

                       Computation: 3242 steps/s (collection: 0.480s, learning 2.046s)
               Value function loss: 68853.8128
                    Surrogate loss: 0.0171
             Mean action noise std: 0.90
                       Mean reward: 7039.28
               Mean episode length: 323.81
                 Mean success rate: 60.00
                  Mean reward/step: 22.44
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15654912
                    Iteration time: 2.53s
                        Total time: 4900.30s
                               ETA: 5359.3s

################################################################################
                     [1m Learning iteration 1911/4000 [0m

                       Computation: 3076 steps/s (collection: 0.500s, learning 2.163s)
               Value function loss: 112985.7809
                    Surrogate loss: 0.0142
             Mean action noise std: 0.90
                       Mean reward: 7342.24
               Mean episode length: 333.44
                 Mean success rate: 63.00
                  Mean reward/step: 22.45
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 2.66s
                        Total time: 4902.96s
                               ETA: 5356.8s

################################################################################
                     [1m Learning iteration 1912/4000 [0m

                       Computation: 3254 steps/s (collection: 0.471s, learning 2.046s)
               Value function loss: 96084.6145
                    Surrogate loss: 0.0146
             Mean action noise std: 0.90
                       Mean reward: 7391.62
               Mean episode length: 339.97
                 Mean success rate: 65.00
                  Mean reward/step: 21.14
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15671296
                    Iteration time: 2.52s
                        Total time: 4905.48s
                               ETA: 5354.2s

################################################################################
                     [1m Learning iteration 1913/4000 [0m

                       Computation: 3238 steps/s (collection: 0.441s, learning 2.089s)
               Value function loss: 71014.3669
                    Surrogate loss: 0.0154
             Mean action noise std: 0.90
                       Mean reward: 7578.68
               Mean episode length: 350.14
                 Mean success rate: 67.00
                  Mean reward/step: 21.07
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 2.53s
                        Total time: 4908.01s
                               ETA: 5351.6s

################################################################################
                     [1m Learning iteration 1914/4000 [0m

                       Computation: 3213 steps/s (collection: 0.455s, learning 2.094s)
               Value function loss: 79531.7934
                    Surrogate loss: 0.0208
             Mean action noise std: 0.90
                       Mean reward: 7970.81
               Mean episode length: 363.32
                 Mean success rate: 70.00
                  Mean reward/step: 21.17
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15687680
                    Iteration time: 2.55s
                        Total time: 4910.56s
                               ETA: 5349.0s

################################################################################
                     [1m Learning iteration 1915/4000 [0m

                       Computation: 3178 steps/s (collection: 0.472s, learning 2.105s)
               Value function loss: 71113.6754
                    Surrogate loss: 0.0157
             Mean action noise std: 0.89
                       Mean reward: 7755.82
               Mean episode length: 357.98
                 Mean success rate: 69.00
                  Mean reward/step: 21.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 2.58s
                        Total time: 4913.14s
                               ETA: 5346.5s

################################################################################
                     [1m Learning iteration 1916/4000 [0m

                       Computation: 3239 steps/s (collection: 0.464s, learning 2.065s)
               Value function loss: 88449.7086
                    Surrogate loss: 0.0162
             Mean action noise std: 0.89
                       Mean reward: 7842.33
               Mean episode length: 363.31
                 Mean success rate: 70.50
                  Mean reward/step: 21.60
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15704064
                    Iteration time: 2.53s
                        Total time: 4915.67s
                               ETA: 5343.9s

################################################################################
                     [1m Learning iteration 1917/4000 [0m

                       Computation: 3137 steps/s (collection: 0.482s, learning 2.129s)
               Value function loss: 102835.1199
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 7649.01
               Mean episode length: 355.22
                 Mean success rate: 68.50
                  Mean reward/step: 20.76
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 2.61s
                        Total time: 4918.28s
                               ETA: 5341.4s

################################################################################
                     [1m Learning iteration 1918/4000 [0m

                       Computation: 3236 steps/s (collection: 0.474s, learning 2.058s)
               Value function loss: 74569.6028
                    Surrogate loss: 0.0150
             Mean action noise std: 0.89
                       Mean reward: 7849.93
               Mean episode length: 363.47
                 Mean success rate: 71.00
                  Mean reward/step: 20.52
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15720448
                    Iteration time: 2.53s
                        Total time: 4920.81s
                               ETA: 5338.8s

################################################################################
                     [1m Learning iteration 1919/4000 [0m

                       Computation: 3216 steps/s (collection: 0.482s, learning 2.065s)
               Value function loss: 71819.1125
                    Surrogate loss: 0.0178
             Mean action noise std: 0.90
                       Mean reward: 7713.25
               Mean episode length: 360.29
                 Mean success rate: 70.50
                  Mean reward/step: 21.31
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 2.55s
                        Total time: 4923.36s
                               ETA: 5336.2s

################################################################################
                     [1m Learning iteration 1920/4000 [0m

                       Computation: 3218 steps/s (collection: 0.467s, learning 2.078s)
               Value function loss: 80446.5571
                    Surrogate loss: 0.0219
             Mean action noise std: 0.89
                       Mean reward: 7399.99
               Mean episode length: 348.36
                 Mean success rate: 67.50
                  Mean reward/step: 22.19
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15736832
                    Iteration time: 2.54s
                        Total time: 4925.90s
                               ETA: 5333.6s

################################################################################
                     [1m Learning iteration 1921/4000 [0m

                       Computation: 3147 steps/s (collection: 0.480s, learning 2.123s)
               Value function loss: 62258.0882
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 7027.63
               Mean episode length: 335.10
                 Mean success rate: 63.00
                  Mean reward/step: 22.32
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 2.60s
                        Total time: 4928.50s
                               ETA: 5331.1s

################################################################################
                     [1m Learning iteration 1922/4000 [0m

                       Computation: 3225 steps/s (collection: 0.467s, learning 2.073s)
               Value function loss: 70986.1537
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 6854.52
               Mean episode length: 329.81
                 Mean success rate: 62.00
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15753216
                    Iteration time: 2.54s
                        Total time: 4931.04s
                               ETA: 5328.5s

################################################################################
                     [1m Learning iteration 1923/4000 [0m

                       Computation: 3234 steps/s (collection: 0.452s, learning 2.081s)
               Value function loss: 67586.3268
                    Surrogate loss: 0.0187
             Mean action noise std: 0.90
                       Mean reward: 6807.21
               Mean episode length: 329.15
                 Mean success rate: 62.00
                  Mean reward/step: 21.43
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 2.53s
                        Total time: 4933.58s
                               ETA: 5325.9s

################################################################################
                     [1m Learning iteration 1924/4000 [0m

                       Computation: 3155 steps/s (collection: 0.495s, learning 2.101s)
               Value function loss: 86142.9453
                    Surrogate loss: 0.0196
             Mean action noise std: 0.90
                       Mean reward: 6773.94
               Mean episode length: 326.48
                 Mean success rate: 61.50
                  Mean reward/step: 21.80
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15769600
                    Iteration time: 2.60s
                        Total time: 4936.17s
                               ETA: 5323.4s

################################################################################
                     [1m Learning iteration 1925/4000 [0m

                       Computation: 3260 steps/s (collection: 0.451s, learning 2.062s)
               Value function loss: 88764.5295
                    Surrogate loss: 0.0161
             Mean action noise std: 0.90
                       Mean reward: 6616.59
               Mean episode length: 319.82
                 Mean success rate: 60.50
                  Mean reward/step: 22.22
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 2.51s
                        Total time: 4938.68s
                               ETA: 5320.8s

################################################################################
                     [1m Learning iteration 1926/4000 [0m

                       Computation: 3185 steps/s (collection: 0.510s, learning 2.062s)
               Value function loss: 68230.4372
                    Surrogate loss: 0.0164
             Mean action noise std: 0.90
                       Mean reward: 6446.38
               Mean episode length: 312.80
                 Mean success rate: 58.00
                  Mean reward/step: 22.21
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15785984
                    Iteration time: 2.57s
                        Total time: 4941.26s
                               ETA: 5318.2s

################################################################################
                     [1m Learning iteration 1927/4000 [0m

                       Computation: 3242 steps/s (collection: 0.470s, learning 2.056s)
               Value function loss: 125424.8342
                    Surrogate loss: 0.0174
             Mean action noise std: 0.90
                       Mean reward: 6748.27
               Mean episode length: 321.30
                 Mean success rate: 59.00
                  Mean reward/step: 22.12
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 2.53s
                        Total time: 4943.78s
                               ETA: 5315.6s

################################################################################
                     [1m Learning iteration 1928/4000 [0m

                       Computation: 3287 steps/s (collection: 0.473s, learning 2.019s)
               Value function loss: 91870.3171
                    Surrogate loss: 0.0182
             Mean action noise std: 0.89
                       Mean reward: 6840.42
               Mean episode length: 323.33
                 Mean success rate: 59.50
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15802368
                    Iteration time: 2.49s
                        Total time: 4946.27s
                               ETA: 5312.9s

################################################################################
                     [1m Learning iteration 1929/4000 [0m

                       Computation: 3185 steps/s (collection: 0.506s, learning 2.066s)
               Value function loss: 76328.9965
                    Surrogate loss: 0.0200
             Mean action noise std: 0.89
                       Mean reward: 7235.33
               Mean episode length: 333.83
                 Mean success rate: 63.50
                  Mean reward/step: 22.46
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 2.57s
                        Total time: 4948.84s
                               ETA: 5310.4s

################################################################################
                     [1m Learning iteration 1930/4000 [0m

                       Computation: 3226 steps/s (collection: 0.488s, learning 2.051s)
               Value function loss: 70038.8600
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 7009.83
               Mean episode length: 322.48
                 Mean success rate: 62.00
                  Mean reward/step: 22.48
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15818752
                    Iteration time: 2.54s
                        Total time: 4951.38s
                               ETA: 5307.8s

################################################################################
                     [1m Learning iteration 1931/4000 [0m

                       Computation: 3210 steps/s (collection: 0.491s, learning 2.060s)
               Value function loss: 80646.8439
                    Surrogate loss: 0.0172
             Mean action noise std: 0.89
                       Mean reward: 7293.64
               Mean episode length: 329.86
                 Mean success rate: 63.00
                  Mean reward/step: 22.33
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 2.55s
                        Total time: 4953.94s
                               ETA: 5305.2s

################################################################################
                     [1m Learning iteration 1932/4000 [0m

                       Computation: 3196 steps/s (collection: 0.485s, learning 2.077s)
               Value function loss: 106791.3279
                    Surrogate loss: 0.0146
             Mean action noise std: 0.89
                       Mean reward: 7408.83
               Mean episode length: 334.32
                 Mean success rate: 63.00
                  Mean reward/step: 21.87
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 15835136
                    Iteration time: 2.56s
                        Total time: 4956.50s
                               ETA: 5302.7s

################################################################################
                     [1m Learning iteration 1933/4000 [0m

                       Computation: 3247 steps/s (collection: 0.445s, learning 2.078s)
               Value function loss: 87216.6084
                    Surrogate loss: 0.0164
             Mean action noise std: 0.89
                       Mean reward: 7549.04
               Mean episode length: 338.44
                 Mean success rate: 63.50
                  Mean reward/step: 21.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 2.52s
                        Total time: 4959.02s
                               ETA: 5300.0s

################################################################################
                     [1m Learning iteration 1934/4000 [0m

                       Computation: 3253 steps/s (collection: 0.469s, learning 2.048s)
               Value function loss: 84079.4645
                    Surrogate loss: 0.0151
             Mean action noise std: 0.89
                       Mean reward: 7680.67
               Mean episode length: 340.54
                 Mean success rate: 64.50
                  Mean reward/step: 21.63
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15851520
                    Iteration time: 2.52s
                        Total time: 4961.54s
                               ETA: 5297.4s

################################################################################
                     [1m Learning iteration 1935/4000 [0m

                       Computation: 3153 steps/s (collection: 0.469s, learning 2.129s)
               Value function loss: 74815.4294
                    Surrogate loss: 0.0122
             Mean action noise std: 0.89
                       Mean reward: 7505.49
               Mean episode length: 334.70
                 Mean success rate: 63.00
                  Mean reward/step: 21.82
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 2.60s
                        Total time: 4964.14s
                               ETA: 5294.9s

################################################################################
                     [1m Learning iteration 1936/4000 [0m

                       Computation: 3280 steps/s (collection: 0.448s, learning 2.049s)
               Value function loss: 110907.2305
                    Surrogate loss: 0.0164
             Mean action noise std: 0.89
                       Mean reward: 6999.09
               Mean episode length: 321.56
                 Mean success rate: 61.00
                  Mean reward/step: 21.98
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 15867904
                    Iteration time: 2.50s
                        Total time: 4966.63s
                               ETA: 5292.3s

################################################################################
                     [1m Learning iteration 1937/4000 [0m

                       Computation: 3149 steps/s (collection: 0.508s, learning 2.093s)
               Value function loss: 83359.1884
                    Surrogate loss: 0.0158
             Mean action noise std: 0.89
                       Mean reward: 7232.55
               Mean episode length: 329.06
                 Mean success rate: 62.50
                  Mean reward/step: 21.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 2.60s
                        Total time: 4969.23s
                               ETA: 5289.7s

################################################################################
                     [1m Learning iteration 1938/4000 [0m

                       Computation: 3210 steps/s (collection: 0.483s, learning 2.069s)
               Value function loss: 85132.3349
                    Surrogate loss: 0.0150
             Mean action noise std: 0.89
                       Mean reward: 7533.85
               Mean episode length: 340.20
                 Mean success rate: 64.00
                  Mean reward/step: 21.63
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15884288
                    Iteration time: 2.55s
                        Total time: 4971.79s
                               ETA: 5287.2s

################################################################################
                     [1m Learning iteration 1939/4000 [0m

                       Computation: 3222 steps/s (collection: 0.486s, learning 2.056s)
               Value function loss: 79716.7159
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 7365.65
               Mean episode length: 335.71
                 Mean success rate: 63.00
                  Mean reward/step: 22.21
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 2.54s
                        Total time: 4974.33s
                               ETA: 5284.6s

################################################################################
                     [1m Learning iteration 1940/4000 [0m

                       Computation: 3168 steps/s (collection: 0.456s, learning 2.129s)
               Value function loss: 78512.6278
                    Surrogate loss: 0.0161
             Mean action noise std: 0.89
                       Mean reward: 7201.75
               Mean episode length: 329.61
                 Mean success rate: 63.00
                  Mean reward/step: 22.12
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15900672
                    Iteration time: 2.59s
                        Total time: 4976.91s
                               ETA: 5282.0s

################################################################################
                     [1m Learning iteration 1941/4000 [0m

                       Computation: 3215 steps/s (collection: 0.488s, learning 2.059s)
               Value function loss: 81668.1168
                    Surrogate loss: 0.0158
             Mean action noise std: 0.89
                       Mean reward: 7323.39
               Mean episode length: 336.29
                 Mean success rate: 64.50
                  Mean reward/step: 22.38
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 2.55s
                        Total time: 4979.46s
                               ETA: 5279.5s

################################################################################
                     [1m Learning iteration 1942/4000 [0m

                       Computation: 3223 steps/s (collection: 0.467s, learning 2.074s)
               Value function loss: 105840.2057
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 7476.52
               Mean episode length: 336.24
                 Mean success rate: 65.50
                  Mean reward/step: 22.36
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15917056
                    Iteration time: 2.54s
                        Total time: 4982.00s
                               ETA: 5276.9s

################################################################################
                     [1m Learning iteration 1943/4000 [0m

                       Computation: 3265 steps/s (collection: 0.478s, learning 2.031s)
               Value function loss: 99609.3645
                    Surrogate loss: 0.0145
             Mean action noise std: 0.89
                       Mean reward: 7816.71
               Mean episode length: 348.98
                 Mean success rate: 68.00
                  Mean reward/step: 21.14
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 2.51s
                        Total time: 4984.51s
                               ETA: 5274.2s

################################################################################
                     [1m Learning iteration 1944/4000 [0m

                       Computation: 3274 steps/s (collection: 0.459s, learning 2.042s)
               Value function loss: 68644.7570
                    Surrogate loss: 0.0164
             Mean action noise std: 0.89
                       Mean reward: 7695.75
               Mean episode length: 343.10
                 Mean success rate: 66.50
                  Mean reward/step: 20.81
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 15933440
                    Iteration time: 2.50s
                        Total time: 4987.01s
                               ETA: 5271.6s

################################################################################
                     [1m Learning iteration 1945/4000 [0m

                       Computation: 3207 steps/s (collection: 0.485s, learning 2.070s)
               Value function loss: 83349.7120
                    Surrogate loss: 0.0179
             Mean action noise std: 0.89
                       Mean reward: 7394.80
               Mean episode length: 336.24
                 Mean success rate: 65.00
                  Mean reward/step: 20.81
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 2.55s
                        Total time: 4989.57s
                               ETA: 5269.0s

################################################################################
                     [1m Learning iteration 1946/4000 [0m

                       Computation: 3242 steps/s (collection: 0.456s, learning 2.070s)
               Value function loss: 91462.5074
                    Surrogate loss: 0.0166
             Mean action noise std: 0.89
                       Mean reward: 7389.26
               Mean episode length: 336.15
                 Mean success rate: 65.00
                  Mean reward/step: 20.92
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15949824
                    Iteration time: 2.53s
                        Total time: 4992.09s
                               ETA: 5266.4s

################################################################################
                     [1m Learning iteration 1947/4000 [0m

                       Computation: 3242 steps/s (collection: 0.466s, learning 2.061s)
               Value function loss: 82388.4021
                    Surrogate loss: 0.0157
             Mean action noise std: 0.89
                       Mean reward: 7430.95
               Mean episode length: 337.52
                 Mean success rate: 65.50
                  Mean reward/step: 21.22
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 2.53s
                        Total time: 4994.62s
                               ETA: 5263.8s

################################################################################
                     [1m Learning iteration 1948/4000 [0m

                       Computation: 3328 steps/s (collection: 0.437s, learning 2.024s)
               Value function loss: 90396.5659
                    Surrogate loss: 0.0175
             Mean action noise std: 0.89
                       Mean reward: 7199.29
               Mean episode length: 331.95
                 Mean success rate: 64.00
                  Mean reward/step: 20.82
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 15966208
                    Iteration time: 2.46s
                        Total time: 4997.08s
                               ETA: 5261.2s

################################################################################
                     [1m Learning iteration 1949/4000 [0m

                       Computation: 3249 steps/s (collection: 0.447s, learning 2.074s)
               Value function loss: 76787.2416
                    Surrogate loss: 0.0162
             Mean action noise std: 0.89
                       Mean reward: 7089.05
               Mean episode length: 333.94
                 Mean success rate: 64.50
                  Mean reward/step: 21.13
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 2.52s
                        Total time: 4999.60s
                               ETA: 5258.6s

################################################################################
                     [1m Learning iteration 1950/4000 [0m

                       Computation: 3241 steps/s (collection: 0.472s, learning 2.055s)
               Value function loss: 78854.6650
                    Surrogate loss: 0.0147
             Mean action noise std: 0.89
                       Mean reward: 6705.16
               Mean episode length: 322.56
                 Mean success rate: 62.50
                  Mean reward/step: 21.96
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15982592
                    Iteration time: 2.53s
                        Total time: 5002.13s
                               ETA: 5256.0s

################################################################################
                     [1m Learning iteration 1951/4000 [0m

                       Computation: 3239 steps/s (collection: 0.467s, learning 2.061s)
               Value function loss: 72655.6094
                    Surrogate loss: 0.0164
             Mean action noise std: 0.89
                       Mean reward: 6750.04
               Mean episode length: 324.60
                 Mean success rate: 63.50
                  Mean reward/step: 22.09
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 2.53s
                        Total time: 5004.66s
                               ETA: 5253.4s

################################################################################
                     [1m Learning iteration 1952/4000 [0m

                       Computation: 3184 steps/s (collection: 0.488s, learning 2.085s)
               Value function loss: 95670.4673
                    Surrogate loss: 0.0164
             Mean action noise std: 0.89
                       Mean reward: 7382.16
               Mean episode length: 346.55
                 Mean success rate: 68.50
                  Mean reward/step: 22.07
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15998976
                    Iteration time: 2.57s
                        Total time: 5007.23s
                               ETA: 5250.8s

################################################################################
                     [1m Learning iteration 1953/4000 [0m

                       Computation: 3285 steps/s (collection: 0.431s, learning 2.063s)
               Value function loss: 68269.6269
                    Surrogate loss: 0.0156
             Mean action noise std: 0.89
                       Mean reward: 7367.34
               Mean episode length: 344.87
                 Mean success rate: 68.50
                  Mean reward/step: 22.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 2.49s
                        Total time: 5009.72s
                               ETA: 5248.2s

################################################################################
                     [1m Learning iteration 1954/4000 [0m

                       Computation: 3233 steps/s (collection: 0.471s, learning 2.063s)
               Value function loss: 94325.8407
                    Surrogate loss: 0.0148
             Mean action noise std: 0.89
                       Mean reward: 7614.29
               Mean episode length: 347.75
                 Mean success rate: 69.00
                  Mean reward/step: 23.06
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16015360
                    Iteration time: 2.53s
                        Total time: 5012.26s
                               ETA: 5245.6s

################################################################################
                     [1m Learning iteration 1955/4000 [0m

                       Computation: 3206 steps/s (collection: 0.499s, learning 2.056s)
               Value function loss: 63030.6368
                    Surrogate loss: 0.0174
             Mean action noise std: 0.89
                       Mean reward: 7298.69
               Mean episode length: 337.16
                 Mean success rate: 67.00
                  Mean reward/step: 22.86
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 2.55s
                        Total time: 5014.81s
                               ETA: 5243.0s

################################################################################
                     [1m Learning iteration 1956/4000 [0m

                       Computation: 3226 steps/s (collection: 0.469s, learning 2.070s)
               Value function loss: 67920.6707
                    Surrogate loss: 0.0166
             Mean action noise std: 0.89
                       Mean reward: 7420.11
               Mean episode length: 345.26
                 Mean success rate: 69.00
                  Mean reward/step: 23.40
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16031744
                    Iteration time: 2.54s
                        Total time: 5017.35s
                               ETA: 5240.4s

################################################################################
                     [1m Learning iteration 1957/4000 [0m

                       Computation: 3195 steps/s (collection: 0.482s, learning 2.082s)
               Value function loss: 71906.9577
                    Surrogate loss: 0.0182
             Mean action noise std: 0.89
                       Mean reward: 7614.35
               Mean episode length: 352.19
                 Mean success rate: 70.00
                  Mean reward/step: 23.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 2.56s
                        Total time: 5019.91s
                               ETA: 5237.8s

################################################################################
                     [1m Learning iteration 1958/4000 [0m

                       Computation: 3288 steps/s (collection: 0.452s, learning 2.039s)
               Value function loss: 103673.1181
                    Surrogate loss: 0.0147
             Mean action noise std: 0.89
                       Mean reward: 8189.41
               Mean episode length: 371.88
                 Mean success rate: 74.50
                  Mean reward/step: 23.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16048128
                    Iteration time: 2.49s
                        Total time: 5022.40s
                               ETA: 5235.2s

################################################################################
                     [1m Learning iteration 1959/4000 [0m

                       Computation: 3150 steps/s (collection: 0.505s, learning 2.095s)
               Value function loss: 117900.3947
                    Surrogate loss: 0.0150
             Mean action noise std: 0.89
                       Mean reward: 8693.88
               Mean episode length: 386.56
                 Mean success rate: 77.50
                  Mean reward/step: 23.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 2.60s
                        Total time: 5025.00s
                               ETA: 5232.7s

################################################################################
                     [1m Learning iteration 1960/4000 [0m

                       Computation: 3229 steps/s (collection: 0.488s, learning 2.049s)
               Value function loss: 75106.5151
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 8792.80
               Mean episode length: 394.00
                 Mean success rate: 78.00
                  Mean reward/step: 22.95
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16064512
                    Iteration time: 2.54s
                        Total time: 5027.54s
                               ETA: 5230.1s

################################################################################
                     [1m Learning iteration 1961/4000 [0m

                       Computation: 3247 steps/s (collection: 0.437s, learning 2.086s)
               Value function loss: 54653.5564
                    Surrogate loss: 0.0150
             Mean action noise std: 0.89
                       Mean reward: 8821.87
               Mean episode length: 393.99
                 Mean success rate: 78.50
                  Mean reward/step: 23.72
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 2.52s
                        Total time: 5030.06s
                               ETA: 5227.5s

################################################################################
                     [1m Learning iteration 1962/4000 [0m

                       Computation: 3199 steps/s (collection: 0.494s, learning 2.066s)
               Value function loss: 104200.4610
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 8636.53
               Mean episode length: 382.83
                 Mean success rate: 76.00
                  Mean reward/step: 24.27
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 16080896
                    Iteration time: 2.56s
                        Total time: 5032.62s
                               ETA: 5224.9s

################################################################################
                     [1m Learning iteration 1963/4000 [0m

                       Computation: 3245 steps/s (collection: 0.460s, learning 2.064s)
               Value function loss: 137437.1330
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 8713.73
               Mean episode length: 393.37
                 Mean success rate: 76.00
                  Mean reward/step: 24.11
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 2.52s
                        Total time: 5035.15s
                               ETA: 5222.3s

################################################################################
                     [1m Learning iteration 1964/4000 [0m

                       Computation: 3265 steps/s (collection: 0.457s, learning 2.051s)
               Value function loss: 87936.9660
                    Surrogate loss: 0.0155
             Mean action noise std: 0.89
                       Mean reward: 9080.34
               Mean episode length: 403.94
                 Mean success rate: 78.50
                  Mean reward/step: 22.38
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16097280
                    Iteration time: 2.51s
                        Total time: 5037.66s
                               ETA: 5219.7s

################################################################################
                     [1m Learning iteration 1965/4000 [0m

                       Computation: 3142 steps/s (collection: 0.454s, learning 2.153s)
               Value function loss: 77250.6299
                    Surrogate loss: 0.0152
             Mean action noise std: 0.89
                       Mean reward: 9054.20
               Mean episode length: 400.00
                 Mean success rate: 77.50
                  Mean reward/step: 23.08
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 2.61s
                        Total time: 5040.26s
                               ETA: 5217.2s

################################################################################
                     [1m Learning iteration 1966/4000 [0m

                       Computation: 3198 steps/s (collection: 0.477s, learning 2.084s)
               Value function loss: 92332.0006
                    Surrogate loss: 0.0171
             Mean action noise std: 0.89
                       Mean reward: 8799.74
               Mean episode length: 384.32
                 Mean success rate: 74.00
                  Mean reward/step: 23.16
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16113664
                    Iteration time: 2.56s
                        Total time: 5042.82s
                               ETA: 5214.6s

################################################################################
                     [1m Learning iteration 1967/4000 [0m

                       Computation: 3222 steps/s (collection: 0.443s, learning 2.100s)
               Value function loss: 130744.1422
                    Surrogate loss: 0.0169
             Mean action noise std: 0.89
                       Mean reward: 9025.49
               Mean episode length: 390.12
                 Mean success rate: 75.00
                  Mean reward/step: 22.93
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 2.54s
                        Total time: 5045.37s
                               ETA: 5212.0s

################################################################################
                     [1m Learning iteration 1968/4000 [0m

                       Computation: 3140 steps/s (collection: 0.454s, learning 2.154s)
               Value function loss: 46592.1528
                    Surrogate loss: 0.0169
             Mean action noise std: 0.89
                       Mean reward: 8836.16
               Mean episode length: 383.09
                 Mean success rate: 73.50
                  Mean reward/step: 22.29
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 16130048
                    Iteration time: 2.61s
                        Total time: 5047.97s
                               ETA: 5209.5s

################################################################################
                     [1m Learning iteration 1969/4000 [0m

                       Computation: 3152 steps/s (collection: 0.489s, learning 2.110s)
               Value function loss: 72410.6993
                    Surrogate loss: 0.0193
             Mean action noise std: 0.89
                       Mean reward: 8340.94
               Mean episode length: 360.93
                 Mean success rate: 70.00
                  Mean reward/step: 22.62
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 2.60s
                        Total time: 5050.57s
                               ETA: 5207.0s

################################################################################
                     [1m Learning iteration 1970/4000 [0m

                       Computation: 3189 steps/s (collection: 0.453s, learning 2.116s)
               Value function loss: 90604.1423
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 8402.00
               Mean episode length: 363.52
                 Mean success rate: 70.00
                  Mean reward/step: 22.55
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16146432
                    Iteration time: 2.57s
                        Total time: 5053.14s
                               ETA: 5204.4s

################################################################################
                     [1m Learning iteration 1971/4000 [0m

                       Computation: 3237 steps/s (collection: 0.455s, learning 2.075s)
               Value function loss: 105611.9395
                    Surrogate loss: 0.0164
             Mean action noise std: 0.89
                       Mean reward: 8213.18
               Mean episode length: 353.57
                 Mean success rate: 68.50
                  Mean reward/step: 22.05
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 2.53s
                        Total time: 5055.67s
                               ETA: 5201.8s

################################################################################
                     [1m Learning iteration 1972/4000 [0m

                       Computation: 3217 steps/s (collection: 0.443s, learning 2.103s)
               Value function loss: 94331.2227
                    Surrogate loss: 0.0158
             Mean action noise std: 0.89
                       Mean reward: 7832.16
               Mean episode length: 340.63
                 Mean success rate: 66.00
                  Mean reward/step: 21.84
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16162816
                    Iteration time: 2.55s
                        Total time: 5058.22s
                               ETA: 5199.2s

################################################################################
                     [1m Learning iteration 1973/4000 [0m

                       Computation: 3240 steps/s (collection: 0.453s, learning 2.075s)
               Value function loss: 77622.0836
                    Surrogate loss: 0.0175
             Mean action noise std: 0.89
                       Mean reward: 7976.84
               Mean episode length: 347.21
                 Mean success rate: 67.50
                  Mean reward/step: 22.12
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 2.53s
                        Total time: 5060.75s
                               ETA: 5196.6s

################################################################################
                     [1m Learning iteration 1974/4000 [0m

                       Computation: 3219 steps/s (collection: 0.438s, learning 2.107s)
               Value function loss: 105735.3198
                    Surrogate loss: 0.0156
             Mean action noise std: 0.89
                       Mean reward: 8213.49
               Mean episode length: 353.12
                 Mean success rate: 69.00
                  Mean reward/step: 22.30
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16179200
                    Iteration time: 2.54s
                        Total time: 5063.29s
                               ETA: 5194.0s

################################################################################
                     [1m Learning iteration 1975/4000 [0m

                       Computation: 3226 steps/s (collection: 0.471s, learning 2.068s)
               Value function loss: 84364.2127
                    Surrogate loss: 0.0136
             Mean action noise std: 0.89
                       Mean reward: 8348.00
               Mean episode length: 358.63
                 Mean success rate: 70.50
                  Mean reward/step: 22.41
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 2.54s
                        Total time: 5065.83s
                               ETA: 5191.4s

################################################################################
                     [1m Learning iteration 1976/4000 [0m

                       Computation: 3255 steps/s (collection: 0.427s, learning 2.089s)
               Value function loss: 84153.0771
                    Surrogate loss: 0.0152
             Mean action noise std: 0.89
                       Mean reward: 7820.31
               Mean episode length: 341.68
                 Mean success rate: 67.00
                  Mean reward/step: 22.81
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16195584
                    Iteration time: 2.52s
                        Total time: 5068.34s
                               ETA: 5188.8s

################################################################################
                     [1m Learning iteration 1977/4000 [0m

                       Computation: 3091 steps/s (collection: 0.513s, learning 2.137s)
               Value function loss: 89636.9471
                    Surrogate loss: 0.0172
             Mean action noise std: 0.89
                       Mean reward: 7879.41
               Mean episode length: 343.51
                 Mean success rate: 67.50
                  Mean reward/step: 22.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 2.65s
                        Total time: 5070.99s
                               ETA: 5186.4s

################################################################################
                     [1m Learning iteration 1978/4000 [0m

                       Computation: 3109 steps/s (collection: 0.489s, learning 2.145s)
               Value function loss: 79792.1101
                    Surrogate loss: 0.0162
             Mean action noise std: 0.89
                       Mean reward: 8303.22
               Mean episode length: 360.50
                 Mean success rate: 70.50
                  Mean reward/step: 22.10
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16211968
                    Iteration time: 2.63s
                        Total time: 5073.63s
                               ETA: 5183.9s

################################################################################
                     [1m Learning iteration 1979/4000 [0m

                       Computation: 3128 steps/s (collection: 0.506s, learning 2.112s)
               Value function loss: 96370.9890
                    Surrogate loss: 0.0174
             Mean action noise std: 0.89
                       Mean reward: 8180.34
               Mean episode length: 362.29
                 Mean success rate: 70.50
                  Mean reward/step: 21.72
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 2.62s
                        Total time: 5076.25s
                               ETA: 5181.4s

################################################################################
                     [1m Learning iteration 1980/4000 [0m

                       Computation: 3154 steps/s (collection: 0.459s, learning 2.138s)
               Value function loss: 79265.0947
                    Surrogate loss: 0.0166
             Mean action noise std: 0.89
                       Mean reward: 8245.28
               Mean episode length: 369.75
                 Mean success rate: 72.50
                  Mean reward/step: 21.43
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16228352
                    Iteration time: 2.60s
                        Total time: 5078.84s
                               ETA: 5178.8s

################################################################################
                     [1m Learning iteration 1981/4000 [0m

                       Computation: 3104 steps/s (collection: 0.509s, learning 2.130s)
               Value function loss: 116349.9986
                    Surrogate loss: 0.0171
             Mean action noise std: 0.89
                       Mean reward: 8439.03
               Mean episode length: 371.50
                 Mean success rate: 74.00
                  Mean reward/step: 21.39
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 2.64s
                        Total time: 5081.48s
                               ETA: 5176.3s

################################################################################
                     [1m Learning iteration 1982/4000 [0m

                       Computation: 3070 steps/s (collection: 0.522s, learning 2.145s)
               Value function loss: 51447.8720
                    Surrogate loss: 0.0155
             Mean action noise std: 0.89
                       Mean reward: 8339.81
               Mean episode length: 369.48
                 Mean success rate: 73.50
                  Mean reward/step: 21.00
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16244736
                    Iteration time: 2.67s
                        Total time: 5084.15s
                               ETA: 5173.9s

################################################################################
                     [1m Learning iteration 1983/4000 [0m

                       Computation: 3119 steps/s (collection: 0.511s, learning 2.115s)
               Value function loss: 96738.4782
                    Surrogate loss: 0.0143
             Mean action noise std: 0.89
                       Mean reward: 8141.60
               Mean episode length: 365.00
                 Mean success rate: 73.50
                  Mean reward/step: 21.72
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 2.63s
                        Total time: 5086.78s
                               ETA: 5171.4s

################################################################################
                     [1m Learning iteration 1984/4000 [0m

                       Computation: 3171 steps/s (collection: 0.469s, learning 2.114s)
               Value function loss: 52621.5689
                    Surrogate loss: 0.0137
             Mean action noise std: 0.89
                       Mean reward: 7580.62
               Mean episode length: 344.57
                 Mean success rate: 69.50
                  Mean reward/step: 22.94
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16261120
                    Iteration time: 2.58s
                        Total time: 5089.36s
                               ETA: 5168.8s

################################################################################
                     [1m Learning iteration 1985/4000 [0m

                       Computation: 3116 steps/s (collection: 0.463s, learning 2.165s)
               Value function loss: 107026.8226
                    Surrogate loss: 0.0127
             Mean action noise std: 0.89
                       Mean reward: 7254.96
               Mean episode length: 340.28
                 Mean success rate: 68.00
                  Mean reward/step: 23.16
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 2.63s
                        Total time: 5091.99s
                               ETA: 5166.3s

################################################################################
                     [1m Learning iteration 1986/4000 [0m

                       Computation: 3187 steps/s (collection: 0.476s, learning 2.094s)
               Value function loss: 77776.5260
                    Surrogate loss: 0.0164
             Mean action noise std: 0.89
                       Mean reward: 7226.07
               Mean episode length: 335.19
                 Mean success rate: 67.00
                  Mean reward/step: 23.05
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16277504
                    Iteration time: 2.57s
                        Total time: 5094.56s
                               ETA: 5163.8s

################################################################################
                     [1m Learning iteration 1987/4000 [0m

                       Computation: 3076 steps/s (collection: 0.510s, learning 2.153s)
               Value function loss: 81839.8105
                    Surrogate loss: 0.0165
             Mean action noise std: 0.89
                       Mean reward: 7193.07
               Mean episode length: 330.07
                 Mean success rate: 66.00
                  Mean reward/step: 22.71
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 2.66s
                        Total time: 5097.22s
                               ETA: 5161.3s

################################################################################
                     [1m Learning iteration 1988/4000 [0m

                       Computation: 3215 steps/s (collection: 0.479s, learning 2.069s)
               Value function loss: 102667.6901
                    Surrogate loss: 0.0179
             Mean action noise std: 0.89
                       Mean reward: 7401.01
               Mean episode length: 328.71
                 Mean success rate: 66.50
                  Mean reward/step: 22.93
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16293888
                    Iteration time: 2.55s
                        Total time: 5099.77s
                               ETA: 5158.7s

################################################################################
                     [1m Learning iteration 1989/4000 [0m

                       Computation: 3204 steps/s (collection: 0.476s, learning 2.081s)
               Value function loss: 72897.2387
                    Surrogate loss: 0.0179
             Mean action noise std: 0.89
                       Mean reward: 7400.78
               Mean episode length: 327.63
                 Mean success rate: 65.00
                  Mean reward/step: 22.76
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 2.56s
                        Total time: 5102.32s
                               ETA: 5156.2s

################################################################################
                     [1m Learning iteration 1990/4000 [0m

                       Computation: 3220 steps/s (collection: 0.481s, learning 2.063s)
               Value function loss: 78441.4528
                    Surrogate loss: 0.0183
             Mean action noise std: 0.89
                       Mean reward: 6740.95
               Mean episode length: 312.10
                 Mean success rate: 62.00
                  Mean reward/step: 22.98
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 16310272
                    Iteration time: 2.54s
                        Total time: 5104.87s
                               ETA: 5153.6s

################################################################################
                     [1m Learning iteration 1991/4000 [0m

                       Computation: 3244 steps/s (collection: 0.495s, learning 2.030s)
               Value function loss: 93679.6191
                    Surrogate loss: 0.0181
             Mean action noise std: 0.89
                       Mean reward: 6806.34
               Mean episode length: 314.36
                 Mean success rate: 61.50
                  Mean reward/step: 22.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 2.52s
                        Total time: 5107.39s
                               ETA: 5151.0s

################################################################################
                     [1m Learning iteration 1992/4000 [0m

                       Computation: 3230 steps/s (collection: 0.486s, learning 2.050s)
               Value function loss: 87926.4616
                    Surrogate loss: 0.0175
             Mean action noise std: 0.89
                       Mean reward: 6973.38
               Mean episode length: 315.42
                 Mean success rate: 61.00
                  Mean reward/step: 22.30
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 16326656
                    Iteration time: 2.54s
                        Total time: 5109.93s
                               ETA: 5148.4s

################################################################################
                     [1m Learning iteration 1993/4000 [0m

                       Computation: 3201 steps/s (collection: 0.477s, learning 2.082s)
               Value function loss: 98821.7981
                    Surrogate loss: 0.0184
             Mean action noise std: 0.89
                       Mean reward: 6942.19
               Mean episode length: 312.54
                 Mean success rate: 60.50
                  Mean reward/step: 22.18
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 2.56s
                        Total time: 5112.49s
                               ETA: 5145.8s

################################################################################
                     [1m Learning iteration 1994/4000 [0m

                       Computation: 3168 steps/s (collection: 0.512s, learning 2.073s)
               Value function loss: 80442.9971
                    Surrogate loss: 0.0178
             Mean action noise std: 0.89
                       Mean reward: 6773.19
               Mean episode length: 306.15
                 Mean success rate: 60.50
                  Mean reward/step: 22.54
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 16343040
                    Iteration time: 2.59s
                        Total time: 5115.07s
                               ETA: 5143.3s

################################################################################
                     [1m Learning iteration 1995/4000 [0m

                       Computation: 3266 steps/s (collection: 0.446s, learning 2.062s)
               Value function loss: 116415.4756
                    Surrogate loss: 0.0157
             Mean action noise std: 0.89
                       Mean reward: 6828.80
               Mean episode length: 310.46
                 Mean success rate: 60.50
                  Mean reward/step: 22.08
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 2.51s
                        Total time: 5117.58s
                               ETA: 5140.7s

################################################################################
                     [1m Learning iteration 1996/4000 [0m

                       Computation: 3167 steps/s (collection: 0.509s, learning 2.077s)
               Value function loss: 100963.5095
                    Surrogate loss: 0.0155
             Mean action noise std: 0.89
                       Mean reward: 6831.73
               Mean episode length: 311.44
                 Mean success rate: 60.50
                  Mean reward/step: 21.69
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16359424
                    Iteration time: 2.59s
                        Total time: 5120.17s
                               ETA: 5138.1s

################################################################################
                     [1m Learning iteration 1997/4000 [0m

                       Computation: 3228 steps/s (collection: 0.480s, learning 2.057s)
               Value function loss: 76871.5967
                    Surrogate loss: 0.0179
             Mean action noise std: 0.89
                       Mean reward: 7237.71
               Mean episode length: 321.86
                 Mean success rate: 62.00
                  Mean reward/step: 21.98
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 2.54s
                        Total time: 5122.70s
                               ETA: 5135.5s

################################################################################
                     [1m Learning iteration 1998/4000 [0m

                       Computation: 3173 steps/s (collection: 0.482s, learning 2.099s)
               Value function loss: 69147.0618
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 7390.92
               Mean episode length: 322.94
                 Mean success rate: 62.50
                  Mean reward/step: 22.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16375808
                    Iteration time: 2.58s
                        Total time: 5125.29s
                               ETA: 5133.0s

################################################################################
                     [1m Learning iteration 1999/4000 [0m

                       Computation: 3256 steps/s (collection: 0.455s, learning 2.061s)
               Value function loss: 58931.8748
                    Surrogate loss: 0.0152
             Mean action noise std: 0.89
                       Mean reward: 7547.51
               Mean episode length: 329.87
                 Mean success rate: 64.00
                  Mean reward/step: 23.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 2.52s
                        Total time: 5127.80s
                               ETA: 5130.4s

################################################################################
                     [1m Learning iteration 2000/4000 [0m

                       Computation: 3204 steps/s (collection: 0.455s, learning 2.101s)
               Value function loss: 80689.1198
                    Surrogate loss: 0.0152
             Mean action noise std: 0.89
                       Mean reward: 7608.88
               Mean episode length: 330.57
                 Mean success rate: 65.00
                  Mean reward/step: 23.88
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16392192
                    Iteration time: 2.56s
                        Total time: 5130.36s
                               ETA: 5127.8s

################################################################################
                     [1m Learning iteration 2001/4000 [0m

                       Computation: 3033 steps/s (collection: 0.573s, learning 2.128s)
               Value function loss: 78156.0169
                    Surrogate loss: 0.0141
             Mean action noise std: 0.89
                       Mean reward: 7598.71
               Mean episode length: 330.02
                 Mean success rate: 65.00
                  Mean reward/step: 23.55
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 2.70s
                        Total time: 5133.06s
                               ETA: 5125.4s

################################################################################
                     [1m Learning iteration 2002/4000 [0m

                       Computation: 3020 steps/s (collection: 0.558s, learning 2.154s)
               Value function loss: 91501.1639
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 7433.96
               Mean episode length: 326.38
                 Mean success rate: 63.00
                  Mean reward/step: 23.53
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 16408576
                    Iteration time: 2.71s
                        Total time: 5135.77s
                               ETA: 5123.0s

################################################################################
                     [1m Learning iteration 2003/4000 [0m

                       Computation: 3142 steps/s (collection: 0.521s, learning 2.085s)
               Value function loss: 95157.4275
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 7525.78
               Mean episode length: 328.61
                 Mean success rate: 64.00
                  Mean reward/step: 23.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 2.61s
                        Total time: 5138.38s
                               ETA: 5120.4s

################################################################################
                     [1m Learning iteration 2004/4000 [0m

                       Computation: 3138 steps/s (collection: 0.493s, learning 2.117s)
               Value function loss: 65876.5694
                    Surrogate loss: 0.0117
             Mean action noise std: 0.89
                       Mean reward: 7736.88
               Mean episode length: 335.93
                 Mean success rate: 65.50
                  Mean reward/step: 23.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16424960
                    Iteration time: 2.61s
                        Total time: 5140.99s
                               ETA: 5117.9s

################################################################################
                     [1m Learning iteration 2005/4000 [0m

                       Computation: 3139 steps/s (collection: 0.478s, learning 2.132s)
               Value function loss: 82251.1625
                    Surrogate loss: 0.0171
             Mean action noise std: 0.89
                       Mean reward: 7733.97
               Mean episode length: 338.29
                 Mean success rate: 66.00
                  Mean reward/step: 22.89
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 2.61s
                        Total time: 5143.60s
                               ETA: 5115.4s

################################################################################
                     [1m Learning iteration 2006/4000 [0m

                       Computation: 3146 steps/s (collection: 0.499s, learning 2.104s)
               Value function loss: 75901.2368
                    Surrogate loss: 0.0163
             Mean action noise std: 0.89
                       Mean reward: 7333.41
               Mean episode length: 328.67
                 Mean success rate: 63.50
                  Mean reward/step: 22.92
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 16441344
                    Iteration time: 2.60s
                        Total time: 5146.20s
                               ETA: 5112.9s

################################################################################
                     [1m Learning iteration 2007/4000 [0m

                       Computation: 3128 steps/s (collection: 0.484s, learning 2.135s)
               Value function loss: 101004.9331
                    Surrogate loss: 0.0177
             Mean action noise std: 0.89
                       Mean reward: 7452.03
               Mean episode length: 335.27
                 Mean success rate: 64.50
                  Mean reward/step: 23.29
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 2.62s
                        Total time: 5148.82s
                               ETA: 5110.4s

################################################################################
                     [1m Learning iteration 2008/4000 [0m

                       Computation: 3064 steps/s (collection: 0.513s, learning 2.160s)
               Value function loss: 55596.8270
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 7568.42
               Mean episode length: 339.67
                 Mean success rate: 65.00
                  Mean reward/step: 23.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16457728
                    Iteration time: 2.67s
                        Total time: 5151.49s
                               ETA: 5107.9s

################################################################################
                     [1m Learning iteration 2009/4000 [0m

                       Computation: 3116 steps/s (collection: 0.540s, learning 2.089s)
               Value function loss: 107352.6568
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 7795.20
               Mean episode length: 350.87
                 Mean success rate: 66.50
                  Mean reward/step: 23.98
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 2.63s
                        Total time: 5154.12s
                               ETA: 5105.4s

################################################################################
                     [1m Learning iteration 2010/4000 [0m

                       Computation: 3195 steps/s (collection: 0.479s, learning 2.085s)
               Value function loss: 110940.2439
                    Surrogate loss: 0.0170
             Mean action noise std: 0.89
                       Mean reward: 8289.37
               Mean episode length: 365.44
                 Mean success rate: 69.50
                  Mean reward/step: 23.66
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16474112
                    Iteration time: 2.56s
                        Total time: 5156.68s
                               ETA: 5102.8s

################################################################################
                     [1m Learning iteration 2011/4000 [0m

                       Computation: 3199 steps/s (collection: 0.464s, learning 2.097s)
               Value function loss: 116794.2943
                    Surrogate loss: 0.0193
             Mean action noise std: 0.89
                       Mean reward: 8570.13
               Mean episode length: 374.73
                 Mean success rate: 72.00
                  Mean reward/step: 23.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 2.56s
                        Total time: 5159.24s
                               ETA: 5100.3s

################################################################################
                     [1m Learning iteration 2012/4000 [0m

                       Computation: 3135 steps/s (collection: 0.497s, learning 2.116s)
               Value function loss: 83874.7551
                    Surrogate loss: 0.0165
             Mean action noise std: 0.89
                       Mean reward: 8329.53
               Mean episode length: 364.86
                 Mean success rate: 70.00
                  Mean reward/step: 23.32
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16490496
                    Iteration time: 2.61s
                        Total time: 5161.86s
                               ETA: 5097.7s

################################################################################
                     [1m Learning iteration 2013/4000 [0m

                       Computation: 3197 steps/s (collection: 0.494s, learning 2.068s)
               Value function loss: 78774.1700
                    Surrogate loss: 0.0168
             Mean action noise std: 0.89
                       Mean reward: 8497.75
               Mean episode length: 369.73
                 Mean success rate: 71.00
                  Mean reward/step: 23.57
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 2.56s
                        Total time: 5164.42s
                               ETA: 5095.2s

################################################################################
                     [1m Learning iteration 2014/4000 [0m

                       Computation: 3139 steps/s (collection: 0.515s, learning 2.095s)
               Value function loss: 63089.9571
                    Surrogate loss: 0.0168
             Mean action noise std: 0.89
                       Mean reward: 8610.91
               Mean episode length: 371.97
                 Mean success rate: 72.00
                  Mean reward/step: 24.33
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16506880
                    Iteration time: 2.61s
                        Total time: 5167.03s
                               ETA: 5092.7s

################################################################################
                     [1m Learning iteration 2015/4000 [0m

                       Computation: 3087 steps/s (collection: 0.527s, learning 2.126s)
               Value function loss: 85339.4476
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 9132.63
               Mean episode length: 383.63
                 Mean success rate: 75.00
                  Mean reward/step: 24.98
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 2.65s
                        Total time: 5169.68s
                               ETA: 5090.2s

################################################################################
                     [1m Learning iteration 2016/4000 [0m

                       Computation: 3209 steps/s (collection: 0.530s, learning 2.022s)
               Value function loss: 101562.6617
                    Surrogate loss: 0.0147
             Mean action noise std: 0.89
                       Mean reward: 9145.01
               Mean episode length: 381.91
                 Mean success rate: 74.00
                  Mean reward/step: 24.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16523264
                    Iteration time: 2.55s
                        Total time: 5172.23s
                               ETA: 5087.6s

################################################################################
                     [1m Learning iteration 2017/4000 [0m

                       Computation: 3119 steps/s (collection: 0.548s, learning 2.078s)
               Value function loss: 86400.6093
                    Surrogate loss: 0.0145
             Mean action noise std: 0.89
                       Mean reward: 9186.60
               Mean episode length: 380.56
                 Mean success rate: 74.50
                  Mean reward/step: 23.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 2.63s
                        Total time: 5174.86s
                               ETA: 5085.1s

################################################################################
                     [1m Learning iteration 2018/4000 [0m

                       Computation: 3102 steps/s (collection: 0.548s, learning 2.093s)
               Value function loss: 84031.4677
                    Surrogate loss: 0.0196
             Mean action noise std: 0.89
                       Mean reward: 8704.01
               Mean episode length: 364.94
                 Mean success rate: 71.00
                  Mean reward/step: 23.36
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16539648
                    Iteration time: 2.64s
                        Total time: 5177.50s
                               ETA: 5082.6s

################################################################################
                     [1m Learning iteration 2019/4000 [0m

                       Computation: 3262 steps/s (collection: 0.463s, learning 2.048s)
               Value function loss: 101818.4213
                    Surrogate loss: 0.0154
             Mean action noise std: 0.89
                       Mean reward: 8602.25
               Mean episode length: 362.12
                 Mean success rate: 70.50
                  Mean reward/step: 22.56
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 2.51s
                        Total time: 5180.01s
                               ETA: 5080.0s

################################################################################
                     [1m Learning iteration 2020/4000 [0m

                       Computation: 3206 steps/s (collection: 0.500s, learning 2.055s)
               Value function loss: 89307.5524
                    Surrogate loss: 0.0168
             Mean action noise std: 0.89
                       Mean reward: 8530.94
               Mean episode length: 360.40
                 Mean success rate: 69.50
                  Mean reward/step: 22.34
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16556032
                    Iteration time: 2.56s
                        Total time: 5182.57s
                               ETA: 5077.4s

################################################################################
                     [1m Learning iteration 2021/4000 [0m

                       Computation: 3234 steps/s (collection: 0.484s, learning 2.049s)
               Value function loss: 126682.8342
                    Surrogate loss: 0.0176
             Mean action noise std: 0.89
                       Mean reward: 8479.17
               Mean episode length: 357.45
                 Mean success rate: 68.50
                  Mean reward/step: 22.04
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 2.53s
                        Total time: 5185.10s
                               ETA: 5074.8s

################################################################################
                     [1m Learning iteration 2022/4000 [0m

                       Computation: 3135 steps/s (collection: 0.536s, learning 2.077s)
               Value function loss: 86020.1831
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 8314.63
               Mean episode length: 353.62
                 Mean success rate: 68.00
                  Mean reward/step: 21.86
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16572416
                    Iteration time: 2.61s
                        Total time: 5187.71s
                               ETA: 5072.3s

################################################################################
                     [1m Learning iteration 2023/4000 [0m

                       Computation: 3096 steps/s (collection: 0.512s, learning 2.133s)
               Value function loss: 112334.9593
                    Surrogate loss: 0.0173
             Mean action noise std: 0.89
                       Mean reward: 8084.19
               Mean episode length: 347.06
                 Mean success rate: 67.00
                  Mean reward/step: 21.29
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 2.65s
                        Total time: 5190.36s
                               ETA: 5069.8s

################################################################################
                     [1m Learning iteration 2024/4000 [0m

                       Computation: 3169 steps/s (collection: 0.469s, learning 2.116s)
               Value function loss: 76321.2064
                    Surrogate loss: 0.0178
             Mean action noise std: 0.89
                       Mean reward: 7307.32
               Mean episode length: 323.56
                 Mean success rate: 61.50
                  Mean reward/step: 21.24
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 16588800
                    Iteration time: 2.59s
                        Total time: 5192.94s
                               ETA: 5067.3s

################################################################################
                     [1m Learning iteration 2025/4000 [0m

                       Computation: 3192 steps/s (collection: 0.498s, learning 2.067s)
               Value function loss: 88512.3931
                    Surrogate loss: 0.0172
             Mean action noise std: 0.89
                       Mean reward: 7576.22
               Mean episode length: 331.29
                 Mean success rate: 62.50
                  Mean reward/step: 20.76
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 2.57s
                        Total time: 5195.51s
                               ETA: 5064.7s

################################################################################
                     [1m Learning iteration 2026/4000 [0m

                       Computation: 3127 steps/s (collection: 0.472s, learning 2.147s)
               Value function loss: 91746.0646
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 7757.47
               Mean episode length: 335.19
                 Mean success rate: 64.00
                  Mean reward/step: 21.38
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 16605184
                    Iteration time: 2.62s
                        Total time: 5198.13s
                               ETA: 5062.2s

################################################################################
                     [1m Learning iteration 2027/4000 [0m

                       Computation: 3193 steps/s (collection: 0.496s, learning 2.069s)
               Value function loss: 100393.3770
                    Surrogate loss: 0.0165
             Mean action noise std: 0.89
                       Mean reward: 7636.69
               Mean episode length: 332.29
                 Mean success rate: 63.50
                  Mean reward/step: 21.86
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 2.56s
                        Total time: 5200.69s
                               ETA: 5059.6s

################################################################################
                     [1m Learning iteration 2028/4000 [0m

                       Computation: 3153 steps/s (collection: 0.493s, learning 2.105s)
               Value function loss: 72857.1743
                    Surrogate loss: 0.0175
             Mean action noise std: 0.89
                       Mean reward: 7642.94
               Mean episode length: 333.03
                 Mean success rate: 63.50
                  Mean reward/step: 21.87
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16621568
                    Iteration time: 2.60s
                        Total time: 5203.29s
                               ETA: 5057.1s

################################################################################
                     [1m Learning iteration 2029/4000 [0m

                       Computation: 3183 steps/s (collection: 0.487s, learning 2.086s)
               Value function loss: 80664.4312
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 7692.29
               Mean episode length: 334.83
                 Mean success rate: 63.50
                  Mean reward/step: 22.94
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 2.57s
                        Total time: 5205.86s
                               ETA: 5054.6s

################################################################################
                     [1m Learning iteration 2030/4000 [0m

                       Computation: 3163 steps/s (collection: 0.486s, learning 2.104s)
               Value function loss: 55198.9827
                    Surrogate loss: 0.0160
             Mean action noise std: 0.89
                       Mean reward: 7294.37
               Mean episode length: 322.40
                 Mean success rate: 60.50
                  Mean reward/step: 23.97
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16637952
                    Iteration time: 2.59s
                        Total time: 5208.45s
                               ETA: 5052.0s

################################################################################
                     [1m Learning iteration 2031/4000 [0m

                       Computation: 3091 steps/s (collection: 0.552s, learning 2.097s)
               Value function loss: 59670.3795
                    Surrogate loss: 0.0166
             Mean action noise std: 0.89
                       Mean reward: 6962.15
               Mean episode length: 310.04
                 Mean success rate: 58.00
                  Mean reward/step: 24.41
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 2.65s
                        Total time: 5211.10s
                               ETA: 5049.5s

################################################################################
                     [1m Learning iteration 2032/4000 [0m

                       Computation: 3205 steps/s (collection: 0.477s, learning 2.079s)
               Value function loss: 86177.0077
                    Surrogate loss: 0.0159
             Mean action noise std: 0.89
                       Mean reward: 7334.51
               Mean episode length: 325.62
                 Mean success rate: 61.00
                  Mean reward/step: 24.36
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16654336
                    Iteration time: 2.56s
                        Total time: 5213.66s
                               ETA: 5047.0s

################################################################################
                     [1m Learning iteration 2033/4000 [0m

                       Computation: 3077 steps/s (collection: 0.511s, learning 2.151s)
               Value function loss: 79027.6438
                    Surrogate loss: 0.0164
             Mean action noise std: 0.89
                       Mean reward: 7630.63
               Mean episode length: 335.62
                 Mean success rate: 64.00
                  Mean reward/step: 24.15
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 2.66s
                        Total time: 5216.32s
                               ETA: 5044.5s

################################################################################
                     [1m Learning iteration 2034/4000 [0m

                       Computation: 3228 steps/s (collection: 0.462s, learning 2.075s)
               Value function loss: 87319.4868
                    Surrogate loss: 0.0157
             Mean action noise std: 0.89
                       Mean reward: 7659.02
               Mean episode length: 337.46
                 Mean success rate: 66.00
                  Mean reward/step: 23.44
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16670720
                    Iteration time: 2.54s
                        Total time: 5218.86s
                               ETA: 5041.9s

################################################################################
                     [1m Learning iteration 2035/4000 [0m

                       Computation: 3177 steps/s (collection: 0.513s, learning 2.066s)
               Value function loss: 72736.7609
                    Surrogate loss: 0.0147
             Mean action noise std: 0.89
                       Mean reward: 7691.25
               Mean episode length: 340.72
                 Mean success rate: 67.00
                  Mean reward/step: 23.17
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 2.58s
                        Total time: 5221.43s
                               ETA: 5039.4s

################################################################################
                     [1m Learning iteration 2036/4000 [0m

                       Computation: 3216 steps/s (collection: 0.464s, learning 2.083s)
               Value function loss: 95033.4481
                    Surrogate loss: 0.0169
             Mean action noise std: 0.89
                       Mean reward: 7452.32
               Mean episode length: 330.75
                 Mean success rate: 66.50
                  Mean reward/step: 23.16
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16687104
                    Iteration time: 2.55s
                        Total time: 5223.98s
                               ETA: 5036.8s

################################################################################
                     [1m Learning iteration 2037/4000 [0m

                       Computation: 3141 steps/s (collection: 0.479s, learning 2.129s)
               Value function loss: 92510.8239
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 7648.99
               Mean episode length: 337.07
                 Mean success rate: 68.50
                  Mean reward/step: 23.02
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 2.61s
                        Total time: 5226.59s
                               ETA: 5034.2s

################################################################################
                     [1m Learning iteration 2038/4000 [0m

                       Computation: 3138 steps/s (collection: 0.494s, learning 2.116s)
               Value function loss: 103316.3768
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 7781.84
               Mean episode length: 342.60
                 Mean success rate: 70.00
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16703488
                    Iteration time: 2.61s
                        Total time: 5229.20s
                               ETA: 5031.7s

################################################################################
                     [1m Learning iteration 2039/4000 [0m

                       Computation: 3176 steps/s (collection: 0.470s, learning 2.109s)
               Value function loss: 91508.0929
                    Surrogate loss: 0.0111
             Mean action noise std: 0.89
                       Mean reward: 8042.69
               Mean episode length: 353.69
                 Mean success rate: 72.00
                  Mean reward/step: 21.94
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 2.58s
                        Total time: 5231.78s
                               ETA: 5029.2s

################################################################################
                     [1m Learning iteration 2040/4000 [0m

                       Computation: 3149 steps/s (collection: 0.464s, learning 2.137s)
               Value function loss: 124297.4567
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 8065.51
               Mean episode length: 352.88
                 Mean success rate: 72.00
                  Mean reward/step: 21.57
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 16719872
                    Iteration time: 2.60s
                        Total time: 5234.38s
                               ETA: 5026.6s

################################################################################
                     [1m Learning iteration 2041/4000 [0m

                       Computation: 3175 steps/s (collection: 0.445s, learning 2.134s)
               Value function loss: 86848.8863
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 8133.06
               Mean episode length: 356.40
                 Mean success rate: 73.00
                  Mean reward/step: 22.32
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 2.58s
                        Total time: 5236.96s
                               ETA: 5024.1s

################################################################################
                     [1m Learning iteration 2042/4000 [0m

                       Computation: 3144 steps/s (collection: 0.500s, learning 2.105s)
               Value function loss: 91941.8989
                    Surrogate loss: 0.0183
             Mean action noise std: 0.89
                       Mean reward: 8327.30
               Mean episode length: 361.55
                 Mean success rate: 73.50
                  Mean reward/step: 22.85
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16736256
                    Iteration time: 2.61s
                        Total time: 5239.56s
                               ETA: 5021.6s

################################################################################
                     [1m Learning iteration 2043/4000 [0m

                       Computation: 3228 steps/s (collection: 0.469s, learning 2.069s)
               Value function loss: 85352.5447
                    Surrogate loss: 0.0174
             Mean action noise std: 0.89
                       Mean reward: 8547.80
               Mean episode length: 368.29
                 Mean success rate: 74.50
                  Mean reward/step: 22.91
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 2.54s
                        Total time: 5242.10s
                               ETA: 5019.0s

################################################################################
                     [1m Learning iteration 2044/4000 [0m

                       Computation: 3170 steps/s (collection: 0.501s, learning 2.083s)
               Value function loss: 62112.7543
                    Surrogate loss: 0.0161
             Mean action noise std: 0.89
                       Mean reward: 8705.33
               Mean episode length: 372.86
                 Mean success rate: 74.50
                  Mean reward/step: 23.03
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16752640
                    Iteration time: 2.58s
                        Total time: 5244.69s
                               ETA: 5016.4s

################################################################################
                     [1m Learning iteration 2045/4000 [0m

                       Computation: 3165 steps/s (collection: 0.483s, learning 2.105s)
               Value function loss: 95107.7658
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 8092.48
               Mean episode length: 353.17
                 Mean success rate: 69.00
                  Mean reward/step: 23.17
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 2.59s
                        Total time: 5247.27s
                               ETA: 5013.9s

################################################################################
                     [1m Learning iteration 2046/4000 [0m

                       Computation: 3138 steps/s (collection: 0.487s, learning 2.123s)
               Value function loss: 71679.2464
                    Surrogate loss: 0.0170
             Mean action noise std: 0.89
                       Mean reward: 7839.02
               Mean episode length: 342.23
                 Mean success rate: 67.00
                  Mean reward/step: 22.93
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16769024
                    Iteration time: 2.61s
                        Total time: 5249.88s
                               ETA: 5011.4s

################################################################################
                     [1m Learning iteration 2047/4000 [0m

                       Computation: 3200 steps/s (collection: 0.486s, learning 2.073s)
               Value function loss: 62211.9592
                    Surrogate loss: 0.0148
             Mean action noise std: 0.89
                       Mean reward: 8007.86
               Mean episode length: 346.63
                 Mean success rate: 69.00
                  Mean reward/step: 23.53
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 2.56s
                        Total time: 5252.44s
                               ETA: 5008.8s

################################################################################
                     [1m Learning iteration 2048/4000 [0m

                       Computation: 3378 steps/s (collection: 0.451s, learning 1.973s)
               Value function loss: 84474.7578
                    Surrogate loss: 0.0130
             Mean action noise std: 0.89
                       Mean reward: 8053.37
               Mean episode length: 345.78
                 Mean success rate: 68.50
                  Mean reward/step: 24.11
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16785408
                    Iteration time: 2.42s
                        Total time: 5254.87s
                               ETA: 5006.1s

################################################################################
                     [1m Learning iteration 2049/4000 [0m

                       Computation: 3228 steps/s (collection: 0.469s, learning 2.068s)
               Value function loss: 97675.1967
                    Surrogate loss: 0.0141
             Mean action noise std: 0.89
                       Mean reward: 8023.38
               Mean episode length: 346.48
                 Mean success rate: 68.50
                  Mean reward/step: 23.55
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 2.54s
                        Total time: 5257.40s
                               ETA: 5003.5s

################################################################################
                     [1m Learning iteration 2050/4000 [0m

                       Computation: 3244 steps/s (collection: 0.472s, learning 2.053s)
               Value function loss: 100415.7874
                    Surrogate loss: 0.0161
             Mean action noise std: 0.89
                       Mean reward: 7945.65
               Mean episode length: 344.70
                 Mean success rate: 69.00
                  Mean reward/step: 23.23
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16801792
                    Iteration time: 2.53s
                        Total time: 5259.93s
                               ETA: 5000.9s

################################################################################
                     [1m Learning iteration 2051/4000 [0m

                       Computation: 3250 steps/s (collection: 0.474s, learning 2.046s)
               Value function loss: 76035.0652
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 7678.92
               Mean episode length: 338.18
                 Mean success rate: 68.00
                  Mean reward/step: 22.56
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 2.52s
                        Total time: 5262.45s
                               ETA: 4998.3s

################################################################################
                     [1m Learning iteration 2052/4000 [0m

                       Computation: 3231 steps/s (collection: 0.483s, learning 2.053s)
               Value function loss: 84712.7501
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 7627.60
               Mean episode length: 337.38
                 Mean success rate: 67.00
                  Mean reward/step: 22.81
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16818176
                    Iteration time: 2.54s
                        Total time: 5264.98s
                               ETA: 4995.7s

################################################################################
                     [1m Learning iteration 2053/4000 [0m

                       Computation: 3258 steps/s (collection: 0.482s, learning 2.032s)
               Value function loss: 66306.4270
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 7574.08
               Mean episode length: 331.55
                 Mean success rate: 66.50
                  Mean reward/step: 22.28
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 2.51s
                        Total time: 5267.50s
                               ETA: 4993.1s

################################################################################
                     [1m Learning iteration 2054/4000 [0m

                       Computation: 3156 steps/s (collection: 0.522s, learning 2.074s)
               Value function loss: 137718.0967
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 8148.76
               Mean episode length: 354.80
                 Mean success rate: 72.00
                  Mean reward/step: 21.87
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 16834560
                    Iteration time: 2.60s
                        Total time: 5270.09s
                               ETA: 4990.6s

################################################################################
                     [1m Learning iteration 2055/4000 [0m

                       Computation: 3182 steps/s (collection: 0.463s, learning 2.111s)
               Value function loss: 96193.8939
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 8156.41
               Mean episode length: 357.31
                 Mean success rate: 72.00
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 2.57s
                        Total time: 5272.67s
                               ETA: 4988.0s

################################################################################
                     [1m Learning iteration 2056/4000 [0m

                       Computation: 3204 steps/s (collection: 0.488s, learning 2.068s)
               Value function loss: 96013.9941
                    Surrogate loss: 0.0125
             Mean action noise std: 0.89
                       Mean reward: 8522.84
               Mean episode length: 371.19
                 Mean success rate: 74.50
                  Mean reward/step: 21.42
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16850944
                    Iteration time: 2.56s
                        Total time: 5275.22s
                               ETA: 4985.4s

################################################################################
                     [1m Learning iteration 2057/4000 [0m

                       Computation: 3192 steps/s (collection: 0.444s, learning 2.122s)
               Value function loss: 87527.7772
                    Surrogate loss: 0.0162
             Mean action noise std: 0.89
                       Mean reward: 8472.87
               Mean episode length: 366.27
                 Mean success rate: 74.50
                  Mean reward/step: 22.67
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 2.57s
                        Total time: 5277.79s
                               ETA: 4982.9s

################################################################################
                     [1m Learning iteration 2058/4000 [0m

                       Computation: 3190 steps/s (collection: 0.486s, learning 2.081s)
               Value function loss: 65904.2781
                    Surrogate loss: 0.0155
             Mean action noise std: 0.89
                       Mean reward: 8402.46
               Mean episode length: 364.87
                 Mean success rate: 73.50
                  Mean reward/step: 22.96
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16867328
                    Iteration time: 2.57s
                        Total time: 5280.36s
                               ETA: 4980.3s

################################################################################
                     [1m Learning iteration 2059/4000 [0m

                       Computation: 3205 steps/s (collection: 0.457s, learning 2.099s)
               Value function loss: 83040.5743
                    Surrogate loss: 0.0160
             Mean action noise std: 0.89
                       Mean reward: 8831.13
               Mean episode length: 375.15
                 Mean success rate: 75.00
                  Mean reward/step: 23.23
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 2.56s
                        Total time: 5282.91s
                               ETA: 4977.7s

################################################################################
                     [1m Learning iteration 2060/4000 [0m

                       Computation: 3238 steps/s (collection: 0.493s, learning 2.037s)
               Value function loss: 61297.3257
                    Surrogate loss: 0.0181
             Mean action noise std: 0.89
                       Mean reward: 8519.75
               Mean episode length: 366.05
                 Mean success rate: 73.50
                  Mean reward/step: 24.12
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16883712
                    Iteration time: 2.53s
                        Total time: 5285.44s
                               ETA: 4975.1s

################################################################################
                     [1m Learning iteration 2061/4000 [0m

                       Computation: 3233 steps/s (collection: 0.488s, learning 2.045s)
               Value function loss: 101126.6126
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 8539.52
               Mean episode length: 367.68
                 Mean success rate: 74.50
                  Mean reward/step: 24.52
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 2.53s
                        Total time: 5287.98s
                               ETA: 4972.5s

################################################################################
                     [1m Learning iteration 2062/4000 [0m

                       Computation: 3323 steps/s (collection: 0.434s, learning 2.031s)
               Value function loss: 59511.9871
                    Surrogate loss: 0.0137
             Mean action noise std: 0.89
                       Mean reward: 8758.45
               Mean episode length: 375.25
                 Mean success rate: 76.00
                  Mean reward/step: 24.35
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16900096
                    Iteration time: 2.47s
                        Total time: 5290.44s
                               ETA: 4969.9s

################################################################################
                     [1m Learning iteration 2063/4000 [0m

                       Computation: 3204 steps/s (collection: 0.457s, learning 2.100s)
               Value function loss: 71246.8266
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 8793.52
               Mean episode length: 373.72
                 Mean success rate: 75.50
                  Mean reward/step: 24.19
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 2.56s
                        Total time: 5293.00s
                               ETA: 4967.3s

################################################################################
                     [1m Learning iteration 2064/4000 [0m

                       Computation: 3206 steps/s (collection: 0.497s, learning 2.057s)
               Value function loss: 88119.5291
                    Surrogate loss: 0.0193
             Mean action noise std: 0.89
                       Mean reward: 8457.25
               Mean episode length: 367.19
                 Mean success rate: 74.50
                  Mean reward/step: 23.88
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16916480
                    Iteration time: 2.55s
                        Total time: 5295.55s
                               ETA: 4964.7s

################################################################################
                     [1m Learning iteration 2065/4000 [0m

                       Computation: 3252 steps/s (collection: 0.465s, learning 2.053s)
               Value function loss: 104152.4210
                    Surrogate loss: 0.0147
             Mean action noise std: 0.89
                       Mean reward: 8260.72
               Mean episode length: 359.46
                 Mean success rate: 72.50
                  Mean reward/step: 22.98
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 2.52s
                        Total time: 5298.07s
                               ETA: 4962.1s

################################################################################
                     [1m Learning iteration 2066/4000 [0m

                       Computation: 3210 steps/s (collection: 0.471s, learning 2.081s)
               Value function loss: 108820.1554
                    Surrogate loss: 0.0157
             Mean action noise std: 0.89
                       Mean reward: 8089.55
               Mean episode length: 358.84
                 Mean success rate: 71.50
                  Mean reward/step: 22.30
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 16932864
                    Iteration time: 2.55s
                        Total time: 5300.62s
                               ETA: 4959.6s

################################################################################
                     [1m Learning iteration 2067/4000 [0m

                       Computation: 3213 steps/s (collection: 0.468s, learning 2.081s)
               Value function loss: 84178.7583
                    Surrogate loss: 0.0165
             Mean action noise std: 0.89
                       Mean reward: 8025.36
               Mean episode length: 355.91
                 Mean success rate: 72.50
                  Mean reward/step: 21.82
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 2.55s
                        Total time: 5303.17s
                               ETA: 4957.0s

################################################################################
                     [1m Learning iteration 2068/4000 [0m

                       Computation: 3159 steps/s (collection: 0.516s, learning 2.077s)
               Value function loss: 91203.0938
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 7708.45
               Mean episode length: 348.69
                 Mean success rate: 71.00
                  Mean reward/step: 21.97
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16949248
                    Iteration time: 2.59s
                        Total time: 5305.77s
                               ETA: 4954.4s

################################################################################
                     [1m Learning iteration 2069/4000 [0m

                       Computation: 3258 steps/s (collection: 0.448s, learning 2.067s)
               Value function loss: 96309.5631
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 8027.59
               Mean episode length: 355.77
                 Mean success rate: 72.50
                  Mean reward/step: 21.88
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 2.51s
                        Total time: 5308.28s
                               ETA: 4951.8s

################################################################################
                     [1m Learning iteration 2070/4000 [0m

                       Computation: 3159 steps/s (collection: 0.498s, learning 2.094s)
               Value function loss: 122316.5625
                    Surrogate loss: 0.0155
             Mean action noise std: 0.89
                       Mean reward: 7937.20
               Mean episode length: 354.33
                 Mean success rate: 72.50
                  Mean reward/step: 21.65
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 16965632
                    Iteration time: 2.59s
                        Total time: 5310.87s
                               ETA: 4949.3s

################################################################################
                     [1m Learning iteration 2071/4000 [0m

                       Computation: 3260 steps/s (collection: 0.472s, learning 2.041s)
               Value function loss: 93430.0729
                    Surrogate loss: 0.0125
             Mean action noise std: 0.89
                       Mean reward: 8132.05
               Mean episode length: 359.83
                 Mean success rate: 73.50
                  Mean reward/step: 21.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 2.51s
                        Total time: 5313.38s
                               ETA: 4946.7s

################################################################################
                     [1m Learning iteration 2072/4000 [0m

                       Computation: 3204 steps/s (collection: 0.488s, learning 2.069s)
               Value function loss: 64185.9257
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 7850.98
               Mean episode length: 345.42
                 Mean success rate: 70.50
                  Mean reward/step: 21.43
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16982016
                    Iteration time: 2.56s
                        Total time: 5315.94s
                               ETA: 4944.1s

################################################################################
                     [1m Learning iteration 2073/4000 [0m

                       Computation: 3182 steps/s (collection: 0.506s, learning 2.068s)
               Value function loss: 53981.0997
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 7923.46
               Mean episode length: 348.58
                 Mean success rate: 71.50
                  Mean reward/step: 22.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 2.57s
                        Total time: 5318.52s
                               ETA: 4941.6s

################################################################################
                     [1m Learning iteration 2074/4000 [0m

                       Computation: 3195 steps/s (collection: 0.492s, learning 2.072s)
               Value function loss: 76727.2690
                    Surrogate loss: 0.0143
             Mean action noise std: 0.89
                       Mean reward: 7927.98
               Mean episode length: 343.98
                 Mean success rate: 71.00
                  Mean reward/step: 22.59
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16998400
                    Iteration time: 2.56s
                        Total time: 5321.08s
                               ETA: 4939.0s

################################################################################
                     [1m Learning iteration 2075/4000 [0m

                       Computation: 3271 steps/s (collection: 0.457s, learning 2.046s)
               Value function loss: 86552.8000
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 7933.13
               Mean episode length: 343.19
                 Mean success rate: 70.00
                  Mean reward/step: 22.49
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.50s
                        Total time: 5323.58s
                               ETA: 4936.4s

################################################################################
                     [1m Learning iteration 2076/4000 [0m

                       Computation: 3205 steps/s (collection: 0.464s, learning 2.091s)
               Value function loss: 85921.5492
                    Surrogate loss: 0.0146
             Mean action noise std: 0.89
                       Mean reward: 8119.87
               Mean episode length: 351.02
                 Mean success rate: 71.50
                  Mean reward/step: 22.85
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 17014784
                    Iteration time: 2.56s
                        Total time: 5326.14s
                               ETA: 4933.8s

################################################################################
                     [1m Learning iteration 2077/4000 [0m

                       Computation: 3212 steps/s (collection: 0.464s, learning 2.086s)
               Value function loss: 72523.9991
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 7993.15
               Mean episode length: 348.78
                 Mean success rate: 71.50
                  Mean reward/step: 22.74
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 2.55s
                        Total time: 5328.69s
                               ETA: 4931.2s

################################################################################
                     [1m Learning iteration 2078/4000 [0m

                       Computation: 3291 steps/s (collection: 0.434s, learning 2.054s)
               Value function loss: 52651.9346
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 7569.58
               Mean episode length: 336.06
                 Mean success rate: 69.50
                  Mean reward/step: 22.62
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17031168
                    Iteration time: 2.49s
                        Total time: 5331.18s
                               ETA: 4928.6s

################################################################################
                     [1m Learning iteration 2079/4000 [0m

                       Computation: 3279 steps/s (collection: 0.459s, learning 2.039s)
               Value function loss: 95335.6450
                    Surrogate loss: 0.0164
             Mean action noise std: 0.89
                       Mean reward: 7579.10
               Mean episode length: 336.00
                 Mean success rate: 69.50
                  Mean reward/step: 23.39
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 2.50s
                        Total time: 5333.67s
                               ETA: 4926.0s

################################################################################
                     [1m Learning iteration 2080/4000 [0m

                       Computation: 3210 steps/s (collection: 0.484s, learning 2.068s)
               Value function loss: 109303.3579
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 7274.03
               Mean episode length: 326.92
                 Mean success rate: 68.50
                  Mean reward/step: 22.97
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 17047552
                    Iteration time: 2.55s
                        Total time: 5336.23s
                               ETA: 4923.4s

################################################################################
                     [1m Learning iteration 2081/4000 [0m

                       Computation: 3217 steps/s (collection: 0.492s, learning 2.054s)
               Value function loss: 96479.3471
                    Surrogate loss: 0.0145
             Mean action noise std: 0.89
                       Mean reward: 7768.65
               Mean episode length: 344.50
                 Mean success rate: 72.50
                  Mean reward/step: 22.27
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 2.55s
                        Total time: 5338.77s
                               ETA: 4920.8s

################################################################################
                     [1m Learning iteration 2082/4000 [0m

                       Computation: 3225 steps/s (collection: 0.490s, learning 2.049s)
               Value function loss: 66533.2837
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 7894.89
               Mean episode length: 345.95
                 Mean success rate: 72.00
                  Mean reward/step: 22.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17063936
                    Iteration time: 2.54s
                        Total time: 5341.31s
                               ETA: 4918.2s

################################################################################
                     [1m Learning iteration 2083/4000 [0m

                       Computation: 3194 steps/s (collection: 0.476s, learning 2.089s)
               Value function loss: 103062.7445
                    Surrogate loss: 0.0136
             Mean action noise std: 0.89
                       Mean reward: 7742.06
               Mean episode length: 348.94
                 Mean success rate: 72.50
                  Mean reward/step: 22.90
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 2.56s
                        Total time: 5343.88s
                               ETA: 4915.6s

################################################################################
                     [1m Learning iteration 2084/4000 [0m

                       Computation: 3272 steps/s (collection: 0.471s, learning 2.032s)
               Value function loss: 93780.3534
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 7919.77
               Mean episode length: 353.44
                 Mean success rate: 73.00
                  Mean reward/step: 22.28
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17080320
                    Iteration time: 2.50s
                        Total time: 5346.38s
                               ETA: 4913.0s

################################################################################
                     [1m Learning iteration 2085/4000 [0m

                       Computation: 3312 steps/s (collection: 0.442s, learning 2.031s)
               Value function loss: 123192.0322
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 7957.14
               Mean episode length: 350.50
                 Mean success rate: 71.50
                  Mean reward/step: 22.19
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 2.47s
                        Total time: 5348.85s
                               ETA: 4910.4s

################################################################################
                     [1m Learning iteration 2086/4000 [0m

                       Computation: 3225 steps/s (collection: 0.493s, learning 2.047s)
               Value function loss: 64071.8558
                    Surrogate loss: 0.0156
             Mean action noise std: 0.89
                       Mean reward: 8361.34
               Mean episode length: 362.68
                 Mean success rate: 73.50
                  Mean reward/step: 21.92
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17096704
                    Iteration time: 2.54s
                        Total time: 5351.39s
                               ETA: 4907.8s

################################################################################
                     [1m Learning iteration 2087/4000 [0m

                       Computation: 3250 steps/s (collection: 0.487s, learning 2.033s)
               Value function loss: 110709.6236
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 8427.44
               Mean episode length: 367.89
                 Mean success rate: 74.50
                  Mean reward/step: 22.23
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 2.52s
                        Total time: 5353.91s
                               ETA: 4905.2s

################################################################################
                     [1m Learning iteration 2088/4000 [0m

                       Computation: 3248 steps/s (collection: 0.473s, learning 2.049s)
               Value function loss: 84173.2870
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 8681.12
               Mean episode length: 376.36
                 Mean success rate: 76.00
                  Mean reward/step: 21.75
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17113088
                    Iteration time: 2.52s
                        Total time: 5356.43s
                               ETA: 4902.6s

################################################################################
                     [1m Learning iteration 2089/4000 [0m

                       Computation: 3263 steps/s (collection: 0.468s, learning 2.043s)
               Value function loss: 76676.4933
                    Surrogate loss: 0.0117
             Mean action noise std: 0.89
                       Mean reward: 8453.68
               Mean episode length: 372.10
                 Mean success rate: 74.50
                  Mean reward/step: 21.87
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 2.51s
                        Total time: 5358.95s
                               ETA: 4900.0s

################################################################################
                     [1m Learning iteration 2090/4000 [0m

                       Computation: 3255 steps/s (collection: 0.492s, learning 2.024s)
               Value function loss: 85438.5511
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 8282.33
               Mean episode length: 366.61
                 Mean success rate: 74.00
                  Mean reward/step: 21.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17129472
                    Iteration time: 2.52s
                        Total time: 5361.46s
                               ETA: 4897.4s

################################################################################
                     [1m Learning iteration 2091/4000 [0m

                       Computation: 3272 steps/s (collection: 0.466s, learning 2.037s)
               Value function loss: 79747.5588
                    Surrogate loss: 0.0120
             Mean action noise std: 0.89
                       Mean reward: 8249.61
               Mean episode length: 364.68
                 Mean success rate: 73.00
                  Mean reward/step: 22.04
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 2.50s
                        Total time: 5363.96s
                               ETA: 4894.7s

################################################################################
                     [1m Learning iteration 2092/4000 [0m

                       Computation: 3280 steps/s (collection: 0.461s, learning 2.036s)
               Value function loss: 76583.3275
                    Surrogate loss: 0.0136
             Mean action noise std: 0.89
                       Mean reward: 8395.77
               Mean episode length: 368.20
                 Mean success rate: 74.00
                  Mean reward/step: 22.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17145856
                    Iteration time: 2.50s
                        Total time: 5366.46s
                               ETA: 4892.1s

################################################################################
                     [1m Learning iteration 2093/4000 [0m

                       Computation: 3296 steps/s (collection: 0.464s, learning 2.021s)
               Value function loss: 95842.4129
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 8399.13
               Mean episode length: 368.51
                 Mean success rate: 73.50
                  Mean reward/step: 22.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 2.49s
                        Total time: 5368.95s
                               ETA: 4889.5s

################################################################################
                     [1m Learning iteration 2094/4000 [0m

                       Computation: 3228 steps/s (collection: 0.490s, learning 2.047s)
               Value function loss: 64118.7375
                    Surrogate loss: 0.0127
             Mean action noise std: 0.89
                       Mean reward: 8287.46
               Mean episode length: 364.46
                 Mean success rate: 73.00
                  Mean reward/step: 21.81
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17162240
                    Iteration time: 2.54s
                        Total time: 5371.48s
                               ETA: 4886.9s

################################################################################
                     [1m Learning iteration 2095/4000 [0m

                       Computation: 3199 steps/s (collection: 0.525s, learning 2.035s)
               Value function loss: 99161.3363
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 7982.92
               Mean episode length: 362.90
                 Mean success rate: 72.00
                  Mean reward/step: 22.44
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 2.56s
                        Total time: 5374.04s
                               ETA: 4884.3s

################################################################################
                     [1m Learning iteration 2096/4000 [0m

                       Computation: 3285 steps/s (collection: 0.454s, learning 2.040s)
               Value function loss: 88616.1648
                    Surrogate loss: 0.0114
             Mean action noise std: 0.89
                       Mean reward: 8006.91
               Mean episode length: 364.37
                 Mean success rate: 71.50
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17178624
                    Iteration time: 2.49s
                        Total time: 5376.54s
                               ETA: 4881.7s

################################################################################
                     [1m Learning iteration 2097/4000 [0m

                       Computation: 3279 steps/s (collection: 0.478s, learning 2.020s)
               Value function loss: 83803.8501
                    Surrogate loss: 0.0126
             Mean action noise std: 0.89
                       Mean reward: 7963.25
               Mean episode length: 359.82
                 Mean success rate: 70.00
                  Mean reward/step: 22.16
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 2.50s
                        Total time: 5379.04s
                               ETA: 4879.1s

################################################################################
                     [1m Learning iteration 2098/4000 [0m

                       Computation: 3250 steps/s (collection: 0.468s, learning 2.053s)
               Value function loss: 85921.3958
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 8316.74
               Mean episode length: 372.34
                 Mean success rate: 72.50
                  Mean reward/step: 22.36
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17195008
                    Iteration time: 2.52s
                        Total time: 5381.56s
                               ETA: 4876.5s

################################################################################
                     [1m Learning iteration 2099/4000 [0m

                       Computation: 3284 steps/s (collection: 0.452s, learning 2.042s)
               Value function loss: 100155.2562
                    Surrogate loss: 0.0164
             Mean action noise std: 0.89
                       Mean reward: 8339.29
               Mean episode length: 375.04
                 Mean success rate: 72.00
                  Mean reward/step: 22.37
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 2.49s
                        Total time: 5384.05s
                               ETA: 4873.8s

################################################################################
                     [1m Learning iteration 2100/4000 [0m

                       Computation: 3175 steps/s (collection: 0.536s, learning 2.044s)
               Value function loss: 70896.8783
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 8543.53
               Mean episode length: 384.17
                 Mean success rate: 75.00
                  Mean reward/step: 21.56
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17211392
                    Iteration time: 2.58s
                        Total time: 5386.63s
                               ETA: 4871.3s

################################################################################
                     [1m Learning iteration 2101/4000 [0m

                       Computation: 3211 steps/s (collection: 0.498s, learning 2.053s)
               Value function loss: 103410.4089
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 8553.47
               Mean episode length: 389.59
                 Mean success rate: 76.00
                  Mean reward/step: 21.09
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 2.55s
                        Total time: 5389.18s
                               ETA: 4868.7s
Traceback (most recent call last):
  File "tools/train_ppo.py", line 51, in <module>
    train()
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "tools/train_ppo.py", line 47, in train
    ppo.run(num_learning_iterations=max_iterations, log_interval=cfg.train.learn.save_interval)
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/mvp/ppo/ppo.py", line 267, in run
    mean_value_loss, mean_surrogate_loss = self.update(it, num_learning_iterations)
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/mvp/ppo/ppo.py", line 415, in update
    mean_value_loss += value_loss.item()
KeyboardInterrupt
