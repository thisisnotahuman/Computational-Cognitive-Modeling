PopSpikeActor(
  (encoder): PopSpikeEncoderRegularSpike()
  (snn): SpikeMLP(
    (hidden_layers): ModuleList(
      (0): Linear(in_features=770, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=64, bias=True)
    )
    (out_pop_layer): Linear(in_features=64, out_features=230, bias=True)
  )
  (decoder): PopSpikeDecoder(
    (decoder): Conv1d(23, 23, kernel_size=(10,), stride=(1,), groups=23)
    (output_activation): ELU(alpha=1.0)
  )
)
Sequential(
  (0): Linear(in_features=77, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=1, bias=True)
)
################################################################################
                      [1m Learning iteration 0/4000 [0m

                       Computation: 1624 steps/s (collection: 0.869s, learning 4.175s)
               Value function loss: 2.0082
                    Surrogate loss: 0.0515
             Mean action noise std: 1.00
                       Mean reward: 2.30
               Mean episode length: 11.27
                 Mean success rate: 0.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 18.58
--------------------------------------------------------------------------------
                   Total timesteps: 8192
                    Iteration time: 5.04s
                        Total time: 5.04s
                               ETA: 20176.6s

################################################################################
                      [1m Learning iteration 1/4000 [0m

                       Computation: 1712 steps/s (collection: 0.736s, learning 4.047s)
               Value function loss: 1.2214
                    Surrogate loss: 0.0175
             Mean action noise std: 1.00
                       Mean reward: 3.96
               Mean episode length: 20.83
                 Mean success rate: 0.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 17.81
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 4.78s
                        Total time: 9.83s
                               ETA: 19649.0s

################################################################################
                      [1m Learning iteration 2/4000 [0m

                       Computation: 1741 steps/s (collection: 0.633s, learning 4.070s)
               Value function loss: 1.3202
                    Surrogate loss: 0.0125
             Mean action noise std: 1.00
                       Mean reward: 4.84
               Mean episode length: 31.32
                 Mean success rate: 0.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 18.37
--------------------------------------------------------------------------------
                   Total timesteps: 24576
                    Iteration time: 4.70s
                        Total time: 14.53s
                               ETA: 19364.2s

################################################################################
                      [1m Learning iteration 3/4000 [0m

                       Computation: 1736 steps/s (collection: 0.658s, learning 4.058s)
               Value function loss: 1.7150
                    Surrogate loss: 0.0119
             Mean action noise std: 1.00
                       Mean reward: 5.80
               Mean episode length: 36.86
                 Mean success rate: 0.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 17.69
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 4.72s
                        Total time: 19.25s
                               ETA: 19232.3s

################################################################################
                      [1m Learning iteration 4/4000 [0m

                       Computation: 1749 steps/s (collection: 0.650s, learning 4.032s)
               Value function loss: 2.9819
                    Surrogate loss: 0.0106
             Mean action noise std: 1.00
                       Mean reward: 6.78
               Mean episode length: 41.24
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 19.32
--------------------------------------------------------------------------------
                   Total timesteps: 40960
                    Iteration time: 4.68s
                        Total time: 23.93s
                               ETA: 19124.0s

################################################################################
                      [1m Learning iteration 5/4000 [0m

                       Computation: 1721 steps/s (collection: 0.702s, learning 4.056s)
               Value function loss: 5.3388
                    Surrogate loss: 0.0092
             Mean action noise std: 1.00
                       Mean reward: 8.00
               Mean episode length: 41.65
                 Mean success rate: 0.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 19.41
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 4.76s
                        Total time: 28.69s
                               ETA: 19101.3s

################################################################################
                      [1m Learning iteration 6/4000 [0m

                       Computation: 1731 steps/s (collection: 0.680s, learning 4.051s)
               Value function loss: 6.4300
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 9.13
               Mean episode length: 46.67
                 Mean success rate: 0.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 18.92
--------------------------------------------------------------------------------
                   Total timesteps: 57344
                    Iteration time: 4.73s
                        Total time: 33.42s
                               ETA: 19067.5s

################################################################################
                      [1m Learning iteration 7/4000 [0m

                       Computation: 1734 steps/s (collection: 0.680s, learning 4.042s)
               Value function loss: 7.2178
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 8.51
               Mean episode length: 41.41
                 Mean success rate: 0.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 19.14
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 4.72s
                        Total time: 38.14s
                               ETA: 19037.1s

################################################################################
                      [1m Learning iteration 8/4000 [0m

                       Computation: 1698 steps/s (collection: 0.699s, learning 4.125s)
               Value function loss: 17.5907
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 10.14
               Mean episode length: 43.05
                 Mean success rate: 0.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 18.62
--------------------------------------------------------------------------------
                   Total timesteps: 73728
                    Iteration time: 4.82s
                        Total time: 42.96s
                               ETA: 19057.3s

################################################################################
                      [1m Learning iteration 9/4000 [0m

                       Computation: 1646 steps/s (collection: 0.887s, learning 4.088s)
               Value function loss: 17.3334
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 12.27
               Mean episode length: 46.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 21.56
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 4.97s
                        Total time: 47.94s
                               ETA: 19132.6s

################################################################################
                      [1m Learning iteration 10/4000 [0m

                       Computation: 1643 steps/s (collection: 0.902s, learning 4.082s)
               Value function loss: 21.4542
                    Surrogate loss: 0.0111
             Mean action noise std: 1.00
                       Mean reward: 14.75
               Mean episode length: 48.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.38
       Mean episode length/episode: 21.50
--------------------------------------------------------------------------------
                   Total timesteps: 90112
                    Iteration time: 4.98s
                        Total time: 52.92s
                               ETA: 19196.8s

################################################################################
                      [1m Learning iteration 11/4000 [0m

                       Computation: 1681 steps/s (collection: 0.785s, learning 4.088s)
               Value function loss: 29.9847
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 15.46
               Mean episode length: 44.99
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 20.74
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 4.87s
                        Total time: 57.80s
                               ETA: 19212.2s

################################################################################
                      [1m Learning iteration 12/4000 [0m

                       Computation: 1704 steps/s (collection: 0.725s, learning 4.080s)
               Value function loss: 39.4536
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 18.34
               Mean episode length: 50.90
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 20.43
--------------------------------------------------------------------------------
                   Total timesteps: 106496
                    Iteration time: 4.81s
                        Total time: 62.60s
                               ETA: 19204.0s

################################################################################
                      [1m Learning iteration 13/4000 [0m

                       Computation: 1663 steps/s (collection: 0.807s, learning 4.117s)
               Value function loss: 39.9188
                    Surrogate loss: 0.0093
             Mean action noise std: 1.00
                       Mean reward: 21.87
               Mean episode length: 57.86
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 22.57
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 4.92s
                        Total time: 67.53s
                               ETA: 19230.2s

################################################################################
                      [1m Learning iteration 14/4000 [0m

                       Computation: 1635 steps/s (collection: 0.933s, learning 4.077s)
               Value function loss: 39.6576
                    Surrogate loss: 0.0125
             Mean action noise std: 1.00
                       Mean reward: 24.26
               Mean episode length: 61.80
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 22.82
--------------------------------------------------------------------------------
                   Total timesteps: 122880
                    Iteration time: 5.01s
                        Total time: 72.54s
                               ETA: 19275.0s

################################################################################
                      [1m Learning iteration 15/4000 [0m

                       Computation: 1677 steps/s (collection: 0.800s, learning 4.084s)
               Value function loss: 50.6207
                    Surrogate loss: 0.0139
             Mean action noise std: 1.00
                       Mean reward: 29.57
               Mean episode length: 67.42
                 Mean success rate: 0.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 24.45
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 4.88s
                        Total time: 77.42s
                               ETA: 19282.2s

################################################################################
                      [1m Learning iteration 16/4000 [0m

                       Computation: 1686 steps/s (collection: 0.804s, learning 4.054s)
               Value function loss: 57.6000
                    Surrogate loss: 0.0131
             Mean action noise std: 1.00
                       Mean reward: 36.18
               Mean episode length: 73.16
                 Mean success rate: 0.00
                  Mean reward/step: 0.62
       Mean episode length/episode: 25.28
--------------------------------------------------------------------------------
                   Total timesteps: 139264
                    Iteration time: 4.86s
                        Total time: 82.28s
                               ETA: 19281.8s

################################################################################
                      [1m Learning iteration 17/4000 [0m

                       Computation: 1667 steps/s (collection: 0.864s, learning 4.048s)
               Value function loss: 66.9872
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 41.27
               Mean episode length: 77.74
                 Mean success rate: 0.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 23.41
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 4.91s
                        Total time: 87.19s
                               ETA: 19292.9s

################################################################################
                      [1m Learning iteration 18/4000 [0m

                       Computation: 1711 steps/s (collection: 0.743s, learning 4.045s)
               Value function loss: 66.7068
                    Surrogate loss: 0.0072
             Mean action noise std: 1.00
                       Mean reward: 44.35
               Mean episode length: 81.86
                 Mean success rate: 0.00
                  Mean reward/step: 0.66
       Mean episode length/episode: 24.90
--------------------------------------------------------------------------------
                   Total timesteps: 155648
                    Iteration time: 4.79s
                        Total time: 91.98s
                               ETA: 19276.3s

################################################################################
                      [1m Learning iteration 19/4000 [0m

                       Computation: 1674 steps/s (collection: 0.766s, learning 4.125s)
               Value function loss: 65.0332
                    Surrogate loss: 0.0033
             Mean action noise std: 1.00
                       Mean reward: 44.69
               Mean episode length: 79.04
                 Mean success rate: 0.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 24.31
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 4.89s
                        Total time: 96.87s
                               ETA: 19281.5s

################################################################################
                      [1m Learning iteration 20/4000 [0m

                       Computation: 1681 steps/s (collection: 0.820s, learning 4.052s)
               Value function loss: 91.9432
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 47.52
               Mean episode length: 80.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.66
       Mean episode length/episode: 22.02
--------------------------------------------------------------------------------
                   Total timesteps: 172032
                    Iteration time: 4.87s
                        Total time: 101.74s
                               ETA: 19282.2s

################################################################################
                      [1m Learning iteration 21/4000 [0m

                       Computation: 1695 steps/s (collection: 0.781s, learning 4.052s)
               Value function loss: 93.8236
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 45.32
               Mean episode length: 77.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.74
       Mean episode length/episode: 24.98
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 4.83s
                        Total time: 106.57s
                               ETA: 19275.2s

################################################################################
                      [1m Learning iteration 22/4000 [0m

                       Computation: 1686 steps/s (collection: 0.798s, learning 4.059s)
               Value function loss: 85.4636
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 48.29
               Mean episode length: 80.20
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 25.92
--------------------------------------------------------------------------------
                   Total timesteps: 188416
                    Iteration time: 4.86s
                        Total time: 111.43s
                               ETA: 19272.5s

################################################################################
                      [1m Learning iteration 23/4000 [0m

                       Computation: 1708 steps/s (collection: 0.760s, learning 4.035s)
               Value function loss: 140.8627
                    Surrogate loss: 0.0032
             Mean action noise std: 1.00
                       Mean reward: 58.03
               Mean episode length: 83.04
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 22.63
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 4.79s
                        Total time: 116.22s
                               ETA: 19259.4s

################################################################################
                      [1m Learning iteration 24/4000 [0m

                       Computation: 1700 steps/s (collection: 0.770s, learning 4.048s)
               Value function loss: 103.2278
                    Surrogate loss: 0.0102
             Mean action noise std: 1.00
                       Mean reward: 59.78
               Mean episode length: 88.30
                 Mean success rate: 0.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 24.60
--------------------------------------------------------------------------------
                   Total timesteps: 204800
                    Iteration time: 4.82s
                        Total time: 121.04s
                               ETA: 19250.6s

################################################################################
                      [1m Learning iteration 25/4000 [0m

                       Computation: 1718 steps/s (collection: 0.720s, learning 4.046s)
               Value function loss: 101.8999
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 65.97
               Mean episode length: 94.84
                 Mean success rate: 0.00
                  Mean reward/step: 0.82
       Mean episode length/episode: 25.36
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 4.77s
                        Total time: 125.81s
                               ETA: 19234.2s

################################################################################
                      [1m Learning iteration 26/4000 [0m

                       Computation: 1695 steps/s (collection: 0.773s, learning 4.059s)
               Value function loss: 98.1377
                    Surrogate loss: 0.0081
             Mean action noise std: 1.00
                       Mean reward: 68.27
               Mean episode length: 94.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.81
       Mean episode length/episode: 24.98
--------------------------------------------------------------------------------
                   Total timesteps: 221184
                    Iteration time: 4.83s
                        Total time: 130.64s
                               ETA: 19228.2s

################################################################################
                      [1m Learning iteration 27/4000 [0m

                       Computation: 1708 steps/s (collection: 0.767s, learning 4.028s)
               Value function loss: 115.1553
                    Surrogate loss: 0.0098
             Mean action noise std: 1.00
                       Mean reward: 84.27
               Mean episode length: 109.45
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 24.09
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 4.79s
                        Total time: 135.43s
                               ETA: 19217.2s

################################################################################
                      [1m Learning iteration 28/4000 [0m

                       Computation: 1697 steps/s (collection: 0.773s, learning 4.052s)
               Value function loss: 99.0480
                    Surrogate loss: 0.0023
             Mean action noise std: 1.00
                       Mean reward: 91.53
               Mean episode length: 117.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.82
       Mean episode length/episode: 24.60
--------------------------------------------------------------------------------
                   Total timesteps: 237568
                    Iteration time: 4.83s
                        Total time: 140.26s
                               ETA: 19210.7s

################################################################################
                      [1m Learning iteration 29/4000 [0m

                       Computation: 1704 steps/s (collection: 0.743s, learning 4.062s)
               Value function loss: 118.7850
                    Surrogate loss: 0.0038
             Mean action noise std: 1.00
                       Mean reward: 91.14
               Mean episode length: 121.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 25.76
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 4.80s
                        Total time: 145.06s
                               ETA: 19201.7s

################################################################################
                      [1m Learning iteration 30/4000 [0m

                       Computation: 1694 steps/s (collection: 0.801s, learning 4.033s)
               Value function loss: 107.9436
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 93.81
               Mean episode length: 123.20
                 Mean success rate: 0.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 253952
                    Iteration time: 4.83s
                        Total time: 149.90s
                               ETA: 19196.7s

################################################################################
                      [1m Learning iteration 31/4000 [0m

                       Computation: 1689 steps/s (collection: 0.790s, learning 4.059s)
               Value function loss: 103.7387
                    Surrogate loss: 0.0096
             Mean action noise std: 1.00
                       Mean reward: 95.55
               Mean episode length: 121.29
                 Mean success rate: 0.00
                  Mean reward/step: 0.96
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 4.85s
                        Total time: 154.75s
                               ETA: 19193.5s

################################################################################
                      [1m Learning iteration 32/4000 [0m

                       Computation: 1698 steps/s (collection: 0.774s, learning 4.049s)
               Value function loss: 106.6593
                    Surrogate loss: 0.0040
             Mean action noise std: 1.00
                       Mean reward: 101.89
               Mean episode length: 125.84
                 Mean success rate: 0.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 270336
                    Iteration time: 4.82s
                        Total time: 159.57s
                               ETA: 19187.1s

################################################################################
                      [1m Learning iteration 33/4000 [0m

                       Computation: 1693 steps/s (collection: 0.770s, learning 4.067s)
               Value function loss: 125.2020
                    Surrogate loss: 0.0053
             Mean action noise std: 1.00
                       Mean reward: 107.05
               Mean episode length: 128.57
                 Mean success rate: 0.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 25.21
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 4.84s
                        Total time: 164.41s
                               ETA: 19182.4s

################################################################################
                      [1m Learning iteration 34/4000 [0m

                       Computation: 1689 steps/s (collection: 0.782s, learning 4.067s)
               Value function loss: 129.9726
                    Surrogate loss: 0.0063
             Mean action noise std: 1.00
                       Mean reward: 108.02
               Mean episode length: 127.64
                 Mean success rate: 0.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 286720
                    Iteration time: 4.85s
                        Total time: 169.26s
                               ETA: 19179.1s

################################################################################
                      [1m Learning iteration 35/4000 [0m

                       Computation: 1687 steps/s (collection: 0.790s, learning 4.065s)
               Value function loss: 166.9810
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 109.35
               Mean episode length: 126.35
                 Mean success rate: 0.00
                  Mean reward/step: 0.98
       Mean episode length/episode: 25.21
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 4.85s
                        Total time: 174.11s
                               ETA: 19176.3s

################################################################################
                      [1m Learning iteration 36/4000 [0m

                       Computation: 1700 steps/s (collection: 0.761s, learning 4.055s)
               Value function loss: 212.9513
                    Surrogate loss: 0.0042
             Mean action noise std: 1.00
                       Mean reward: 101.29
               Mean episode length: 114.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 24.09
--------------------------------------------------------------------------------
                   Total timesteps: 303104
                    Iteration time: 4.82s
                        Total time: 178.93s
                               ETA: 19169.3s

################################################################################
                      [1m Learning iteration 37/4000 [0m

                       Computation: 1705 steps/s (collection: 0.762s, learning 4.043s)
               Value function loss: 171.1703
                    Surrogate loss: 0.0040
             Mean action noise std: 1.00
                       Mean reward: 107.45
               Mean episode length: 115.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 24.82
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 4.80s
                        Total time: 183.73s
                               ETA: 19161.1s

################################################################################
                      [1m Learning iteration 38/4000 [0m

                       Computation: 1692 steps/s (collection: 0.740s, learning 4.101s)
               Value function loss: 147.2538
                    Surrogate loss: 0.0113
             Mean action noise std: 1.00
                       Mean reward: 112.68
               Mean episode length: 118.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 319488
                    Iteration time: 4.84s
                        Total time: 188.57s
                               ETA: 19156.8s

################################################################################
                      [1m Learning iteration 39/4000 [0m

                       Computation: 1703 steps/s (collection: 0.767s, learning 4.042s)
               Value function loss: 185.8071
                    Surrogate loss: 0.0143
             Mean action noise std: 1.00
                       Mean reward: 124.09
               Mean episode length: 126.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 25.92
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 4.81s
                        Total time: 193.38s
                               ETA: 19149.4s

################################################################################
                      [1m Learning iteration 40/4000 [0m

                       Computation: 1715 steps/s (collection: 0.745s, learning 4.030s)
               Value function loss: 181.7432
                    Surrogate loss: 0.0039
             Mean action noise std: 1.00
                       Mean reward: 126.46
               Mean episode length: 129.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 25.68
--------------------------------------------------------------------------------
                   Total timesteps: 335872
                    Iteration time: 4.78s
                        Total time: 198.15s
                               ETA: 19138.8s

################################################################################
                      [1m Learning iteration 41/4000 [0m

                       Computation: 1711 steps/s (collection: 0.757s, learning 4.028s)
               Value function loss: 141.3044
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 126.62
               Mean episode length: 130.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 4.79s
                        Total time: 202.94s
                               ETA: 19129.5s

################################################################################
                      [1m Learning iteration 42/4000 [0m

                       Computation: 1713 steps/s (collection: 0.767s, learning 4.015s)
               Value function loss: 201.9186
                    Surrogate loss: 0.0029
             Mean action noise std: 1.00
                       Mean reward: 122.05
               Mean episode length: 121.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 24.67
--------------------------------------------------------------------------------
                   Total timesteps: 352256
                    Iteration time: 4.78s
                        Total time: 207.72s
                               ETA: 19120.0s

################################################################################
                      [1m Learning iteration 43/4000 [0m

                       Computation: 1725 steps/s (collection: 0.719s, learning 4.030s)
               Value function loss: 131.7708
                    Surrogate loss: 0.0033
             Mean action noise std: 1.00
                       Mean reward: 128.64
               Mean episode length: 127.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 4.75s
                        Total time: 212.47s
                               ETA: 19107.8s

################################################################################
                      [1m Learning iteration 44/4000 [0m

                       Computation: 1704 steps/s (collection: 0.767s, learning 4.040s)
               Value function loss: 177.1970
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 126.70
               Mean episode length: 121.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 25.52
--------------------------------------------------------------------------------
                   Total timesteps: 368640
                    Iteration time: 4.81s
                        Total time: 217.28s
                               ETA: 19101.0s

################################################################################
                      [1m Learning iteration 45/4000 [0m

                       Computation: 1700 steps/s (collection: 0.762s, learning 4.055s)
               Value function loss: 201.1650
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 135.98
               Mean episode length: 132.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 4.82s
                        Total time: 222.09s
                               ETA: 19095.3s

################################################################################
                      [1m Learning iteration 46/4000 [0m

                       Computation: 1689 steps/s (collection: 0.775s, learning 4.075s)
               Value function loss: 199.5729
                    Surrogate loss: 0.0032
             Mean action noise std: 1.00
                       Mean reward: 140.42
               Mean episode length: 133.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 385024
                    Iteration time: 4.85s
                        Total time: 226.94s
                               ETA: 19092.3s

################################################################################
                      [1m Learning iteration 47/4000 [0m

                       Computation: 1680 steps/s (collection: 0.826s, learning 4.049s)
               Value function loss: 185.0729
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 149.46
               Mean episode length: 137.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 4.88s
                        Total time: 231.82s
                               ETA: 19091.3s

################################################################################
                      [1m Learning iteration 48/4000 [0m

                       Computation: 1682 steps/s (collection: 0.827s, learning 4.041s)
               Value function loss: 146.6799
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 158.65
               Mean episode length: 145.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 401408
                    Iteration time: 4.87s
                        Total time: 236.69s
                               ETA: 19089.6s

################################################################################
                      [1m Learning iteration 49/4000 [0m

                       Computation: 1669 steps/s (collection: 0.856s, learning 4.050s)
               Value function loss: 147.1558
                    Surrogate loss: 0.0084
             Mean action noise std: 1.00
                       Mean reward: 164.37
               Mean episode length: 146.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 4.91s
                        Total time: 241.59s
                               ETA: 19090.7s

################################################################################
                      [1m Learning iteration 50/4000 [0m

                       Computation: 1709 steps/s (collection: 0.762s, learning 4.032s)
               Value function loss: 160.4446
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 172.26
               Mean episode length: 149.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 417792
                    Iteration time: 4.79s
                        Total time: 246.39s
                               ETA: 19082.9s

################################################################################
                      [1m Learning iteration 51/4000 [0m

                       Computation: 1705 steps/s (collection: 0.771s, learning 4.033s)
               Value function loss: 166.2103
                    Surrogate loss: 0.0097
             Mean action noise std: 1.00
                       Mean reward: 178.53
               Mean episode length: 152.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 25.92
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 4.80s
                        Total time: 251.19s
                               ETA: 19075.9s

################################################################################
                      [1m Learning iteration 52/4000 [0m

                       Computation: 1711 steps/s (collection: 0.753s, learning 4.034s)
               Value function loss: 158.3703
                    Surrogate loss: 0.0034
             Mean action noise std: 1.00
                       Mean reward: 186.86
               Mean episode length: 157.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 434176
                    Iteration time: 4.79s
                        Total time: 255.98s
                               ETA: 19067.9s

################################################################################
                      [1m Learning iteration 53/4000 [0m

                       Computation: 1717 steps/s (collection: 0.725s, learning 4.045s)
               Value function loss: 150.8064
                    Surrogate loss: 0.0123
             Mean action noise std: 1.00
                       Mean reward: 192.02
               Mean episode length: 160.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 4.77s
                        Total time: 260.75s
                               ETA: 19058.7s

################################################################################
                      [1m Learning iteration 54/4000 [0m

                       Computation: 1715 steps/s (collection: 0.737s, learning 4.039s)
               Value function loss: 135.5508
                    Surrogate loss: 0.0095
             Mean action noise std: 1.00
                       Mean reward: 183.95
               Mean episode length: 158.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 450560
                    Iteration time: 4.78s
                        Total time: 265.52s
                               ETA: 19050.1s

################################################################################
                      [1m Learning iteration 55/4000 [0m

                       Computation: 1700 steps/s (collection: 0.783s, learning 4.034s)
               Value function loss: 170.4317
                    Surrogate loss: 0.0094
             Mean action noise std: 1.00
                       Mean reward: 181.83
               Mean episode length: 156.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 4.82s
                        Total time: 270.34s
                               ETA: 19044.5s

################################################################################
                      [1m Learning iteration 56/4000 [0m

                       Computation: 1711 steps/s (collection: 0.750s, learning 4.035s)
               Value function loss: 170.6740
                    Surrogate loss: 0.0151
             Mean action noise std: 1.00
                       Mean reward: 177.85
               Mean episode length: 155.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 466944
                    Iteration time: 4.79s
                        Total time: 275.13s
                               ETA: 19036.8s

################################################################################
                      [1m Learning iteration 57/4000 [0m

                       Computation: 1720 steps/s (collection: 0.711s, learning 4.052s)
               Value function loss: 119.2485
                    Surrogate loss: 0.0135
             Mean action noise std: 1.00
                       Mean reward: 170.13
               Mean episode length: 151.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 4.76s
                        Total time: 279.89s
                               ETA: 19027.6s

################################################################################
                      [1m Learning iteration 58/4000 [0m

                       Computation: 1698 steps/s (collection: 0.784s, learning 4.038s)
               Value function loss: 120.1474
                    Surrogate loss: 0.0119
             Mean action noise std: 1.00
                       Mean reward: 188.89
               Mean episode length: 171.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 483328
                    Iteration time: 4.82s
                        Total time: 284.71s
                               ETA: 19022.5s

################################################################################
                      [1m Learning iteration 59/4000 [0m

                       Computation: 1707 steps/s (collection: 0.740s, learning 4.058s)
               Value function loss: 109.2319
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 189.76
               Mean episode length: 172.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 4.80s
                        Total time: 289.51s
                               ETA: 19015.9s

################################################################################
                      [1m Learning iteration 60/4000 [0m

                       Computation: 1698 steps/s (collection: 0.777s, learning 4.045s)
               Value function loss: 171.7139
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 190.81
               Mean episode length: 176.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 499712
                    Iteration time: 4.82s
                        Total time: 294.33s
                               ETA: 19010.8s

################################################################################
                      [1m Learning iteration 61/4000 [0m

                       Computation: 1679 steps/s (collection: 0.822s, learning 4.057s)
               Value function loss: 167.4167
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 182.81
               Mean episode length: 169.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 25.92
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 4.88s
                        Total time: 299.21s
                               ETA: 19009.4s

################################################################################
                      [1m Learning iteration 62/4000 [0m

                       Computation: 1699 steps/s (collection: 0.774s, learning 4.046s)
               Value function loss: 175.2971
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 186.41
               Mean episode length: 167.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 516096
                    Iteration time: 4.82s
                        Total time: 304.03s
                               ETA: 19004.2s

################################################################################
                      [1m Learning iteration 63/4000 [0m

                       Computation: 1698 steps/s (collection: 0.795s, learning 4.028s)
               Value function loss: 172.9105
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 187.15
               Mean episode length: 168.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 4.82s
                        Total time: 308.85s
                               ETA: 18999.2s

################################################################################
                      [1m Learning iteration 64/4000 [0m

                       Computation: 1720 steps/s (collection: 0.741s, learning 4.020s)
               Value function loss: 148.6276
                    Surrogate loss: 0.0124
             Mean action noise std: 1.00
                       Mean reward: 186.34
               Mean episode length: 165.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 532480
                    Iteration time: 4.76s
                        Total time: 313.61s
                               ETA: 18990.5s

################################################################################
                      [1m Learning iteration 65/4000 [0m

                       Computation: 1714 steps/s (collection: 0.745s, learning 4.034s)
               Value function loss: 121.0354
                    Surrogate loss: 0.0111
             Mean action noise std: 1.00
                       Mean reward: 206.02
               Mean episode length: 179.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 4.78s
                        Total time: 318.39s
                               ETA: 18983.0s

################################################################################
                      [1m Learning iteration 66/4000 [0m

                       Computation: 1713 steps/s (collection: 0.744s, learning 4.038s)
               Value function loss: 164.5241
                    Surrogate loss: 0.0150
             Mean action noise std: 1.00
                       Mean reward: 213.18
               Mean episode length: 186.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 548864
                    Iteration time: 4.78s
                        Total time: 323.17s
                               ETA: 18975.6s

################################################################################
                      [1m Learning iteration 67/4000 [0m

                       Computation: 1708 steps/s (collection: 0.755s, learning 4.040s)
               Value function loss: 155.4440
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 205.83
               Mean episode length: 180.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 4.79s
                        Total time: 327.97s
                               ETA: 18969.1s

################################################################################
                      [1m Learning iteration 68/4000 [0m

                       Computation: 1714 steps/s (collection: 0.748s, learning 4.029s)
               Value function loss: 124.1860
                    Surrogate loss: 0.0114
             Mean action noise std: 1.00
                       Mean reward: 219.62
               Mean episode length: 193.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 565248
                    Iteration time: 4.78s
                        Total time: 332.75s
                               ETA: 18961.7s

################################################################################
                      [1m Learning iteration 69/4000 [0m

                       Computation: 1722 steps/s (collection: 0.741s, learning 4.016s)
               Value function loss: 114.2382
                    Surrogate loss: 0.0086
             Mean action noise std: 1.00
                       Mean reward: 226.15
               Mean episode length: 193.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 4.76s
                        Total time: 337.50s
                               ETA: 18953.2s

################################################################################
                      [1m Learning iteration 70/4000 [0m

                       Computation: 1717 steps/s (collection: 0.735s, learning 4.034s)
               Value function loss: 173.8250
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 222.88
               Mean episode length: 190.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 581632
                    Iteration time: 4.77s
                        Total time: 342.27s
                               ETA: 18945.4s

################################################################################
                      [1m Learning iteration 71/4000 [0m

                       Computation: 1720 steps/s (collection: 0.747s, learning 4.016s)
               Value function loss: 160.8993
                    Surrogate loss: 0.0122
             Mean action noise std: 1.00
                       Mean reward: 233.10
               Mean episode length: 197.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 4.76s
                        Total time: 347.03s
                               ETA: 18937.4s

################################################################################
                      [1m Learning iteration 72/4000 [0m

                       Computation: 1706 steps/s (collection: 0.756s, learning 4.043s)
               Value function loss: 119.7821
                    Surrogate loss: 0.0100
             Mean action noise std: 1.00
                       Mean reward: 242.84
               Mean episode length: 202.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 598016
                    Iteration time: 4.80s
                        Total time: 351.83s
                               ETA: 18931.5s

################################################################################
                      [1m Learning iteration 73/4000 [0m

                       Computation: 1681 steps/s (collection: 0.852s, learning 4.019s)
               Value function loss: 144.4658
                    Surrogate loss: 0.0110
             Mean action noise std: 1.00
                       Mean reward: 233.73
               Mean episode length: 194.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 4.87s
                        Total time: 356.70s
                               ETA: 18929.5s

################################################################################
                      [1m Learning iteration 74/4000 [0m

                       Computation: 1700 steps/s (collection: 0.789s, learning 4.030s)
               Value function loss: 119.5736
                    Surrogate loss: 0.0123
             Mean action noise std: 1.00
                       Mean reward: 246.92
               Mean episode length: 206.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 614400
                    Iteration time: 4.82s
                        Total time: 361.52s
                               ETA: 18924.5s

################################################################################
                      [1m Learning iteration 75/4000 [0m

                       Computation: 1705 steps/s (collection: 0.742s, learning 4.061s)
               Value function loss: 129.4396
                    Surrogate loss: 0.0117
             Mean action noise std: 1.00
                       Mean reward: 240.34
               Mean episode length: 201.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 4.80s
                        Total time: 366.33s
                               ETA: 18918.8s

################################################################################
                      [1m Learning iteration 76/4000 [0m

                       Computation: 1700 steps/s (collection: 0.781s, learning 4.035s)
               Value function loss: 105.7853
                    Surrogate loss: 0.0423
             Mean action noise std: 1.00
                       Mean reward: 253.76
               Mean episode length: 217.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 630784
                    Iteration time: 4.82s
                        Total time: 371.14s
                               ETA: 18913.8s

################################################################################
                      [1m Learning iteration 77/4000 [0m

                       Computation: 1700 steps/s (collection: 0.794s, learning 4.023s)
               Value function loss: 116.5968
                    Surrogate loss: 0.0074
             Mean action noise std: 1.00
                       Mean reward: 270.10
               Mean episode length: 230.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 4.82s
                        Total time: 375.96s
                               ETA: 18908.8s

################################################################################
                      [1m Learning iteration 78/4000 [0m

                       Computation: 1689 steps/s (collection: 0.806s, learning 4.042s)
               Value function loss: 148.2294
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 263.22
               Mean episode length: 227.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 647168
                    Iteration time: 4.85s
                        Total time: 380.81s
                               ETA: 18905.4s

################################################################################
                      [1m Learning iteration 79/4000 [0m

                       Computation: 1718 steps/s (collection: 0.742s, learning 4.025s)
               Value function loss: 120.0967
                    Surrogate loss: 0.0069
             Mean action noise std: 1.00
                       Mean reward: 283.00
               Mean episode length: 242.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 4.77s
                        Total time: 385.58s
                               ETA: 18898.0s

################################################################################
                      [1m Learning iteration 80/4000 [0m

                       Computation: 1689 steps/s (collection: 0.809s, learning 4.041s)
               Value function loss: 106.0053
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 270.87
               Mean episode length: 240.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 663552
                    Iteration time: 4.85s
                        Total time: 390.42s
                               ETA: 18894.6s

################################################################################
                      [1m Learning iteration 81/4000 [0m

                       Computation: 1691 steps/s (collection: 0.797s, learning 4.046s)
               Value function loss: 174.8617
                    Surrogate loss: 0.0114
             Mean action noise std: 1.00
                       Mean reward: 262.85
               Mean episode length: 237.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 4.84s
                        Total time: 395.27s
                               ETA: 18890.9s

################################################################################
                      [1m Learning iteration 82/4000 [0m

                       Computation: 1661 steps/s (collection: 0.826s, learning 4.106s)
               Value function loss: 140.4523
                    Surrogate loss: 0.0085
             Mean action noise std: 1.00
                       Mean reward: 264.52
               Mean episode length: 238.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 679936
                    Iteration time: 4.93s
                        Total time: 400.20s
                               ETA: 18891.4s

################################################################################
                      [1m Learning iteration 83/4000 [0m

                       Computation: 1644 steps/s (collection: 0.870s, learning 4.112s)
               Value function loss: 189.7542
                    Surrogate loss: 0.0080
             Mean action noise std: 1.00
                       Mean reward: 247.79
               Mean episode length: 224.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 4.98s
                        Total time: 405.18s
                               ETA: 18894.0s

################################################################################
                      [1m Learning iteration 84/4000 [0m

                       Computation: 1671 steps/s (collection: 0.831s, learning 4.071s)
               Value function loss: 150.0295
                    Surrogate loss: 0.0107
             Mean action noise std: 1.00
                       Mean reward: 242.17
               Mean episode length: 218.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 696320
                    Iteration time: 4.90s
                        Total time: 410.08s
                               ETA: 18892.8s

################################################################################
                      [1m Learning iteration 85/4000 [0m

                       Computation: 1704 steps/s (collection: 0.772s, learning 4.034s)
               Value function loss: 129.8950
                    Surrogate loss: 0.0112
             Mean action noise std: 1.00
                       Mean reward: 251.10
               Mean episode length: 221.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 4.81s
                        Total time: 414.89s
                               ETA: 18887.1s

################################################################################
                      [1m Learning iteration 86/4000 [0m

                       Computation: 1708 steps/s (collection: 0.767s, learning 4.027s)
               Value function loss: 124.6111
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 259.68
               Mean episode length: 227.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 712704
                    Iteration time: 4.79s
                        Total time: 419.68s
                               ETA: 18880.9s

################################################################################
                      [1m Learning iteration 87/4000 [0m

                       Computation: 1716 steps/s (collection: 0.757s, learning 4.016s)
               Value function loss: 212.0935
                    Surrogate loss: 0.0081
             Mean action noise std: 1.00
                       Mean reward: 251.45
               Mean episode length: 215.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 4.77s
                        Total time: 424.46s
                               ETA: 18873.8s

################################################################################
                      [1m Learning iteration 88/4000 [0m

                       Computation: 1714 steps/s (collection: 0.747s, learning 4.031s)
               Value function loss: 224.5862
                    Surrogate loss: 0.0120
             Mean action noise std: 1.00
                       Mean reward: 257.31
               Mean episode length: 215.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 729088
                    Iteration time: 4.78s
                        Total time: 429.23s
                               ETA: 18867.0s

################################################################################
                      [1m Learning iteration 89/4000 [0m

                       Computation: 1711 steps/s (collection: 0.741s, learning 4.046s)
               Value function loss: 155.7092
                    Surrogate loss: 0.0080
             Mean action noise std: 1.00
                       Mean reward: 247.83
               Mean episode length: 207.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 4.79s
                        Total time: 434.02s
                               ETA: 18860.6s

################################################################################
                      [1m Learning iteration 90/4000 [0m

                       Computation: 1695 steps/s (collection: 0.792s, learning 4.040s)
               Value function loss: 160.3722
                    Surrogate loss: 0.0053
             Mean action noise std: 1.00
                       Mean reward: 228.32
               Mean episode length: 189.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 745472
                    Iteration time: 4.83s
                        Total time: 438.85s
                               ETA: 18856.2s

################################################################################
                      [1m Learning iteration 91/4000 [0m

                       Computation: 1689 steps/s (collection: 0.813s, learning 4.035s)
               Value function loss: 127.7453
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 222.19
               Mean episode length: 184.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 4.85s
                        Total time: 443.70s
                               ETA: 18852.5s

################################################################################
                      [1m Learning iteration 92/4000 [0m

                       Computation: 1695 steps/s (collection: 0.778s, learning 4.052s)
               Value function loss: 165.9058
                    Surrogate loss: 0.0085
             Mean action noise std: 1.00
                       Mean reward: 226.62
               Mean episode length: 191.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 761856
                    Iteration time: 4.83s
                        Total time: 448.53s
                               ETA: 18848.0s

################################################################################
                      [1m Learning iteration 93/4000 [0m

                       Computation: 1671 steps/s (collection: 0.764s, learning 4.137s)
               Value function loss: 184.5953
                    Surrogate loss: 0.0085
             Mean action noise std: 1.00
                       Mean reward: 231.75
               Mean episode length: 193.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 4.90s
                        Total time: 453.43s
                               ETA: 18846.4s

################################################################################
                      [1m Learning iteration 94/4000 [0m

                       Computation: 1697 steps/s (collection: 0.795s, learning 4.031s)
               Value function loss: 110.1521
                    Surrogate loss: 0.0383
             Mean action noise std: 1.00
                       Mean reward: 243.00
               Mean episode length: 199.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 778240
                    Iteration time: 4.83s
                        Total time: 458.26s
                               ETA: 18841.7s

################################################################################
                      [1m Learning iteration 95/4000 [0m

                       Computation: 1708 steps/s (collection: 0.751s, learning 4.044s)
               Value function loss: 151.3476
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 242.14
               Mean episode length: 201.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 4.79s
                        Total time: 463.05s
                               ETA: 18835.7s

################################################################################
                      [1m Learning iteration 96/4000 [0m

                       Computation: 1709 steps/s (collection: 0.722s, learning 4.069s)
               Value function loss: 139.9896
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 261.26
               Mean episode length: 220.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 794624
                    Iteration time: 4.79s
                        Total time: 467.84s
                               ETA: 18829.5s

################################################################################
                      [1m Learning iteration 97/4000 [0m

                       Computation: 1680 steps/s (collection: 0.824s, learning 4.051s)
               Value function loss: 131.3548
                    Surrogate loss: 0.0090
             Mean action noise std: 1.00
                       Mean reward: 255.49
               Mean episode length: 216.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 4.88s
                        Total time: 472.72s
                               ETA: 18826.8s

################################################################################
                      [1m Learning iteration 98/4000 [0m

                       Computation: 1677 steps/s (collection: 0.823s, learning 4.060s)
               Value function loss: 159.1733
                    Surrogate loss: 0.0095
             Mean action noise std: 1.00
                       Mean reward: 275.27
               Mean episode length: 229.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 811008
                    Iteration time: 4.88s
                        Total time: 477.60s
                               ETA: 18824.3s

################################################################################
                      [1m Learning iteration 99/4000 [0m

                       Computation: 1718 steps/s (collection: 0.727s, learning 4.039s)
               Value function loss: 182.7503
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 282.97
               Mean episode length: 232.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 4.77s
                        Total time: 482.37s
                               ETA: 18817.3s

################################################################################
                     [1m Learning iteration 100/4000 [0m

                       Computation: 1729 steps/s (collection: 0.720s, learning 4.016s)
               Value function loss: 146.8266
                    Surrogate loss: 0.0101
             Mean action noise std: 1.00
                       Mean reward: 291.68
               Mean episode length: 237.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 827392
                    Iteration time: 4.74s
                        Total time: 487.11s
                               ETA: 18809.1s

################################################################################
                     [1m Learning iteration 101/4000 [0m

                       Computation: 1711 steps/s (collection: 0.732s, learning 4.054s)
               Value function loss: 188.6404
                    Surrogate loss: 0.0094
             Mean action noise std: 1.00
                       Mean reward: 302.79
               Mean episode length: 244.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 4.79s
                        Total time: 491.89s
                               ETA: 18802.9s

################################################################################
                     [1m Learning iteration 102/4000 [0m

                       Computation: 1701 steps/s (collection: 0.755s, learning 4.059s)
               Value function loss: 193.2492
                    Surrogate loss: 0.0128
             Mean action noise std: 1.00
                       Mean reward: 294.73
               Mean episode length: 233.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 843776
                    Iteration time: 4.81s
                        Total time: 496.71s
                               ETA: 18797.7s

################################################################################
                     [1m Learning iteration 103/4000 [0m

                       Computation: 1677 steps/s (collection: 0.825s, learning 4.057s)
               Value function loss: 222.7084
                    Surrogate loss: 0.0160
             Mean action noise std: 1.00
                       Mean reward: 306.13
               Mean episode length: 236.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 4.88s
                        Total time: 501.59s
                               ETA: 18795.1s

################################################################################
                     [1m Learning iteration 104/4000 [0m

                       Computation: 1686 steps/s (collection: 0.763s, learning 4.095s)
               Value function loss: 203.3103
                    Surrogate loss: 0.0316
             Mean action noise std: 1.00
                       Mean reward: 308.65
               Mean episode length: 237.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 860160
                    Iteration time: 4.86s
                        Total time: 506.45s
                               ETA: 18791.6s

################################################################################
                     [1m Learning iteration 105/4000 [0m

                       Computation: 1682 steps/s (collection: 0.783s, learning 4.085s)
               Value function loss: 180.5740
                    Surrogate loss: 0.0166
             Mean action noise std: 1.00
                       Mean reward: 279.99
               Mean episode length: 212.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 4.87s
                        Total time: 511.32s
                               ETA: 18788.5s

################################################################################
                     [1m Learning iteration 106/4000 [0m

                       Computation: 1677 steps/s (collection: 0.799s, learning 4.084s)
               Value function loss: 204.2167
                    Surrogate loss: 0.0158
             Mean action noise std: 1.00
                       Mean reward: 300.84
               Mean episode length: 222.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 876544
                    Iteration time: 4.88s
                        Total time: 516.20s
                               ETA: 18785.8s

################################################################################
                     [1m Learning iteration 107/4000 [0m

                       Computation: 1684 steps/s (collection: 0.763s, learning 4.099s)
               Value function loss: 144.7037
                    Surrogate loss: 0.0175
             Mean action noise std: 1.00
                       Mean reward: 303.86
               Mean episode length: 225.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 4.86s
                        Total time: 521.06s
                               ETA: 18782.3s

################################################################################
                     [1m Learning iteration 108/4000 [0m

                       Computation: 1693 steps/s (collection: 0.765s, learning 4.073s)
               Value function loss: 275.9523
                    Surrogate loss: 0.0144
             Mean action noise std: 1.00
                       Mean reward: 327.33
               Mean episode length: 241.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 892928
                    Iteration time: 4.84s
                        Total time: 525.90s
                               ETA: 18778.0s

################################################################################
                     [1m Learning iteration 109/4000 [0m

                       Computation: 1682 steps/s (collection: 0.766s, learning 4.101s)
               Value function loss: 193.7210
                    Surrogate loss: 0.0100
             Mean action noise std: 1.00
                       Mean reward: 321.74
               Mean episode length: 244.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 4.87s
                        Total time: 530.77s
                               ETA: 18774.7s

################################################################################
                     [1m Learning iteration 110/4000 [0m

                       Computation: 1688 steps/s (collection: 0.778s, learning 4.075s)
               Value function loss: 93.0820
                    Surrogate loss: 0.0106
             Mean action noise std: 1.00
                       Mean reward: 321.36
               Mean episode length: 246.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 909312
                    Iteration time: 4.85s
                        Total time: 535.62s
                               ETA: 18770.9s

################################################################################
                     [1m Learning iteration 111/4000 [0m

                       Computation: 1684 steps/s (collection: 0.802s, learning 4.062s)
               Value function loss: 138.1553
                    Surrogate loss: 0.0123
             Mean action noise std: 1.00
                       Mean reward: 298.88
               Mean episode length: 231.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 4.86s
                        Total time: 540.48s
                               ETA: 18767.3s

################################################################################
                     [1m Learning iteration 112/4000 [0m

                       Computation: 1700 steps/s (collection: 0.782s, learning 4.034s)
               Value function loss: 136.3010
                    Surrogate loss: 0.0095
             Mean action noise std: 1.00
                       Mean reward: 275.21
               Mean episode length: 215.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 925696
                    Iteration time: 4.82s
                        Total time: 545.30s
                               ETA: 18762.2s

################################################################################
                     [1m Learning iteration 113/4000 [0m

                       Computation: 1692 steps/s (collection: 0.772s, learning 4.069s)
               Value function loss: 166.9103
                    Surrogate loss: 0.0041
             Mean action noise std: 1.00
                       Mean reward: 249.25
               Mean episode length: 199.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 4.84s
                        Total time: 550.14s
                               ETA: 18757.9s

################################################################################
                     [1m Learning iteration 114/4000 [0m

                       Computation: 1670 steps/s (collection: 0.754s, learning 4.149s)
               Value function loss: 142.5701
                    Surrogate loss: 0.0066
             Mean action noise std: 1.00
                       Mean reward: 231.98
               Mean episode length: 189.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 942080
                    Iteration time: 4.90s
                        Total time: 555.05s
                               ETA: 18755.7s

################################################################################
                     [1m Learning iteration 115/4000 [0m

                       Computation: 1721 steps/s (collection: 0.715s, learning 4.043s)
               Value function loss: 139.4521
                    Surrogate loss: 0.0087
             Mean action noise std: 1.00
                       Mean reward: 225.51
               Mean episode length: 185.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 4.76s
                        Total time: 559.80s
                               ETA: 18748.6s

################################################################################
                     [1m Learning iteration 116/4000 [0m

                       Computation: 1690 steps/s (collection: 0.786s, learning 4.060s)
               Value function loss: 152.6459
                    Surrogate loss: 0.0149
             Mean action noise std: 1.00
                       Mean reward: 235.90
               Mean episode length: 190.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 958464
                    Iteration time: 4.85s
                        Total time: 564.65s
                               ETA: 18744.4s

################################################################################
                     [1m Learning iteration 117/4000 [0m

                       Computation: 1712 steps/s (collection: 0.739s, learning 4.045s)
               Value function loss: 146.4300
                    Surrogate loss: 0.0160
             Mean action noise std: 1.00
                       Mean reward: 244.88
               Mean episode length: 200.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 4.78s
                        Total time: 569.43s
                               ETA: 18738.2s

################################################################################
                     [1m Learning iteration 118/4000 [0m

                       Computation: 1727 steps/s (collection: 0.719s, learning 4.025s)
               Value function loss: 308.9230
                    Surrogate loss: 0.0061
             Mean action noise std: 1.00
                       Mean reward: 268.76
               Mean episode length: 220.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 974848
                    Iteration time: 4.74s
                        Total time: 574.18s
                               ETA: 18730.7s

################################################################################
                     [1m Learning iteration 119/4000 [0m

                       Computation: 1692 steps/s (collection: 0.756s, learning 4.084s)
               Value function loss: 168.0038
                    Surrogate loss: 0.0119
             Mean action noise std: 1.00
                       Mean reward: 272.44
               Mean episode length: 226.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 4.84s
                        Total time: 579.02s
                               ETA: 18726.4s

################################################################################
                     [1m Learning iteration 120/4000 [0m

                       Computation: 1687 steps/s (collection: 0.779s, learning 4.076s)
               Value function loss: 210.2700
                    Surrogate loss: 0.0162
             Mean action noise std: 1.00
                       Mean reward: 290.57
               Mean episode length: 233.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 991232
                    Iteration time: 4.85s
                        Total time: 583.87s
                               ETA: 18722.5s

################################################################################
                     [1m Learning iteration 121/4000 [0m

                       Computation: 1705 steps/s (collection: 0.751s, learning 4.053s)
               Value function loss: 135.6187
                    Surrogate loss: 0.0131
             Mean action noise std: 1.00
                       Mean reward: 297.89
               Mean episode length: 241.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 4.80s
                        Total time: 588.68s
                               ETA: 18717.0s

################################################################################
                     [1m Learning iteration 122/4000 [0m

                       Computation: 1707 steps/s (collection: 0.741s, learning 4.057s)
               Value function loss: 314.6280
                    Surrogate loss: 0.0072
             Mean action noise std: 1.00
                       Mean reward: 314.20
               Mean episode length: 250.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1007616
                    Iteration time: 4.80s
                        Total time: 593.47s
                               ETA: 18711.3s

################################################################################
                     [1m Learning iteration 123/4000 [0m

                       Computation: 1690 steps/s (collection: 0.752s, learning 4.094s)
               Value function loss: 232.5993
                    Surrogate loss: 0.0085
             Mean action noise std: 1.00
                       Mean reward: 309.02
               Mean episode length: 245.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 4.85s
                        Total time: 598.32s
                               ETA: 18707.2s

################################################################################
                     [1m Learning iteration 124/4000 [0m

                       Computation: 1708 steps/s (collection: 0.755s, learning 4.041s)
               Value function loss: 161.9119
                    Surrogate loss: 0.0131
             Mean action noise std: 1.00
                       Mean reward: 318.03
               Mean episode length: 246.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1024000
                    Iteration time: 4.80s
                        Total time: 603.12s
                               ETA: 18701.4s

################################################################################
                     [1m Learning iteration 125/4000 [0m

                       Computation: 1701 steps/s (collection: 0.720s, learning 4.094s)
               Value function loss: 206.5685
                    Surrogate loss: 0.0193
             Mean action noise std: 1.00
                       Mean reward: 316.98
               Mean episode length: 246.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 4.81s
                        Total time: 607.93s
                               ETA: 18696.3s

################################################################################
                     [1m Learning iteration 126/4000 [0m

                       Computation: 1702 steps/s (collection: 0.776s, learning 4.035s)
               Value function loss: 282.4230
                    Surrogate loss: 0.0117
             Mean action noise std: 1.00
                       Mean reward: 316.45
               Mean episode length: 248.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1040384
                    Iteration time: 4.81s
                        Total time: 612.74s
                               ETA: 18691.0s

################################################################################
                     [1m Learning iteration 127/4000 [0m

                       Computation: 1721 steps/s (collection: 0.724s, learning 4.036s)
               Value function loss: 230.8196
                    Surrogate loss: 0.0152
             Mean action noise std: 1.00
                       Mean reward: 313.73
               Mean episode length: 245.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 4.76s
                        Total time: 617.50s
                               ETA: 18684.2s

################################################################################
                     [1m Learning iteration 128/4000 [0m

                       Computation: 1722 steps/s (collection: 0.734s, learning 4.023s)
               Value function loss: 178.9205
                    Surrogate loss: 0.0107
             Mean action noise std: 1.00
                       Mean reward: 318.88
               Mean episode length: 251.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1056768
                    Iteration time: 4.76s
                        Total time: 622.26s
                               ETA: 18677.4s

################################################################################
                     [1m Learning iteration 129/4000 [0m

                       Computation: 1720 steps/s (collection: 0.725s, learning 4.036s)
               Value function loss: 148.1386
                    Surrogate loss: 0.0104
             Mean action noise std: 1.00
                       Mean reward: 325.49
               Mean episode length: 257.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 4.76s
                        Total time: 627.02s
                               ETA: 18670.7s

################################################################################
                     [1m Learning iteration 130/4000 [0m

                       Computation: 1703 steps/s (collection: 0.738s, learning 4.071s)
               Value function loss: 145.9491
                    Surrogate loss: 0.0186
             Mean action noise std: 1.00
                       Mean reward: 337.97
               Mean episode length: 269.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 1073152
                    Iteration time: 4.81s
                        Total time: 631.83s
                               ETA: 18665.5s

################################################################################
                     [1m Learning iteration 131/4000 [0m

                       Computation: 1708 steps/s (collection: 0.715s, learning 4.079s)
               Value function loss: 167.2024
                    Surrogate loss: 0.0256
             Mean action noise std: 1.00
                       Mean reward: 340.12
               Mean episode length: 270.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 4.79s
                        Total time: 636.62s
                               ETA: 18659.8s

################################################################################
                     [1m Learning iteration 132/4000 [0m

                       Computation: 1710 steps/s (collection: 0.707s, learning 4.083s)
               Value function loss: 236.9743
                    Surrogate loss: 0.0139
             Mean action noise std: 1.00
                       Mean reward: 333.11
               Mean episode length: 269.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1089536
                    Iteration time: 4.79s
                        Total time: 641.41s
                               ETA: 18654.0s

################################################################################
                     [1m Learning iteration 133/4000 [0m

                       Computation: 1698 steps/s (collection: 0.754s, learning 4.069s)
               Value function loss: 220.4827
                    Surrogate loss: 0.0132
             Mean action noise std: 1.00
                       Mean reward: 350.27
               Mean episode length: 278.45
                 Mean success rate: 0.50
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 4.82s
                        Total time: 646.23s
                               ETA: 18649.2s

################################################################################
                     [1m Learning iteration 134/4000 [0m

                       Computation: 1716 steps/s (collection: 0.727s, learning 4.046s)
               Value function loss: 238.9668
                    Surrogate loss: 0.0086
             Mean action noise std: 1.00
                       Mean reward: 382.88
               Mean episode length: 299.23
                 Mean success rate: 0.50
                  Mean reward/step: 1.31
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1105920
                    Iteration time: 4.77s
                        Total time: 651.01s
                               ETA: 18642.9s

################################################################################
                     [1m Learning iteration 135/4000 [0m

                       Computation: 1715 steps/s (collection: 0.742s, learning 4.034s)
               Value function loss: 250.0313
                    Surrogate loss: 0.0290
             Mean action noise std: 1.00
                       Mean reward: 386.17
               Mean episode length: 302.87
                 Mean success rate: 0.50
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 4.78s
                        Total time: 655.78s
                               ETA: 18636.8s

################################################################################
                     [1m Learning iteration 136/4000 [0m

                       Computation: 1709 steps/s (collection: 0.765s, learning 4.027s)
               Value function loss: 372.4121
                    Surrogate loss: 0.0170
             Mean action noise std: 1.00
                       Mean reward: 381.15
               Mean episode length: 297.66
                 Mean success rate: 0.50
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1122304
                    Iteration time: 4.79s
                        Total time: 660.57s
                               ETA: 18631.1s

################################################################################
                     [1m Learning iteration 137/4000 [0m

                       Computation: 1715 steps/s (collection: 0.738s, learning 4.038s)
               Value function loss: 261.6779
                    Surrogate loss: 0.0115
             Mean action noise std: 1.00
                       Mean reward: 391.14
               Mean episode length: 298.00
                 Mean success rate: 0.50
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 4.78s
                        Total time: 665.35s
                               ETA: 18625.0s

################################################################################
                     [1m Learning iteration 138/4000 [0m

                       Computation: 1723 steps/s (collection: 0.700s, learning 4.053s)
               Value function loss: 285.9366
                    Surrogate loss: 0.0155
             Mean action noise std: 1.00
                       Mean reward: 404.04
               Mean episode length: 305.69
                 Mean success rate: 0.50
                  Mean reward/step: 1.31
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1138688
                    Iteration time: 4.75s
                        Total time: 670.10s
                               ETA: 18618.3s

################################################################################
                     [1m Learning iteration 139/4000 [0m

                       Computation: 1712 steps/s (collection: 0.729s, learning 4.054s)
               Value function loss: 348.7269
                    Surrogate loss: 0.0108
             Mean action noise std: 1.00
                       Mean reward: 369.10
               Mean episode length: 290.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 4.78s
                        Total time: 674.89s
                               ETA: 18612.4s

################################################################################
                     [1m Learning iteration 140/4000 [0m

                       Computation: 1712 steps/s (collection: 0.731s, learning 4.054s)
               Value function loss: 215.7521
                    Surrogate loss: 0.0115
             Mean action noise std: 1.00
                       Mean reward: 340.12
               Mean episode length: 265.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1155072
                    Iteration time: 4.78s
                        Total time: 679.67s
                               ETA: 18606.6s

################################################################################
                     [1m Learning iteration 141/4000 [0m

                       Computation: 1705 steps/s (collection: 0.760s, learning 4.043s)
               Value function loss: 211.9198
                    Surrogate loss: 0.0132
             Mean action noise std: 1.00
                       Mean reward: 358.90
               Mean episode length: 275.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 4.80s
                        Total time: 684.48s
                               ETA: 18601.3s

################################################################################
                     [1m Learning iteration 142/4000 [0m

                       Computation: 1706 steps/s (collection: 0.752s, learning 4.048s)
               Value function loss: 185.3958
                    Surrogate loss: 0.0305
             Mean action noise std: 1.00
                       Mean reward: 351.00
               Mean episode length: 269.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 1171456
                    Iteration time: 4.80s
                        Total time: 689.28s
                               ETA: 18596.0s

################################################################################
                     [1m Learning iteration 143/4000 [0m

                       Computation: 1704 steps/s (collection: 0.749s, learning 4.056s)
               Value function loss: 336.8945
                    Surrogate loss: 0.0356
             Mean action noise std: 1.00
                       Mean reward: 327.44
               Mean episode length: 248.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 4.81s
                        Total time: 694.08s
                               ETA: 18590.8s

################################################################################
                     [1m Learning iteration 144/4000 [0m

                       Computation: 1718 steps/s (collection: 0.717s, learning 4.052s)
               Value function loss: 203.1923
                    Surrogate loss: 0.0098
             Mean action noise std: 1.00
                       Mean reward: 307.73
               Mean episode length: 226.37
                 Mean success rate: 0.50
                  Mean reward/step: 1.45
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1187840
                    Iteration time: 4.77s
                        Total time: 698.85s
                               ETA: 18584.6s

################################################################################
                     [1m Learning iteration 145/4000 [0m

                       Computation: 1687 steps/s (collection: 0.792s, learning 4.061s)
               Value function loss: 237.7662
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 297.02
               Mean episode length: 211.15
                 Mean success rate: 0.50
                  Mean reward/step: 1.32
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 4.85s
                        Total time: 703.70s
                               ETA: 18580.7s

################################################################################
                     [1m Learning iteration 146/4000 [0m

                       Computation: 1714 steps/s (collection: 0.727s, learning 4.051s)
               Value function loss: 111.3079
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 299.47
               Mean episode length: 208.58
                 Mean success rate: 0.50
                  Mean reward/step: 1.24
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1204224
                    Iteration time: 4.78s
                        Total time: 708.48s
                               ETA: 18574.8s

################################################################################
                     [1m Learning iteration 147/4000 [0m

                       Computation: 1713 steps/s (collection: 0.732s, learning 4.049s)
               Value function loss: 137.2186
                    Surrogate loss: 0.0123
             Mean action noise std: 1.01
                       Mean reward: 286.50
               Mean episode length: 204.03
                 Mean success rate: 0.50
                  Mean reward/step: 1.27
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 4.78s
                        Total time: 713.26s
                               ETA: 18568.9s

################################################################################
                     [1m Learning iteration 148/4000 [0m

                       Computation: 1717 steps/s (collection: 0.745s, learning 4.025s)
               Value function loss: 143.7959
                    Surrogate loss: 0.0106
             Mean action noise std: 1.01
                       Mean reward: 275.34
               Mean episode length: 197.63
                 Mean success rate: 0.50
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1220608
                    Iteration time: 4.77s
                        Total time: 718.03s
                               ETA: 18562.8s

################################################################################
                     [1m Learning iteration 149/4000 [0m

                       Computation: 1729 steps/s (collection: 0.711s, learning 4.026s)
               Value function loss: 178.2069
                    Surrogate loss: 0.0043
             Mean action noise std: 1.01
                       Mean reward: 269.56
               Mean episode length: 192.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 4.74s
                        Total time: 722.77s
                               ETA: 18555.9s

################################################################################
                     [1m Learning iteration 150/4000 [0m

                       Computation: 1716 steps/s (collection: 0.739s, learning 4.035s)
               Value function loss: 156.6367
                    Surrogate loss: 0.0096
             Mean action noise std: 1.01
                       Mean reward: 282.42
               Mean episode length: 202.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1236992
                    Iteration time: 4.77s
                        Total time: 727.54s
                               ETA: 18550.0s

################################################################################
                     [1m Learning iteration 151/4000 [0m

                       Computation: 1734 steps/s (collection: 0.695s, learning 4.029s)
               Value function loss: 162.7536
                    Surrogate loss: 0.0102
             Mean action noise std: 1.01
                       Mean reward: 294.45
               Mean episode length: 215.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 4.72s
                        Total time: 732.27s
                               ETA: 18542.7s

################################################################################
                     [1m Learning iteration 152/4000 [0m

                       Computation: 1703 steps/s (collection: 0.768s, learning 4.042s)
               Value function loss: 102.4115
                    Surrogate loss: 0.0139
             Mean action noise std: 1.01
                       Mean reward: 292.62
               Mean episode length: 215.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1253376
                    Iteration time: 4.81s
                        Total time: 737.08s
                               ETA: 18537.7s

################################################################################
                     [1m Learning iteration 153/4000 [0m

                       Computation: 1711 steps/s (collection: 0.738s, learning 4.047s)
               Value function loss: 192.1219
                    Surrogate loss: 0.0111
             Mean action noise std: 1.01
                       Mean reward: 298.89
               Mean episode length: 217.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 4.79s
                        Total time: 741.86s
                               ETA: 18532.1s

################################################################################
                     [1m Learning iteration 154/4000 [0m

                       Computation: 1686 steps/s (collection: 0.777s, learning 4.081s)
               Value function loss: 221.6230
                    Surrogate loss: 0.0018
             Mean action noise std: 1.00
                       Mean reward: 285.89
               Mean episode length: 214.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1269760
                    Iteration time: 4.86s
                        Total time: 746.72s
                               ETA: 18528.3s

################################################################################
                     [1m Learning iteration 155/4000 [0m

                       Computation: 1681 steps/s (collection: 0.744s, learning 4.127s)
               Value function loss: 292.0495
                    Surrogate loss: 0.0072
             Mean action noise std: 1.00
                       Mean reward: 303.95
               Mean episode length: 229.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 4.87s
                        Total time: 751.59s
                               ETA: 18524.8s

################################################################################
                     [1m Learning iteration 156/4000 [0m

                       Computation: 1665 steps/s (collection: 0.779s, learning 4.139s)
               Value function loss: 216.2131
                    Surrogate loss: 0.0074
             Mean action noise std: 1.00
                       Mean reward: 279.18
               Mean episode length: 210.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 1286144
                    Iteration time: 4.92s
                        Total time: 756.51s
                               ETA: 18522.4s

################################################################################
                     [1m Learning iteration 157/4000 [0m

                       Computation: 1680 steps/s (collection: 0.773s, learning 4.101s)
               Value function loss: 82.2236
                    Surrogate loss: 0.0576
             Mean action noise std: 1.00
                       Mean reward: 271.79
               Mean episode length: 205.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 4.87s
                        Total time: 761.38s
                               ETA: 18519.0s

################################################################################
                     [1m Learning iteration 158/4000 [0m

                       Computation: 1673 steps/s (collection: 0.737s, learning 4.159s)
               Value function loss: 130.7664
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 270.98
               Mean episode length: 206.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1302528
                    Iteration time: 4.90s
                        Total time: 766.28s
                               ETA: 18516.0s

################################################################################
                     [1m Learning iteration 159/4000 [0m

                       Computation: 1680 steps/s (collection: 0.775s, learning 4.101s)
               Value function loss: 219.8739
                    Surrogate loss: 0.0026
             Mean action noise std: 1.00
                       Mean reward: 295.14
               Mean episode length: 223.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 4.88s
                        Total time: 771.16s
                               ETA: 18512.6s

################################################################################
                     [1m Learning iteration 160/4000 [0m

                       Computation: 1694 steps/s (collection: 0.782s, learning 4.051s)
               Value function loss: 181.1952
                    Surrogate loss: 0.0053
             Mean action noise std: 1.00
                       Mean reward: 285.63
               Mean episode length: 215.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1318912
                    Iteration time: 4.83s
                        Total time: 775.99s
                               ETA: 18508.1s

################################################################################
                     [1m Learning iteration 161/4000 [0m

                       Computation: 1701 steps/s (collection: 0.731s, learning 4.084s)
               Value function loss: 211.2966
                    Surrogate loss: 0.0037
             Mean action noise std: 1.00
                       Mean reward: 260.43
               Mean episode length: 201.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 4.81s
                        Total time: 780.80s
                               ETA: 18503.1s

################################################################################
                     [1m Learning iteration 162/4000 [0m

                       Computation: 1685 steps/s (collection: 0.767s, learning 4.094s)
               Value function loss: 231.1454
                    Surrogate loss: 0.0000
             Mean action noise std: 1.00
                       Mean reward: 277.01
               Mean episode length: 220.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1335296
                    Iteration time: 4.86s
                        Total time: 785.66s
                               ETA: 18499.3s

################################################################################
                     [1m Learning iteration 163/4000 [0m

                       Computation: 1692 steps/s (collection: 0.778s, learning 4.062s)
               Value function loss: 252.9149
                    Surrogate loss: 0.0037
             Mean action noise std: 1.00
                       Mean reward: 281.08
               Mean episode length: 224.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 4.84s
                        Total time: 790.50s
                               ETA: 18494.9s

################################################################################
                     [1m Learning iteration 164/4000 [0m

                       Computation: 1699 steps/s (collection: 0.759s, learning 4.061s)
               Value function loss: 292.6732
                    Surrogate loss: 0.0113
             Mean action noise std: 1.00
                       Mean reward: 281.29
               Mean episode length: 230.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1351680
                    Iteration time: 4.82s
                        Total time: 795.32s
                               ETA: 18490.1s

################################################################################
                     [1m Learning iteration 165/4000 [0m

                       Computation: 1682 steps/s (collection: 0.763s, learning 4.105s)
               Value function loss: 190.4260
                    Surrogate loss: 0.0094
             Mean action noise std: 1.00
                       Mean reward: 283.63
               Mean episode length: 234.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 4.87s
                        Total time: 800.19s
                               ETA: 18486.4s

################################################################################
                     [1m Learning iteration 166/4000 [0m

                       Computation: 1701 steps/s (collection: 0.703s, learning 4.112s)
               Value function loss: 161.1569
                    Surrogate loss: 0.0163
             Mean action noise std: 1.00
                       Mean reward: 289.15
               Mean episode length: 239.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 1368064
                    Iteration time: 4.82s
                        Total time: 805.01s
                               ETA: 18481.4s

################################################################################
                     [1m Learning iteration 167/4000 [0m

                       Computation: 1723 steps/s (collection: 0.704s, learning 4.048s)
               Value function loss: 165.9730
                    Surrogate loss: 0.0156
             Mean action noise std: 1.00
                       Mean reward: 311.14
               Mean episode length: 253.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 4.75s
                        Total time: 809.76s
                               ETA: 18475.1s

################################################################################
                     [1m Learning iteration 168/4000 [0m

                       Computation: 1726 steps/s (collection: 0.699s, learning 4.046s)
               Value function loss: 168.7007
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 324.29
               Mean episode length: 262.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1384448
                    Iteration time: 4.75s
                        Total time: 814.51s
                               ETA: 18468.5s

################################################################################
                     [1m Learning iteration 169/4000 [0m

                       Computation: 1729 steps/s (collection: 0.701s, learning 4.037s)
               Value function loss: 186.3546
                    Surrogate loss: 0.0118
             Mean action noise std: 1.00
                       Mean reward: 332.70
               Mean episode length: 266.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 4.74s
                        Total time: 819.24s
                               ETA: 18461.9s

################################################################################
                     [1m Learning iteration 170/4000 [0m

                       Computation: 1722 steps/s (collection: 0.697s, learning 4.060s)
               Value function loss: 202.8651
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 350.95
               Mean episode length: 281.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1400832
                    Iteration time: 4.76s
                        Total time: 824.00s
                               ETA: 18455.7s

################################################################################
                     [1m Learning iteration 171/4000 [0m

                       Computation: 1708 steps/s (collection: 0.749s, learning 4.047s)
               Value function loss: 232.3112
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 342.82
               Mean episode length: 272.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 4.80s
                        Total time: 828.80s
                               ETA: 18450.3s

################################################################################
                     [1m Learning iteration 172/4000 [0m

                       Computation: 1708 steps/s (collection: 0.736s, learning 4.060s)
               Value function loss: 141.1008
                    Surrogate loss: 0.0110
             Mean action noise std: 1.00
                       Mean reward: 351.84
               Mean episode length: 279.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1417216
                    Iteration time: 4.80s
                        Total time: 833.59s
                               ETA: 18445.0s

################################################################################
                     [1m Learning iteration 173/4000 [0m

                       Computation: 1675 steps/s (collection: 0.743s, learning 4.147s)
               Value function loss: 124.9419
                    Surrogate loss: 0.0216
             Mean action noise std: 1.00
                       Mean reward: 344.82
               Mean episode length: 278.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 4.89s
                        Total time: 838.48s
                               ETA: 18441.8s

################################################################################
                     [1m Learning iteration 174/4000 [0m

                       Computation: 1690 steps/s (collection: 0.713s, learning 4.134s)
               Value function loss: 230.3739
                    Surrogate loss: 0.0093
             Mean action noise std: 1.00
                       Mean reward: 346.60
               Mean episode length: 278.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1433600
                    Iteration time: 4.85s
                        Total time: 843.33s
                               ETA: 18437.6s

################################################################################
                     [1m Learning iteration 175/4000 [0m

                       Computation: 1714 steps/s (collection: 0.740s, learning 4.038s)
               Value function loss: 236.6569
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 345.92
               Mean episode length: 276.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 4.78s
                        Total time: 848.11s
                               ETA: 18431.9s

################################################################################
                     [1m Learning iteration 176/4000 [0m

                       Computation: 1700 steps/s (collection: 0.796s, learning 4.022s)
               Value function loss: 155.7764
                    Surrogate loss: 0.0026
             Mean action noise std: 1.00
                       Mean reward: 334.53
               Mean episode length: 266.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1449984
                    Iteration time: 4.82s
                        Total time: 852.93s
                               ETA: 18427.0s

################################################################################
                     [1m Learning iteration 177/4000 [0m

                       Computation: 1702 steps/s (collection: 0.734s, learning 4.077s)
               Value function loss: 123.3323
                    Surrogate loss: 0.0243
             Mean action noise std: 1.00
                       Mean reward: 343.72
               Mean episode length: 271.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 4.81s
                        Total time: 857.74s
                               ETA: 18422.1s

################################################################################
                     [1m Learning iteration 178/4000 [0m

                       Computation: 1661 steps/s (collection: 0.830s, learning 4.102s)
               Value function loss: 156.6455
                    Surrogate loss: 0.0095
             Mean action noise std: 1.00
                       Mean reward: 336.80
               Mean episode length: 266.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1466368
                    Iteration time: 4.93s
                        Total time: 862.67s
                               ETA: 18419.6s

################################################################################
                     [1m Learning iteration 179/4000 [0m

                       Computation: 1717 steps/s (collection: 0.719s, learning 4.052s)
               Value function loss: 166.1796
                    Surrogate loss: -0.0017
             Mean action noise std: 1.00
                       Mean reward: 346.74
               Mean episode length: 267.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 4.77s
                        Total time: 867.44s
                               ETA: 18413.8s

################################################################################
                     [1m Learning iteration 180/4000 [0m

                       Computation: 1704 steps/s (collection: 0.751s, learning 4.054s)
               Value function loss: 165.0558
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 342.24
               Mean episode length: 265.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1482752
                    Iteration time: 4.81s
                        Total time: 872.24s
                               ETA: 18408.7s

################################################################################
                     [1m Learning iteration 181/4000 [0m

                       Computation: 1708 steps/s (collection: 0.713s, learning 4.083s)
               Value function loss: 148.0052
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 327.72
               Mean episode length: 256.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 4.80s
                        Total time: 877.04s
                               ETA: 18403.4s

################################################################################
                     [1m Learning iteration 182/4000 [0m

                       Computation: 1687 steps/s (collection: 0.742s, learning 4.113s)
               Value function loss: 189.0824
                    Surrogate loss: 0.0080
             Mean action noise std: 1.00
                       Mean reward: 338.28
               Mean episode length: 259.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1499136
                    Iteration time: 4.85s
                        Total time: 881.89s
                               ETA: 18399.3s

################################################################################
                     [1m Learning iteration 183/4000 [0m

                       Computation: 1720 steps/s (collection: 0.694s, learning 4.066s)
               Value function loss: 173.4733
                    Surrogate loss: -0.0024
             Mean action noise std: 1.00
                       Mean reward: 320.81
               Mean episode length: 246.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 4.76s
                        Total time: 886.65s
                               ETA: 18393.3s

################################################################################
                     [1m Learning iteration 184/4000 [0m

                       Computation: 1700 steps/s (collection: 0.748s, learning 4.069s)
               Value function loss: 210.3718
                    Surrogate loss: 0.0068
             Mean action noise std: 1.00
                       Mean reward: 322.06
               Mean episode length: 244.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1515520
                    Iteration time: 4.82s
                        Total time: 891.47s
                               ETA: 18388.4s

################################################################################
                     [1m Learning iteration 185/4000 [0m

                       Computation: 1679 steps/s (collection: 0.787s, learning 4.091s)
               Value function loss: 181.3001
                    Surrogate loss: 0.0039
             Mean action noise std: 1.00
                       Mean reward: 324.21
               Mean episode length: 247.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 4.88s
                        Total time: 896.35s
                               ETA: 18384.8s

################################################################################
                     [1m Learning iteration 186/4000 [0m

                       Computation: 1712 steps/s (collection: 0.733s, learning 4.050s)
               Value function loss: 244.7494
                    Surrogate loss: 0.0044
             Mean action noise std: 1.01
                       Mean reward: 330.66
               Mean episode length: 251.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1531904
                    Iteration time: 4.78s
                        Total time: 901.13s
                               ETA: 18379.3s

################################################################################
                     [1m Learning iteration 187/4000 [0m

                       Computation: 1691 steps/s (collection: 0.809s, learning 4.035s)
               Value function loss: 243.5449
                    Surrogate loss: 0.0121
             Mean action noise std: 1.01
                       Mean reward: 351.24
               Mean episode length: 263.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 4.84s
                        Total time: 905.98s
                               ETA: 18374.9s

################################################################################
                     [1m Learning iteration 188/4000 [0m

                       Computation: 1689 steps/s (collection: 0.804s, learning 4.044s)
               Value function loss: 184.9351
                    Surrogate loss: 0.0075
             Mean action noise std: 1.01
                       Mean reward: 358.67
               Mean episode length: 265.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1548288
                    Iteration time: 4.85s
                        Total time: 910.82s
                               ETA: 18370.7s

################################################################################
                     [1m Learning iteration 189/4000 [0m

                       Computation: 1688 steps/s (collection: 0.791s, learning 4.061s)
               Value function loss: 191.5660
                    Surrogate loss: 0.0092
             Mean action noise std: 1.01
                       Mean reward: 352.84
               Mean episode length: 260.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 4.85s
                        Total time: 915.68s
                               ETA: 18366.5s

################################################################################
                     [1m Learning iteration 190/4000 [0m

                       Computation: 1688 steps/s (collection: 0.787s, learning 4.065s)
               Value function loss: 268.4342
                    Surrogate loss: 0.0130
             Mean action noise std: 1.01
                       Mean reward: 377.11
               Mean episode length: 277.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1564672
                    Iteration time: 4.85s
                        Total time: 920.53s
                               ETA: 18362.4s

################################################################################
                     [1m Learning iteration 191/4000 [0m

                       Computation: 1691 steps/s (collection: 0.748s, learning 4.096s)
               Value function loss: 232.1779
                    Surrogate loss: 0.0075
             Mean action noise std: 1.01
                       Mean reward: 423.51
               Mean episode length: 307.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 4.84s
                        Total time: 925.37s
                               ETA: 18358.0s

################################################################################
                     [1m Learning iteration 192/4000 [0m

                       Computation: 1678 steps/s (collection: 0.767s, learning 4.113s)
               Value function loss: 170.0826
                    Surrogate loss: 0.0094
             Mean action noise std: 1.00
                       Mean reward: 435.11
               Mean episode length: 314.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1581056
                    Iteration time: 4.88s
                        Total time: 930.25s
                               ETA: 18354.4s

################################################################################
                     [1m Learning iteration 193/4000 [0m

                       Computation: 1671 steps/s (collection: 0.766s, learning 4.134s)
               Value function loss: 249.6449
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 451.26
               Mean episode length: 319.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 4.90s
                        Total time: 935.15s
                               ETA: 18351.2s

################################################################################
                     [1m Learning iteration 194/4000 [0m

                       Computation: 1680 steps/s (collection: 0.777s, learning 4.097s)
               Value function loss: 241.5100
                    Surrogate loss: 0.0119
             Mean action noise std: 1.00
                       Mean reward: 423.13
               Mean episode length: 300.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 1597440
                    Iteration time: 4.87s
                        Total time: 940.03s
                               ETA: 18347.4s

################################################################################
                     [1m Learning iteration 195/4000 [0m

                       Computation: 1681 steps/s (collection: 0.781s, learning 4.090s)
               Value function loss: 194.3873
                    Surrogate loss: 0.0071
             Mean action noise std: 1.01
                       Mean reward: 430.66
               Mean episode length: 307.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 4.87s
                        Total time: 944.90s
                               ETA: 18343.6s

################################################################################
                     [1m Learning iteration 196/4000 [0m

                       Computation: 1684 steps/s (collection: 0.755s, learning 4.108s)
               Value function loss: 172.2962
                    Surrogate loss: 0.0114
             Mean action noise std: 1.00
                       Mean reward: 422.18
               Mean episode length: 301.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1613824
                    Iteration time: 4.86s
                        Total time: 949.76s
                               ETA: 18339.5s

################################################################################
                     [1m Learning iteration 197/4000 [0m

                       Computation: 1690 steps/s (collection: 0.755s, learning 4.091s)
               Value function loss: 180.7209
                    Surrogate loss: 0.0033
             Mean action noise std: 1.00
                       Mean reward: 405.15
               Mean episode length: 287.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 4.85s
                        Total time: 954.61s
                               ETA: 18335.2s

################################################################################
                     [1m Learning iteration 198/4000 [0m

                       Computation: 1693 steps/s (collection: 0.746s, learning 4.092s)
               Value function loss: 232.2472
                    Surrogate loss: 0.0076
             Mean action noise std: 1.00
                       Mean reward: 396.14
               Mean episode length: 284.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1630208
                    Iteration time: 4.84s
                        Total time: 959.44s
                               ETA: 18330.7s

################################################################################
                     [1m Learning iteration 199/4000 [0m

                       Computation: 1702 steps/s (collection: 0.728s, learning 4.084s)
               Value function loss: 172.6178
                    Surrogate loss: 0.0128
             Mean action noise std: 1.00
                       Mean reward: 388.25
               Mean episode length: 279.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 4.81s
                        Total time: 964.26s
                               ETA: 18325.7s

################################################################################
                     [1m Learning iteration 200/4000 [0m

                       Computation: 1724 steps/s (collection: 0.727s, learning 4.023s)
               Value function loss: 211.9831
                    Surrogate loss: 0.0066
             Mean action noise std: 1.01
                       Mean reward: 388.73
               Mean episode length: 279.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1646592
                    Iteration time: 4.75s
                        Total time: 969.01s
                               ETA: 18319.5s

################################################################################
                     [1m Learning iteration 201/4000 [0m

                       Computation: 1722 steps/s (collection: 0.725s, learning 4.031s)
               Value function loss: 178.2881
                    Surrogate loss: 0.0100
             Mean action noise std: 1.01
                       Mean reward: 405.69
               Mean episode length: 287.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 4.76s
                        Total time: 973.76s
                               ETA: 18313.5s

################################################################################
                     [1m Learning iteration 202/4000 [0m

                       Computation: 1705 steps/s (collection: 0.707s, learning 4.095s)
               Value function loss: 200.0932
                    Surrogate loss: 0.0119
             Mean action noise std: 1.00
                       Mean reward: 395.76
               Mean episode length: 281.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1662976
                    Iteration time: 4.80s
                        Total time: 978.56s
                               ETA: 18308.3s

################################################################################
                     [1m Learning iteration 203/4000 [0m

                       Computation: 1740 steps/s (collection: 0.687s, learning 4.020s)
               Value function loss: 278.8191
                    Surrogate loss: 0.0075
             Mean action noise std: 1.01
                       Mean reward: 374.74
               Mean episode length: 261.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 4.71s
                        Total time: 983.27s
                               ETA: 18301.4s

################################################################################
                     [1m Learning iteration 204/4000 [0m

                       Computation: 1706 steps/s (collection: 0.755s, learning 4.045s)
               Value function loss: 221.7360
                    Surrogate loss: 0.0037
             Mean action noise std: 1.00
                       Mean reward: 368.20
               Mean episode length: 255.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1679360
                    Iteration time: 4.80s
                        Total time: 988.07s
                               ETA: 18296.2s

################################################################################
                     [1m Learning iteration 205/4000 [0m

                       Computation: 1741 steps/s (collection: 0.696s, learning 4.009s)
               Value function loss: 207.5290
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 353.62
               Mean episode length: 243.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 4.70s
                        Total time: 992.78s
                               ETA: 18289.2s

################################################################################
                     [1m Learning iteration 206/4000 [0m

                       Computation: 1726 steps/s (collection: 0.695s, learning 4.050s)
               Value function loss: 266.4320
                    Surrogate loss: 0.0068
             Mean action noise std: 1.00
                       Mean reward: 357.20
               Mean episode length: 244.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1695744
                    Iteration time: 4.75s
                        Total time: 997.52s
                               ETA: 18283.1s

################################################################################
                     [1m Learning iteration 207/4000 [0m

                       Computation: 1693 steps/s (collection: 0.778s, learning 4.059s)
               Value function loss: 178.8909
                    Surrogate loss: 0.0034
             Mean action noise std: 1.00
                       Mean reward: 374.12
               Mean episode length: 255.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 4.84s
                        Total time: 1002.36s
                               ETA: 18278.6s

################################################################################
                     [1m Learning iteration 208/4000 [0m

                       Computation: 1693 steps/s (collection: 0.758s, learning 4.080s)
               Value function loss: 192.0579
                    Surrogate loss: 0.0031
             Mean action noise std: 1.00
                       Mean reward: 364.77
               Mean episode length: 252.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1712128
                    Iteration time: 4.84s
                        Total time: 1007.20s
                               ETA: 18274.1s

################################################################################
                     [1m Learning iteration 209/4000 [0m

                       Computation: 1686 steps/s (collection: 0.790s, learning 4.067s)
               Value function loss: 185.8082
                    Surrogate loss: 0.0163
             Mean action noise std: 1.01
                       Mean reward: 378.33
               Mean episode length: 260.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 4.86s
                        Total time: 1012.05s
                               ETA: 18270.0s

################################################################################
                     [1m Learning iteration 210/4000 [0m

                       Computation: 1716 steps/s (collection: 0.748s, learning 4.024s)
               Value function loss: 266.1217
                    Surrogate loss: 0.0112
             Mean action noise std: 1.00
                       Mean reward: 383.56
               Mean episode length: 263.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1728512
                    Iteration time: 4.77s
                        Total time: 1016.83s
                               ETA: 18264.3s

################################################################################
                     [1m Learning iteration 211/4000 [0m

                       Computation: 1681 steps/s (collection: 0.823s, learning 4.049s)
               Value function loss: 205.8984
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 399.01
               Mean episode length: 278.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 4.87s
                        Total time: 1021.70s
                               ETA: 18260.5s

################################################################################
                     [1m Learning iteration 212/4000 [0m

                       Computation: 1688 steps/s (collection: 0.786s, learning 4.066s)
               Value function loss: 236.5153
                    Surrogate loss: 0.0058
             Mean action noise std: 1.01
                       Mean reward: 394.95
               Mean episode length: 274.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1744896
                    Iteration time: 4.85s
                        Total time: 1026.55s
                               ETA: 18256.2s

################################################################################
                     [1m Learning iteration 213/4000 [0m

                       Computation: 1689 steps/s (collection: 0.787s, learning 4.062s)
               Value function loss: 218.5712
                    Surrogate loss: 0.0017
             Mean action noise std: 1.00
                       Mean reward: 371.34
               Mean episode length: 258.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 4.85s
                        Total time: 1031.40s
                               ETA: 18251.9s

################################################################################
                     [1m Learning iteration 214/4000 [0m

                       Computation: 1706 steps/s (collection: 0.739s, learning 4.060s)
               Value function loss: 227.4403
                    Surrogate loss: 0.0097
             Mean action noise std: 1.00
                       Mean reward: 374.57
               Mean episode length: 262.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1761280
                    Iteration time: 4.80s
                        Total time: 1036.20s
                               ETA: 18246.7s

################################################################################
                     [1m Learning iteration 215/4000 [0m

                       Computation: 1687 steps/s (collection: 0.794s, learning 4.061s)
               Value function loss: 234.5631
                    Surrogate loss: 0.0149
             Mean action noise std: 1.00
                       Mean reward: 390.49
               Mean episode length: 273.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 4.86s
                        Total time: 1041.05s
                               ETA: 18242.5s

################################################################################
                     [1m Learning iteration 216/4000 [0m

                       Computation: 1730 steps/s (collection: 0.698s, learning 4.035s)
               Value function loss: 173.7894
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 396.18
               Mean episode length: 278.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1777664
                    Iteration time: 4.73s
                        Total time: 1045.79s
                               ETA: 18236.2s

################################################################################
                     [1m Learning iteration 217/4000 [0m

                       Computation: 1718 steps/s (collection: 0.763s, learning 4.003s)
               Value function loss: 162.3669
                    Surrogate loss: 0.0095
             Mean action noise std: 1.00
                       Mean reward: 401.48
               Mean episode length: 282.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 4.77s
                        Total time: 1050.55s
                               ETA: 18230.5s

################################################################################
                     [1m Learning iteration 218/4000 [0m

                       Computation: 1686 steps/s (collection: 0.786s, learning 4.070s)
               Value function loss: 207.8943
                    Surrogate loss: 0.0082
             Mean action noise std: 1.01
                       Mean reward: 415.12
               Mean episode length: 292.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1794048
                    Iteration time: 4.86s
                        Total time: 1055.41s
                               ETA: 18226.3s

################################################################################
                     [1m Learning iteration 219/4000 [0m

                       Computation: 1695 steps/s (collection: 0.744s, learning 4.088s)
               Value function loss: 279.4781
                    Surrogate loss: 0.0097
             Mean action noise std: 1.01
                       Mean reward: 450.73
               Mean episode length: 316.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 4.83s
                        Total time: 1060.24s
                               ETA: 18221.7s

################################################################################
                     [1m Learning iteration 220/4000 [0m

                       Computation: 1689 steps/s (collection: 0.789s, learning 4.059s)
               Value function loss: 225.0196
                    Surrogate loss: 0.0056
             Mean action noise std: 1.01
                       Mean reward: 440.96
               Mean episode length: 310.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1810432
                    Iteration time: 4.85s
                        Total time: 1065.09s
                               ETA: 18217.4s

################################################################################
                     [1m Learning iteration 221/4000 [0m

                       Computation: 1696 steps/s (collection: 0.732s, learning 4.097s)
               Value function loss: 272.2093
                    Surrogate loss: 0.0081
             Mean action noise std: 1.01
                       Mean reward: 450.19
               Mean episode length: 316.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 4.83s
                        Total time: 1069.92s
                               ETA: 18212.7s

################################################################################
                     [1m Learning iteration 222/4000 [0m

                       Computation: 1707 steps/s (collection: 0.707s, learning 4.090s)
               Value function loss: 224.0852
                    Surrogate loss: 0.0171
             Mean action noise std: 1.01
                       Mean reward: 437.18
               Mean episode length: 309.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1826816
                    Iteration time: 4.80s
                        Total time: 1074.71s
                               ETA: 18207.5s

################################################################################
                     [1m Learning iteration 223/4000 [0m

                       Computation: 1714 steps/s (collection: 0.689s, learning 4.090s)
               Value function loss: 216.7897
                    Surrogate loss: 0.0207
             Mean action noise std: 1.01
                       Mean reward: 435.28
               Mean episode length: 308.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 4.78s
                        Total time: 1079.49s
                               ETA: 18202.0s

################################################################################
                     [1m Learning iteration 224/4000 [0m

                       Computation: 1723 steps/s (collection: 0.681s, learning 4.072s)
               Value function loss: 207.2483
                    Surrogate loss: -0.0020
             Mean action noise std: 1.01
                       Mean reward: 432.22
               Mean episode length: 308.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1843200
                    Iteration time: 4.75s
                        Total time: 1084.25s
                               ETA: 18196.1s

################################################################################
                     [1m Learning iteration 225/4000 [0m

                       Computation: 1729 steps/s (collection: 0.704s, learning 4.032s)
               Value function loss: 251.7215
                    Surrogate loss: -0.0027
             Mean action noise std: 1.01
                       Mean reward: 395.94
               Mean episode length: 285.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 4.74s
                        Total time: 1088.98s
                               ETA: 18189.9s

################################################################################
                     [1m Learning iteration 226/4000 [0m

                       Computation: 1695 steps/s (collection: 0.756s, learning 4.077s)
               Value function loss: 164.2411
                    Surrogate loss: 0.0092
             Mean action noise std: 1.01
                       Mean reward: 386.67
               Mean episode length: 279.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1859584
                    Iteration time: 4.83s
                        Total time: 1093.82s
                               ETA: 18185.3s

################################################################################
                     [1m Learning iteration 227/4000 [0m

                       Computation: 1695 steps/s (collection: 0.711s, learning 4.119s)
               Value function loss: 301.7918
                    Surrogate loss: 0.0060
             Mean action noise std: 1.01
                       Mean reward: 369.91
               Mean episode length: 269.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 4.83s
                        Total time: 1098.65s
                               ETA: 18180.7s

################################################################################
                     [1m Learning iteration 228/4000 [0m

                       Computation: 1691 steps/s (collection: 0.730s, learning 4.112s)
               Value function loss: 161.5463
                    Surrogate loss: 0.0013
             Mean action noise std: 1.01
                       Mean reward: 366.01
               Mean episode length: 265.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1875968
                    Iteration time: 4.84s
                        Total time: 1103.49s
                               ETA: 18176.2s

################################################################################
                     [1m Learning iteration 229/4000 [0m

                       Computation: 1684 steps/s (collection: 0.742s, learning 4.122s)
               Value function loss: 285.3953
                    Surrogate loss: 0.0014
             Mean action noise std: 1.01
                       Mean reward: 354.53
               Mean episode length: 258.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 4.86s
                        Total time: 1108.35s
                               ETA: 18172.1s
Traceback (most recent call last):
  File "tools/train_ppo.py", line 51, in <module>
    train()
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "tools/train_ppo.py", line 47, in train
    ppo.run(num_learning_iterations=max_iterations, log_interval=cfg.train.learn.save_interval)
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/mvp/ppo/ppo.py", line 267, in run
    mean_value_loss, mean_surrogate_loss = self.update(it, num_learning_iterations)
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/mvp/ppo/ppo.py", line 388, in update
    self.actor_critic(obs_batch, states_batch, actions_batch)
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/mvp/ppo/actor_critic.py", line 436, in forward
    actions_mean, _ = self.actor(observations)
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/mvp/ppo/actor_critic.py", line 277, in forward
    out_pop_activity = self.snn(in_pop_spikes, batch_size)
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/mvp/ppo/actor_critic.py", line 228, in forward
    hidden_states[layer][0], hidden_states[layer][1], hidden_states[layer][2] = self.neuron_model(
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/mvp/ppo/actor_critic.py", line 197, in neuron_model
    current = current * NEURON_CDECAY + syn_func(pre_layer_output)  # NOTE if sim cant work, change rl to cpu
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
