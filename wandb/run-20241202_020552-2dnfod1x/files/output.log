check obs_shape!!! (34,) 34
check actions_shape!!! (9,) 9
Sequential(
  (0): Linear(in_features=34, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=9, bias=True)
)
Sequential(
  (0): Linear(in_features=34, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=1, bias=True)
)
################################################################################
                      [1m Learning iteration 0/2000 [0m

                       Computation: 13072 steps/s (collection: 0.345s, learning 0.282s)
               Value function loss: 2.5304
                    Surrogate loss: -0.0028
             Mean action noise std: 1.00
                       Mean reward: 5.41
               Mean episode length: 15.29
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8192
                    Iteration time: 0.63s
                        Total time: 0.63s
                               ETA: 1253.4s

################################################################################
                      [1m Learning iteration 1/2000 [0m

                       Computation: 17673 steps/s (collection: 0.264s, learning 0.200s)
               Value function loss: 2.5032
                    Surrogate loss: -0.0044
             Mean action noise std: 1.00
                       Mean reward: 6.55
               Mean episode length: 21.60
                 Mean success rate: 0.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 22.88
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 0.46s
                        Total time: 1.09s
                               ETA: 1089.7s

################################################################################
                      [1m Learning iteration 2/2000 [0m

                       Computation: 17854 steps/s (collection: 0.259s, learning 0.200s)
               Value function loss: 2.2054
                    Surrogate loss: -0.0062
             Mean action noise std: 1.00
                       Mean reward: 7.84
               Mean episode length: 26.93
                 Mean success rate: 0.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 24576
                    Iteration time: 0.46s
                        Total time: 1.55s
                               ETA: 1031.7s

################################################################################
                      [1m Learning iteration 3/2000 [0m

                       Computation: 19033 steps/s (collection: 0.229s, learning 0.201s)
               Value function loss: 3.1666
                    Surrogate loss: -0.0045
             Mean action noise std: 1.00
                       Mean reward: 9.93
               Mean episode length: 37.23
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 25.60
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 0.43s
                        Total time: 1.98s
                               ETA: 988.2s

################################################################################
                      [1m Learning iteration 4/2000 [0m

                       Computation: 18374 steps/s (collection: 0.245s, learning 0.201s)
               Value function loss: 3.0350
                    Surrogate loss: -0.0027
             Mean action noise std: 0.99
                       Mean reward: 11.88
               Mean episode length: 47.61
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 40960
                    Iteration time: 0.45s
                        Total time: 2.43s
                               ETA: 968.2s

################################################################################
                      [1m Learning iteration 5/2000 [0m

                       Computation: 18436 steps/s (collection: 0.243s, learning 0.202s)
               Value function loss: 2.9129
                    Surrogate loss: -0.0078
             Mean action noise std: 0.99
                       Mean reward: 13.21
               Mean episode length: 54.95
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 0.44s
                        Total time: 2.87s
                               ETA: 954.1s

################################################################################
                      [1m Learning iteration 6/2000 [0m

                       Computation: 18020 steps/s (collection: 0.242s, learning 0.213s)
               Value function loss: 4.2515
                    Surrogate loss: -0.0045
             Mean action noise std: 0.99
                       Mean reward: 14.64
               Mean episode length: 60.95
                 Mean success rate: 0.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 25.76
--------------------------------------------------------------------------------
                   Total timesteps: 57344
                    Iteration time: 0.45s
                        Total time: 3.32s
                               ETA: 946.9s

################################################################################
                      [1m Learning iteration 7/2000 [0m

                       Computation: 16729 steps/s (collection: 0.272s, learning 0.218s)
               Value function loss: 6.7881
                    Surrogate loss: -0.0070
             Mean action noise std: 0.99
                       Mean reward: 15.53
               Mean episode length: 64.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 25.60
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 0.49s
                        Total time: 3.81s
                               ETA: 950.1s

################################################################################
                      [1m Learning iteration 8/2000 [0m

                       Computation: 16394 steps/s (collection: 0.282s, learning 0.217s)
               Value function loss: 4.8891
                    Surrogate loss: -0.0065
             Mean action noise std: 0.99
                       Mean reward: 17.18
               Mean episode length: 70.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 73728
                    Iteration time: 0.50s
                        Total time: 4.31s
                               ETA: 954.7s

################################################################################
                      [1m Learning iteration 9/2000 [0m

                       Computation: 17149 steps/s (collection: 0.274s, learning 0.204s)
               Value function loss: 8.1135
                    Surrogate loss: -0.0040
             Mean action noise std: 0.99
                       Mean reward: 20.02
               Mean episode length: 83.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 25.76
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 0.48s
                        Total time: 4.79s
                               ETA: 953.9s

################################################################################
                      [1m Learning iteration 10/2000 [0m

                       Computation: 16528 steps/s (collection: 0.291s, learning 0.205s)
               Value function loss: 13.0955
                    Surrogate loss: -0.0023
             Mean action noise std: 0.99
                       Mean reward: 25.33
               Mean episode length: 107.05
                 Mean success rate: 0.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 24.24
--------------------------------------------------------------------------------
                   Total timesteps: 90112
                    Iteration time: 0.50s
                        Total time: 5.29s
                               ETA: 956.4s

################################################################################
                      [1m Learning iteration 11/2000 [0m

                       Computation: 16008 steps/s (collection: 0.295s, learning 0.217s)
               Value function loss: 22.8744
                    Surrogate loss: -0.0063
             Mean action noise std: 0.99
                       Mean reward: 27.68
               Mean episode length: 110.48
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 22.82
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 0.51s
                        Total time: 5.80s
                               ETA: 961.1s

################################################################################
                      [1m Learning iteration 12/2000 [0m

                       Computation: 16123 steps/s (collection: 0.293s, learning 0.216s)
               Value function loss: 28.8887
                    Surrogate loss: -0.0049
             Mean action noise std: 0.99
                       Mean reward: 29.23
               Mean episode length: 110.04
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 24.02
--------------------------------------------------------------------------------
                   Total timesteps: 106496
                    Iteration time: 0.51s
                        Total time: 6.31s
                               ETA: 964.4s

################################################################################
                      [1m Learning iteration 13/2000 [0m

                       Computation: 15849 steps/s (collection: 0.304s, learning 0.213s)
               Value function loss: 24.1122
                    Surrogate loss: -0.0012
             Mean action noise std: 0.99
                       Mean reward: 30.27
               Mean episode length: 108.58
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 0.52s
                        Total time: 6.82s
                               ETA: 968.5s

################################################################################
                      [1m Learning iteration 14/2000 [0m

                       Computation: 16309 steps/s (collection: 0.301s, learning 0.201s)
               Value function loss: 34.1435
                    Surrogate loss: -0.0038
             Mean action noise std: 0.99
                       Mean reward: 33.33
               Mean episode length: 108.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 122880
                    Iteration time: 0.50s
                        Total time: 7.33s
                               ETA: 969.9s

################################################################################
                      [1m Learning iteration 15/2000 [0m

                       Computation: 16345 steps/s (collection: 0.298s, learning 0.203s)
               Value function loss: 42.6169
                    Surrogate loss: -0.0056
             Mean action noise std: 0.99
                       Mean reward: 48.99
               Mean episode length: 164.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 23.81
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 0.50s
                        Total time: 7.83s
                               ETA: 971.0s

################################################################################
                      [1m Learning iteration 16/2000 [0m

                       Computation: 16352 steps/s (collection: 0.294s, learning 0.207s)
               Value function loss: 32.9729
                    Surrogate loss: -0.0052
             Mean action noise std: 0.99
                       Mean reward: 53.75
               Mean episode length: 159.51
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 139264
                    Iteration time: 0.50s
                        Total time: 8.33s
                               ETA: 971.9s

################################################################################
                      [1m Learning iteration 17/2000 [0m

                       Computation: 16587 steps/s (collection: 0.289s, learning 0.205s)
               Value function loss: 38.8010
                    Surrogate loss: -0.0067
             Mean action noise std: 0.99
                       Mean reward: 59.45
               Mean episode length: 169.40
                 Mean success rate: 0.00
                  Mean reward/step: 0.62
       Mean episode length/episode: 25.28
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 0.49s
                        Total time: 8.82s
                               ETA: 971.9s

################################################################################
                      [1m Learning iteration 18/2000 [0m

                       Computation: 16435 steps/s (collection: 0.291s, learning 0.208s)
               Value function loss: 40.5964
                    Surrogate loss: -0.0058
             Mean action noise std: 0.99
                       Mean reward: 52.21
               Mean episode length: 108.60
                 Mean success rate: 0.00
                  Mean reward/step: 0.64
       Mean episode length/episode: 24.53
--------------------------------------------------------------------------------
                   Total timesteps: 155648
                    Iteration time: 0.50s
                        Total time: 9.32s
                               ETA: 972.3s

################################################################################
                      [1m Learning iteration 19/2000 [0m

                       Computation: 16694 steps/s (collection: 0.282s, learning 0.209s)
               Value function loss: 44.3821
                    Surrogate loss: -0.0059
             Mean action noise std: 0.99
                       Mean reward: 54.01
               Mean episode length: 112.54
                 Mean success rate: 0.00
                  Mean reward/step: 0.68
       Mean episode length/episode: 25.36
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 0.49s
                        Total time: 9.81s
                               ETA: 971.8s

################################################################################
                      [1m Learning iteration 20/2000 [0m

                       Computation: 14329 steps/s (collection: 0.301s, learning 0.270s)
               Value function loss: 48.9985
                    Surrogate loss: -0.0065
             Mean action noise std: 0.99
                       Mean reward: 58.45
               Mean episode length: 111.33
                 Mean success rate: 0.00
                  Mean reward/step: 0.68
       Mean episode length/episode: 24.82
--------------------------------------------------------------------------------
                   Total timesteps: 172032
                    Iteration time: 0.57s
                        Total time: 10.38s
                               ETA: 978.9s

################################################################################
                      [1m Learning iteration 21/2000 [0m

                       Computation: 14486 steps/s (collection: 0.345s, learning 0.220s)
               Value function loss: 55.3864
                    Surrogate loss: -0.0052
             Mean action noise std: 0.99
                       Mean reward: 63.20
               Mean episode length: 118.58
                 Mean success rate: 0.00
                  Mean reward/step: 0.76
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 0.57s
                        Total time: 10.95s
                               ETA: 984.8s

################################################################################
                      [1m Learning iteration 22/2000 [0m

                       Computation: 16351 steps/s (collection: 0.296s, learning 0.205s)
               Value function loss: 130.5869
                    Surrogate loss: -0.0032
             Mean action noise std: 0.99
                       Mean reward: 68.57
               Mean episode length: 120.80
                 Mean success rate: 0.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 188416
                    Iteration time: 0.50s
                        Total time: 11.45s
                               ETA: 984.6s

################################################################################
                      [1m Learning iteration 23/2000 [0m

                       Computation: 15479 steps/s (collection: 0.318s, learning 0.211s)
               Value function loss: 92.5601
                    Surrogate loss: -0.0047
             Mean action noise std: 0.99
                       Mean reward: 72.62
               Mean episode length: 118.52
                 Mean success rate: 0.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 0.53s
                        Total time: 11.98s
                               ETA: 986.7s

################################################################################
                      [1m Learning iteration 24/2000 [0m

                       Computation: 16784 steps/s (collection: 0.273s, learning 0.215s)
               Value function loss: 50.7333
                    Surrogate loss: -0.0062
             Mean action noise std: 0.99
                       Mean reward: 81.46
               Mean episode length: 131.06
                 Mean success rate: 0.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 204800
                    Iteration time: 0.49s
                        Total time: 12.47s
                               ETA: 985.3s

################################################################################
                      [1m Learning iteration 25/2000 [0m

                       Computation: 16594 steps/s (collection: 0.288s, learning 0.206s)
               Value function loss: 71.5930
                    Surrogate loss: -0.0035
             Mean action noise std: 0.99
                       Mean reward: 88.62
               Mean episode length: 141.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 0.49s
                        Total time: 12.96s
                               ETA: 984.5s

################################################################################
                      [1m Learning iteration 26/2000 [0m

                       Computation: 16936 steps/s (collection: 0.278s, learning 0.206s)
               Value function loss: 83.8897
                    Surrogate loss: -0.0055
             Mean action noise std: 0.99
                       Mean reward: 92.23
               Mean episode length: 136.73
                 Mean success rate: 0.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 221184
                    Iteration time: 0.48s
                        Total time: 13.44s
                               ETA: 982.9s

################################################################################
                      [1m Learning iteration 27/2000 [0m

                       Computation: 15965 steps/s (collection: 0.308s, learning 0.205s)
               Value function loss: 75.5268
                    Surrogate loss: -0.0050
             Mean action noise std: 0.99
                       Mean reward: 101.65
               Mean episode length: 143.26
                 Mean success rate: 0.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 0.51s
                        Total time: 13.96s
                               ETA: 983.5s

################################################################################
                      [1m Learning iteration 28/2000 [0m

                       Computation: 16343 steps/s (collection: 0.294s, learning 0.207s)
               Value function loss: 73.9453
                    Surrogate loss: -0.0053
             Mean action noise std: 0.99
                       Mean reward: 107.47
               Mean episode length: 146.18
                 Mean success rate: 0.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 237568
                    Iteration time: 0.50s
                        Total time: 14.46s
                               ETA: 983.2s

################################################################################
                      [1m Learning iteration 29/2000 [0m

                       Computation: 16679 steps/s (collection: 0.285s, learning 0.206s)
               Value function loss: 77.3234
                    Surrogate loss: -0.0068
             Mean action noise std: 0.98
                       Mean reward: 114.75
               Mean episode length: 153.98
                 Mean success rate: 0.00
                  Mean reward/step: 0.97
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 0.49s
                        Total time: 14.95s
                               ETA: 982.2s

################################################################################
                      [1m Learning iteration 30/2000 [0m

                       Computation: 16722 steps/s (collection: 0.279s, learning 0.211s)
               Value function loss: 77.0291
                    Surrogate loss: -0.0043
             Mean action noise std: 0.98
                       Mean reward: 124.36
               Mean episode length: 162.79
                 Mean success rate: 0.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 253952
                    Iteration time: 0.49s
                        Total time: 15.44s
                               ETA: 981.1s

################################################################################
                      [1m Learning iteration 31/2000 [0m

                       Computation: 16546 steps/s (collection: 0.292s, learning 0.203s)
               Value function loss: 94.4211
                    Surrogate loss: -0.0047
             Mean action noise std: 0.98
                       Mean reward: 145.35
               Mean episode length: 187.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 0.50s
                        Total time: 15.93s
                               ETA: 980.5s

################################################################################
                      [1m Learning iteration 32/2000 [0m

                       Computation: 16844 steps/s (collection: 0.280s, learning 0.206s)
               Value function loss: 127.5985
                    Surrogate loss: -0.0040
             Mean action noise std: 0.98
                       Mean reward: 166.75
               Mean episode length: 204.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 270336
                    Iteration time: 0.49s
                        Total time: 16.42s
                               ETA: 979.3s

################################################################################
                      [1m Learning iteration 33/2000 [0m

                       Computation: 16462 steps/s (collection: 0.293s, learning 0.204s)
               Value function loss: 118.6998
                    Surrogate loss: -0.0037
             Mean action noise std: 0.98
                       Mean reward: 203.38
               Mean episode length: 242.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 0.50s
                        Total time: 16.92s
                               ETA: 978.8s

################################################################################
                      [1m Learning iteration 34/2000 [0m

                       Computation: 15913 steps/s (collection: 0.298s, learning 0.216s)
               Value function loss: 142.3142
                    Surrogate loss: -0.0051
             Mean action noise std: 0.98
                       Mean reward: 231.46
               Mean episode length: 270.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 286720
                    Iteration time: 0.51s
                        Total time: 17.43s
                               ETA: 979.2s

################################################################################
                      [1m Learning iteration 35/2000 [0m

                       Computation: 16298 steps/s (collection: 0.297s, learning 0.205s)
               Value function loss: 138.6705
                    Surrogate loss: -0.0044
             Mean action noise std: 0.98
                       Mean reward: 269.38
               Mean episode length: 305.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 0.50s
                        Total time: 17.94s
                               ETA: 979.0s

################################################################################
                      [1m Learning iteration 36/2000 [0m

                       Computation: 16320 steps/s (collection: 0.297s, learning 0.205s)
               Value function loss: 129.1857
                    Surrogate loss: -0.0045
             Mean action noise std: 0.98
                       Mean reward: 295.02
               Mean episode length: 323.50
                 Mean success rate: 0.50
                  Mean reward/step: 1.10
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 303104
                    Iteration time: 0.50s
                        Total time: 18.44s
                               ETA: 978.7s

################################################################################
                      [1m Learning iteration 37/2000 [0m

                       Computation: 16677 steps/s (collection: 0.291s, learning 0.201s)
               Value function loss: 123.5298
                    Surrogate loss: -0.0055
             Mean action noise std: 0.98
                       Mean reward: 309.57
               Mean episode length: 330.05
                 Mean success rate: 0.50
                  Mean reward/step: 1.16
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 0.49s
                        Total time: 18.93s
                               ETA: 977.8s

################################################################################
                      [1m Learning iteration 38/2000 [0m

                       Computation: 16940 steps/s (collection: 0.272s, learning 0.211s)
               Value function loss: 128.0506
                    Surrogate loss: -0.0051
             Mean action noise std: 0.98
                       Mean reward: 307.08
               Mean episode length: 320.63
                 Mean success rate: 0.50
                  Mean reward/step: 1.18
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 319488
                    Iteration time: 0.48s
                        Total time: 19.41s
                               ETA: 976.6s

################################################################################
                      [1m Learning iteration 39/2000 [0m

                       Computation: 16820 steps/s (collection: 0.285s, learning 0.202s)
               Value function loss: 146.9794
                    Surrogate loss: -0.0068
             Mean action noise std: 0.98
                       Mean reward: 298.45
               Mean episode length: 298.14
                 Mean success rate: 0.50
                  Mean reward/step: 1.17
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 0.49s
                        Total time: 19.90s
                               ETA: 975.6s

################################################################################
                      [1m Learning iteration 40/2000 [0m

                       Computation: 18314 steps/s (collection: 0.251s, learning 0.197s)
               Value function loss: 114.0978
                    Surrogate loss: -0.0045
             Mean action noise std: 0.98
                       Mean reward: 302.44
               Mean episode length: 298.43
                 Mean success rate: 0.50
                  Mean reward/step: 1.19
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 335872
                    Iteration time: 0.45s
                        Total time: 20.35s
                               ETA: 972.7s

################################################################################
                      [1m Learning iteration 41/2000 [0m

                       Computation: 17060 steps/s (collection: 0.281s, learning 0.199s)
               Value function loss: 130.7930
                    Surrogate loss: -0.0001
             Mean action noise std: 0.98
                       Mean reward: 298.15
               Mean episode length: 289.62
                 Mean success rate: 0.50
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 0.48s
                        Total time: 20.83s
                               ETA: 971.4s

################################################################################
                      [1m Learning iteration 42/2000 [0m

                       Computation: 17108 steps/s (collection: 0.277s, learning 0.202s)
               Value function loss: 116.4658
                    Surrogate loss: -0.0056
             Mean action noise std: 0.98
                       Mean reward: 283.37
               Mean episode length: 271.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 352256
                    Iteration time: 0.48s
                        Total time: 21.31s
                               ETA: 970.1s

################################################################################
                      [1m Learning iteration 43/2000 [0m

                       Computation: 17685 steps/s (collection: 0.259s, learning 0.204s)
               Value function loss: 152.2832
                    Surrogate loss: -0.0024
             Mean action noise std: 0.98
                       Mean reward: 275.08
               Mean episode length: 254.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 0.46s
                        Total time: 21.77s
                               ETA: 968.2s

################################################################################
                      [1m Learning iteration 44/2000 [0m

                       Computation: 16923 steps/s (collection: 0.282s, learning 0.203s)
               Value function loss: 82.9302
                    Surrogate loss: -0.0058
             Mean action noise std: 0.98
                       Mean reward: 270.45
               Mean episode length: 246.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 368640
                    Iteration time: 0.48s
                        Total time: 22.25s
                               ETA: 967.3s

################################################################################
                      [1m Learning iteration 45/2000 [0m

                       Computation: 16735 steps/s (collection: 0.275s, learning 0.214s)
               Value function loss: 137.8781
                    Surrogate loss: -0.0057
             Mean action noise std: 0.98
                       Mean reward: 263.58
               Mean episode length: 234.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 0.49s
                        Total time: 22.74s
                               ETA: 966.6s

################################################################################
                      [1m Learning iteration 46/2000 [0m

                       Computation: 17105 steps/s (collection: 0.277s, learning 0.202s)
               Value function loss: 175.1305
                    Surrogate loss: -0.0056
             Mean action noise std: 0.98
                       Mean reward: 273.60
               Mean episode length: 240.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 385024
                    Iteration time: 0.48s
                        Total time: 23.22s
                               ETA: 965.4s

################################################################################
                      [1m Learning iteration 47/2000 [0m

                       Computation: 16927 steps/s (collection: 0.280s, learning 0.204s)
               Value function loss: 110.6091
                    Surrogate loss: 0.0065
             Mean action noise std: 0.98
                       Mean reward: 288.09
               Mean episode length: 253.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 0.48s
                        Total time: 23.71s
                               ETA: 964.5s

################################################################################
                      [1m Learning iteration 48/2000 [0m

                       Computation: 16055 steps/s (collection: 0.301s, learning 0.209s)
               Value function loss: 135.8117
                    Surrogate loss: -0.0038
             Mean action noise std: 0.98
                       Mean reward: 297.85
               Mean episode length: 258.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 401408
                    Iteration time: 0.51s
                        Total time: 24.22s
                               ETA: 964.7s

################################################################################
                      [1m Learning iteration 49/2000 [0m

                       Computation: 17270 steps/s (collection: 0.270s, learning 0.204s)
               Value function loss: 135.7197
                    Surrogate loss: -0.0059
             Mean action noise std: 0.98
                       Mean reward: 319.79
               Mean episode length: 275.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 0.47s
                        Total time: 24.69s
                               ETA: 963.4s

################################################################################
                      [1m Learning iteration 50/2000 [0m

                       Computation: 16591 steps/s (collection: 0.288s, learning 0.206s)
               Value function loss: 135.8979
                    Surrogate loss: 0.0055
             Mean action noise std: 0.98
                       Mean reward: 350.30
               Mean episode length: 301.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 417792
                    Iteration time: 0.49s
                        Total time: 25.18s
                               ETA: 962.9s

################################################################################
                      [1m Learning iteration 51/2000 [0m

                       Computation: 17203 steps/s (collection: 0.264s, learning 0.212s)
               Value function loss: 177.7864
                    Surrogate loss: -0.0032
             Mean action noise std: 0.98
                       Mean reward: 392.02
               Mean episode length: 333.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 0.48s
                        Total time: 25.66s
                               ETA: 961.7s

################################################################################
                      [1m Learning iteration 52/2000 [0m

                       Computation: 17837 steps/s (collection: 0.253s, learning 0.206s)
               Value function loss: 96.2261
                    Surrogate loss: -0.0082
             Mean action noise std: 0.98
                       Mean reward: 410.11
               Mean episode length: 345.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 434176
                    Iteration time: 0.46s
                        Total time: 26.12s
                               ETA: 960.0s

################################################################################
                      [1m Learning iteration 53/2000 [0m

                       Computation: 17046 steps/s (collection: 0.261s, learning 0.219s)
               Value function loss: 139.3762
                    Surrogate loss: -0.0019
             Mean action noise std: 0.98
                       Mean reward: 428.82
               Mean episode length: 361.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 0.48s
                        Total time: 26.60s
                               ETA: 959.1s

################################################################################
                      [1m Learning iteration 54/2000 [0m

                       Computation: 17690 steps/s (collection: 0.257s, learning 0.206s)
               Value function loss: 156.5471
                    Surrogate loss: -0.0043
             Mean action noise std: 0.98
                       Mean reward: 454.77
               Mean episode length: 383.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 450560
                    Iteration time: 0.46s
                        Total time: 27.06s
                               ETA: 957.5s

################################################################################
                      [1m Learning iteration 55/2000 [0m

                       Computation: 17849 steps/s (collection: 0.258s, learning 0.201s)
               Value function loss: 189.1502
                    Surrogate loss: -0.0054
             Mean action noise std: 0.98
                       Mean reward: 465.70
               Mean episode length: 388.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 0.46s
                        Total time: 27.52s
                               ETA: 955.9s

################################################################################
                      [1m Learning iteration 56/2000 [0m

                       Computation: 17299 steps/s (collection: 0.265s, learning 0.209s)
               Value function loss: 210.2584
                    Surrogate loss: -0.0028
             Mean action noise std: 0.98
                       Mean reward: 463.94
               Mean episode length: 383.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 466944
                    Iteration time: 0.47s
                        Total time: 28.00s
                               ETA: 954.8s

################################################################################
                      [1m Learning iteration 57/2000 [0m

                       Computation: 16506 steps/s (collection: 0.293s, learning 0.203s)
               Value function loss: 137.0205
                    Surrogate loss: -0.0040
             Mean action noise std: 0.98
                       Mean reward: 441.46
               Mean episode length: 360.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 0.50s
                        Total time: 28.49s
                               ETA: 954.5s

################################################################################
                      [1m Learning iteration 58/2000 [0m

                       Computation: 16716 steps/s (collection: 0.285s, learning 0.205s)
               Value function loss: 150.5611
                    Surrogate loss: -0.0062
             Mean action noise std: 0.98
                       Mean reward: 435.04
               Mean episode length: 349.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 483328
                    Iteration time: 0.49s
                        Total time: 28.98s
                               ETA: 953.9s

################################################################################
                      [1m Learning iteration 59/2000 [0m

                       Computation: 16592 steps/s (collection: 0.287s, learning 0.206s)
               Value function loss: 127.8364
                    Surrogate loss: -0.0054
             Mean action noise std: 0.98
                       Mean reward: 433.41
               Mean episode length: 341.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 0.49s
                        Total time: 29.48s
                               ETA: 953.5s

################################################################################
                      [1m Learning iteration 60/2000 [0m

                       Computation: 17184 steps/s (collection: 0.266s, learning 0.211s)
               Value function loss: 142.8702
                    Surrogate loss: -0.0047
             Mean action noise std: 0.98
                       Mean reward: 418.56
               Mean episode length: 328.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 499712
                    Iteration time: 0.48s
                        Total time: 29.95s
                               ETA: 952.6s

################################################################################
                      [1m Learning iteration 61/2000 [0m

                       Computation: 17271 steps/s (collection: 0.265s, learning 0.209s)
               Value function loss: 129.0531
                    Surrogate loss: -0.0027
             Mean action noise std: 0.98
                       Mean reward: 420.11
               Mean episode length: 328.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 0.47s
                        Total time: 30.43s
                               ETA: 951.6s

################################################################################
                      [1m Learning iteration 62/2000 [0m

                       Computation: 17132 steps/s (collection: 0.273s, learning 0.205s)
               Value function loss: 240.1619
                    Surrogate loss: -0.0054
             Mean action noise std: 0.98
                       Mean reward: 436.61
               Mean episode length: 337.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 516096
                    Iteration time: 0.48s
                        Total time: 30.90s
                               ETA: 950.7s

################################################################################
                      [1m Learning iteration 63/2000 [0m

                       Computation: 16891 steps/s (collection: 0.280s, learning 0.205s)
               Value function loss: 206.9009
                    Surrogate loss: -0.0051
             Mean action noise std: 0.98
                       Mean reward: 417.07
               Mean episode length: 315.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 0.48s
                        Total time: 31.39s
                               ETA: 950.0s

################################################################################
                      [1m Learning iteration 64/2000 [0m

                       Computation: 17443 steps/s (collection: 0.261s, learning 0.209s)
               Value function loss: 231.0902
                    Surrogate loss: -0.0046
             Mean action noise std: 0.98
                       Mean reward: 427.72
               Mean episode length: 321.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 532480
                    Iteration time: 0.47s
                        Total time: 31.86s
                               ETA: 948.9s

################################################################################
                      [1m Learning iteration 65/2000 [0m

                       Computation: 16793 steps/s (collection: 0.270s, learning 0.218s)
               Value function loss: 246.4496
                    Surrogate loss: -0.0025
             Mean action noise std: 0.98
                       Mean reward: 430.39
               Mean episode length: 318.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 0.49s
                        Total time: 32.35s
                               ETA: 948.3s

################################################################################
                      [1m Learning iteration 66/2000 [0m

                       Computation: 15597 steps/s (collection: 0.318s, learning 0.207s)
               Value function loss: 208.1375
                    Surrogate loss: -0.0049
             Mean action noise std: 0.98
                       Mean reward: 430.60
               Mean episode length: 317.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 548864
                    Iteration time: 0.53s
                        Total time: 32.87s
                               ETA: 948.9s

################################################################################
                      [1m Learning iteration 67/2000 [0m

                       Computation: 16117 steps/s (collection: 0.306s, learning 0.203s)
               Value function loss: 229.1818
                    Surrogate loss: -0.0037
             Mean action noise std: 0.98
                       Mean reward: 432.24
               Mean episode length: 315.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 0.51s
                        Total time: 33.38s
                               ETA: 948.9s

################################################################################
                      [1m Learning iteration 68/2000 [0m

                       Computation: 17122 steps/s (collection: 0.275s, learning 0.204s)
               Value function loss: 182.9638
                    Surrogate loss: -0.0063
             Mean action noise std: 0.98
                       Mean reward: 415.50
               Mean episode length: 297.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 565248
                    Iteration time: 0.48s
                        Total time: 33.86s
                               ETA: 948.0s

################################################################################
                      [1m Learning iteration 69/2000 [0m

                       Computation: 16307 steps/s (collection: 0.283s, learning 0.219s)
               Value function loss: 270.6331
                    Surrogate loss: -0.0029
             Mean action noise std: 0.98
                       Mean reward: 413.79
               Mean episode length: 292.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 0.50s
                        Total time: 34.36s
                               ETA: 947.9s

################################################################################
                      [1m Learning iteration 70/2000 [0m

                       Computation: 15748 steps/s (collection: 0.296s, learning 0.225s)
               Value function loss: 256.5188
                    Surrogate loss: -0.0031
             Mean action noise std: 0.98
                       Mean reward: 411.79
               Mean episode length: 287.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 581632
                    Iteration time: 0.52s
                        Total time: 34.88s
                               ETA: 948.2s

################################################################################
                      [1m Learning iteration 71/2000 [0m

                       Computation: 14949 steps/s (collection: 0.310s, learning 0.238s)
               Value function loss: 230.1896
                    Surrogate loss: -0.0049
             Mean action noise std: 0.98
                       Mean reward: 417.97
               Mean episode length: 287.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 0.55s
                        Total time: 35.43s
                               ETA: 949.2s

################################################################################
                      [1m Learning iteration 72/2000 [0m

                       Computation: 15625 steps/s (collection: 0.295s, learning 0.229s)
               Value function loss: 157.3469
                    Surrogate loss: -0.0051
             Mean action noise std: 0.98
                       Mean reward: 394.14
               Mean episode length: 266.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 598016
                    Iteration time: 0.52s
                        Total time: 35.95s
                               ETA: 949.6s

################################################################################
                      [1m Learning iteration 73/2000 [0m

                       Computation: 16161 steps/s (collection: 0.294s, learning 0.213s)
               Value function loss: 205.7188
                    Surrogate loss: -0.0036
             Mean action noise std: 0.98
                       Mean reward: 391.93
               Mean episode length: 261.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 0.51s
                        Total time: 36.46s
                               ETA: 949.4s

################################################################################
                      [1m Learning iteration 74/2000 [0m

                       Computation: 16615 steps/s (collection: 0.268s, learning 0.225s)
               Value function loss: 133.3565
                    Surrogate loss: -0.0032
             Mean action noise std: 0.98
                       Mean reward: 382.45
               Mean episode length: 254.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 614400
                    Iteration time: 0.49s
                        Total time: 36.95s
                               ETA: 949.0s

################################################################################
                      [1m Learning iteration 75/2000 [0m

                       Computation: 16258 steps/s (collection: 0.283s, learning 0.221s)
               Value function loss: 150.8762
                    Surrogate loss: 0.0051
             Mean action noise std: 0.98
                       Mean reward: 378.75
               Mean episode length: 252.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 0.50s
                        Total time: 37.46s
                               ETA: 948.7s

################################################################################
                      [1m Learning iteration 76/2000 [0m

                       Computation: 16043 steps/s (collection: 0.298s, learning 0.213s)
               Value function loss: 230.6658
                    Surrogate loss: -0.0026
             Mean action noise std: 0.98
                       Mean reward: 352.62
               Mean episode length: 233.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 630784
                    Iteration time: 0.51s
                        Total time: 37.97s
                               ETA: 948.7s

################################################################################
                      [1m Learning iteration 77/2000 [0m

                       Computation: 16006 steps/s (collection: 0.298s, learning 0.214s)
               Value function loss: 218.2471
                    Surrogate loss: -0.0062
             Mean action noise std: 0.98
                       Mean reward: 327.47
               Mean episode length: 218.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 0.51s
                        Total time: 38.48s
                               ETA: 948.7s

################################################################################
                      [1m Learning iteration 78/2000 [0m

                       Computation: 15219 steps/s (collection: 0.291s, learning 0.247s)
               Value function loss: 215.0026
                    Surrogate loss: -0.0059
             Mean action noise std: 0.98
                       Mean reward: 332.11
               Mean episode length: 216.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 647168
                    Iteration time: 0.54s
                        Total time: 39.02s
                               ETA: 949.3s

################################################################################
                      [1m Learning iteration 79/2000 [0m

                       Computation: 16050 steps/s (collection: 0.289s, learning 0.221s)
               Value function loss: 176.4760
                    Surrogate loss: 0.0033
             Mean action noise std: 0.98
                       Mean reward: 312.65
               Mean episode length: 202.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 0.51s
                        Total time: 39.53s
                               ETA: 949.2s

################################################################################
                      [1m Learning iteration 80/2000 [0m

                       Computation: 15371 steps/s (collection: 0.298s, learning 0.235s)
               Value function loss: 171.6818
                    Surrogate loss: -0.0035
             Mean action noise std: 0.98
                       Mean reward: 318.46
               Mean episode length: 203.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 663552
                    Iteration time: 0.53s
                        Total time: 40.06s
                               ETA: 949.6s

################################################################################
                      [1m Learning iteration 81/2000 [0m

                       Computation: 14022 steps/s (collection: 0.331s, learning 0.253s)
               Value function loss: 202.5831
                    Surrogate loss: -0.0058
             Mean action noise std: 0.98
                       Mean reward: 335.75
               Mean episode length: 209.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 0.58s
                        Total time: 40.65s
                               ETA: 951.2s

################################################################################
                      [1m Learning iteration 82/2000 [0m

                       Computation: 17099 steps/s (collection: 0.270s, learning 0.209s)
               Value function loss: 203.0299
                    Surrogate loss: -0.0061
             Mean action noise std: 0.98
                       Mean reward: 343.27
               Mean episode length: 217.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 679936
                    Iteration time: 0.48s
                        Total time: 41.12s
                               ETA: 950.3s

################################################################################
                      [1m Learning iteration 83/2000 [0m

                       Computation: 16150 steps/s (collection: 0.295s, learning 0.212s)
               Value function loss: 215.8631
                    Surrogate loss: -0.0065
             Mean action noise std: 0.98
                       Mean reward: 319.12
               Mean episode length: 204.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 0.51s
                        Total time: 41.63s
                               ETA: 950.1s

################################################################################
                      [1m Learning iteration 84/2000 [0m

                       Computation: 16897 steps/s (collection: 0.272s, learning 0.213s)
               Value function loss: 147.7553
                    Surrogate loss: -0.0053
             Mean action noise std: 0.98
                       Mean reward: 324.63
               Mean episode length: 211.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 696320
                    Iteration time: 0.48s
                        Total time: 42.12s
                               ETA: 949.4s

################################################################################
                      [1m Learning iteration 85/2000 [0m

                       Computation: 16496 steps/s (collection: 0.278s, learning 0.219s)
               Value function loss: 101.6486
                    Surrogate loss: -0.0051
             Mean action noise std: 0.98
                       Mean reward: 328.56
               Mean episode length: 216.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 0.50s
                        Total time: 42.61s
                               ETA: 948.9s

################################################################################
                      [1m Learning iteration 86/2000 [0m

                       Computation: 17486 steps/s (collection: 0.261s, learning 0.207s)
               Value function loss: 142.5458
                    Surrogate loss: -0.0049
             Mean action noise std: 0.98
                       Mean reward: 322.56
               Mean episode length: 214.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 712704
                    Iteration time: 0.47s
                        Total time: 43.08s
                               ETA: 947.8s

################################################################################
                      [1m Learning iteration 87/2000 [0m

                       Computation: 16328 steps/s (collection: 0.288s, learning 0.214s)
               Value function loss: 152.2965
                    Surrogate loss: -0.0046
             Mean action noise std: 0.98
                       Mean reward: 319.56
               Mean episode length: 217.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 0.50s
                        Total time: 43.58s
                               ETA: 947.4s

################################################################################
                      [1m Learning iteration 88/2000 [0m

                       Computation: 16666 steps/s (collection: 0.272s, learning 0.219s)
               Value function loss: 73.6591
                    Surrogate loss: -0.0027
             Mean action noise std: 0.98
                       Mean reward: 313.64
               Mean episode length: 212.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 729088
                    Iteration time: 0.49s
                        Total time: 44.07s
                               ETA: 946.9s

################################################################################
                      [1m Learning iteration 89/2000 [0m

                       Computation: 17236 steps/s (collection: 0.261s, learning 0.214s)
               Value function loss: 84.3998
                    Surrogate loss: -0.0028
             Mean action noise std: 0.98
                       Mean reward: 313.81
               Mean episode length: 213.83
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 0.48s
                        Total time: 44.55s
                               ETA: 945.9s

################################################################################
                      [1m Learning iteration 90/2000 [0m

                       Computation: 16844 steps/s (collection: 0.271s, learning 0.215s)
               Value function loss: 126.6557
                    Surrogate loss: -0.0033
             Mean action noise std: 0.98
                       Mean reward: 326.95
               Mean episode length: 225.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 745472
                    Iteration time: 0.49s
                        Total time: 45.04s
                               ETA: 945.3s

################################################################################
                      [1m Learning iteration 91/2000 [0m

                       Computation: 17658 steps/s (collection: 0.254s, learning 0.210s)
               Value function loss: 97.1527
                    Surrogate loss: -0.0049
             Mean action noise std: 0.98
                       Mean reward: 347.58
               Mean episode length: 240.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 0.46s
                        Total time: 45.50s
                               ETA: 944.1s

################################################################################
                      [1m Learning iteration 92/2000 [0m

                       Computation: 17045 steps/s (collection: 0.272s, learning 0.209s)
               Value function loss: 215.5748
                    Surrogate loss: -0.0026
             Mean action noise std: 0.98
                       Mean reward: 395.62
               Mean episode length: 276.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 761856
                    Iteration time: 0.48s
                        Total time: 45.98s
                               ETA: 943.3s

################################################################################
                      [1m Learning iteration 93/2000 [0m

                       Computation: 16958 steps/s (collection: 0.274s, learning 0.209s)
               Value function loss: 144.7590
                    Surrogate loss: -0.0047
             Mean action noise std: 0.98
                       Mean reward: 413.15
               Mean episode length: 288.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 0.48s
                        Total time: 46.46s
                               ETA: 942.6s

################################################################################
                      [1m Learning iteration 94/2000 [0m

                       Computation: 15711 steps/s (collection: 0.291s, learning 0.230s)
               Value function loss: 170.5816
                    Surrogate loss: -0.0031
             Mean action noise std: 0.98
                       Mean reward: 446.05
               Mean episode length: 316.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 778240
                    Iteration time: 0.52s
                        Total time: 46.99s
                               ETA: 942.7s

################################################################################
                      [1m Learning iteration 95/2000 [0m

                       Computation: 17040 steps/s (collection: 0.273s, learning 0.207s)
               Value function loss: 150.6810
                    Surrogate loss: -0.0033
             Mean action noise std: 0.98
                       Mean reward: 473.19
               Mean episode length: 338.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 0.48s
                        Total time: 47.47s
                               ETA: 941.9s

################################################################################
                      [1m Learning iteration 96/2000 [0m

                       Computation: 16804 steps/s (collection: 0.280s, learning 0.207s)
               Value function loss: 165.9962
                    Surrogate loss: -0.0057
             Mean action noise std: 0.97
                       Mean reward: 507.87
               Mean episode length: 362.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 794624
                    Iteration time: 0.49s
                        Total time: 47.95s
                               ETA: 941.3s

################################################################################
                      [1m Learning iteration 97/2000 [0m

                       Computation: 16138 steps/s (collection: 0.278s, learning 0.229s)
               Value function loss: 176.2931
                    Surrogate loss: -0.0039
             Mean action noise std: 0.97
                       Mean reward: 520.60
               Mean episode length: 372.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 0.51s
                        Total time: 48.46s
                               ETA: 941.0s

################################################################################
                      [1m Learning iteration 98/2000 [0m

                       Computation: 16640 steps/s (collection: 0.261s, learning 0.231s)
               Value function loss: 192.8091
                    Surrogate loss: -0.0037
             Mean action noise std: 0.97
                       Mean reward: 550.54
               Mean episode length: 398.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 811008
                    Iteration time: 0.49s
                        Total time: 48.95s
                               ETA: 940.5s

################################################################################
                      [1m Learning iteration 99/2000 [0m

                       Computation: 17430 steps/s (collection: 0.262s, learning 0.208s)
               Value function loss: 219.9873
                    Surrogate loss: 0.0002
             Mean action noise std: 0.98
                       Mean reward: 553.14
               Mean episode length: 398.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 0.47s
                        Total time: 49.42s
                               ETA: 939.5s

################################################################################
                     [1m Learning iteration 100/2000 [0m

                       Computation: 17242 steps/s (collection: 0.269s, learning 0.206s)
               Value function loss: 189.5639
                    Surrogate loss: 0.0009
             Mean action noise std: 0.97
                       Mean reward: 544.99
               Mean episode length: 393.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 827392
                    Iteration time: 0.48s
                        Total time: 49.90s
                               ETA: 938.7s

################################################################################
                     [1m Learning iteration 101/2000 [0m

                       Computation: 17265 steps/s (collection: 0.268s, learning 0.207s)
               Value function loss: 169.7004
                    Surrogate loss: -0.0050
             Mean action noise std: 0.97
                       Mean reward: 527.49
               Mean episode length: 378.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 0.47s
                        Total time: 50.37s
                               ETA: 937.8s

################################################################################
                     [1m Learning iteration 102/2000 [0m

                       Computation: 17155 steps/s (collection: 0.265s, learning 0.212s)
               Value function loss: 169.5303
                    Surrogate loss: -0.0051
             Mean action noise std: 0.97
                       Mean reward: 526.22
               Mean episode length: 374.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 843776
                    Iteration time: 0.48s
                        Total time: 50.85s
                               ETA: 937.0s

################################################################################
                     [1m Learning iteration 103/2000 [0m

                       Computation: 17172 steps/s (collection: 0.265s, learning 0.212s)
               Value function loss: 172.6447
                    Surrogate loss: -0.0044
             Mean action noise std: 0.97
                       Mean reward: 537.09
               Mean episode length: 380.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 0.48s
                        Total time: 51.33s
                               ETA: 936.2s

################################################################################
                     [1m Learning iteration 104/2000 [0m

                       Computation: 17837 steps/s (collection: 0.249s, learning 0.211s)
               Value function loss: 143.8460
                    Surrogate loss: -0.0043
             Mean action noise std: 0.97
                       Mean reward: 542.82
               Mean episode length: 381.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 31.27
--------------------------------------------------------------------------------
                   Total timesteps: 860160
                    Iteration time: 0.46s
                        Total time: 51.79s
                               ETA: 935.1s

################################################################################
                     [1m Learning iteration 105/2000 [0m

                       Computation: 16450 steps/s (collection: 0.275s, learning 0.223s)
               Value function loss: 171.9494
                    Surrogate loss: -0.0045
             Mean action noise std: 0.97
                       Mean reward: 507.57
               Mean episode length: 355.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 0.50s
                        Total time: 52.28s
                               ETA: 934.7s

################################################################################
                     [1m Learning iteration 106/2000 [0m

                       Computation: 16247 steps/s (collection: 0.287s, learning 0.217s)
               Value function loss: 193.7822
                    Surrogate loss: -0.0045
             Mean action noise std: 0.97
                       Mean reward: 483.59
               Mean episode length: 332.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 876544
                    Iteration time: 0.50s
                        Total time: 52.79s
                               ETA: 934.4s

################################################################################
                     [1m Learning iteration 107/2000 [0m

                       Computation: 16052 steps/s (collection: 0.281s, learning 0.230s)
               Value function loss: 205.1988
                    Surrogate loss: -0.0033
             Mean action noise std: 0.97
                       Mean reward: 468.84
               Mean episode length: 317.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 0.51s
                        Total time: 53.30s
                               ETA: 934.2s

################################################################################
                     [1m Learning iteration 108/2000 [0m

                       Computation: 15688 steps/s (collection: 0.277s, learning 0.245s)
               Value function loss: 218.8927
                    Surrogate loss: -0.0043
             Mean action noise std: 0.97
                       Mean reward: 469.10
               Mean episode length: 311.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 892928
                    Iteration time: 0.52s
                        Total time: 53.82s
                               ETA: 934.2s

################################################################################
                     [1m Learning iteration 109/2000 [0m

                       Computation: 15907 steps/s (collection: 0.288s, learning 0.227s)
               Value function loss: 253.5859
                    Surrogate loss: -0.0017
             Mean action noise std: 0.97
                       Mean reward: 482.85
               Mean episode length: 314.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 0.51s
                        Total time: 54.34s
                               ETA: 934.1s

################################################################################
                     [1m Learning iteration 110/2000 [0m

                       Computation: 16024 steps/s (collection: 0.273s, learning 0.239s)
               Value function loss: 239.8181
                    Surrogate loss: -0.0041
             Mean action noise std: 0.97
                       Mean reward: 435.06
               Mean episode length: 282.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 909312
                    Iteration time: 0.51s
                        Total time: 54.85s
                               ETA: 933.9s

################################################################################
                     [1m Learning iteration 111/2000 [0m

                       Computation: 14779 steps/s (collection: 0.307s, learning 0.248s)
               Value function loss: 235.7169
                    Surrogate loss: -0.0054
             Mean action noise std: 0.97
                       Mean reward: 408.54
               Mean episode length: 262.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 0.55s
                        Total time: 55.40s
                               ETA: 934.4s

################################################################################
                     [1m Learning iteration 112/2000 [0m

                       Computation: 15765 steps/s (collection: 0.301s, learning 0.218s)
               Value function loss: 191.2862
                    Surrogate loss: -0.0060
             Mean action noise std: 0.97
                       Mean reward: 392.37
               Mean episode length: 246.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 925696
                    Iteration time: 0.52s
                        Total time: 55.92s
                               ETA: 934.3s

################################################################################
                     [1m Learning iteration 113/2000 [0m

                       Computation: 16571 steps/s (collection: 0.274s, learning 0.220s)
               Value function loss: 275.0175
                    Surrogate loss: -0.0038
             Mean action noise std: 0.97
                       Mean reward: 359.21
               Mean episode length: 226.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 0.49s
                        Total time: 56.42s
                               ETA: 933.8s

################################################################################
                     [1m Learning iteration 114/2000 [0m

                       Computation: 15649 steps/s (collection: 0.313s, learning 0.210s)
               Value function loss: 230.2733
                    Surrogate loss: -0.0046
             Mean action noise std: 0.97
                       Mean reward: 342.72
               Mean episode length: 214.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 942080
                    Iteration time: 0.52s
                        Total time: 56.94s
                               ETA: 933.8s

################################################################################
                     [1m Learning iteration 115/2000 [0m

                       Computation: 15601 steps/s (collection: 0.286s, learning 0.239s)
               Value function loss: 228.4244
                    Surrogate loss: -0.0075
             Mean action noise std: 0.97
                       Mean reward: 365.72
               Mean episode length: 227.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 0.53s
                        Total time: 57.46s
                               ETA: 933.8s

################################################################################
                     [1m Learning iteration 116/2000 [0m

                       Computation: 15725 steps/s (collection: 0.304s, learning 0.217s)
               Value function loss: 173.2484
                    Surrogate loss: 0.0014
             Mean action noise std: 0.97
                       Mean reward: 377.28
               Mean episode length: 232.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 958464
                    Iteration time: 0.52s
                        Total time: 57.99s
                               ETA: 933.7s

################################################################################
                     [1m Learning iteration 117/2000 [0m

                       Computation: 16903 steps/s (collection: 0.268s, learning 0.216s)
               Value function loss: 152.5951
                    Surrogate loss: -0.0012
             Mean action noise std: 0.97
                       Mean reward: 389.49
               Mean episode length: 240.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 0.48s
                        Total time: 58.47s
                               ETA: 933.0s

################################################################################
                     [1m Learning iteration 118/2000 [0m

                       Computation: 16595 steps/s (collection: 0.279s, learning 0.215s)
               Value function loss: 136.8084
                    Surrogate loss: -0.0036
             Mean action noise std: 0.97
                       Mean reward: 379.19
               Mean episode length: 235.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 974848
                    Iteration time: 0.49s
                        Total time: 58.96s
                               ETA: 932.5s

################################################################################
                     [1m Learning iteration 119/2000 [0m

                       Computation: 16875 steps/s (collection: 0.276s, learning 0.209s)
               Value function loss: 156.3213
                    Surrogate loss: -0.0063
             Mean action noise std: 0.97
                       Mean reward: 372.33
               Mean episode length: 230.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.49s
                        Total time: 59.45s
                               ETA: 931.9s

################################################################################
                     [1m Learning iteration 120/2000 [0m

                       Computation: 17059 steps/s (collection: 0.271s, learning 0.209s)
               Value function loss: 164.5862
                    Surrogate loss: -0.0037
             Mean action noise std: 0.97
                       Mean reward: 388.87
               Mean episode length: 240.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 991232
                    Iteration time: 0.48s
                        Total time: 59.93s
                               ETA: 931.1s

################################################################################
                     [1m Learning iteration 121/2000 [0m

                       Computation: 17799 steps/s (collection: 0.258s, learning 0.202s)
               Value function loss: 150.4687
                    Surrogate loss: -0.0008
             Mean action noise std: 0.97
                       Mean reward: 394.86
               Mean episode length: 243.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 0.46s
                        Total time: 60.39s
                               ETA: 930.1s

################################################################################
                     [1m Learning iteration 122/2000 [0m

                       Computation: 17468 steps/s (collection: 0.262s, learning 0.207s)
               Value function loss: 151.3931
                    Surrogate loss: 0.0010
             Mean action noise std: 0.96
                       Mean reward: 400.01
               Mean episode length: 245.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1007616
                    Iteration time: 0.47s
                        Total time: 60.86s
                               ETA: 929.2s

################################################################################
                     [1m Learning iteration 123/2000 [0m

                       Computation: 16629 steps/s (collection: 0.281s, learning 0.212s)
               Value function loss: 214.3335
                    Surrogate loss: -0.0010
             Mean action noise std: 0.96
                       Mean reward: 429.11
               Mean episode length: 267.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 0.49s
                        Total time: 61.35s
                               ETA: 928.7s

################################################################################
                     [1m Learning iteration 124/2000 [0m

                       Computation: 16379 steps/s (collection: 0.297s, learning 0.203s)
               Value function loss: 320.3041
                    Surrogate loss: -0.0012
             Mean action noise std: 0.96
                       Mean reward: 429.37
               Mean episode length: 267.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1024000
                    Iteration time: 0.50s
                        Total time: 61.85s
                               ETA: 928.3s

################################################################################
                     [1m Learning iteration 125/2000 [0m

                       Computation: 16383 steps/s (collection: 0.287s, learning 0.213s)
               Value function loss: 317.7722
                    Surrogate loss: -0.0061
             Mean action noise std: 0.96
                       Mean reward: 478.12
               Mean episode length: 298.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 0.50s
                        Total time: 62.35s
                               ETA: 927.8s

################################################################################
                     [1m Learning iteration 126/2000 [0m

                       Computation: 14610 steps/s (collection: 0.313s, learning 0.247s)
               Value function loss: 381.6653
                    Surrogate loss: -0.0064
             Mean action noise std: 0.97
                       Mean reward: 506.12
               Mean episode length: 318.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1040384
                    Iteration time: 0.56s
                        Total time: 62.91s
                               ETA: 928.3s

################################################################################
                     [1m Learning iteration 127/2000 [0m

                       Computation: 14796 steps/s (collection: 0.337s, learning 0.216s)
               Value function loss: 385.1090
                    Surrogate loss: -0.0051
             Mean action noise std: 0.97
                       Mean reward: 529.51
               Mean episode length: 330.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 0.55s
                        Total time: 63.47s
                               ETA: 928.7s

################################################################################
                     [1m Learning iteration 128/2000 [0m

                       Computation: 16371 steps/s (collection: 0.290s, learning 0.211s)
               Value function loss: 298.1848
                    Surrogate loss: -0.0056
             Mean action noise std: 0.97
                       Mean reward: 552.08
               Mean episode length: 342.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1056768
                    Iteration time: 0.50s
                        Total time: 63.97s
                               ETA: 928.2s

################################################################################
                     [1m Learning iteration 129/2000 [0m

                       Computation: 16501 steps/s (collection: 0.281s, learning 0.216s)
               Value function loss: 242.9911
                    Surrogate loss: -0.0074
             Mean action noise std: 0.97
                       Mean reward: 542.46
               Mean episode length: 340.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 0.50s
                        Total time: 64.46s
                               ETA: 927.8s

################################################################################
                     [1m Learning iteration 130/2000 [0m

                       Computation: 15540 steps/s (collection: 0.300s, learning 0.227s)
               Value function loss: 404.4766
                    Surrogate loss: -0.0061
             Mean action noise std: 0.97
                       Mean reward: 547.85
               Mean episode length: 338.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1073152
                    Iteration time: 0.53s
                        Total time: 64.99s
                               ETA: 927.7s

################################################################################
                     [1m Learning iteration 131/2000 [0m

                       Computation: 15984 steps/s (collection: 0.297s, learning 0.216s)
               Value function loss: 290.6810
                    Surrogate loss: 0.0007
             Mean action noise std: 0.97
                       Mean reward: 506.49
               Mean episode length: 307.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.51s
                        Total time: 65.50s
                               ETA: 927.4s

################################################################################
                     [1m Learning iteration 132/2000 [0m

                       Computation: 16638 steps/s (collection: 0.291s, learning 0.202s)
               Value function loss: 420.2765
                    Surrogate loss: -0.0046
             Mean action noise std: 0.97
                       Mean reward: 509.69
               Mean episode length: 306.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1089536
                    Iteration time: 0.49s
                        Total time: 65.99s
                               ETA: 926.9s

################################################################################
                     [1m Learning iteration 133/2000 [0m

                       Computation: 17133 steps/s (collection: 0.270s, learning 0.208s)
               Value function loss: 293.4101
                    Surrogate loss: 0.0009
             Mean action noise std: 0.97
                       Mean reward: 526.92
               Mean episode length: 313.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 0.48s
                        Total time: 66.47s
                               ETA: 926.1s

################################################################################
                     [1m Learning iteration 134/2000 [0m

                       Computation: 16328 steps/s (collection: 0.278s, learning 0.223s)
               Value function loss: 305.7009
                    Surrogate loss: 0.0096
             Mean action noise std: 0.97
                       Mean reward: 496.46
               Mean episode length: 294.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.92
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1105920
                    Iteration time: 0.50s
                        Total time: 66.97s
                               ETA: 925.7s

################################################################################
                     [1m Learning iteration 135/2000 [0m

                       Computation: 14443 steps/s (collection: 0.312s, learning 0.255s)
               Value function loss: 301.7473
                    Surrogate loss: 0.0023
             Mean action noise std: 0.97
                       Mean reward: 486.46
               Mean episode length: 289.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.87
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 0.57s
                        Total time: 67.54s
                               ETA: 926.2s

################################################################################
                     [1m Learning iteration 136/2000 [0m

                       Computation: 17186 steps/s (collection: 0.271s, learning 0.206s)
               Value function loss: 268.5065
                    Surrogate loss: -0.0015
             Mean action noise std: 0.97
                       Mean reward: 476.18
               Mean episode length: 281.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 1122304
                    Iteration time: 0.48s
                        Total time: 68.02s
                               ETA: 925.4s

################################################################################
                     [1m Learning iteration 137/2000 [0m

                       Computation: 17176 steps/s (collection: 0.273s, learning 0.204s)
               Value function loss: 443.6419
                    Surrogate loss: -0.0055
             Mean action noise std: 0.96
                       Mean reward: 462.98
               Mean episode length: 272.88
                 Mean success rate: 0.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 0.48s
                        Total time: 68.49s
                               ETA: 924.7s

################################################################################
                     [1m Learning iteration 138/2000 [0m

                       Computation: 16585 steps/s (collection: 0.281s, learning 0.213s)
               Value function loss: 373.1604
                    Surrogate loss: 0.0018
             Mean action noise std: 0.96
                       Mean reward: 466.74
               Mean episode length: 275.08
                 Mean success rate: 0.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 1138688
                    Iteration time: 0.49s
                        Total time: 68.99s
                               ETA: 924.2s

################################################################################
                     [1m Learning iteration 139/2000 [0m

                       Computation: 16983 steps/s (collection: 0.266s, learning 0.216s)
               Value function loss: 390.9883
                    Surrogate loss: -0.0013
             Mean action noise std: 0.96
                       Mean reward: 486.75
               Mean episode length: 285.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.97
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 0.48s
                        Total time: 69.47s
                               ETA: 923.5s

################################################################################
                     [1m Learning iteration 140/2000 [0m

                       Computation: 17618 steps/s (collection: 0.267s, learning 0.198s)
               Value function loss: 458.9130
                    Surrogate loss: 0.0204
             Mean action noise std: 0.96
                       Mean reward: 495.75
               Mean episode length: 288.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.86
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1155072
                    Iteration time: 0.46s
                        Total time: 69.94s
                               ETA: 922.6s

################################################################################
                     [1m Learning iteration 141/2000 [0m

                       Computation: 17404 steps/s (collection: 0.263s, learning 0.207s)
               Value function loss: 547.6194
                    Surrogate loss: -0.0039
             Mean action noise std: 0.96
                       Mean reward: 493.76
               Mean episode length: 287.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.98
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 0.47s
                        Total time: 70.41s
                               ETA: 921.7s

################################################################################
                     [1m Learning iteration 142/2000 [0m

                       Computation: 17263 steps/s (collection: 0.267s, learning 0.208s)
               Value function loss: 683.4741
                    Surrogate loss: -0.0047
             Mean action noise std: 0.96
                       Mean reward: 512.12
               Mean episode length: 296.06
                 Mean success rate: 0.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1171456
                    Iteration time: 0.47s
                        Total time: 70.88s
                               ETA: 921.0s

################################################################################
                     [1m Learning iteration 143/2000 [0m

                       Computation: 17338 steps/s (collection: 0.255s, learning 0.217s)
               Value function loss: 524.8506
                    Surrogate loss: -0.0065
             Mean action noise std: 0.96
                       Mean reward: 524.23
               Mean episode length: 297.34
                 Mean success rate: 0.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.47s
                        Total time: 71.35s
                               ETA: 920.2s

################################################################################
                     [1m Learning iteration 144/2000 [0m

                       Computation: 17251 steps/s (collection: 0.270s, learning 0.205s)
               Value function loss: 794.2700
                    Surrogate loss: -0.0050
             Mean action noise std: 0.96
                       Mean reward: 563.19
               Mean episode length: 316.19
                 Mean success rate: 0.00
                  Mean reward/step: 2.14
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1187840
                    Iteration time: 0.47s
                        Total time: 71.83s
                               ETA: 919.4s

################################################################################
                     [1m Learning iteration 145/2000 [0m

                       Computation: 15839 steps/s (collection: 0.292s, learning 0.226s)
               Value function loss: 970.1877
                    Surrogate loss: -0.0049
             Mean action noise std: 0.96
                       Mean reward: 591.15
               Mean episode length: 320.63
                 Mean success rate: 0.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 0.52s
                        Total time: 72.35s
                               ETA: 919.2s

################################################################################
                     [1m Learning iteration 146/2000 [0m

                       Computation: 16273 steps/s (collection: 0.280s, learning 0.223s)
               Value function loss: 855.5687
                    Surrogate loss: -0.0055
             Mean action noise std: 0.96
                       Mean reward: 612.34
               Mean episode length: 321.10
                 Mean success rate: 0.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1204224
                    Iteration time: 0.50s
                        Total time: 72.85s
                               ETA: 918.8s

################################################################################
                     [1m Learning iteration 147/2000 [0m

                       Computation: 17342 steps/s (collection: 0.268s, learning 0.204s)
               Value function loss: 719.5192
                    Surrogate loss: -0.0068
             Mean action noise std: 0.96
                       Mean reward: 624.52
               Mean episode length: 326.20
                 Mean success rate: 0.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 0.47s
                        Total time: 73.32s
                               ETA: 918.0s

################################################################################
                     [1m Learning iteration 148/2000 [0m

                       Computation: 16764 steps/s (collection: 0.275s, learning 0.213s)
               Value function loss: 917.5584
                    Surrogate loss: -0.0051
             Mean action noise std: 0.96
                       Mean reward: 641.22
               Mean episode length: 328.47
                 Mean success rate: 0.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1220608
                    Iteration time: 0.49s
                        Total time: 73.81s
                               ETA: 917.4s

################################################################################
                     [1m Learning iteration 149/2000 [0m

                       Computation: 17392 steps/s (collection: 0.272s, learning 0.199s)
               Value function loss: 1443.9702
                    Surrogate loss: -0.0052
             Mean action noise std: 0.96
                       Mean reward: 662.16
               Mean episode length: 329.63
                 Mean success rate: 0.00
                  Mean reward/step: 2.45
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 0.47s
                        Total time: 74.28s
                               ETA: 916.6s

################################################################################
                     [1m Learning iteration 150/2000 [0m

                       Computation: 17003 steps/s (collection: 0.267s, learning 0.215s)
               Value function loss: 1404.1615
                    Surrogate loss: -0.0058
             Mean action noise std: 0.96
                       Mean reward: 659.15
               Mean episode length: 324.06
                 Mean success rate: 0.00
                  Mean reward/step: 2.66
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1236992
                    Iteration time: 0.48s
                        Total time: 74.76s
                               ETA: 916.0s

################################################################################
                     [1m Learning iteration 151/2000 [0m

                       Computation: 16654 steps/s (collection: 0.289s, learning 0.203s)
               Value function loss: 1173.5310
                    Surrogate loss: -0.0041
             Mean action noise std: 0.96
                       Mean reward: 652.15
               Mean episode length: 316.60
                 Mean success rate: 0.00
                  Mean reward/step: 2.61
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 0.49s
                        Total time: 75.25s
                               ETA: 915.4s

################################################################################
                     [1m Learning iteration 152/2000 [0m

                       Computation: 17630 steps/s (collection: 0.268s, learning 0.196s)
               Value function loss: 1368.1067
                    Surrogate loss: 0.0010
             Mean action noise std: 0.96
                       Mean reward: 648.56
               Mean episode length: 310.34
                 Mean success rate: 0.00
                  Mean reward/step: 2.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1253376
                    Iteration time: 0.46s
                        Total time: 75.72s
                               ETA: 914.6s

################################################################################
                     [1m Learning iteration 153/2000 [0m

                       Computation: 17179 steps/s (collection: 0.272s, learning 0.205s)
               Value function loss: 1527.8484
                    Surrogate loss: -0.0054
             Mean action noise std: 0.96
                       Mean reward: 682.44
               Mean episode length: 319.08
                 Mean success rate: 0.00
                  Mean reward/step: 2.56
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 0.48s
                        Total time: 76.20s
                               ETA: 913.9s

################################################################################
                     [1m Learning iteration 154/2000 [0m

                       Computation: 18496 steps/s (collection: 0.244s, learning 0.199s)
               Value function loss: 1265.7523
                    Surrogate loss: -0.0046
             Mean action noise std: 0.96
                       Mean reward: 651.48
               Mean episode length: 308.12
                 Mean success rate: 0.00
                  Mean reward/step: 2.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1269760
                    Iteration time: 0.44s
                        Total time: 76.64s
                               ETA: 912.8s

################################################################################
                     [1m Learning iteration 155/2000 [0m

                       Computation: 15717 steps/s (collection: 0.263s, learning 0.258s)
               Value function loss: 1481.3862
                    Surrogate loss: 0.0010
             Mean action noise std: 0.96
                       Mean reward: 661.82
               Mean episode length: 307.26
                 Mean success rate: 0.00
                  Mean reward/step: 2.86
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.52s
                        Total time: 77.16s
                               ETA: 912.6s

################################################################################
                     [1m Learning iteration 156/2000 [0m

                       Computation: 17235 steps/s (collection: 0.260s, learning 0.215s)
               Value function loss: 1991.2083
                    Surrogate loss: -0.0046
             Mean action noise std: 0.96
                       Mean reward: 684.86
               Mean episode length: 306.39
                 Mean success rate: 0.00
                  Mean reward/step: 3.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 1286144
                    Iteration time: 0.48s
                        Total time: 77.64s
                               ETA: 911.9s

################################################################################
                     [1m Learning iteration 157/2000 [0m

                       Computation: 16550 steps/s (collection: 0.273s, learning 0.222s)
               Value function loss: 1885.8215
                    Surrogate loss: -0.0052
             Mean action noise std: 0.96
                       Mean reward: 729.43
               Mean episode length: 316.94
                 Mean success rate: 0.00
                  Mean reward/step: 3.08
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 0.49s
                        Total time: 78.13s
                               ETA: 911.4s

################################################################################
                     [1m Learning iteration 158/2000 [0m

                       Computation: 14564 steps/s (collection: 0.289s, learning 0.273s)
               Value function loss: 2117.1034
                    Surrogate loss: -0.0046
             Mean action noise std: 0.96
                       Mean reward: 736.20
               Mean episode length: 309.04
                 Mean success rate: 0.00
                  Mean reward/step: 2.99
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1302528
                    Iteration time: 0.56s
                        Total time: 78.69s
                               ETA: 911.7s

################################################################################
                     [1m Learning iteration 159/2000 [0m

                       Computation: 13493 steps/s (collection: 0.338s, learning 0.270s)
               Value function loss: 2392.3194
                    Surrogate loss: -0.0047
             Mean action noise std: 0.96
                       Mean reward: 767.33
               Mean episode length: 310.29
                 Mean success rate: 0.00
                  Mean reward/step: 3.05
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 0.61s
                        Total time: 79.30s
                               ETA: 912.4s

################################################################################
                     [1m Learning iteration 160/2000 [0m

                       Computation: 13630 steps/s (collection: 0.324s, learning 0.277s)
               Value function loss: 2180.6074
                    Surrogate loss: -0.0043
             Mean action noise std: 0.96
                       Mean reward: 773.88
               Mean episode length: 304.51
                 Mean success rate: 0.00
                  Mean reward/step: 2.96
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1318912
                    Iteration time: 0.60s
                        Total time: 79.90s
                               ETA: 913.2s

################################################################################
                     [1m Learning iteration 161/2000 [0m

                       Computation: 15397 steps/s (collection: 0.308s, learning 0.224s)
               Value function loss: 2329.4835
                    Surrogate loss: -0.0037
             Mean action noise std: 0.96
                       Mean reward: 824.32
               Mean episode length: 310.76
                 Mean success rate: 0.00
                  Mean reward/step: 2.99
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 0.53s
                        Total time: 80.43s
                               ETA: 913.1s

################################################################################
                     [1m Learning iteration 162/2000 [0m

                       Computation: 16845 steps/s (collection: 0.281s, learning 0.206s)
               Value function loss: 3626.8000
                    Surrogate loss: -0.0022
             Mean action noise std: 0.96
                       Mean reward: 855.09
               Mean episode length: 307.96
                 Mean success rate: 0.00
                  Mean reward/step: 3.30
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1335296
                    Iteration time: 0.49s
                        Total time: 80.92s
                               ETA: 912.5s

################################################################################
                     [1m Learning iteration 163/2000 [0m

                       Computation: 14316 steps/s (collection: 0.325s, learning 0.247s)
               Value function loss: 3350.2551
                    Surrogate loss: -0.0032
             Mean action noise std: 0.96
                       Mean reward: 838.96
               Mean episode length: 289.70
                 Mean success rate: 0.00
                  Mean reward/step: 3.18
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 0.57s
                        Total time: 81.49s
                               ETA: 912.8s

################################################################################
                     [1m Learning iteration 164/2000 [0m

                       Computation: 15710 steps/s (collection: 0.309s, learning 0.212s)
               Value function loss: 1870.8777
                    Surrogate loss: -0.0037
             Mean action noise std: 0.96
                       Mean reward: 833.27
               Mean episode length: 287.33
                 Mean success rate: 0.00
                  Mean reward/step: 3.48
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 1351680
                    Iteration time: 0.52s
                        Total time: 82.01s
                               ETA: 912.6s

################################################################################
                     [1m Learning iteration 165/2000 [0m

                       Computation: 16713 steps/s (collection: 0.283s, learning 0.207s)
               Value function loss: 2963.6327
                    Surrogate loss: -0.0048
             Mean action noise std: 0.96
                       Mean reward: 894.41
               Mean episode length: 302.56
                 Mean success rate: 0.00
                  Mean reward/step: 3.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 0.49s
                        Total time: 82.50s
                               ETA: 912.0s

################################################################################
                     [1m Learning iteration 166/2000 [0m

                       Computation: 16220 steps/s (collection: 0.285s, learning 0.221s)
               Value function loss: 3033.8451
                    Surrogate loss: -0.0020
             Mean action noise std: 0.96
                       Mean reward: 910.32
               Mean episode length: 301.49
                 Mean success rate: 0.00
                  Mean reward/step: 3.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1368064
                    Iteration time: 0.51s
                        Total time: 83.01s
                               ETA: 911.6s

################################################################################
                     [1m Learning iteration 167/2000 [0m

                       Computation: 16727 steps/s (collection: 0.287s, learning 0.203s)
               Value function loss: 3862.6306
                    Surrogate loss: -0.0047
             Mean action noise std: 0.96
                       Mean reward: 956.19
               Mean episode length: 303.97
                 Mean success rate: 0.00
                  Mean reward/step: 3.83
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.49s
                        Total time: 83.50s
                               ETA: 911.0s

################################################################################
                     [1m Learning iteration 168/2000 [0m

                       Computation: 16135 steps/s (collection: 0.296s, learning 0.212s)
               Value function loss: 4007.0199
                    Surrogate loss: -0.0037
             Mean action noise std: 0.96
                       Mean reward: 942.30
               Mean episode length: 293.17
                 Mean success rate: 0.00
                  Mean reward/step: 3.71
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1384448
                    Iteration time: 0.51s
                        Total time: 84.01s
                               ETA: 910.6s

################################################################################
                     [1m Learning iteration 169/2000 [0m

                       Computation: 16824 steps/s (collection: 0.282s, learning 0.205s)
               Value function loss: 3400.5618
                    Surrogate loss: -0.0045
             Mean action noise std: 0.96
                       Mean reward: 959.26
               Mean episode length: 292.33
                 Mean success rate: 0.00
                  Mean reward/step: 3.77
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 0.49s
                        Total time: 84.49s
                               ETA: 910.0s

################################################################################
                     [1m Learning iteration 170/2000 [0m

                       Computation: 16860 steps/s (collection: 0.285s, learning 0.201s)
               Value function loss: 3398.5026
                    Surrogate loss: -0.0058
             Mean action noise std: 0.96
                       Mean reward: 1011.64
               Mean episode length: 294.95
                 Mean success rate: 0.00
                  Mean reward/step: 3.80
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1400832
                    Iteration time: 0.49s
                        Total time: 84.98s
                               ETA: 909.4s

################################################################################
                     [1m Learning iteration 171/2000 [0m

                       Computation: 16083 steps/s (collection: 0.296s, learning 0.213s)
               Value function loss: 2946.5246
                    Surrogate loss: -0.0049
             Mean action noise std: 0.96
                       Mean reward: 1003.34
               Mean episode length: 281.22
                 Mean success rate: 0.00
                  Mean reward/step: 3.85
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 0.51s
                        Total time: 85.49s
                               ETA: 909.1s

################################################################################
                     [1m Learning iteration 172/2000 [0m

                       Computation: 15554 steps/s (collection: 0.286s, learning 0.240s)
               Value function loss: 3036.8993
                    Surrogate loss: -0.0041
             Mean action noise std: 0.96
                       Mean reward: 945.14
               Mean episode length: 259.20
                 Mean success rate: 0.00
                  Mean reward/step: 4.16
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1417216
                    Iteration time: 0.53s
                        Total time: 86.01s
                               ETA: 908.9s

################################################################################
                     [1m Learning iteration 173/2000 [0m

                       Computation: 16230 steps/s (collection: 0.294s, learning 0.211s)
               Value function loss: 4371.5913
                    Surrogate loss: -0.0045
             Mean action noise std: 0.96
                       Mean reward: 948.32
               Mean episode length: 255.89
                 Mean success rate: 0.00
                  Mean reward/step: 4.39
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 0.50s
                        Total time: 86.52s
                               ETA: 908.5s

################################################################################
                     [1m Learning iteration 174/2000 [0m

                       Computation: 15034 steps/s (collection: 0.335s, learning 0.209s)
               Value function loss: 3363.2149
                    Surrogate loss: -0.0051
             Mean action noise std: 0.96
                       Mean reward: 990.75
               Mean episode length: 258.64
                 Mean success rate: 0.00
                  Mean reward/step: 4.43
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1433600
                    Iteration time: 0.54s
                        Total time: 87.06s
                               ETA: 908.5s

################################################################################
                     [1m Learning iteration 175/2000 [0m

                       Computation: 16792 steps/s (collection: 0.282s, learning 0.206s)
               Value function loss: 3320.5909
                    Surrogate loss: -0.0055
             Mean action noise std: 0.96
                       Mean reward: 980.11
               Mean episode length: 249.10
                 Mean success rate: 0.00
                  Mean reward/step: 4.35
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 0.49s
                        Total time: 87.55s
                               ETA: 907.9s

################################################################################
                     [1m Learning iteration 176/2000 [0m

                       Computation: 16724 steps/s (collection: 0.267s, learning 0.223s)
               Value function loss: 5415.1596
                    Surrogate loss: -0.0040
             Mean action noise std: 0.95
                       Mean reward: 1022.80
               Mean episode length: 257.69
                 Mean success rate: 0.00
                  Mean reward/step: 4.49
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1449984
                    Iteration time: 0.49s
                        Total time: 88.04s
                               ETA: 907.3s

################################################################################
                     [1m Learning iteration 177/2000 [0m

                       Computation: 16804 steps/s (collection: 0.276s, learning 0.212s)
               Value function loss: 4464.8296
                    Surrogate loss: -0.0010
             Mean action noise std: 0.96
                       Mean reward: 1015.80
               Mean episode length: 262.86
                 Mean success rate: 0.00
                  Mean reward/step: 4.51
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 0.49s
                        Total time: 88.53s
                               ETA: 906.7s

################################################################################
                     [1m Learning iteration 178/2000 [0m

                       Computation: 14517 steps/s (collection: 0.328s, learning 0.236s)
               Value function loss: 6535.8966
                    Surrogate loss: -0.0032
             Mean action noise std: 0.95
                       Mean reward: 1135.24
               Mean episode length: 291.90
                 Mean success rate: 2.00
                  Mean reward/step: 4.82
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1466368
                    Iteration time: 0.56s
                        Total time: 89.09s
                               ETA: 906.9s

################################################################################
                     [1m Learning iteration 179/2000 [0m

                       Computation: 16478 steps/s (collection: 0.281s, learning 0.216s)
               Value function loss: 5878.9445
                    Surrogate loss: -0.0044
             Mean action noise std: 0.95
                       Mean reward: 1174.32
               Mean episode length: 289.33
                 Mean success rate: 3.50
                  Mean reward/step: 4.65
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.50s
                        Total time: 89.59s
                               ETA: 906.4s

################################################################################
                     [1m Learning iteration 180/2000 [0m

                       Computation: 17568 steps/s (collection: 0.260s, learning 0.207s)
               Value function loss: 4240.7123
                    Surrogate loss: -0.0045
             Mean action noise std: 0.95
                       Mean reward: 1146.89
               Mean episode length: 285.58
                 Mean success rate: 4.00
                  Mean reward/step: 4.86
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1482752
                    Iteration time: 0.47s
                        Total time: 90.06s
                               ETA: 905.5s

################################################################################
                     [1m Learning iteration 181/2000 [0m

                       Computation: 17275 steps/s (collection: 0.274s, learning 0.201s)
               Value function loss: 5542.9638
                    Surrogate loss: 0.0023
             Mean action noise std: 0.95
                       Mean reward: 1161.11
               Mean episode length: 289.85
                 Mean success rate: 5.00
                  Mean reward/step: 5.14
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 0.47s
                        Total time: 90.53s
                               ETA: 904.8s

################################################################################
                     [1m Learning iteration 182/2000 [0m

                       Computation: 16343 steps/s (collection: 0.296s, learning 0.205s)
               Value function loss: 7342.1167
                    Surrogate loss: -0.0001
             Mean action noise std: 0.95
                       Mean reward: 1126.33
               Mean episode length: 281.40
                 Mean success rate: 6.50
                  Mean reward/step: 5.67
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1499136
                    Iteration time: 0.50s
                        Total time: 91.03s
                               ETA: 904.4s

################################################################################
                     [1m Learning iteration 183/2000 [0m

                       Computation: 16623 steps/s (collection: 0.290s, learning 0.203s)
               Value function loss: 7818.3852
                    Surrogate loss: -0.0009
             Mean action noise std: 0.95
                       Mean reward: 1171.46
               Mean episode length: 280.24
                 Mean success rate: 8.00
                  Mean reward/step: 5.82
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 0.49s
                        Total time: 91.53s
                               ETA: 903.8s

################################################################################
                     [1m Learning iteration 184/2000 [0m

                       Computation: 16147 steps/s (collection: 0.297s, learning 0.210s)
               Value function loss: 7608.3695
                    Surrogate loss: -0.0025
             Mean action noise std: 0.95
                       Mean reward: 1173.58
               Mean episode length: 266.14
                 Mean success rate: 11.00
                  Mean reward/step: 6.20
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1515520
                    Iteration time: 0.51s
                        Total time: 92.03s
                               ETA: 903.4s

################################################################################
                     [1m Learning iteration 185/2000 [0m

                       Computation: 16829 steps/s (collection: 0.282s, learning 0.205s)
               Value function loss: 7555.5639
                    Surrogate loss: -0.0024
             Mean action noise std: 0.95
                       Mean reward: 1190.94
               Mean episode length: 268.27
                 Mean success rate: 12.50
                  Mean reward/step: 6.62
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 0.49s
                        Total time: 92.52s
                               ETA: 902.8s

################################################################################
                     [1m Learning iteration 186/2000 [0m

                       Computation: 16449 steps/s (collection: 0.285s, learning 0.213s)
               Value function loss: 7480.1501
                    Surrogate loss: -0.0010
             Mean action noise std: 0.95
                       Mean reward: 1308.16
               Mean episode length: 276.81
                 Mean success rate: 16.00
                  Mean reward/step: 7.03
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1531904
                    Iteration time: 0.50s
                        Total time: 93.02s
                               ETA: 902.3s

################################################################################
                     [1m Learning iteration 187/2000 [0m

                       Computation: 16511 steps/s (collection: 0.285s, learning 0.211s)
               Value function loss: 9051.9381
                    Surrogate loss: -0.0030
             Mean action noise std: 0.95
                       Mean reward: 1338.55
               Mean episode length: 263.75
                 Mean success rate: 19.50
                  Mean reward/step: 7.08
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 0.50s
                        Total time: 93.51s
                               ETA: 901.8s

################################################################################
                     [1m Learning iteration 188/2000 [0m

                       Computation: 16705 steps/s (collection: 0.281s, learning 0.210s)
               Value function loss: 7468.4657
                    Surrogate loss: -0.0036
             Mean action noise std: 0.95
                       Mean reward: 1403.08
               Mean episode length: 267.65
                 Mean success rate: 22.50
                  Mean reward/step: 7.69
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1548288
                    Iteration time: 0.49s
                        Total time: 94.00s
                               ETA: 901.2s

################################################################################
                     [1m Learning iteration 189/2000 [0m

                       Computation: 14967 steps/s (collection: 0.295s, learning 0.253s)
               Value function loss: 9735.5163
                    Surrogate loss: -0.0032
             Mean action noise std: 0.95
                       Mean reward: 1408.89
               Mean episode length: 260.89
                 Mean success rate: 24.50
                  Mean reward/step: 8.42
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 0.55s
                        Total time: 94.55s
                               ETA: 901.2s

################################################################################
                     [1m Learning iteration 190/2000 [0m

                       Computation: 16997 steps/s (collection: 0.268s, learning 0.214s)
               Value function loss: 9811.0013
                    Surrogate loss: -0.0031
             Mean action noise std: 0.95
                       Mean reward: 1456.60
               Mean episode length: 256.96
                 Mean success rate: 27.00
                  Mean reward/step: 9.36
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1564672
                    Iteration time: 0.48s
                        Total time: 95.03s
                               ETA: 900.6s

################################################################################
                     [1m Learning iteration 191/2000 [0m

                       Computation: 16486 steps/s (collection: 0.284s, learning 0.213s)
               Value function loss: 14557.5147
                    Surrogate loss: -0.0028
             Mean action noise std: 0.95
                       Mean reward: 1533.79
               Mean episode length: 258.94
                 Mean success rate: 30.50
                  Mean reward/step: 9.94
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.50s
                        Total time: 95.53s
                               ETA: 900.1s

################################################################################
                     [1m Learning iteration 192/2000 [0m

                       Computation: 16434 steps/s (collection: 0.286s, learning 0.212s)
               Value function loss: 21307.5779
                    Surrogate loss: -0.0002
             Mean action noise std: 0.95
                       Mean reward: 1662.01
               Mean episode length: 266.45
                 Mean success rate: 35.00
                  Mean reward/step: 10.48
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1581056
                    Iteration time: 0.50s
                        Total time: 96.03s
                               ETA: 899.6s

################################################################################
                     [1m Learning iteration 193/2000 [0m

                       Computation: 16734 steps/s (collection: 0.286s, learning 0.203s)
               Value function loss: 20015.0834
                    Surrogate loss: -0.0033
             Mean action noise std: 0.95
                       Mean reward: 1796.24
               Mean episode length: 266.25
                 Mean success rate: 39.00
                  Mean reward/step: 10.58
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 0.49s
                        Total time: 96.52s
                               ETA: 899.0s

################################################################################
                     [1m Learning iteration 194/2000 [0m

                       Computation: 16277 steps/s (collection: 0.294s, learning 0.210s)
               Value function loss: 25739.6687
                    Surrogate loss: -0.0031
             Mean action noise std: 0.95
                       Mean reward: 2094.49
               Mean episode length: 286.53
                 Mean success rate: 45.00
                  Mean reward/step: 9.77
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 1597440
                    Iteration time: 0.50s
                        Total time: 97.02s
                               ETA: 898.6s

################################################################################
                     [1m Learning iteration 195/2000 [0m

                       Computation: 16285 steps/s (collection: 0.264s, learning 0.239s)
               Value function loss: 20109.9418
                    Surrogate loss: -0.0035
             Mean action noise std: 0.95
                       Mean reward: 2190.16
               Mean episode length: 296.71
                 Mean success rate: 47.00
                  Mean reward/step: 10.04
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 0.50s
                        Total time: 97.52s
                               ETA: 898.1s

################################################################################
                     [1m Learning iteration 196/2000 [0m

                       Computation: 15822 steps/s (collection: 0.288s, learning 0.230s)
               Value function loss: 21228.7609
                    Surrogate loss: -0.0036
             Mean action noise std: 0.95
                       Mean reward: 2356.53
               Mean episode length: 298.61
                 Mean success rate: 50.50
                  Mean reward/step: 10.93
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1613824
                    Iteration time: 0.52s
                        Total time: 98.04s
                               ETA: 897.8s

################################################################################
                     [1m Learning iteration 197/2000 [0m

                       Computation: 16850 steps/s (collection: 0.277s, learning 0.209s)
               Value function loss: 28127.2134
                    Surrogate loss: -0.0033
             Mean action noise std: 0.95
                       Mean reward: 2386.50
               Mean episode length: 293.79
                 Mean success rate: 50.00
                  Mean reward/step: 11.63
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 0.49s
                        Total time: 98.53s
                               ETA: 897.2s

################################################################################
                     [1m Learning iteration 198/2000 [0m

                       Computation: 16500 steps/s (collection: 0.292s, learning 0.205s)
               Value function loss: 32603.6633
                    Surrogate loss: -0.0027
             Mean action noise std: 0.95
                       Mean reward: 2509.51
               Mean episode length: 286.77
                 Mean success rate: 51.00
                  Mean reward/step: 11.78
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1630208
                    Iteration time: 0.50s
                        Total time: 99.02s
                               ETA: 896.7s

################################################################################
                     [1m Learning iteration 199/2000 [0m

                       Computation: 17238 steps/s (collection: 0.258s, learning 0.218s)
               Value function loss: 30938.6842
                    Surrogate loss: -0.0037
             Mean action noise std: 0.95
                       Mean reward: 2679.15
               Mean episode length: 295.75
                 Mean success rate: 53.00
                  Mean reward/step: 12.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 0.48s
                        Total time: 99.50s
                               ETA: 896.0s

################################################################################
                     [1m Learning iteration 200/2000 [0m

                       Computation: 17187 steps/s (collection: 0.265s, learning 0.211s)
               Value function loss: 40348.3185
                    Surrogate loss: -0.0022
             Mean action noise std: 0.95
                       Mean reward: 2935.52
               Mean episode length: 300.48
                 Mean success rate: 56.50
                  Mean reward/step: 13.14
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1646592
                    Iteration time: 0.48s
                        Total time: 99.98s
                               ETA: 895.3s

################################################################################
                     [1m Learning iteration 201/2000 [0m

                       Computation: 17250 steps/s (collection: 0.260s, learning 0.215s)
               Value function loss: 28034.0660
                    Surrogate loss: -0.0026
             Mean action noise std: 0.95
                       Mean reward: 2876.43
               Mean episode length: 293.88
                 Mean success rate: 54.00
                  Mean reward/step: 13.65
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 0.47s
                        Total time: 100.45s
                               ETA: 894.6s

################################################################################
                     [1m Learning iteration 202/2000 [0m

                       Computation: 13993 steps/s (collection: 0.342s, learning 0.244s)
               Value function loss: 34266.4648
                    Surrogate loss: -0.0030
             Mean action noise std: 0.95
                       Mean reward: 2743.95
               Mean episode length: 265.51
                 Mean success rate: 49.50
                  Mean reward/step: 13.47
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1662976
                    Iteration time: 0.59s
                        Total time: 101.04s
                               ETA: 894.9s

################################################################################
                     [1m Learning iteration 203/2000 [0m

                       Computation: 15220 steps/s (collection: 0.291s, learning 0.247s)
               Value function loss: 42893.3681
                    Surrogate loss: -0.0021
             Mean action noise std: 0.95
                       Mean reward: 2810.32
               Mean episode length: 267.25
                 Mean success rate: 50.00
                  Mean reward/step: 14.08
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.54s
                        Total time: 101.58s
                               ETA: 894.8s

################################################################################
                     [1m Learning iteration 204/2000 [0m

                       Computation: 15495 steps/s (collection: 0.302s, learning 0.227s)
               Value function loss: 32114.5225
                    Surrogate loss: -0.0025
             Mean action noise std: 0.95
                       Mean reward: 2752.10
               Mean episode length: 255.43
                 Mean success rate: 48.50
                  Mean reward/step: 14.05
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1679360
                    Iteration time: 0.53s
                        Total time: 102.10s
                               ETA: 894.5s

################################################################################
                     [1m Learning iteration 205/2000 [0m

                       Computation: 15216 steps/s (collection: 0.313s, learning 0.226s)
               Value function loss: 35621.7428
                    Surrogate loss: -0.0009
             Mean action noise std: 0.95
                       Mean reward: 2742.33
               Mean episode length: 243.36
                 Mean success rate: 48.00
                  Mean reward/step: 14.88
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 0.54s
                        Total time: 102.64s
                               ETA: 894.4s

################################################################################
                     [1m Learning iteration 206/2000 [0m

                       Computation: 15750 steps/s (collection: 0.289s, learning 0.231s)
               Value function loss: 49895.2424
                    Surrogate loss: -0.0025
             Mean action noise std: 0.95
                       Mean reward: 2800.09
               Mean episode length: 241.30
                 Mean success rate: 46.50
                  Mean reward/step: 14.98
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1695744
                    Iteration time: 0.52s
                        Total time: 103.16s
                               ETA: 894.1s

################################################################################
                     [1m Learning iteration 207/2000 [0m

                       Computation: 15327 steps/s (collection: 0.281s, learning 0.253s)
               Value function loss: 39319.6811
                    Surrogate loss: -0.0021
             Mean action noise std: 0.95
                       Mean reward: 2877.93
               Mean episode length: 233.53
                 Mean success rate: 47.00
                  Mean reward/step: 15.38
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 0.53s
                        Total time: 103.70s
                               ETA: 893.9s

################################################################################
                     [1m Learning iteration 208/2000 [0m

                       Computation: 15555 steps/s (collection: 0.294s, learning 0.233s)
               Value function loss: 50097.0020
                    Surrogate loss: -0.0022
             Mean action noise std: 0.95
                       Mean reward: 3150.11
               Mean episode length: 243.26
                 Mean success rate: 50.50
                  Mean reward/step: 15.64
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1712128
                    Iteration time: 0.53s
                        Total time: 104.22s
                               ETA: 893.6s

################################################################################
                     [1m Learning iteration 209/2000 [0m

                       Computation: 15865 steps/s (collection: 0.289s, learning 0.227s)
               Value function loss: 55420.1599
                    Surrogate loss: -0.0022
             Mean action noise std: 0.95
                       Mean reward: 3565.92
               Mean episode length: 261.23
                 Mean success rate: 55.00
                  Mean reward/step: 16.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 0.52s
                        Total time: 104.74s
                               ETA: 893.3s

################################################################################
                     [1m Learning iteration 210/2000 [0m

                       Computation: 16146 steps/s (collection: 0.292s, learning 0.215s)
               Value function loss: 42785.1353
                    Surrogate loss: -0.0026
             Mean action noise std: 0.95
                       Mean reward: 3704.73
               Mean episode length: 267.25
                 Mean success rate: 56.00
                  Mean reward/step: 16.00
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1728512
                    Iteration time: 0.51s
                        Total time: 105.25s
                               ETA: 892.9s

################################################################################
                     [1m Learning iteration 211/2000 [0m

                       Computation: 16173 steps/s (collection: 0.295s, learning 0.212s)
               Value function loss: 39908.8252
                    Surrogate loss: -0.0023
             Mean action noise std: 0.95
                       Mean reward: 4023.21
               Mean episode length: 278.81
                 Mean success rate: 58.50
                  Mean reward/step: 16.10
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 0.51s
                        Total time: 105.75s
                               ETA: 892.4s

################################################################################
                     [1m Learning iteration 212/2000 [0m

                       Computation: 16032 steps/s (collection: 0.300s, learning 0.211s)
               Value function loss: 53820.0619
                    Surrogate loss: -0.0020
             Mean action noise std: 0.95
                       Mean reward: 4298.99
               Mean episode length: 288.69
                 Mean success rate: 60.00
                  Mean reward/step: 16.44
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1744896
                    Iteration time: 0.51s
                        Total time: 106.26s
                               ETA: 892.0s

################################################################################
                     [1m Learning iteration 213/2000 [0m

                       Computation: 16397 steps/s (collection: 0.281s, learning 0.219s)
               Value function loss: 48586.3627
                    Surrogate loss: -0.0027
             Mean action noise std: 0.95
                       Mean reward: 4612.11
               Mean episode length: 298.88
                 Mean success rate: 63.00
                  Mean reward/step: 16.50
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 0.50s
                        Total time: 106.76s
                               ETA: 891.5s

################################################################################
                     [1m Learning iteration 214/2000 [0m

                       Computation: 16207 steps/s (collection: 0.294s, learning 0.211s)
               Value function loss: 56537.9821
                    Surrogate loss: -0.0021
             Mean action noise std: 0.95
                       Mean reward: 4613.67
               Mean episode length: 299.93
                 Mean success rate: 62.50
                  Mean reward/step: 16.39
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1761280
                    Iteration time: 0.51s
                        Total time: 107.27s
                               ETA: 891.1s

################################################################################
                     [1m Learning iteration 215/2000 [0m

                       Computation: 16656 steps/s (collection: 0.279s, learning 0.213s)
               Value function loss: 42326.4187
                    Surrogate loss: -0.0017
             Mean action noise std: 0.95
                       Mean reward: 4565.55
               Mean episode length: 305.79
                 Mean success rate: 61.50
                  Mean reward/step: 16.86
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.49s
                        Total time: 107.76s
                               ETA: 890.5s

################################################################################
                     [1m Learning iteration 216/2000 [0m

                       Computation: 16755 steps/s (collection: 0.277s, learning 0.212s)
               Value function loss: 50514.8411
                    Surrogate loss: -0.0018
             Mean action noise std: 0.95
                       Mean reward: 4539.94
               Mean episode length: 308.37
                 Mean success rate: 60.00
                  Mean reward/step: 16.57
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1777664
                    Iteration time: 0.49s
                        Total time: 108.25s
                               ETA: 889.9s

################################################################################
                     [1m Learning iteration 217/2000 [0m

                       Computation: 17116 steps/s (collection: 0.269s, learning 0.209s)
               Value function loss: 51957.9762
                    Surrogate loss: -0.0018
             Mean action noise std: 0.95
                       Mean reward: 4528.75
               Mean episode length: 301.60
                 Mean success rate: 58.50
                  Mean reward/step: 16.91
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 0.48s
                        Total time: 108.73s
                               ETA: 889.3s

################################################################################
                     [1m Learning iteration 218/2000 [0m

                       Computation: 16452 steps/s (collection: 0.286s, learning 0.212s)
               Value function loss: 65987.8642
                    Surrogate loss: -0.0018
             Mean action noise std: 0.95
                       Mean reward: 4795.61
               Mean episode length: 312.29
                 Mean success rate: 61.50
                  Mean reward/step: 16.71
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1794048
                    Iteration time: 0.50s
                        Total time: 109.23s
                               ETA: 888.8s

################################################################################
                     [1m Learning iteration 219/2000 [0m

                       Computation: 15616 steps/s (collection: 0.301s, learning 0.224s)
               Value function loss: 50593.6612
                    Surrogate loss: -0.0027
             Mean action noise std: 0.95
                       Mean reward: 4767.79
               Mean episode length: 311.21
                 Mean success rate: 61.00
                  Mean reward/step: 16.98
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 0.52s
                        Total time: 109.75s
                               ETA: 888.5s

################################################################################
                     [1m Learning iteration 220/2000 [0m

                       Computation: 15894 steps/s (collection: 0.285s, learning 0.231s)
               Value function loss: 62896.1115
                    Surrogate loss: -0.0023
             Mean action noise std: 0.95
                       Mean reward: 4786.59
               Mean episode length: 313.73
                 Mean success rate: 61.50
                  Mean reward/step: 17.69
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1810432
                    Iteration time: 0.52s
                        Total time: 110.27s
                               ETA: 888.1s

################################################################################
                     [1m Learning iteration 221/2000 [0m

                       Computation: 15240 steps/s (collection: 0.324s, learning 0.213s)
               Value function loss: 61676.3463
                    Surrogate loss: -0.0018
             Mean action noise std: 0.95
                       Mean reward: 4850.68
               Mean episode length: 310.83
                 Mean success rate: 62.00
                  Mean reward/step: 18.23
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 0.54s
                        Total time: 110.80s
                               ETA: 887.9s

################################################################################
                     [1m Learning iteration 222/2000 [0m

                       Computation: 16356 steps/s (collection: 0.294s, learning 0.207s)
               Value function loss: 49012.7255
                    Surrogate loss: -0.0024
             Mean action noise std: 0.95
                       Mean reward: 5059.97
               Mean episode length: 317.72
                 Mean success rate: 63.50
                  Mean reward/step: 17.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1826816
                    Iteration time: 0.50s
                        Total time: 111.31s
                               ETA: 887.4s

################################################################################
                     [1m Learning iteration 223/2000 [0m

                       Computation: 16602 steps/s (collection: 0.279s, learning 0.214s)
               Value function loss: 53512.1099
                    Surrogate loss: -0.0025
             Mean action noise std: 0.95
                       Mean reward: 5063.92
               Mean episode length: 307.49
                 Mean success rate: 62.00
                  Mean reward/step: 18.81
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 0.49s
                        Total time: 111.80s
                               ETA: 886.9s

################################################################################
                     [1m Learning iteration 224/2000 [0m

                       Computation: 16192 steps/s (collection: 0.297s, learning 0.208s)
               Value function loss: 49913.9177
                    Surrogate loss: -0.0016
             Mean action noise std: 0.95
                       Mean reward: 5025.36
               Mean episode length: 299.61
                 Mean success rate: 62.00
                  Mean reward/step: 19.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1843200
                    Iteration time: 0.51s
                        Total time: 112.30s
                               ETA: 886.5s

################################################################################
                     [1m Learning iteration 225/2000 [0m

                       Computation: 16254 steps/s (collection: 0.299s, learning 0.205s)
               Value function loss: 85972.4177
                    Surrogate loss: -0.0014
             Mean action noise std: 0.95
                       Mean reward: 5037.20
               Mean episode length: 298.65
                 Mean success rate: 62.00
                  Mean reward/step: 18.97
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 0.50s
                        Total time: 112.81s
                               ETA: 886.0s

################################################################################
                     [1m Learning iteration 226/2000 [0m

                       Computation: 15821 steps/s (collection: 0.283s, learning 0.235s)
               Value function loss: 59218.3561
                    Surrogate loss: -0.0023
             Mean action noise std: 0.95
                       Mean reward: 5053.91
               Mean episode length: 297.08
                 Mean success rate: 60.00
                  Mean reward/step: 17.91
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1859584
                    Iteration time: 0.52s
                        Total time: 113.33s
                               ETA: 885.6s

################################################################################
                     [1m Learning iteration 227/2000 [0m

                       Computation: 16405 steps/s (collection: 0.284s, learning 0.215s)
               Value function loss: 49192.2335
                    Surrogate loss: -0.0016
             Mean action noise std: 0.95
                       Mean reward: 4939.36
               Mean episode length: 291.05
                 Mean success rate: 57.50
                  Mean reward/step: 17.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.50s
                        Total time: 113.83s
                               ETA: 885.1s

################################################################################
                     [1m Learning iteration 228/2000 [0m

                       Computation: 16476 steps/s (collection: 0.278s, learning 0.219s)
               Value function loss: 81499.1871
                    Surrogate loss: -0.0020
             Mean action noise std: 0.95
                       Mean reward: 5245.92
               Mean episode length: 300.79
                 Mean success rate: 59.50
                  Mean reward/step: 17.65
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1875968
                    Iteration time: 0.50s
                        Total time: 114.32s
                               ETA: 884.6s

################################################################################
                     [1m Learning iteration 229/2000 [0m

                       Computation: 16761 steps/s (collection: 0.278s, learning 0.211s)
               Value function loss: 43654.2924
                    Surrogate loss: -0.0024
             Mean action noise std: 0.95
                       Mean reward: 4906.31
               Mean episode length: 284.00
                 Mean success rate: 55.50
                  Mean reward/step: 17.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 0.49s
                        Total time: 114.81s
                               ETA: 884.0s

################################################################################
                     [1m Learning iteration 230/2000 [0m

                       Computation: 16640 steps/s (collection: 0.285s, learning 0.207s)
               Value function loss: 59075.1733
                    Surrogate loss: -0.0021
             Mean action noise std: 0.95
                       Mean reward: 5043.65
               Mean episode length: 289.83
                 Mean success rate: 57.50
                  Mean reward/step: 17.20
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1892352
                    Iteration time: 0.49s
                        Total time: 115.30s
                               ETA: 883.5s

################################################################################
                     [1m Learning iteration 231/2000 [0m

                       Computation: 16780 steps/s (collection: 0.274s, learning 0.214s)
               Value function loss: 75887.7548
                    Surrogate loss: -0.0013
             Mean action noise std: 0.95
                       Mean reward: 5254.97
               Mean episode length: 297.89
                 Mean success rate: 59.00
                  Mean reward/step: 17.12
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 0.49s
                        Total time: 115.79s
                               ETA: 882.9s

################################################################################
                     [1m Learning iteration 232/2000 [0m

                       Computation: 16306 steps/s (collection: 0.282s, learning 0.221s)
               Value function loss: 66259.7693
                    Surrogate loss: -0.0026
             Mean action noise std: 0.95
                       Mean reward: 5287.44
               Mean episode length: 291.04
                 Mean success rate: 59.00
                  Mean reward/step: 16.05
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1908736
                    Iteration time: 0.50s
                        Total time: 116.29s
                               ETA: 882.4s

################################################################################
                     [1m Learning iteration 233/2000 [0m

                       Computation: 16169 steps/s (collection: 0.293s, learning 0.213s)
               Value function loss: 87683.0494
                    Surrogate loss: -0.0018
             Mean action noise std: 0.95
                       Mean reward: 5515.52
               Mean episode length: 300.86
                 Mean success rate: 62.00
                  Mean reward/step: 16.13
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 0.51s
                        Total time: 116.80s
                               ETA: 882.0s

################################################################################
                     [1m Learning iteration 234/2000 [0m

                       Computation: 15748 steps/s (collection: 0.291s, learning 0.230s)
               Value function loss: 64935.4128
                    Surrogate loss: -0.0026
             Mean action noise std: 0.95
                       Mean reward: 5477.94
               Mean episode length: 293.43
                 Mean success rate: 61.00
                  Mean reward/step: 15.20
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1925120
                    Iteration time: 0.52s
                        Total time: 117.32s
                               ETA: 881.7s

################################################################################
                     [1m Learning iteration 235/2000 [0m

                       Computation: 16221 steps/s (collection: 0.289s, learning 0.216s)
               Value function loss: 53364.5201
                    Surrogate loss: -0.0029
             Mean action noise std: 0.95
                       Mean reward: 5496.99
               Mean episode length: 297.56
                 Mean success rate: 62.00
                  Mean reward/step: 16.27
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 0.51s
                        Total time: 117.83s
                               ETA: 881.2s

################################################################################
                     [1m Learning iteration 236/2000 [0m

                       Computation: 16765 steps/s (collection: 0.282s, learning 0.207s)
               Value function loss: 81925.6083
                    Surrogate loss: -0.0020
             Mean action noise std: 0.95
                       Mean reward: 5807.30
               Mean episode length: 308.81
                 Mean success rate: 64.50
                  Mean reward/step: 17.10
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1941504
                    Iteration time: 0.49s
                        Total time: 118.31s
                               ETA: 880.6s

################################################################################
                     [1m Learning iteration 237/2000 [0m

                       Computation: 16646 steps/s (collection: 0.276s, learning 0.217s)
               Value function loss: 59770.3393
                    Surrogate loss: -0.0024
             Mean action noise std: 0.95
                       Mean reward: 5768.40
               Mean episode length: 307.54
                 Mean success rate: 64.00
                  Mean reward/step: 17.31
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 0.49s
                        Total time: 118.81s
                               ETA: 880.1s

################################################################################
                     [1m Learning iteration 238/2000 [0m

                       Computation: 16485 steps/s (collection: 0.288s, learning 0.209s)
               Value function loss: 45626.6092
                    Surrogate loss: -0.0029
             Mean action noise std: 0.95
                       Mean reward: 5271.79
               Mean episode length: 292.80
                 Mean success rate: 59.50
                  Mean reward/step: 17.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1957888
                    Iteration time: 0.50s
                        Total time: 119.30s
                               ETA: 879.6s

################################################################################
                     [1m Learning iteration 239/2000 [0m

                       Computation: 16424 steps/s (collection: 0.287s, learning 0.211s)
               Value function loss: 58891.4517
                    Surrogate loss: -0.0022
             Mean action noise std: 0.95
                       Mean reward: 4723.57
               Mean episode length: 278.50
                 Mean success rate: 53.00
                  Mean reward/step: 18.35
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.50s
                        Total time: 119.80s
                               ETA: 879.1s

################################################################################
                     [1m Learning iteration 240/2000 [0m

                       Computation: 17265 steps/s (collection: 0.270s, learning 0.204s)
               Value function loss: 89188.0229
                    Surrogate loss: -0.0019
             Mean action noise std: 0.95
                       Mean reward: 4309.85
               Mean episode length: 262.46
                 Mean success rate: 50.50
                  Mean reward/step: 18.40
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1974272
                    Iteration time: 0.47s
                        Total time: 120.28s
                               ETA: 878.4s

################################################################################
                     [1m Learning iteration 241/2000 [0m

                       Computation: 16547 steps/s (collection: 0.284s, learning 0.211s)
               Value function loss: 66058.3696
                    Surrogate loss: -0.0029
             Mean action noise std: 0.95
                       Mean reward: 4275.67
               Mean episode length: 267.46
                 Mean success rate: 51.00
                  Mean reward/step: 16.73
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 0.50s
                        Total time: 120.77s
                               ETA: 877.8s

################################################################################
                     [1m Learning iteration 242/2000 [0m

                       Computation: 16029 steps/s (collection: 0.300s, learning 0.211s)
               Value function loss: 76753.4726
                    Surrogate loss: -0.0028
             Mean action noise std: 0.95
                       Mean reward: 3770.77
               Mean episode length: 249.02
                 Mean success rate: 45.50
                  Mean reward/step: 16.75
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1990656
                    Iteration time: 0.51s
                        Total time: 121.28s
                               ETA: 877.4s

################################################################################
                     [1m Learning iteration 243/2000 [0m

                       Computation: 17029 steps/s (collection: 0.274s, learning 0.207s)
               Value function loss: 62055.4915
                    Surrogate loss: -0.0028
             Mean action noise std: 0.95
                       Mean reward: 3630.89
               Mean episode length: 244.38
                 Mean success rate: 44.50
                  Mean reward/step: 16.95
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 0.48s
                        Total time: 121.76s
                               ETA: 876.8s

################################################################################
                     [1m Learning iteration 244/2000 [0m

                       Computation: 16742 steps/s (collection: 0.275s, learning 0.214s)
               Value function loss: 82060.4376
                    Surrogate loss: -0.0018
             Mean action noise std: 0.95
                       Mean reward: 3928.86
               Mean episode length: 250.38
                 Mean success rate: 47.00
                  Mean reward/step: 16.50
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2007040
                    Iteration time: 0.49s
                        Total time: 122.25s
                               ETA: 876.2s

################################################################################
                     [1m Learning iteration 245/2000 [0m

                       Computation: 16415 steps/s (collection: 0.283s, learning 0.216s)
               Value function loss: 63598.0586
                    Surrogate loss: -0.0025
             Mean action noise std: 0.95
                       Mean reward: 4165.39
               Mean episode length: 256.93
                 Mean success rate: 50.50
                  Mean reward/step: 17.07
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 0.50s
                        Total time: 122.75s
                               ETA: 875.7s

################################################################################
                     [1m Learning iteration 246/2000 [0m

                       Computation: 16417 steps/s (collection: 0.288s, learning 0.211s)
               Value function loss: 79329.1931
                    Surrogate loss: -0.0025
             Mean action noise std: 0.95
                       Mean reward: 4391.99
               Mean episode length: 261.56
                 Mean success rate: 51.00
                  Mean reward/step: 16.92
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2023424
                    Iteration time: 0.50s
                        Total time: 123.25s
                               ETA: 875.2s

################################################################################
                     [1m Learning iteration 247/2000 [0m

                       Computation: 16621 steps/s (collection: 0.287s, learning 0.206s)
               Value function loss: 74334.7128
                    Surrogate loss: -0.0022
             Mean action noise std: 0.95
                       Mean reward: 4644.20
               Mean episode length: 273.25
                 Mean success rate: 53.50
                  Mean reward/step: 16.78
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 0.49s
                        Total time: 123.74s
                               ETA: 874.7s

################################################################################
                     [1m Learning iteration 248/2000 [0m

                       Computation: 16740 steps/s (collection: 0.282s, learning 0.207s)
               Value function loss: 65455.5214
                    Surrogate loss: -0.0036
             Mean action noise std: 0.95
                       Mean reward: 5057.98
               Mean episode length: 287.16
                 Mean success rate: 57.50
                  Mean reward/step: 17.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2039808
                    Iteration time: 0.49s
                        Total time: 124.23s
                               ETA: 874.1s

################################################################################
                     [1m Learning iteration 249/2000 [0m

                       Computation: 15364 steps/s (collection: 0.326s, learning 0.207s)
               Value function loss: 70955.1507
                    Surrogate loss: -0.0022
             Mean action noise std: 0.95
                       Mean reward: 5223.02
               Mean episode length: 291.50
                 Mean success rate: 58.50
                  Mean reward/step: 18.06
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 0.53s
                        Total time: 124.77s
                               ETA: 873.9s

################################################################################
                     [1m Learning iteration 250/2000 [0m

                       Computation: 15689 steps/s (collection: 0.306s, learning 0.216s)
               Value function loss: 67360.0103
                    Surrogate loss: -0.0019
             Mean action noise std: 0.95
                       Mean reward: 5254.29
               Mean episode length: 297.39
                 Mean success rate: 59.00
                  Mean reward/step: 18.14
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2056192
                    Iteration time: 0.52s
                        Total time: 125.29s
                               ETA: 873.5s

################################################################################
                     [1m Learning iteration 251/2000 [0m

                       Computation: 15764 steps/s (collection: 0.302s, learning 0.218s)
               Value function loss: 53486.0654
                    Surrogate loss: -0.0023
             Mean action noise std: 0.95
                       Mean reward: 5364.11
               Mean episode length: 301.21
                 Mean success rate: 60.00
                  Mean reward/step: 18.14
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.52s
                        Total time: 125.81s
                               ETA: 873.2s

################################################################################
                     [1m Learning iteration 252/2000 [0m

                       Computation: 16627 steps/s (collection: 0.282s, learning 0.210s)
               Value function loss: 75643.4279
                    Surrogate loss: -0.0024
             Mean action noise std: 0.95
                       Mean reward: 5620.88
               Mean episode length: 313.69
                 Mean success rate: 62.00
                  Mean reward/step: 18.65
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2072576
                    Iteration time: 0.49s
                        Total time: 126.30s
                               ETA: 872.6s

################################################################################
                     [1m Learning iteration 253/2000 [0m

                       Computation: 15784 steps/s (collection: 0.291s, learning 0.228s)
               Value function loss: 32952.5399
                    Surrogate loss: -0.0033
             Mean action noise std: 0.95
                       Mean reward: 4970.06
               Mean episode length: 287.20
                 Mean success rate: 57.00
                  Mean reward/step: 19.35
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 0.52s
                        Total time: 126.82s
                               ETA: 872.3s

################################################################################
                     [1m Learning iteration 254/2000 [0m

                       Computation: 15442 steps/s (collection: 0.310s, learning 0.221s)
               Value function loss: 70575.5520
                    Surrogate loss: -0.0016
             Mean action noise std: 0.95
                       Mean reward: 4546.34
               Mean episode length: 274.89
                 Mean success rate: 53.50
                  Mean reward/step: 19.57
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2088960
                    Iteration time: 0.53s
                        Total time: 127.35s
                               ETA: 872.0s

################################################################################
                     [1m Learning iteration 255/2000 [0m

                       Computation: 15748 steps/s (collection: 0.293s, learning 0.227s)
               Value function loss: 65546.4956
                    Surrogate loss: -0.0019
             Mean action noise std: 0.95
                       Mean reward: 4529.41
               Mean episode length: 270.18
                 Mean success rate: 52.00
                  Mean reward/step: 19.86
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 0.52s
                        Total time: 127.87s
                               ETA: 871.6s

################################################################################
                     [1m Learning iteration 256/2000 [0m

                       Computation: 14936 steps/s (collection: 0.321s, learning 0.227s)
               Value function loss: 106721.9436
                    Surrogate loss: -0.0021
             Mean action noise std: 0.95
                       Mean reward: 4454.22
               Mean episode length: 270.93
                 Mean success rate: 51.50
                  Mean reward/step: 20.06
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2105344
                    Iteration time: 0.55s
                        Total time: 128.42s
                               ETA: 871.5s

################################################################################
                     [1m Learning iteration 257/2000 [0m

                       Computation: 15370 steps/s (collection: 0.300s, learning 0.233s)
               Value function loss: 88516.9920
                    Surrogate loss: -0.0025
             Mean action noise std: 0.95
                       Mean reward: 4678.48
               Mean episode length: 275.86
                 Mean success rate: 52.00
                  Mean reward/step: 19.16
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 0.53s
                        Total time: 128.95s
                               ETA: 871.2s

################################################################################
                     [1m Learning iteration 258/2000 [0m

                       Computation: 15553 steps/s (collection: 0.287s, learning 0.240s)
               Value function loss: 76376.9341
                    Surrogate loss: -0.0030
             Mean action noise std: 0.95
                       Mean reward: 4646.56
               Mean episode length: 275.37
                 Mean success rate: 52.50
                  Mean reward/step: 18.40
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2121728
                    Iteration time: 0.53s
                        Total time: 129.48s
                               ETA: 870.9s

################################################################################
                     [1m Learning iteration 259/2000 [0m

                       Computation: 14722 steps/s (collection: 0.316s, learning 0.240s)
               Value function loss: 92724.3359
                    Surrogate loss: -0.0014
             Mean action noise std: 0.95
                       Mean reward: 5009.70
               Mean episode length: 288.51
                 Mean success rate: 56.00
                  Mean reward/step: 18.87
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 0.56s
                        Total time: 130.04s
                               ETA: 870.7s

################################################################################
                     [1m Learning iteration 260/2000 [0m

                       Computation: 14411 steps/s (collection: 0.301s, learning 0.268s)
               Value function loss: 84778.8659
                    Surrogate loss: -0.0022
             Mean action noise std: 0.95
                       Mean reward: 5748.56
               Mean episode length: 310.56
                 Mean success rate: 59.00
                  Mean reward/step: 18.76
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2138112
                    Iteration time: 0.57s
                        Total time: 130.60s
                               ETA: 870.7s

################################################################################
                     [1m Learning iteration 261/2000 [0m

                       Computation: 16352 steps/s (collection: 0.297s, learning 0.204s)
               Value function loss: 87995.8877
                    Surrogate loss: -0.0017
             Mean action noise std: 0.95
                       Mean reward: 6252.53
               Mean episode length: 327.72
                 Mean success rate: 63.50
                  Mean reward/step: 18.91
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 0.50s
                        Total time: 131.10s
                               ETA: 870.2s

################################################################################
                     [1m Learning iteration 262/2000 [0m

                       Computation: 14963 steps/s (collection: 0.300s, learning 0.247s)
               Value function loss: 76034.3221
                    Surrogate loss: -0.0029
             Mean action noise std: 0.95
                       Mean reward: 6445.48
               Mean episode length: 340.36
                 Mean success rate: 66.00
                  Mean reward/step: 18.98
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2154496
                    Iteration time: 0.55s
                        Total time: 131.65s
                               ETA: 870.0s

################################################################################
                     [1m Learning iteration 263/2000 [0m

                       Computation: 16988 steps/s (collection: 0.269s, learning 0.213s)
               Value function loss: 95911.8902
                    Surrogate loss: -0.0022
             Mean action noise std: 0.95
                       Mean reward: 6367.11
               Mean episode length: 336.81
                 Mean success rate: 66.50
                  Mean reward/step: 18.78
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.48s
                        Total time: 132.13s
                               ETA: 869.4s

################################################################################
                     [1m Learning iteration 264/2000 [0m

                       Computation: 17274 steps/s (collection: 0.255s, learning 0.219s)
               Value function loss: 65674.4994
                    Surrogate loss: -0.0017
             Mean action noise std: 0.95
                       Mean reward: 6345.67
               Mean episode length: 334.93
                 Mean success rate: 67.00
                  Mean reward/step: 19.16
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2170880
                    Iteration time: 0.47s
                        Total time: 132.61s
                               ETA: 868.7s

################################################################################
                     [1m Learning iteration 265/2000 [0m

                       Computation: 17280 steps/s (collection: 0.260s, learning 0.214s)
               Value function loss: 98492.0007
                    Surrogate loss: -0.0016
             Mean action noise std: 0.95
                       Mean reward: 6351.89
               Mean episode length: 334.31
                 Mean success rate: 66.00
                  Mean reward/step: 19.36
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 0.47s
                        Total time: 133.08s
                               ETA: 868.0s

################################################################################
                     [1m Learning iteration 266/2000 [0m

                       Computation: 16613 steps/s (collection: 0.286s, learning 0.207s)
               Value function loss: 65583.7314
                    Surrogate loss: -0.0032
             Mean action noise std: 0.95
                       Mean reward: 6389.21
               Mean episode length: 334.14
                 Mean success rate: 66.00
                  Mean reward/step: 18.56
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2187264
                    Iteration time: 0.49s
                        Total time: 133.58s
                               ETA: 867.5s

################################################################################
                     [1m Learning iteration 267/2000 [0m

                       Computation: 17025 steps/s (collection: 0.270s, learning 0.211s)
               Value function loss: 50185.9387
                    Surrogate loss: -0.0019
             Mean action noise std: 0.95
                       Mean reward: 6429.86
               Mean episode length: 330.36
                 Mean success rate: 66.00
                  Mean reward/step: 19.82
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 0.48s
                        Total time: 134.06s
                               ETA: 866.9s

################################################################################
                     [1m Learning iteration 268/2000 [0m

                       Computation: 16752 steps/s (collection: 0.291s, learning 0.198s)
               Value function loss: 56057.8819
                    Surrogate loss: -0.0021
             Mean action noise std: 0.95
                       Mean reward: 5920.46
               Mean episode length: 312.96
                 Mean success rate: 64.00
                  Mean reward/step: 21.29
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2203648
                    Iteration time: 0.49s
                        Total time: 134.55s
                               ETA: 866.3s

################################################################################
                     [1m Learning iteration 269/2000 [0m

                       Computation: 16190 steps/s (collection: 0.283s, learning 0.223s)
               Value function loss: 89326.8754
                    Surrogate loss: -0.0014
             Mean action noise std: 0.95
                       Mean reward: 6229.39
               Mean episode length: 323.48
                 Mean success rate: 67.50
                  Mean reward/step: 21.11
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 0.51s
                        Total time: 135.05s
                               ETA: 865.8s

################################################################################
                     [1m Learning iteration 270/2000 [0m

                       Computation: 16460 steps/s (collection: 0.274s, learning 0.223s)
               Value function loss: 82866.9326
                    Surrogate loss: -0.0020
             Mean action noise std: 0.95
                       Mean reward: 6405.49
               Mean episode length: 330.70
                 Mean success rate: 67.50
                  Mean reward/step: 20.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2220032
                    Iteration time: 0.50s
                        Total time: 135.55s
                               ETA: 865.3s

################################################################################
                     [1m Learning iteration 271/2000 [0m

                       Computation: 16517 steps/s (collection: 0.286s, learning 0.210s)
               Value function loss: 44798.8462
                    Surrogate loss: -0.0026
             Mean action noise std: 0.95
                       Mean reward: 6084.11
               Mean episode length: 320.31
                 Mean success rate: 66.00
                  Mean reward/step: 20.43
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 0.50s
                        Total time: 136.05s
                               ETA: 864.8s

################################################################################
                     [1m Learning iteration 272/2000 [0m

                       Computation: 16884 steps/s (collection: 0.277s, learning 0.208s)
               Value function loss: 88290.9951
                    Surrogate loss: -0.0021
             Mean action noise std: 0.95
                       Mean reward: 6048.20
               Mean episode length: 307.56
                 Mean success rate: 64.00
                  Mean reward/step: 20.14
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2236416
                    Iteration time: 0.49s
                        Total time: 136.53s
                               ETA: 864.2s

################################################################################
                     [1m Learning iteration 273/2000 [0m

                       Computation: 16420 steps/s (collection: 0.292s, learning 0.207s)
               Value function loss: 68511.2938
                    Surrogate loss: -0.0026
             Mean action noise std: 0.95
                       Mean reward: 5933.25
               Mean episode length: 303.92
                 Mean success rate: 65.00
                  Mean reward/step: 19.84
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 0.50s
                        Total time: 137.03s
                               ETA: 863.7s

################################################################################
                     [1m Learning iteration 274/2000 [0m

                       Computation: 16742 steps/s (collection: 0.278s, learning 0.212s)
               Value function loss: 93507.7626
                    Surrogate loss: -0.0017
             Mean action noise std: 0.95
                       Mean reward: 6291.69
               Mean episode length: 322.00
                 Mean success rate: 69.00
                  Mean reward/step: 19.62
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2252800
                    Iteration time: 0.49s
                        Total time: 137.52s
                               ETA: 863.1s

################################################################################
                     [1m Learning iteration 275/2000 [0m

                       Computation: 16933 steps/s (collection: 0.271s, learning 0.212s)
               Value function loss: 84982.6404
                    Surrogate loss: -0.0011
             Mean action noise std: 0.95
                       Mean reward: 6537.97
               Mean episode length: 331.34
                 Mean success rate: 70.00
                  Mean reward/step: 19.39
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.48s
                        Total time: 138.00s
                               ETA: 862.5s

################################################################################
                     [1m Learning iteration 276/2000 [0m

                       Computation: 16446 steps/s (collection: 0.277s, learning 0.221s)
               Value function loss: 61267.8730
                    Surrogate loss: -0.0022
             Mean action noise std: 0.95
                       Mean reward: 6510.63
               Mean episode length: 329.65
                 Mean success rate: 69.50
                  Mean reward/step: 19.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2269184
                    Iteration time: 0.50s
                        Total time: 138.50s
                               ETA: 862.0s

################################################################################
                     [1m Learning iteration 277/2000 [0m

                       Computation: 15777 steps/s (collection: 0.302s, learning 0.217s)
               Value function loss: 63516.7847
                    Surrogate loss: -0.0018
             Mean action noise std: 0.95
                       Mean reward: 6432.50
               Mean episode length: 333.61
                 Mean success rate: 68.00
                  Mean reward/step: 20.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 0.52s
                        Total time: 139.02s
                               ETA: 861.6s

################################################################################
                     [1m Learning iteration 278/2000 [0m

                       Computation: 15908 steps/s (collection: 0.293s, learning 0.222s)
               Value function loss: 95218.2768
                    Surrogate loss: -0.0013
             Mean action noise std: 0.95
                       Mean reward: 6621.84
               Mean episode length: 331.82
                 Mean success rate: 69.50
                  Mean reward/step: 19.91
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2285568
                    Iteration time: 0.51s
                        Total time: 139.54s
                               ETA: 861.2s

################################################################################
                     [1m Learning iteration 279/2000 [0m

                       Computation: 16882 steps/s (collection: 0.275s, learning 0.210s)
               Value function loss: 52092.9137
                    Surrogate loss: -0.0031
             Mean action noise std: 0.95
                       Mean reward: 6407.35
               Mean episode length: 325.39
                 Mean success rate: 68.00
                  Mean reward/step: 19.82
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 0.49s
                        Total time: 140.02s
                               ETA: 860.6s

################################################################################
                     [1m Learning iteration 280/2000 [0m

                       Computation: 15949 steps/s (collection: 0.284s, learning 0.230s)
               Value function loss: 95161.7264
                    Surrogate loss: -0.0024
             Mean action noise std: 0.95
                       Mean reward: 6547.41
               Mean episode length: 335.44
                 Mean success rate: 68.50
                  Mean reward/step: 20.71
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2301952
                    Iteration time: 0.51s
                        Total time: 140.53s
                               ETA: 860.2s

################################################################################
                     [1m Learning iteration 281/2000 [0m

                       Computation: 16910 steps/s (collection: 0.278s, learning 0.206s)
               Value function loss: 78776.5380
                    Surrogate loss: -0.0022
             Mean action noise std: 0.95
                       Mean reward: 6757.50
               Mean episode length: 341.42
                 Mean success rate: 68.50
                  Mean reward/step: 20.78
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 0.48s
                        Total time: 141.02s
                               ETA: 859.6s

################################################################################
                     [1m Learning iteration 282/2000 [0m

                       Computation: 16404 steps/s (collection: 0.283s, learning 0.217s)
               Value function loss: 66149.4071
                    Surrogate loss: -0.0021
             Mean action noise std: 0.95
                       Mean reward: 6716.10
               Mean episode length: 332.39
                 Mean success rate: 66.50
                  Mean reward/step: 21.10
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2318336
                    Iteration time: 0.50s
                        Total time: 141.52s
                               ETA: 859.1s

################################################################################
                     [1m Learning iteration 283/2000 [0m

                       Computation: 13780 steps/s (collection: 0.338s, learning 0.257s)
               Value function loss: 86182.8959
                    Surrogate loss: -0.0020
             Mean action noise std: 0.95
                       Mean reward: 6703.79
               Mean episode length: 331.44
                 Mean success rate: 65.00
                  Mean reward/step: 22.02
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 0.59s
                        Total time: 142.11s
                               ETA: 859.2s

################################################################################
                     [1m Learning iteration 284/2000 [0m

                       Computation: 15216 steps/s (collection: 0.322s, learning 0.216s)
               Value function loss: 71494.8573
                    Surrogate loss: -0.0029
             Mean action noise std: 0.95
                       Mean reward: 6201.34
               Mean episode length: 307.44
                 Mean success rate: 63.00
                  Mean reward/step: 21.87
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2334720
                    Iteration time: 0.54s
                        Total time: 142.65s
                               ETA: 858.9s

################################################################################
                     [1m Learning iteration 285/2000 [0m

                       Computation: 15280 steps/s (collection: 0.314s, learning 0.222s)
               Value function loss: 72507.0648
                    Surrogate loss: -0.0006
             Mean action noise std: 0.95
                       Mean reward: 6140.34
               Mean episode length: 307.05
                 Mean success rate: 61.50
                  Mean reward/step: 21.58
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 0.54s
                        Total time: 143.19s
                               ETA: 858.6s

################################################################################
                     [1m Learning iteration 286/2000 [0m

                       Computation: 16235 steps/s (collection: 0.284s, learning 0.221s)
               Value function loss: 64906.7094
                    Surrogate loss: -0.0018
             Mean action noise std: 0.95
                       Mean reward: 6297.73
               Mean episode length: 315.21
                 Mean success rate: 62.00
                  Mean reward/step: 21.83
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 2351104
                    Iteration time: 0.50s
                        Total time: 143.69s
                               ETA: 858.1s

################################################################################
                     [1m Learning iteration 287/2000 [0m

                       Computation: 16174 steps/s (collection: 0.288s, learning 0.218s)
               Value function loss: 90691.4203
                    Surrogate loss: -0.0008
             Mean action noise std: 0.95
                       Mean reward: 6588.03
               Mean episode length: 325.99
                 Mean success rate: 63.50
                  Mean reward/step: 22.07
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.51s
                        Total time: 144.20s
                               ETA: 857.7s

################################################################################
                     [1m Learning iteration 288/2000 [0m

                       Computation: 14874 steps/s (collection: 0.298s, learning 0.252s)
               Value function loss: 98304.5562
                    Surrogate loss: 0.0014
             Mean action noise std: 0.95
                       Mean reward: 6802.72
               Mean episode length: 331.82
                 Mean success rate: 65.50
                  Mean reward/step: 22.12
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2367488
                    Iteration time: 0.55s
                        Total time: 144.75s
                               ETA: 857.5s

################################################################################
                     [1m Learning iteration 289/2000 [0m

                       Computation: 16475 steps/s (collection: 0.280s, learning 0.217s)
               Value function loss: 102053.8383
                    Surrogate loss: -0.0017
             Mean action noise std: 0.95
                       Mean reward: 6857.33
               Mean episode length: 335.38
                 Mean success rate: 67.50
                  Mean reward/step: 21.58
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 0.50s
                        Total time: 145.25s
                               ETA: 857.0s

################################################################################
                     [1m Learning iteration 290/2000 [0m

                       Computation: 16422 steps/s (collection: 0.285s, learning 0.214s)
               Value function loss: 86340.2247
                    Surrogate loss: -0.0028
             Mean action noise std: 0.95
                       Mean reward: 7095.18
               Mean episode length: 345.19
                 Mean success rate: 69.00
                  Mean reward/step: 21.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2383872
                    Iteration time: 0.50s
                        Total time: 145.74s
                               ETA: 856.4s

################################################################################
                     [1m Learning iteration 291/2000 [0m

                       Computation: 14251 steps/s (collection: 0.347s, learning 0.228s)
               Value function loss: 83592.2656
                    Surrogate loss: -0.0029
             Mean action noise std: 0.95
                       Mean reward: 7485.61
               Mean episode length: 355.08
                 Mean success rate: 72.00
                  Mean reward/step: 20.81
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 0.57s
                        Total time: 146.32s
                               ETA: 856.4s

################################################################################
                     [1m Learning iteration 292/2000 [0m

                       Computation: 13894 steps/s (collection: 0.315s, learning 0.274s)
               Value function loss: 82598.9688
                    Surrogate loss: -0.0028
             Mean action noise std: 0.95
                       Mean reward: 7473.49
               Mean episode length: 352.55
                 Mean success rate: 70.00
                  Mean reward/step: 21.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2400256
                    Iteration time: 0.59s
                        Total time: 146.91s
                               ETA: 856.4s

################################################################################
                     [1m Learning iteration 293/2000 [0m

                       Computation: 12079 steps/s (collection: 0.370s, learning 0.308s)
               Value function loss: 91209.5854
                    Surrogate loss: -0.0020
             Mean action noise std: 0.95
                       Mean reward: 7880.50
               Mean episode length: 368.10
                 Mean success rate: 72.00
                  Mean reward/step: 22.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 0.68s
                        Total time: 147.59s
                               ETA: 856.9s

################################################################################
                     [1m Learning iteration 294/2000 [0m

                       Computation: 15248 steps/s (collection: 0.321s, learning 0.216s)
               Value function loss: 87666.3223
                    Surrogate loss: -0.0020
             Mean action noise std: 0.95
                       Mean reward: 8201.48
               Mean episode length: 376.11
                 Mean success rate: 75.00
                  Mean reward/step: 21.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2416640
                    Iteration time: 0.54s
                        Total time: 148.12s
                               ETA: 856.6s

################################################################################
                     [1m Learning iteration 295/2000 [0m

                       Computation: 15716 steps/s (collection: 0.309s, learning 0.212s)
               Value function loss: 74769.4382
                    Surrogate loss: -0.0019
             Mean action noise std: 0.95
                       Mean reward: 7863.55
               Mean episode length: 362.12
                 Mean success rate: 72.50
                  Mean reward/step: 21.97
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 0.52s
                        Total time: 148.65s
                               ETA: 856.2s

################################################################################
                     [1m Learning iteration 296/2000 [0m

                       Computation: 15377 steps/s (collection: 0.319s, learning 0.213s)
               Value function loss: 123029.0154
                    Surrogate loss: -0.0008
             Mean action noise std: 0.95
                       Mean reward: 7724.58
               Mean episode length: 355.67
                 Mean success rate: 72.50
                  Mean reward/step: 21.82
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2433024
                    Iteration time: 0.53s
                        Total time: 149.18s
                               ETA: 855.9s

################################################################################
                     [1m Learning iteration 297/2000 [0m

                       Computation: 15978 steps/s (collection: 0.289s, learning 0.224s)
               Value function loss: 83359.0192
                    Surrogate loss: -0.0016
             Mean action noise std: 0.95
                       Mean reward: 7688.88
               Mean episode length: 353.23
                 Mean success rate: 72.00
                  Mean reward/step: 21.10
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 0.51s
                        Total time: 149.69s
                               ETA: 855.4s

################################################################################
                     [1m Learning iteration 298/2000 [0m

                       Computation: 16756 steps/s (collection: 0.273s, learning 0.216s)
               Value function loss: 66737.5753
                    Surrogate loss: -0.0023
             Mean action noise std: 0.95
                       Mean reward: 7497.44
               Mean episode length: 343.63
                 Mean success rate: 70.50
                  Mean reward/step: 21.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2449408
                    Iteration time: 0.49s
                        Total time: 150.18s
                               ETA: 854.9s

################################################################################
                     [1m Learning iteration 299/2000 [0m

                       Computation: 16282 steps/s (collection: 0.277s, learning 0.226s)
               Value function loss: 93820.6049
                    Surrogate loss: -0.0025
             Mean action noise std: 0.95
                       Mean reward: 7274.55
               Mean episode length: 336.19
                 Mean success rate: 70.00
                  Mean reward/step: 21.77
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.50s
                        Total time: 150.68s
                               ETA: 854.4s

################################################################################
                     [1m Learning iteration 300/2000 [0m

                       Computation: 16484 steps/s (collection: 0.279s, learning 0.218s)
               Value function loss: 118356.7500
                    Surrogate loss: 0.0033
             Mean action noise std: 0.95
                       Mean reward: 7447.51
               Mean episode length: 347.31
                 Mean success rate: 72.50
                  Mean reward/step: 21.12
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2465792
                    Iteration time: 0.50s
                        Total time: 151.18s
                               ETA: 853.8s

################################################################################
                     [1m Learning iteration 301/2000 [0m

                       Computation: 16337 steps/s (collection: 0.278s, learning 0.223s)
               Value function loss: 77418.9902
                    Surrogate loss: -0.0008
             Mean action noise std: 0.95
                       Mean reward: 7471.88
               Mean episode length: 346.84
                 Mean success rate: 73.00
                  Mean reward/step: 21.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 0.50s
                        Total time: 151.68s
                               ETA: 853.3s

################################################################################
                     [1m Learning iteration 302/2000 [0m

                       Computation: 16841 steps/s (collection: 0.276s, learning 0.210s)
               Value function loss: 55769.7875
                    Surrogate loss: -0.0027
             Mean action noise std: 0.95
                       Mean reward: 6746.23
               Mean episode length: 323.69
                 Mean success rate: 69.50
                  Mean reward/step: 22.21
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2482176
                    Iteration time: 0.49s
                        Total time: 152.17s
                               ETA: 852.7s

################################################################################
                     [1m Learning iteration 303/2000 [0m

                       Computation: 16241 steps/s (collection: 0.279s, learning 0.226s)
               Value function loss: 99757.9160
                    Surrogate loss: -0.0024
             Mean action noise std: 0.95
                       Mean reward: 7057.65
               Mean episode length: 336.25
                 Mean success rate: 72.00
                  Mean reward/step: 22.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 0.50s
                        Total time: 152.67s
                               ETA: 852.3s

################################################################################
                     [1m Learning iteration 304/2000 [0m

                       Computation: 15609 steps/s (collection: 0.302s, learning 0.223s)
               Value function loss: 100493.2996
                    Surrogate loss: -0.0020
             Mean action noise std: 0.95
                       Mean reward: 7318.41
               Mean episode length: 343.61
                 Mean success rate: 73.50
                  Mean reward/step: 22.22
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2498560
                    Iteration time: 0.52s
                        Total time: 153.20s
                               ETA: 851.9s

################################################################################
                     [1m Learning iteration 305/2000 [0m

                       Computation: 17348 steps/s (collection: 0.254s, learning 0.218s)
               Value function loss: 108710.0763
                    Surrogate loss: -0.0024
             Mean action noise std: 0.95
                       Mean reward: 7654.22
               Mean episode length: 353.02
                 Mean success rate: 76.00
                  Mean reward/step: 21.75
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 0.47s
                        Total time: 153.67s
                               ETA: 851.2s

################################################################################
                     [1m Learning iteration 306/2000 [0m

                       Computation: 15331 steps/s (collection: 0.300s, learning 0.234s)
               Value function loss: 91235.4546
                    Surrogate loss: 0.0001
             Mean action noise std: 0.95
                       Mean reward: 7765.16
               Mean episode length: 354.51
                 Mean success rate: 76.50
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2514944
                    Iteration time: 0.53s
                        Total time: 154.20s
                               ETA: 850.9s

################################################################################
                     [1m Learning iteration 307/2000 [0m

                       Computation: 16245 steps/s (collection: 0.283s, learning 0.221s)
               Value function loss: 55448.2457
                    Surrogate loss: -0.0010
             Mean action noise std: 0.95
                       Mean reward: 7493.27
               Mean episode length: 349.79
                 Mean success rate: 76.00
                  Mean reward/step: 21.85
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 0.50s
                        Total time: 154.71s
                               ETA: 850.4s

################################################################################
                     [1m Learning iteration 308/2000 [0m

                       Computation: 16296 steps/s (collection: 0.281s, learning 0.222s)
               Value function loss: 55375.0733
                    Surrogate loss: -0.0018
             Mean action noise std: 0.95
                       Mean reward: 7824.81
               Mean episode length: 360.07
                 Mean success rate: 77.00
                  Mean reward/step: 22.36
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2531328
                    Iteration time: 0.50s
                        Total time: 155.21s
                               ETA: 849.9s

################################################################################
                     [1m Learning iteration 309/2000 [0m

                       Computation: 15708 steps/s (collection: 0.277s, learning 0.245s)
               Value function loss: 75646.1182
                    Surrogate loss: -0.0020
             Mean action noise std: 0.95
                       Mean reward: 7927.18
               Mean episode length: 362.29
                 Mean success rate: 76.50
                  Mean reward/step: 22.71
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 0.52s
                        Total time: 155.73s
                               ETA: 849.5s

################################################################################
                     [1m Learning iteration 310/2000 [0m

                       Computation: 16397 steps/s (collection: 0.285s, learning 0.214s)
               Value function loss: 83446.5200
                    Surrogate loss: -0.0010
             Mean action noise std: 0.95
                       Mean reward: 7800.20
               Mean episode length: 354.83
                 Mean success rate: 76.50
                  Mean reward/step: 23.01
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2547712
                    Iteration time: 0.50s
                        Total time: 156.23s
                               ETA: 849.0s

################################################################################
                     [1m Learning iteration 311/2000 [0m

                       Computation: 16042 steps/s (collection: 0.299s, learning 0.211s)
               Value function loss: 101515.8201
                    Surrogate loss: -0.0016
             Mean action noise std: 0.95
                       Mean reward: 8311.77
               Mean episode length: 366.78
                 Mean success rate: 79.00
                  Mean reward/step: 22.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.51s
                        Total time: 156.74s
                               ETA: 848.5s

################################################################################
                     [1m Learning iteration 312/2000 [0m

                       Computation: 15967 steps/s (collection: 0.301s, learning 0.212s)
               Value function loss: 109918.5824
                    Surrogate loss: -0.0006
             Mean action noise std: 0.95
                       Mean reward: 8093.87
               Mean episode length: 355.45
                 Mean success rate: 78.50
                  Mean reward/step: 21.25
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2564096
                    Iteration time: 0.51s
                        Total time: 157.26s
                               ETA: 848.1s

################################################################################
                     [1m Learning iteration 313/2000 [0m

                       Computation: 15010 steps/s (collection: 0.317s, learning 0.229s)
               Value function loss: 71282.6089
                    Surrogate loss: 0.0024
             Mean action noise std: 0.95
                       Mean reward: 8025.34
               Mean episode length: 354.38
                 Mean success rate: 78.50
                  Mean reward/step: 20.50
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 0.55s
                        Total time: 157.80s
                               ETA: 847.8s

################################################################################
                     [1m Learning iteration 314/2000 [0m

                       Computation: 16062 steps/s (collection: 0.294s, learning 0.216s)
               Value function loss: 58500.3905
                    Surrogate loss: -0.0005
             Mean action noise std: 0.95
                       Mean reward: 7888.82
               Mean episode length: 349.75
                 Mean success rate: 77.50
                  Mean reward/step: 20.86
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2580480
                    Iteration time: 0.51s
                        Total time: 158.31s
                               ETA: 847.3s

################################################################################
                     [1m Learning iteration 315/2000 [0m

                       Computation: 16506 steps/s (collection: 0.295s, learning 0.201s)
               Value function loss: 108408.3483
                    Surrogate loss: 0.0031
             Mean action noise std: 0.95
                       Mean reward: 8010.79
               Mean episode length: 353.85
                 Mean success rate: 77.50
                  Mean reward/step: 21.28
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 0.50s
                        Total time: 158.81s
                               ETA: 846.8s

################################################################################
                     [1m Learning iteration 316/2000 [0m

                       Computation: 16958 steps/s (collection: 0.277s, learning 0.207s)
               Value function loss: 75921.4631
                    Surrogate loss: 0.0049
             Mean action noise std: 0.95
                       Mean reward: 7814.72
               Mean episode length: 351.88
                 Mean success rate: 76.00
                  Mean reward/step: 20.79
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2596864
                    Iteration time: 0.48s
                        Total time: 159.29s
                               ETA: 846.2s

################################################################################
                     [1m Learning iteration 317/2000 [0m

                       Computation: 16613 steps/s (collection: 0.269s, learning 0.224s)
               Value function loss: 68443.2325
                    Surrogate loss: 0.0003
             Mean action noise std: 0.95
                       Mean reward: 7712.81
               Mean episode length: 353.33
                 Mean success rate: 76.00
                  Mean reward/step: 20.88
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 0.49s
                        Total time: 159.78s
                               ETA: 845.6s

################################################################################
                     [1m Learning iteration 318/2000 [0m

                       Computation: 16042 steps/s (collection: 0.287s, learning 0.223s)
               Value function loss: 74176.8644
                    Surrogate loss: -0.0031
             Mean action noise std: 0.95
                       Mean reward: 7792.35
               Mean episode length: 353.05
                 Mean success rate: 77.00
                  Mean reward/step: 21.49
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2613248
                    Iteration time: 0.51s
                        Total time: 160.29s
                               ETA: 845.2s

################################################################################
                     [1m Learning iteration 319/2000 [0m

                       Computation: 17353 steps/s (collection: 0.272s, learning 0.200s)
               Value function loss: 67917.1243
                    Surrogate loss: -0.0023
             Mean action noise std: 0.95
                       Mean reward: 7976.13
               Mean episode length: 365.88
                 Mean success rate: 78.50
                  Mean reward/step: 21.42
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 0.47s
                        Total time: 160.77s
                               ETA: 844.5s

################################################################################
                     [1m Learning iteration 320/2000 [0m

                       Computation: 18131 steps/s (collection: 0.252s, learning 0.200s)
               Value function loss: 69570.8333
                    Surrogate loss: 0.0002
             Mean action noise std: 0.95
                       Mean reward: 8070.82
               Mean episode length: 372.69
                 Mean success rate: 78.00
                  Mean reward/step: 21.86
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2629632
                    Iteration time: 0.45s
                        Total time: 161.22s
                               ETA: 843.8s

################################################################################
                     [1m Learning iteration 321/2000 [0m

                       Computation: 16947 steps/s (collection: 0.267s, learning 0.217s)
               Value function loss: 67985.8900
                    Surrogate loss: 0.0026
             Mean action noise std: 0.95
                       Mean reward: 8017.58
               Mean episode length: 376.57
                 Mean success rate: 77.50
                  Mean reward/step: 22.27
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 0.48s
                        Total time: 161.70s
                               ETA: 843.2s

################################################################################
                     [1m Learning iteration 322/2000 [0m

                       Computation: 16102 steps/s (collection: 0.300s, learning 0.208s)
               Value function loss: 105014.0792
                    Surrogate loss: 0.0015
             Mean action noise std: 0.95
                       Mean reward: 8176.50
               Mean episode length: 377.84
                 Mean success rate: 78.50
                  Mean reward/step: 21.75
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2646016
                    Iteration time: 0.51s
                        Total time: 162.21s
                               ETA: 842.7s

################################################################################
                     [1m Learning iteration 323/2000 [0m

                       Computation: 16891 steps/s (collection: 0.282s, learning 0.203s)
               Value function loss: 65192.8629
                    Surrogate loss: 0.0003
             Mean action noise std: 0.94
                       Mean reward: 7685.31
               Mean episode length: 371.69
                 Mean success rate: 75.50
                  Mean reward/step: 21.60
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.48s
                        Total time: 162.70s
                               ETA: 842.1s

################################################################################
                     [1m Learning iteration 324/2000 [0m

                       Computation: 18030 steps/s (collection: 0.257s, learning 0.197s)
               Value function loss: 67809.4016
                    Surrogate loss: 0.0024
             Mean action noise std: 0.94
                       Mean reward: 7600.48
               Mean episode length: 364.90
                 Mean success rate: 75.00
                  Mean reward/step: 22.33
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2662400
                    Iteration time: 0.45s
                        Total time: 163.15s
                               ETA: 841.3s

################################################################################
                     [1m Learning iteration 325/2000 [0m

                       Computation: 17469 steps/s (collection: 0.265s, learning 0.204s)
               Value function loss: 77334.0762
                    Surrogate loss: -0.0031
             Mean action noise std: 0.94
                       Mean reward: 7374.87
               Mean episode length: 348.74
                 Mean success rate: 74.00
                  Mean reward/step: 23.01
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 0.47s
                        Total time: 163.62s
                               ETA: 840.7s

################################################################################
                     [1m Learning iteration 326/2000 [0m

                       Computation: 16747 steps/s (collection: 0.282s, learning 0.207s)
               Value function loss: 90603.4132
                    Surrogate loss: 0.0045
             Mean action noise std: 0.94
                       Mean reward: 7281.05
               Mean episode length: 343.21
                 Mean success rate: 75.00
                  Mean reward/step: 23.04
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2678784
                    Iteration time: 0.49s
                        Total time: 164.11s
                               ETA: 840.1s

################################################################################
                     [1m Learning iteration 327/2000 [0m

                       Computation: 17134 steps/s (collection: 0.271s, learning 0.208s)
               Value function loss: 92879.5891
                    Surrogate loss: 0.0088
             Mean action noise std: 0.94
                       Mean reward: 7287.89
               Mean episode length: 350.01
                 Mean success rate: 74.00
                  Mean reward/step: 21.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 0.48s
                        Total time: 164.59s
                               ETA: 839.5s

################################################################################
                     [1m Learning iteration 328/2000 [0m

                       Computation: 17247 steps/s (collection: 0.271s, learning 0.204s)
               Value function loss: 83613.5344
                    Surrogate loss: 0.0151
             Mean action noise std: 0.94
                       Mean reward: 7057.92
               Mean episode length: 341.37
                 Mean success rate: 71.50
                  Mean reward/step: 21.92
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2695168
                    Iteration time: 0.47s
                        Total time: 165.06s
                               ETA: 838.8s

################################################################################
                     [1m Learning iteration 329/2000 [0m

                       Computation: 17300 steps/s (collection: 0.272s, learning 0.201s)
               Value function loss: 74432.5643
                    Surrogate loss: -0.0025
             Mean action noise std: 0.94
                       Mean reward: 7223.36
               Mean episode length: 343.86
                 Mean success rate: 73.00
                  Mean reward/step: 22.27
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 0.47s
                        Total time: 165.53s
                               ETA: 838.2s

################################################################################
                     [1m Learning iteration 330/2000 [0m

                       Computation: 14635 steps/s (collection: 0.289s, learning 0.271s)
               Value function loss: 90640.7793
                    Surrogate loss: -0.0019
             Mean action noise std: 0.94
                       Mean reward: 7155.10
               Mean episode length: 341.25
                 Mean success rate: 71.50
                  Mean reward/step: 22.13
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2711552
                    Iteration time: 0.56s
                        Total time: 166.09s
                               ETA: 838.0s

################################################################################
                     [1m Learning iteration 331/2000 [0m

                       Computation: 16133 steps/s (collection: 0.289s, learning 0.219s)
               Value function loss: 129895.7450
                    Surrogate loss: 0.0003
             Mean action noise std: 0.94
                       Mean reward: 7637.77
               Mean episode length: 352.81
                 Mean success rate: 74.00
                  Mean reward/step: 21.47
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 0.51s
                        Total time: 166.60s
                               ETA: 837.5s

################################################################################
                     [1m Learning iteration 332/2000 [0m

                       Computation: 16882 steps/s (collection: 0.272s, learning 0.213s)
               Value function loss: 83754.1168
                    Surrogate loss: 0.0151
             Mean action noise std: 0.94
                       Mean reward: 7591.61
               Mean episode length: 352.48
                 Mean success rate: 74.00
                  Mean reward/step: 20.61
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2727936
                    Iteration time: 0.49s
                        Total time: 167.09s
                               ETA: 836.9s

################################################################################
                     [1m Learning iteration 333/2000 [0m

                       Computation: 16332 steps/s (collection: 0.285s, learning 0.216s)
               Value function loss: 78402.1321
                    Surrogate loss: 0.0001
             Mean action noise std: 0.94
                       Mean reward: 7608.82
               Mean episode length: 353.81
                 Mean success rate: 73.50
                  Mean reward/step: 20.67
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 0.50s
                        Total time: 167.59s
                               ETA: 836.4s

################################################################################
                     [1m Learning iteration 334/2000 [0m

                       Computation: 16224 steps/s (collection: 0.278s, learning 0.227s)
               Value function loss: 96365.7984
                    Surrogate loss: -0.0002
             Mean action noise std: 0.94
                       Mean reward: 7900.67
               Mean episode length: 358.76
                 Mean success rate: 74.50
                  Mean reward/step: 21.12
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2744320
                    Iteration time: 0.50s
                        Total time: 168.09s
                               ETA: 836.0s

################################################################################
                     [1m Learning iteration 335/2000 [0m

                       Computation: 16459 steps/s (collection: 0.280s, learning 0.217s)
               Value function loss: 75209.7542
                    Surrogate loss: 0.0190
             Mean action noise std: 0.94
                       Mean reward: 7594.51
               Mean episode length: 345.72
                 Mean success rate: 74.00
                  Mean reward/step: 21.64
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.50s
                        Total time: 168.59s
                               ETA: 835.4s

################################################################################
                     [1m Learning iteration 336/2000 [0m

                       Computation: 16061 steps/s (collection: 0.292s, learning 0.218s)
               Value function loss: 78306.1337
                    Surrogate loss: 0.0017
             Mean action noise std: 0.94
                       Mean reward: 7285.83
               Mean episode length: 333.37
                 Mean success rate: 73.00
                  Mean reward/step: 21.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2760704
                    Iteration time: 0.51s
                        Total time: 169.10s
                               ETA: 835.0s

################################################################################
                     [1m Learning iteration 337/2000 [0m

                       Computation: 16299 steps/s (collection: 0.292s, learning 0.210s)
               Value function loss: 75164.5724
                    Surrogate loss: 0.0024
             Mean action noise std: 0.94
                       Mean reward: 7125.27
               Mean episode length: 329.21
                 Mean success rate: 73.00
                  Mean reward/step: 21.58
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 0.50s
                        Total time: 169.60s
                               ETA: 834.5s

################################################################################
                     [1m Learning iteration 338/2000 [0m

                       Computation: 16607 steps/s (collection: 0.268s, learning 0.225s)
               Value function loss: 71616.0015
                    Surrogate loss: 0.0016
             Mean action noise std: 0.94
                       Mean reward: 7003.36
               Mean episode length: 322.67
                 Mean success rate: 72.50
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2777088
                    Iteration time: 0.49s
                        Total time: 170.10s
                               ETA: 833.9s

################################################################################
                     [1m Learning iteration 339/2000 [0m

                       Computation: 17108 steps/s (collection: 0.268s, learning 0.211s)
               Value function loss: 67043.4906
                    Surrogate loss: 0.0038
             Mean action noise std: 0.94
                       Mean reward: 6837.79
               Mean episode length: 316.19
                 Mean success rate: 71.50
                  Mean reward/step: 21.82
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 0.48s
                        Total time: 170.58s
                               ETA: 833.3s

################################################################################
                     [1m Learning iteration 340/2000 [0m

                       Computation: 16559 steps/s (collection: 0.283s, learning 0.212s)
               Value function loss: 81939.6057
                    Surrogate loss: 0.0076
             Mean action noise std: 0.94
                       Mean reward: 6842.37
               Mean episode length: 319.60
                 Mean success rate: 71.00
                  Mean reward/step: 22.18
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2793472
                    Iteration time: 0.49s
                        Total time: 171.07s
                               ETA: 832.8s

################################################################################
                     [1m Learning iteration 341/2000 [0m

                       Computation: 17102 steps/s (collection: 0.276s, learning 0.203s)
               Value function loss: 121750.1797
                    Surrogate loss: -0.0012
             Mean action noise std: 0.94
                       Mean reward: 6947.69
               Mean episode length: 321.60
                 Mean success rate: 72.50
                  Mean reward/step: 22.45
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 0.48s
                        Total time: 171.55s
                               ETA: 832.2s

################################################################################
                     [1m Learning iteration 342/2000 [0m

                       Computation: 16755 steps/s (collection: 0.284s, learning 0.205s)
               Value function loss: 116899.6430
                    Surrogate loss: -0.0021
             Mean action noise std: 0.94
                       Mean reward: 6873.91
               Mean episode length: 317.06
                 Mean success rate: 73.50
                  Mean reward/step: 21.49
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2809856
                    Iteration time: 0.49s
                        Total time: 172.04s
                               ETA: 831.6s

################################################################################
                     [1m Learning iteration 343/2000 [0m

                       Computation: 17220 steps/s (collection: 0.272s, learning 0.203s)
               Value function loss: 78178.2332
                    Surrogate loss: 0.0037
             Mean action noise std: 0.94
                       Mean reward: 7053.79
               Mean episode length: 323.71
                 Mean success rate: 74.50
                  Mean reward/step: 20.90
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 0.48s
                        Total time: 172.51s
                               ETA: 831.0s

################################################################################
                     [1m Learning iteration 344/2000 [0m

                       Computation: 16912 steps/s (collection: 0.271s, learning 0.213s)
               Value function loss: 89757.2066
                    Surrogate loss: 0.0064
             Mean action noise std: 0.94
                       Mean reward: 7013.88
               Mean episode length: 322.58
                 Mean success rate: 73.50
                  Mean reward/step: 21.76
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2826240
                    Iteration time: 0.48s
                        Total time: 173.00s
                               ETA: 830.4s

################################################################################
                     [1m Learning iteration 345/2000 [0m

                       Computation: 17552 steps/s (collection: 0.257s, learning 0.210s)
               Value function loss: 87666.7345
                    Surrogate loss: 0.0013
             Mean action noise std: 0.94
                       Mean reward: 7166.52
               Mean episode length: 329.95
                 Mean success rate: 74.50
                  Mean reward/step: 22.37
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 0.47s
                        Total time: 173.47s
                               ETA: 829.7s

################################################################################
                     [1m Learning iteration 346/2000 [0m

                       Computation: 15978 steps/s (collection: 0.292s, learning 0.221s)
               Value function loss: 130097.7914
                    Surrogate loss: 0.0004
             Mean action noise std: 0.94
                       Mean reward: 6684.50
               Mean episode length: 315.85
                 Mean success rate: 73.50
                  Mean reward/step: 22.09
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2842624
                    Iteration time: 0.51s
                        Total time: 173.98s
                               ETA: 829.3s

################################################################################
                     [1m Learning iteration 347/2000 [0m

                       Computation: 16500 steps/s (collection: 0.297s, learning 0.200s)
               Value function loss: 97045.0171
                    Surrogate loss: 0.0001
             Mean action noise std: 0.94
                       Mean reward: 6394.14
               Mean episode length: 305.15
                 Mean success rate: 73.00
                  Mean reward/step: 21.26
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.50s
                        Total time: 174.47s
                               ETA: 828.8s

################################################################################
                     [1m Learning iteration 348/2000 [0m

                       Computation: 16561 steps/s (collection: 0.287s, learning 0.207s)
               Value function loss: 85759.4535
                    Surrogate loss: -0.0003
             Mean action noise std: 0.94
                       Mean reward: 6542.66
               Mean episode length: 309.24
                 Mean success rate: 73.50
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2859008
                    Iteration time: 0.49s
                        Total time: 174.97s
                               ETA: 828.2s

################################################################################
                     [1m Learning iteration 349/2000 [0m

                       Computation: 16668 steps/s (collection: 0.286s, learning 0.205s)
               Value function loss: 93976.1147
                    Surrogate loss: -0.0026
             Mean action noise std: 0.94
                       Mean reward: 6613.89
               Mean episode length: 312.79
                 Mean success rate: 73.50
                  Mean reward/step: 22.87
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 0.49s
                        Total time: 175.46s
                               ETA: 827.7s

################################################################################
                     [1m Learning iteration 350/2000 [0m

                       Computation: 17350 steps/s (collection: 0.273s, learning 0.199s)
               Value function loss: 107429.5434
                    Surrogate loss: -0.0028
             Mean action noise std: 0.94
                       Mean reward: 6641.27
               Mean episode length: 313.83
                 Mean success rate: 74.00
                  Mean reward/step: 23.45
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2875392
                    Iteration time: 0.47s
                        Total time: 175.93s
                               ETA: 827.0s

################################################################################
                     [1m Learning iteration 351/2000 [0m

                       Computation: 16696 steps/s (collection: 0.278s, learning 0.213s)
               Value function loss: 95802.8739
                    Surrogate loss: -0.0021
             Mean action noise std: 0.94
                       Mean reward: 6641.77
               Mean episode length: 307.68
                 Mean success rate: 74.50
                  Mean reward/step: 23.20
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 0.49s
                        Total time: 176.42s
                               ETA: 826.5s

################################################################################
                     [1m Learning iteration 352/2000 [0m

                       Computation: 16875 steps/s (collection: 0.283s, learning 0.203s)
               Value function loss: 98125.8557
                    Surrogate loss: -0.0025
             Mean action noise std: 0.94
                       Mean reward: 6393.67
               Mean episode length: 301.88
                 Mean success rate: 73.00
                  Mean reward/step: 22.84
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2891776
                    Iteration time: 0.49s
                        Total time: 176.91s
                               ETA: 825.9s

################################################################################
                     [1m Learning iteration 353/2000 [0m

                       Computation: 16366 steps/s (collection: 0.292s, learning 0.208s)
               Value function loss: 70722.9673
                    Surrogate loss: -0.0036
             Mean action noise std: 0.94
                       Mean reward: 6529.08
               Mean episode length: 302.81
                 Mean success rate: 72.50
                  Mean reward/step: 22.18
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 0.50s
                        Total time: 177.41s
                               ETA: 825.4s

################################################################################
                     [1m Learning iteration 354/2000 [0m

                       Computation: 17352 steps/s (collection: 0.270s, learning 0.202s)
               Value function loss: 93448.1206
                    Surrogate loss: 0.0022
             Mean action noise std: 0.94
                       Mean reward: 6774.50
               Mean episode length: 307.45
                 Mean success rate: 73.50
                  Mean reward/step: 22.36
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2908160
                    Iteration time: 0.47s
                        Total time: 177.88s
                               ETA: 824.8s

################################################################################
                     [1m Learning iteration 355/2000 [0m

                       Computation: 17126 steps/s (collection: 0.277s, learning 0.202s)
               Value function loss: 83765.5271
                    Surrogate loss: 0.0039
             Mean action noise std: 0.94
                       Mean reward: 6394.57
               Mean episode length: 287.38
                 Mean success rate: 72.50
                  Mean reward/step: 22.63
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 0.48s
                        Total time: 178.36s
                               ETA: 824.2s

################################################################################
                     [1m Learning iteration 356/2000 [0m

                       Computation: 16951 steps/s (collection: 0.275s, learning 0.208s)
               Value function loss: 82881.0408
                    Surrogate loss: -0.0004
             Mean action noise std: 0.94
                       Mean reward: 6029.59
               Mean episode length: 271.85
                 Mean success rate: 70.50
                  Mean reward/step: 22.52
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2924544
                    Iteration time: 0.48s
                        Total time: 178.84s
                               ETA: 823.6s

################################################################################
                     [1m Learning iteration 357/2000 [0m

                       Computation: 17380 steps/s (collection: 0.268s, learning 0.203s)
               Value function loss: 133284.2375
                    Surrogate loss: 0.0011
             Mean action noise std: 0.94
                       Mean reward: 6528.57
               Mean episode length: 284.96
                 Mean success rate: 72.00
                  Mean reward/step: 22.08
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 0.47s
                        Total time: 179.31s
                               ETA: 822.9s

################################################################################
                     [1m Learning iteration 358/2000 [0m

                       Computation: 16680 steps/s (collection: 0.280s, learning 0.211s)
               Value function loss: 65346.7034
                    Surrogate loss: 0.0056
             Mean action noise std: 0.94
                       Mean reward: 6520.08
               Mean episode length: 281.72
                 Mean success rate: 72.50
                  Mean reward/step: 21.92
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2940928
                    Iteration time: 0.49s
                        Total time: 179.81s
                               ETA: 822.4s

################################################################################
                     [1m Learning iteration 359/2000 [0m

                       Computation: 16313 steps/s (collection: 0.282s, learning 0.220s)
               Value function loss: 78025.1011
                    Surrogate loss: 0.0054
             Mean action noise std: 0.94
                       Mean reward: 6797.75
               Mean episode length: 290.07
                 Mean success rate: 74.50
                  Mean reward/step: 23.03
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.50s
                        Total time: 180.31s
                               ETA: 821.9s

################################################################################
                     [1m Learning iteration 360/2000 [0m

                       Computation: 15692 steps/s (collection: 0.300s, learning 0.222s)
               Value function loss: 70302.4038
                    Surrogate loss: 0.0015
             Mean action noise std: 0.94
                       Mean reward: 6826.90
               Mean episode length: 289.96
                 Mean success rate: 75.00
                  Mean reward/step: 24.40
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 2957312
                    Iteration time: 0.52s
                        Total time: 180.83s
                               ETA: 821.5s

################################################################################
                     [1m Learning iteration 361/2000 [0m

                       Computation: 16568 steps/s (collection: 0.291s, learning 0.204s)
               Value function loss: 106520.0553
                    Surrogate loss: 0.0153
             Mean action noise std: 0.94
                       Mean reward: 6598.31
               Mean episode length: 284.39
                 Mean success rate: 74.00
                  Mean reward/step: 24.30
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 0.49s
                        Total time: 181.32s
                               ETA: 821.0s

################################################################################
                     [1m Learning iteration 362/2000 [0m

                       Computation: 16334 steps/s (collection: 0.295s, learning 0.207s)
               Value function loss: 108557.7604
                    Surrogate loss: 0.0023
             Mean action noise std: 0.94
                       Mean reward: 6925.48
               Mean episode length: 300.36
                 Mean success rate: 74.00
                  Mean reward/step: 21.90
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2973696
                    Iteration time: 0.50s
                        Total time: 181.83s
                               ETA: 820.5s

################################################################################
                     [1m Learning iteration 363/2000 [0m

                       Computation: 14579 steps/s (collection: 0.317s, learning 0.245s)
               Value function loss: 126836.4186
                    Surrogate loss: 0.0284
             Mean action noise std: 0.94
                       Mean reward: 7175.82
               Mean episode length: 308.68
                 Mean success rate: 76.00
                  Mean reward/step: 20.57
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 0.56s
                        Total time: 182.39s
                               ETA: 820.2s

################################################################################
                     [1m Learning iteration 364/2000 [0m

                       Computation: 16093 steps/s (collection: 0.294s, learning 0.215s)
               Value function loss: 119467.9047
                    Surrogate loss: 0.0087
             Mean action noise std: 0.94
                       Mean reward: 6767.04
               Mean episode length: 294.88
                 Mean success rate: 74.00
                  Mean reward/step: 20.50
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2990080
                    Iteration time: 0.51s
                        Total time: 182.90s
                               ETA: 819.8s

################################################################################
                     [1m Learning iteration 365/2000 [0m

                       Computation: 14798 steps/s (collection: 0.297s, learning 0.256s)
               Value function loss: 113185.0213
                    Surrogate loss: 0.0229
             Mean action noise std: 0.94
                       Mean reward: 6278.95
               Mean episode length: 282.52
                 Mean success rate: 72.50
                  Mean reward/step: 20.22
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 0.55s
                        Total time: 183.45s
                               ETA: 819.5s

################################################################################
                     [1m Learning iteration 366/2000 [0m

                       Computation: 15378 steps/s (collection: 0.296s, learning 0.236s)
               Value function loss: 131541.3582
                    Surrogate loss: -0.0002
             Mean action noise std: 0.94
                       Mean reward: 6157.07
               Mean episode length: 277.39
                 Mean success rate: 72.50
                  Mean reward/step: 19.53
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3006464
                    Iteration time: 0.53s
                        Total time: 183.98s
                               ETA: 819.1s

################################################################################
                     [1m Learning iteration 367/2000 [0m

                       Computation: 16081 steps/s (collection: 0.278s, learning 0.231s)
               Value function loss: 90364.4175
                    Surrogate loss: -0.0002
             Mean action noise std: 0.94
                       Mean reward: 6048.12
               Mean episode length: 280.95
                 Mean success rate: 73.50
                  Mean reward/step: 19.18
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 0.51s
                        Total time: 184.49s
                               ETA: 818.7s

################################################################################
                     [1m Learning iteration 368/2000 [0m

                       Computation: 16859 steps/s (collection: 0.281s, learning 0.204s)
               Value function loss: 126552.1581
                    Surrogate loss: 0.0093
             Mean action noise std: 0.94
                       Mean reward: 5655.09
               Mean episode length: 267.48
                 Mean success rate: 71.50
                  Mean reward/step: 19.55
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3022848
                    Iteration time: 0.49s
                        Total time: 184.98s
                               ETA: 818.1s

################################################################################
                     [1m Learning iteration 369/2000 [0m

                       Computation: 16844 steps/s (collection: 0.273s, learning 0.213s)
               Value function loss: 75112.6167
                    Surrogate loss: 0.0056
             Mean action noise std: 0.94
                       Mean reward: 5203.80
               Mean episode length: 256.48
                 Mean success rate: 69.00
                  Mean reward/step: 19.46
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 0.49s
                        Total time: 185.46s
                               ETA: 817.5s

################################################################################
                     [1m Learning iteration 370/2000 [0m

                       Computation: 16536 steps/s (collection: 0.283s, learning 0.213s)
               Value function loss: 75618.9766
                    Surrogate loss: 0.0012
             Mean action noise std: 0.94
                       Mean reward: 4904.79
               Mean episode length: 252.24
                 Mean success rate: 68.00
                  Mean reward/step: 19.82
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3039232
                    Iteration time: 0.50s
                        Total time: 185.96s
                               ETA: 817.0s

################################################################################
                     [1m Learning iteration 371/2000 [0m

                       Computation: 16980 steps/s (collection: 0.280s, learning 0.203s)
               Value function loss: 119049.3898
                    Surrogate loss: -0.0020
             Mean action noise std: 0.94
                       Mean reward: 5213.83
               Mean episode length: 262.76
                 Mean success rate: 70.50
                  Mean reward/step: 19.37
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.48s
                        Total time: 186.44s
                               ETA: 816.4s

################################################################################
                     [1m Learning iteration 372/2000 [0m

                       Computation: 16765 steps/s (collection: 0.279s, learning 0.209s)
               Value function loss: 77760.1573
                    Surrogate loss: 0.0008
             Mean action noise std: 0.94
                       Mean reward: 5241.86
               Mean episode length: 259.75
                 Mean success rate: 70.50
                  Mean reward/step: 19.64
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3055616
                    Iteration time: 0.49s
                        Total time: 186.93s
                               ETA: 815.9s

################################################################################
                     [1m Learning iteration 373/2000 [0m

                       Computation: 16490 steps/s (collection: 0.288s, learning 0.209s)
               Value function loss: 110233.5840
                    Surrogate loss: 0.0486
             Mean action noise std: 0.94
                       Mean reward: 5582.90
               Mean episode length: 268.18
                 Mean success rate: 71.00
                  Mean reward/step: 18.39
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 0.50s
                        Total time: 187.43s
                               ETA: 815.4s

################################################################################
                     [1m Learning iteration 374/2000 [0m

                       Computation: 17261 steps/s (collection: 0.268s, learning 0.207s)
               Value function loss: 70916.8159
                    Surrogate loss: 0.0069
             Mean action noise std: 0.94
                       Mean reward: 5619.07
               Mean episode length: 267.11
                 Mean success rate: 71.50
                  Mean reward/step: 16.18
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3072000
                    Iteration time: 0.47s
                        Total time: 187.90s
                               ETA: 814.7s

################################################################################
                     [1m Learning iteration 375/2000 [0m

                       Computation: 17300 steps/s (collection: 0.268s, learning 0.205s)
               Value function loss: 35710.3223
                    Surrogate loss: -0.0026
             Mean action noise std: 0.94
                       Mean reward: 5497.48
               Mean episode length: 263.02
                 Mean success rate: 70.50
                  Mean reward/step: 16.95
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 0.47s
                        Total time: 188.38s
                               ETA: 814.1s

################################################################################
                     [1m Learning iteration 376/2000 [0m

                       Computation: 17394 steps/s (collection: 0.264s, learning 0.207s)
               Value function loss: 41861.8284
                    Surrogate loss: 0.0109
             Mean action noise std: 0.94
                       Mean reward: 5501.30
               Mean episode length: 266.62
                 Mean success rate: 70.50
                  Mean reward/step: 18.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3088384
                    Iteration time: 0.47s
                        Total time: 188.85s
                               ETA: 813.5s

################################################################################
                     [1m Learning iteration 377/2000 [0m

                       Computation: 17650 steps/s (collection: 0.264s, learning 0.201s)
               Value function loss: 45176.0099
                    Surrogate loss: 0.0125
             Mean action noise std: 0.94
                       Mean reward: 5443.78
               Mean episode length: 265.05
                 Mean success rate: 69.50
                  Mean reward/step: 18.41
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 0.46s
                        Total time: 189.31s
                               ETA: 812.8s

################################################################################
                     [1m Learning iteration 378/2000 [0m

                       Computation: 17161 steps/s (collection: 0.266s, learning 0.211s)
               Value function loss: 61702.3054
                    Surrogate loss: 0.0476
             Mean action noise std: 0.94
                       Mean reward: 5915.76
               Mean episode length: 287.75
                 Mean success rate: 71.00
                  Mean reward/step: 18.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3104768
                    Iteration time: 0.48s
                        Total time: 189.79s
                               ETA: 812.2s

################################################################################
                     [1m Learning iteration 379/2000 [0m

                       Computation: 16149 steps/s (collection: 0.298s, learning 0.210s)
               Value function loss: 41354.5151
                    Surrogate loss: 0.0208
             Mean action noise std: 0.94
                       Mean reward: 5736.17
               Mean episode length: 286.24
                 Mean success rate: 68.00
                  Mean reward/step: 18.96
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 0.51s
                        Total time: 190.30s
                               ETA: 811.8s

################################################################################
                     [1m Learning iteration 380/2000 [0m

                       Computation: 16592 steps/s (collection: 0.265s, learning 0.228s)
               Value function loss: 53392.0267
                    Surrogate loss: 0.0061
             Mean action noise std: 0.94
                       Mean reward: 5789.39
               Mean episode length: 297.16
                 Mean success rate: 67.50
                  Mean reward/step: 18.67
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3121152
                    Iteration time: 0.49s
                        Total time: 190.79s
                               ETA: 811.2s

################################################################################
                     [1m Learning iteration 381/2000 [0m

                       Computation: 16523 steps/s (collection: 0.289s, learning 0.207s)
               Value function loss: 61339.5276
                    Surrogate loss: 0.0049
             Mean action noise std: 0.94
                       Mean reward: 5576.13
               Mean episode length: 290.49
                 Mean success rate: 65.50
                  Mean reward/step: 18.60
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 0.50s
                        Total time: 191.28s
                               ETA: 810.7s

################################################################################
                     [1m Learning iteration 382/2000 [0m

                       Computation: 16265 steps/s (collection: 0.284s, learning 0.219s)
               Value function loss: 75111.5524
                    Surrogate loss: 0.0113
             Mean action noise std: 0.93
                       Mean reward: 5462.38
               Mean episode length: 293.21
                 Mean success rate: 65.00
                  Mean reward/step: 17.89
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3137536
                    Iteration time: 0.50s
                        Total time: 191.79s
                               ETA: 810.2s

################################################################################
                     [1m Learning iteration 383/2000 [0m

                       Computation: 16552 steps/s (collection: 0.283s, learning 0.212s)
               Value function loss: 53690.9793
                    Surrogate loss: 0.0017
             Mean action noise std: 0.93
                       Mean reward: 5574.61
               Mean episode length: 297.87
                 Mean success rate: 66.00
                  Mean reward/step: 16.08
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.49s
                        Total time: 192.28s
                               ETA: 809.7s

################################################################################
                     [1m Learning iteration 384/2000 [0m

                       Computation: 16581 steps/s (collection: 0.277s, learning 0.217s)
               Value function loss: 52149.4247
                    Surrogate loss: -0.0042
             Mean action noise std: 0.93
                       Mean reward: 5089.34
               Mean episode length: 272.94
                 Mean success rate: 63.00
                  Mean reward/step: 15.98
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3153920
                    Iteration time: 0.49s
                        Total time: 192.78s
                               ETA: 809.2s

################################################################################
                     [1m Learning iteration 385/2000 [0m

                       Computation: 15463 steps/s (collection: 0.305s, learning 0.225s)
               Value function loss: 51417.5655
                    Surrogate loss: -0.0027
             Mean action noise std: 0.93
                       Mean reward: 4937.35
               Mean episode length: 265.58
                 Mean success rate: 62.00
                  Mean reward/step: 16.08
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 0.53s
                        Total time: 193.31s
                               ETA: 808.8s

################################################################################
                     [1m Learning iteration 386/2000 [0m

                       Computation: 15674 steps/s (collection: 0.310s, learning 0.213s)
               Value function loss: 42122.7280
                    Surrogate loss: 0.0056
             Mean action noise std: 0.93
                       Mean reward: 4992.44
               Mean episode length: 265.65
                 Mean success rate: 63.50
                  Mean reward/step: 16.54
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3170304
                    Iteration time: 0.52s
                        Total time: 193.83s
                               ETA: 808.4s

################################################################################
                     [1m Learning iteration 387/2000 [0m

                       Computation: 15695 steps/s (collection: 0.305s, learning 0.217s)
               Value function loss: 40848.5003
                    Surrogate loss: -0.0025
             Mean action noise std: 0.93
                       Mean reward: 4891.02
               Mean episode length: 270.01
                 Mean success rate: 62.00
                  Mean reward/step: 16.47
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 0.52s
                        Total time: 194.35s
                               ETA: 808.0s

################################################################################
                     [1m Learning iteration 388/2000 [0m

                       Computation: 15966 steps/s (collection: 0.291s, learning 0.222s)
               Value function loss: 43504.5641
                    Surrogate loss: 0.0005
             Mean action noise std: 0.93
                       Mean reward: 4980.69
               Mean episode length: 272.77
                 Mean success rate: 62.00
                  Mean reward/step: 16.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3186688
                    Iteration time: 0.51s
                        Total time: 194.86s
                               ETA: 807.5s

################################################################################
                     [1m Learning iteration 389/2000 [0m

                       Computation: 16133 steps/s (collection: 0.289s, learning 0.219s)
               Value function loss: 49484.8537
                    Surrogate loss: 0.0052
             Mean action noise std: 0.93
                       Mean reward: 5185.11
               Mean episode length: 282.33
                 Mean success rate: 63.00
                  Mean reward/step: 16.30
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 0.51s
                        Total time: 195.37s
                               ETA: 807.0s

################################################################################
                     [1m Learning iteration 390/2000 [0m

                       Computation: 16071 steps/s (collection: 0.287s, learning 0.222s)
               Value function loss: 29248.6770
                    Surrogate loss: 0.0083
             Mean action noise std: 0.93
                       Mean reward: 5174.35
               Mean episode length: 280.23
                 Mean success rate: 61.00
                  Mean reward/step: 16.19
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3203072
                    Iteration time: 0.51s
                        Total time: 195.88s
                               ETA: 806.6s

################################################################################
                     [1m Learning iteration 391/2000 [0m

                       Computation: 16764 steps/s (collection: 0.270s, learning 0.218s)
               Value function loss: 29305.6122
                    Surrogate loss: 0.0021
             Mean action noise std: 0.93
                       Mean reward: 5093.87
               Mean episode length: 277.77
                 Mean success rate: 60.00
                  Mean reward/step: 17.29
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 0.49s
                        Total time: 196.37s
                               ETA: 806.0s

################################################################################
                     [1m Learning iteration 392/2000 [0m

                       Computation: 16044 steps/s (collection: 0.278s, learning 0.233s)
               Value function loss: 36084.4487
                    Surrogate loss: -0.0004
             Mean action noise std: 0.93
                       Mean reward: 5089.43
               Mean episode length: 286.13
                 Mean success rate: 59.50
                  Mean reward/step: 17.94
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3219456
                    Iteration time: 0.51s
                        Total time: 196.88s
                               ETA: 805.6s

################################################################################
                     [1m Learning iteration 393/2000 [0m

                       Computation: 16086 steps/s (collection: 0.291s, learning 0.218s)
               Value function loss: 44516.8357
                    Surrogate loss: 0.0016
             Mean action noise std: 0.93
                       Mean reward: 5332.79
               Mean episode length: 299.12
                 Mean success rate: 61.50
                  Mean reward/step: 18.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 0.51s
                        Total time: 197.39s
                               ETA: 805.1s

################################################################################
                     [1m Learning iteration 394/2000 [0m

                       Computation: 15769 steps/s (collection: 0.300s, learning 0.220s)
               Value function loss: 61110.4299
                    Surrogate loss: -0.0012
             Mean action noise std: 0.93
                       Mean reward: 5258.31
               Mean episode length: 306.80
                 Mean success rate: 62.00
                  Mean reward/step: 18.51
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3235840
                    Iteration time: 0.52s
                        Total time: 197.91s
                               ETA: 804.7s

################################################################################
                     [1m Learning iteration 395/2000 [0m

                       Computation: 16304 steps/s (collection: 0.287s, learning 0.216s)
               Value function loss: 43006.7692
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 5121.69
               Mean episode length: 302.54
                 Mean success rate: 60.50
                  Mean reward/step: 19.17
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.50s
                        Total time: 198.41s
                               ETA: 804.2s

################################################################################
                     [1m Learning iteration 396/2000 [0m

                       Computation: 16660 steps/s (collection: 0.281s, learning 0.210s)
               Value function loss: 54180.9040
                    Surrogate loss: 0.0036
             Mean action noise std: 0.93
                       Mean reward: 4825.36
               Mean episode length: 289.38
                 Mean success rate: 61.00
                  Mean reward/step: 16.75
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3252224
                    Iteration time: 0.49s
                        Total time: 198.90s
                               ETA: 803.6s

################################################################################
                     [1m Learning iteration 397/2000 [0m

                       Computation: 16292 steps/s (collection: 0.289s, learning 0.214s)
               Value function loss: 48547.2892
                    Surrogate loss: -0.0016
             Mean action noise std: 0.93
                       Mean reward: 4948.50
               Mean episode length: 296.29
                 Mean success rate: 62.50
                  Mean reward/step: 14.82
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 0.50s
                        Total time: 199.41s
                               ETA: 803.1s

################################################################################
                     [1m Learning iteration 398/2000 [0m

                       Computation: 15280 steps/s (collection: 0.300s, learning 0.236s)
               Value function loss: 49811.2677
                    Surrogate loss: -0.0034
             Mean action noise std: 0.93
                       Mean reward: 4974.83
               Mean episode length: 306.88
                 Mean success rate: 65.50
                  Mean reward/step: 14.75
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3268608
                    Iteration time: 0.54s
                        Total time: 199.94s
                               ETA: 802.8s

################################################################################
                     [1m Learning iteration 399/2000 [0m

                       Computation: 15605 steps/s (collection: 0.294s, learning 0.231s)
               Value function loss: 45136.1944
                    Surrogate loss: 0.0092
             Mean action noise std: 0.93
                       Mean reward: 5065.01
               Mean episode length: 308.35
                 Mean success rate: 66.50
                  Mean reward/step: 14.78
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 0.52s
                        Total time: 200.47s
                               ETA: 802.4s

################################################################################
                     [1m Learning iteration 400/2000 [0m

                       Computation: 14724 steps/s (collection: 0.291s, learning 0.265s)
               Value function loss: 55525.1426
                    Surrogate loss: 0.0047
             Mean action noise std: 0.93
                       Mean reward: 5077.28
               Mean episode length: 307.92
                 Mean success rate: 66.00
                  Mean reward/step: 15.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3284992
                    Iteration time: 0.56s
                        Total time: 201.02s
                               ETA: 802.1s

################################################################################
                     [1m Learning iteration 401/2000 [0m

                       Computation: 14075 steps/s (collection: 0.326s, learning 0.256s)
               Value function loss: 47000.7286
                    Surrogate loss: 0.0018
             Mean action noise std: 0.93
                       Mean reward: 5178.42
               Mean episode length: 310.91
                 Mean success rate: 66.00
                  Mean reward/step: 14.42
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 0.58s
                        Total time: 201.61s
                               ETA: 801.9s

################################################################################
                     [1m Learning iteration 402/2000 [0m

                       Computation: 14443 steps/s (collection: 0.317s, learning 0.251s)
               Value function loss: 42396.4608
                    Surrogate loss: -0.0003
             Mean action noise std: 0.93
                       Mean reward: 5162.84
               Mean episode length: 313.04
                 Mean success rate: 66.00
                  Mean reward/step: 15.02
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3301376
                    Iteration time: 0.57s
                        Total time: 202.17s
                               ETA: 801.7s

################################################################################
                     [1m Learning iteration 403/2000 [0m

                       Computation: 13885 steps/s (collection: 0.325s, learning 0.265s)
               Value function loss: 43530.4465
                    Surrogate loss: 0.0105
             Mean action noise std: 0.93
                       Mean reward: 5108.66
               Mean episode length: 308.29
                 Mean success rate: 66.00
                  Mean reward/step: 14.32
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 0.59s
                        Total time: 202.76s
                               ETA: 801.5s

################################################################################
                     [1m Learning iteration 404/2000 [0m

                       Computation: 15291 steps/s (collection: 0.304s, learning 0.232s)
               Value function loss: 25847.4471
                    Surrogate loss: -0.0022
             Mean action noise std: 0.93
                       Mean reward: 4774.85
               Mean episode length: 294.50
                 Mean success rate: 64.00
                  Mean reward/step: 14.60
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3317760
                    Iteration time: 0.54s
                        Total time: 203.30s
                               ETA: 801.2s

################################################################################
                     [1m Learning iteration 405/2000 [0m

                       Computation: 14665 steps/s (collection: 0.292s, learning 0.267s)
               Value function loss: 28719.7575
                    Surrogate loss: 0.0278
             Mean action noise std: 0.93
                       Mean reward: 4809.32
               Mean episode length: 294.38
                 Mean success rate: 63.50
                  Mean reward/step: 15.48
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 0.56s
                        Total time: 203.86s
                               ETA: 800.9s

################################################################################
                     [1m Learning iteration 406/2000 [0m

                       Computation: 14810 steps/s (collection: 0.329s, learning 0.224s)
               Value function loss: 27233.3152
                    Surrogate loss: 0.0193
             Mean action noise std: 0.93
                       Mean reward: 4666.38
               Mean episode length: 286.06
                 Mean success rate: 62.00
                  Mean reward/step: 14.01
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3334144
                    Iteration time: 0.55s
                        Total time: 204.41s
                               ETA: 800.6s

################################################################################
                     [1m Learning iteration 407/2000 [0m

                       Computation: 14464 steps/s (collection: 0.314s, learning 0.253s)
               Value function loss: 22284.3779
                    Surrogate loss: 0.1545
             Mean action noise std: 0.93
                       Mean reward: 4673.26
               Mean episode length: 287.51
                 Mean success rate: 62.50
                  Mean reward/step: 17.18
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.57s
                        Total time: 204.98s
                               ETA: 800.3s

################################################################################
                     [1m Learning iteration 408/2000 [0m

                       Computation: 14198 steps/s (collection: 0.308s, learning 0.269s)
               Value function loss: 58510.2735
                    Surrogate loss: 0.0044
             Mean action noise std: 0.93
                       Mean reward: 4566.98
               Mean episode length: 284.39
                 Mean success rate: 62.50
                  Mean reward/step: 19.83
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3350528
                    Iteration time: 0.58s
                        Total time: 205.55s
                               ETA: 800.1s

################################################################################
                     [1m Learning iteration 409/2000 [0m

                       Computation: 13543 steps/s (collection: 0.357s, learning 0.248s)
               Value function loss: 61904.5543
                    Surrogate loss: -0.0024
             Mean action noise std: 0.93
                       Mean reward: 4254.95
               Mean episode length: 272.51
                 Mean success rate: 63.50
                  Mean reward/step: 20.42
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 0.60s
                        Total time: 206.16s
                               ETA: 800.0s

################################################################################
                     [1m Learning iteration 410/2000 [0m

                       Computation: 15159 steps/s (collection: 0.332s, learning 0.209s)
               Value function loss: 62414.3811
                    Surrogate loss: 0.0125
             Mean action noise std: 0.93
                       Mean reward: 4167.31
               Mean episode length: 265.10
                 Mean success rate: 63.00
                  Mean reward/step: 19.71
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 3366912
                    Iteration time: 0.54s
                        Total time: 206.70s
                               ETA: 799.6s

################################################################################
                     [1m Learning iteration 411/2000 [0m

                       Computation: 16377 steps/s (collection: 0.300s, learning 0.200s)
               Value function loss: 51580.9395
                    Surrogate loss: 0.0613
             Mean action noise std: 0.93
                       Mean reward: 4148.16
               Mean episode length: 265.77
                 Mean success rate: 62.50
                  Mean reward/step: 19.21
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 0.50s
                        Total time: 207.20s
                               ETA: 799.1s

################################################################################
                     [1m Learning iteration 412/2000 [0m

                       Computation: 16693 steps/s (collection: 0.288s, learning 0.203s)
               Value function loss: 69152.6501
                    Surrogate loss: 0.0083
             Mean action noise std: 0.93
                       Mean reward: 4541.32
               Mean episode length: 282.33
                 Mean success rate: 65.50
                  Mean reward/step: 18.71
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3383296
                    Iteration time: 0.49s
                        Total time: 207.69s
                               ETA: 798.6s

################################################################################
                     [1m Learning iteration 413/2000 [0m

                       Computation: 17349 steps/s (collection: 0.266s, learning 0.206s)
               Value function loss: 61392.8860
                    Surrogate loss: 0.0041
             Mean action noise std: 0.93
                       Mean reward: 4884.71
               Mean episode length: 298.08
                 Mean success rate: 69.50
                  Mean reward/step: 18.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 0.47s
                        Total time: 208.16s
                               ETA: 798.0s

################################################################################
                     [1m Learning iteration 414/2000 [0m

                       Computation: 17317 steps/s (collection: 0.272s, learning 0.201s)
               Value function loss: 46531.5341
                    Surrogate loss: 0.0128
             Mean action noise std: 0.93
                       Mean reward: 5082.11
               Mean episode length: 302.04
                 Mean success rate: 69.00
                  Mean reward/step: 17.93
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3399680
                    Iteration time: 0.47s
                        Total time: 208.64s
                               ETA: 797.3s

################################################################################
                     [1m Learning iteration 415/2000 [0m

                       Computation: 17381 steps/s (collection: 0.271s, learning 0.201s)
               Value function loss: 38475.7219
                    Surrogate loss: 0.0034
             Mean action noise std: 0.93
                       Mean reward: 5156.61
               Mean episode length: 302.29
                 Mean success rate: 67.50
                  Mean reward/step: 17.96
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 0.47s
                        Total time: 209.11s
                               ETA: 796.7s

################################################################################
                     [1m Learning iteration 416/2000 [0m

                       Computation: 16866 steps/s (collection: 0.277s, learning 0.208s)
               Value function loss: 52667.1540
                    Surrogate loss: 0.0098
             Mean action noise std: 0.93
                       Mean reward: 5680.46
               Mean episode length: 321.04
                 Mean success rate: 69.00
                  Mean reward/step: 18.22
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3416064
                    Iteration time: 0.49s
                        Total time: 209.59s
                               ETA: 796.2s

################################################################################
                     [1m Learning iteration 417/2000 [0m

                       Computation: 17147 steps/s (collection: 0.272s, learning 0.205s)
               Value function loss: 44667.9586
                    Surrogate loss: 0.0267
             Mean action noise std: 0.93
                       Mean reward: 6019.89
               Mean episode length: 336.64
                 Mean success rate: 71.00
                  Mean reward/step: 18.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 0.48s
                        Total time: 210.07s
                               ETA: 795.6s

################################################################################
                     [1m Learning iteration 418/2000 [0m

                       Computation: 16706 steps/s (collection: 0.287s, learning 0.203s)
               Value function loss: 54931.2368
                    Surrogate loss: 0.0226
             Mean action noise std: 0.93
                       Mean reward: 6205.43
               Mean episode length: 341.64
                 Mean success rate: 71.50
                  Mean reward/step: 19.18
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3432448
                    Iteration time: 0.49s
                        Total time: 210.56s
                               ETA: 795.0s

################################################################################
                     [1m Learning iteration 419/2000 [0m

                       Computation: 16734 steps/s (collection: 0.286s, learning 0.203s)
               Value function loss: 53598.8606
                    Surrogate loss: 0.0086
             Mean action noise std: 0.93
                       Mean reward: 6476.66
               Mean episode length: 346.25
                 Mean success rate: 72.50
                  Mean reward/step: 18.17
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.49s
                        Total time: 211.05s
                               ETA: 794.5s

################################################################################
                     [1m Learning iteration 420/2000 [0m

                       Computation: 15648 steps/s (collection: 0.299s, learning 0.224s)
               Value function loss: 39059.2803
                    Surrogate loss: -0.0049
             Mean action noise std: 0.93
                       Mean reward: 6182.70
               Mean episode length: 330.13
                 Mean success rate: 69.50
                  Mean reward/step: 16.90
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3448832
                    Iteration time: 0.52s
                        Total time: 211.57s
                               ETA: 794.0s

################################################################################
                     [1m Learning iteration 421/2000 [0m

                       Computation: 14561 steps/s (collection: 0.341s, learning 0.222s)
               Value function loss: 31243.1393
                    Surrogate loss: 0.0131
             Mean action noise std: 0.93
                       Mean reward: 6103.91
               Mean episode length: 327.55
                 Mean success rate: 68.00
                  Mean reward/step: 16.25
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 0.56s
                        Total time: 212.14s
                               ETA: 793.8s

################################################################################
                     [1m Learning iteration 422/2000 [0m

                       Computation: 15256 steps/s (collection: 0.313s, learning 0.224s)
               Value function loss: 42645.9009
                    Surrogate loss: 0.0043
             Mean action noise std: 0.93
                       Mean reward: 5958.76
               Mean episode length: 320.73
                 Mean success rate: 68.00
                  Mean reward/step: 17.93
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3465216
                    Iteration time: 0.54s
                        Total time: 212.67s
                               ETA: 793.4s

################################################################################
                     [1m Learning iteration 423/2000 [0m

                       Computation: 16155 steps/s (collection: 0.291s, learning 0.216s)
               Value function loss: 42580.8931
                    Surrogate loss: 0.0007
             Mean action noise std: 0.93
                       Mean reward: 6030.68
               Mean episode length: 322.08
                 Mean success rate: 69.00
                  Mean reward/step: 17.93
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 0.51s
                        Total time: 213.18s
                               ETA: 792.9s

################################################################################
                     [1m Learning iteration 424/2000 [0m

                       Computation: 16254 steps/s (collection: 0.295s, learning 0.209s)
               Value function loss: 40314.6577
                    Surrogate loss: -0.0049
             Mean action noise std: 0.93
                       Mean reward: 5838.94
               Mean episode length: 315.39
                 Mean success rate: 68.50
                  Mean reward/step: 18.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3481600
                    Iteration time: 0.50s
                        Total time: 213.68s
                               ETA: 792.4s

################################################################################
                     [1m Learning iteration 425/2000 [0m

                       Computation: 17207 steps/s (collection: 0.275s, learning 0.201s)
               Value function loss: 59102.7284
                    Surrogate loss: 0.0020
             Mean action noise std: 0.93
                       Mean reward: 5809.73
               Mean episode length: 316.98
                 Mean success rate: 68.50
                  Mean reward/step: 19.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 0.48s
                        Total time: 214.16s
                               ETA: 791.8s

################################################################################
                     [1m Learning iteration 426/2000 [0m

                       Computation: 16747 steps/s (collection: 0.289s, learning 0.200s)
               Value function loss: 66257.1316
                    Surrogate loss: 0.0064
             Mean action noise std: 0.93
                       Mean reward: 5909.85
               Mean episode length: 329.24
                 Mean success rate: 69.00
                  Mean reward/step: 19.64
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3497984
                    Iteration time: 0.49s
                        Total time: 214.65s
                               ETA: 791.2s

################################################################################
                     [1m Learning iteration 427/2000 [0m

                       Computation: 17030 steps/s (collection: 0.277s, learning 0.204s)
               Value function loss: 61839.9571
                    Surrogate loss: 0.0004
             Mean action noise std: 0.93
                       Mean reward: 6067.96
               Mean episode length: 337.37
                 Mean success rate: 70.50
                  Mean reward/step: 20.27
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 0.48s
                        Total time: 215.13s
                               ETA: 790.7s

################################################################################
                     [1m Learning iteration 428/2000 [0m

                       Computation: 17188 steps/s (collection: 0.273s, learning 0.203s)
               Value function loss: 52647.9833
                    Surrogate loss: 0.0042
             Mean action noise std: 0.93
                       Mean reward: 6255.28
               Mean episode length: 349.76
                 Mean success rate: 72.50
                  Mean reward/step: 19.73
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3514368
                    Iteration time: 0.48s
                        Total time: 215.61s
                               ETA: 790.1s

################################################################################
                     [1m Learning iteration 429/2000 [0m

                       Computation: 16822 steps/s (collection: 0.280s, learning 0.207s)
               Value function loss: 63985.6262
                    Surrogate loss: -0.0024
             Mean action noise std: 0.93
                       Mean reward: 6476.26
               Mean episode length: 363.32
                 Mean success rate: 73.50
                  Mean reward/step: 20.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 0.49s
                        Total time: 216.09s
                               ETA: 789.5s

################################################################################
                     [1m Learning iteration 430/2000 [0m

                       Computation: 16728 steps/s (collection: 0.283s, learning 0.207s)
               Value function loss: 60394.4979
                    Surrogate loss: 0.0020
             Mean action noise std: 0.93
                       Mean reward: 6564.23
               Mean episode length: 363.93
                 Mean success rate: 74.00
                  Mean reward/step: 20.11
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3530752
                    Iteration time: 0.49s
                        Total time: 216.58s
                               ETA: 788.9s

################################################################################
                     [1m Learning iteration 431/2000 [0m

                       Computation: 16106 steps/s (collection: 0.295s, learning 0.213s)
               Value function loss: 61743.6552
                    Surrogate loss: -0.0001
             Mean action noise std: 0.93
                       Mean reward: 6422.17
               Mean episode length: 359.41
                 Mean success rate: 73.50
                  Mean reward/step: 20.03
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.51s
                        Total time: 217.09s
                               ETA: 788.5s

################################################################################
                     [1m Learning iteration 432/2000 [0m

                       Computation: 16501 steps/s (collection: 0.284s, learning 0.212s)
               Value function loss: 75322.8958
                    Surrogate loss: 0.0080
             Mean action noise std: 0.93
                       Mean reward: 6816.92
               Mean episode length: 378.09
                 Mean success rate: 76.00
                  Mean reward/step: 20.11
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3547136
                    Iteration time: 0.50s
                        Total time: 217.59s
                               ETA: 787.9s

################################################################################
                     [1m Learning iteration 433/2000 [0m

                       Computation: 17003 steps/s (collection: 0.273s, learning 0.209s)
               Value function loss: 39279.0907
                    Surrogate loss: 0.0103
             Mean action noise std: 0.93
                       Mean reward: 6973.88
               Mean episode length: 384.96
                 Mean success rate: 77.00
                  Mean reward/step: 20.31
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 0.48s
                        Total time: 218.07s
                               ETA: 787.4s

################################################################################
                     [1m Learning iteration 434/2000 [0m

                       Computation: 16638 steps/s (collection: 0.279s, learning 0.214s)
               Value function loss: 100832.3709
                    Surrogate loss: 0.0218
             Mean action noise std: 0.93
                       Mean reward: 7201.25
               Mean episode length: 385.99
                 Mean success rate: 76.50
                  Mean reward/step: 21.08
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3563520
                    Iteration time: 0.49s
                        Total time: 218.56s
                               ETA: 786.8s

################################################################################
                     [1m Learning iteration 435/2000 [0m

                       Computation: 15812 steps/s (collection: 0.307s, learning 0.211s)
               Value function loss: 90886.2962
                    Surrogate loss: -0.0038
             Mean action noise std: 0.93
                       Mean reward: 6825.33
               Mean episode length: 361.17
                 Mean success rate: 73.00
                  Mean reward/step: 18.02
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 0.52s
                        Total time: 219.08s
                               ETA: 786.4s

################################################################################
                     [1m Learning iteration 436/2000 [0m

                       Computation: 16450 steps/s (collection: 0.294s, learning 0.204s)
               Value function loss: 69378.5957
                    Surrogate loss: 0.0026
             Mean action noise std: 0.93
                       Mean reward: 6887.17
               Mean episode length: 356.25
                 Mean success rate: 73.50
                  Mean reward/step: 16.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3579904
                    Iteration time: 0.50s
                        Total time: 219.58s
                               ETA: 785.9s

################################################################################
                     [1m Learning iteration 437/2000 [0m

                       Computation: 16999 steps/s (collection: 0.274s, learning 0.208s)
               Value function loss: 58886.2381
                    Surrogate loss: -0.0025
             Mean action noise std: 0.93
                       Mean reward: 6868.71
               Mean episode length: 349.69
                 Mean success rate: 72.00
                  Mean reward/step: 17.22
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 0.48s
                        Total time: 220.06s
                               ETA: 785.3s

################################################################################
                     [1m Learning iteration 438/2000 [0m

                       Computation: 16896 steps/s (collection: 0.280s, learning 0.205s)
               Value function loss: 57464.5829
                    Surrogate loss: -0.0007
             Mean action noise std: 0.93
                       Mean reward: 6747.78
               Mean episode length: 341.24
                 Mean success rate: 71.50
                  Mean reward/step: 18.17
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3596288
                    Iteration time: 0.48s
                        Total time: 220.55s
                               ETA: 784.7s

################################################################################
                     [1m Learning iteration 439/2000 [0m

                       Computation: 17703 steps/s (collection: 0.262s, learning 0.201s)
               Value function loss: 39233.4053
                    Surrogate loss: 0.0039
             Mean action noise std: 0.93
                       Mean reward: 6872.63
               Mean episode length: 351.89
                 Mean success rate: 73.50
                  Mean reward/step: 19.38
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 0.46s
                        Total time: 221.01s
                               ETA: 784.1s

################################################################################
                     [1m Learning iteration 440/2000 [0m

                       Computation: 17254 steps/s (collection: 0.267s, learning 0.207s)
               Value function loss: 62305.2715
                    Surrogate loss: 0.0011
             Mean action noise std: 0.93
                       Mean reward: 7110.62
               Mean episode length: 358.75
                 Mean success rate: 74.00
                  Mean reward/step: 20.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3612672
                    Iteration time: 0.47s
                        Total time: 221.48s
                               ETA: 783.5s

################################################################################
                     [1m Learning iteration 441/2000 [0m

                       Computation: 16802 steps/s (collection: 0.276s, learning 0.212s)
               Value function loss: 86584.5483
                    Surrogate loss: -0.0028
             Mean action noise std: 0.93
                       Mean reward: 7183.37
               Mean episode length: 358.65
                 Mean success rate: 74.00
                  Mean reward/step: 20.66
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 0.49s
                        Total time: 221.97s
                               ETA: 782.9s

################################################################################
                     [1m Learning iteration 442/2000 [0m

                       Computation: 17460 steps/s (collection: 0.258s, learning 0.212s)
               Value function loss: 45575.1367
                    Surrogate loss: -0.0039
             Mean action noise std: 0.93
                       Mean reward: 7238.20
               Mean episode length: 363.49
                 Mean success rate: 75.00
                  Mean reward/step: 21.29
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 3629056
                    Iteration time: 0.47s
                        Total time: 222.44s
                               ETA: 782.3s

################################################################################
                     [1m Learning iteration 443/2000 [0m

                       Computation: 16263 steps/s (collection: 0.301s, learning 0.203s)
               Value function loss: 74294.4432
                    Surrogate loss: 0.0069
             Mean action noise std: 0.93
                       Mean reward: 7147.65
               Mean episode length: 360.45
                 Mean success rate: 75.50
                  Mean reward/step: 22.02
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.50s
                        Total time: 222.94s
                               ETA: 781.8s

################################################################################
                     [1m Learning iteration 444/2000 [0m

                       Computation: 16677 steps/s (collection: 0.270s, learning 0.221s)
               Value function loss: 41593.5183
                    Surrogate loss: 0.0068
             Mean action noise std: 0.92
                       Mean reward: 7103.18
               Mean episode length: 360.69
                 Mean success rate: 75.50
                  Mean reward/step: 21.74
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3645440
                    Iteration time: 0.49s
                        Total time: 223.44s
                               ETA: 781.3s

################################################################################
                     [1m Learning iteration 445/2000 [0m

                       Computation: 16721 steps/s (collection: 0.283s, learning 0.206s)
               Value function loss: 70027.2382
                    Surrogate loss: -0.0023
             Mean action noise std: 0.93
                       Mean reward: 7269.71
               Mean episode length: 370.86
                 Mean success rate: 76.50
                  Mean reward/step: 22.30
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 0.49s
                        Total time: 223.93s
                               ETA: 780.7s

################################################################################
                     [1m Learning iteration 446/2000 [0m

                       Computation: 16381 steps/s (collection: 0.293s, learning 0.207s)
               Value function loss: 78022.7948
                    Surrogate loss: -0.0019
             Mean action noise std: 0.93
                       Mean reward: 7298.31
               Mean episode length: 373.90
                 Mean success rate: 77.50
                  Mean reward/step: 22.20
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3661824
                    Iteration time: 0.50s
                        Total time: 224.43s
                               ETA: 780.2s

################################################################################
                     [1m Learning iteration 447/2000 [0m

                       Computation: 16752 steps/s (collection: 0.280s, learning 0.209s)
               Value function loss: 67654.2964
                    Surrogate loss: -0.0034
             Mean action noise std: 0.93
                       Mean reward: 7464.06
               Mean episode length: 380.13
                 Mean success rate: 78.50
                  Mean reward/step: 21.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 0.49s
                        Total time: 224.91s
                               ETA: 779.7s

################################################################################
                     [1m Learning iteration 448/2000 [0m

                       Computation: 16808 steps/s (collection: 0.279s, learning 0.208s)
               Value function loss: 80654.6645
                    Surrogate loss: -0.0041
             Mean action noise std: 0.92
                       Mean reward: 7448.23
               Mean episode length: 376.01
                 Mean success rate: 77.50
                  Mean reward/step: 22.08
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3678208
                    Iteration time: 0.49s
                        Total time: 225.40s
                               ETA: 779.1s

################################################################################
                     [1m Learning iteration 449/2000 [0m

                       Computation: 17257 steps/s (collection: 0.263s, learning 0.212s)
               Value function loss: 64719.3252
                    Surrogate loss: -0.0052
             Mean action noise std: 0.92
                       Mean reward: 7419.95
               Mean episode length: 372.15
                 Mean success rate: 77.00
                  Mean reward/step: 22.48
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 0.47s
                        Total time: 225.88s
                               ETA: 778.5s

################################################################################
                     [1m Learning iteration 450/2000 [0m

                       Computation: 17183 steps/s (collection: 0.267s, learning 0.210s)
               Value function loss: 111811.0117
                    Surrogate loss: -0.0010
             Mean action noise std: 0.92
                       Mean reward: 7367.04
               Mean episode length: 372.25
                 Mean success rate: 75.50
                  Mean reward/step: 22.67
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3694592
                    Iteration time: 0.48s
                        Total time: 226.35s
                               ETA: 777.9s

################################################################################
                     [1m Learning iteration 451/2000 [0m

                       Computation: 17511 steps/s (collection: 0.264s, learning 0.204s)
               Value function loss: 103420.2455
                    Surrogate loss: 0.0080
             Mean action noise std: 0.92
                       Mean reward: 7435.76
               Mean episode length: 369.50
                 Mean success rate: 76.00
                  Mean reward/step: 22.93
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 0.47s
                        Total time: 226.82s
                               ETA: 777.3s

################################################################################
                     [1m Learning iteration 452/2000 [0m

                       Computation: 18037 steps/s (collection: 0.250s, learning 0.204s)
               Value function loss: 53791.1096
                    Surrogate loss: 0.0119
             Mean action noise std: 0.92
                       Mean reward: 7467.08
               Mean episode length: 368.22
                 Mean success rate: 76.00
                  Mean reward/step: 20.98
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 3710976
                    Iteration time: 0.45s
                        Total time: 227.28s
                               ETA: 776.6s

################################################################################
                     [1m Learning iteration 453/2000 [0m

                       Computation: 15737 steps/s (collection: 0.289s, learning 0.231s)
               Value function loss: 96129.7762
                    Surrogate loss: 0.0045
             Mean action noise std: 0.92
                       Mean reward: 7732.03
               Mean episode length: 373.52
                 Mean success rate: 77.00
                  Mean reward/step: 21.33
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 0.52s
                        Total time: 227.80s
                               ETA: 776.2s

################################################################################
                     [1m Learning iteration 454/2000 [0m

                       Computation: 17037 steps/s (collection: 0.275s, learning 0.206s)
               Value function loss: 55075.6508
                    Surrogate loss: -0.0029
             Mean action noise std: 0.92
                       Mean reward: 7800.89
               Mean episode length: 373.13
                 Mean success rate: 77.50
                  Mean reward/step: 21.19
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3727360
                    Iteration time: 0.48s
                        Total time: 228.28s
                               ETA: 775.6s

################################################################################
                     [1m Learning iteration 455/2000 [0m

                       Computation: 16186 steps/s (collection: 0.279s, learning 0.227s)
               Value function loss: 65211.9484
                    Surrogate loss: -0.0014
             Mean action noise std: 0.92
                       Mean reward: 7886.46
               Mean episode length: 372.86
                 Mean success rate: 78.50
                  Mean reward/step: 21.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.51s
                        Total time: 228.78s
                               ETA: 775.2s

################################################################################
                     [1m Learning iteration 456/2000 [0m

                       Computation: 17042 steps/s (collection: 0.270s, learning 0.210s)
               Value function loss: 69243.5250
                    Surrogate loss: -0.0021
             Mean action noise std: 0.92
                       Mean reward: 7843.63
               Mean episode length: 371.75
                 Mean success rate: 79.00
                  Mean reward/step: 21.80
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3743744
                    Iteration time: 0.48s
                        Total time: 229.26s
                               ETA: 774.6s

################################################################################
                     [1m Learning iteration 457/2000 [0m

                       Computation: 15458 steps/s (collection: 0.314s, learning 0.216s)
               Value function loss: 73940.8924
                    Surrogate loss: -0.0012
             Mean action noise std: 0.92
                       Mean reward: 8042.02
               Mean episode length: 373.06
                 Mean success rate: 79.50
                  Mean reward/step: 22.08
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 0.53s
                        Total time: 229.79s
                               ETA: 774.2s

################################################################################
                     [1m Learning iteration 458/2000 [0m

                       Computation: 16856 steps/s (collection: 0.276s, learning 0.210s)
               Value function loss: 57043.4728
                    Surrogate loss: 0.0118
             Mean action noise std: 0.92
                       Mean reward: 8113.01
               Mean episode length: 373.62
                 Mean success rate: 80.00
                  Mean reward/step: 21.69
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3760128
                    Iteration time: 0.49s
                        Total time: 230.28s
                               ETA: 773.6s

################################################################################
                     [1m Learning iteration 459/2000 [0m

                       Computation: 15784 steps/s (collection: 0.284s, learning 0.235s)
               Value function loss: 67742.6161
                    Surrogate loss: -0.0021
             Mean action noise std: 0.92
                       Mean reward: 8138.38
               Mean episode length: 372.73
                 Mean success rate: 80.50
                  Mean reward/step: 21.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 0.52s
                        Total time: 230.80s
                               ETA: 773.2s

################################################################################
                     [1m Learning iteration 460/2000 [0m

                       Computation: 15253 steps/s (collection: 0.289s, learning 0.248s)
               Value function loss: 56483.3246
                    Surrogate loss: 0.0050
             Mean action noise std: 0.92
                       Mean reward: 8133.95
               Mean episode length: 369.80
                 Mean success rate: 80.00
                  Mean reward/step: 22.22
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3776512
                    Iteration time: 0.54s
                        Total time: 231.34s
                               ETA: 772.8s

################################################################################
                     [1m Learning iteration 461/2000 [0m

                       Computation: 15808 steps/s (collection: 0.296s, learning 0.222s)
               Value function loss: 59600.6910
                    Surrogate loss: 0.0074
             Mean action noise std: 0.92
                       Mean reward: 8267.30
               Mean episode length: 374.50
                 Mean success rate: 79.50
                  Mean reward/step: 22.56
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 0.52s
                        Total time: 231.85s
                               ETA: 772.3s

################################################################################
                     [1m Learning iteration 462/2000 [0m

                       Computation: 16644 steps/s (collection: 0.287s, learning 0.205s)
               Value function loss: 89872.9255
                    Surrogate loss: -0.0013
             Mean action noise std: 0.92
                       Mean reward: 8392.01
               Mean episode length: 379.13
                 Mean success rate: 80.50
                  Mean reward/step: 22.84
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3792896
                    Iteration time: 0.49s
                        Total time: 232.35s
                               ETA: 771.8s

################################################################################
                     [1m Learning iteration 463/2000 [0m

                       Computation: 17867 steps/s (collection: 0.250s, learning 0.208s)
               Value function loss: 96097.5682
                    Surrogate loss: 0.0083
             Mean action noise std: 0.92
                       Mean reward: 8458.87
               Mean episode length: 382.69
                 Mean success rate: 80.00
                  Mean reward/step: 21.68
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 0.46s
                        Total time: 232.80s
                               ETA: 771.2s

################################################################################
                     [1m Learning iteration 464/2000 [0m

                       Computation: 17909 steps/s (collection: 0.245s, learning 0.212s)
               Value function loss: 55523.2739
                    Surrogate loss: 0.0069
             Mean action noise std: 0.92
                       Mean reward: 8577.30
               Mean episode length: 390.79
                 Mean success rate: 81.00
                  Mean reward/step: 22.16
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 3809280
                    Iteration time: 0.46s
                        Total time: 233.26s
                               ETA: 770.5s

################################################################################
                     [1m Learning iteration 465/2000 [0m

                       Computation: 17270 steps/s (collection: 0.263s, learning 0.211s)
               Value function loss: 72559.4479
                    Surrogate loss: -0.0006
             Mean action noise std: 0.92
                       Mean reward: 8523.41
               Mean episode length: 389.27
                 Mean success rate: 79.50
                  Mean reward/step: 23.22
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 0.47s
                        Total time: 233.74s
                               ETA: 769.9s

################################################################################
                     [1m Learning iteration 466/2000 [0m

                       Computation: 17329 steps/s (collection: 0.253s, learning 0.220s)
               Value function loss: 84673.0828
                    Surrogate loss: 0.0009
             Mean action noise std: 0.92
                       Mean reward: 8621.05
               Mean episode length: 392.33
                 Mean success rate: 80.00
                  Mean reward/step: 22.02
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3825664
                    Iteration time: 0.47s
                        Total time: 234.21s
                               ETA: 769.3s

################################################################################
                     [1m Learning iteration 467/2000 [0m

                       Computation: 16944 steps/s (collection: 0.270s, learning 0.213s)
               Value function loss: 62204.7710
                    Surrogate loss: -0.0002
             Mean action noise std: 0.92
                       Mean reward: 8784.78
               Mean episode length: 401.16
                 Mean success rate: 81.00
                  Mean reward/step: 20.83
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.48s
                        Total time: 234.69s
                               ETA: 768.8s

################################################################################
                     [1m Learning iteration 468/2000 [0m

                       Computation: 17119 steps/s (collection: 0.260s, learning 0.219s)
               Value function loss: 64696.5072
                    Surrogate loss: -0.0020
             Mean action noise std: 0.92
                       Mean reward: 8598.91
               Mean episode length: 399.08
                 Mean success rate: 79.50
                  Mean reward/step: 21.39
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3842048
                    Iteration time: 0.48s
                        Total time: 235.17s
                               ETA: 768.2s

################################################################################
                     [1m Learning iteration 469/2000 [0m

                       Computation: 17145 steps/s (collection: 0.263s, learning 0.215s)
               Value function loss: 104337.0267
                    Surrogate loss: 0.0006
             Mean action noise std: 0.92
                       Mean reward: 8840.51
               Mean episode length: 402.18
                 Mean success rate: 81.50
                  Mean reward/step: 22.14
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 0.48s
                        Total time: 235.65s
                               ETA: 767.6s

################################################################################
                     [1m Learning iteration 470/2000 [0m

                       Computation: 16941 steps/s (collection: 0.257s, learning 0.227s)
               Value function loss: 55499.2788
                    Surrogate loss: 0.0025
             Mean action noise std: 0.92
                       Mean reward: 8806.22
               Mean episode length: 403.79
                 Mean success rate: 82.00
                  Mean reward/step: 22.17
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3858432
                    Iteration time: 0.48s
                        Total time: 236.13s
                               ETA: 767.1s

################################################################################
                     [1m Learning iteration 471/2000 [0m

                       Computation: 15814 steps/s (collection: 0.276s, learning 0.242s)
               Value function loss: 75452.0390
                    Surrogate loss: 0.0093
             Mean action noise std: 0.92
                       Mean reward: 8667.67
               Mean episode length: 396.67
                 Mean success rate: 80.50
                  Mean reward/step: 23.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 0.52s
                        Total time: 236.65s
                               ETA: 766.6s

################################################################################
                     [1m Learning iteration 472/2000 [0m

                       Computation: 17172 steps/s (collection: 0.262s, learning 0.215s)
               Value function loss: 86357.6134
                    Surrogate loss: 0.0018
             Mean action noise std: 0.92
                       Mean reward: 8594.01
               Mean episode length: 396.96
                 Mean success rate: 80.50
                  Mean reward/step: 23.97
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3874816
                    Iteration time: 0.48s
                        Total time: 237.13s
                               ETA: 766.0s

################################################################################
                     [1m Learning iteration 473/2000 [0m

                       Computation: 17289 steps/s (collection: 0.266s, learning 0.208s)
               Value function loss: 78339.2281
                    Surrogate loss: 0.0035
             Mean action noise std: 0.92
                       Mean reward: 8698.48
               Mean episode length: 393.44
                 Mean success rate: 81.50
                  Mean reward/step: 23.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 0.47s
                        Total time: 237.60s
                               ETA: 765.4s

################################################################################
                     [1m Learning iteration 474/2000 [0m

                       Computation: 16718 steps/s (collection: 0.284s, learning 0.206s)
               Value function loss: 70055.0232
                    Surrogate loss: 0.0009
             Mean action noise std: 0.92
                       Mean reward: 8550.51
               Mean episode length: 385.67
                 Mean success rate: 80.50
                  Mean reward/step: 23.62
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3891200
                    Iteration time: 0.49s
                        Total time: 238.09s
                               ETA: 764.9s

################################################################################
                     [1m Learning iteration 475/2000 [0m

                       Computation: 17217 steps/s (collection: 0.273s, learning 0.202s)
               Value function loss: 77003.3099
                    Surrogate loss: 0.0002
             Mean action noise std: 0.92
                       Mean reward: 8457.98
               Mean episode length: 381.60
                 Mean success rate: 80.00
                  Mean reward/step: 23.27
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 0.48s
                        Total time: 238.57s
                               ETA: 764.3s

################################################################################
                     [1m Learning iteration 476/2000 [0m

                       Computation: 17498 steps/s (collection: 0.262s, learning 0.207s)
               Value function loss: 80455.8369
                    Surrogate loss: -0.0005
             Mean action noise std: 0.92
                       Mean reward: 8109.75
               Mean episode length: 371.65
                 Mean success rate: 79.00
                  Mean reward/step: 23.10
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3907584
                    Iteration time: 0.47s
                        Total time: 239.03s
                               ETA: 763.7s

################################################################################
                     [1m Learning iteration 477/2000 [0m

                       Computation: 17224 steps/s (collection: 0.260s, learning 0.216s)
               Value function loss: 77988.4202
                    Surrogate loss: -0.0008
             Mean action noise std: 0.92
                       Mean reward: 8310.96
               Mean episode length: 377.29
                 Mean success rate: 80.50
                  Mean reward/step: 23.54
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 0.48s
                        Total time: 239.51s
                               ETA: 763.1s

################################################################################
                     [1m Learning iteration 478/2000 [0m

                       Computation: 16121 steps/s (collection: 0.298s, learning 0.210s)
               Value function loss: 85531.1512
                    Surrogate loss: -0.0037
             Mean action noise std: 0.92
                       Mean reward: 8254.79
               Mean episode length: 371.61
                 Mean success rate: 79.50
                  Mean reward/step: 23.00
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3923968
                    Iteration time: 0.51s
                        Total time: 240.02s
                               ETA: 762.6s

################################################################################
                     [1m Learning iteration 479/2000 [0m

                       Computation: 17480 steps/s (collection: 0.261s, learning 0.208s)
               Value function loss: 108047.6828
                    Surrogate loss: 0.0002
             Mean action noise std: 0.92
                       Mean reward: 8453.40
               Mean episode length: 374.75
                 Mean success rate: 79.50
                  Mean reward/step: 22.80
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.47s
                        Total time: 240.49s
                               ETA: 762.0s

################################################################################
                     [1m Learning iteration 480/2000 [0m

                       Computation: 17345 steps/s (collection: 0.268s, learning 0.205s)
               Value function loss: 74451.5264
                    Surrogate loss: -0.0026
             Mean action noise std: 0.92
                       Mean reward: 8341.29
               Mean episode length: 370.59
                 Mean success rate: 78.50
                  Mean reward/step: 23.17
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3940352
                    Iteration time: 0.47s
                        Total time: 240.96s
                               ETA: 761.5s

################################################################################
                     [1m Learning iteration 481/2000 [0m

                       Computation: 16959 steps/s (collection: 0.266s, learning 0.217s)
               Value function loss: 93122.0578
                    Surrogate loss: 0.0115
             Mean action noise std: 0.92
                       Mean reward: 8240.86
               Mean episode length: 362.56
                 Mean success rate: 77.00
                  Mean reward/step: 23.89
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 0.48s
                        Total time: 241.44s
                               ETA: 760.9s

################################################################################
                     [1m Learning iteration 482/2000 [0m

                       Computation: 17340 steps/s (collection: 0.266s, learning 0.207s)
               Value function loss: 110395.1931
                    Surrogate loss: 0.0190
             Mean action noise std: 0.92
                       Mean reward: 8167.65
               Mean episode length: 361.60
                 Mean success rate: 76.00
                  Mean reward/step: 23.29
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3956736
                    Iteration time: 0.47s
                        Total time: 241.91s
                               ETA: 760.3s

################################################################################
                     [1m Learning iteration 483/2000 [0m

                       Computation: 17215 steps/s (collection: 0.268s, learning 0.208s)
               Value function loss: 79455.5199
                    Surrogate loss: -0.0000
             Mean action noise std: 0.92
                       Mean reward: 8320.61
               Mean episode length: 372.00
                 Mean success rate: 77.00
                  Mean reward/step: 23.22
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 0.48s
                        Total time: 242.39s
                               ETA: 759.7s

################################################################################
                     [1m Learning iteration 484/2000 [0m

                       Computation: 16559 steps/s (collection: 0.292s, learning 0.203s)
               Value function loss: 91034.1502
                    Surrogate loss: 0.0177
             Mean action noise std: 0.92
                       Mean reward: 8618.25
               Mean episode length: 379.86
                 Mean success rate: 78.00
                  Mean reward/step: 22.34
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3973120
                    Iteration time: 0.49s
                        Total time: 242.89s
                               ETA: 759.2s

################################################################################
                     [1m Learning iteration 485/2000 [0m

                       Computation: 16104 steps/s (collection: 0.293s, learning 0.215s)
               Value function loss: 84239.1496
                    Surrogate loss: 0.0076
             Mean action noise std: 0.92
                       Mean reward: 9136.94
               Mean episode length: 390.07
                 Mean success rate: 79.50
                  Mean reward/step: 21.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 0.51s
                        Total time: 243.39s
                               ETA: 758.7s

################################################################################
                     [1m Learning iteration 486/2000 [0m

                       Computation: 16936 steps/s (collection: 0.277s, learning 0.207s)
               Value function loss: 57356.6440
                    Surrogate loss: 0.0053
             Mean action noise std: 0.92
                       Mean reward: 9011.41
               Mean episode length: 386.07
                 Mean success rate: 79.00
                  Mean reward/step: 20.09
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 3989504
                    Iteration time: 0.48s
                        Total time: 243.88s
                               ETA: 758.2s

################################################################################
                     [1m Learning iteration 487/2000 [0m

                       Computation: 17090 steps/s (collection: 0.266s, learning 0.213s)
               Value function loss: 57190.7802
                    Surrogate loss: 0.0043
             Mean action noise std: 0.92
                       Mean reward: 9046.12
               Mean episode length: 387.61
                 Mean success rate: 79.50
                  Mean reward/step: 20.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 0.48s
                        Total time: 244.36s
                               ETA: 757.6s

################################################################################
                     [1m Learning iteration 488/2000 [0m

                       Computation: 16912 steps/s (collection: 0.275s, learning 0.209s)
               Value function loss: 81710.3553
                    Surrogate loss: 0.0004
             Mean action noise std: 0.92
                       Mean reward: 8851.26
               Mean episode length: 382.54
                 Mean success rate: 79.00
                  Mean reward/step: 21.82
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4005888
                    Iteration time: 0.48s
                        Total time: 244.84s
                               ETA: 757.1s

################################################################################
                     [1m Learning iteration 489/2000 [0m

                       Computation: 17523 steps/s (collection: 0.269s, learning 0.198s)
               Value function loss: 58216.8265
                    Surrogate loss: -0.0022
             Mean action noise std: 0.92
                       Mean reward: 8724.24
               Mean episode length: 381.06
                 Mean success rate: 79.00
                  Mean reward/step: 22.20
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 0.47s
                        Total time: 245.31s
                               ETA: 756.5s

################################################################################
                     [1m Learning iteration 490/2000 [0m

                       Computation: 15926 steps/s (collection: 0.295s, learning 0.220s)
               Value function loss: 83930.5041
                    Surrogate loss: 0.0047
             Mean action noise std: 0.92
                       Mean reward: 8736.30
               Mean episode length: 386.81
                 Mean success rate: 80.00
                  Mean reward/step: 23.16
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4022272
                    Iteration time: 0.51s
                        Total time: 245.82s
                               ETA: 756.0s

################################################################################
                     [1m Learning iteration 491/2000 [0m

                       Computation: 16034 steps/s (collection: 0.282s, learning 0.229s)
               Value function loss: 68966.7604
                    Surrogate loss: 0.0005
             Mean action noise std: 0.92
                       Mean reward: 8739.57
               Mean episode length: 386.97
                 Mean success rate: 79.50
                  Mean reward/step: 23.24
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.51s
                        Total time: 246.33s
                               ETA: 755.5s

################################################################################
                     [1m Learning iteration 492/2000 [0m

                       Computation: 15672 steps/s (collection: 0.307s, learning 0.216s)
               Value function loss: 58466.4676
                    Surrogate loss: -0.0029
             Mean action noise std: 0.92
                       Mean reward: 8629.66
               Mean episode length: 382.59
                 Mean success rate: 79.00
                  Mean reward/step: 23.23
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4038656
                    Iteration time: 0.52s
                        Total time: 246.86s
                               ETA: 755.1s

################################################################################
                     [1m Learning iteration 493/2000 [0m

                       Computation: 15894 steps/s (collection: 0.290s, learning 0.226s)
               Value function loss: 73949.7158
                    Surrogate loss: -0.0013
             Mean action noise std: 0.92
                       Mean reward: 8752.62
               Mean episode length: 384.20
                 Mean success rate: 80.00
                  Mean reward/step: 24.00
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 0.52s
                        Total time: 247.37s
                               ETA: 754.6s

################################################################################
                     [1m Learning iteration 494/2000 [0m

                       Computation: 15810 steps/s (collection: 0.297s, learning 0.221s)
               Value function loss: 88917.8931
                    Surrogate loss: 0.0003
             Mean action noise std: 0.92
                       Mean reward: 8799.01
               Mean episode length: 388.61
                 Mean success rate: 81.00
                  Mean reward/step: 24.01
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4055040
                    Iteration time: 0.52s
                        Total time: 247.89s
                               ETA: 754.2s

################################################################################
                     [1m Learning iteration 495/2000 [0m

                       Computation: 16661 steps/s (collection: 0.272s, learning 0.220s)
               Value function loss: 66941.6689
                    Surrogate loss: 0.0170
             Mean action noise std: 0.92
                       Mean reward: 8841.81
               Mean episode length: 394.73
                 Mean success rate: 82.00
                  Mean reward/step: 23.51
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 0.49s
                        Total time: 248.38s
                               ETA: 753.7s

################################################################################
                     [1m Learning iteration 496/2000 [0m

                       Computation: 16677 steps/s (collection: 0.273s, learning 0.218s)
               Value function loss: 54711.7057
                    Surrogate loss: 0.0041
             Mean action noise std: 0.92
                       Mean reward: 8812.85
               Mean episode length: 393.26
                 Mean success rate: 82.00
                  Mean reward/step: 23.15
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4071424
                    Iteration time: 0.49s
                        Total time: 248.87s
                               ETA: 753.1s

################################################################################
                     [1m Learning iteration 497/2000 [0m

                       Computation: 14857 steps/s (collection: 0.313s, learning 0.239s)
               Value function loss: 92821.4062
                    Surrogate loss: 0.0019
             Mean action noise std: 0.92
                       Mean reward: 8462.49
               Mean episode length: 384.49
                 Mean success rate: 79.00
                  Mean reward/step: 21.47
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 0.55s
                        Total time: 249.42s
                               ETA: 752.8s

################################################################################
                     [1m Learning iteration 498/2000 [0m

                       Computation: 15710 steps/s (collection: 0.309s, learning 0.213s)
               Value function loss: 80389.4850
                    Surrogate loss: 0.0044
             Mean action noise std: 0.92
                       Mean reward: 8434.94
               Mean episode length: 378.60
                 Mean success rate: 77.50
                  Mean reward/step: 21.15
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4087808
                    Iteration time: 0.52s
                        Total time: 249.95s
                               ETA: 752.3s

################################################################################
                     [1m Learning iteration 499/2000 [0m

                       Computation: 15323 steps/s (collection: 0.286s, learning 0.248s)
               Value function loss: 61486.3447
                    Surrogate loss: 0.0081
             Mean action noise std: 0.92
                       Mean reward: 8488.01
               Mean episode length: 376.84
                 Mean success rate: 76.50
                  Mean reward/step: 21.03
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 0.53s
                        Total time: 250.48s
                               ETA: 751.9s

################################################################################
                     [1m Learning iteration 500/2000 [0m

                       Computation: 14548 steps/s (collection: 0.321s, learning 0.242s)
               Value function loss: 96078.2481
                    Surrogate loss: 0.0050
             Mean action noise std: 0.92
                       Mean reward: 8670.24
               Mean episode length: 384.27
                 Mean success rate: 78.00
                  Mean reward/step: 21.75
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4104192
                    Iteration time: 0.56s
                        Total time: 251.04s
                               ETA: 751.6s

################################################################################
                     [1m Learning iteration 501/2000 [0m

                       Computation: 14742 steps/s (collection: 0.312s, learning 0.244s)
               Value function loss: 58038.7595
                    Surrogate loss: -0.0015
             Mean action noise std: 0.92
                       Mean reward: 8374.70
               Mean episode length: 372.29
                 Mean success rate: 75.50
                  Mean reward/step: 23.07
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 0.56s
                        Total time: 251.60s
                               ETA: 751.3s

################################################################################
                     [1m Learning iteration 502/2000 [0m

                       Computation: 15467 steps/s (collection: 0.298s, learning 0.232s)
               Value function loss: 69310.6333
                    Surrogate loss: 0.0351
             Mean action noise std: 0.92
                       Mean reward: 8385.88
               Mean episode length: 371.92
                 Mean success rate: 75.50
                  Mean reward/step: 24.44
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4120576
                    Iteration time: 0.53s
                        Total time: 252.13s
                               ETA: 750.9s

################################################################################
                     [1m Learning iteration 503/2000 [0m

                       Computation: 15513 steps/s (collection: 0.309s, learning 0.219s)
               Value function loss: 72563.6647
                    Surrogate loss: 0.0028
             Mean action noise std: 0.92
                       Mean reward: 8022.99
               Mean episode length: 357.68
                 Mean success rate: 72.00
                  Mean reward/step: 24.48
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.53s
                        Total time: 252.66s
                               ETA: 750.5s

################################################################################
                     [1m Learning iteration 504/2000 [0m

                       Computation: 14861 steps/s (collection: 0.311s, learning 0.240s)
               Value function loss: 83066.0954
                    Surrogate loss: 0.0001
             Mean action noise std: 0.92
                       Mean reward: 8050.44
               Mean episode length: 357.55
                 Mean success rate: 72.50
                  Mean reward/step: 24.08
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4136960
                    Iteration time: 0.55s
                        Total time: 253.21s
                               ETA: 750.1s

################################################################################
                     [1m Learning iteration 505/2000 [0m

                       Computation: 15600 steps/s (collection: 0.302s, learning 0.223s)
               Value function loss: 42591.8956
                    Surrogate loss: 0.0184
             Mean action noise std: 0.92
                       Mean reward: 8213.69
               Mean episode length: 361.02
                 Mean success rate: 73.50
                  Mean reward/step: 24.63
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 0.53s
                        Total time: 253.73s
                               ETA: 749.7s

################################################################################
                     [1m Learning iteration 506/2000 [0m

                       Computation: 15359 steps/s (collection: 0.303s, learning 0.230s)
               Value function loss: 90863.3687
                    Surrogate loss: 0.0041
             Mean action noise std: 0.92
                       Mean reward: 8638.52
               Mean episode length: 374.12
                 Mean success rate: 76.50
                  Mean reward/step: 21.05
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4153344
                    Iteration time: 0.53s
                        Total time: 254.27s
                               ETA: 749.3s

################################################################################
                     [1m Learning iteration 507/2000 [0m

                       Computation: 14812 steps/s (collection: 0.329s, learning 0.224s)
               Value function loss: 87052.0891
                    Surrogate loss: 0.0000
             Mean action noise std: 0.91
                       Mean reward: 8577.43
               Mean episode length: 372.35
                 Mean success rate: 75.50
                  Mean reward/step: 20.47
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 0.55s
                        Total time: 254.82s
                               ETA: 748.9s

################################################################################
                     [1m Learning iteration 508/2000 [0m

                       Computation: 14960 steps/s (collection: 0.318s, learning 0.229s)
               Value function loss: 72597.2249
                    Surrogate loss: 0.0056
             Mean action noise std: 0.91
                       Mean reward: 8389.66
               Mean episode length: 366.04
                 Mean success rate: 74.00
                  Mean reward/step: 20.85
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4169728
                    Iteration time: 0.55s
                        Total time: 255.37s
                               ETA: 748.5s

################################################################################
                     [1m Learning iteration 509/2000 [0m

                       Computation: 15729 steps/s (collection: 0.285s, learning 0.236s)
               Value function loss: 56667.2290
                    Surrogate loss: 0.0075
             Mean action noise std: 0.91
                       Mean reward: 8521.87
               Mean episode length: 371.51
                 Mean success rate: 75.00
                  Mean reward/step: 21.89
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 0.52s
                        Total time: 255.89s
                               ETA: 748.1s

################################################################################
                     [1m Learning iteration 510/2000 [0m

                       Computation: 14648 steps/s (collection: 0.306s, learning 0.254s)
               Value function loss: 119880.3350
                    Surrogate loss: 0.0065
             Mean action noise std: 0.91
                       Mean reward: 8575.49
               Mean episode length: 377.18
                 Mean success rate: 76.50
                  Mean reward/step: 21.79
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4186112
                    Iteration time: 0.56s
                        Total time: 256.45s
                               ETA: 747.8s

################################################################################
                     [1m Learning iteration 511/2000 [0m

                       Computation: 16235 steps/s (collection: 0.276s, learning 0.229s)
               Value function loss: 41491.6637
                    Surrogate loss: 0.0037
             Mean action noise std: 0.91
                       Mean reward: 8474.85
               Mean episode length: 373.42
                 Mean success rate: 75.00
                  Mean reward/step: 21.42
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 0.50s
                        Total time: 256.95s
                               ETA: 747.3s

################################################################################
                     [1m Learning iteration 512/2000 [0m

                       Computation: 16164 steps/s (collection: 0.280s, learning 0.227s)
               Value function loss: 94092.9845
                    Surrogate loss: 0.0188
             Mean action noise std: 0.91
                       Mean reward: 8668.26
               Mean episode length: 380.94
                 Mean success rate: 77.00
                  Mean reward/step: 23.06
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4202496
                    Iteration time: 0.51s
                        Total time: 257.46s
                               ETA: 746.8s

################################################################################
                     [1m Learning iteration 513/2000 [0m

                       Computation: 16421 steps/s (collection: 0.283s, learning 0.216s)
               Value function loss: 98141.2414
                    Surrogate loss: 0.0016
             Mean action noise std: 0.91
                       Mean reward: 8652.43
               Mean episode length: 384.43
                 Mean success rate: 77.50
                  Mean reward/step: 22.42
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 0.50s
                        Total time: 257.96s
                               ETA: 746.3s

################################################################################
                     [1m Learning iteration 514/2000 [0m

                       Computation: 14999 steps/s (collection: 0.305s, learning 0.241s)
               Value function loss: 74480.5834
                    Surrogate loss: -0.0001
             Mean action noise std: 0.91
                       Mean reward: 8359.18
               Mean episode length: 374.61
                 Mean success rate: 76.50
                  Mean reward/step: 20.87
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4218880
                    Iteration time: 0.55s
                        Total time: 258.50s
                               ETA: 745.9s

################################################################################
                     [1m Learning iteration 515/2000 [0m

                       Computation: 16534 steps/s (collection: 0.287s, learning 0.208s)
               Value function loss: 61497.7132
                    Surrogate loss: 0.0040
             Mean action noise std: 0.91
                       Mean reward: 8383.76
               Mean episode length: 375.31
                 Mean success rate: 77.00
                  Mean reward/step: 21.08
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.50s
                        Total time: 259.00s
                               ETA: 745.4s

################################################################################
                     [1m Learning iteration 516/2000 [0m

                       Computation: 15942 steps/s (collection: 0.287s, learning 0.227s)
               Value function loss: 82822.2877
                    Surrogate loss: -0.0003
             Mean action noise std: 0.91
                       Mean reward: 8719.79
               Mean episode length: 384.87
                 Mean success rate: 79.00
                  Mean reward/step: 22.02
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4235264
                    Iteration time: 0.51s
                        Total time: 259.51s
                               ETA: 744.9s

################################################################################
                     [1m Learning iteration 517/2000 [0m

                       Computation: 15931 steps/s (collection: 0.285s, learning 0.229s)
               Value function loss: 62337.2448
                    Surrogate loss: 0.0062
             Mean action noise std: 0.91
                       Mean reward: 8905.93
               Mean episode length: 392.74
                 Mean success rate: 81.00
                  Mean reward/step: 22.89
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 0.51s
                        Total time: 260.03s
                               ETA: 744.4s

################################################################################
                     [1m Learning iteration 518/2000 [0m

                       Computation: 16866 steps/s (collection: 0.276s, learning 0.210s)
               Value function loss: 67743.1335
                    Surrogate loss: 0.0068
             Mean action noise std: 0.91
                       Mean reward: 9010.10
               Mean episode length: 394.76
                 Mean success rate: 81.50
                  Mean reward/step: 24.10
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4251648
                    Iteration time: 0.49s
                        Total time: 260.51s
                               ETA: 743.9s

################################################################################
                     [1m Learning iteration 519/2000 [0m

                       Computation: 16563 steps/s (collection: 0.275s, learning 0.220s)
               Value function loss: 80045.1127
                    Surrogate loss: -0.0025
             Mean action noise std: 0.91
                       Mean reward: 9134.68
               Mean episode length: 397.72
                 Mean success rate: 82.00
                  Mean reward/step: 24.62
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 0.49s
                        Total time: 261.01s
                               ETA: 743.4s

################################################################################
                     [1m Learning iteration 520/2000 [0m

                       Computation: 17084 steps/s (collection: 0.265s, learning 0.215s)
               Value function loss: 61324.7600
                    Surrogate loss: 0.0001
             Mean action noise std: 0.91
                       Mean reward: 9130.25
               Mean episode length: 398.99
                 Mean success rate: 83.00
                  Mean reward/step: 23.98
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4268032
                    Iteration time: 0.48s
                        Total time: 261.49s
                               ETA: 742.8s

################################################################################
                     [1m Learning iteration 521/2000 [0m

                       Computation: 16477 steps/s (collection: 0.289s, learning 0.208s)
               Value function loss: 64435.2781
                    Surrogate loss: 0.0003
             Mean action noise std: 0.92
                       Mean reward: 8947.16
               Mean episode length: 395.11
                 Mean success rate: 82.50
                  Mean reward/step: 24.27
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 0.50s
                        Total time: 261.98s
                               ETA: 742.3s

################################################################################
                     [1m Learning iteration 522/2000 [0m

                       Computation: 16036 steps/s (collection: 0.291s, learning 0.220s)
               Value function loss: 68343.5312
                    Surrogate loss: -0.0047
             Mean action noise std: 0.92
                       Mean reward: 8598.62
               Mean episode length: 385.50
                 Mean success rate: 80.50
                  Mean reward/step: 24.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4284416
                    Iteration time: 0.51s
                        Total time: 262.50s
                               ETA: 741.8s

################################################################################
                     [1m Learning iteration 523/2000 [0m

                       Computation: 16490 steps/s (collection: 0.279s, learning 0.217s)
               Value function loss: 72759.6941
                    Surrogate loss: -0.0037
             Mean action noise std: 0.91
                       Mean reward: 8885.30
               Mean episode length: 393.95
                 Mean success rate: 82.50
                  Mean reward/step: 24.92
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 0.50s
                        Total time: 262.99s
                               ETA: 741.3s

################################################################################
                     [1m Learning iteration 524/2000 [0m

                       Computation: 16088 steps/s (collection: 0.291s, learning 0.218s)
               Value function loss: 71015.1300
                    Surrogate loss: -0.0011
             Mean action noise std: 0.91
                       Mean reward: 9051.70
               Mean episode length: 402.05
                 Mean success rate: 83.50
                  Mean reward/step: 25.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4300800
                    Iteration time: 0.51s
                        Total time: 263.50s
                               ETA: 740.8s

################################################################################
                     [1m Learning iteration 525/2000 [0m

                       Computation: 16009 steps/s (collection: 0.283s, learning 0.229s)
               Value function loss: 61698.0415
                    Surrogate loss: -0.0030
             Mean action noise std: 0.91
                       Mean reward: 9139.50
               Mean episode length: 404.68
                 Mean success rate: 84.50
                  Mean reward/step: 25.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 0.51s
                        Total time: 264.01s
                               ETA: 740.3s

################################################################################
                     [1m Learning iteration 526/2000 [0m

                       Computation: 14815 steps/s (collection: 0.324s, learning 0.229s)
               Value function loss: 75651.6398
                    Surrogate loss: -0.0022
             Mean action noise std: 0.91
                       Mean reward: 9121.02
               Mean episode length: 402.71
                 Mean success rate: 84.00
                  Mean reward/step: 24.53
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4317184
                    Iteration time: 0.55s
                        Total time: 264.57s
                               ETA: 740.0s

################################################################################
                     [1m Learning iteration 527/2000 [0m

                       Computation: 16062 steps/s (collection: 0.284s, learning 0.226s)
               Value function loss: 60596.1632
                    Surrogate loss: -0.0039
             Mean action noise std: 0.91
                       Mean reward: 9027.42
               Mean episode length: 398.55
                 Mean success rate: 83.50
                  Mean reward/step: 24.90
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.51s
                        Total time: 265.08s
                               ETA: 739.5s

################################################################################
                     [1m Learning iteration 528/2000 [0m

                       Computation: 16577 steps/s (collection: 0.278s, learning 0.216s)
               Value function loss: 110030.0021
                    Surrogate loss: 0.0079
             Mean action noise std: 0.91
                       Mean reward: 9048.49
               Mean episode length: 397.64
                 Mean success rate: 83.00
                  Mean reward/step: 24.69
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4333568
                    Iteration time: 0.49s
                        Total time: 265.57s
                               ETA: 739.0s

################################################################################
                     [1m Learning iteration 529/2000 [0m

                       Computation: 15426 steps/s (collection: 0.316s, learning 0.215s)
               Value function loss: 160900.0027
                    Surrogate loss: 0.0169
             Mean action noise std: 0.91
                       Mean reward: 9290.28
               Mean episode length: 403.94
                 Mean success rate: 83.50
                  Mean reward/step: 23.64
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 0.53s
                        Total time: 266.10s
                               ETA: 738.6s

################################################################################
                     [1m Learning iteration 530/2000 [0m

                       Computation: 15992 steps/s (collection: 0.294s, learning 0.219s)
               Value function loss: 69546.6962
                    Surrogate loss: 0.0029
             Mean action noise std: 0.91
                       Mean reward: 9179.48
               Mean episode length: 395.10
                 Mean success rate: 83.50
                  Mean reward/step: 21.97
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4349952
                    Iteration time: 0.51s
                        Total time: 266.61s
                               ETA: 738.1s

################################################################################
                     [1m Learning iteration 531/2000 [0m

                       Computation: 15591 steps/s (collection: 0.289s, learning 0.236s)
               Value function loss: 74116.5020
                    Surrogate loss: 0.0103
             Mean action noise std: 0.91
                       Mean reward: 9478.61
               Mean episode length: 400.73
                 Mean success rate: 85.00
                  Mean reward/step: 21.95
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 0.53s
                        Total time: 267.14s
                               ETA: 737.6s

################################################################################
                     [1m Learning iteration 532/2000 [0m

                       Computation: 15603 steps/s (collection: 0.303s, learning 0.222s)
               Value function loss: 65620.7211
                    Surrogate loss: 0.0076
             Mean action noise std: 0.91
                       Mean reward: 9156.90
               Mean episode length: 387.94
                 Mean success rate: 82.50
                  Mean reward/step: 23.18
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4366336
                    Iteration time: 0.53s
                        Total time: 267.66s
                               ETA: 737.2s

################################################################################
                     [1m Learning iteration 533/2000 [0m

                       Computation: 16393 steps/s (collection: 0.280s, learning 0.220s)
               Value function loss: 51060.2302
                    Surrogate loss: 0.0107
             Mean action noise std: 0.91
                       Mean reward: 9174.98
               Mean episode length: 386.06
                 Mean success rate: 82.00
                  Mean reward/step: 24.02
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 0.50s
                        Total time: 268.16s
                               ETA: 736.7s

################################################################################
                     [1m Learning iteration 534/2000 [0m

                       Computation: 15040 steps/s (collection: 0.327s, learning 0.218s)
               Value function loss: 73431.2499
                    Surrogate loss: -0.0028
             Mean action noise std: 0.91
                       Mean reward: 9074.47
               Mean episode length: 382.35
                 Mean success rate: 81.00
                  Mean reward/step: 23.85
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4382720
                    Iteration time: 0.54s
                        Total time: 268.71s
                               ETA: 736.3s

################################################################################
                     [1m Learning iteration 535/2000 [0m

                       Computation: 15237 steps/s (collection: 0.315s, learning 0.223s)
               Value function loss: 72258.4136
                    Surrogate loss: -0.0012
             Mean action noise std: 0.91
                       Mean reward: 8933.00
               Mean episode length: 377.11
                 Mean success rate: 80.50
                  Mean reward/step: 22.97
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 0.54s
                        Total time: 269.25s
                               ETA: 735.9s

################################################################################
                     [1m Learning iteration 536/2000 [0m

                       Computation: 15805 steps/s (collection: 0.299s, learning 0.220s)
               Value function loss: 51643.4939
                    Surrogate loss: -0.0037
             Mean action noise std: 0.91
                       Mean reward: 9136.40
               Mean episode length: 384.11
                 Mean success rate: 82.50
                  Mean reward/step: 22.09
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4399104
                    Iteration time: 0.52s
                        Total time: 269.76s
                               ETA: 735.4s

################################################################################
                     [1m Learning iteration 537/2000 [0m

                       Computation: 16070 steps/s (collection: 0.297s, learning 0.213s)
               Value function loss: 44036.6948
                    Surrogate loss: -0.0023
             Mean action noise std: 0.91
                       Mean reward: 9067.07
               Mean episode length: 381.90
                 Mean success rate: 82.50
                  Mean reward/step: 24.26
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 0.51s
                        Total time: 270.27s
                               ETA: 735.0s

################################################################################
                     [1m Learning iteration 538/2000 [0m

                       Computation: 16298 steps/s (collection: 0.281s, learning 0.222s)
               Value function loss: 69872.9745
                    Surrogate loss: 0.0015
             Mean action noise std: 0.91
                       Mean reward: 9214.75
               Mean episode length: 384.63
                 Mean success rate: 83.50
                  Mean reward/step: 24.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4415488
                    Iteration time: 0.50s
                        Total time: 270.78s
                               ETA: 734.5s

################################################################################
                     [1m Learning iteration 539/2000 [0m

                       Computation: 15567 steps/s (collection: 0.300s, learning 0.226s)
               Value function loss: 56110.4213
                    Surrogate loss: 0.0036
             Mean action noise std: 0.91
                       Mean reward: 9065.91
               Mean episode length: 380.83
                 Mean success rate: 83.50
                  Mean reward/step: 23.85
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.53s
                        Total time: 271.30s
                               ETA: 734.0s

################################################################################
                     [1m Learning iteration 540/2000 [0m

                       Computation: 14625 steps/s (collection: 0.312s, learning 0.249s)
               Value function loss: 58528.8963
                    Surrogate loss: -0.0011
             Mean action noise std: 0.91
                       Mean reward: 8881.63
               Mean episode length: 371.34
                 Mean success rate: 81.50
                  Mean reward/step: 24.07
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4431872
                    Iteration time: 0.56s
                        Total time: 271.86s
                               ETA: 733.7s

################################################################################
                     [1m Learning iteration 541/2000 [0m

                       Computation: 15661 steps/s (collection: 0.295s, learning 0.228s)
               Value function loss: 113510.6371
                    Surrogate loss: -0.0034
             Mean action noise std: 0.91
                       Mean reward: 8864.24
               Mean episode length: 372.61
                 Mean success rate: 81.00
                  Mean reward/step: 24.66
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 0.52s
                        Total time: 272.39s
                               ETA: 733.2s

################################################################################
                     [1m Learning iteration 542/2000 [0m

                       Computation: 16432 steps/s (collection: 0.292s, learning 0.207s)
               Value function loss: 38481.2469
                    Surrogate loss: 0.0231
             Mean action noise std: 0.91
                       Mean reward: 9137.08
               Mean episode length: 383.37
                 Mean success rate: 83.00
                  Mean reward/step: 23.94
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 4448256
                    Iteration time: 0.50s
                        Total time: 272.88s
                               ETA: 732.7s

################################################################################
                     [1m Learning iteration 543/2000 [0m

                       Computation: 14701 steps/s (collection: 0.323s, learning 0.234s)
               Value function loss: 73540.3495
                    Surrogate loss: 0.0092
             Mean action noise std: 0.91
                       Mean reward: 9312.28
               Mean episode length: 388.39
                 Mean success rate: 84.00
                  Mean reward/step: 22.82
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 0.56s
                        Total time: 273.44s
                               ETA: 732.4s

################################################################################
                     [1m Learning iteration 544/2000 [0m

                       Computation: 16152 steps/s (collection: 0.284s, learning 0.223s)
               Value function loss: 106170.2266
                    Surrogate loss: -0.0012
             Mean action noise std: 0.91
                       Mean reward: 9422.19
               Mean episode length: 396.50
                 Mean success rate: 84.50
                  Mean reward/step: 21.36
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4464640
                    Iteration time: 0.51s
                        Total time: 273.95s
                               ETA: 731.9s

################################################################################
                     [1m Learning iteration 545/2000 [0m

                       Computation: 16188 steps/s (collection: 0.290s, learning 0.216s)
               Value function loss: 105312.1934
                    Surrogate loss: -0.0016
             Mean action noise std: 0.91
                       Mean reward: 9427.18
               Mean episode length: 396.75
                 Mean success rate: 82.50
                  Mean reward/step: 20.89
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 0.51s
                        Total time: 274.45s
                               ETA: 731.4s

################################################################################
                     [1m Learning iteration 546/2000 [0m

                       Computation: 17117 steps/s (collection: 0.266s, learning 0.212s)
               Value function loss: 78806.3526
                    Surrogate loss: -0.0058
             Mean action noise std: 0.91
                       Mean reward: 9475.59
               Mean episode length: 399.25
                 Mean success rate: 83.50
                  Mean reward/step: 20.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4481024
                    Iteration time: 0.48s
                        Total time: 274.93s
                               ETA: 730.8s

################################################################################
                     [1m Learning iteration 547/2000 [0m

                       Computation: 15455 steps/s (collection: 0.287s, learning 0.243s)
               Value function loss: 77459.2599
                    Surrogate loss: -0.0002
             Mean action noise std: 0.91
                       Mean reward: 9157.68
               Mean episode length: 388.32
                 Mean success rate: 81.50
                  Mean reward/step: 21.76
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 0.53s
                        Total time: 275.46s
                               ETA: 730.4s

################################################################################
                     [1m Learning iteration 548/2000 [0m

                       Computation: 15455 steps/s (collection: 0.294s, learning 0.236s)
               Value function loss: 56105.4986
                    Surrogate loss: 0.0030
             Mean action noise std: 0.91
                       Mean reward: 9151.83
               Mean episode length: 390.33
                 Mean success rate: 81.00
                  Mean reward/step: 22.00
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4497408
                    Iteration time: 0.53s
                        Total time: 275.99s
                               ETA: 729.9s

################################################################################
                     [1m Learning iteration 549/2000 [0m

                       Computation: 16513 steps/s (collection: 0.290s, learning 0.206s)
               Value function loss: 61095.4502
                    Surrogate loss: -0.0042
             Mean action noise std: 0.91
                       Mean reward: 9071.66
               Mean episode length: 389.92
                 Mean success rate: 80.00
                  Mean reward/step: 21.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 0.50s
                        Total time: 276.49s
                               ETA: 729.4s

################################################################################
                     [1m Learning iteration 550/2000 [0m

                       Computation: 16554 steps/s (collection: 0.286s, learning 0.209s)
               Value function loss: 81184.9438
                    Surrogate loss: 0.0003
             Mean action noise std: 0.91
                       Mean reward: 8921.26
               Mean episode length: 389.82
                 Mean success rate: 80.00
                  Mean reward/step: 21.79
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4513792
                    Iteration time: 0.49s
                        Total time: 276.98s
                               ETA: 728.9s

################################################################################
                     [1m Learning iteration 551/2000 [0m

                       Computation: 16608 steps/s (collection: 0.282s, learning 0.212s)
               Value function loss: 75790.4948
                    Surrogate loss: -0.0030
             Mean action noise std: 0.91
                       Mean reward: 8653.27
               Mean episode length: 380.13
                 Mean success rate: 78.00
                  Mean reward/step: 21.96
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.49s
                        Total time: 277.48s
                               ETA: 728.4s

################################################################################
                     [1m Learning iteration 552/2000 [0m

                       Computation: 16656 steps/s (collection: 0.283s, learning 0.209s)
               Value function loss: 53788.3347
                    Surrogate loss: 0.0001
             Mean action noise std: 0.91
                       Mean reward: 8508.69
               Mean episode length: 372.05
                 Mean success rate: 78.00
                  Mean reward/step: 21.91
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4530176
                    Iteration time: 0.49s
                        Total time: 277.97s
                               ETA: 727.8s

################################################################################
                     [1m Learning iteration 553/2000 [0m

                       Computation: 17360 steps/s (collection: 0.266s, learning 0.206s)
               Value function loss: 51303.8230
                    Surrogate loss: 0.0033
             Mean action noise std: 0.91
                       Mean reward: 8424.38
               Mean episode length: 369.36
                 Mean success rate: 78.50
                  Mean reward/step: 22.37
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 0.47s
                        Total time: 278.44s
                               ETA: 727.3s

################################################################################
                     [1m Learning iteration 554/2000 [0m

                       Computation: 16950 steps/s (collection: 0.275s, learning 0.209s)
               Value function loss: 80581.4779
                    Surrogate loss: -0.0034
             Mean action noise std: 0.91
                       Mean reward: 8403.21
               Mean episode length: 369.15
                 Mean success rate: 78.50
                  Mean reward/step: 22.87
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4546560
                    Iteration time: 0.48s
                        Total time: 278.92s
                               ETA: 726.7s

################################################################################
                     [1m Learning iteration 555/2000 [0m

                       Computation: 16915 steps/s (collection: 0.274s, learning 0.210s)
               Value function loss: 38308.0821
                    Surrogate loss: 0.0016
             Mean action noise std: 0.91
                       Mean reward: 7931.68
               Mean episode length: 354.86
                 Mean success rate: 75.50
                  Mean reward/step: 22.15
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 0.48s
                        Total time: 279.41s
                               ETA: 726.2s

################################################################################
                     [1m Learning iteration 556/2000 [0m

                       Computation: 15777 steps/s (collection: 0.287s, learning 0.232s)
               Value function loss: 59305.1936
                    Surrogate loss: 0.0021
             Mean action noise std: 0.91
                       Mean reward: 7830.87
               Mean episode length: 353.64
                 Mean success rate: 75.00
                  Mean reward/step: 23.08
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4562944
                    Iteration time: 0.52s
                        Total time: 279.93s
                               ETA: 725.7s

################################################################################
                     [1m Learning iteration 557/2000 [0m

                       Computation: 14607 steps/s (collection: 0.312s, learning 0.249s)
               Value function loss: 88708.8618
                    Surrogate loss: -0.0005
             Mean action noise std: 0.91
                       Mean reward: 8028.71
               Mean episode length: 360.12
                 Mean success rate: 76.00
                  Mean reward/step: 23.46
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 0.56s
                        Total time: 280.49s
                               ETA: 725.3s

################################################################################
                     [1m Learning iteration 558/2000 [0m

                       Computation: 15308 steps/s (collection: 0.313s, learning 0.222s)
               Value function loss: 58297.8006
                    Surrogate loss: -0.0012
             Mean action noise std: 0.91
                       Mean reward: 8061.88
               Mean episode length: 362.84
                 Mean success rate: 76.50
                  Mean reward/step: 24.15
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4579328
                    Iteration time: 0.54s
                        Total time: 281.02s
                               ETA: 724.9s

################################################################################
                     [1m Learning iteration 559/2000 [0m

                       Computation: 13667 steps/s (collection: 0.297s, learning 0.303s)
               Value function loss: 99488.8218
                    Surrogate loss: 0.0011
             Mean action noise std: 0.91
                       Mean reward: 8053.46
               Mean episode length: 360.57
                 Mean success rate: 76.50
                  Mean reward/step: 22.84
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 0.60s
                        Total time: 281.62s
                               ETA: 724.7s

################################################################################
                     [1m Learning iteration 560/2000 [0m

                       Computation: 12157 steps/s (collection: 0.412s, learning 0.262s)
               Value function loss: 105109.1314
                    Surrogate loss: 0.0187
             Mean action noise std: 0.91
                       Mean reward: 7911.61
               Mean episode length: 357.01
                 Mean success rate: 75.50
                  Mean reward/step: 22.27
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4595712
                    Iteration time: 0.67s
                        Total time: 282.30s
                               ETA: 724.6s

################################################################################
                     [1m Learning iteration 561/2000 [0m

                       Computation: 15821 steps/s (collection: 0.299s, learning 0.219s)
               Value function loss: 106543.7656
                    Surrogate loss: -0.0024
             Mean action noise std: 0.90
                       Mean reward: 8322.89
               Mean episode length: 370.97
                 Mean success rate: 77.50
                  Mean reward/step: 21.10
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 0.52s
                        Total time: 282.81s
                               ETA: 724.1s

################################################################################
                     [1m Learning iteration 562/2000 [0m

                       Computation: 15260 steps/s (collection: 0.304s, learning 0.233s)
               Value function loss: 65532.5308
                    Surrogate loss: -0.0000
             Mean action noise std: 0.90
                       Mean reward: 8278.10
               Mean episode length: 370.65
                 Mean success rate: 77.50
                  Mean reward/step: 20.47
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4612096
                    Iteration time: 0.54s
                        Total time: 283.35s
                               ETA: 723.7s

################################################################################
                     [1m Learning iteration 563/2000 [0m

                       Computation: 16042 steps/s (collection: 0.292s, learning 0.218s)
               Value function loss: 98454.4028
                    Surrogate loss: -0.0018
             Mean action noise std: 0.90
                       Mean reward: 8385.77
               Mean episode length: 372.17
                 Mean success rate: 78.50
                  Mean reward/step: 21.11
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.51s
                        Total time: 283.86s
                               ETA: 723.2s

################################################################################
                     [1m Learning iteration 564/2000 [0m

                       Computation: 15469 steps/s (collection: 0.302s, learning 0.228s)
               Value function loss: 67244.6166
                    Surrogate loss: -0.0052
             Mean action noise std: 0.90
                       Mean reward: 8405.26
               Mean episode length: 377.73
                 Mean success rate: 79.00
                  Mean reward/step: 22.34
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4628480
                    Iteration time: 0.53s
                        Total time: 284.39s
                               ETA: 722.8s

################################################################################
                     [1m Learning iteration 565/2000 [0m

                       Computation: 16903 steps/s (collection: 0.279s, learning 0.205s)
               Value function loss: 92084.5645
                    Surrogate loss: -0.0040
             Mean action noise std: 0.90
                       Mean reward: 8394.25
               Mean episode length: 374.67
                 Mean success rate: 79.00
                  Mean reward/step: 22.52
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 0.48s
                        Total time: 284.88s
                               ETA: 722.3s

################################################################################
                     [1m Learning iteration 566/2000 [0m

                       Computation: 17560 steps/s (collection: 0.267s, learning 0.199s)
               Value function loss: 96221.0372
                    Surrogate loss: -0.0014
             Mean action noise std: 0.90
                       Mean reward: 8683.78
               Mean episode length: 385.54
                 Mean success rate: 81.50
                  Mean reward/step: 21.86
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4644864
                    Iteration time: 0.47s
                        Total time: 285.34s
                               ETA: 721.7s

################################################################################
                     [1m Learning iteration 567/2000 [0m

                       Computation: 17696 steps/s (collection: 0.266s, learning 0.197s)
               Value function loss: 54924.3917
                    Surrogate loss: -0.0031
             Mean action noise std: 0.90
                       Mean reward: 8363.52
               Mean episode length: 376.14
                 Mean success rate: 80.00
                  Mean reward/step: 22.23
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 0.46s
                        Total time: 285.81s
                               ETA: 721.1s

################################################################################
                     [1m Learning iteration 568/2000 [0m

                       Computation: 17848 steps/s (collection: 0.256s, learning 0.203s)
               Value function loss: 55969.0225
                    Surrogate loss: -0.0049
             Mean action noise std: 0.90
                       Mean reward: 8160.62
               Mean episode length: 366.61
                 Mean success rate: 78.50
                  Mean reward/step: 22.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4661248
                    Iteration time: 0.46s
                        Total time: 286.26s
                               ETA: 720.4s

################################################################################
                     [1m Learning iteration 569/2000 [0m

                       Computation: 18138 steps/s (collection: 0.252s, learning 0.199s)
               Value function loss: 57803.8326
                    Surrogate loss: 0.0040
             Mean action noise std: 0.90
                       Mean reward: 7965.69
               Mean episode length: 355.75
                 Mean success rate: 77.00
                  Mean reward/step: 23.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 0.45s
                        Total time: 286.72s
                               ETA: 719.8s

################################################################################
                     [1m Learning iteration 570/2000 [0m

                       Computation: 18367 steps/s (collection: 0.246s, learning 0.200s)
               Value function loss: 48890.7627
                    Surrogate loss: 0.0149
             Mean action noise std: 0.90
                       Mean reward: 7953.86
               Mean episode length: 355.96
                 Mean success rate: 76.50
                  Mean reward/step: 23.96
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4677632
                    Iteration time: 0.45s
                        Total time: 287.16s
                               ETA: 719.2s

################################################################################
                     [1m Learning iteration 571/2000 [0m

                       Computation: 16853 steps/s (collection: 0.259s, learning 0.227s)
               Value function loss: 61178.9130
                    Surrogate loss: 0.0236
             Mean action noise std: 0.90
                       Mean reward: 7797.31
               Mean episode length: 349.61
                 Mean success rate: 76.00
                  Mean reward/step: 24.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 0.49s
                        Total time: 287.65s
                               ETA: 718.6s

################################################################################
                     [1m Learning iteration 572/2000 [0m

                       Computation: 16595 steps/s (collection: 0.276s, learning 0.217s)
               Value function loss: 58690.7614
                    Surrogate loss: -0.0030
             Mean action noise std: 0.90
                       Mean reward: 7850.64
               Mean episode length: 353.71
                 Mean success rate: 76.50
                  Mean reward/step: 23.64
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4694016
                    Iteration time: 0.49s
                        Total time: 288.14s
                               ETA: 718.1s

################################################################################
                     [1m Learning iteration 573/2000 [0m

                       Computation: 17305 steps/s (collection: 0.271s, learning 0.203s)
               Value function loss: 82449.5796
                    Surrogate loss: -0.0027
             Mean action noise std: 0.90
                       Mean reward: 7826.27
               Mean episode length: 349.32
                 Mean success rate: 75.50
                  Mean reward/step: 23.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 0.47s
                        Total time: 288.62s
                               ETA: 717.5s

################################################################################
                     [1m Learning iteration 574/2000 [0m

                       Computation: 17436 steps/s (collection: 0.257s, learning 0.213s)
               Value function loss: 71889.1382
                    Surrogate loss: -0.0019
             Mean action noise std: 0.90
                       Mean reward: 7750.63
               Mean episode length: 344.99
                 Mean success rate: 74.50
                  Mean reward/step: 23.36
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4710400
                    Iteration time: 0.47s
                        Total time: 289.09s
                               ETA: 716.9s

################################################################################
                     [1m Learning iteration 575/2000 [0m

                       Computation: 17056 steps/s (collection: 0.266s, learning 0.214s)
               Value function loss: 110044.1237
                    Surrogate loss: -0.0039
             Mean action noise std: 0.90
                       Mean reward: 7811.68
               Mean episode length: 349.61
                 Mean success rate: 75.50
                  Mean reward/step: 23.09
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.48s
                        Total time: 289.57s
                               ETA: 716.4s

################################################################################
                     [1m Learning iteration 576/2000 [0m

                       Computation: 16780 steps/s (collection: 0.283s, learning 0.205s)
               Value function loss: 163593.1240
                    Surrogate loss: -0.0028
             Mean action noise std: 0.90
                       Mean reward: 8238.52
               Mean episode length: 362.31
                 Mean success rate: 77.00
                  Mean reward/step: 21.90
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4726784
                    Iteration time: 0.49s
                        Total time: 290.05s
                               ETA: 715.8s

################################################################################
                     [1m Learning iteration 577/2000 [0m

                       Computation: 17426 steps/s (collection: 0.261s, learning 0.209s)
               Value function loss: 84572.2087
                    Surrogate loss: -0.0009
             Mean action noise std: 0.90
                       Mean reward: 8374.90
               Mean episode length: 366.96
                 Mean success rate: 78.00
                  Mean reward/step: 21.35
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 0.47s
                        Total time: 290.52s
                               ETA: 715.3s

################################################################################
                     [1m Learning iteration 578/2000 [0m

                       Computation: 17198 steps/s (collection: 0.265s, learning 0.211s)
               Value function loss: 90932.2130
                    Surrogate loss: -0.0033
             Mean action noise std: 0.90
                       Mean reward: 8531.75
               Mean episode length: 371.74
                 Mean success rate: 79.00
                  Mean reward/step: 22.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4743168
                    Iteration time: 0.48s
                        Total time: 291.00s
                               ETA: 714.7s

################################################################################
                     [1m Learning iteration 579/2000 [0m

                       Computation: 17024 steps/s (collection: 0.264s, learning 0.217s)
               Value function loss: 73070.3619
                    Surrogate loss: -0.0036
             Mean action noise std: 0.90
                       Mean reward: 8584.84
               Mean episode length: 373.93
                 Mean success rate: 79.00
                  Mean reward/step: 22.83
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 0.48s
                        Total time: 291.48s
                               ETA: 714.1s

################################################################################
                     [1m Learning iteration 580/2000 [0m

                       Computation: 16733 steps/s (collection: 0.262s, learning 0.228s)
               Value function loss: 60964.2096
                    Surrogate loss: -0.0004
             Mean action noise std: 0.90
                       Mean reward: 8906.02
               Mean episode length: 386.29
                 Mean success rate: 81.00
                  Mean reward/step: 22.85
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 4759552
                    Iteration time: 0.49s
                        Total time: 291.97s
                               ETA: 713.6s

################################################################################
                     [1m Learning iteration 581/2000 [0m

                       Computation: 15858 steps/s (collection: 0.299s, learning 0.218s)
               Value function loss: 85351.3839
                    Surrogate loss: -0.0033
             Mean action noise std: 0.90
                       Mean reward: 8949.35
               Mean episode length: 388.13
                 Mean success rate: 80.50
                  Mean reward/step: 22.75
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 0.52s
                        Total time: 292.49s
                               ETA: 713.1s

################################################################################
                     [1m Learning iteration 582/2000 [0m

                       Computation: 17275 steps/s (collection: 0.268s, learning 0.207s)
               Value function loss: 70847.6239
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 8932.92
               Mean episode length: 386.20
                 Mean success rate: 80.00
                  Mean reward/step: 22.24
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4775936
                    Iteration time: 0.47s
                        Total time: 292.96s
                               ETA: 712.6s

################################################################################
                     [1m Learning iteration 583/2000 [0m

                       Computation: 15871 steps/s (collection: 0.306s, learning 0.210s)
               Value function loss: 75825.2243
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 8491.76
               Mean episode length: 366.69
                 Mean success rate: 76.00
                  Mean reward/step: 22.69
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 0.52s
                        Total time: 293.48s
                               ETA: 712.1s

################################################################################
                     [1m Learning iteration 584/2000 [0m

                       Computation: 16269 steps/s (collection: 0.265s, learning 0.239s)
               Value function loss: 58705.8699
                    Surrogate loss: -0.0003
             Mean action noise std: 0.90
                       Mean reward: 8203.39
               Mean episode length: 353.13
                 Mean success rate: 73.00
                  Mean reward/step: 23.25
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4792320
                    Iteration time: 0.50s
                        Total time: 293.98s
                               ETA: 711.6s

################################################################################
                     [1m Learning iteration 585/2000 [0m

                       Computation: 16447 steps/s (collection: 0.286s, learning 0.212s)
               Value function loss: 108366.7040
                    Surrogate loss: -0.0015
             Mean action noise std: 0.90
                       Mean reward: 8071.99
               Mean episode length: 350.38
                 Mean success rate: 71.50
                  Mean reward/step: 23.50
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 0.50s
                        Total time: 294.48s
                               ETA: 711.1s

################################################################################
                     [1m Learning iteration 586/2000 [0m

                       Computation: 17314 steps/s (collection: 0.266s, learning 0.208s)
               Value function loss: 69458.2010
                    Surrogate loss: -0.0009
             Mean action noise std: 0.90
                       Mean reward: 7647.32
               Mean episode length: 336.81
                 Mean success rate: 69.50
                  Mean reward/step: 23.37
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4808704
                    Iteration time: 0.47s
                        Total time: 294.95s
                               ETA: 710.5s

################################################################################
                     [1m Learning iteration 587/2000 [0m

                       Computation: 17643 steps/s (collection: 0.259s, learning 0.206s)
               Value function loss: 99139.8702
                    Surrogate loss: 0.0048
             Mean action noise std: 0.90
                       Mean reward: 7479.91
               Mean episode length: 333.58
                 Mean success rate: 69.00
                  Mean reward/step: 23.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.46s
                        Total time: 295.42s
                               ETA: 709.9s

################################################################################
                     [1m Learning iteration 588/2000 [0m

                       Computation: 17825 steps/s (collection: 0.253s, learning 0.206s)
               Value function loss: 110855.0916
                    Surrogate loss: -0.0039
             Mean action noise std: 0.90
                       Mean reward: 7706.87
               Mean episode length: 341.42
                 Mean success rate: 71.00
                  Mean reward/step: 24.15
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4825088
                    Iteration time: 0.46s
                        Total time: 295.88s
                               ETA: 709.3s

################################################################################
                     [1m Learning iteration 589/2000 [0m

                       Computation: 18307 steps/s (collection: 0.246s, learning 0.202s)
               Value function loss: 46509.8447
                    Surrogate loss: -0.0057
             Mean action noise std: 0.90
                       Mean reward: 7370.79
               Mean episode length: 328.26
                 Mean success rate: 69.00
                  Mean reward/step: 24.57
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 0.45s
                        Total time: 296.32s
                               ETA: 708.7s

################################################################################
                     [1m Learning iteration 590/2000 [0m

                       Computation: 17779 steps/s (collection: 0.252s, learning 0.209s)
               Value function loss: 90447.3129
                    Surrogate loss: -0.0003
             Mean action noise std: 0.90
                       Mean reward: 7581.03
               Mean episode length: 337.95
                 Mean success rate: 71.00
                  Mean reward/step: 25.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4841472
                    Iteration time: 0.46s
                        Total time: 296.78s
                               ETA: 708.1s

################################################################################
                     [1m Learning iteration 591/2000 [0m

                       Computation: 17109 steps/s (collection: 0.274s, learning 0.204s)
               Value function loss: 137839.8920
                    Surrogate loss: -0.0034
             Mean action noise std: 0.90
                       Mean reward: 7997.38
               Mean episode length: 350.38
                 Mean success rate: 74.50
                  Mean reward/step: 24.39
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 0.48s
                        Total time: 297.26s
                               ETA: 707.5s

################################################################################
                     [1m Learning iteration 592/2000 [0m

                       Computation: 16696 steps/s (collection: 0.270s, learning 0.221s)
               Value function loss: 107191.4821
                    Surrogate loss: 0.0067
             Mean action noise std: 0.90
                       Mean reward: 8303.71
               Mean episode length: 358.41
                 Mean success rate: 76.00
                  Mean reward/step: 23.28
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4857856
                    Iteration time: 0.49s
                        Total time: 297.75s
                               ETA: 707.0s

################################################################################
                     [1m Learning iteration 593/2000 [0m

                       Computation: 17844 steps/s (collection: 0.257s, learning 0.203s)
               Value function loss: 80991.5494
                    Surrogate loss: -0.0036
             Mean action noise std: 0.90
                       Mean reward: 8476.84
               Mean episode length: 368.41
                 Mean success rate: 77.00
                  Mean reward/step: 23.98
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 0.46s
                        Total time: 298.21s
                               ETA: 706.4s

################################################################################
                     [1m Learning iteration 594/2000 [0m

                       Computation: 17468 steps/s (collection: 0.264s, learning 0.205s)
               Value function loss: 109585.0814
                    Surrogate loss: -0.0031
             Mean action noise std: 0.90
                       Mean reward: 8691.14
               Mean episode length: 378.99
                 Mean success rate: 79.00
                  Mean reward/step: 23.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4874240
                    Iteration time: 0.47s
                        Total time: 298.68s
                               ETA: 705.8s

################################################################################
                     [1m Learning iteration 595/2000 [0m

                       Computation: 17508 steps/s (collection: 0.263s, learning 0.205s)
               Value function loss: 66293.7470
                    Surrogate loss: -0.0031
             Mean action noise std: 0.90
                       Mean reward: 8791.73
               Mean episode length: 382.61
                 Mean success rate: 80.00
                  Mean reward/step: 23.62
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 0.47s
                        Total time: 299.15s
                               ETA: 705.2s

################################################################################
                     [1m Learning iteration 596/2000 [0m

                       Computation: 18128 steps/s (collection: 0.254s, learning 0.198s)
               Value function loss: 93546.6234
                    Surrogate loss: -0.0035
             Mean action noise std: 0.90
                       Mean reward: 9038.85
               Mean episode length: 385.24
                 Mean success rate: 82.50
                  Mean reward/step: 24.81
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4890624
                    Iteration time: 0.45s
                        Total time: 299.60s
                               ETA: 704.6s

################################################################################
                     [1m Learning iteration 597/2000 [0m

                       Computation: 16117 steps/s (collection: 0.275s, learning 0.233s)
               Value function loss: 73532.9805
                    Surrogate loss: 0.0020
             Mean action noise std: 0.90
                       Mean reward: 9097.98
               Mean episode length: 385.92
                 Mean success rate: 82.50
                  Mean reward/step: 25.21
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 0.51s
                        Total time: 300.11s
                               ETA: 704.1s

################################################################################
                     [1m Learning iteration 598/2000 [0m

                       Computation: 17562 steps/s (collection: 0.252s, learning 0.215s)
               Value function loss: 99781.8385
                    Surrogate loss: -0.0017
             Mean action noise std: 0.90
                       Mean reward: 9319.42
               Mean episode length: 393.62
                 Mean success rate: 82.50
                  Mean reward/step: 25.28
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4907008
                    Iteration time: 0.47s
                        Total time: 300.58s
                               ETA: 703.5s

################################################################################
                     [1m Learning iteration 599/2000 [0m

                       Computation: 16596 steps/s (collection: 0.273s, learning 0.220s)
               Value function loss: 84434.3757
                    Surrogate loss: -0.0041
             Mean action noise std: 0.90
                       Mean reward: 9525.65
               Mean episode length: 395.89
                 Mean success rate: 83.50
                  Mean reward/step: 24.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.49s
                        Total time: 301.07s
                               ETA: 703.0s

################################################################################
                     [1m Learning iteration 600/2000 [0m

                       Computation: 15767 steps/s (collection: 0.299s, learning 0.221s)
               Value function loss: 48885.0415
                    Surrogate loss: -0.0063
             Mean action noise std: 0.90
                       Mean reward: 8854.67
               Mean episode length: 372.06
                 Mean success rate: 78.50
                  Mean reward/step: 24.09
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4923392
                    Iteration time: 0.52s
                        Total time: 301.59s
                               ETA: 702.5s

################################################################################
                     [1m Learning iteration 601/2000 [0m

                       Computation: 16253 steps/s (collection: 0.262s, learning 0.242s)
               Value function loss: 124469.8799
                    Surrogate loss: 0.0005
             Mean action noise std: 0.90
                       Mean reward: 8959.29
               Mean episode length: 377.23
                 Mean success rate: 80.00
                  Mean reward/step: 24.05
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 0.50s
                        Total time: 302.09s
                               ETA: 702.0s

################################################################################
                     [1m Learning iteration 602/2000 [0m

                       Computation: 17493 steps/s (collection: 0.263s, learning 0.205s)
               Value function loss: 64191.1081
                    Surrogate loss: 0.0001
             Mean action noise std: 0.90
                       Mean reward: 9105.66
               Mean episode length: 382.61
                 Mean success rate: 81.50
                  Mean reward/step: 23.95
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4939776
                    Iteration time: 0.47s
                        Total time: 302.56s
                               ETA: 701.5s

################################################################################
                     [1m Learning iteration 603/2000 [0m

                       Computation: 17208 steps/s (collection: 0.259s, learning 0.217s)
               Value function loss: 74504.0115
                    Surrogate loss: 0.0031
             Mean action noise std: 0.90
                       Mean reward: 9414.39
               Mean episode length: 387.81
                 Mean success rate: 83.50
                  Mean reward/step: 25.17
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 0.48s
                        Total time: 303.04s
                               ETA: 700.9s

################################################################################
                     [1m Learning iteration 604/2000 [0m

                       Computation: 16970 steps/s (collection: 0.280s, learning 0.203s)
               Value function loss: 85522.5786
                    Surrogate loss: -0.0024
             Mean action noise std: 0.90
                       Mean reward: 8893.19
               Mean episode length: 364.38
                 Mean success rate: 79.00
                  Mean reward/step: 25.24
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4956160
                    Iteration time: 0.48s
                        Total time: 303.52s
                               ETA: 700.4s

################################################################################
                     [1m Learning iteration 605/2000 [0m

                       Computation: 16567 steps/s (collection: 0.281s, learning 0.213s)
               Value function loss: 73010.2711
                    Surrogate loss: -0.0045
             Mean action noise std: 0.90
                       Mean reward: 9246.31
               Mean episode length: 377.81
                 Mean success rate: 80.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 0.49s
                        Total time: 304.02s
                               ETA: 699.8s

################################################################################
                     [1m Learning iteration 606/2000 [0m

                       Computation: 16212 steps/s (collection: 0.293s, learning 0.212s)
               Value function loss: 101382.2250
                    Surrogate loss: -0.0000
             Mean action noise std: 0.90
                       Mean reward: 9072.07
               Mean episode length: 372.28
                 Mean success rate: 79.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4972544
                    Iteration time: 0.51s
                        Total time: 304.52s
                               ETA: 699.3s

################################################################################
                     [1m Learning iteration 607/2000 [0m

                       Computation: 16660 steps/s (collection: 0.284s, learning 0.207s)
               Value function loss: 90663.9857
                    Surrogate loss: 0.0044
             Mean action noise std: 0.90
                       Mean reward: 9205.11
               Mean episode length: 376.76
                 Mean success rate: 80.00
                  Mean reward/step: 25.07
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 0.49s
                        Total time: 305.01s
                               ETA: 698.8s

################################################################################
                     [1m Learning iteration 608/2000 [0m

                       Computation: 16943 steps/s (collection: 0.276s, learning 0.207s)
               Value function loss: 63020.9750
                    Surrogate loss: -0.0051
             Mean action noise std: 0.90
                       Mean reward: 9287.41
               Mean episode length: 381.18
                 Mean success rate: 81.00
                  Mean reward/step: 23.26
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4988928
                    Iteration time: 0.48s
                        Total time: 305.50s
                               ETA: 698.3s

################################################################################
                     [1m Learning iteration 609/2000 [0m

                       Computation: 16570 steps/s (collection: 0.283s, learning 0.212s)
               Value function loss: 87285.9540
                    Surrogate loss: 0.0002
             Mean action noise std: 0.90
                       Mean reward: 9733.65
               Mean episode length: 396.54
                 Mean success rate: 84.00
                  Mean reward/step: 23.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 0.49s
                        Total time: 305.99s
                               ETA: 697.8s

################################################################################
                     [1m Learning iteration 610/2000 [0m

                       Computation: 17075 steps/s (collection: 0.267s, learning 0.213s)
               Value function loss: 93442.4594
                    Surrogate loss: 0.0005
             Mean action noise std: 0.90
                       Mean reward: 9951.92
               Mean episode length: 402.76
                 Mean success rate: 85.50
                  Mean reward/step: 23.79
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5005312
                    Iteration time: 0.48s
                        Total time: 306.47s
                               ETA: 697.2s

################################################################################
                     [1m Learning iteration 611/2000 [0m

                       Computation: 17465 steps/s (collection: 0.261s, learning 0.208s)
               Value function loss: 45976.4751
                    Surrogate loss: 0.0005
             Mean action noise std: 0.90
                       Mean reward: 9885.47
               Mean episode length: 402.58
                 Mean success rate: 84.50
                  Mean reward/step: 23.85
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.47s
                        Total time: 306.94s
                               ETA: 696.6s

################################################################################
                     [1m Learning iteration 612/2000 [0m

                       Computation: 17265 steps/s (collection: 0.268s, learning 0.206s)
               Value function loss: 79253.8342
                    Surrogate loss: 0.0001
             Mean action noise std: 0.91
                       Mean reward: 9714.91
               Mean episode length: 395.70
                 Mean success rate: 82.50
                  Mean reward/step: 24.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5021696
                    Iteration time: 0.47s
                        Total time: 307.41s
                               ETA: 696.1s

################################################################################
                     [1m Learning iteration 613/2000 [0m

                       Computation: 17432 steps/s (collection: 0.260s, learning 0.210s)
               Value function loss: 77270.8559
                    Surrogate loss: 0.0078
             Mean action noise std: 0.91
                       Mean reward: 9554.61
               Mean episode length: 391.13
                 Mean success rate: 82.00
                  Mean reward/step: 24.91
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 0.47s
                        Total time: 307.88s
                               ETA: 695.5s

################################################################################
                     [1m Learning iteration 614/2000 [0m

                       Computation: 16552 steps/s (collection: 0.278s, learning 0.217s)
               Value function loss: 85990.4667
                    Surrogate loss: 0.0063
             Mean action noise std: 0.91
                       Mean reward: 9916.09
               Mean episode length: 403.82
                 Mean success rate: 84.50
                  Mean reward/step: 26.00
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5038080
                    Iteration time: 0.49s
                        Total time: 308.38s
                               ETA: 695.0s

################################################################################
                     [1m Learning iteration 615/2000 [0m

                       Computation: 16782 steps/s (collection: 0.273s, learning 0.216s)
               Value function loss: 116796.7041
                    Surrogate loss: 0.0014
             Mean action noise std: 0.91
                       Mean reward: 10402.83
               Mean episode length: 421.45
                 Mean success rate: 87.50
                  Mean reward/step: 25.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 0.49s
                        Total time: 308.87s
                               ETA: 694.4s

################################################################################
                     [1m Learning iteration 616/2000 [0m

                       Computation: 17190 steps/s (collection: 0.270s, learning 0.207s)
               Value function loss: 99365.4271
                    Surrogate loss: -0.0030
             Mean action noise std: 0.91
                       Mean reward: 10419.11
               Mean episode length: 419.64
                 Mean success rate: 88.00
                  Mean reward/step: 24.76
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5054464
                    Iteration time: 0.48s
                        Total time: 309.34s
                               ETA: 693.9s

################################################################################
                     [1m Learning iteration 617/2000 [0m

                       Computation: 16279 steps/s (collection: 0.273s, learning 0.230s)
               Value function loss: 85746.3551
                    Surrogate loss: 0.0073
             Mean action noise std: 0.91
                       Mean reward: 10227.75
               Mean episode length: 412.81
                 Mean success rate: 86.50
                  Mean reward/step: 24.47
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 0.50s
                        Total time: 309.85s
                               ETA: 693.4s

################################################################################
                     [1m Learning iteration 618/2000 [0m

                       Computation: 17211 steps/s (collection: 0.272s, learning 0.204s)
               Value function loss: 79092.9149
                    Surrogate loss: 0.0005
             Mean action noise std: 0.91
                       Mean reward: 9962.79
               Mean episode length: 402.81
                 Mean success rate: 85.00
                  Mean reward/step: 24.93
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5070848
                    Iteration time: 0.48s
                        Total time: 310.32s
                               ETA: 692.8s

################################################################################
                     [1m Learning iteration 619/2000 [0m

                       Computation: 17888 steps/s (collection: 0.255s, learning 0.203s)
               Value function loss: 84608.6242
                    Surrogate loss: 0.0000
             Mean action noise std: 0.91
                       Mean reward: 9509.20
               Mean episode length: 388.56
                 Mean success rate: 82.00
                  Mean reward/step: 25.20
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 0.46s
                        Total time: 310.78s
                               ETA: 692.2s

################################################################################
                     [1m Learning iteration 620/2000 [0m

                       Computation: 17409 steps/s (collection: 0.250s, learning 0.221s)
               Value function loss: 103856.4655
                    Surrogate loss: -0.0048
             Mean action noise std: 0.91
                       Mean reward: 9423.04
               Mean episode length: 385.01
                 Mean success rate: 81.50
                  Mean reward/step: 24.75
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5087232
                    Iteration time: 0.47s
                        Total time: 311.25s
                               ETA: 691.7s

################################################################################
                     [1m Learning iteration 621/2000 [0m

                       Computation: 17726 steps/s (collection: 0.257s, learning 0.206s)
               Value function loss: 101588.1454
                    Surrogate loss: -0.0012
             Mean action noise std: 0.91
                       Mean reward: 9542.89
               Mean episode length: 388.82
                 Mean success rate: 82.50
                  Mean reward/step: 24.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 0.46s
                        Total time: 311.71s
                               ETA: 691.1s

################################################################################
                     [1m Learning iteration 622/2000 [0m

                       Computation: 18357 steps/s (collection: 0.246s, learning 0.201s)
               Value function loss: 117311.5181
                    Surrogate loss: 0.0015
             Mean action noise std: 0.91
                       Mean reward: 9798.12
               Mean episode length: 394.94
                 Mean success rate: 83.00
                  Mean reward/step: 24.01
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5103616
                    Iteration time: 0.45s
                        Total time: 312.16s
                               ETA: 690.5s

################################################################################
                     [1m Learning iteration 623/2000 [0m

                       Computation: 17247 steps/s (collection: 0.265s, learning 0.210s)
               Value function loss: 85162.6361
                    Surrogate loss: -0.0041
             Mean action noise std: 0.91
                       Mean reward: 9583.46
               Mean episode length: 388.89
                 Mean success rate: 81.00
                  Mean reward/step: 24.37
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.47s
                        Total time: 312.63s
                               ETA: 689.9s

################################################################################
                     [1m Learning iteration 624/2000 [0m

                       Computation: 18049 steps/s (collection: 0.235s, learning 0.219s)
               Value function loss: 87225.3625
                    Surrogate loss: 0.0027
             Mean action noise std: 0.90
                       Mean reward: 9563.58
               Mean episode length: 388.53
                 Mean success rate: 81.00
                  Mean reward/step: 24.76
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5120000
                    Iteration time: 0.45s
                        Total time: 313.09s
                               ETA: 689.3s

################################################################################
                     [1m Learning iteration 625/2000 [0m

                       Computation: 17662 steps/s (collection: 0.251s, learning 0.213s)
               Value function loss: 62448.4906
                    Surrogate loss: 0.0008
             Mean action noise std: 0.91
                       Mean reward: 9403.01
               Mean episode length: 383.12
                 Mean success rate: 80.50
                  Mean reward/step: 24.99
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 0.46s
                        Total time: 313.55s
                               ETA: 688.7s

################################################################################
                     [1m Learning iteration 626/2000 [0m

                       Computation: 17562 steps/s (collection: 0.247s, learning 0.220s)
               Value function loss: 82781.7097
                    Surrogate loss: -0.0022
             Mean action noise std: 0.91
                       Mean reward: 9617.91
               Mean episode length: 389.55
                 Mean success rate: 82.00
                  Mean reward/step: 25.01
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5136384
                    Iteration time: 0.47s
                        Total time: 314.02s
                               ETA: 688.1s

################################################################################
                     [1m Learning iteration 627/2000 [0m

                       Computation: 17702 steps/s (collection: 0.243s, learning 0.219s)
               Value function loss: 91625.8680
                    Surrogate loss: -0.0010
             Mean action noise std: 0.91
                       Mean reward: 9899.13
               Mean episode length: 399.23
                 Mean success rate: 83.50
                  Mean reward/step: 25.41
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 0.46s
                        Total time: 314.48s
                               ETA: 687.6s

################################################################################
                     [1m Learning iteration 628/2000 [0m

                       Computation: 17827 steps/s (collection: 0.254s, learning 0.205s)
               Value function loss: 58647.5890
                    Surrogate loss: -0.0002
             Mean action noise std: 0.90
                       Mean reward: 10135.50
               Mean episode length: 409.02
                 Mean success rate: 84.00
                  Mean reward/step: 25.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5152768
                    Iteration time: 0.46s
                        Total time: 314.94s
                               ETA: 687.0s

################################################################################
                     [1m Learning iteration 629/2000 [0m

                       Computation: 16886 steps/s (collection: 0.256s, learning 0.229s)
               Value function loss: 95253.4512
                    Surrogate loss: -0.0048
             Mean action noise std: 0.90
                       Mean reward: 10611.30
               Mean episode length: 423.42
                 Mean success rate: 87.50
                  Mean reward/step: 25.44
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 0.49s
                        Total time: 315.43s
                               ETA: 686.4s

################################################################################
                     [1m Learning iteration 630/2000 [0m

                       Computation: 15862 steps/s (collection: 0.284s, learning 0.232s)
               Value function loss: 104891.8166
                    Surrogate loss: -0.0045
             Mean action noise std: 0.90
                       Mean reward: 10499.13
               Mean episode length: 417.40
                 Mean success rate: 86.00
                  Mean reward/step: 24.82
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5169152
                    Iteration time: 0.52s
                        Total time: 315.94s
                               ETA: 686.0s

################################################################################
                     [1m Learning iteration 631/2000 [0m

                       Computation: 16186 steps/s (collection: 0.260s, learning 0.246s)
               Value function loss: 87739.6270
                    Surrogate loss: 0.0060
             Mean action noise std: 0.90
                       Mean reward: 10372.80
               Mean episode length: 415.27
                 Mean success rate: 86.00
                  Mean reward/step: 25.54
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 0.51s
                        Total time: 316.45s
                               ETA: 685.5s

################################################################################
                     [1m Learning iteration 632/2000 [0m

                       Computation: 16804 steps/s (collection: 0.270s, learning 0.217s)
               Value function loss: 102349.5412
                    Surrogate loss: -0.0029
             Mean action noise std: 0.90
                       Mean reward: 10496.45
               Mean episode length: 419.01
                 Mean success rate: 86.50
                  Mean reward/step: 25.77
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5185536
                    Iteration time: 0.49s
                        Total time: 316.94s
                               ETA: 684.9s

################################################################################
                     [1m Learning iteration 633/2000 [0m

                       Computation: 16687 steps/s (collection: 0.285s, learning 0.206s)
               Value function loss: 93212.1469
                    Surrogate loss: -0.0047
             Mean action noise std: 0.90
                       Mean reward: 10192.35
               Mean episode length: 406.58
                 Mean success rate: 86.00
                  Mean reward/step: 25.04
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 0.49s
                        Total time: 317.43s
                               ETA: 684.4s

################################################################################
                     [1m Learning iteration 634/2000 [0m

                       Computation: 17596 steps/s (collection: 0.258s, learning 0.208s)
               Value function loss: 91305.7534
                    Surrogate loss: -0.0015
             Mean action noise std: 0.90
                       Mean reward: 10327.49
               Mean episode length: 406.19
                 Mean success rate: 86.50
                  Mean reward/step: 24.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5201920
                    Iteration time: 0.47s
                        Total time: 317.89s
                               ETA: 683.8s

################################################################################
                     [1m Learning iteration 635/2000 [0m

                       Computation: 17142 steps/s (collection: 0.256s, learning 0.222s)
               Value function loss: 117658.6097
                    Surrogate loss: 0.0068
             Mean action noise std: 0.91
                       Mean reward: 10626.24
               Mean episode length: 416.69
                 Mean success rate: 87.00
                  Mean reward/step: 23.82
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.48s
                        Total time: 318.37s
                               ETA: 683.3s

################################################################################
                     [1m Learning iteration 636/2000 [0m

                       Computation: 15160 steps/s (collection: 0.282s, learning 0.259s)
               Value function loss: 72073.6645
                    Surrogate loss: -0.0012
             Mean action noise std: 0.90
                       Mean reward: 10584.72
               Mean episode length: 414.46
                 Mean success rate: 85.50
                  Mean reward/step: 22.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5218304
                    Iteration time: 0.54s
                        Total time: 318.91s
                               ETA: 682.9s

################################################################################
                     [1m Learning iteration 637/2000 [0m

                       Computation: 16704 steps/s (collection: 0.285s, learning 0.205s)
               Value function loss: 121459.1838
                    Surrogate loss: -0.0000
             Mean action noise std: 0.90
                       Mean reward: 10401.75
               Mean episode length: 411.19
                 Mean success rate: 85.50
                  Mean reward/step: 23.64
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 0.49s
                        Total time: 319.40s
                               ETA: 682.4s

################################################################################
                     [1m Learning iteration 638/2000 [0m

                       Computation: 14432 steps/s (collection: 0.317s, learning 0.250s)
               Value function loss: 94554.1062
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 10173.44
               Mean episode length: 405.38
                 Mean success rate: 83.50
                  Mean reward/step: 24.29
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5234688
                    Iteration time: 0.57s
                        Total time: 319.97s
                               ETA: 682.0s

################################################################################
                     [1m Learning iteration 639/2000 [0m

                       Computation: 16287 steps/s (collection: 0.292s, learning 0.211s)
               Value function loss: 76956.4670
                    Surrogate loss: 0.0076
             Mean action noise std: 0.90
                       Mean reward: 10014.53
               Mean episode length: 403.15
                 Mean success rate: 83.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 0.50s
                        Total time: 320.47s
                               ETA: 681.5s

################################################################################
                     [1m Learning iteration 640/2000 [0m

                       Computation: 16715 steps/s (collection: 0.272s, learning 0.218s)
               Value function loss: 92728.1721
                    Surrogate loss: -0.0036
             Mean action noise std: 0.90
                       Mean reward: 10074.72
               Mean episode length: 404.29
                 Mean success rate: 84.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5251072
                    Iteration time: 0.49s
                        Total time: 320.96s
                               ETA: 681.0s

################################################################################
                     [1m Learning iteration 641/2000 [0m

                       Computation: 16778 steps/s (collection: 0.257s, learning 0.231s)
               Value function loss: 68072.7698
                    Surrogate loss: 0.0054
             Mean action noise std: 0.90
                       Mean reward: 10061.36
               Mean episode length: 403.24
                 Mean success rate: 84.00
                  Mean reward/step: 26.12
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 0.49s
                        Total time: 321.45s
                               ETA: 680.5s

################################################################################
                     [1m Learning iteration 642/2000 [0m

                       Computation: 17834 steps/s (collection: 0.258s, learning 0.202s)
               Value function loss: 48388.7386
                    Surrogate loss: -0.0001
             Mean action noise std: 0.91
                       Mean reward: 9902.56
               Mean episode length: 398.98
                 Mean success rate: 83.00
                  Mean reward/step: 26.37
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5267456
                    Iteration time: 0.46s
                        Total time: 321.91s
                               ETA: 679.9s

################################################################################
                     [1m Learning iteration 643/2000 [0m

                       Computation: 17258 steps/s (collection: 0.265s, learning 0.209s)
               Value function loss: 91209.1883
                    Surrogate loss: -0.0026
             Mean action noise std: 0.91
                       Mean reward: 10280.21
               Mean episode length: 415.65
                 Mean success rate: 84.50
                  Mean reward/step: 26.58
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 0.47s
                        Total time: 322.38s
                               ETA: 679.3s

################################################################################
                     [1m Learning iteration 644/2000 [0m

                       Computation: 17561 steps/s (collection: 0.256s, learning 0.211s)
               Value function loss: 81983.4456
                    Surrogate loss: -0.0038
             Mean action noise std: 0.91
                       Mean reward: 10089.87
               Mean episode length: 412.44
                 Mean success rate: 84.00
                  Mean reward/step: 26.87
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5283840
                    Iteration time: 0.47s
                        Total time: 322.85s
                               ETA: 678.7s

################################################################################
                     [1m Learning iteration 645/2000 [0m

                       Computation: 17918 steps/s (collection: 0.256s, learning 0.202s)
               Value function loss: 86687.0757
                    Surrogate loss: -0.0021
             Mean action noise std: 0.91
                       Mean reward: 9824.57
               Mean episode length: 402.63
                 Mean success rate: 82.50
                  Mean reward/step: 26.44
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 0.46s
                        Total time: 323.31s
                               ETA: 678.1s

################################################################################
                     [1m Learning iteration 646/2000 [0m

                       Computation: 16553 steps/s (collection: 0.278s, learning 0.217s)
               Value function loss: 79044.4596
                    Surrogate loss: -0.0039
             Mean action noise std: 0.90
                       Mean reward: 9522.94
               Mean episode length: 392.42
                 Mean success rate: 81.50
                  Mean reward/step: 25.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5300224
                    Iteration time: 0.49s
                        Total time: 323.80s
                               ETA: 677.6s

################################################################################
                     [1m Learning iteration 647/2000 [0m

                       Computation: 17608 steps/s (collection: 0.257s, learning 0.208s)
               Value function loss: 94546.8779
                    Surrogate loss: 0.0259
             Mean action noise std: 0.90
                       Mean reward: 9605.57
               Mean episode length: 393.39
                 Mean success rate: 82.00
                  Mean reward/step: 25.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.47s
                        Total time: 324.27s
                               ETA: 677.1s

################################################################################
                     [1m Learning iteration 648/2000 [0m

                       Computation: 17548 steps/s (collection: 0.261s, learning 0.206s)
               Value function loss: 121476.3491
                    Surrogate loss: -0.0018
             Mean action noise std: 0.90
                       Mean reward: 9579.55
               Mean episode length: 389.88
                 Mean success rate: 82.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5316608
                    Iteration time: 0.47s
                        Total time: 324.73s
                               ETA: 676.5s

################################################################################
                     [1m Learning iteration 649/2000 [0m

                       Computation: 15283 steps/s (collection: 0.278s, learning 0.258s)
               Value function loss: 102138.6234
                    Surrogate loss: 0.0031
             Mean action noise std: 0.90
                       Mean reward: 9846.82
               Mean episode length: 397.98
                 Mean success rate: 84.50
                  Mean reward/step: 24.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 0.54s
                        Total time: 325.27s
                               ETA: 676.1s

################################################################################
                     [1m Learning iteration 650/2000 [0m

                       Computation: 15001 steps/s (collection: 0.316s, learning 0.230s)
               Value function loss: 72338.5751
                    Surrogate loss: -0.0041
             Mean action noise std: 0.90
                       Mean reward: 9906.77
               Mean episode length: 400.47
                 Mean success rate: 84.50
                  Mean reward/step: 24.85
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5332992
                    Iteration time: 0.55s
                        Total time: 325.82s
                               ETA: 675.7s

################################################################################
                     [1m Learning iteration 651/2000 [0m

                       Computation: 16641 steps/s (collection: 0.280s, learning 0.212s)
               Value function loss: 109954.5169
                    Surrogate loss: -0.0026
             Mean action noise std: 0.90
                       Mean reward: 10052.40
               Mean episode length: 403.62
                 Mean success rate: 85.00
                  Mean reward/step: 24.85
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 0.49s
                        Total time: 326.31s
                               ETA: 675.1s

################################################################################
                     [1m Learning iteration 652/2000 [0m

                       Computation: 16570 steps/s (collection: 0.265s, learning 0.229s)
               Value function loss: 69499.2152
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 10178.99
               Mean episode length: 403.79
                 Mean success rate: 85.50
                  Mean reward/step: 25.43
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5349376
                    Iteration time: 0.49s
                        Total time: 326.80s
                               ETA: 674.6s

################################################################################
                     [1m Learning iteration 653/2000 [0m

                       Computation: 17309 steps/s (collection: 0.252s, learning 0.221s)
               Value function loss: 97155.7867
                    Surrogate loss: -0.0046
             Mean action noise std: 0.90
                       Mean reward: 10130.13
               Mean episode length: 402.68
                 Mean success rate: 86.00
                  Mean reward/step: 25.68
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 0.47s
                        Total time: 327.28s
                               ETA: 674.1s

################################################################################
                     [1m Learning iteration 654/2000 [0m

                       Computation: 17883 steps/s (collection: 0.247s, learning 0.211s)
               Value function loss: 96056.1730
                    Surrogate loss: 0.0023
             Mean action noise std: 0.90
                       Mean reward: 10124.00
               Mean episode length: 405.10
                 Mean success rate: 86.50
                  Mean reward/step: 25.84
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5365760
                    Iteration time: 0.46s
                        Total time: 327.73s
                               ETA: 673.5s

################################################################################
                     [1m Learning iteration 655/2000 [0m

                       Computation: 16602 steps/s (collection: 0.265s, learning 0.228s)
               Value function loss: 60648.3355
                    Surrogate loss: 0.0122
             Mean action noise std: 0.90
                       Mean reward: 10382.29
               Mean episode length: 411.38
                 Mean success rate: 87.50
                  Mean reward/step: 25.18
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 0.49s
                        Total time: 328.23s
                               ETA: 673.0s

################################################################################
                     [1m Learning iteration 656/2000 [0m

                       Computation: 16065 steps/s (collection: 0.275s, learning 0.235s)
               Value function loss: 79310.6491
                    Surrogate loss: -0.0020
             Mean action noise std: 0.90
                       Mean reward: 10604.17
               Mean episode length: 418.65
                 Mean success rate: 88.50
                  Mean reward/step: 26.32
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5382144
                    Iteration time: 0.51s
                        Total time: 328.74s
                               ETA: 672.5s

################################################################################
                     [1m Learning iteration 657/2000 [0m

                       Computation: 17211 steps/s (collection: 0.266s, learning 0.210s)
               Value function loss: 60874.2201
                    Surrogate loss: -0.0028
             Mean action noise std: 0.90
                       Mean reward: 10663.22
               Mean episode length: 420.90
                 Mean success rate: 89.00
                  Mean reward/step: 26.35
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 0.48s
                        Total time: 329.21s
                               ETA: 671.9s

################################################################################
                     [1m Learning iteration 658/2000 [0m

                       Computation: 17585 steps/s (collection: 0.253s, learning 0.213s)
               Value function loss: 58560.1177
                    Surrogate loss: -0.0005
             Mean action noise std: 0.90
                       Mean reward: 10753.42
               Mean episode length: 422.35
                 Mean success rate: 89.50
                  Mean reward/step: 26.89
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5398528
                    Iteration time: 0.47s
                        Total time: 329.68s
                               ETA: 671.4s

################################################################################
                     [1m Learning iteration 659/2000 [0m

                       Computation: 16847 steps/s (collection: 0.269s, learning 0.217s)
               Value function loss: 66615.2751
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 10780.63
               Mean episode length: 423.42
                 Mean success rate: 89.00
                  Mean reward/step: 27.16
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.49s
                        Total time: 330.17s
                               ETA: 670.8s

################################################################################
                     [1m Learning iteration 660/2000 [0m

                       Computation: 16594 steps/s (collection: 0.279s, learning 0.215s)
               Value function loss: 96105.6830
                    Surrogate loss: 0.0072
             Mean action noise std: 0.90
                       Mean reward: 10943.24
               Mean episode length: 426.82
                 Mean success rate: 89.00
                  Mean reward/step: 26.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5414912
                    Iteration time: 0.49s
                        Total time: 330.66s
                               ETA: 670.3s

################################################################################
                     [1m Learning iteration 661/2000 [0m

                       Computation: 16727 steps/s (collection: 0.272s, learning 0.218s)
               Value function loss: 89147.0534
                    Surrogate loss: 0.0035
             Mean action noise std: 0.91
                       Mean reward: 10904.56
               Mean episode length: 423.50
                 Mean success rate: 88.50
                  Mean reward/step: 25.64
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 0.49s
                        Total time: 331.15s
                               ETA: 669.8s

################################################################################
                     [1m Learning iteration 662/2000 [0m

                       Computation: 17115 steps/s (collection: 0.271s, learning 0.208s)
               Value function loss: 89515.3033
                    Surrogate loss: -0.0028
             Mean action noise std: 0.91
                       Mean reward: 10755.70
               Mean episode length: 419.93
                 Mean success rate: 88.00
                  Mean reward/step: 24.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5431296
                    Iteration time: 0.48s
                        Total time: 331.63s
                               ETA: 669.3s

################################################################################
                     [1m Learning iteration 663/2000 [0m

                       Computation: 17068 steps/s (collection: 0.269s, learning 0.211s)
               Value function loss: 91129.9714
                    Surrogate loss: -0.0028
             Mean action noise std: 0.91
                       Mean reward: 10793.41
               Mean episode length: 418.86
                 Mean success rate: 88.00
                  Mean reward/step: 24.19
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 0.48s
                        Total time: 332.11s
                               ETA: 668.7s

################################################################################
                     [1m Learning iteration 664/2000 [0m

                       Computation: 16297 steps/s (collection: 0.290s, learning 0.213s)
               Value function loss: 91291.9053
                    Surrogate loss: -0.0025
             Mean action noise std: 0.91
                       Mean reward: 10591.55
               Mean episode length: 410.07
                 Mean success rate: 87.00
                  Mean reward/step: 23.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5447680
                    Iteration time: 0.50s
                        Total time: 332.61s
                               ETA: 668.2s

################################################################################
                     [1m Learning iteration 665/2000 [0m

                       Computation: 16197 steps/s (collection: 0.301s, learning 0.205s)
               Value function loss: 87866.1938
                    Surrogate loss: -0.0057
             Mean action noise std: 0.91
                       Mean reward: 10445.63
               Mean episode length: 406.21
                 Mean success rate: 85.00
                  Mean reward/step: 24.25
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 0.51s
                        Total time: 333.12s
                               ETA: 667.7s

################################################################################
                     [1m Learning iteration 666/2000 [0m

                       Computation: 16239 steps/s (collection: 0.295s, learning 0.209s)
               Value function loss: 84782.7232
                    Surrogate loss: -0.0026
             Mean action noise std: 0.91
                       Mean reward: 10121.59
               Mean episode length: 394.44
                 Mean success rate: 82.50
                  Mean reward/step: 24.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5464064
                    Iteration time: 0.50s
                        Total time: 333.62s
                               ETA: 667.2s

################################################################################
                     [1m Learning iteration 667/2000 [0m

                       Computation: 16345 steps/s (collection: 0.289s, learning 0.212s)
               Value function loss: 107626.4923
                    Surrogate loss: 0.0011
             Mean action noise std: 0.91
                       Mean reward: 10144.05
               Mean episode length: 393.20
                 Mean success rate: 82.00
                  Mean reward/step: 24.18
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 0.50s
                        Total time: 334.12s
                               ETA: 666.7s

################################################################################
                     [1m Learning iteration 668/2000 [0m

                       Computation: 15935 steps/s (collection: 0.301s, learning 0.213s)
               Value function loss: 97880.5377
                    Surrogate loss: 0.0053
             Mean action noise std: 0.90
                       Mean reward: 9754.10
               Mean episode length: 379.52
                 Mean success rate: 80.00
                  Mean reward/step: 24.54
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5480448
                    Iteration time: 0.51s
                        Total time: 334.64s
                               ETA: 666.3s

################################################################################
                     [1m Learning iteration 669/2000 [0m

                       Computation: 17088 steps/s (collection: 0.265s, learning 0.214s)
               Value function loss: 83457.2686
                    Surrogate loss: 0.0098
             Mean action noise std: 0.91
                       Mean reward: 10026.89
               Mean episode length: 389.67
                 Mean success rate: 82.50
                  Mean reward/step: 24.06
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 0.48s
                        Total time: 335.12s
                               ETA: 665.7s

################################################################################
                     [1m Learning iteration 670/2000 [0m

                       Computation: 17570 steps/s (collection: 0.259s, learning 0.207s)
               Value function loss: 95336.5911
                    Surrogate loss: 0.0030
             Mean action noise std: 0.91
                       Mean reward: 9905.34
               Mean episode length: 386.42
                 Mean success rate: 81.50
                  Mean reward/step: 24.97
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5496832
                    Iteration time: 0.47s
                        Total time: 335.58s
                               ETA: 665.2s

################################################################################
                     [1m Learning iteration 671/2000 [0m

                       Computation: 17574 steps/s (collection: 0.260s, learning 0.206s)
               Value function loss: 51883.7705
                    Surrogate loss: 0.0048
             Mean action noise std: 0.91
                       Mean reward: 9983.62
               Mean episode length: 390.22
                 Mean success rate: 82.00
                  Mean reward/step: 25.74
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.47s
                        Total time: 336.05s
                               ETA: 664.6s

################################################################################
                     [1m Learning iteration 672/2000 [0m

                       Computation: 17969 steps/s (collection: 0.249s, learning 0.207s)
               Value function loss: 53937.7312
                    Surrogate loss: 0.0024
             Mean action noise std: 0.91
                       Mean reward: 10054.32
               Mean episode length: 394.09
                 Mean success rate: 81.50
                  Mean reward/step: 26.09
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5513216
                    Iteration time: 0.46s
                        Total time: 336.50s
                               ETA: 664.0s

################################################################################
                     [1m Learning iteration 673/2000 [0m

                       Computation: 17284 steps/s (collection: 0.271s, learning 0.203s)
               Value function loss: 75620.9461
                    Surrogate loss: 0.0081
             Mean action noise std: 0.91
                       Mean reward: 9794.17
               Mean episode length: 385.40
                 Mean success rate: 80.00
                  Mean reward/step: 26.03
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 0.47s
                        Total time: 336.98s
                               ETA: 663.5s

################################################################################
                     [1m Learning iteration 674/2000 [0m

                       Computation: 17622 steps/s (collection: 0.254s, learning 0.210s)
               Value function loss: 63066.2088
                    Surrogate loss: -0.0019
             Mean action noise std: 0.91
                       Mean reward: 9825.45
               Mean episode length: 386.65
                 Mean success rate: 80.50
                  Mean reward/step: 25.52
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 5529600
                    Iteration time: 0.46s
                        Total time: 337.44s
                               ETA: 662.9s

################################################################################
                     [1m Learning iteration 675/2000 [0m

                       Computation: 17316 steps/s (collection: 0.252s, learning 0.221s)
               Value function loss: 57214.7151
                    Surrogate loss: -0.0008
             Mean action noise std: 0.91
                       Mean reward: 9937.86
               Mean episode length: 391.88
                 Mean success rate: 82.00
                  Mean reward/step: 24.89
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 0.47s
                        Total time: 337.92s
                               ETA: 662.3s

################################################################################
                     [1m Learning iteration 676/2000 [0m

                       Computation: 17557 steps/s (collection: 0.263s, learning 0.204s)
               Value function loss: 118547.3229
                    Surrogate loss: 0.0034
             Mean action noise std: 0.91
                       Mean reward: 9658.38
               Mean episode length: 385.83
                 Mean success rate: 81.50
                  Mean reward/step: 25.09
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5545984
                    Iteration time: 0.47s
                        Total time: 338.38s
                               ETA: 661.8s

################################################################################
                     [1m Learning iteration 677/2000 [0m

                       Computation: 16562 steps/s (collection: 0.276s, learning 0.219s)
               Value function loss: 110932.4567
                    Surrogate loss: -0.0007
             Mean action noise std: 0.91
                       Mean reward: 9521.47
               Mean episode length: 383.97
                 Mean success rate: 81.00
                  Mean reward/step: 23.67
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 0.49s
                        Total time: 338.88s
                               ETA: 661.3s

################################################################################
                     [1m Learning iteration 678/2000 [0m

                       Computation: 17382 steps/s (collection: 0.267s, learning 0.204s)
               Value function loss: 80802.1127
                    Surrogate loss: 0.0049
             Mean action noise std: 0.91
                       Mean reward: 9577.80
               Mean episode length: 388.81
                 Mean success rate: 81.50
                  Mean reward/step: 23.78
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5562368
                    Iteration time: 0.47s
                        Total time: 339.35s
                               ETA: 660.7s

################################################################################
                     [1m Learning iteration 679/2000 [0m

                       Computation: 16688 steps/s (collection: 0.278s, learning 0.213s)
               Value function loss: 137041.3564
                    Surrogate loss: -0.0033
             Mean action noise std: 0.91
                       Mean reward: 9286.92
               Mean episode length: 378.17
                 Mean success rate: 80.00
                  Mean reward/step: 23.42
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 0.49s
                        Total time: 339.84s
                               ETA: 660.2s

################################################################################
                     [1m Learning iteration 680/2000 [0m

                       Computation: 17172 steps/s (collection: 0.265s, learning 0.212s)
               Value function loss: 94380.4666
                    Surrogate loss: -0.0057
             Mean action noise std: 0.91
                       Mean reward: 9140.23
               Mean episode length: 372.21
                 Mean success rate: 80.00
                  Mean reward/step: 23.08
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5578752
                    Iteration time: 0.48s
                        Total time: 340.32s
                               ETA: 659.6s

################################################################################
                     [1m Learning iteration 681/2000 [0m

                       Computation: 16906 steps/s (collection: 0.273s, learning 0.211s)
               Value function loss: 75105.3952
                    Surrogate loss: -0.0029
             Mean action noise std: 0.91
                       Mean reward: 9068.53
               Mean episode length: 367.79
                 Mean success rate: 80.00
                  Mean reward/step: 23.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 0.48s
                        Total time: 340.80s
                               ETA: 659.1s

################################################################################
                     [1m Learning iteration 682/2000 [0m

                       Computation: 16942 steps/s (collection: 0.271s, learning 0.212s)
               Value function loss: 107165.2395
                    Surrogate loss: -0.0044
             Mean action noise std: 0.91
                       Mean reward: 9082.11
               Mean episode length: 370.06
                 Mean success rate: 81.00
                  Mean reward/step: 23.61
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5595136
                    Iteration time: 0.48s
                        Total time: 341.28s
                               ETA: 658.6s

################################################################################
                     [1m Learning iteration 683/2000 [0m

                       Computation: 17967 steps/s (collection: 0.247s, learning 0.209s)
               Value function loss: 49563.8657
                    Surrogate loss: -0.0018
             Mean action noise std: 0.91
                       Mean reward: 9174.77
               Mean episode length: 373.81
                 Mean success rate: 82.00
                  Mean reward/step: 23.03
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.46s
                        Total time: 341.74s
                               ETA: 658.0s

################################################################################
                     [1m Learning iteration 684/2000 [0m

                       Computation: 17163 steps/s (collection: 0.271s, learning 0.207s)
               Value function loss: 92581.2493
                    Surrogate loss: 0.0005
             Mean action noise std: 0.91
                       Mean reward: 9379.91
               Mean episode length: 380.85
                 Mean success rate: 83.00
                  Mean reward/step: 22.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5611520
                    Iteration time: 0.48s
                        Total time: 342.22s
                               ETA: 657.5s

################################################################################
                     [1m Learning iteration 685/2000 [0m

                       Computation: 17167 steps/s (collection: 0.261s, learning 0.216s)
               Value function loss: 68176.5675
                    Surrogate loss: 0.0027
             Mean action noise std: 0.91
                       Mean reward: 9250.83
               Mean episode length: 372.96
                 Mean success rate: 83.00
                  Mean reward/step: 21.45
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 0.48s
                        Total time: 342.69s
                               ETA: 656.9s

################################################################################
                     [1m Learning iteration 686/2000 [0m

                       Computation: 16516 steps/s (collection: 0.275s, learning 0.221s)
               Value function loss: 49882.2147
                    Surrogate loss: 0.0081
             Mean action noise std: 0.91
                       Mean reward: 9201.44
               Mean episode length: 374.70
                 Mean success rate: 83.50
                  Mean reward/step: 21.10
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5627904
                    Iteration time: 0.50s
                        Total time: 343.19s
                               ETA: 656.4s

################################################################################
                     [1m Learning iteration 687/2000 [0m

                       Computation: 17588 steps/s (collection: 0.263s, learning 0.203s)
               Value function loss: 54828.7776
                    Surrogate loss: -0.0020
             Mean action noise std: 0.91
                       Mean reward: 9156.57
               Mean episode length: 372.41
                 Mean success rate: 84.00
                  Mean reward/step: 22.05
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 0.47s
                        Total time: 343.66s
                               ETA: 655.8s

################################################################################
                     [1m Learning iteration 688/2000 [0m

                       Computation: 17593 steps/s (collection: 0.259s, learning 0.206s)
               Value function loss: 65872.1606
                    Surrogate loss: -0.0034
             Mean action noise std: 0.91
                       Mean reward: 9258.18
               Mean episode length: 377.93
                 Mean success rate: 85.00
                  Mean reward/step: 23.71
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5644288
                    Iteration time: 0.47s
                        Total time: 344.12s
                               ETA: 655.3s

################################################################################
                     [1m Learning iteration 689/2000 [0m

                       Computation: 17802 steps/s (collection: 0.258s, learning 0.202s)
               Value function loss: 46145.1876
                    Surrogate loss: 0.0025
             Mean action noise std: 0.91
                       Mean reward: 9290.21
               Mean episode length: 380.37
                 Mean success rate: 86.00
                  Mean reward/step: 24.65
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 0.46s
                        Total time: 344.58s
                               ETA: 654.7s

################################################################################
                     [1m Learning iteration 690/2000 [0m

                       Computation: 17436 steps/s (collection: 0.263s, learning 0.207s)
               Value function loss: 54912.1519
                    Surrogate loss: 0.0044
             Mean action noise std: 0.91
                       Mean reward: 9227.94
               Mean episode length: 383.19
                 Mean success rate: 86.00
                  Mean reward/step: 25.78
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5660672
                    Iteration time: 0.47s
                        Total time: 345.05s
                               ETA: 654.1s

################################################################################
                     [1m Learning iteration 691/2000 [0m

                       Computation: 16485 steps/s (collection: 0.281s, learning 0.216s)
               Value function loss: 79281.6639
                    Surrogate loss: 0.0004
             Mean action noise std: 0.91
                       Mean reward: 9380.51
               Mean episode length: 389.95
                 Mean success rate: 86.50
                  Mean reward/step: 25.99
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 0.50s
                        Total time: 345.55s
                               ETA: 653.6s

################################################################################
                     [1m Learning iteration 692/2000 [0m

                       Computation: 16508 steps/s (collection: 0.283s, learning 0.213s)
               Value function loss: 87374.0057
                    Surrogate loss: 0.0201
             Mean action noise std: 0.91
                       Mean reward: 9613.45
               Mean episode length: 400.62
                 Mean success rate: 86.50
                  Mean reward/step: 25.27
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5677056
                    Iteration time: 0.50s
                        Total time: 346.04s
                               ETA: 653.1s

################################################################################
                     [1m Learning iteration 693/2000 [0m

                       Computation: 15551 steps/s (collection: 0.303s, learning 0.223s)
               Value function loss: 79030.1456
                    Surrogate loss: 0.0074
             Mean action noise std: 0.91
                       Mean reward: 9789.87
               Mean episode length: 408.78
                 Mean success rate: 87.50
                  Mean reward/step: 24.55
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 0.53s
                        Total time: 346.57s
                               ETA: 652.7s

################################################################################
                     [1m Learning iteration 694/2000 [0m

                       Computation: 16963 steps/s (collection: 0.271s, learning 0.212s)
               Value function loss: 78539.4446
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 9901.03
               Mean episode length: 412.79
                 Mean success rate: 88.00
                  Mean reward/step: 24.13
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5693440
                    Iteration time: 0.48s
                        Total time: 347.05s
                               ETA: 652.2s

################################################################################
                     [1m Learning iteration 695/2000 [0m

                       Computation: 15331 steps/s (collection: 0.319s, learning 0.215s)
               Value function loss: 102135.3948
                    Surrogate loss: 0.0038
             Mean action noise std: 0.91
                       Mean reward: 10159.40
               Mean episode length: 428.29
                 Mean success rate: 90.00
                  Mean reward/step: 24.13
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.53s
                        Total time: 347.59s
                               ETA: 651.7s

################################################################################
                     [1m Learning iteration 696/2000 [0m

                       Computation: 16290 steps/s (collection: 0.283s, learning 0.220s)
               Value function loss: 81544.1604
                    Surrogate loss: -0.0043
             Mean action noise std: 0.92
                       Mean reward: 10134.82
               Mean episode length: 429.11
                 Mean success rate: 89.50
                  Mean reward/step: 24.10
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5709824
                    Iteration time: 0.50s
                        Total time: 348.09s
                               ETA: 651.2s

################################################################################
                     [1m Learning iteration 697/2000 [0m

                       Computation: 16572 steps/s (collection: 0.270s, learning 0.224s)
               Value function loss: 95933.6544
                    Surrogate loss: -0.0049
             Mean action noise std: 0.92
                       Mean reward: 10302.72
               Mean episode length: 433.82
                 Mean success rate: 89.50
                  Mean reward/step: 24.39
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 0.49s
                        Total time: 348.59s
                               ETA: 650.7s

################################################################################
                     [1m Learning iteration 698/2000 [0m

                       Computation: 15538 steps/s (collection: 0.309s, learning 0.218s)
               Value function loss: 94186.2027
                    Surrogate loss: -0.0054
             Mean action noise std: 0.92
                       Mean reward: 10212.79
               Mean episode length: 430.47
                 Mean success rate: 89.00
                  Mean reward/step: 24.30
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5726208
                    Iteration time: 0.53s
                        Total time: 349.11s
                               ETA: 650.3s

################################################################################
                     [1m Learning iteration 699/2000 [0m

                       Computation: 16955 steps/s (collection: 0.272s, learning 0.211s)
               Value function loss: 56314.3829
                    Surrogate loss: -0.0014
             Mean action noise std: 0.92
                       Mean reward: 10102.23
               Mean episode length: 427.77
                 Mean success rate: 88.00
                  Mean reward/step: 25.22
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 0.48s
                        Total time: 349.60s
                               ETA: 649.7s

################################################################################
                     [1m Learning iteration 700/2000 [0m

                       Computation: 16503 steps/s (collection: 0.271s, learning 0.225s)
               Value function loss: 86366.5044
                    Surrogate loss: -0.0051
             Mean action noise std: 0.92
                       Mean reward: 10162.64
               Mean episode length: 427.38
                 Mean success rate: 88.50
                  Mean reward/step: 26.13
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5742592
                    Iteration time: 0.50s
                        Total time: 350.09s
                               ETA: 649.2s

################################################################################
                     [1m Learning iteration 701/2000 [0m

                       Computation: 16419 steps/s (collection: 0.287s, learning 0.212s)
               Value function loss: 73409.3379
                    Surrogate loss: -0.0035
             Mean action noise std: 0.92
                       Mean reward: 10401.79
               Mean episode length: 436.01
                 Mean success rate: 90.50
                  Mean reward/step: 25.19
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 0.50s
                        Total time: 350.59s
                               ETA: 648.7s

################################################################################
                     [1m Learning iteration 702/2000 [0m

                       Computation: 17579 steps/s (collection: 0.260s, learning 0.206s)
               Value function loss: 69963.1010
                    Surrogate loss: -0.0049
             Mean action noise std: 0.92
                       Mean reward: 10367.11
               Mean episode length: 432.58
                 Mean success rate: 90.50
                  Mean reward/step: 25.15
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5758976
                    Iteration time: 0.47s
                        Total time: 351.06s
                               ETA: 648.2s

################################################################################
                     [1m Learning iteration 703/2000 [0m

                       Computation: 17498 steps/s (collection: 0.263s, learning 0.206s)
               Value function loss: 49817.1891
                    Surrogate loss: 0.0015
             Mean action noise std: 0.92
                       Mean reward: 10125.35
               Mean episode length: 424.69
                 Mean success rate: 89.50
                  Mean reward/step: 26.03
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 0.47s
                        Total time: 351.53s
                               ETA: 647.6s

################################################################################
                     [1m Learning iteration 704/2000 [0m

                       Computation: 16909 steps/s (collection: 0.270s, learning 0.214s)
               Value function loss: 80319.5302
                    Surrogate loss: -0.0021
             Mean action noise std: 0.92
                       Mean reward: 9894.51
               Mean episode length: 413.70
                 Mean success rate: 87.50
                  Mean reward/step: 26.23
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5775360
                    Iteration time: 0.48s
                        Total time: 352.01s
                               ETA: 647.1s

################################################################################
                     [1m Learning iteration 705/2000 [0m

                       Computation: 16389 steps/s (collection: 0.266s, learning 0.234s)
               Value function loss: 46897.9035
                    Surrogate loss: -0.0023
             Mean action noise std: 0.92
                       Mean reward: 9777.67
               Mean episode length: 408.98
                 Mean success rate: 87.00
                  Mean reward/step: 26.91
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 0.50s
                        Total time: 352.51s
                               ETA: 646.6s

################################################################################
                     [1m Learning iteration 706/2000 [0m

                       Computation: 16536 steps/s (collection: 0.284s, learning 0.212s)
               Value function loss: 74482.8299
                    Surrogate loss: 0.0037
             Mean action noise std: 0.92
                       Mean reward: 9830.59
               Mean episode length: 405.25
                 Mean success rate: 87.50
                  Mean reward/step: 27.08
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5791744
                    Iteration time: 0.50s
                        Total time: 353.01s
                               ETA: 646.1s

################################################################################
                     [1m Learning iteration 707/2000 [0m

                       Computation: 17252 steps/s (collection: 0.267s, learning 0.208s)
               Value function loss: 116614.5283
                    Surrogate loss: -0.0031
             Mean action noise std: 0.92
                       Mean reward: 9867.65
               Mean episode length: 405.56
                 Mean success rate: 88.50
                  Mean reward/step: 26.76
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.47s
                        Total time: 353.48s
                               ETA: 645.6s

################################################################################
                     [1m Learning iteration 708/2000 [0m

                       Computation: 17001 steps/s (collection: 0.281s, learning 0.200s)
               Value function loss: 91037.6153
                    Surrogate loss: 0.0011
             Mean action noise std: 0.92
                       Mean reward: 9717.80
               Mean episode length: 400.64
                 Mean success rate: 86.50
                  Mean reward/step: 25.48
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5808128
                    Iteration time: 0.48s
                        Total time: 353.96s
                               ETA: 645.0s

################################################################################
                     [1m Learning iteration 709/2000 [0m

                       Computation: 15697 steps/s (collection: 0.273s, learning 0.249s)
               Value function loss: 83427.5980
                    Surrogate loss: 0.0033
             Mean action noise std: 0.92
                       Mean reward: 9969.56
               Mean episode length: 405.06
                 Mean success rate: 87.50
                  Mean reward/step: 25.31
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 0.52s
                        Total time: 354.48s
                               ETA: 644.6s

################################################################################
                     [1m Learning iteration 710/2000 [0m

                       Computation: 14572 steps/s (collection: 0.320s, learning 0.242s)
               Value function loss: 102892.0016
                    Surrogate loss: -0.0002
             Mean action noise std: 0.92
                       Mean reward: 10059.29
               Mean episode length: 405.37
                 Mean success rate: 86.50
                  Mean reward/step: 26.15
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5824512
                    Iteration time: 0.56s
                        Total time: 355.05s
                               ETA: 644.2s

################################################################################
                     [1m Learning iteration 711/2000 [0m

                       Computation: 13832 steps/s (collection: 0.323s, learning 0.270s)
               Value function loss: 86505.7936
                    Surrogate loss: -0.0029
             Mean action noise std: 0.92
                       Mean reward: 9929.58
               Mean episode length: 396.31
                 Mean success rate: 85.50
                  Mean reward/step: 25.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 0.59s
                        Total time: 355.64s
                               ETA: 643.8s

################################################################################
                     [1m Learning iteration 712/2000 [0m

                       Computation: 14262 steps/s (collection: 0.343s, learning 0.231s)
               Value function loss: 84422.2978
                    Surrogate loss: -0.0041
             Mean action noise std: 0.92
                       Mean reward: 10021.54
               Mean episode length: 398.04
                 Mean success rate: 85.00
                  Mean reward/step: 26.23
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5840896
                    Iteration time: 0.57s
                        Total time: 356.21s
                               ETA: 643.5s

################################################################################
                     [1m Learning iteration 713/2000 [0m

                       Computation: 16207 steps/s (collection: 0.303s, learning 0.202s)
               Value function loss: 109454.3959
                    Surrogate loss: -0.0045
             Mean action noise std: 0.92
                       Mean reward: 10290.63
               Mean episode length: 403.86
                 Mean success rate: 85.50
                  Mean reward/step: 25.79
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 0.51s
                        Total time: 356.72s
                               ETA: 643.0s

################################################################################
                     [1m Learning iteration 714/2000 [0m

                       Computation: 15901 steps/s (collection: 0.303s, learning 0.212s)
               Value function loss: 101124.5165
                    Surrogate loss: 0.0022
             Mean action noise std: 0.92
                       Mean reward: 10158.09
               Mean episode length: 398.90
                 Mean success rate: 85.00
                  Mean reward/step: 25.34
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5857280
                    Iteration time: 0.52s
                        Total time: 357.23s
                               ETA: 642.5s

################################################################################
                     [1m Learning iteration 715/2000 [0m

                       Computation: 16702 steps/s (collection: 0.275s, learning 0.215s)
               Value function loss: 69673.1868
                    Surrogate loss: -0.0002
             Mean action noise std: 0.92
                       Mean reward: 10343.31
               Mean episode length: 403.38
                 Mean success rate: 85.50
                  Mean reward/step: 25.80
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 0.49s
                        Total time: 357.72s
                               ETA: 642.0s

################################################################################
                     [1m Learning iteration 716/2000 [0m

                       Computation: 16923 steps/s (collection: 0.285s, learning 0.199s)
               Value function loss: 122571.1257
                    Surrogate loss: -0.0000
             Mean action noise std: 0.92
                       Mean reward: 10282.37
               Mean episode length: 398.14
                 Mean success rate: 85.00
                  Mean reward/step: 25.81
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5873664
                    Iteration time: 0.48s
                        Total time: 358.21s
                               ETA: 641.5s

################################################################################
                     [1m Learning iteration 717/2000 [0m

                       Computation: 15865 steps/s (collection: 0.303s, learning 0.213s)
               Value function loss: 102436.8687
                    Surrogate loss: -0.0042
             Mean action noise std: 0.92
                       Mean reward: 10277.35
               Mean episode length: 392.02
                 Mean success rate: 84.50
                  Mean reward/step: 25.34
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 0.52s
                        Total time: 358.72s
                               ETA: 641.0s

################################################################################
                     [1m Learning iteration 718/2000 [0m

                       Computation: 15198 steps/s (collection: 0.308s, learning 0.231s)
               Value function loss: 82070.9710
                    Surrogate loss: -0.0045
             Mean action noise std: 0.92
                       Mean reward: 10060.03
               Mean episode length: 384.96
                 Mean success rate: 83.50
                  Mean reward/step: 24.85
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5890048
                    Iteration time: 0.54s
                        Total time: 359.26s
                               ETA: 640.6s

################################################################################
                     [1m Learning iteration 719/2000 [0m

                       Computation: 15747 steps/s (collection: 0.302s, learning 0.219s)
               Value function loss: 90646.8867
                    Surrogate loss: 0.0421
             Mean action noise std: 0.93
                       Mean reward: 10388.02
               Mean episode length: 394.60
                 Mean success rate: 85.50
                  Mean reward/step: 25.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.52s
                        Total time: 359.78s
                               ETA: 640.1s

################################################################################
                     [1m Learning iteration 720/2000 [0m

                       Computation: 16233 steps/s (collection: 0.277s, learning 0.227s)
               Value function loss: 68626.0003
                    Surrogate loss: -0.0001
             Mean action noise std: 0.92
                       Mean reward: 10053.64
               Mean episode length: 383.96
                 Mean success rate: 84.00
                  Mean reward/step: 24.78
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5906432
                    Iteration time: 0.50s
                        Total time: 360.29s
                               ETA: 639.6s

################################################################################
                     [1m Learning iteration 721/2000 [0m

                       Computation: 17001 steps/s (collection: 0.270s, learning 0.212s)
               Value function loss: 56049.4986
                    Surrogate loss: 0.0086
             Mean action noise std: 0.92
                       Mean reward: 10201.86
               Mean episode length: 389.69
                 Mean success rate: 84.50
                  Mean reward/step: 24.99
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 0.48s
                        Total time: 360.77s
                               ETA: 639.1s

################################################################################
                     [1m Learning iteration 722/2000 [0m

                       Computation: 17756 steps/s (collection: 0.258s, learning 0.204s)
               Value function loss: 60955.3542
                    Surrogate loss: -0.0020
             Mean action noise std: 0.93
                       Mean reward: 10033.52
               Mean episode length: 383.85
                 Mean success rate: 84.00
                  Mean reward/step: 25.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5922816
                    Iteration time: 0.46s
                        Total time: 361.23s
                               ETA: 638.5s

################################################################################
                     [1m Learning iteration 723/2000 [0m

                       Computation: 17687 steps/s (collection: 0.262s, learning 0.201s)
               Value function loss: 125747.4751
                    Surrogate loss: -0.0023
             Mean action noise std: 0.93
                       Mean reward: 10238.63
               Mean episode length: 391.72
                 Mean success rate: 86.00
                  Mean reward/step: 24.78
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 0.46s
                        Total time: 361.69s
                               ETA: 638.0s

################################################################################
                     [1m Learning iteration 724/2000 [0m

                       Computation: 17912 steps/s (collection: 0.255s, learning 0.202s)
               Value function loss: 76992.9914
                    Surrogate loss: -0.0013
             Mean action noise std: 0.93
                       Mean reward: 10247.59
               Mean episode length: 392.82
                 Mean success rate: 85.00
                  Mean reward/step: 24.09
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5939200
                    Iteration time: 0.46s
                        Total time: 362.15s
                               ETA: 637.4s

################################################################################
                     [1m Learning iteration 725/2000 [0m

                       Computation: 16405 steps/s (collection: 0.266s, learning 0.234s)
               Value function loss: 51961.9195
                    Surrogate loss: -0.0050
             Mean action noise std: 0.92
                       Mean reward: 9903.23
               Mean episode length: 382.40
                 Mean success rate: 83.00
                  Mean reward/step: 24.96
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 0.50s
                        Total time: 362.65s
                               ETA: 636.9s

################################################################################
                     [1m Learning iteration 726/2000 [0m

                       Computation: 15543 steps/s (collection: 0.296s, learning 0.231s)
               Value function loss: 97055.7947
                    Surrogate loss: 0.0008
             Mean action noise std: 0.92
                       Mean reward: 10485.97
               Mean episode length: 405.04
                 Mean success rate: 85.50
                  Mean reward/step: 24.88
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5955584
                    Iteration time: 0.53s
                        Total time: 363.18s
                               ETA: 636.4s

################################################################################
                     [1m Learning iteration 727/2000 [0m

                       Computation: 14001 steps/s (collection: 0.329s, learning 0.256s)
               Value function loss: 75245.1507
                    Surrogate loss: 0.0016
             Mean action noise std: 0.92
                       Mean reward: 10538.18
               Mean episode length: 408.76
                 Mean success rate: 86.50
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 0.59s
                        Total time: 363.76s
                               ETA: 636.1s

################################################################################
                     [1m Learning iteration 728/2000 [0m

                       Computation: 15120 steps/s (collection: 0.302s, learning 0.240s)
               Value function loss: 72784.6422
                    Surrogate loss: -0.0026
             Mean action noise std: 0.92
                       Mean reward: 10710.87
               Mean episode length: 415.82
                 Mean success rate: 87.50
                  Mean reward/step: 25.03
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5971968
                    Iteration time: 0.54s
                        Total time: 364.30s
                               ETA: 635.7s

################################################################################
                     [1m Learning iteration 729/2000 [0m

                       Computation: 15250 steps/s (collection: 0.299s, learning 0.238s)
               Value function loss: 77158.0046
                    Surrogate loss: 0.0066
             Mean action noise std: 0.92
                       Mean reward: 10577.00
               Mean episode length: 414.73
                 Mean success rate: 87.00
                  Mean reward/step: 24.21
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 0.54s
                        Total time: 364.84s
                               ETA: 635.2s

################################################################################
                     [1m Learning iteration 730/2000 [0m

                       Computation: 15445 steps/s (collection: 0.265s, learning 0.265s)
               Value function loss: 65088.9399
                    Surrogate loss: 0.0022
             Mean action noise std: 0.93
                       Mean reward: 10643.54
               Mean episode length: 418.82
                 Mean success rate: 87.50
                  Mean reward/step: 24.27
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 5988352
                    Iteration time: 0.53s
                        Total time: 365.37s
                               ETA: 634.8s

################################################################################
                     [1m Learning iteration 731/2000 [0m

                       Computation: 15229 steps/s (collection: 0.320s, learning 0.218s)
               Value function loss: 85369.1107
                    Surrogate loss: -0.0000
             Mean action noise std: 0.93
                       Mean reward: 10443.15
               Mean episode length: 415.75
                 Mean success rate: 87.00
                  Mean reward/step: 25.19
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.54s
                        Total time: 365.91s
                               ETA: 634.3s

################################################################################
                     [1m Learning iteration 732/2000 [0m

                       Computation: 16841 steps/s (collection: 0.268s, learning 0.218s)
               Value function loss: 96843.7770
                    Surrogate loss: -0.0052
             Mean action noise std: 0.93
                       Mean reward: 10665.80
               Mean episode length: 426.36
                 Mean success rate: 88.50
                  Mean reward/step: 24.96
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6004736
                    Iteration time: 0.49s
                        Total time: 366.40s
                               ETA: 633.8s

################################################################################
                     [1m Learning iteration 733/2000 [0m

                       Computation: 17384 steps/s (collection: 0.263s, learning 0.208s)
               Value function loss: 67958.2916
                    Surrogate loss: -0.0039
             Mean action noise std: 0.93
                       Mean reward: 10506.81
               Mean episode length: 420.81
                 Mean success rate: 88.00
                  Mean reward/step: 24.73
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 0.47s
                        Total time: 366.87s
                               ETA: 633.3s

################################################################################
                     [1m Learning iteration 734/2000 [0m

                       Computation: 16685 steps/s (collection: 0.274s, learning 0.217s)
               Value function loss: 71016.9165
                    Surrogate loss: -0.0010
             Mean action noise std: 0.93
                       Mean reward: 10613.91
               Mean episode length: 424.81
                 Mean success rate: 89.00
                  Mean reward/step: 24.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6021120
                    Iteration time: 0.49s
                        Total time: 367.36s
                               ETA: 632.8s

################################################################################
                     [1m Learning iteration 735/2000 [0m

                       Computation: 17040 steps/s (collection: 0.264s, learning 0.217s)
               Value function loss: 70080.2198
                    Surrogate loss: -0.0037
             Mean action noise std: 0.93
                       Mean reward: 10611.51
               Mean episode length: 427.30
                 Mean success rate: 89.50
                  Mean reward/step: 25.53
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 0.48s
                        Total time: 367.84s
                               ETA: 632.2s

################################################################################
                     [1m Learning iteration 736/2000 [0m

                       Computation: 17014 steps/s (collection: 0.266s, learning 0.215s)
               Value function loss: 53479.7185
                    Surrogate loss: -0.0016
             Mean action noise std: 0.93
                       Mean reward: 10686.71
               Mean episode length: 429.51
                 Mean success rate: 90.00
                  Mean reward/step: 26.22
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6037504
                    Iteration time: 0.48s
                        Total time: 368.32s
                               ETA: 631.7s

################################################################################
                     [1m Learning iteration 737/2000 [0m

                       Computation: 18196 steps/s (collection: 0.247s, learning 0.203s)
               Value function loss: 83466.8576
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 10692.00
               Mean episode length: 429.90
                 Mean success rate: 89.50
                  Mean reward/step: 26.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 0.45s
                        Total time: 368.77s
                               ETA: 631.1s

################################################################################
                     [1m Learning iteration 738/2000 [0m

                       Computation: 17151 steps/s (collection: 0.268s, learning 0.210s)
               Value function loss: 92465.3993
                    Surrogate loss: 0.0166
             Mean action noise std: 0.93
                       Mean reward: 10751.69
               Mean episode length: 432.73
                 Mean success rate: 89.50
                  Mean reward/step: 26.34
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6053888
                    Iteration time: 0.48s
                        Total time: 369.25s
                               ETA: 630.6s

################################################################################
                     [1m Learning iteration 739/2000 [0m

                       Computation: 16938 steps/s (collection: 0.271s, learning 0.212s)
               Value function loss: 100608.9805
                    Surrogate loss: -0.0041
             Mean action noise std: 0.93
                       Mean reward: 10651.90
               Mean episode length: 429.55
                 Mean success rate: 89.00
                  Mean reward/step: 25.51
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 0.48s
                        Total time: 369.73s
                               ETA: 630.0s

################################################################################
                     [1m Learning iteration 740/2000 [0m

                       Computation: 16768 steps/s (collection: 0.279s, learning 0.209s)
               Value function loss: 81354.4399
                    Surrogate loss: -0.0047
             Mean action noise std: 0.93
                       Mean reward: 10280.92
               Mean episode length: 413.56
                 Mean success rate: 87.50
                  Mean reward/step: 24.74
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6070272
                    Iteration time: 0.49s
                        Total time: 370.22s
                               ETA: 629.5s

################################################################################
                     [1m Learning iteration 741/2000 [0m

                       Computation: 17183 steps/s (collection: 0.278s, learning 0.199s)
               Value function loss: 110083.4680
                    Surrogate loss: -0.0009
             Mean action noise std: 0.92
                       Mean reward: 10025.30
               Mean episode length: 400.25
                 Mean success rate: 84.50
                  Mean reward/step: 25.07
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 0.48s
                        Total time: 370.70s
                               ETA: 629.0s

################################################################################
                     [1m Learning iteration 742/2000 [0m

                       Computation: 16515 steps/s (collection: 0.282s, learning 0.214s)
               Value function loss: 99803.7423
                    Surrogate loss: -0.0038
             Mean action noise std: 0.93
                       Mean reward: 9633.54
               Mean episode length: 384.63
                 Mean success rate: 81.50
                  Mean reward/step: 24.37
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6086656
                    Iteration time: 0.50s
                        Total time: 371.19s
                               ETA: 628.5s

################################################################################
                     [1m Learning iteration 743/2000 [0m

                       Computation: 17086 steps/s (collection: 0.269s, learning 0.211s)
               Value function loss: 96850.8014
                    Surrogate loss: -0.0049
             Mean action noise std: 0.92
                       Mean reward: 9141.67
               Mean episode length: 367.55
                 Mean success rate: 78.50
                  Mean reward/step: 24.44
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.48s
                        Total time: 371.67s
                               ETA: 627.9s

################################################################################
                     [1m Learning iteration 744/2000 [0m

                       Computation: 15657 steps/s (collection: 0.297s, learning 0.226s)
               Value function loss: 84359.2543
                    Surrogate loss: -0.0051
             Mean action noise std: 0.92
                       Mean reward: 8955.59
               Mean episode length: 359.25
                 Mean success rate: 77.50
                  Mean reward/step: 24.83
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6103040
                    Iteration time: 0.52s
                        Total time: 372.20s
                               ETA: 627.5s

################################################################################
                     [1m Learning iteration 745/2000 [0m

                       Computation: 16477 steps/s (collection: 0.264s, learning 0.234s)
               Value function loss: 86006.1862
                    Surrogate loss: 0.0018
             Mean action noise std: 0.92
                       Mean reward: 8778.06
               Mean episode length: 354.05
                 Mean success rate: 77.50
                  Mean reward/step: 24.94
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 0.50s
                        Total time: 372.69s
                               ETA: 627.0s

################################################################################
                     [1m Learning iteration 746/2000 [0m

                       Computation: 17382 steps/s (collection: 0.268s, learning 0.203s)
               Value function loss: 54668.2217
                    Surrogate loss: -0.0001
             Mean action noise std: 0.92
                       Mean reward: 8868.78
               Mean episode length: 357.66
                 Mean success rate: 78.00
                  Mean reward/step: 25.99
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6119424
                    Iteration time: 0.47s
                        Total time: 373.16s
                               ETA: 626.4s

################################################################################
                     [1m Learning iteration 747/2000 [0m

                       Computation: 16341 steps/s (collection: 0.285s, learning 0.216s)
               Value function loss: 94415.6824
                    Surrogate loss: -0.0034
             Mean action noise std: 0.92
                       Mean reward: 8746.23
               Mean episode length: 353.05
                 Mean success rate: 77.50
                  Mean reward/step: 26.04
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 0.50s
                        Total time: 373.67s
                               ETA: 625.9s

################################################################################
                     [1m Learning iteration 748/2000 [0m

                       Computation: 17079 steps/s (collection: 0.264s, learning 0.216s)
               Value function loss: 86110.3703
                    Surrogate loss: -0.0040
             Mean action noise std: 0.93
                       Mean reward: 9004.61
               Mean episode length: 361.90
                 Mean success rate: 78.50
                  Mean reward/step: 25.90
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6135808
                    Iteration time: 0.48s
                        Total time: 374.15s
                               ETA: 625.4s

################################################################################
                     [1m Learning iteration 749/2000 [0m

                       Computation: 17503 steps/s (collection: 0.262s, learning 0.206s)
               Value function loss: 101864.5918
                    Surrogate loss: -0.0025
             Mean action noise std: 0.92
                       Mean reward: 9444.20
               Mean episode length: 376.79
                 Mean success rate: 80.50
                  Mean reward/step: 25.83
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 0.47s
                        Total time: 374.61s
                               ETA: 624.9s

################################################################################
                     [1m Learning iteration 750/2000 [0m

                       Computation: 17759 steps/s (collection: 0.258s, learning 0.203s)
               Value function loss: 55304.3612
                    Surrogate loss: -0.0031
             Mean action noise std: 0.93
                       Mean reward: 9451.95
               Mean episode length: 375.70
                 Mean success rate: 81.50
                  Mean reward/step: 25.61
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6152192
                    Iteration time: 0.46s
                        Total time: 375.08s
                               ETA: 624.3s

################################################################################
                     [1m Learning iteration 751/2000 [0m

                       Computation: 17599 steps/s (collection: 0.252s, learning 0.214s)
               Value function loss: 72362.9342
                    Surrogate loss: -0.0059
             Mean action noise std: 0.93
                       Mean reward: 9617.23
               Mean episode length: 380.19
                 Mean success rate: 82.00
                  Mean reward/step: 25.44
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 0.47s
                        Total time: 375.54s
                               ETA: 623.7s

################################################################################
                     [1m Learning iteration 752/2000 [0m

                       Computation: 17552 steps/s (collection: 0.258s, learning 0.208s)
               Value function loss: 48104.6810
                    Surrogate loss: -0.0017
             Mean action noise std: 0.93
                       Mean reward: 9723.08
               Mean episode length: 383.19
                 Mean success rate: 83.00
                  Mean reward/step: 26.59
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6168576
                    Iteration time: 0.47s
                        Total time: 376.01s
                               ETA: 623.2s

################################################################################
                     [1m Learning iteration 753/2000 [0m

                       Computation: 17689 steps/s (collection: 0.259s, learning 0.204s)
               Value function loss: 72494.9497
                    Surrogate loss: -0.0023
             Mean action noise std: 0.93
                       Mean reward: 10102.44
               Mean episode length: 395.51
                 Mean success rate: 85.00
                  Mean reward/step: 26.53
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 0.46s
                        Total time: 376.47s
                               ETA: 622.6s

################################################################################
                     [1m Learning iteration 754/2000 [0m

                       Computation: 17252 steps/s (collection: 0.268s, learning 0.207s)
               Value function loss: 100148.4367
                    Surrogate loss: 0.0048
             Mean action noise std: 0.92
                       Mean reward: 10584.71
               Mean episode length: 409.43
                 Mean success rate: 87.00
                  Mean reward/step: 25.45
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6184960
                    Iteration time: 0.47s
                        Total time: 376.95s
                               ETA: 622.1s

################################################################################
                     [1m Learning iteration 755/2000 [0m

                       Computation: 17199 steps/s (collection: 0.268s, learning 0.208s)
               Value function loss: 107285.6516
                    Surrogate loss: 0.0017
             Mean action noise std: 0.92
                       Mean reward: 10867.34
               Mean episode length: 419.05
                 Mean success rate: 88.50
                  Mean reward/step: 26.24
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.48s
                        Total time: 377.42s
                               ETA: 621.5s

################################################################################
                     [1m Learning iteration 756/2000 [0m

                       Computation: 16717 steps/s (collection: 0.262s, learning 0.228s)
               Value function loss: 98322.2176
                    Surrogate loss: 0.0042
             Mean action noise std: 0.92
                       Mean reward: 11218.01
               Mean episode length: 432.81
                 Mean success rate: 90.50
                  Mean reward/step: 26.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6201344
                    Iteration time: 0.49s
                        Total time: 377.91s
                               ETA: 621.0s

################################################################################
                     [1m Learning iteration 757/2000 [0m

                       Computation: 17046 steps/s (collection: 0.258s, learning 0.223s)
               Value function loss: 116087.5418
                    Surrogate loss: 0.0001
             Mean action noise std: 0.92
                       Mean reward: 11093.76
               Mean episode length: 426.34
                 Mean success rate: 89.50
                  Mean reward/step: 24.89
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 0.48s
                        Total time: 378.39s
                               ETA: 620.5s

################################################################################
                     [1m Learning iteration 758/2000 [0m

                       Computation: 15517 steps/s (collection: 0.293s, learning 0.235s)
               Value function loss: 103457.1587
                    Surrogate loss: -0.0027
             Mean action noise std: 0.92
                       Mean reward: 10727.24
               Mean episode length: 413.27
                 Mean success rate: 87.50
                  Mean reward/step: 24.35
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6217728
                    Iteration time: 0.53s
                        Total time: 378.92s
                               ETA: 620.1s

################################################################################
                     [1m Learning iteration 759/2000 [0m

                       Computation: 14509 steps/s (collection: 0.307s, learning 0.257s)
               Value function loss: 91010.0267
                    Surrogate loss: -0.0006
             Mean action noise std: 0.92
                       Mean reward: 10642.23
               Mean episode length: 412.01
                 Mean success rate: 86.50
                  Mean reward/step: 23.95
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 0.56s
                        Total time: 379.48s
                               ETA: 619.7s

################################################################################
                     [1m Learning iteration 760/2000 [0m

                       Computation: 15182 steps/s (collection: 0.281s, learning 0.259s)
               Value function loss: 83347.0718
                    Surrogate loss: 0.0044
             Mean action noise std: 0.92
                       Mean reward: 11032.73
               Mean episode length: 425.77
                 Mean success rate: 88.00
                  Mean reward/step: 23.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6234112
                    Iteration time: 0.54s
                        Total time: 380.02s
                               ETA: 619.2s

################################################################################
                     [1m Learning iteration 761/2000 [0m

                       Computation: 15268 steps/s (collection: 0.313s, learning 0.223s)
               Value function loss: 69385.0196
                    Surrogate loss: -0.0029
             Mean action noise std: 0.92
                       Mean reward: 11208.13
               Mean episode length: 434.00
                 Mean success rate: 89.50
                  Mean reward/step: 24.40
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 0.54s
                        Total time: 380.56s
                               ETA: 618.8s

################################################################################
                     [1m Learning iteration 762/2000 [0m

                       Computation: 15138 steps/s (collection: 0.303s, learning 0.238s)
               Value function loss: 93117.7018
                    Surrogate loss: 0.0025
             Mean action noise std: 0.92
                       Mean reward: 11204.18
               Mean episode length: 435.95
                 Mean success rate: 89.50
                  Mean reward/step: 26.27
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6250496
                    Iteration time: 0.54s
                        Total time: 381.10s
                               ETA: 618.4s

################################################################################
                     [1m Learning iteration 763/2000 [0m

                       Computation: 16818 steps/s (collection: 0.277s, learning 0.210s)
               Value function loss: 101389.3781
                    Surrogate loss: -0.0016
             Mean action noise std: 0.92
                       Mean reward: 11017.97
               Mean episode length: 430.50
                 Mean success rate: 88.50
                  Mean reward/step: 24.97
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 0.49s
                        Total time: 381.59s
                               ETA: 617.8s

################################################################################
                     [1m Learning iteration 764/2000 [0m

                       Computation: 16707 steps/s (collection: 0.277s, learning 0.214s)
               Value function loss: 103659.8430
                    Surrogate loss: -0.0024
             Mean action noise std: 0.92
                       Mean reward: 10584.40
               Mean episode length: 416.06
                 Mean success rate: 86.50
                  Mean reward/step: 23.72
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6266880
                    Iteration time: 0.49s
                        Total time: 382.08s
                               ETA: 617.3s

################################################################################
                     [1m Learning iteration 765/2000 [0m

                       Computation: 16490 steps/s (collection: 0.287s, learning 0.209s)
               Value function loss: 87702.4074
                    Surrogate loss: 0.0021
             Mean action noise std: 0.92
                       Mean reward: 10495.31
               Mean episode length: 413.56
                 Mean success rate: 85.50
                  Mean reward/step: 23.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 0.50s
                        Total time: 382.58s
                               ETA: 616.8s

################################################################################
                     [1m Learning iteration 766/2000 [0m

                       Computation: 17176 steps/s (collection: 0.270s, learning 0.207s)
               Value function loss: 90770.7571
                    Surrogate loss: -0.0036
             Mean action noise std: 0.92
                       Mean reward: 10744.94
               Mean episode length: 421.94
                 Mean success rate: 86.50
                  Mean reward/step: 22.88
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6283264
                    Iteration time: 0.48s
                        Total time: 383.05s
                               ETA: 616.3s

################################################################################
                     [1m Learning iteration 767/2000 [0m

                       Computation: 15396 steps/s (collection: 0.296s, learning 0.236s)
               Value function loss: 67725.5005
                    Surrogate loss: -0.0026
             Mean action noise std: 0.93
                       Mean reward: 10780.69
               Mean episode length: 422.87
                 Mean success rate: 86.50
                  Mean reward/step: 23.00
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.53s
                        Total time: 383.59s
                               ETA: 615.8s

################################################################################
                     [1m Learning iteration 768/2000 [0m

                       Computation: 17274 steps/s (collection: 0.268s, learning 0.206s)
               Value function loss: 47667.0236
                    Surrogate loss: 0.0001
             Mean action noise std: 0.93
                       Mean reward: 10581.03
               Mean episode length: 418.65
                 Mean success rate: 86.50
                  Mean reward/step: 24.37
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6299648
                    Iteration time: 0.47s
                        Total time: 384.06s
                               ETA: 615.3s

################################################################################
                     [1m Learning iteration 769/2000 [0m

                       Computation: 16337 steps/s (collection: 0.297s, learning 0.205s)
               Value function loss: 77145.3132
                    Surrogate loss: -0.0008
             Mean action noise std: 0.93
                       Mean reward: 10304.49
               Mean episode length: 410.31
                 Mean success rate: 84.50
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 0.50s
                        Total time: 384.56s
                               ETA: 614.8s

################################################################################
                     [1m Learning iteration 770/2000 [0m

                       Computation: 16081 steps/s (collection: 0.295s, learning 0.215s)
               Value function loss: 82556.1577
                    Surrogate loss: -0.0028
             Mean action noise std: 0.92
                       Mean reward: 10301.01
               Mean episode length: 412.25
                 Mean success rate: 86.00
                  Mean reward/step: 25.53
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6316032
                    Iteration time: 0.51s
                        Total time: 385.07s
                               ETA: 614.3s

################################################################################
                     [1m Learning iteration 771/2000 [0m

                       Computation: 16545 steps/s (collection: 0.288s, learning 0.207s)
               Value function loss: 84050.2472
                    Surrogate loss: -0.0045
             Mean action noise std: 0.93
                       Mean reward: 10262.52
               Mean episode length: 413.27
                 Mean success rate: 86.50
                  Mean reward/step: 26.21
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 0.50s
                        Total time: 385.57s
                               ETA: 613.8s

################################################################################
                     [1m Learning iteration 772/2000 [0m

                       Computation: 16894 steps/s (collection: 0.277s, learning 0.208s)
               Value function loss: 145660.4312
                    Surrogate loss: 0.0015
             Mean action noise std: 0.93
                       Mean reward: 9872.90
               Mean episode length: 405.87
                 Mean success rate: 84.50
                  Mean reward/step: 26.24
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6332416
                    Iteration time: 0.48s
                        Total time: 386.05s
                               ETA: 613.3s

################################################################################
                     [1m Learning iteration 773/2000 [0m

                       Computation: 17253 steps/s (collection: 0.272s, learning 0.203s)
               Value function loss: 111237.7549
                    Surrogate loss: 0.0104
             Mean action noise std: 0.93
                       Mean reward: 9443.66
               Mean episode length: 391.93
                 Mean success rate: 83.00
                  Mean reward/step: 24.98
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 0.47s
                        Total time: 386.52s
                               ETA: 612.7s

################################################################################
                     [1m Learning iteration 774/2000 [0m

                       Computation: 16798 steps/s (collection: 0.272s, learning 0.216s)
               Value function loss: 147174.7707
                    Surrogate loss: -0.0007
             Mean action noise std: 0.92
                       Mean reward: 9271.92
               Mean episode length: 386.00
                 Mean success rate: 83.00
                  Mean reward/step: 24.48
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6348800
                    Iteration time: 0.49s
                        Total time: 387.01s
                               ETA: 612.2s

################################################################################
                     [1m Learning iteration 775/2000 [0m

                       Computation: 16713 steps/s (collection: 0.279s, learning 0.211s)
               Value function loss: 118492.8931
                    Surrogate loss: -0.0038
             Mean action noise std: 0.92
                       Mean reward: 8755.22
               Mean episode length: 369.64
                 Mean success rate: 81.00
                  Mean reward/step: 24.00
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 0.49s
                        Total time: 387.50s
                               ETA: 611.7s

################################################################################
                     [1m Learning iteration 776/2000 [0m

                       Computation: 16090 steps/s (collection: 0.294s, learning 0.215s)
               Value function loss: 106403.7170
                    Surrogate loss: -0.0032
             Mean action noise std: 0.92
                       Mean reward: 8827.03
               Mean episode length: 372.38
                 Mean success rate: 81.50
                  Mean reward/step: 24.23
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6365184
                    Iteration time: 0.51s
                        Total time: 388.01s
                               ETA: 611.2s

################################################################################
                     [1m Learning iteration 777/2000 [0m

                       Computation: 16140 steps/s (collection: 0.256s, learning 0.252s)
               Value function loss: 48866.9181
                    Surrogate loss: 0.0008
             Mean action noise std: 0.93
                       Mean reward: 8530.95
               Mean episode length: 360.45
                 Mean success rate: 80.00
                  Mean reward/step: 25.71
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 0.51s
                        Total time: 388.52s
                               ETA: 610.7s

################################################################################
                     [1m Learning iteration 778/2000 [0m

                       Computation: 16544 steps/s (collection: 0.287s, learning 0.209s)
               Value function loss: 90831.7994
                    Surrogate loss: 0.0101
             Mean action noise std: 0.93
                       Mean reward: 8418.79
               Mean episode length: 350.92
                 Mean success rate: 77.50
                  Mean reward/step: 27.11
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6381568
                    Iteration time: 0.50s
                        Total time: 389.01s
                               ETA: 610.2s

################################################################################
                     [1m Learning iteration 779/2000 [0m

                       Computation: 16628 steps/s (collection: 0.282s, learning 0.211s)
               Value function loss: 94153.3644
                    Surrogate loss: 0.0006
             Mean action noise std: 0.93
                       Mean reward: 8508.49
               Mean episode length: 343.35
                 Mean success rate: 78.00
                  Mean reward/step: 24.99
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.49s
                        Total time: 389.51s
                               ETA: 609.7s

################################################################################
                     [1m Learning iteration 780/2000 [0m

                       Computation: 15053 steps/s (collection: 0.305s, learning 0.239s)
               Value function loss: 109790.5820
                    Surrogate loss: -0.0026
             Mean action noise std: 0.93
                       Mean reward: 7961.57
               Mean episode length: 323.48
                 Mean success rate: 75.00
                  Mean reward/step: 21.79
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6397952
                    Iteration time: 0.54s
                        Total time: 390.05s
                               ETA: 609.3s

################################################################################
                     [1m Learning iteration 781/2000 [0m

                       Computation: 15304 steps/s (collection: 0.321s, learning 0.215s)
               Value function loss: 102043.4753
                    Surrogate loss: 0.0120
             Mean action noise std: 0.93
                       Mean reward: 7128.05
               Mean episode length: 287.51
                 Mean success rate: 71.50
                  Mean reward/step: 20.70
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 0.54s
                        Total time: 390.59s
                               ETA: 608.9s

################################################################################
                     [1m Learning iteration 782/2000 [0m

                       Computation: 16349 steps/s (collection: 0.295s, learning 0.206s)
               Value function loss: 65087.2573
                    Surrogate loss: -0.0042
             Mean action noise std: 0.93
                       Mean reward: 6905.07
               Mean episode length: 280.05
                 Mean success rate: 70.00
                  Mean reward/step: 21.10
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6414336
                    Iteration time: 0.50s
                        Total time: 391.09s
                               ETA: 608.4s

################################################################################
                     [1m Learning iteration 783/2000 [0m

                       Computation: 16008 steps/s (collection: 0.279s, learning 0.233s)
               Value function loss: 76285.4432
                    Surrogate loss: -0.0048
             Mean action noise std: 0.93
                       Mean reward: 6964.51
               Mean episode length: 278.93
                 Mean success rate: 70.50
                  Mean reward/step: 23.04
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 0.51s
                        Total time: 391.60s
                               ETA: 607.9s

################################################################################
                     [1m Learning iteration 784/2000 [0m

                       Computation: 16278 steps/s (collection: 0.294s, learning 0.210s)
               Value function loss: 51228.5624
                    Surrogate loss: 0.0067
             Mean action noise std: 0.93
                       Mean reward: 6935.59
               Mean episode length: 278.04
                 Mean success rate: 71.00
                  Mean reward/step: 24.81
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6430720
                    Iteration time: 0.50s
                        Total time: 392.10s
                               ETA: 607.4s

################################################################################
                     [1m Learning iteration 785/2000 [0m

                       Computation: 16417 steps/s (collection: 0.285s, learning 0.214s)
               Value function loss: 84026.1896
                    Surrogate loss: 0.0001
             Mean action noise std: 0.93
                       Mean reward: 7090.90
               Mean episode length: 284.88
                 Mean success rate: 72.00
                  Mean reward/step: 25.93
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 0.50s
                        Total time: 392.60s
                               ETA: 606.9s

################################################################################
                     [1m Learning iteration 786/2000 [0m

                       Computation: 16505 steps/s (collection: 0.283s, learning 0.214s)
               Value function loss: 74143.6719
                    Surrogate loss: 0.0037
             Mean action noise std: 0.93
                       Mean reward: 7213.28
               Mean episode length: 290.50
                 Mean success rate: 72.50
                  Mean reward/step: 26.35
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6447104
                    Iteration time: 0.50s
                        Total time: 393.10s
                               ETA: 606.4s

################################################################################
                     [1m Learning iteration 787/2000 [0m

                       Computation: 16253 steps/s (collection: 0.282s, learning 0.222s)
               Value function loss: 57578.1551
                    Surrogate loss: 0.0046
             Mean action noise std: 0.93
                       Mean reward: 7157.80
               Mean episode length: 291.70
                 Mean success rate: 73.00
                  Mean reward/step: 26.93
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 0.50s
                        Total time: 393.60s
                               ETA: 605.9s

################################################################################
                     [1m Learning iteration 788/2000 [0m

                       Computation: 16066 steps/s (collection: 0.296s, learning 0.214s)
               Value function loss: 111752.4696
                    Surrogate loss: -0.0019
             Mean action noise std: 0.93
                       Mean reward: 7370.95
               Mean episode length: 304.01
                 Mean success rate: 73.00
                  Mean reward/step: 26.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6463488
                    Iteration time: 0.51s
                        Total time: 394.11s
                               ETA: 605.4s

################################################################################
                     [1m Learning iteration 789/2000 [0m

                       Computation: 16844 steps/s (collection: 0.270s, learning 0.216s)
               Value function loss: 87764.6707
                    Surrogate loss: 0.0097
             Mean action noise std: 0.93
                       Mean reward: 7797.74
               Mean episode length: 319.40
                 Mean success rate: 74.50
                  Mean reward/step: 26.28
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 0.49s
                        Total time: 394.60s
                               ETA: 604.9s

################################################################################
                     [1m Learning iteration 790/2000 [0m

                       Computation: 16032 steps/s (collection: 0.301s, learning 0.210s)
               Value function loss: 135484.5141
                    Surrogate loss: -0.0001
             Mean action noise std: 0.93
                       Mean reward: 8655.44
               Mean episode length: 352.65
                 Mean success rate: 78.50
                  Mean reward/step: 25.55
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6479872
                    Iteration time: 0.51s
                        Total time: 395.11s
                               ETA: 604.4s

################################################################################
                     [1m Learning iteration 791/2000 [0m

                       Computation: 16082 steps/s (collection: 0.285s, learning 0.225s)
               Value function loss: 85128.7180
                    Surrogate loss: 0.0088
             Mean action noise std: 0.93
                       Mean reward: 8921.09
               Mean episode length: 363.26
                 Mean success rate: 79.50
                  Mean reward/step: 24.28
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.51s
                        Total time: 395.62s
                               ETA: 603.9s

################################################################################
                     [1m Learning iteration 792/2000 [0m

                       Computation: 17253 steps/s (collection: 0.271s, learning 0.203s)
               Value function loss: 72246.2218
                    Surrogate loss: -0.0006
             Mean action noise std: 0.93
                       Mean reward: 9378.98
               Mean episode length: 383.90
                 Mean success rate: 82.50
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6496256
                    Iteration time: 0.47s
                        Total time: 396.09s
                               ETA: 603.4s

################################################################################
                     [1m Learning iteration 793/2000 [0m

                       Computation: 17168 steps/s (collection: 0.271s, learning 0.206s)
               Value function loss: 55724.3635
                    Surrogate loss: 0.0034
             Mean action noise std: 0.93
                       Mean reward: 9401.62
               Mean episode length: 386.82
                 Mean success rate: 83.00
                  Mean reward/step: 25.83
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 0.48s
                        Total time: 396.57s
                               ETA: 602.8s

################################################################################
                     [1m Learning iteration 794/2000 [0m

                       Computation: 15343 steps/s (collection: 0.285s, learning 0.249s)
               Value function loss: 99061.4338
                    Surrogate loss: -0.0003
             Mean action noise std: 0.93
                       Mean reward: 9678.09
               Mean episode length: 397.04
                 Mean success rate: 84.00
                  Mean reward/step: 25.88
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6512640
                    Iteration time: 0.53s
                        Total time: 397.10s
                               ETA: 602.4s

################################################################################
                     [1m Learning iteration 795/2000 [0m

                       Computation: 15588 steps/s (collection: 0.277s, learning 0.249s)
               Value function loss: 106648.9124
                    Surrogate loss: -0.0039
             Mean action noise std: 0.93
                       Mean reward: 10187.10
               Mean episode length: 416.54
                 Mean success rate: 86.50
                  Mean reward/step: 25.26
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 0.53s
                        Total time: 397.63s
                               ETA: 601.9s

################################################################################
                     [1m Learning iteration 796/2000 [0m

                       Computation: 15700 steps/s (collection: 0.274s, learning 0.248s)
               Value function loss: 145851.3864
                    Surrogate loss: 0.0165
             Mean action noise std: 0.93
                       Mean reward: 10711.40
               Mean episode length: 434.19
                 Mean success rate: 89.50
                  Mean reward/step: 24.94
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6529024
                    Iteration time: 0.52s
                        Total time: 398.15s
                               ETA: 601.5s

################################################################################
                     [1m Learning iteration 797/2000 [0m

                       Computation: 16928 steps/s (collection: 0.271s, learning 0.213s)
               Value function loss: 117659.7943
                    Surrogate loss: -0.0040
             Mean action noise std: 0.93
                       Mean reward: 11077.94
               Mean episode length: 439.93
                 Mean success rate: 90.00
                  Mean reward/step: 24.35
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 0.48s
                        Total time: 398.64s
                               ETA: 601.0s

################################################################################
                     [1m Learning iteration 798/2000 [0m

                       Computation: 17802 steps/s (collection: 0.257s, learning 0.203s)
               Value function loss: 76003.4521
                    Surrogate loss: 0.0028
             Mean action noise std: 0.93
                       Mean reward: 11098.14
               Mean episode length: 438.65
                 Mean success rate: 90.50
                  Mean reward/step: 24.00
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6545408
                    Iteration time: 0.46s
                        Total time: 399.10s
                               ETA: 600.4s

################################################################################
                     [1m Learning iteration 799/2000 [0m

                       Computation: 18254 steps/s (collection: 0.248s, learning 0.201s)
               Value function loss: 75571.2323
                    Surrogate loss: -0.0001
             Mean action noise std: 0.93
                       Mean reward: 11168.68
               Mean episode length: 440.31
                 Mean success rate: 91.00
                  Mean reward/step: 24.55
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 0.45s
                        Total time: 399.54s
                               ETA: 599.8s

################################################################################
                     [1m Learning iteration 800/2000 [0m

                       Computation: 17982 steps/s (collection: 0.246s, learning 0.209s)
               Value function loss: 76988.5908
                    Surrogate loss: 0.0022
             Mean action noise std: 0.93
                       Mean reward: 11096.17
               Mean episode length: 437.67
                 Mean success rate: 90.50
                  Mean reward/step: 25.31
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6561792
                    Iteration time: 0.46s
                        Total time: 400.00s
                               ETA: 599.3s

################################################################################
                     [1m Learning iteration 801/2000 [0m

                       Computation: 17395 steps/s (collection: 0.254s, learning 0.217s)
               Value function loss: 90420.5705
                    Surrogate loss: 0.0001
             Mean action noise std: 0.93
                       Mean reward: 10674.49
               Mean episode length: 422.69
                 Mean success rate: 88.50
                  Mean reward/step: 25.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 0.47s
                        Total time: 400.47s
                               ETA: 598.7s

################################################################################
                     [1m Learning iteration 802/2000 [0m

                       Computation: 17261 steps/s (collection: 0.260s, learning 0.214s)
               Value function loss: 75083.3164
                    Surrogate loss: 0.0058
             Mean action noise std: 0.93
                       Mean reward: 10421.36
               Mean episode length: 411.29
                 Mean success rate: 87.00
                  Mean reward/step: 25.00
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6578176
                    Iteration time: 0.47s
                        Total time: 400.95s
                               ETA: 598.2s

################################################################################
                     [1m Learning iteration 803/2000 [0m

                       Computation: 17384 steps/s (collection: 0.263s, learning 0.209s)
               Value function loss: 80675.3775
                    Surrogate loss: -0.0044
             Mean action noise std: 0.93
                       Mean reward: 10260.53
               Mean episode length: 404.92
                 Mean success rate: 86.50
                  Mean reward/step: 25.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.47s
                        Total time: 401.42s
                               ETA: 597.6s

################################################################################
                     [1m Learning iteration 804/2000 [0m

                       Computation: 17751 steps/s (collection: 0.256s, learning 0.206s)
               Value function loss: 94174.2160
                    Surrogate loss: -0.0024
             Mean action noise std: 0.93
                       Mean reward: 10223.40
               Mean episode length: 401.55
                 Mean success rate: 85.50
                  Mean reward/step: 25.71
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6594560
                    Iteration time: 0.46s
                        Total time: 401.88s
                               ETA: 597.1s

################################################################################
                     [1m Learning iteration 805/2000 [0m

                       Computation: 17021 steps/s (collection: 0.272s, learning 0.210s)
               Value function loss: 101647.8438
                    Surrogate loss: -0.0036
             Mean action noise std: 0.93
                       Mean reward: 9884.60
               Mean episode length: 390.00
                 Mean success rate: 83.50
                  Mean reward/step: 26.00
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 0.48s
                        Total time: 402.36s
                               ETA: 596.6s

################################################################################
                     [1m Learning iteration 806/2000 [0m

                       Computation: 17149 steps/s (collection: 0.268s, learning 0.210s)
               Value function loss: 80617.6714
                    Surrogate loss: -0.0014
             Mean action noise std: 0.93
                       Mean reward: 9808.33
               Mean episode length: 388.64
                 Mean success rate: 83.50
                  Mean reward/step: 26.03
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6610944
                    Iteration time: 0.48s
                        Total time: 402.84s
                               ETA: 596.0s

################################################################################
                     [1m Learning iteration 807/2000 [0m

                       Computation: 17970 steps/s (collection: 0.258s, learning 0.198s)
               Value function loss: 125124.6123
                    Surrogate loss: -0.0034
             Mean action noise std: 0.93
                       Mean reward: 9900.10
               Mean episode length: 392.55
                 Mean success rate: 84.50
                  Mean reward/step: 26.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 0.46s
                        Total time: 403.29s
                               ETA: 595.5s

################################################################################
                     [1m Learning iteration 808/2000 [0m

                       Computation: 18346 steps/s (collection: 0.251s, learning 0.196s)
               Value function loss: 55189.0497
                    Surrogate loss: -0.0005
             Mean action noise std: 0.93
                       Mean reward: 9709.94
               Mean episode length: 386.45
                 Mean success rate: 84.00
                  Mean reward/step: 26.28
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6627328
                    Iteration time: 0.45s
                        Total time: 403.74s
                               ETA: 594.9s

################################################################################
                     [1m Learning iteration 809/2000 [0m

                       Computation: 17963 steps/s (collection: 0.247s, learning 0.209s)
               Value function loss: 62211.6413
                    Surrogate loss: 0.0050
             Mean action noise std: 0.93
                       Mean reward: 9769.46
               Mean episode length: 386.67
                 Mean success rate: 84.00
                  Mean reward/step: 26.17
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 0.46s
                        Total time: 404.20s
                               ETA: 594.3s

################################################################################
                     [1m Learning iteration 810/2000 [0m

                       Computation: 17686 steps/s (collection: 0.255s, learning 0.208s)
               Value function loss: 106673.9962
                    Surrogate loss: -0.0025
             Mean action noise std: 0.93
                       Mean reward: 9494.81
               Mean episode length: 376.43
                 Mean success rate: 82.50
                  Mean reward/step: 24.44
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6643712
                    Iteration time: 0.46s
                        Total time: 404.66s
                               ETA: 593.8s

################################################################################
                     [1m Learning iteration 811/2000 [0m

                       Computation: 17831 steps/s (collection: 0.263s, learning 0.197s)
               Value function loss: 88191.6011
                    Surrogate loss: -0.0057
             Mean action noise std: 0.93
                       Mean reward: 9929.17
               Mean episode length: 387.42
                 Mean success rate: 84.00
                  Mean reward/step: 24.33
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 0.46s
                        Total time: 405.12s
                               ETA: 593.2s

################################################################################
                     [1m Learning iteration 812/2000 [0m

                       Computation: 17379 steps/s (collection: 0.267s, learning 0.204s)
               Value function loss: 145983.8021
                    Surrogate loss: -0.0032
             Mean action noise std: 0.93
                       Mean reward: 10481.11
               Mean episode length: 407.75
                 Mean success rate: 86.00
                  Mean reward/step: 24.19
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6660096
                    Iteration time: 0.47s
                        Total time: 405.59s
                               ETA: 592.7s

################################################################################
                     [1m Learning iteration 813/2000 [0m

                       Computation: 17001 steps/s (collection: 0.278s, learning 0.204s)
               Value function loss: 88960.9493
                    Surrogate loss: 0.0033
             Mean action noise std: 0.93
                       Mean reward: 10352.63
               Mean episode length: 405.70
                 Mean success rate: 86.50
                  Mean reward/step: 24.05
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 0.48s
                        Total time: 406.07s
                               ETA: 592.1s

################################################################################
                     [1m Learning iteration 814/2000 [0m

                       Computation: 15990 steps/s (collection: 0.292s, learning 0.220s)
               Value function loss: 84339.6363
                    Surrogate loss: -0.0048
             Mean action noise std: 0.93
                       Mean reward: 10312.64
               Mean episode length: 404.09
                 Mean success rate: 87.50
                  Mean reward/step: 24.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6676480
                    Iteration time: 0.51s
                        Total time: 406.58s
                               ETA: 591.7s

################################################################################
                     [1m Learning iteration 815/2000 [0m

                       Computation: 17498 steps/s (collection: 0.271s, learning 0.197s)
               Value function loss: 44049.5780
                    Surrogate loss: -0.0030
             Mean action noise std: 0.93
                       Mean reward: 10145.55
               Mean episode length: 399.66
                 Mean success rate: 87.00
                  Mean reward/step: 25.46
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.47s
                        Total time: 407.05s
                               ETA: 591.1s

################################################################################
                     [1m Learning iteration 816/2000 [0m

                       Computation: 15283 steps/s (collection: 0.301s, learning 0.235s)
               Value function loss: 62866.6558
                    Surrogate loss: 0.0001
             Mean action noise std: 0.93
                       Mean reward: 10280.03
               Mean episode length: 402.79
                 Mean success rate: 87.50
                  Mean reward/step: 26.60
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 6692864
                    Iteration time: 0.54s
                        Total time: 407.59s
                               ETA: 590.7s

################################################################################
                     [1m Learning iteration 817/2000 [0m

                       Computation: 15709 steps/s (collection: 0.266s, learning 0.255s)
               Value function loss: 84666.5378
                    Surrogate loss: -0.0026
             Mean action noise std: 0.93
                       Mean reward: 10097.94
               Mean episode length: 397.39
                 Mean success rate: 86.00
                  Mean reward/step: 25.98
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 0.52s
                        Total time: 408.11s
                               ETA: 590.2s

################################################################################
                     [1m Learning iteration 818/2000 [0m

                       Computation: 17109 steps/s (collection: 0.280s, learning 0.199s)
               Value function loss: 76308.7138
                    Surrogate loss: 0.0007
             Mean action noise std: 0.93
                       Mean reward: 10084.31
               Mean episode length: 395.87
                 Mean success rate: 85.50
                  Mean reward/step: 25.34
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6709248
                    Iteration time: 0.48s
                        Total time: 408.59s
                               ETA: 589.7s

################################################################################
                     [1m Learning iteration 819/2000 [0m

                       Computation: 17223 steps/s (collection: 0.272s, learning 0.204s)
               Value function loss: 102614.6350
                    Surrogate loss: 0.0027
             Mean action noise std: 0.93
                       Mean reward: 9949.37
               Mean episode length: 394.32
                 Mean success rate: 86.00
                  Mean reward/step: 25.70
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 0.48s
                        Total time: 409.06s
                               ETA: 589.2s

################################################################################
                     [1m Learning iteration 820/2000 [0m

                       Computation: 15776 steps/s (collection: 0.315s, learning 0.204s)
               Value function loss: 75546.4668
                    Surrogate loss: -0.0010
             Mean action noise std: 0.93
                       Mean reward: 9951.23
               Mean episode length: 396.03
                 Mean success rate: 86.00
                  Mean reward/step: 25.14
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6725632
                    Iteration time: 0.52s
                        Total time: 409.58s
                               ETA: 588.7s

################################################################################
                     [1m Learning iteration 821/2000 [0m

                       Computation: 17231 steps/s (collection: 0.277s, learning 0.199s)
               Value function loss: 97557.4533
                    Surrogate loss: -0.0024
             Mean action noise std: 0.93
                       Mean reward: 9966.20
               Mean episode length: 398.10
                 Mean success rate: 86.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 0.48s
                        Total time: 410.06s
                               ETA: 588.1s

################################################################################
                     [1m Learning iteration 822/2000 [0m

                       Computation: 16700 steps/s (collection: 0.273s, learning 0.218s)
               Value function loss: 104100.1804
                    Surrogate loss: -0.0039
             Mean action noise std: 0.93
                       Mean reward: 9917.99
               Mean episode length: 395.10
                 Mean success rate: 86.00
                  Mean reward/step: 25.21
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6742016
                    Iteration time: 0.49s
                        Total time: 410.55s
                               ETA: 587.6s

################################################################################
                     [1m Learning iteration 823/2000 [0m

                       Computation: 16861 steps/s (collection: 0.267s, learning 0.219s)
               Value function loss: 103490.5756
                    Surrogate loss: -0.0056
             Mean action noise std: 0.94
                       Mean reward: 9644.93
               Mean episode length: 384.87
                 Mean success rate: 83.50
                  Mean reward/step: 25.04
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 0.49s
                        Total time: 411.03s
                               ETA: 587.1s

################################################################################
                     [1m Learning iteration 824/2000 [0m

                       Computation: 15532 steps/s (collection: 0.280s, learning 0.247s)
               Value function loss: 70925.0050
                    Surrogate loss: -0.0058
             Mean action noise std: 0.94
                       Mean reward: 9363.07
               Mean episode length: 374.75
                 Mean success rate: 81.50
                  Mean reward/step: 24.95
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6758400
                    Iteration time: 0.53s
                        Total time: 411.56s
                               ETA: 586.7s

################################################################################
                     [1m Learning iteration 825/2000 [0m

                       Computation: 16301 steps/s (collection: 0.279s, learning 0.224s)
               Value function loss: 105748.0780
                    Surrogate loss: -0.0057
             Mean action noise std: 0.94
                       Mean reward: 9105.11
               Mean episode length: 361.98
                 Mean success rate: 78.50
                  Mean reward/step: 26.14
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 0.50s
                        Total time: 412.06s
                               ETA: 586.2s

################################################################################
                     [1m Learning iteration 826/2000 [0m

                       Computation: 16684 steps/s (collection: 0.289s, learning 0.202s)
               Value function loss: 134740.3192
                    Surrogate loss: -0.0055
             Mean action noise std: 0.94
                       Mean reward: 9276.89
               Mean episode length: 365.81
                 Mean success rate: 79.50
                  Mean reward/step: 25.92
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6774784
                    Iteration time: 0.49s
                        Total time: 412.56s
                               ETA: 585.7s

################################################################################
                     [1m Learning iteration 827/2000 [0m

                       Computation: 17446 steps/s (collection: 0.272s, learning 0.198s)
               Value function loss: 127633.3326
                    Surrogate loss: -0.0020
             Mean action noise std: 0.94
                       Mean reward: 9398.26
               Mean episode length: 368.59
                 Mean success rate: 80.00
                  Mean reward/step: 25.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.47s
                        Total time: 413.03s
                               ETA: 585.1s

################################################################################
                     [1m Learning iteration 828/2000 [0m

                       Computation: 15734 steps/s (collection: 0.309s, learning 0.211s)
               Value function loss: 92134.4861
                    Surrogate loss: -0.0042
             Mean action noise std: 0.94
                       Mean reward: 9288.40
               Mean episode length: 366.11
                 Mean success rate: 79.50
                  Mean reward/step: 24.21
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6791168
                    Iteration time: 0.52s
                        Total time: 413.55s
                               ETA: 584.7s

################################################################################
                     [1m Learning iteration 829/2000 [0m

                       Computation: 16351 steps/s (collection: 0.288s, learning 0.213s)
               Value function loss: 117709.2387
                    Surrogate loss: -0.0003
             Mean action noise std: 0.94
                       Mean reward: 8711.75
               Mean episode length: 346.66
                 Mean success rate: 74.50
                  Mean reward/step: 24.89
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 0.50s
                        Total time: 414.05s
                               ETA: 584.2s

################################################################################
                     [1m Learning iteration 830/2000 [0m

                       Computation: 16948 steps/s (collection: 0.264s, learning 0.219s)
               Value function loss: 58539.5198
                    Surrogate loss: -0.0025
             Mean action noise std: 0.94
                       Mean reward: 8786.25
               Mean episode length: 347.89
                 Mean success rate: 75.50
                  Mean reward/step: 24.82
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6807552
                    Iteration time: 0.48s
                        Total time: 414.53s
                               ETA: 583.6s

################################################################################
                     [1m Learning iteration 831/2000 [0m

                       Computation: 17262 steps/s (collection: 0.271s, learning 0.204s)
               Value function loss: 57802.3095
                    Surrogate loss: -0.0042
             Mean action noise std: 0.94
                       Mean reward: 8637.62
               Mean episode length: 344.25
                 Mean success rate: 74.50
                  Mean reward/step: 26.25
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 0.47s
                        Total time: 415.00s
                               ETA: 583.1s

################################################################################
                     [1m Learning iteration 832/2000 [0m

                       Computation: 16808 steps/s (collection: 0.286s, learning 0.202s)
               Value function loss: 64461.3344
                    Surrogate loss: -0.0018
             Mean action noise std: 0.94
                       Mean reward: 8919.54
               Mean episode length: 353.00
                 Mean success rate: 75.50
                  Mean reward/step: 26.02
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6823936
                    Iteration time: 0.49s
                        Total time: 415.49s
                               ETA: 582.6s

################################################################################
                     [1m Learning iteration 833/2000 [0m

                       Computation: 17379 steps/s (collection: 0.263s, learning 0.208s)
               Value function loss: 97010.6131
                    Surrogate loss: 0.0089
             Mean action noise std: 0.94
                       Mean reward: 9414.62
               Mean episode length: 369.18
                 Mean success rate: 78.00
                  Mean reward/step: 26.05
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 0.47s
                        Total time: 415.96s
                               ETA: 582.0s

################################################################################
                     [1m Learning iteration 834/2000 [0m

                       Computation: 16828 steps/s (collection: 0.265s, learning 0.222s)
               Value function loss: 60503.2607
                    Surrogate loss: 0.0013
             Mean action noise std: 0.94
                       Mean reward: 9534.67
               Mean episode length: 376.33
                 Mean success rate: 78.50
                  Mean reward/step: 24.00
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 6840320
                    Iteration time: 0.49s
                        Total time: 416.45s
                               ETA: 581.5s

################################################################################
                     [1m Learning iteration 835/2000 [0m

                       Computation: 17025 steps/s (collection: 0.276s, learning 0.205s)
               Value function loss: 88621.1021
                    Surrogate loss: -0.0051
             Mean action noise std: 0.94
                       Mean reward: 9830.97
               Mean episode length: 387.98
                 Mean success rate: 81.00
                  Mean reward/step: 24.76
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 0.48s
                        Total time: 416.93s
                               ETA: 581.0s

################################################################################
                     [1m Learning iteration 836/2000 [0m

                       Computation: 17282 steps/s (collection: 0.266s, learning 0.208s)
               Value function loss: 79637.4297
                    Surrogate loss: -0.0049
             Mean action noise std: 0.94
                       Mean reward: 9951.32
               Mean episode length: 394.48
                 Mean success rate: 82.00
                  Mean reward/step: 25.59
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6856704
                    Iteration time: 0.47s
                        Total time: 417.41s
                               ETA: 580.5s

################################################################################
                     [1m Learning iteration 837/2000 [0m

                       Computation: 16930 steps/s (collection: 0.273s, learning 0.211s)
               Value function loss: 86378.5655
                    Surrogate loss: -0.0053
             Mean action noise std: 0.94
                       Mean reward: 9861.67
               Mean episode length: 394.63
                 Mean success rate: 82.00
                  Mean reward/step: 25.43
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 0.48s
                        Total time: 417.89s
                               ETA: 580.0s

################################################################################
                     [1m Learning iteration 838/2000 [0m

                       Computation: 17404 steps/s (collection: 0.262s, learning 0.209s)
               Value function loss: 56151.1390
                    Surrogate loss: -0.0064
             Mean action noise std: 0.94
                       Mean reward: 9888.76
               Mean episode length: 392.76
                 Mean success rate: 82.00
                  Mean reward/step: 25.52
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 6873088
                    Iteration time: 0.47s
                        Total time: 418.36s
                               ETA: 579.4s

################################################################################
                     [1m Learning iteration 839/2000 [0m

                       Computation: 16502 steps/s (collection: 0.267s, learning 0.229s)
               Value function loss: 109689.8082
                    Surrogate loss: -0.0023
             Mean action noise std: 0.94
                       Mean reward: 9721.00
               Mean episode length: 386.26
                 Mean success rate: 81.50
                  Mean reward/step: 25.89
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.50s
                        Total time: 418.86s
                               ETA: 578.9s

################################################################################
                     [1m Learning iteration 840/2000 [0m

                       Computation: 17044 steps/s (collection: 0.262s, learning 0.218s)
               Value function loss: 56913.8971
                    Surrogate loss: -0.0060
             Mean action noise std: 0.94
                       Mean reward: 10220.38
               Mean episode length: 404.60
                 Mean success rate: 84.50
                  Mean reward/step: 24.89
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6889472
                    Iteration time: 0.48s
                        Total time: 419.34s
                               ETA: 578.4s

################################################################################
                     [1m Learning iteration 841/2000 [0m

                       Computation: 16765 steps/s (collection: 0.284s, learning 0.204s)
               Value function loss: 102364.5351
                    Surrogate loss: -0.0025
             Mean action noise std: 0.94
                       Mean reward: 10560.30
               Mean episode length: 417.21
                 Mean success rate: 87.00
                  Mean reward/step: 25.27
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 0.49s
                        Total time: 419.83s
                               ETA: 577.9s

################################################################################
                     [1m Learning iteration 842/2000 [0m

                       Computation: 16480 steps/s (collection: 0.282s, learning 0.215s)
               Value function loss: 73713.7518
                    Surrogate loss: 0.0096
             Mean action noise std: 0.94
                       Mean reward: 10839.59
               Mean episode length: 424.96
                 Mean success rate: 89.00
                  Mean reward/step: 25.58
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6905856
                    Iteration time: 0.50s
                        Total time: 420.32s
                               ETA: 577.4s

################################################################################
                     [1m Learning iteration 843/2000 [0m

                       Computation: 16789 steps/s (collection: 0.275s, learning 0.213s)
               Value function loss: 123374.6448
                    Surrogate loss: 0.0020
             Mean action noise std: 0.94
                       Mean reward: 10985.44
               Mean episode length: 432.40
                 Mean success rate: 90.00
                  Mean reward/step: 24.27
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 0.49s
                        Total time: 420.81s
                               ETA: 576.9s

################################################################################
                     [1m Learning iteration 844/2000 [0m

                       Computation: 16847 steps/s (collection: 0.272s, learning 0.214s)
               Value function loss: 108831.0432
                    Surrogate loss: -0.0051
             Mean action noise std: 0.94
                       Mean reward: 11385.06
               Mean episode length: 444.87
                 Mean success rate: 92.50
                  Mean reward/step: 22.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6922240
                    Iteration time: 0.49s
                        Total time: 421.30s
                               ETA: 576.4s

################################################################################
                     [1m Learning iteration 845/2000 [0m

                       Computation: 16168 steps/s (collection: 0.274s, learning 0.233s)
               Value function loss: 96956.4905
                    Surrogate loss: -0.0039
             Mean action noise std: 0.94
                       Mean reward: 10975.12
               Mean episode length: 432.62
                 Mean success rate: 90.00
                  Mean reward/step: 22.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 0.51s
                        Total time: 421.80s
                               ETA: 575.9s

################################################################################
                     [1m Learning iteration 846/2000 [0m

                       Computation: 17391 steps/s (collection: 0.259s, learning 0.212s)
               Value function loss: 59481.8306
                    Surrogate loss: -0.0021
             Mean action noise std: 0.94
                       Mean reward: 10550.66
               Mean episode length: 420.99
                 Mean success rate: 87.50
                  Mean reward/step: 23.55
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6938624
                    Iteration time: 0.47s
                        Total time: 422.27s
                               ETA: 575.3s

################################################################################
                     [1m Learning iteration 847/2000 [0m

                       Computation: 17825 steps/s (collection: 0.255s, learning 0.205s)
               Value function loss: 56067.9993
                    Surrogate loss: -0.0008
             Mean action noise std: 0.94
                       Mean reward: 10648.13
               Mean episode length: 423.85
                 Mean success rate: 87.50
                  Mean reward/step: 24.40
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 0.46s
                        Total time: 422.73s
                               ETA: 574.8s

################################################################################
                     [1m Learning iteration 848/2000 [0m

                       Computation: 16211 steps/s (collection: 0.279s, learning 0.227s)
               Value function loss: 70561.9153
                    Surrogate loss: -0.0045
             Mean action noise std: 0.94
                       Mean reward: 10640.86
               Mean episode length: 426.80
                 Mean success rate: 88.00
                  Mean reward/step: 25.58
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6955008
                    Iteration time: 0.51s
                        Total time: 423.24s
                               ETA: 574.3s

################################################################################
                     [1m Learning iteration 849/2000 [0m

                       Computation: 17328 steps/s (collection: 0.264s, learning 0.209s)
               Value function loss: 92347.7549
                    Surrogate loss: 0.0008
             Mean action noise std: 0.94
                       Mean reward: 10755.41
               Mean episode length: 431.08
                 Mean success rate: 88.50
                  Mean reward/step: 26.17
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 0.47s
                        Total time: 423.71s
                               ETA: 573.8s

################################################################################
                     [1m Learning iteration 850/2000 [0m

                       Computation: 17700 steps/s (collection: 0.249s, learning 0.214s)
               Value function loss: 56839.3632
                    Surrogate loss: -0.0016
             Mean action noise std: 0.94
                       Mean reward: 10770.53
               Mean episode length: 433.06
                 Mean success rate: 89.00
                  Mean reward/step: 26.33
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 6971392
                    Iteration time: 0.46s
                        Total time: 424.18s
                               ETA: 573.2s

################################################################################
                     [1m Learning iteration 851/2000 [0m

                       Computation: 17337 steps/s (collection: 0.261s, learning 0.212s)
               Value function loss: 63838.6164
                    Surrogate loss: -0.0038
             Mean action noise std: 0.94
                       Mean reward: 10899.15
               Mean episode length: 436.44
                 Mean success rate: 90.00
                  Mean reward/step: 26.13
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.47s
                        Total time: 424.65s
                               ETA: 572.7s

################################################################################
                     [1m Learning iteration 852/2000 [0m

                       Computation: 17042 steps/s (collection: 0.257s, learning 0.224s)
               Value function loss: 117972.3246
                    Surrogate loss: 0.0002
             Mean action noise std: 0.94
                       Mean reward: 10918.64
               Mean episode length: 438.33
                 Mean success rate: 90.00
                  Mean reward/step: 26.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6987776
                    Iteration time: 0.48s
                        Total time: 425.13s
                               ETA: 572.2s

################################################################################
                     [1m Learning iteration 853/2000 [0m

                       Computation: 16289 steps/s (collection: 0.289s, learning 0.214s)
               Value function loss: 86057.1521
                    Surrogate loss: -0.0006
             Mean action noise std: 0.94
                       Mean reward: 10753.01
               Mean episode length: 432.94
                 Mean success rate: 89.00
                  Mean reward/step: 25.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 0.50s
                        Total time: 425.63s
                               ETA: 571.7s

################################################################################
                     [1m Learning iteration 854/2000 [0m

                       Computation: 16892 steps/s (collection: 0.276s, learning 0.209s)
               Value function loss: 78619.8185
                    Surrogate loss: -0.0041
             Mean action noise std: 0.94
                       Mean reward: 10487.44
               Mean episode length: 422.69
                 Mean success rate: 87.50
                  Mean reward/step: 25.38
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7004160
                    Iteration time: 0.48s
                        Total time: 426.12s
                               ETA: 571.1s

################################################################################
                     [1m Learning iteration 855/2000 [0m

                       Computation: 16357 steps/s (collection: 0.283s, learning 0.218s)
               Value function loss: 101859.2471
                    Surrogate loss: 0.0030
             Mean action noise std: 0.94
                       Mean reward: 10323.50
               Mean episode length: 419.45
                 Mean success rate: 87.00
                  Mean reward/step: 25.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 0.50s
                        Total time: 426.62s
                               ETA: 570.7s

################################################################################
                     [1m Learning iteration 856/2000 [0m

                       Computation: 16379 steps/s (collection: 0.276s, learning 0.224s)
               Value function loss: 91445.8506
                    Surrogate loss: -0.0037
             Mean action noise std: 0.94
                       Mean reward: 10638.80
               Mean episode length: 428.66
                 Mean success rate: 89.00
                  Mean reward/step: 25.63
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7020544
                    Iteration time: 0.50s
                        Total time: 427.12s
                               ETA: 570.2s

################################################################################
                     [1m Learning iteration 857/2000 [0m

                       Computation: 17328 steps/s (collection: 0.264s, learning 0.209s)
               Value function loss: 110239.8486
                    Surrogate loss: -0.0048
             Mean action noise std: 0.94
                       Mean reward: 10915.09
               Mean episode length: 437.04
                 Mean success rate: 91.00
                  Mean reward/step: 24.95
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 0.47s
                        Total time: 427.59s
                               ETA: 569.6s

################################################################################
                     [1m Learning iteration 858/2000 [0m

                       Computation: 15633 steps/s (collection: 0.288s, learning 0.236s)
               Value function loss: 80201.8934
                    Surrogate loss: -0.0023
             Mean action noise std: 0.94
                       Mean reward: 11059.26
               Mean episode length: 440.69
                 Mean success rate: 92.50
                  Mean reward/step: 24.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7036928
                    Iteration time: 0.52s
                        Total time: 428.11s
                               ETA: 569.2s

################################################################################
                     [1m Learning iteration 859/2000 [0m

                       Computation: 16083 steps/s (collection: 0.288s, learning 0.221s)
               Value function loss: 109040.6345
                    Surrogate loss: -0.0021
             Mean action noise std: 0.94
                       Mean reward: 11281.57
               Mean episode length: 442.06
                 Mean success rate: 93.00
                  Mean reward/step: 24.19
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 0.51s
                        Total time: 428.62s
                               ETA: 568.7s

################################################################################
                     [1m Learning iteration 860/2000 [0m

                       Computation: 16175 steps/s (collection: 0.296s, learning 0.210s)
               Value function loss: 127815.9773
                    Surrogate loss: 0.0023
             Mean action noise std: 0.94
                       Mean reward: 11289.18
               Mean episode length: 443.38
                 Mean success rate: 92.00
                  Mean reward/step: 24.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7053312
                    Iteration time: 0.51s
                        Total time: 429.13s
                               ETA: 568.2s

################################################################################
                     [1m Learning iteration 861/2000 [0m

                       Computation: 16371 steps/s (collection: 0.287s, learning 0.213s)
               Value function loss: 64934.4602
                    Surrogate loss: -0.0051
             Mean action noise std: 0.94
                       Mean reward: 11074.98
               Mean episode length: 437.26
                 Mean success rate: 91.00
                  Mean reward/step: 23.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 0.50s
                        Total time: 429.63s
                               ETA: 567.7s

################################################################################
                     [1m Learning iteration 862/2000 [0m

                       Computation: 15467 steps/s (collection: 0.301s, learning 0.228s)
               Value function loss: 87882.9464
                    Surrogate loss: -0.0020
             Mean action noise std: 0.94
                       Mean reward: 10800.45
               Mean episode length: 428.43
                 Mean success rate: 90.00
                  Mean reward/step: 23.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7069696
                    Iteration time: 0.53s
                        Total time: 430.16s
                               ETA: 567.2s

################################################################################
                     [1m Learning iteration 863/2000 [0m

                       Computation: 16056 steps/s (collection: 0.295s, learning 0.215s)
               Value function loss: 75144.3909
                    Surrogate loss: -0.0041
             Mean action noise std: 0.94
                       Mean reward: 10785.30
               Mean episode length: 429.28
                 Mean success rate: 90.50
                  Mean reward/step: 24.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.51s
                        Total time: 430.67s
                               ETA: 566.7s

################################################################################
                     [1m Learning iteration 864/2000 [0m

                       Computation: 16527 steps/s (collection: 0.272s, learning 0.224s)
               Value function loss: 84850.6123
                    Surrogate loss: -0.0045
             Mean action noise std: 0.94
                       Mean reward: 10646.81
               Mean episode length: 425.07
                 Mean success rate: 89.50
                  Mean reward/step: 25.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7086080
                    Iteration time: 0.50s
                        Total time: 431.17s
                               ETA: 566.2s

################################################################################
                     [1m Learning iteration 865/2000 [0m

                       Computation: 16579 steps/s (collection: 0.269s, learning 0.225s)
               Value function loss: 61282.4200
                    Surrogate loss: -0.0039
             Mean action noise std: 0.94
                       Mean reward: 10290.48
               Mean episode length: 412.00
                 Mean success rate: 88.00
                  Mean reward/step: 25.26
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 0.49s
                        Total time: 431.66s
                               ETA: 565.7s

################################################################################
                     [1m Learning iteration 866/2000 [0m

                       Computation: 17416 steps/s (collection: 0.258s, learning 0.212s)
               Value function loss: 73067.9780
                    Surrogate loss: -0.0036
             Mean action noise std: 0.94
                       Mean reward: 10173.94
               Mean episode length: 407.95
                 Mean success rate: 87.00
                  Mean reward/step: 26.03
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7102464
                    Iteration time: 0.47s
                        Total time: 432.13s
                               ETA: 565.2s

################################################################################
                     [1m Learning iteration 867/2000 [0m

                       Computation: 17336 steps/s (collection: 0.264s, learning 0.208s)
               Value function loss: 58905.6738
                    Surrogate loss: -0.0009
             Mean action noise std: 0.94
                       Mean reward: 10262.44
               Mean episode length: 412.39
                 Mean success rate: 87.50
                  Mean reward/step: 26.79
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 0.47s
                        Total time: 432.60s
                               ETA: 564.7s

################################################################################
                     [1m Learning iteration 868/2000 [0m

                       Computation: 16060 steps/s (collection: 0.271s, learning 0.239s)
               Value function loss: 123125.4607
                    Surrogate loss: 0.0060
             Mean action noise std: 0.94
                       Mean reward: 10351.10
               Mean episode length: 416.10
                 Mean success rate: 87.00
                  Mean reward/step: 26.80
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7118848
                    Iteration time: 0.51s
                        Total time: 433.11s
                               ETA: 564.2s

################################################################################
                     [1m Learning iteration 869/2000 [0m

                       Computation: 16577 steps/s (collection: 0.283s, learning 0.211s)
               Value function loss: 70791.7173
                    Surrogate loss: -0.0038
             Mean action noise std: 0.94
                       Mean reward: 10496.69
               Mean episode length: 421.95
                 Mean success rate: 87.50
                  Mean reward/step: 26.09
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 0.49s
                        Total time: 433.61s
                               ETA: 563.7s

################################################################################
                     [1m Learning iteration 870/2000 [0m

                       Computation: 16159 steps/s (collection: 0.265s, learning 0.242s)
               Value function loss: 111859.2820
                    Surrogate loss: 0.0031
             Mean action noise std: 0.94
                       Mean reward: 10344.67
               Mean episode length: 419.61
                 Mean success rate: 86.50
                  Mean reward/step: 26.51
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7135232
                    Iteration time: 0.51s
                        Total time: 434.11s
                               ETA: 563.2s

################################################################################
                     [1m Learning iteration 871/2000 [0m

                       Computation: 14812 steps/s (collection: 0.294s, learning 0.259s)
               Value function loss: 69104.2110
                    Surrogate loss: -0.0023
             Mean action noise std: 0.93
                       Mean reward: 10134.71
               Mean episode length: 410.23
                 Mean success rate: 85.50
                  Mean reward/step: 26.16
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 0.55s
                        Total time: 434.67s
                               ETA: 562.8s

################################################################################
                     [1m Learning iteration 872/2000 [0m

                       Computation: 13552 steps/s (collection: 0.309s, learning 0.295s)
               Value function loss: 81396.3972
                    Surrogate loss: -0.0033
             Mean action noise std: 0.93
                       Mean reward: 10292.56
               Mean episode length: 413.38
                 Mean success rate: 86.00
                  Mean reward/step: 26.98
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7151616
                    Iteration time: 0.60s
                        Total time: 435.27s
                               ETA: 562.4s

################################################################################
                     [1m Learning iteration 873/2000 [0m

                       Computation: 17351 steps/s (collection: 0.266s, learning 0.206s)
               Value function loss: 88385.1426
                    Surrogate loss: 0.0021
             Mean action noise std: 0.93
                       Mean reward: 10476.10
               Mean episode length: 423.83
                 Mean success rate: 87.50
                  Mean reward/step: 26.77
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 0.47s
                        Total time: 435.74s
                               ETA: 561.9s

################################################################################
                     [1m Learning iteration 874/2000 [0m

                       Computation: 17605 steps/s (collection: 0.257s, learning 0.209s)
               Value function loss: 139178.6766
                    Surrogate loss: 0.0237
             Mean action noise std: 0.93
                       Mean reward: 10997.95
               Mean episode length: 438.25
                 Mean success rate: 88.50
                  Mean reward/step: 26.80
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7168000
                    Iteration time: 0.47s
                        Total time: 436.21s
                               ETA: 561.3s

################################################################################
                     [1m Learning iteration 875/2000 [0m

                       Computation: 17107 steps/s (collection: 0.256s, learning 0.222s)
               Value function loss: 70094.0645
                    Surrogate loss: 0.0074
             Mean action noise std: 0.93
                       Mean reward: 11258.73
               Mean episode length: 445.14
                 Mean success rate: 90.00
                  Mean reward/step: 26.07
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.48s
                        Total time: 436.69s
                               ETA: 560.8s

################################################################################
                     [1m Learning iteration 876/2000 [0m

                       Computation: 16767 steps/s (collection: 0.279s, learning 0.209s)
               Value function loss: 124040.3250
                    Surrogate loss: 0.0086
             Mean action noise std: 0.93
                       Mean reward: 11735.75
               Mean episode length: 458.82
                 Mean success rate: 91.50
                  Mean reward/step: 26.69
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7184384
                    Iteration time: 0.49s
                        Total time: 437.18s
                               ETA: 560.3s

################################################################################
                     [1m Learning iteration 877/2000 [0m

                       Computation: 17071 steps/s (collection: 0.260s, learning 0.220s)
               Value function loss: 82082.7082
                    Surrogate loss: 0.0071
             Mean action noise std: 0.93
                       Mean reward: 11901.16
               Mean episode length: 462.49
                 Mean success rate: 92.50
                  Mean reward/step: 26.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 0.48s
                        Total time: 437.66s
                               ETA: 559.8s

################################################################################
                     [1m Learning iteration 878/2000 [0m

                       Computation: 16191 steps/s (collection: 0.288s, learning 0.217s)
               Value function loss: 69146.3677
                    Surrogate loss: -0.0010
             Mean action noise std: 0.93
                       Mean reward: 11923.68
               Mean episode length: 462.49
                 Mean success rate: 92.50
                  Mean reward/step: 26.38
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7200768
                    Iteration time: 0.51s
                        Total time: 438.16s
                               ETA: 559.3s

################################################################################
                     [1m Learning iteration 879/2000 [0m

                       Computation: 17351 steps/s (collection: 0.261s, learning 0.212s)
               Value function loss: 93995.8799
                    Surrogate loss: -0.0023
             Mean action noise std: 0.93
                       Mean reward: 12123.18
               Mean episode length: 468.81
                 Mean success rate: 94.00
                  Mean reward/step: 25.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 0.47s
                        Total time: 438.63s
                               ETA: 558.8s

################################################################################
                     [1m Learning iteration 880/2000 [0m

                       Computation: 17650 steps/s (collection: 0.254s, learning 0.210s)
               Value function loss: 76513.0784
                    Surrogate loss: -0.0030
             Mean action noise std: 0.93
                       Mean reward: 12053.20
               Mean episode length: 465.85
                 Mean success rate: 94.00
                  Mean reward/step: 23.97
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7217152
                    Iteration time: 0.46s
                        Total time: 439.10s
                               ETA: 558.2s

################################################################################
                     [1m Learning iteration 881/2000 [0m

                       Computation: 17731 steps/s (collection: 0.251s, learning 0.211s)
               Value function loss: 63631.6019
                    Surrogate loss: 0.0060
             Mean action noise std: 0.93
                       Mean reward: 11918.61
               Mean episode length: 458.32
                 Mean success rate: 92.50
                  Mean reward/step: 24.25
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 0.46s
                        Total time: 439.56s
                               ETA: 557.7s

################################################################################
                     [1m Learning iteration 882/2000 [0m

                       Computation: 17436 steps/s (collection: 0.258s, learning 0.212s)
               Value function loss: 53858.3709
                    Surrogate loss: 0.0027
             Mean action noise std: 0.93
                       Mean reward: 12273.62
               Mean episode length: 469.40
                 Mean success rate: 95.00
                  Mean reward/step: 24.56
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7233536
                    Iteration time: 0.47s
                        Total time: 440.03s
                               ETA: 557.1s

################################################################################
                     [1m Learning iteration 883/2000 [0m

                       Computation: 17512 steps/s (collection: 0.263s, learning 0.205s)
               Value function loss: 61556.4069
                    Surrogate loss: -0.0021
             Mean action noise std: 0.94
                       Mean reward: 12260.62
               Mean episode length: 469.21
                 Mean success rate: 95.00
                  Mean reward/step: 24.55
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 0.47s
                        Total time: 440.50s
                               ETA: 556.6s

################################################################################
                     [1m Learning iteration 884/2000 [0m

                       Computation: 16729 steps/s (collection: 0.277s, learning 0.213s)
               Value function loss: 109053.1677
                    Surrogate loss: -0.0032
             Mean action noise std: 0.94
                       Mean reward: 12032.46
               Mean episode length: 458.35
                 Mean success rate: 93.50
                  Mean reward/step: 23.61
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7249920
                    Iteration time: 0.49s
                        Total time: 440.99s
                               ETA: 556.1s

################################################################################
                     [1m Learning iteration 885/2000 [0m

                       Computation: 16656 steps/s (collection: 0.265s, learning 0.227s)
               Value function loss: 66570.3483
                    Surrogate loss: -0.0011
             Mean action noise std: 0.94
                       Mean reward: 12071.37
               Mean episode length: 461.15
                 Mean success rate: 94.00
                  Mean reward/step: 22.77
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 0.49s
                        Total time: 441.48s
                               ETA: 555.6s

################################################################################
                     [1m Learning iteration 886/2000 [0m

                       Computation: 16456 steps/s (collection: 0.287s, learning 0.211s)
               Value function loss: 100469.3020
                    Surrogate loss: -0.0030
             Mean action noise std: 0.94
                       Mean reward: 11698.60
               Mean episode length: 449.97
                 Mean success rate: 92.50
                  Mean reward/step: 23.70
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7266304
                    Iteration time: 0.50s
                        Total time: 441.98s
                               ETA: 555.1s

################################################################################
                     [1m Learning iteration 887/2000 [0m

                       Computation: 16553 steps/s (collection: 0.272s, learning 0.223s)
               Value function loss: 47713.6897
                    Surrogate loss: -0.0006
             Mean action noise std: 0.94
                       Mean reward: 11712.51
               Mean episode length: 452.05
                 Mean success rate: 93.00
                  Mean reward/step: 24.64
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.49s
                        Total time: 442.47s
                               ETA: 554.6s

################################################################################
                     [1m Learning iteration 888/2000 [0m

                       Computation: 16913 steps/s (collection: 0.278s, learning 0.206s)
               Value function loss: 76924.2711
                    Surrogate loss: -0.0042
             Mean action noise std: 0.94
                       Mean reward: 11669.70
               Mean episode length: 455.38
                 Mean success rate: 93.50
                  Mean reward/step: 25.51
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7282688
                    Iteration time: 0.48s
                        Total time: 442.96s
                               ETA: 554.1s

################################################################################
                     [1m Learning iteration 889/2000 [0m

                       Computation: 17516 steps/s (collection: 0.267s, learning 0.200s)
               Value function loss: 59463.9078
                    Surrogate loss: -0.0033
             Mean action noise std: 0.94
                       Mean reward: 11433.18
               Mean episode length: 448.61
                 Mean success rate: 92.50
                  Mean reward/step: 25.70
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 0.47s
                        Total time: 443.42s
                               ETA: 553.5s

################################################################################
                     [1m Learning iteration 890/2000 [0m

                       Computation: 16797 steps/s (collection: 0.282s, learning 0.206s)
               Value function loss: 115055.4669
                    Surrogate loss: 0.0069
             Mean action noise std: 0.94
                       Mean reward: 10938.07
               Mean episode length: 433.34
                 Mean success rate: 90.00
                  Mean reward/step: 26.33
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7299072
                    Iteration time: 0.49s
                        Total time: 443.91s
                               ETA: 553.0s

################################################################################
                     [1m Learning iteration 891/2000 [0m

                       Computation: 17161 steps/s (collection: 0.263s, learning 0.214s)
               Value function loss: 80176.2760
                    Surrogate loss: 0.0068
             Mean action noise std: 0.94
                       Mean reward: 11006.39
               Mean episode length: 437.27
                 Mean success rate: 91.00
                  Mean reward/step: 26.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 0.48s
                        Total time: 444.39s
                               ETA: 552.5s

################################################################################
                     [1m Learning iteration 892/2000 [0m

                       Computation: 17123 steps/s (collection: 0.267s, learning 0.211s)
               Value function loss: 105076.6201
                    Surrogate loss: 0.0036
             Mean action noise std: 0.94
                       Mean reward: 10853.59
               Mean episode length: 434.95
                 Mean success rate: 90.50
                  Mean reward/step: 24.21
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7315456
                    Iteration time: 0.48s
                        Total time: 444.87s
                               ETA: 552.0s

################################################################################
                     [1m Learning iteration 893/2000 [0m

                       Computation: 17659 steps/s (collection: 0.249s, learning 0.214s)
               Value function loss: 88984.2001
                    Surrogate loss: 0.0053
             Mean action noise std: 0.94
                       Mean reward: 10937.45
               Mean episode length: 438.42
                 Mean success rate: 91.00
                  Mean reward/step: 24.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 0.46s
                        Total time: 445.33s
                               ETA: 551.4s

################################################################################
                     [1m Learning iteration 894/2000 [0m

                       Computation: 17014 steps/s (collection: 0.274s, learning 0.208s)
               Value function loss: 70483.6032
                    Surrogate loss: -0.0041
             Mean action noise std: 0.94
                       Mean reward: 10870.71
               Mean episode length: 437.55
                 Mean success rate: 90.50
                  Mean reward/step: 25.33
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7331840
                    Iteration time: 0.48s
                        Total time: 445.81s
                               ETA: 550.9s

################################################################################
                     [1m Learning iteration 895/2000 [0m

                       Computation: 17864 steps/s (collection: 0.253s, learning 0.206s)
               Value function loss: 52472.9875
                    Surrogate loss: -0.0027
             Mean action noise std: 0.94
                       Mean reward: 10909.49
               Mean episode length: 441.37
                 Mean success rate: 91.00
                  Mean reward/step: 24.71
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 0.46s
                        Total time: 446.27s
                               ETA: 550.4s

################################################################################
                     [1m Learning iteration 896/2000 [0m

                       Computation: 16595 steps/s (collection: 0.276s, learning 0.217s)
               Value function loss: 83231.1164
                    Surrogate loss: -0.0031
             Mean action noise std: 0.94
                       Mean reward: 10951.25
               Mean episode length: 443.38
                 Mean success rate: 91.50
                  Mean reward/step: 24.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7348224
                    Iteration time: 0.49s
                        Total time: 446.77s
                               ETA: 549.9s

################################################################################
                     [1m Learning iteration 897/2000 [0m

                       Computation: 17511 steps/s (collection: 0.262s, learning 0.206s)
               Value function loss: 62029.2720
                    Surrogate loss: -0.0063
             Mean action noise std: 0.94
                       Mean reward: 10984.37
               Mean episode length: 445.23
                 Mean success rate: 92.00
                  Mean reward/step: 25.23
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 0.47s
                        Total time: 447.23s
                               ETA: 549.3s

################################################################################
                     [1m Learning iteration 898/2000 [0m

                       Computation: 16968 steps/s (collection: 0.270s, learning 0.212s)
               Value function loss: 45093.7096
                    Surrogate loss: -0.0019
             Mean action noise std: 0.93
                       Mean reward: 11063.70
               Mean episode length: 445.23
                 Mean success rate: 92.50
                  Mean reward/step: 25.89
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 7364608
                    Iteration time: 0.48s
                        Total time: 447.72s
                               ETA: 548.8s

################################################################################
                     [1m Learning iteration 899/2000 [0m

                       Computation: 17759 steps/s (collection: 0.257s, learning 0.204s)
               Value function loss: 90622.9698
                    Surrogate loss: -0.0006
             Mean action noise std: 0.94
                       Mean reward: 11142.55
               Mean episode length: 447.27
                 Mean success rate: 92.50
                  Mean reward/step: 26.27
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.46s
                        Total time: 448.18s
                               ETA: 548.3s

################################################################################
                     [1m Learning iteration 900/2000 [0m

                       Computation: 17478 steps/s (collection: 0.254s, learning 0.215s)
               Value function loss: 94466.3402
                    Surrogate loss: 0.0025
             Mean action noise std: 0.93
                       Mean reward: 11350.29
               Mean episode length: 454.05
                 Mean success rate: 93.50
                  Mean reward/step: 25.63
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7380992
                    Iteration time: 0.47s
                        Total time: 448.65s
                               ETA: 547.7s

################################################################################
                     [1m Learning iteration 901/2000 [0m

                       Computation: 17182 steps/s (collection: 0.267s, learning 0.210s)
               Value function loss: 69853.0684
                    Surrogate loss: -0.0016
             Mean action noise std: 0.93
                       Mean reward: 11633.32
               Mean episode length: 463.31
                 Mean success rate: 94.50
                  Mean reward/step: 26.70
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 0.48s
                        Total time: 449.12s
                               ETA: 547.2s

################################################################################
                     [1m Learning iteration 902/2000 [0m

                       Computation: 17201 steps/s (collection: 0.261s, learning 0.215s)
               Value function loss: 91485.6495
                    Surrogate loss: -0.0007
             Mean action noise std: 0.94
                       Mean reward: 11702.45
               Mean episode length: 464.13
                 Mean success rate: 95.00
                  Mean reward/step: 27.20
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7397376
                    Iteration time: 0.48s
                        Total time: 449.60s
                               ETA: 546.7s

################################################################################
                     [1m Learning iteration 903/2000 [0m

                       Computation: 16434 steps/s (collection: 0.252s, learning 0.246s)
               Value function loss: 78761.3827
                    Surrogate loss: 0.0033
             Mean action noise std: 0.94
                       Mean reward: 11850.99
               Mean episode length: 468.30
                 Mean success rate: 95.50
                  Mean reward/step: 27.01
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 0.50s
                        Total time: 450.10s
                               ETA: 546.2s

################################################################################
                     [1m Learning iteration 904/2000 [0m

                       Computation: 17678 steps/s (collection: 0.255s, learning 0.209s)
               Value function loss: 79597.8637
                    Surrogate loss: 0.0000
             Mean action noise std: 0.94
                       Mean reward: 11979.85
               Mean episode length: 472.35
                 Mean success rate: 96.00
                  Mean reward/step: 26.76
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7413760
                    Iteration time: 0.46s
                        Total time: 450.56s
                               ETA: 545.7s

################################################################################
                     [1m Learning iteration 905/2000 [0m

                       Computation: 16884 steps/s (collection: 0.284s, learning 0.201s)
               Value function loss: 93858.2617
                    Surrogate loss: 0.0031
             Mean action noise std: 0.94
                       Mean reward: 11978.42
               Mean episode length: 472.35
                 Mean success rate: 96.00
                  Mean reward/step: 26.86
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 0.49s
                        Total time: 451.05s
                               ETA: 545.1s

################################################################################
                     [1m Learning iteration 906/2000 [0m

                       Computation: 17294 steps/s (collection: 0.273s, learning 0.200s)
               Value function loss: 100109.1064
                    Surrogate loss: 0.0016
             Mean action noise std: 0.94
                       Mean reward: 12180.18
               Mean episode length: 477.32
                 Mean success rate: 97.00
                  Mean reward/step: 25.60
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7430144
                    Iteration time: 0.47s
                        Total time: 451.52s
                               ETA: 544.6s

################################################################################
                     [1m Learning iteration 907/2000 [0m

                       Computation: 17504 steps/s (collection: 0.268s, learning 0.200s)
               Value function loss: 110119.6139
                    Surrogate loss: -0.0027
             Mean action noise std: 0.93
                       Mean reward: 12259.74
               Mean episode length: 477.32
                 Mean success rate: 97.00
                  Mean reward/step: 25.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 0.47s
                        Total time: 451.99s
                               ETA: 544.1s

################################################################################
                     [1m Learning iteration 908/2000 [0m

                       Computation: 16325 steps/s (collection: 0.269s, learning 0.233s)
               Value function loss: 69572.0837
                    Surrogate loss: 0.0028
             Mean action noise std: 0.94
                       Mean reward: 12418.57
               Mean episode length: 481.94
                 Mean success rate: 97.00
                  Mean reward/step: 25.53
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7446528
                    Iteration time: 0.50s
                        Total time: 452.49s
                               ETA: 543.6s

################################################################################
                     [1m Learning iteration 909/2000 [0m

                       Computation: 17198 steps/s (collection: 0.269s, learning 0.207s)
               Value function loss: 69659.0265
                    Surrogate loss: -0.0025
             Mean action noise std: 0.94
                       Mean reward: 12261.63
               Mean episode length: 476.38
                 Mean success rate: 96.00
                  Mean reward/step: 26.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 0.48s
                        Total time: 452.97s
                               ETA: 543.1s

################################################################################
                     [1m Learning iteration 910/2000 [0m

                       Computation: 16276 steps/s (collection: 0.284s, learning 0.220s)
               Value function loss: 73260.4259
                    Surrogate loss: 0.0069
             Mean action noise std: 0.94
                       Mean reward: 11944.24
               Mean episode length: 464.69
                 Mean success rate: 94.00
                  Mean reward/step: 26.90
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7462912
                    Iteration time: 0.50s
                        Total time: 453.47s
                               ETA: 542.6s

################################################################################
                     [1m Learning iteration 911/2000 [0m

                       Computation: 17168 steps/s (collection: 0.276s, learning 0.202s)
               Value function loss: 74577.5081
                    Surrogate loss: -0.0035
             Mean action noise std: 0.94
                       Mean reward: 12000.00
               Mean episode length: 464.69
                 Mean success rate: 94.00
                  Mean reward/step: 26.90
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.48s
                        Total time: 453.95s
                               ETA: 542.0s

################################################################################
                     [1m Learning iteration 912/2000 [0m

                       Computation: 16822 steps/s (collection: 0.276s, learning 0.211s)
               Value function loss: 90706.5525
                    Surrogate loss: -0.0031
             Mean action noise std: 0.94
                       Mean reward: 11960.88
               Mean episode length: 460.31
                 Mean success rate: 93.50
                  Mean reward/step: 26.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7479296
                    Iteration time: 0.49s
                        Total time: 454.43s
                               ETA: 541.5s

################################################################################
                     [1m Learning iteration 913/2000 [0m

                       Computation: 16893 steps/s (collection: 0.273s, learning 0.212s)
               Value function loss: 84323.1484
                    Surrogate loss: -0.0002
             Mean action noise std: 0.94
                       Mean reward: 12045.33
               Mean episode length: 462.61
                 Mean success rate: 94.00
                  Mean reward/step: 26.35
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 0.48s
                        Total time: 454.92s
                               ETA: 541.0s

################################################################################
                     [1m Learning iteration 914/2000 [0m

                       Computation: 17777 steps/s (collection: 0.259s, learning 0.202s)
               Value function loss: 38992.8090
                    Surrogate loss: 0.0306
             Mean action noise std: 0.94
                       Mean reward: 11944.91
               Mean episode length: 458.32
                 Mean success rate: 93.50
                  Mean reward/step: 26.71
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 7495680
                    Iteration time: 0.46s
                        Total time: 455.38s
                               ETA: 540.5s

################################################################################
                     [1m Learning iteration 915/2000 [0m

                       Computation: 17559 steps/s (collection: 0.257s, learning 0.209s)
               Value function loss: 125127.1908
                    Surrogate loss: -0.0023
             Mean action noise std: 0.94
                       Mean reward: 11871.08
               Mean episode length: 455.55
                 Mean success rate: 92.50
                  Mean reward/step: 26.37
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 0.47s
                        Total time: 455.85s
                               ETA: 539.9s

################################################################################
                     [1m Learning iteration 916/2000 [0m

                       Computation: 17287 steps/s (collection: 0.264s, learning 0.210s)
               Value function loss: 67286.7442
                    Surrogate loss: -0.0060
             Mean action noise std: 0.94
                       Mean reward: 11541.70
               Mean episode length: 441.25
                 Mean success rate: 91.00
                  Mean reward/step: 25.44
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7512064
                    Iteration time: 0.47s
                        Total time: 456.32s
                               ETA: 539.4s

################################################################################
                     [1m Learning iteration 917/2000 [0m

                       Computation: 17437 steps/s (collection: 0.255s, learning 0.215s)
               Value function loss: 88302.7498
                    Surrogate loss: -0.0059
             Mean action noise std: 0.94
                       Mean reward: 11336.67
               Mean episode length: 437.08
                 Mean success rate: 90.50
                  Mean reward/step: 25.84
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 0.47s
                        Total time: 456.79s
                               ETA: 538.9s

################################################################################
                     [1m Learning iteration 918/2000 [0m

                       Computation: 18219 steps/s (collection: 0.248s, learning 0.201s)
               Value function loss: 65551.6821
                    Surrogate loss: -0.0059
             Mean action noise std: 0.94
                       Mean reward: 11329.95
               Mean episode length: 437.08
                 Mean success rate: 90.50
                  Mean reward/step: 26.00
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7528448
                    Iteration time: 0.45s
                        Total time: 457.24s
                               ETA: 538.3s

################################################################################
                     [1m Learning iteration 919/2000 [0m

                       Computation: 17497 steps/s (collection: 0.256s, learning 0.212s)
               Value function loss: 75563.5155
                    Surrogate loss: -0.0046
             Mean action noise std: 0.93
                       Mean reward: 11366.41
               Mean episode length: 437.08
                 Mean success rate: 90.50
                  Mean reward/step: 27.07
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 0.47s
                        Total time: 457.71s
                               ETA: 537.8s

################################################################################
                     [1m Learning iteration 920/2000 [0m

                       Computation: 17271 steps/s (collection: 0.264s, learning 0.210s)
               Value function loss: 90160.0589
                    Surrogate loss: -0.0048
             Mean action noise std: 0.94
                       Mean reward: 11432.37
               Mean episode length: 437.00
                 Mean success rate: 90.50
                  Mean reward/step: 27.03
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7544832
                    Iteration time: 0.47s
                        Total time: 458.18s
                               ETA: 537.3s

################################################################################
                     [1m Learning iteration 921/2000 [0m

                       Computation: 17684 steps/s (collection: 0.259s, learning 0.204s)
               Value function loss: 132412.3068
                    Surrogate loss: -0.0038
             Mean action noise std: 0.94
                       Mean reward: 11703.50
               Mean episode length: 444.01
                 Mean success rate: 91.50
                  Mean reward/step: 26.48
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 0.46s
                        Total time: 458.64s
                               ETA: 536.7s

################################################################################
                     [1m Learning iteration 922/2000 [0m

                       Computation: 17638 steps/s (collection: 0.258s, learning 0.206s)
               Value function loss: 69948.6992
                    Surrogate loss: -0.0044
             Mean action noise std: 0.94
                       Mean reward: 11525.25
               Mean episode length: 438.79
                 Mean success rate: 89.50
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7561216
                    Iteration time: 0.46s
                        Total time: 459.11s
                               ETA: 536.2s

################################################################################
                     [1m Learning iteration 923/2000 [0m

                       Computation: 17952 steps/s (collection: 0.254s, learning 0.202s)
               Value function loss: 114715.6701
                    Surrogate loss: -0.0029
             Mean action noise std: 0.94
                       Mean reward: 11368.03
               Mean episode length: 434.43
                 Mean success rate: 88.50
                  Mean reward/step: 25.83
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.46s
                        Total time: 459.56s
                               ETA: 535.7s

################################################################################
                     [1m Learning iteration 924/2000 [0m

                       Computation: 17826 steps/s (collection: 0.244s, learning 0.216s)
               Value function loss: 89482.0930
                    Surrogate loss: 0.0008
             Mean action noise std: 0.94
                       Mean reward: 11270.09
               Mean episode length: 429.19
                 Mean success rate: 87.00
                  Mean reward/step: 26.29
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7577600
                    Iteration time: 0.46s
                        Total time: 460.02s
                               ETA: 535.1s

################################################################################
                     [1m Learning iteration 925/2000 [0m

                       Computation: 17449 steps/s (collection: 0.256s, learning 0.213s)
               Value function loss: 63807.2766
                    Surrogate loss: -0.0008
             Mean action noise std: 0.94
                       Mean reward: 11281.02
               Mean episode length: 430.24
                 Mean success rate: 87.00
                  Mean reward/step: 26.19
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 0.47s
                        Total time: 460.49s
                               ETA: 534.6s

################################################################################
                     [1m Learning iteration 926/2000 [0m

                       Computation: 17556 steps/s (collection: 0.254s, learning 0.212s)
               Value function loss: 81040.9536
                    Surrogate loss: 0.0230
             Mean action noise std: 0.94
                       Mean reward: 11544.65
               Mean episode length: 440.11
                 Mean success rate: 88.00
                  Mean reward/step: 25.48
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7593984
                    Iteration time: 0.47s
                        Total time: 460.96s
                               ETA: 534.1s

################################################################################
                     [1m Learning iteration 927/2000 [0m

                       Computation: 17416 steps/s (collection: 0.249s, learning 0.221s)
               Value function loss: 85218.8842
                    Surrogate loss: -0.0036
             Mean action noise std: 0.94
                       Mean reward: 11835.06
               Mean episode length: 449.50
                 Mean success rate: 89.50
                  Mean reward/step: 21.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 0.47s
                        Total time: 461.43s
                               ETA: 533.5s

################################################################################
                     [1m Learning iteration 928/2000 [0m

                       Computation: 17392 steps/s (collection: 0.252s, learning 0.219s)
               Value function loss: 67964.3104
                    Surrogate loss: -0.0045
             Mean action noise std: 0.94
                       Mean reward: 11845.75
               Mean episode length: 449.57
                 Mean success rate: 89.00
                  Mean reward/step: 20.74
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7610368
                    Iteration time: 0.47s
                        Total time: 461.90s
                               ETA: 533.0s

################################################################################
                     [1m Learning iteration 929/2000 [0m

                       Computation: 17581 steps/s (collection: 0.258s, learning 0.208s)
               Value function loss: 53047.2570
                    Surrogate loss: 0.0045
             Mean action noise std: 0.94
                       Mean reward: 11796.19
               Mean episode length: 449.57
                 Mean success rate: 89.00
                  Mean reward/step: 21.71
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 0.47s
                        Total time: 462.37s
                               ETA: 532.5s

################################################################################
                     [1m Learning iteration 930/2000 [0m

                       Computation: 17742 steps/s (collection: 0.254s, learning 0.208s)
               Value function loss: 55893.3326
                    Surrogate loss: -0.0054
             Mean action noise std: 0.94
                       Mean reward: 11518.30
               Mean episode length: 441.06
                 Mean success rate: 87.50
                  Mean reward/step: 23.24
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7626752
                    Iteration time: 0.46s
                        Total time: 462.83s
                               ETA: 531.9s

################################################################################
                     [1m Learning iteration 931/2000 [0m

                       Computation: 16691 steps/s (collection: 0.283s, learning 0.207s)
               Value function loss: 107187.0646
                    Surrogate loss: -0.0045
             Mean action noise std: 0.94
                       Mean reward: 11537.69
               Mean episode length: 445.75
                 Mean success rate: 88.50
                  Mean reward/step: 23.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 0.49s
                        Total time: 463.32s
                               ETA: 531.4s

################################################################################
                     [1m Learning iteration 932/2000 [0m

                       Computation: 16598 steps/s (collection: 0.287s, learning 0.206s)
               Value function loss: 68105.1403
                    Surrogate loss: -0.0057
             Mean action noise std: 0.94
                       Mean reward: 11139.55
               Mean episode length: 435.13
                 Mean success rate: 87.00
                  Mean reward/step: 24.11
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7643136
                    Iteration time: 0.49s
                        Total time: 463.81s
                               ETA: 530.9s

################################################################################
                     [1m Learning iteration 933/2000 [0m

                       Computation: 16959 steps/s (collection: 0.279s, learning 0.204s)
               Value function loss: 99869.4964
                    Surrogate loss: 0.0003
             Mean action noise std: 0.94
                       Mean reward: 11277.82
               Mean episode length: 440.91
                 Mean success rate: 88.50
                  Mean reward/step: 24.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 0.48s
                        Total time: 464.30s
                               ETA: 530.4s

################################################################################
                     [1m Learning iteration 934/2000 [0m

                       Computation: 18119 steps/s (collection: 0.251s, learning 0.201s)
               Value function loss: 52611.1904
                    Surrogate loss: -0.0022
             Mean action noise std: 0.94
                       Mean reward: 11213.35
               Mean episode length: 440.91
                 Mean success rate: 88.50
                  Mean reward/step: 24.25
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 7659520
                    Iteration time: 0.45s
                        Total time: 464.75s
                               ETA: 529.9s

################################################################################
                     [1m Learning iteration 935/2000 [0m

                       Computation: 16938 steps/s (collection: 0.272s, learning 0.211s)
               Value function loss: 65871.1922
                    Surrogate loss: -0.0029
             Mean action noise std: 0.94
                       Mean reward: 11322.01
               Mean episode length: 446.49
                 Mean success rate: 89.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.48s
                        Total time: 465.23s
                               ETA: 529.4s

################################################################################
                     [1m Learning iteration 936/2000 [0m

                       Computation: 17831 steps/s (collection: 0.258s, learning 0.202s)
               Value function loss: 69241.4426
                    Surrogate loss: 0.0022
             Mean action noise std: 0.94
                       Mean reward: 11265.52
               Mean episode length: 448.83
                 Mean success rate: 91.00
                  Mean reward/step: 24.78
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7675904
                    Iteration time: 0.46s
                        Total time: 465.69s
                               ETA: 528.8s

################################################################################
                     [1m Learning iteration 937/2000 [0m

                       Computation: 17304 steps/s (collection: 0.264s, learning 0.209s)
               Value function loss: 143599.7889
                    Surrogate loss: -0.0035
             Mean action noise std: 0.94
                       Mean reward: 11096.77
               Mean episode length: 447.19
                 Mean success rate: 91.00
                  Mean reward/step: 24.33
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 0.47s
                        Total time: 466.17s
                               ETA: 528.3s

################################################################################
                     [1m Learning iteration 938/2000 [0m

                       Computation: 17303 steps/s (collection: 0.269s, learning 0.204s)
               Value function loss: 73518.9604
                    Surrogate loss: 0.0012
             Mean action noise std: 0.94
                       Mean reward: 11006.87
               Mean episode length: 447.21
                 Mean success rate: 91.00
                  Mean reward/step: 24.68
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7692288
                    Iteration time: 0.47s
                        Total time: 466.64s
                               ETA: 527.8s

################################################################################
                     [1m Learning iteration 939/2000 [0m

                       Computation: 17023 steps/s (collection: 0.272s, learning 0.209s)
               Value function loss: 98090.7367
                    Surrogate loss: -0.0030
             Mean action noise std: 0.94
                       Mean reward: 11089.71
               Mean episode length: 451.75
                 Mean success rate: 92.00
                  Mean reward/step: 24.27
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 0.48s
                        Total time: 467.12s
                               ETA: 527.2s

################################################################################
                     [1m Learning iteration 940/2000 [0m

                       Computation: 16328 steps/s (collection: 0.266s, learning 0.236s)
               Value function loss: 100493.7812
                    Surrogate loss: -0.0029
             Mean action noise std: 0.94
                       Mean reward: 11234.29
               Mean episode length: 460.26
                 Mean success rate: 93.50
                  Mean reward/step: 24.80
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7708672
                    Iteration time: 0.50s
                        Total time: 467.62s
                               ETA: 526.8s

################################################################################
                     [1m Learning iteration 941/2000 [0m

                       Computation: 17586 steps/s (collection: 0.257s, learning 0.208s)
               Value function loss: 63338.8958
                    Surrogate loss: 0.0011
             Mean action noise std: 0.94
                       Mean reward: 10939.98
               Mean episode length: 450.83
                 Mean success rate: 91.50
                  Mean reward/step: 25.01
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 0.47s
                        Total time: 468.09s
                               ETA: 526.2s

################################################################################
                     [1m Learning iteration 942/2000 [0m

                       Computation: 17823 steps/s (collection: 0.258s, learning 0.202s)
               Value function loss: 64111.6954
                    Surrogate loss: -0.0037
             Mean action noise std: 0.94
                       Mean reward: 10831.80
               Mean episode length: 446.75
                 Mean success rate: 91.00
                  Mean reward/step: 25.27
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7725056
                    Iteration time: 0.46s
                        Total time: 468.55s
                               ETA: 525.7s

################################################################################
                     [1m Learning iteration 943/2000 [0m

                       Computation: 17912 steps/s (collection: 0.254s, learning 0.204s)
               Value function loss: 90265.1517
                    Surrogate loss: -0.0011
             Mean action noise std: 0.94
                       Mean reward: 11205.89
               Mean episode length: 460.71
                 Mean success rate: 94.00
                  Mean reward/step: 25.23
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 0.46s
                        Total time: 469.00s
                               ETA: 525.1s

################################################################################
                     [1m Learning iteration 944/2000 [0m

                       Computation: 17079 steps/s (collection: 0.263s, learning 0.217s)
               Value function loss: 77822.4002
                    Surrogate loss: -0.0037
             Mean action noise std: 0.94
                       Mean reward: 11151.92
               Mean episode length: 460.71
                 Mean success rate: 94.00
                  Mean reward/step: 24.55
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7741440
                    Iteration time: 0.48s
                        Total time: 469.48s
                               ETA: 524.6s

################################################################################
                     [1m Learning iteration 945/2000 [0m

                       Computation: 18111 steps/s (collection: 0.249s, learning 0.204s)
               Value function loss: 53529.7997
                    Surrogate loss: -0.0055
             Mean action noise std: 0.94
                       Mean reward: 10846.43
               Mean episode length: 449.31
                 Mean success rate: 92.50
                  Mean reward/step: 25.29
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 0.45s
                        Total time: 469.94s
                               ETA: 524.1s

################################################################################
                     [1m Learning iteration 946/2000 [0m

                       Computation: 16949 steps/s (collection: 0.281s, learning 0.202s)
               Value function loss: 91943.5149
                    Surrogate loss: -0.0008
             Mean action noise std: 0.94
                       Mean reward: 10830.22
               Mean episode length: 447.24
                 Mean success rate: 92.50
                  Mean reward/step: 26.31
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7757824
                    Iteration time: 0.48s
                        Total time: 470.42s
                               ETA: 523.6s

################################################################################
                     [1m Learning iteration 947/2000 [0m

                       Computation: 18081 steps/s (collection: 0.254s, learning 0.199s)
               Value function loss: 108276.1977
                    Surrogate loss: -0.0003
             Mean action noise std: 0.94
                       Mean reward: 11219.12
               Mean episode length: 458.77
                 Mean success rate: 94.00
                  Mean reward/step: 26.20
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.45s
                        Total time: 470.87s
                               ETA: 523.0s

################################################################################
                     [1m Learning iteration 948/2000 [0m

                       Computation: 17693 steps/s (collection: 0.258s, learning 0.205s)
               Value function loss: 91489.8455
                    Surrogate loss: -0.0053
             Mean action noise std: 0.94
                       Mean reward: 10961.41
               Mean episode length: 450.42
                 Mean success rate: 92.50
                  Mean reward/step: 26.18
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7774208
                    Iteration time: 0.46s
                        Total time: 471.34s
                               ETA: 522.5s

################################################################################
                     [1m Learning iteration 949/2000 [0m

                       Computation: 17987 steps/s (collection: 0.249s, learning 0.207s)
               Value function loss: 102053.3364
                    Surrogate loss: -0.0046
             Mean action noise std: 0.94
                       Mean reward: 10816.78
               Mean episode length: 441.80
                 Mean success rate: 91.00
                  Mean reward/step: 25.36
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 0.46s
                        Total time: 471.79s
                               ETA: 522.0s

################################################################################
                     [1m Learning iteration 950/2000 [0m

                       Computation: 18527 steps/s (collection: 0.242s, learning 0.200s)
               Value function loss: 68065.8627
                    Surrogate loss: -0.0056
             Mean action noise std: 0.94
                       Mean reward: 10663.21
               Mean episode length: 434.09
                 Mean success rate: 89.50
                  Mean reward/step: 26.57
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7790592
                    Iteration time: 0.44s
                        Total time: 472.23s
                               ETA: 521.4s

################################################################################
                     [1m Learning iteration 951/2000 [0m

                       Computation: 18359 steps/s (collection: 0.247s, learning 0.199s)
               Value function loss: 76481.1467
                    Surrogate loss: -0.0037
             Mean action noise std: 0.94
                       Mean reward: 10366.85
               Mean episode length: 418.76
                 Mean success rate: 87.50
                  Mean reward/step: 27.18
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 0.45s
                        Total time: 472.68s
                               ETA: 520.8s

################################################################################
                     [1m Learning iteration 952/2000 [0m

                       Computation: 17526 steps/s (collection: 0.268s, learning 0.199s)
               Value function loss: 104277.5736
                    Surrogate loss: -0.0024
             Mean action noise std: 0.94
                       Mean reward: 10591.85
               Mean episode length: 423.69
                 Mean success rate: 89.00
                  Mean reward/step: 26.78
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7806976
                    Iteration time: 0.47s
                        Total time: 473.15s
                               ETA: 520.3s

################################################################################
                     [1m Learning iteration 953/2000 [0m

                       Computation: 17460 steps/s (collection: 0.270s, learning 0.200s)
               Value function loss: 83025.1395
                    Surrogate loss: -0.0049
             Mean action noise std: 0.95
                       Mean reward: 10602.06
               Mean episode length: 425.48
                 Mean success rate: 88.50
                  Mean reward/step: 25.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 0.47s
                        Total time: 473.62s
                               ETA: 519.8s

################################################################################
                     [1m Learning iteration 954/2000 [0m

                       Computation: 17545 steps/s (collection: 0.258s, learning 0.209s)
               Value function loss: 111788.9695
                    Surrogate loss: -0.0061
             Mean action noise std: 0.95
                       Mean reward: 10574.20
               Mean episode length: 420.34
                 Mean success rate: 88.00
                  Mean reward/step: 25.93
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7823360
                    Iteration time: 0.47s
                        Total time: 474.08s
                               ETA: 519.3s

################################################################################
                     [1m Learning iteration 955/2000 [0m

                       Computation: 18208 steps/s (collection: 0.252s, learning 0.198s)
               Value function loss: 73842.3839
                    Surrogate loss: -0.0027
             Mean action noise std: 0.94
                       Mean reward: 10781.97
               Mean episode length: 427.63
                 Mean success rate: 89.00
                  Mean reward/step: 26.26
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 0.45s
                        Total time: 474.53s
                               ETA: 518.7s

################################################################################
                     [1m Learning iteration 956/2000 [0m

                       Computation: 17594 steps/s (collection: 0.265s, learning 0.201s)
               Value function loss: 84167.1452
                    Surrogate loss: -0.0042
             Mean action noise std: 0.94
                       Mean reward: 10846.16
               Mean episode length: 428.26
                 Mean success rate: 89.00
                  Mean reward/step: 26.61
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7839744
                    Iteration time: 0.47s
                        Total time: 475.00s
                               ETA: 518.2s

################################################################################
                     [1m Learning iteration 957/2000 [0m

                       Computation: 17052 steps/s (collection: 0.273s, learning 0.207s)
               Value function loss: 86046.8310
                    Surrogate loss: -0.0035
             Mean action noise std: 0.95
                       Mean reward: 10970.83
               Mean episode length: 431.09
                 Mean success rate: 89.50
                  Mean reward/step: 26.63
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 0.48s
                        Total time: 475.48s
                               ETA: 517.7s

################################################################################
                     [1m Learning iteration 958/2000 [0m

                       Computation: 17857 steps/s (collection: 0.253s, learning 0.205s)
               Value function loss: 66768.7886
                    Surrogate loss: -0.0054
             Mean action noise std: 0.94
                       Mean reward: 10856.80
               Mean episode length: 427.19
                 Mean success rate: 88.00
                  Mean reward/step: 26.75
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7856128
                    Iteration time: 0.46s
                        Total time: 475.94s
                               ETA: 517.1s

################################################################################
                     [1m Learning iteration 959/2000 [0m

                       Computation: 17558 steps/s (collection: 0.263s, learning 0.204s)
               Value function loss: 88145.7585
                    Surrogate loss: -0.0008
             Mean action noise std: 0.94
                       Mean reward: 10962.89
               Mean episode length: 427.80
                 Mean success rate: 88.50
                  Mean reward/step: 26.22
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.47s
                        Total time: 476.40s
                               ETA: 516.6s

################################################################################
                     [1m Learning iteration 960/2000 [0m

                       Computation: 17116 steps/s (collection: 0.276s, learning 0.202s)
               Value function loss: 84779.9542
                    Surrogate loss: -0.0010
             Mean action noise std: 0.94
                       Mean reward: 11170.81
               Mean episode length: 435.30
                 Mean success rate: 89.50
                  Mean reward/step: 26.50
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7872512
                    Iteration time: 0.48s
                        Total time: 476.88s
                               ETA: 516.1s

################################################################################
                     [1m Learning iteration 961/2000 [0m

                       Computation: 16830 steps/s (collection: 0.273s, learning 0.214s)
               Value function loss: 50903.2515
                    Surrogate loss: 0.0032
             Mean action noise std: 0.95
                       Mean reward: 11307.28
               Mean episode length: 440.06
                 Mean success rate: 90.50
                  Mean reward/step: 27.28
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 0.49s
                        Total time: 477.37s
                               ETA: 515.6s

################################################################################
                     [1m Learning iteration 962/2000 [0m

                       Computation: 17511 steps/s (collection: 0.267s, learning 0.201s)
               Value function loss: 108911.2632
                    Surrogate loss: 0.0031
             Mean action noise std: 0.95
                       Mean reward: 11742.02
               Mean episode length: 455.39
                 Mean success rate: 92.50
                  Mean reward/step: 27.27
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7888896
                    Iteration time: 0.47s
                        Total time: 477.84s
                               ETA: 515.1s

################################################################################
                     [1m Learning iteration 963/2000 [0m

                       Computation: 16929 steps/s (collection: 0.274s, learning 0.210s)
               Value function loss: 106034.8022
                    Surrogate loss: 0.0020
             Mean action noise std: 0.94
                       Mean reward: 11910.62
               Mean episode length: 463.97
                 Mean success rate: 93.50
                  Mean reward/step: 26.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 0.48s
                        Total time: 478.32s
                               ETA: 514.5s

################################################################################
                     [1m Learning iteration 964/2000 [0m

                       Computation: 17091 steps/s (collection: 0.274s, learning 0.206s)
               Value function loss: 104450.1316
                    Surrogate loss: 0.0043
             Mean action noise std: 0.95
                       Mean reward: 12021.71
               Mean episode length: 466.75
                 Mean success rate: 94.00
                  Mean reward/step: 26.88
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7905280
                    Iteration time: 0.48s
                        Total time: 478.80s
                               ETA: 514.0s

################################################################################
                     [1m Learning iteration 965/2000 [0m

                       Computation: 17142 steps/s (collection: 0.270s, learning 0.208s)
               Value function loss: 61472.1498
                    Surrogate loss: -0.0036
             Mean action noise std: 0.94
                       Mean reward: 12116.02
               Mean episode length: 467.96
                 Mean success rate: 94.50
                  Mean reward/step: 26.70
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 0.48s
                        Total time: 479.28s
                               ETA: 513.5s

################################################################################
                     [1m Learning iteration 966/2000 [0m

                       Computation: 17307 steps/s (collection: 0.267s, learning 0.206s)
               Value function loss: 87302.5433
                    Surrogate loss: -0.0031
             Mean action noise std: 0.94
                       Mean reward: 12333.44
               Mean episode length: 472.61
                 Mean success rate: 95.00
                  Mean reward/step: 27.42
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7921664
                    Iteration time: 0.47s
                        Total time: 479.75s
                               ETA: 513.0s

################################################################################
                     [1m Learning iteration 967/2000 [0m

                       Computation: 17979 steps/s (collection: 0.251s, learning 0.205s)
               Value function loss: 91666.4752
                    Surrogate loss: -0.0023
             Mean action noise std: 0.94
                       Mean reward: 12574.27
               Mean episode length: 477.20
                 Mean success rate: 95.50
                  Mean reward/step: 26.56
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 0.46s
                        Total time: 480.21s
                               ETA: 512.5s

################################################################################
                     [1m Learning iteration 968/2000 [0m

                       Computation: 17477 steps/s (collection: 0.267s, learning 0.201s)
               Value function loss: 127823.4670
                    Surrogate loss: -0.0040
             Mean action noise std: 0.94
                       Mean reward: 12502.74
               Mean episode length: 474.35
                 Mean success rate: 94.50
                  Mean reward/step: 26.65
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7938048
                    Iteration time: 0.47s
                        Total time: 480.68s
                               ETA: 511.9s

################################################################################
                     [1m Learning iteration 969/2000 [0m

                       Computation: 18111 steps/s (collection: 0.254s, learning 0.198s)
               Value function loss: 84941.6513
                    Surrogate loss: -0.0034
             Mean action noise std: 0.94
                       Mean reward: 12567.78
               Mean episode length: 475.23
                 Mean success rate: 95.00
                  Mean reward/step: 25.77
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 0.45s
                        Total time: 481.13s
                               ETA: 511.4s

################################################################################
                     [1m Learning iteration 970/2000 [0m

                       Computation: 17310 steps/s (collection: 0.275s, learning 0.198s)
               Value function loss: 102867.6511
                    Surrogate loss: -0.0026
             Mean action noise std: 0.94
                       Mean reward: 12887.58
               Mean episode length: 482.04
                 Mean success rate: 96.50
                  Mean reward/step: 26.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7954432
                    Iteration time: 0.47s
                        Total time: 481.60s
                               ETA: 510.9s

################################################################################
                     [1m Learning iteration 971/2000 [0m

                       Computation: 16909 steps/s (collection: 0.269s, learning 0.216s)
               Value function loss: 89110.9461
                    Surrogate loss: 0.0033
             Mean action noise std: 0.94
                       Mean reward: 13165.85
               Mean episode length: 490.06
                 Mean success rate: 98.00
                  Mean reward/step: 27.02
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.48s
                        Total time: 482.09s
                               ETA: 510.4s

################################################################################
                     [1m Learning iteration 972/2000 [0m

                       Computation: 17193 steps/s (collection: 0.261s, learning 0.215s)
               Value function loss: 64050.5985
                    Surrogate loss: 0.0009
             Mean action noise std: 0.94
                       Mean reward: 13025.29
               Mean episode length: 485.19
                 Mean success rate: 97.00
                  Mean reward/step: 27.24
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 7970816
                    Iteration time: 0.48s
                        Total time: 482.56s
                               ETA: 509.8s

################################################################################
                     [1m Learning iteration 973/2000 [0m

                       Computation: 16447 steps/s (collection: 0.286s, learning 0.212s)
               Value function loss: 90304.0763
                    Surrogate loss: -0.0030
             Mean action noise std: 0.94
                       Mean reward: 12920.81
               Mean episode length: 480.91
                 Mean success rate: 96.50
                  Mean reward/step: 26.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 0.50s
                        Total time: 483.06s
                               ETA: 509.3s

################################################################################
                     [1m Learning iteration 974/2000 [0m

                       Computation: 17457 steps/s (collection: 0.265s, learning 0.205s)
               Value function loss: 108012.3369
                    Surrogate loss: -0.0051
             Mean action noise std: 0.94
                       Mean reward: 12866.75
               Mean episode length: 478.42
                 Mean success rate: 96.00
                  Mean reward/step: 26.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7987200
                    Iteration time: 0.47s
                        Total time: 483.53s
                               ETA: 508.8s

################################################################################
                     [1m Learning iteration 975/2000 [0m

                       Computation: 16989 steps/s (collection: 0.271s, learning 0.211s)
               Value function loss: 101797.9588
                    Surrogate loss: -0.0038
             Mean action noise std: 0.94
                       Mean reward: 12987.68
               Mean episode length: 477.95
                 Mean success rate: 96.00
                  Mean reward/step: 26.42
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 0.48s
                        Total time: 484.01s
                               ETA: 508.3s

################################################################################
                     [1m Learning iteration 976/2000 [0m

                       Computation: 17251 steps/s (collection: 0.263s, learning 0.211s)
               Value function loss: 78150.8102
                    Surrogate loss: -0.0036
             Mean action noise std: 0.95
                       Mean reward: 12838.71
               Mean episode length: 473.83
                 Mean success rate: 95.50
                  Mean reward/step: 26.56
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8003584
                    Iteration time: 0.47s
                        Total time: 484.49s
                               ETA: 507.8s

################################################################################
                     [1m Learning iteration 977/2000 [0m

                       Computation: 18044 steps/s (collection: 0.249s, learning 0.205s)
               Value function loss: 82862.8459
                    Surrogate loss: -0.0041
             Mean action noise std: 0.95
                       Mean reward: 12742.82
               Mean episode length: 471.66
                 Mean success rate: 95.00
                  Mean reward/step: 26.12
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 0.45s
                        Total time: 484.94s
                               ETA: 507.3s

################################################################################
                     [1m Learning iteration 978/2000 [0m

                       Computation: 17289 steps/s (collection: 0.262s, learning 0.212s)
               Value function loss: 93236.9306
                    Surrogate loss: -0.0045
             Mean action noise std: 0.95
                       Mean reward: 12686.12
               Mean episode length: 471.66
                 Mean success rate: 95.00
                  Mean reward/step: 26.05
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8019968
                    Iteration time: 0.47s
                        Total time: 485.41s
                               ETA: 506.7s

################################################################################
                     [1m Learning iteration 979/2000 [0m

                       Computation: 17352 steps/s (collection: 0.271s, learning 0.201s)
               Value function loss: 82479.3348
                    Surrogate loss: -0.0031
             Mean action noise std: 0.95
                       Mean reward: 12438.13
               Mean episode length: 465.65
                 Mean success rate: 94.50
                  Mean reward/step: 25.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 0.47s
                        Total time: 485.89s
                               ETA: 506.2s

################################################################################
                     [1m Learning iteration 980/2000 [0m

                       Computation: 16797 steps/s (collection: 0.271s, learning 0.216s)
               Value function loss: 99193.1543
                    Surrogate loss: 0.0013
             Mean action noise std: 0.95
                       Mean reward: 12432.22
               Mean episode length: 465.98
                 Mean success rate: 94.50
                  Mean reward/step: 26.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8036352
                    Iteration time: 0.49s
                        Total time: 486.37s
                               ETA: 505.7s

################################################################################
                     [1m Learning iteration 981/2000 [0m

                       Computation: 17341 steps/s (collection: 0.265s, learning 0.208s)
               Value function loss: 53580.4364
                    Surrogate loss: -0.0009
             Mean action noise std: 0.95
                       Mean reward: 12424.47
               Mean episode length: 465.98
                 Mean success rate: 94.50
                  Mean reward/step: 25.76
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 0.47s
                        Total time: 486.85s
                               ETA: 505.2s

################################################################################
                     [1m Learning iteration 982/2000 [0m

                       Computation: 16576 steps/s (collection: 0.272s, learning 0.222s)
               Value function loss: 87334.6422
                    Surrogate loss: 0.0089
             Mean action noise std: 0.95
                       Mean reward: 12184.72
               Mean episode length: 457.95
                 Mean success rate: 93.00
                  Mean reward/step: 25.86
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8052736
                    Iteration time: 0.49s
                        Total time: 487.34s
                               ETA: 504.7s

################################################################################
                     [1m Learning iteration 983/2000 [0m

                       Computation: 16182 steps/s (collection: 0.279s, learning 0.227s)
               Value function loss: 82821.2723
                    Surrogate loss: 0.0044
             Mean action noise std: 0.95
                       Mean reward: 12088.21
               Mean episode length: 457.95
                 Mean success rate: 93.00
                  Mean reward/step: 25.92
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.51s
                        Total time: 487.85s
                               ETA: 504.2s

################################################################################
                     [1m Learning iteration 984/2000 [0m

                       Computation: 16868 steps/s (collection: 0.264s, learning 0.222s)
               Value function loss: 119728.9897
                    Surrogate loss: -0.0007
             Mean action noise std: 0.95
                       Mean reward: 12246.15
               Mean episode length: 465.83
                 Mean success rate: 94.50
                  Mean reward/step: 25.05
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8069120
                    Iteration time: 0.49s
                        Total time: 488.33s
                               ETA: 503.7s

################################################################################
                     [1m Learning iteration 985/2000 [0m

                       Computation: 17067 steps/s (collection: 0.270s, learning 0.210s)
               Value function loss: 86585.5462
                    Surrogate loss: 0.0001
             Mean action noise std: 0.95
                       Mean reward: 12209.43
               Mean episode length: 464.98
                 Mean success rate: 94.00
                  Mean reward/step: 25.06
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 0.48s
                        Total time: 488.81s
                               ETA: 503.2s

################################################################################
                     [1m Learning iteration 986/2000 [0m

                       Computation: 17201 steps/s (collection: 0.269s, learning 0.207s)
               Value function loss: 67041.4489
                    Surrogate loss: 0.0071
             Mean action noise std: 0.95
                       Mean reward: 12186.31
               Mean episode length: 464.98
                 Mean success rate: 94.00
                  Mean reward/step: 25.03
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8085504
                    Iteration time: 0.48s
                        Total time: 489.29s
                               ETA: 502.7s

################################################################################
                     [1m Learning iteration 987/2000 [0m

                       Computation: 16946 steps/s (collection: 0.272s, learning 0.212s)
               Value function loss: 94327.3166
                    Surrogate loss: -0.0004
             Mean action noise std: 0.95
                       Mean reward: 12154.37
               Mean episode length: 464.17
                 Mean success rate: 93.50
                  Mean reward/step: 24.39
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 0.48s
                        Total time: 489.77s
                               ETA: 502.2s

################################################################################
                     [1m Learning iteration 988/2000 [0m

                       Computation: 16676 steps/s (collection: 0.269s, learning 0.223s)
               Value function loss: 88727.4722
                    Surrogate loss: -0.0025
             Mean action noise std: 0.95
                       Mean reward: 12062.68
               Mean episode length: 461.69
                 Mean success rate: 93.00
                  Mean reward/step: 24.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8101888
                    Iteration time: 0.49s
                        Total time: 490.26s
                               ETA: 501.7s

################################################################################
                     [1m Learning iteration 989/2000 [0m

                       Computation: 17123 steps/s (collection: 0.265s, learning 0.214s)
               Value function loss: 85149.8735
                    Surrogate loss: -0.0056
             Mean action noise std: 0.95
                       Mean reward: 11851.79
               Mean episode length: 457.39
                 Mean success rate: 92.00
                  Mean reward/step: 24.87
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 0.48s
                        Total time: 490.74s
                               ETA: 501.2s

################################################################################
                     [1m Learning iteration 990/2000 [0m

                       Computation: 17295 steps/s (collection: 0.270s, learning 0.204s)
               Value function loss: 87649.5859
                    Surrogate loss: -0.0053
             Mean action noise std: 0.95
                       Mean reward: 11923.68
               Mean episode length: 461.24
                 Mean success rate: 92.50
                  Mean reward/step: 25.35
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8118272
                    Iteration time: 0.47s
                        Total time: 491.22s
                               ETA: 500.6s

################################################################################
                     [1m Learning iteration 991/2000 [0m

                       Computation: 16828 steps/s (collection: 0.277s, learning 0.210s)
               Value function loss: 74078.9679
                    Surrogate loss: -0.0058
             Mean action noise std: 0.95
                       Mean reward: 11953.27
               Mean episode length: 463.36
                 Mean success rate: 92.50
                  Mean reward/step: 25.18
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 0.49s
                        Total time: 491.70s
                               ETA: 500.1s

################################################################################
                     [1m Learning iteration 992/2000 [0m

                       Computation: 16845 steps/s (collection: 0.273s, learning 0.213s)
               Value function loss: 48942.4122
                    Surrogate loss: -0.0052
             Mean action noise std: 0.95
                       Mean reward: 11897.69
               Mean episode length: 462.35
                 Mean success rate: 92.50
                  Mean reward/step: 25.51
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8134656
                    Iteration time: 0.49s
                        Total time: 492.19s
                               ETA: 499.6s

################################################################################
                     [1m Learning iteration 993/2000 [0m

                       Computation: 17729 steps/s (collection: 0.260s, learning 0.202s)
               Value function loss: 104026.3347
                    Surrogate loss: -0.0001
             Mean action noise std: 0.95
                       Mean reward: 11942.65
               Mean episode length: 466.79
                 Mean success rate: 93.50
                  Mean reward/step: 26.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 0.46s
                        Total time: 492.65s
                               ETA: 499.1s

################################################################################
                     [1m Learning iteration 994/2000 [0m

                       Computation: 17528 steps/s (collection: 0.254s, learning 0.213s)
               Value function loss: 107014.6414
                    Surrogate loss: 0.0041
             Mean action noise std: 0.95
                       Mean reward: 11806.24
               Mean episode length: 462.35
                 Mean success rate: 93.00
                  Mean reward/step: 25.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8151040
                    Iteration time: 0.47s
                        Total time: 493.12s
                               ETA: 498.6s

################################################################################
                     [1m Learning iteration 995/2000 [0m

                       Computation: 17671 steps/s (collection: 0.257s, learning 0.207s)
               Value function loss: 95839.7955
                    Surrogate loss: -0.0007
             Mean action noise std: 0.95
                       Mean reward: 11730.45
               Mean episode length: 458.85
                 Mean success rate: 92.50
                  Mean reward/step: 24.68
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.46s
                        Total time: 493.58s
                               ETA: 498.0s

################################################################################
                     [1m Learning iteration 996/2000 [0m

                       Computation: 17789 steps/s (collection: 0.260s, learning 0.201s)
               Value function loss: 84831.7701
                    Surrogate loss: -0.0042
             Mean action noise std: 0.95
                       Mean reward: 11660.87
               Mean episode length: 456.25
                 Mean success rate: 92.00
                  Mean reward/step: 24.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8167424
                    Iteration time: 0.46s
                        Total time: 494.04s
                               ETA: 497.5s

################################################################################
                     [1m Learning iteration 997/2000 [0m

                       Computation: 17691 steps/s (collection: 0.256s, learning 0.207s)
               Value function loss: 70677.6021
                    Surrogate loss: -0.0017
             Mean action noise std: 0.95
                       Mean reward: 11589.38
               Mean episode length: 454.13
                 Mean success rate: 92.00
                  Mean reward/step: 26.05
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 0.46s
                        Total time: 494.51s
                               ETA: 497.0s

################################################################################
                     [1m Learning iteration 998/2000 [0m

                       Computation: 17245 steps/s (collection: 0.268s, learning 0.207s)
               Value function loss: 98581.9607
                    Surrogate loss: 0.0029
             Mean action noise std: 0.95
                       Mean reward: 11578.38
               Mean episode length: 455.67
                 Mean success rate: 92.50
                  Mean reward/step: 25.95
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8183808
                    Iteration time: 0.48s
                        Total time: 494.98s
                               ETA: 496.5s

################################################################################
                     [1m Learning iteration 999/2000 [0m

                       Computation: 16916 steps/s (collection: 0.283s, learning 0.201s)
               Value function loss: 111346.8266
                    Surrogate loss: -0.0021
             Mean action noise std: 0.95
                       Mean reward: 11429.27
               Mean episode length: 451.63
                 Mean success rate: 92.00
                  Mean reward/step: 25.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 0.48s
                        Total time: 495.46s
                               ETA: 496.0s

################################################################################
                     [1m Learning iteration 1000/2000 [0m

                       Computation: 17642 steps/s (collection: 0.257s, learning 0.208s)
               Value function loss: 59650.1239
                    Surrogate loss: -0.0060
             Mean action noise std: 0.95
                       Mean reward: 11500.57
               Mean episode length: 453.13
                 Mean success rate: 92.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8200192
                    Iteration time: 0.46s
                        Total time: 495.93s
                               ETA: 495.4s

################################################################################
                     [1m Learning iteration 1001/2000 [0m

                       Computation: 17216 steps/s (collection: 0.271s, learning 0.205s)
               Value function loss: 114876.3301
                    Surrogate loss: -0.0043
             Mean action noise std: 0.95
                       Mean reward: 11143.49
               Mean episode length: 441.68
                 Mean success rate: 90.50
                  Mean reward/step: 25.06
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 0.48s
                        Total time: 496.40s
                               ETA: 494.9s

################################################################################
                     [1m Learning iteration 1002/2000 [0m

                       Computation: 17347 steps/s (collection: 0.267s, learning 0.205s)
               Value function loss: 67566.5294
                    Surrogate loss: 0.0086
             Mean action noise std: 0.95
                       Mean reward: 11155.62
               Mean episode length: 441.60
                 Mean success rate: 91.00
                  Mean reward/step: 25.19
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8216576
                    Iteration time: 0.47s
                        Total time: 496.88s
                               ETA: 494.4s

################################################################################
                     [1m Learning iteration 1003/2000 [0m

                       Computation: 16586 steps/s (collection: 0.285s, learning 0.209s)
               Value function loss: 79478.7637
                    Surrogate loss: 0.0025
             Mean action noise std: 0.95
                       Mean reward: 11154.15
               Mean episode length: 441.66
                 Mean success rate: 91.00
                  Mean reward/step: 24.47
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 0.49s
                        Total time: 497.37s
                               ETA: 493.9s

################################################################################
                     [1m Learning iteration 1004/2000 [0m

                       Computation: 17899 steps/s (collection: 0.255s, learning 0.203s)
               Value function loss: 105422.2802
                    Surrogate loss: -0.0042
             Mean action noise std: 0.95
                       Mean reward: 11177.86
               Mean episode length: 441.88
                 Mean success rate: 90.50
                  Mean reward/step: 24.31
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8232960
                    Iteration time: 0.46s
                        Total time: 497.83s
                               ETA: 493.4s

################################################################################
                     [1m Learning iteration 1005/2000 [0m

                       Computation: 17854 steps/s (collection: 0.251s, learning 0.208s)
               Value function loss: 77756.3744
                    Surrogate loss: -0.0017
             Mean action noise std: 0.95
                       Mean reward: 11148.13
               Mean episode length: 441.88
                 Mean success rate: 90.50
                  Mean reward/step: 24.28
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 0.46s
                        Total time: 498.29s
                               ETA: 492.8s

################################################################################
                     [1m Learning iteration 1006/2000 [0m

                       Computation: 17197 steps/s (collection: 0.267s, learning 0.209s)
               Value function loss: 81459.2297
                    Surrogate loss: -0.0043
             Mean action noise std: 0.95
                       Mean reward: 11148.86
               Mean episode length: 443.19
                 Mean success rate: 90.50
                  Mean reward/step: 24.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8249344
                    Iteration time: 0.48s
                        Total time: 498.76s
                               ETA: 492.3s

################################################################################
                     [1m Learning iteration 1007/2000 [0m

                       Computation: 16852 steps/s (collection: 0.261s, learning 0.225s)
               Value function loss: 58035.1337
                    Surrogate loss: 0.0003
             Mean action noise std: 0.95
                       Mean reward: 11203.16
               Mean episode length: 443.30
                 Mean success rate: 90.50
                  Mean reward/step: 25.45
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.49s
                        Total time: 499.25s
                               ETA: 491.8s

################################################################################
                     [1m Learning iteration 1008/2000 [0m

                       Computation: 17127 steps/s (collection: 0.266s, learning 0.212s)
               Value function loss: 57618.8373
                    Surrogate loss: -0.0031
             Mean action noise std: 0.95
                       Mean reward: 11221.26
               Mean episode length: 443.14
                 Mean success rate: 91.00
                  Mean reward/step: 25.86
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8265728
                    Iteration time: 0.48s
                        Total time: 499.73s
                               ETA: 491.3s

################################################################################
                     [1m Learning iteration 1009/2000 [0m

                       Computation: 16821 steps/s (collection: 0.270s, learning 0.217s)
               Value function loss: 100316.0926
                    Surrogate loss: -0.0048
             Mean action noise std: 0.95
                       Mean reward: 11236.35
               Mean episode length: 443.73
                 Mean success rate: 91.00
                  Mean reward/step: 26.05
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 0.49s
                        Total time: 500.22s
                               ETA: 490.8s

################################################################################
                     [1m Learning iteration 1010/2000 [0m

                       Computation: 16284 steps/s (collection: 0.292s, learning 0.211s)
               Value function loss: 81348.9199
                    Surrogate loss: -0.0040
             Mean action noise std: 0.95
                       Mean reward: 11345.43
               Mean episode length: 448.21
                 Mean success rate: 91.50
                  Mean reward/step: 25.99
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8282112
                    Iteration time: 0.50s
                        Total time: 500.72s
                               ETA: 490.3s

################################################################################
                     [1m Learning iteration 1011/2000 [0m

                       Computation: 16816 steps/s (collection: 0.267s, learning 0.220s)
               Value function loss: 92517.2854
                    Surrogate loss: -0.0014
             Mean action noise std: 0.95
                       Mean reward: 11406.37
               Mean episode length: 452.00
                 Mean success rate: 92.50
                  Mean reward/step: 25.87
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 0.49s
                        Total time: 501.21s
                               ETA: 489.8s

################################################################################
                     [1m Learning iteration 1012/2000 [0m

                       Computation: 16966 steps/s (collection: 0.272s, learning 0.211s)
               Value function loss: 53251.4909
                    Surrogate loss: 0.0015
             Mean action noise std: 0.95
                       Mean reward: 11310.72
               Mean episode length: 448.04
                 Mean success rate: 92.00
                  Mean reward/step: 26.41
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8298496
                    Iteration time: 0.48s
                        Total time: 501.69s
                               ETA: 489.3s

################################################################################
                     [1m Learning iteration 1013/2000 [0m

                       Computation: 16443 steps/s (collection: 0.281s, learning 0.218s)
               Value function loss: 77061.6017
                    Surrogate loss: -0.0058
             Mean action noise std: 0.95
                       Mean reward: 11438.98
               Mean episode length: 451.58
                 Mean success rate: 92.50
                  Mean reward/step: 27.21
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 0.50s
                        Total time: 502.19s
                               ETA: 488.8s

################################################################################
                     [1m Learning iteration 1014/2000 [0m

                       Computation: 16397 steps/s (collection: 0.280s, learning 0.219s)
               Value function loss: 71358.0040
                    Surrogate loss: 0.0008
             Mean action noise std: 0.95
                       Mean reward: 11449.09
               Mean episode length: 452.72
                 Mean success rate: 92.50
                  Mean reward/step: 27.06
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8314880
                    Iteration time: 0.50s
                        Total time: 502.69s
                               ETA: 488.3s

################################################################################
                     [1m Learning iteration 1015/2000 [0m

                       Computation: 16306 steps/s (collection: 0.283s, learning 0.219s)
               Value function loss: 111217.0357
                    Surrogate loss: -0.0034
             Mean action noise std: 0.95
                       Mean reward: 11677.71
               Mean episode length: 459.75
                 Mean success rate: 93.50
                  Mean reward/step: 26.68
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 0.50s
                        Total time: 503.19s
                               ETA: 487.8s

################################################################################
                     [1m Learning iteration 1016/2000 [0m

                       Computation: 16649 steps/s (collection: 0.273s, learning 0.219s)
               Value function loss: 80060.9960
                    Surrogate loss: -0.0055
             Mean action noise std: 0.95
                       Mean reward: 11512.23
               Mean episode length: 452.37
                 Mean success rate: 93.00
                  Mean reward/step: 26.76
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8331264
                    Iteration time: 0.49s
                        Total time: 503.68s
                               ETA: 487.3s

################################################################################
                     [1m Learning iteration 1017/2000 [0m

                       Computation: 16268 steps/s (collection: 0.295s, learning 0.209s)
               Value function loss: 106614.6732
                    Surrogate loss: -0.0036
             Mean action noise std: 0.95
                       Mean reward: 11455.40
               Mean episode length: 450.42
                 Mean success rate: 93.00
                  Mean reward/step: 26.36
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 0.50s
                        Total time: 504.18s
                               ETA: 486.8s

################################################################################
                     [1m Learning iteration 1018/2000 [0m

                       Computation: 16509 steps/s (collection: 0.278s, learning 0.218s)
               Value function loss: 99449.5264
                    Surrogate loss: -0.0049
             Mean action noise std: 0.95
                       Mean reward: 11578.19
               Mean episode length: 454.14
                 Mean success rate: 93.50
                  Mean reward/step: 27.01
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8347648
                    Iteration time: 0.50s
                        Total time: 504.68s
                               ETA: 486.4s

################################################################################
                     [1m Learning iteration 1019/2000 [0m

                       Computation: 16902 steps/s (collection: 0.270s, learning 0.215s)
               Value function loss: 79179.1187
                    Surrogate loss: -0.0002
             Mean action noise std: 0.95
                       Mean reward: 11529.10
               Mean episode length: 451.21
                 Mean success rate: 92.50
                  Mean reward/step: 27.14
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.48s
                        Total time: 505.16s
                               ETA: 485.8s

################################################################################
                     [1m Learning iteration 1020/2000 [0m

                       Computation: 16962 steps/s (collection: 0.273s, learning 0.210s)
               Value function loss: 86448.4106
                    Surrogate loss: 0.0050
             Mean action noise std: 0.95
                       Mean reward: 11515.00
               Mean episode length: 448.77
                 Mean success rate: 92.00
                  Mean reward/step: 26.51
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8364032
                    Iteration time: 0.48s
                        Total time: 505.65s
                               ETA: 485.3s

################################################################################
                     [1m Learning iteration 1021/2000 [0m

                       Computation: 17141 steps/s (collection: 0.259s, learning 0.218s)
               Value function loss: 91447.4181
                    Surrogate loss: -0.0038
             Mean action noise std: 0.95
                       Mean reward: 11408.34
               Mean episode length: 442.91
                 Mean success rate: 91.00
                  Mean reward/step: 26.61
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 0.48s
                        Total time: 506.13s
                               ETA: 484.8s

################################################################################
                     [1m Learning iteration 1022/2000 [0m

                       Computation: 16742 steps/s (collection: 0.280s, learning 0.210s)
               Value function loss: 83925.0854
                    Surrogate loss: -0.0052
             Mean action noise std: 0.95
                       Mean reward: 11608.65
               Mean episode length: 446.12
                 Mean success rate: 91.50
                  Mean reward/step: 27.00
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8380416
                    Iteration time: 0.49s
                        Total time: 506.62s
                               ETA: 484.3s

################################################################################
                     [1m Learning iteration 1023/2000 [0m

                       Computation: 17476 steps/s (collection: 0.256s, learning 0.212s)
               Value function loss: 77391.4876
                    Surrogate loss: -0.0043
             Mean action noise std: 0.95
                       Mean reward: 11822.07
               Mean episode length: 454.65
                 Mean success rate: 93.00
                  Mean reward/step: 27.42
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 0.47s
                        Total time: 507.08s
                               ETA: 483.8s

################################################################################
                     [1m Learning iteration 1024/2000 [0m

                       Computation: 16165 steps/s (collection: 0.297s, learning 0.210s)
               Value function loss: 99221.0974
                    Surrogate loss: -0.0041
             Mean action noise std: 0.95
                       Mean reward: 11801.79
               Mean episode length: 450.64
                 Mean success rate: 92.00
                  Mean reward/step: 27.39
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8396800
                    Iteration time: 0.51s
                        Total time: 507.59s
                               ETA: 483.3s

################################################################################
                     [1m Learning iteration 1025/2000 [0m

                       Computation: 17018 steps/s (collection: 0.274s, learning 0.207s)
               Value function loss: 96445.7807
                    Surrogate loss: -0.0037
             Mean action noise std: 0.95
                       Mean reward: 11737.28
               Mean episode length: 448.84
                 Mean success rate: 91.50
                  Mean reward/step: 27.07
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 0.48s
                        Total time: 508.07s
                               ETA: 482.8s

################################################################################
                     [1m Learning iteration 1026/2000 [0m

                       Computation: 16523 steps/s (collection: 0.286s, learning 0.209s)
               Value function loss: 120047.3900
                    Surrogate loss: -0.0028
             Mean action noise std: 0.95
                       Mean reward: 11581.18
               Mean episode length: 439.64
                 Mean success rate: 89.50
                  Mean reward/step: 26.85
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8413184
                    Iteration time: 0.50s
                        Total time: 508.57s
                               ETA: 482.3s

################################################################################
                     [1m Learning iteration 1027/2000 [0m

                       Computation: 16783 steps/s (collection: 0.282s, learning 0.206s)
               Value function loss: 91608.7795
                    Surrogate loss: -0.0051
             Mean action noise std: 0.95
                       Mean reward: 11732.60
               Mean episode length: 442.54
                 Mean success rate: 89.50
                  Mean reward/step: 26.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 0.49s
                        Total time: 509.06s
                               ETA: 481.8s

################################################################################
                     [1m Learning iteration 1028/2000 [0m

                       Computation: 17251 steps/s (collection: 0.252s, learning 0.222s)
               Value function loss: 59965.3057
                    Surrogate loss: -0.0053
             Mean action noise std: 0.95
                       Mean reward: 11698.47
               Mean episode length: 439.58
                 Mean success rate: 88.50
                  Mean reward/step: 27.11
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 8429568
                    Iteration time: 0.47s
                        Total time: 509.53s
                               ETA: 481.3s

################################################################################
                     [1m Learning iteration 1029/2000 [0m

                       Computation: 17646 steps/s (collection: 0.258s, learning 0.206s)
               Value function loss: 109861.3447
                    Surrogate loss: -0.0044
             Mean action noise std: 0.95
                       Mean reward: 11827.08
               Mean episode length: 442.07
                 Mean success rate: 89.00
                  Mean reward/step: 28.90
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 0.46s
                        Total time: 509.99s
                               ETA: 480.8s

################################################################################
                     [1m Learning iteration 1030/2000 [0m

                       Computation: 17105 steps/s (collection: 0.274s, learning 0.205s)
               Value function loss: 78504.7123
                    Surrogate loss: -0.0046
             Mean action noise std: 0.95
                       Mean reward: 12027.23
               Mean episode length: 448.98
                 Mean success rate: 90.50
                  Mean reward/step: 28.86
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8445952
                    Iteration time: 0.48s
                        Total time: 510.47s
                               ETA: 480.3s

################################################################################
                     [1m Learning iteration 1031/2000 [0m

                       Computation: 17209 steps/s (collection: 0.275s, learning 0.201s)
               Value function loss: 79297.8167
                    Surrogate loss: -0.0046
             Mean action noise std: 0.95
                       Mean reward: 11847.26
               Mean episode length: 444.06
                 Mean success rate: 89.50
                  Mean reward/step: 28.79
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.48s
                        Total time: 510.95s
                               ETA: 479.8s

################################################################################
                     [1m Learning iteration 1032/2000 [0m

                       Computation: 17143 steps/s (collection: 0.266s, learning 0.212s)
               Value function loss: 127051.0318
                    Surrogate loss: -0.0047
             Mean action noise std: 0.95
                       Mean reward: 12214.35
               Mean episode length: 452.43
                 Mean success rate: 91.00
                  Mean reward/step: 28.69
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8462336
                    Iteration time: 0.48s
                        Total time: 511.43s
                               ETA: 479.2s

################################################################################
                     [1m Learning iteration 1033/2000 [0m

                       Computation: 16378 steps/s (collection: 0.285s, learning 0.215s)
               Value function loss: 82844.9013
                    Surrogate loss: -0.0028
             Mean action noise std: 0.95
                       Mean reward: 11905.44
               Mean episode length: 443.44
                 Mean success rate: 90.00
                  Mean reward/step: 28.36
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 0.50s
                        Total time: 511.93s
                               ETA: 478.8s

################################################################################
                     [1m Learning iteration 1034/2000 [0m

                       Computation: 16843 steps/s (collection: 0.282s, learning 0.204s)
               Value function loss: 97625.7813
                    Surrogate loss: -0.0030
             Mean action noise std: 0.95
                       Mean reward: 11899.87
               Mean episode length: 439.19
                 Mean success rate: 89.00
                  Mean reward/step: 28.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8478720
                    Iteration time: 0.49s
                        Total time: 512.41s
                               ETA: 478.3s

################################################################################
                     [1m Learning iteration 1035/2000 [0m

                       Computation: 15603 steps/s (collection: 0.301s, learning 0.224s)
               Value function loss: 99804.7516
                    Surrogate loss: 0.0002
             Mean action noise std: 0.95
                       Mean reward: 12156.36
               Mean episode length: 447.10
                 Mean success rate: 90.50
                  Mean reward/step: 28.08
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 0.53s
                        Total time: 512.94s
                               ETA: 477.8s

################################################################################
                     [1m Learning iteration 1036/2000 [0m

                       Computation: 16751 steps/s (collection: 0.278s, learning 0.211s)
               Value function loss: 103829.2686
                    Surrogate loss: 0.0142
             Mean action noise std: 0.94
                       Mean reward: 12321.43
               Mean episode length: 451.73
                 Mean success rate: 91.50
                  Mean reward/step: 27.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8495104
                    Iteration time: 0.49s
                        Total time: 513.43s
                               ETA: 477.3s

################################################################################
                     [1m Learning iteration 1037/2000 [0m

                       Computation: 16446 steps/s (collection: 0.276s, learning 0.222s)
               Value function loss: 88405.4479
                    Surrogate loss: -0.0020
             Mean action noise std: 0.94
                       Mean reward: 12197.64
               Mean episode length: 447.05
                 Mean success rate: 91.00
                  Mean reward/step: 28.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 0.50s
                        Total time: 513.93s
                               ETA: 476.8s

################################################################################
                     [1m Learning iteration 1038/2000 [0m

                       Computation: 16520 steps/s (collection: 0.278s, learning 0.218s)
               Value function loss: 73922.2755
                    Surrogate loss: -0.0017
             Mean action noise std: 0.94
                       Mean reward: 12283.49
               Mean episode length: 448.04
                 Mean success rate: 91.00
                  Mean reward/step: 27.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8511488
                    Iteration time: 0.50s
                        Total time: 514.42s
                               ETA: 476.3s

################################################################################
                     [1m Learning iteration 1039/2000 [0m

                       Computation: 17140 steps/s (collection: 0.267s, learning 0.211s)
               Value function loss: 48041.4393
                    Surrogate loss: -0.0044
             Mean action noise std: 0.94
                       Mean reward: 12233.48
               Mean episode length: 445.32
                 Mean success rate: 90.50
                  Mean reward/step: 27.96
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 0.48s
                        Total time: 514.90s
                               ETA: 475.8s

################################################################################
                     [1m Learning iteration 1040/2000 [0m

                       Computation: 16776 steps/s (collection: 0.271s, learning 0.218s)
               Value function loss: 99288.0657
                    Surrogate loss: -0.0059
             Mean action noise std: 0.94
                       Mean reward: 12305.92
               Mean episode length: 447.82
                 Mean success rate: 91.00
                  Mean reward/step: 28.38
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8527872
                    Iteration time: 0.49s
                        Total time: 515.39s
                               ETA: 475.3s

################################################################################
                     [1m Learning iteration 1041/2000 [0m

                       Computation: 17163 steps/s (collection: 0.268s, learning 0.209s)
               Value function loss: 115497.9619
                    Surrogate loss: -0.0042
             Mean action noise std: 0.95
                       Mean reward: 12488.86
               Mean episode length: 451.99
                 Mean success rate: 91.50
                  Mean reward/step: 27.72
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 0.48s
                        Total time: 515.87s
                               ETA: 474.8s

################################################################################
                     [1m Learning iteration 1042/2000 [0m

                       Computation: 17213 steps/s (collection: 0.266s, learning 0.210s)
               Value function loss: 87356.0270
                    Surrogate loss: -0.0037
             Mean action noise std: 0.95
                       Mean reward: 12645.10
               Mean episode length: 453.07
                 Mean success rate: 92.00
                  Mean reward/step: 26.98
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8544256
                    Iteration time: 0.48s
                        Total time: 516.34s
                               ETA: 474.3s

################################################################################
                     [1m Learning iteration 1043/2000 [0m

                       Computation: 16920 steps/s (collection: 0.271s, learning 0.213s)
               Value function loss: 83899.6475
                    Surrogate loss: -0.0029
             Mean action noise std: 0.94
                       Mean reward: 12646.10
               Mean episode length: 453.07
                 Mean success rate: 92.00
                  Mean reward/step: 27.37
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.48s
                        Total time: 516.83s
                               ETA: 473.8s

################################################################################
                     [1m Learning iteration 1044/2000 [0m

                       Computation: 16784 steps/s (collection: 0.274s, learning 0.214s)
               Value function loss: 77745.0938
                    Surrogate loss: -0.0029
             Mean action noise std: 0.95
                       Mean reward: 12794.42
               Mean episode length: 455.23
                 Mean success rate: 92.00
                  Mean reward/step: 28.25
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8560640
                    Iteration time: 0.49s
                        Total time: 517.31s
                               ETA: 473.3s

################################################################################
                     [1m Learning iteration 1045/2000 [0m

                       Computation: 16869 steps/s (collection: 0.278s, learning 0.208s)
               Value function loss: 100521.6211
                    Surrogate loss: -0.0050
             Mean action noise std: 0.95
                       Mean reward: 13079.58
               Mean episode length: 466.31
                 Mean success rate: 94.00
                  Mean reward/step: 27.87
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 0.49s
                        Total time: 517.80s
                               ETA: 472.8s

################################################################################
                     [1m Learning iteration 1046/2000 [0m

                       Computation: 17202 steps/s (collection: 0.269s, learning 0.207s)
               Value function loss: 100195.8396
                    Surrogate loss: -0.0013
             Mean action noise std: 0.95
                       Mean reward: 12870.75
               Mean episode length: 461.86
                 Mean success rate: 93.00
                  Mean reward/step: 27.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8577024
                    Iteration time: 0.48s
                        Total time: 518.28s
                               ETA: 472.2s

################################################################################
                     [1m Learning iteration 1047/2000 [0m

                       Computation: 17136 steps/s (collection: 0.269s, learning 0.209s)
               Value function loss: 66752.3425
                    Surrogate loss: 0.0127
             Mean action noise std: 0.95
                       Mean reward: 12900.59
               Mean episode length: 461.86
                 Mean success rate: 93.00
                  Mean reward/step: 24.81
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 0.48s
                        Total time: 518.75s
                               ETA: 471.7s

################################################################################
                     [1m Learning iteration 1048/2000 [0m

                       Computation: 17129 steps/s (collection: 0.268s, learning 0.210s)
               Value function loss: 123668.7940
                    Surrogate loss: -0.0036
             Mean action noise std: 0.95
                       Mean reward: 12964.64
               Mean episode length: 461.81
                 Mean success rate: 93.00
                  Mean reward/step: 23.72
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8593408
                    Iteration time: 0.48s
                        Total time: 519.23s
                               ETA: 471.2s

################################################################################
                     [1m Learning iteration 1049/2000 [0m

                       Computation: 17232 steps/s (collection: 0.265s, learning 0.210s)
               Value function loss: 88155.1620
                    Surrogate loss: -0.0052
             Mean action noise std: 0.95
                       Mean reward: 12918.30
               Mean episode length: 461.98
                 Mean success rate: 92.50
                  Mean reward/step: 22.97
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 0.48s
                        Total time: 519.71s
                               ETA: 470.7s

################################################################################
                     [1m Learning iteration 1050/2000 [0m

                       Computation: 17878 steps/s (collection: 0.259s, learning 0.199s)
               Value function loss: 89958.3629
                    Surrogate loss: -0.0040
             Mean action noise std: 0.95
                       Mean reward: 12825.98
               Mean episode length: 461.22
                 Mean success rate: 92.00
                  Mean reward/step: 23.07
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8609792
                    Iteration time: 0.46s
                        Total time: 520.17s
                               ETA: 470.2s

################################################################################
                     [1m Learning iteration 1051/2000 [0m

                       Computation: 17110 steps/s (collection: 0.273s, learning 0.205s)
               Value function loss: 98325.0325
                    Surrogate loss: -0.0042
             Mean action noise std: 0.95
                       Mean reward: 12567.35
               Mean episode length: 455.42
                 Mean success rate: 91.00
                  Mean reward/step: 23.15
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 0.48s
                        Total time: 520.64s
                               ETA: 469.7s

################################################################################
                     [1m Learning iteration 1052/2000 [0m

                       Computation: 16868 steps/s (collection: 0.271s, learning 0.215s)
               Value function loss: 77014.4937
                    Surrogate loss: -0.0039
             Mean action noise std: 0.95
                       Mean reward: 12323.23
               Mean episode length: 451.00
                 Mean success rate: 90.00
                  Mean reward/step: 23.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8626176
                    Iteration time: 0.49s
                        Total time: 521.13s
                               ETA: 469.2s

################################################################################
                     [1m Learning iteration 1053/2000 [0m

                       Computation: 17351 steps/s (collection: 0.260s, learning 0.212s)
               Value function loss: 79988.2129
                    Surrogate loss: -0.0062
             Mean action noise std: 0.95
                       Mean reward: 12291.87
               Mean episode length: 453.83
                 Mean success rate: 90.00
                  Mean reward/step: 23.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 0.47s
                        Total time: 521.60s
                               ETA: 468.7s

################################################################################
                     [1m Learning iteration 1054/2000 [0m

                       Computation: 16364 steps/s (collection: 0.279s, learning 0.222s)
               Value function loss: 42104.0375
                    Surrogate loss: -0.0011
             Mean action noise std: 0.95
                       Mean reward: 11950.85
               Mean episode length: 445.60
                 Mean success rate: 89.00
                  Mean reward/step: 24.07
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8642560
                    Iteration time: 0.50s
                        Total time: 522.10s
                               ETA: 468.2s

################################################################################
                     [1m Learning iteration 1055/2000 [0m

                       Computation: 16725 steps/s (collection: 0.285s, learning 0.205s)
               Value function loss: 63709.8432
                    Surrogate loss: 0.0073
             Mean action noise std: 0.95
                       Mean reward: 11421.07
               Mean episode length: 430.06
                 Mean success rate: 86.00
                  Mean reward/step: 25.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.49s
                        Total time: 522.59s
                               ETA: 467.7s

################################################################################
                     [1m Learning iteration 1056/2000 [0m

                       Computation: 17379 steps/s (collection: 0.264s, learning 0.207s)
               Value function loss: 84371.1632
                    Surrogate loss: -0.0002
             Mean action noise std: 0.95
                       Mean reward: 11451.38
               Mean episode length: 432.25
                 Mean success rate: 86.50
                  Mean reward/step: 25.55
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8658944
                    Iteration time: 0.47s
                        Total time: 523.06s
                               ETA: 467.1s

################################################################################
                     [1m Learning iteration 1057/2000 [0m

                       Computation: 15858 steps/s (collection: 0.288s, learning 0.229s)
               Value function loss: 89455.8834
                    Surrogate loss: 0.0036
             Mean action noise std: 0.95
                       Mean reward: 11333.54
               Mean episode length: 434.02
                 Mean success rate: 87.00
                  Mean reward/step: 25.16
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 0.52s
                        Total time: 523.58s
                               ETA: 466.7s

################################################################################
                     [1m Learning iteration 1058/2000 [0m

                       Computation: 16897 steps/s (collection: 0.277s, learning 0.208s)
               Value function loss: 84235.5809
                    Surrogate loss: -0.0045
             Mean action noise std: 0.95
                       Mean reward: 11135.44
               Mean episode length: 431.10
                 Mean success rate: 86.50
                  Mean reward/step: 25.17
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8675328
                    Iteration time: 0.48s
                        Total time: 524.07s
                               ETA: 466.2s

################################################################################
                     [1m Learning iteration 1059/2000 [0m

                       Computation: 16475 steps/s (collection: 0.274s, learning 0.224s)
               Value function loss: 55137.9365
                    Surrogate loss: 0.0022
             Mean action noise std: 0.95
                       Mean reward: 10821.61
               Mean episode length: 425.01
                 Mean success rate: 85.50
                  Mean reward/step: 25.28
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 0.50s
                        Total time: 524.56s
                               ETA: 465.7s

################################################################################
                     [1m Learning iteration 1060/2000 [0m

                       Computation: 16684 steps/s (collection: 0.266s, learning 0.225s)
               Value function loss: 75668.6112
                    Surrogate loss: -0.0041
             Mean action noise std: 0.95
                       Mean reward: 10916.79
               Mean episode length: 430.55
                 Mean success rate: 87.00
                  Mean reward/step: 25.17
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8691712
                    Iteration time: 0.49s
                        Total time: 525.05s
                               ETA: 465.2s

################################################################################
                     [1m Learning iteration 1061/2000 [0m

                       Computation: 17231 steps/s (collection: 0.264s, learning 0.212s)
               Value function loss: 84616.0423
                    Surrogate loss: -0.0018
             Mean action noise std: 0.95
                       Mean reward: 10838.44
               Mean episode length: 431.21
                 Mean success rate: 88.00
                  Mean reward/step: 25.82
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 0.48s
                        Total time: 525.53s
                               ETA: 464.7s

################################################################################
                     [1m Learning iteration 1062/2000 [0m

                       Computation: 16427 steps/s (collection: 0.298s, learning 0.201s)
               Value function loss: 96246.7602
                    Surrogate loss: -0.0009
             Mean action noise std: 0.95
                       Mean reward: 10952.53
               Mean episode length: 437.01
                 Mean success rate: 89.00
                  Mean reward/step: 26.22
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8708096
                    Iteration time: 0.50s
                        Total time: 526.03s
                               ETA: 464.2s

################################################################################
                     [1m Learning iteration 1063/2000 [0m

                       Computation: 17020 steps/s (collection: 0.274s, learning 0.208s)
               Value function loss: 80580.8071
                    Surrogate loss: -0.0036
             Mean action noise std: 0.95
                       Mean reward: 10894.08
               Mean episode length: 437.44
                 Mean success rate: 89.00
                  Mean reward/step: 27.01
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 0.48s
                        Total time: 526.51s
                               ETA: 463.7s

################################################################################
                     [1m Learning iteration 1064/2000 [0m

                       Computation: 16917 steps/s (collection: 0.274s, learning 0.210s)
               Value function loss: 89566.6463
                    Surrogate loss: 0.0005
             Mean action noise std: 0.95
                       Mean reward: 10823.66
               Mean episode length: 437.06
                 Mean success rate: 89.00
                  Mean reward/step: 26.48
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8724480
                    Iteration time: 0.48s
                        Total time: 526.99s
                               ETA: 463.2s

################################################################################
                     [1m Learning iteration 1065/2000 [0m

                       Computation: 17463 steps/s (collection: 0.255s, learning 0.214s)
               Value function loss: 92808.5811
                    Surrogate loss: -0.0056
             Mean action noise std: 0.94
                       Mean reward: 10954.29
               Mean episode length: 442.42
                 Mean success rate: 89.50
                  Mean reward/step: 26.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 0.47s
                        Total time: 527.46s
                               ETA: 462.6s

################################################################################
                     [1m Learning iteration 1066/2000 [0m

                       Computation: 17052 steps/s (collection: 0.269s, learning 0.212s)
               Value function loss: 121657.3455
                    Surrogate loss: -0.0038
             Mean action noise std: 0.94
                       Mean reward: 11271.89
               Mean episode length: 453.17
                 Mean success rate: 91.50
                  Mean reward/step: 26.37
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8740864
                    Iteration time: 0.48s
                        Total time: 527.94s
                               ETA: 462.1s

################################################################################
                     [1m Learning iteration 1067/2000 [0m

                       Computation: 17906 steps/s (collection: 0.257s, learning 0.201s)
               Value function loss: 82669.6734
                    Surrogate loss: -0.0052
             Mean action noise std: 0.94
                       Mean reward: 11253.91
               Mean episode length: 453.22
                 Mean success rate: 92.00
                  Mean reward/step: 25.72
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.46s
                        Total time: 528.40s
                               ETA: 461.6s

################################################################################
                     [1m Learning iteration 1068/2000 [0m

                       Computation: 17368 steps/s (collection: 0.261s, learning 0.211s)
               Value function loss: 82147.5524
                    Surrogate loss: -0.0008
             Mean action noise std: 0.94
                       Mean reward: 11002.99
               Mean episode length: 443.12
                 Mean success rate: 90.00
                  Mean reward/step: 26.51
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8757248
                    Iteration time: 0.47s
                        Total time: 528.87s
                               ETA: 461.1s

################################################################################
                     [1m Learning iteration 1069/2000 [0m

                       Computation: 17118 steps/s (collection: 0.267s, learning 0.211s)
               Value function loss: 54211.8369
                    Surrogate loss: -0.0040
             Mean action noise std: 0.94
                       Mean reward: 11107.72
               Mean episode length: 446.04
                 Mean success rate: 90.50
                  Mean reward/step: 27.15
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 0.48s
                        Total time: 529.35s
                               ETA: 460.6s

################################################################################
                     [1m Learning iteration 1070/2000 [0m

                       Computation: 17918 steps/s (collection: 0.253s, learning 0.204s)
               Value function loss: 49868.6402
                    Surrogate loss: -0.0025
             Mean action noise std: 0.94
                       Mean reward: 11300.92
               Mean episode length: 450.30
                 Mean success rate: 91.50
                  Mean reward/step: 27.99
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8773632
                    Iteration time: 0.46s
                        Total time: 529.81s
                               ETA: 460.1s

################################################################################
                     [1m Learning iteration 1071/2000 [0m

                       Computation: 17826 steps/s (collection: 0.257s, learning 0.202s)
               Value function loss: 103105.0093
                    Surrogate loss: 0.0005
             Mean action noise std: 0.94
                       Mean reward: 11194.17
               Mean episode length: 443.17
                 Mean success rate: 90.00
                  Mean reward/step: 28.33
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 0.46s
                        Total time: 530.27s
                               ETA: 459.5s

################################################################################
                     [1m Learning iteration 1072/2000 [0m

                       Computation: 17858 steps/s (collection: 0.254s, learning 0.205s)
               Value function loss: 109926.5205
                    Surrogate loss: -0.0034
             Mean action noise std: 0.94
                       Mean reward: 11374.87
               Mean episode length: 446.56
                 Mean success rate: 90.50
                  Mean reward/step: 27.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8790016
                    Iteration time: 0.46s
                        Total time: 530.73s
                               ETA: 459.0s

################################################################################
                     [1m Learning iteration 1073/2000 [0m

                       Computation: 17005 steps/s (collection: 0.273s, learning 0.209s)
               Value function loss: 92624.2520
                    Surrogate loss: -0.0025
             Mean action noise std: 0.94
                       Mean reward: 11220.88
               Mean episode length: 437.52
                 Mean success rate: 88.50
                  Mean reward/step: 27.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 0.48s
                        Total time: 531.21s
                               ETA: 458.5s

################################################################################
                     [1m Learning iteration 1074/2000 [0m

                       Computation: 18523 steps/s (collection: 0.243s, learning 0.200s)
               Value function loss: 93145.5612
                    Surrogate loss: -0.0031
             Mean action noise std: 0.95
                       Mean reward: 11352.77
               Mean episode length: 437.07
                 Mean success rate: 89.00
                  Mean reward/step: 28.05
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8806400
                    Iteration time: 0.44s
                        Total time: 531.65s
                               ETA: 458.0s

################################################################################
                     [1m Learning iteration 1075/2000 [0m

                       Computation: 18061 steps/s (collection: 0.247s, learning 0.207s)
               Value function loss: 97872.2253
                    Surrogate loss: -0.0028
             Mean action noise std: 0.95
                       Mean reward: 11080.38
               Mean episode length: 424.40
                 Mean success rate: 87.00
                  Mean reward/step: 27.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 0.45s
                        Total time: 532.10s
                               ETA: 457.4s

################################################################################
                     [1m Learning iteration 1076/2000 [0m

                       Computation: 18461 steps/s (collection: 0.244s, learning 0.199s)
               Value function loss: 101788.6400
                    Surrogate loss: -0.0042
             Mean action noise std: 0.95
                       Mean reward: 11106.37
               Mean episode length: 423.90
                 Mean success rate: 87.50
                  Mean reward/step: 27.72
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8822784
                    Iteration time: 0.44s
                        Total time: 532.55s
                               ETA: 456.9s

################################################################################
                     [1m Learning iteration 1077/2000 [0m

                       Computation: 18604 steps/s (collection: 0.244s, learning 0.197s)
               Value function loss: 103668.0394
                    Surrogate loss: -0.0039
             Mean action noise std: 0.94
                       Mean reward: 11187.44
               Mean episode length: 420.94
                 Mean success rate: 86.50
                  Mean reward/step: 27.80
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 0.44s
                        Total time: 532.99s
                               ETA: 456.4s

################################################################################
                     [1m Learning iteration 1078/2000 [0m

                       Computation: 18434 steps/s (collection: 0.245s, learning 0.200s)
               Value function loss: 67052.2125
                    Surrogate loss: -0.0029
             Mean action noise std: 0.94
                       Mean reward: 11249.65
               Mean episode length: 420.94
                 Mean success rate: 86.50
                  Mean reward/step: 28.06
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8839168
                    Iteration time: 0.44s
                        Total time: 533.43s
                               ETA: 455.8s

################################################################################
                     [1m Learning iteration 1079/2000 [0m

                       Computation: 18227 steps/s (collection: 0.248s, learning 0.202s)
               Value function loss: 128078.4051
                    Surrogate loss: -0.0049
             Mean action noise std: 0.94
                       Mean reward: 11502.34
               Mean episode length: 426.65
                 Mean success rate: 87.50
                  Mean reward/step: 28.81
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.45s
                        Total time: 533.88s
                               ETA: 455.3s

################################################################################
                     [1m Learning iteration 1080/2000 [0m

                       Computation: 18500 steps/s (collection: 0.242s, learning 0.200s)
               Value function loss: 73450.6106
                    Surrogate loss: -0.0025
             Mean action noise std: 0.94
                       Mean reward: 11647.84
               Mean episode length: 430.60
                 Mean success rate: 88.00
                  Mean reward/step: 28.34
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8855552
                    Iteration time: 0.44s
                        Total time: 534.32s
                               ETA: 454.7s

################################################################################
                     [1m Learning iteration 1081/2000 [0m

                       Computation: 17753 steps/s (collection: 0.258s, learning 0.203s)
               Value function loss: 125457.1391
                    Surrogate loss: -0.0055
             Mean action noise std: 0.94
                       Mean reward: 11652.74
               Mean episode length: 427.69
                 Mean success rate: 88.00
                  Mean reward/step: 27.98
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 0.46s
                        Total time: 534.79s
                               ETA: 454.2s

################################################################################
                     [1m Learning iteration 1082/2000 [0m

                       Computation: 18003 steps/s (collection: 0.258s, learning 0.197s)
               Value function loss: 109995.9262
                    Surrogate loss: -0.0027
             Mean action noise std: 0.94
                       Mean reward: 11676.11
               Mean episode length: 427.63
                 Mean success rate: 88.00
                  Mean reward/step: 26.49
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8871936
                    Iteration time: 0.46s
                        Total time: 535.24s
                               ETA: 453.7s

################################################################################
                     [1m Learning iteration 1083/2000 [0m

                       Computation: 17685 steps/s (collection: 0.267s, learning 0.196s)
               Value function loss: 86216.0581
                    Surrogate loss: -0.0025
             Mean action noise std: 0.94
                       Mean reward: 11552.64
               Mean episode length: 422.02
                 Mean success rate: 87.00
                  Mean reward/step: 27.17
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 0.46s
                        Total time: 535.70s
                               ETA: 453.2s

################################################################################
                     [1m Learning iteration 1084/2000 [0m

                       Computation: 17586 steps/s (collection: 0.267s, learning 0.199s)
               Value function loss: 86606.1055
                    Surrogate loss: -0.0046
             Mean action noise std: 0.94
                       Mean reward: 11561.07
               Mean episode length: 421.75
                 Mean success rate: 87.00
                  Mean reward/step: 27.80
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8888320
                    Iteration time: 0.47s
                        Total time: 536.17s
                               ETA: 452.7s

################################################################################
                     [1m Learning iteration 1085/2000 [0m

                       Computation: 18331 steps/s (collection: 0.248s, learning 0.199s)
               Value function loss: 66657.9619
                    Surrogate loss: -0.0042
             Mean action noise std: 0.94
                       Mean reward: 11586.87
               Mean episode length: 420.82
                 Mean success rate: 86.50
                  Mean reward/step: 28.25
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 0.45s
                        Total time: 536.62s
                               ETA: 452.1s

################################################################################
                     [1m Learning iteration 1086/2000 [0m

                       Computation: 17218 steps/s (collection: 0.260s, learning 0.216s)
               Value function loss: 72106.2309
                    Surrogate loss: -0.0012
             Mean action noise std: 0.94
                       Mean reward: 11296.82
               Mean episode length: 411.95
                 Mean success rate: 84.50
                  Mean reward/step: 28.83
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8904704
                    Iteration time: 0.48s
                        Total time: 537.09s
                               ETA: 451.6s

################################################################################
                     [1m Learning iteration 1087/2000 [0m

                       Computation: 15109 steps/s (collection: 0.301s, learning 0.241s)
               Value function loss: 122996.1977
                    Surrogate loss: 0.0074
             Mean action noise std: 0.94
                       Mean reward: 11399.42
               Mean episode length: 416.18
                 Mean success rate: 85.50
                  Mean reward/step: 28.68
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 0.54s
                        Total time: 537.63s
                               ETA: 451.2s

################################################################################
                     [1m Learning iteration 1088/2000 [0m

                       Computation: 16761 steps/s (collection: 0.276s, learning 0.212s)
               Value function loss: 75766.6784
                    Surrogate loss: -0.0007
             Mean action noise std: 0.94
                       Mean reward: 11370.70
               Mean episode length: 413.19
                 Mean success rate: 85.00
                  Mean reward/step: 26.88
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8921088
                    Iteration time: 0.49s
                        Total time: 538.12s
                               ETA: 450.7s

################################################################################
                     [1m Learning iteration 1089/2000 [0m

                       Computation: 16314 steps/s (collection: 0.291s, learning 0.211s)
               Value function loss: 72299.4141
                    Surrogate loss: 0.0014
             Mean action noise std: 0.94
                       Mean reward: 11204.74
               Mean episode length: 407.92
                 Mean success rate: 84.00
                  Mean reward/step: 27.40
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 0.50s
                        Total time: 538.63s
                               ETA: 450.2s

################################################################################
                     [1m Learning iteration 1090/2000 [0m

                       Computation: 17627 steps/s (collection: 0.257s, learning 0.208s)
               Value function loss: 77956.4979
                    Surrogate loss: -0.0021
             Mean action noise std: 0.94
                       Mean reward: 10964.58
               Mean episode length: 401.93
                 Mean success rate: 83.00
                  Mean reward/step: 28.50
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8937472
                    Iteration time: 0.46s
                        Total time: 539.09s
                               ETA: 449.7s

################################################################################
                     [1m Learning iteration 1091/2000 [0m

                       Computation: 17515 steps/s (collection: 0.266s, learning 0.202s)
               Value function loss: 92235.3492
                    Surrogate loss: 0.0000
             Mean action noise std: 0.94
                       Mean reward: 11244.37
               Mean episode length: 409.14
                 Mean success rate: 84.00
                  Mean reward/step: 27.77
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.47s
                        Total time: 539.56s
                               ETA: 449.1s

################################################################################
                     [1m Learning iteration 1092/2000 [0m

                       Computation: 17318 steps/s (collection: 0.267s, learning 0.206s)
               Value function loss: 97076.4356
                    Surrogate loss: -0.0058
             Mean action noise std: 0.94
                       Mean reward: 11236.11
               Mean episode length: 409.60
                 Mean success rate: 83.50
                  Mean reward/step: 28.28
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8953856
                    Iteration time: 0.47s
                        Total time: 540.03s
                               ETA: 448.6s

################################################################################
                     [1m Learning iteration 1093/2000 [0m

                       Computation: 17166 steps/s (collection: 0.264s, learning 0.213s)
               Value function loss: 117556.7644
                    Surrogate loss: -0.0054
             Mean action noise std: 0.95
                       Mean reward: 11525.85
               Mean episode length: 415.98
                 Mean success rate: 84.50
                  Mean reward/step: 28.36
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 0.48s
                        Total time: 540.51s
                               ETA: 448.1s

################################################################################
                     [1m Learning iteration 1094/2000 [0m

                       Computation: 17804 steps/s (collection: 0.252s, learning 0.208s)
               Value function loss: 85354.6651
                    Surrogate loss: -0.0044
             Mean action noise std: 0.95
                       Mean reward: 11594.94
               Mean episode length: 417.59
                 Mean success rate: 84.50
                  Mean reward/step: 28.52
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8970240
                    Iteration time: 0.46s
                        Total time: 540.97s
                               ETA: 447.6s

################################################################################
                     [1m Learning iteration 1095/2000 [0m

                       Computation: 16569 steps/s (collection: 0.279s, learning 0.216s)
               Value function loss: 90619.7937
                    Surrogate loss: -0.0056
             Mean action noise std: 0.95
                       Mean reward: 11708.98
               Mean episode length: 420.69
                 Mean success rate: 85.00
                  Mean reward/step: 28.88
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 0.49s
                        Total time: 541.46s
                               ETA: 447.1s

################################################################################
                     [1m Learning iteration 1096/2000 [0m

                       Computation: 16991 steps/s (collection: 0.267s, learning 0.215s)
               Value function loss: 70819.7630
                    Surrogate loss: -0.0041
             Mean action noise std: 0.95
                       Mean reward: 11866.02
               Mean episode length: 425.52
                 Mean success rate: 86.00
                  Mean reward/step: 28.97
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8986624
                    Iteration time: 0.48s
                        Total time: 541.94s
                               ETA: 446.6s

################################################################################
                     [1m Learning iteration 1097/2000 [0m

                       Computation: 17097 steps/s (collection: 0.266s, learning 0.213s)
               Value function loss: 118738.6815
                    Surrogate loss: -0.0045
             Mean action noise std: 0.95
                       Mean reward: 12052.12
               Mean episode length: 429.69
                 Mean success rate: 86.50
                  Mean reward/step: 28.72
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 0.48s
                        Total time: 542.42s
                               ETA: 446.1s

################################################################################
                     [1m Learning iteration 1098/2000 [0m

                       Computation: 17405 steps/s (collection: 0.263s, learning 0.207s)
               Value function loss: 121978.9384
                    Surrogate loss: -0.0046
             Mean action noise std: 0.95
                       Mean reward: 12116.45
               Mean episode length: 431.69
                 Mean success rate: 86.50
                  Mean reward/step: 28.10
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9003008
                    Iteration time: 0.47s
                        Total time: 542.89s
                               ETA: 445.6s

################################################################################
                     [1m Learning iteration 1099/2000 [0m

                       Computation: 17562 steps/s (collection: 0.255s, learning 0.211s)
               Value function loss: 104378.8585
                    Surrogate loss: -0.0034
             Mean action noise std: 0.95
                       Mean reward: 12392.46
               Mean episode length: 439.44
                 Mean success rate: 88.00
                  Mean reward/step: 28.38
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 0.47s
                        Total time: 543.36s
                               ETA: 445.1s

################################################################################
                     [1m Learning iteration 1100/2000 [0m

                       Computation: 17380 steps/s (collection: 0.273s, learning 0.198s)
               Value function loss: 86377.5136
                    Surrogate loss: 0.0029
             Mean action noise std: 0.95
                       Mean reward: 12449.61
               Mean episode length: 439.26
                 Mean success rate: 88.00
                  Mean reward/step: 27.59
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9019392
                    Iteration time: 0.47s
                        Total time: 543.83s
                               ETA: 444.5s

################################################################################
                     [1m Learning iteration 1101/2000 [0m

                       Computation: 17851 steps/s (collection: 0.256s, learning 0.203s)
               Value function loss: 70393.7120
                    Surrogate loss: -0.0029
             Mean action noise std: 0.95
                       Mean reward: 12108.39
               Mean episode length: 427.69
                 Mean success rate: 85.50
                  Mean reward/step: 26.61
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 0.46s
                        Total time: 544.29s
                               ETA: 444.0s

################################################################################
                     [1m Learning iteration 1102/2000 [0m

                       Computation: 17995 steps/s (collection: 0.253s, learning 0.202s)
               Value function loss: 90537.4883
                    Surrogate loss: -0.0033
             Mean action noise std: 0.95
                       Mean reward: 12133.27
               Mean episode length: 427.50
                 Mean success rate: 85.50
                  Mean reward/step: 26.30
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9035776
                    Iteration time: 0.46s
                        Total time: 544.75s
                               ETA: 443.5s

################################################################################
                     [1m Learning iteration 1103/2000 [0m

                       Computation: 17813 steps/s (collection: 0.261s, learning 0.199s)
               Value function loss: 101273.1453
                    Surrogate loss: -0.0036
             Mean action noise std: 0.95
                       Mean reward: 12127.21
               Mean episode length: 428.37
                 Mean success rate: 86.50
                  Mean reward/step: 25.88
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.46s
                        Total time: 545.21s
                               ETA: 443.0s

################################################################################
                     [1m Learning iteration 1104/2000 [0m

                       Computation: 17876 steps/s (collection: 0.255s, learning 0.203s)
               Value function loss: 78542.9005
                    Surrogate loss: 0.0017
             Mean action noise std: 0.95
                       Mean reward: 12302.93
               Mean episode length: 434.75
                 Mean success rate: 87.50
                  Mean reward/step: 26.27
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9052160
                    Iteration time: 0.46s
                        Total time: 545.66s
                               ETA: 442.5s

################################################################################
                     [1m Learning iteration 1105/2000 [0m

                       Computation: 18024 steps/s (collection: 0.251s, learning 0.204s)
               Value function loss: 64547.3217
                    Surrogate loss: -0.0007
             Mean action noise std: 0.95
                       Mean reward: 12339.98
               Mean episode length: 437.20
                 Mean success rate: 88.00
                  Mean reward/step: 27.43
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 0.45s
                        Total time: 546.12s
                               ETA: 441.9s

################################################################################
                     [1m Learning iteration 1106/2000 [0m

                       Computation: 18126 steps/s (collection: 0.252s, learning 0.200s)
               Value function loss: 92631.0503
                    Surrogate loss: -0.0019
             Mean action noise std: 0.95
                       Mean reward: 12634.89
               Mean episode length: 446.21
                 Mean success rate: 90.00
                  Mean reward/step: 27.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9068544
                    Iteration time: 0.45s
                        Total time: 546.57s
                               ETA: 441.4s

################################################################################
                     [1m Learning iteration 1107/2000 [0m

                       Computation: 17355 steps/s (collection: 0.270s, learning 0.202s)
               Value function loss: 79119.6242
                    Surrogate loss: -0.0037
             Mean action noise std: 0.95
                       Mean reward: 12435.87
               Mean episode length: 440.82
                 Mean success rate: 88.50
                  Mean reward/step: 26.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 0.47s
                        Total time: 547.04s
                               ETA: 440.9s

################################################################################
                     [1m Learning iteration 1108/2000 [0m

                       Computation: 17023 steps/s (collection: 0.265s, learning 0.217s)
               Value function loss: 95350.6189
                    Surrogate loss: -0.0039
             Mean action noise std: 0.95
                       Mean reward: 12142.70
               Mean episode length: 433.46
                 Mean success rate: 87.00
                  Mean reward/step: 26.02
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9084928
                    Iteration time: 0.48s
                        Total time: 547.52s
                               ETA: 440.4s

################################################################################
                     [1m Learning iteration 1109/2000 [0m

                       Computation: 17476 steps/s (collection: 0.265s, learning 0.204s)
               Value function loss: 101186.3554
                    Surrogate loss: -0.0025
             Mean action noise std: 0.95
                       Mean reward: 11948.00
               Mean episode length: 428.76
                 Mean success rate: 86.50
                  Mean reward/step: 25.95
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 0.47s
                        Total time: 547.99s
                               ETA: 439.9s

################################################################################
                     [1m Learning iteration 1110/2000 [0m

                       Computation: 17979 steps/s (collection: 0.246s, learning 0.210s)
               Value function loss: 66367.2051
                    Surrogate loss: -0.0039
             Mean action noise std: 0.95
                       Mean reward: 11825.13
               Mean episode length: 426.14
                 Mean success rate: 86.50
                  Mean reward/step: 26.71
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9101312
                    Iteration time: 0.46s
                        Total time: 548.45s
                               ETA: 439.4s

################################################################################
                     [1m Learning iteration 1111/2000 [0m

                       Computation: 17256 steps/s (collection: 0.258s, learning 0.217s)
               Value function loss: 66984.3449
                    Surrogate loss: -0.0040
             Mean action noise std: 0.95
                       Mean reward: 11902.60
               Mean episode length: 429.83
                 Mean success rate: 87.00
                  Mean reward/step: 27.38
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 0.47s
                        Total time: 548.92s
                               ETA: 438.8s

################################################################################
                     [1m Learning iteration 1112/2000 [0m

                       Computation: 17713 steps/s (collection: 0.259s, learning 0.203s)
               Value function loss: 97018.8697
                    Surrogate loss: -0.0059
             Mean action noise std: 0.95
                       Mean reward: 11950.57
               Mean episode length: 434.28
                 Mean success rate: 88.50
                  Mean reward/step: 28.45
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9117696
                    Iteration time: 0.46s
                        Total time: 549.39s
                               ETA: 438.3s

################################################################################
                     [1m Learning iteration 1113/2000 [0m

                       Computation: 17359 steps/s (collection: 0.265s, learning 0.207s)
               Value function loss: 117016.9417
                    Surrogate loss: -0.0051
             Mean action noise std: 0.95
                       Mean reward: 12019.31
               Mean episode length: 438.77
                 Mean success rate: 89.00
                  Mean reward/step: 27.75
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 0.47s
                        Total time: 549.86s
                               ETA: 437.8s

################################################################################
                     [1m Learning iteration 1114/2000 [0m

                       Computation: 17612 steps/s (collection: 0.262s, learning 0.203s)
               Value function loss: 100060.0199
                    Surrogate loss: -0.0049
             Mean action noise std: 0.95
                       Mean reward: 11828.14
               Mean episode length: 433.82
                 Mean success rate: 88.00
                  Mean reward/step: 27.48
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9134080
                    Iteration time: 0.47s
                        Total time: 550.32s
                               ETA: 437.3s

################################################################################
                     [1m Learning iteration 1115/2000 [0m

                       Computation: 17709 steps/s (collection: 0.256s, learning 0.207s)
               Value function loss: 77180.7513
                    Surrogate loss: 0.0007
             Mean action noise std: 0.95
                       Mean reward: 11913.34
               Mean episode length: 436.02
                 Mean success rate: 88.50
                  Mean reward/step: 28.13
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.46s
                        Total time: 550.79s
                               ETA: 436.8s

################################################################################
                     [1m Learning iteration 1116/2000 [0m

                       Computation: 17682 steps/s (collection: 0.261s, learning 0.203s)
               Value function loss: 72799.6541
                    Surrogate loss: -0.0024
             Mean action noise std: 0.96
                       Mean reward: 11753.84
               Mean episode length: 432.73
                 Mean success rate: 88.00
                  Mean reward/step: 28.89
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9150464
                    Iteration time: 0.46s
                        Total time: 551.25s
                               ETA: 436.3s

################################################################################
                     [1m Learning iteration 1117/2000 [0m

                       Computation: 17011 steps/s (collection: 0.261s, learning 0.221s)
               Value function loss: 71352.9269
                    Surrogate loss: -0.0015
             Mean action noise std: 0.96
                       Mean reward: 11448.49
               Mean episode length: 422.75
                 Mean success rate: 86.00
                  Mean reward/step: 29.26
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 0.48s
                        Total time: 551.73s
                               ETA: 435.8s

################################################################################
                     [1m Learning iteration 1118/2000 [0m

                       Computation: 17588 steps/s (collection: 0.260s, learning 0.205s)
               Value function loss: 110074.0633
                    Surrogate loss: -0.0027
             Mean action noise std: 0.96
                       Mean reward: 11652.23
               Mean episode length: 427.39
                 Mean success rate: 87.00
                  Mean reward/step: 29.10
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9166848
                    Iteration time: 0.47s
                        Total time: 552.20s
                               ETA: 435.2s

################################################################################
                     [1m Learning iteration 1119/2000 [0m

                       Computation: 17529 steps/s (collection: 0.257s, learning 0.211s)
               Value function loss: 107349.2422
                    Surrogate loss: -0.0000
             Mean action noise std: 0.96
                       Mean reward: 12107.61
               Mean episode length: 442.99
                 Mean success rate: 90.00
                  Mean reward/step: 28.25
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 0.47s
                        Total time: 552.66s
                               ETA: 434.7s

################################################################################
                     [1m Learning iteration 1120/2000 [0m

                       Computation: 17481 steps/s (collection: 0.265s, learning 0.204s)
               Value function loss: 70725.1832
                    Surrogate loss: -0.0028
             Mean action noise std: 0.96
                       Mean reward: 11945.87
               Mean episode length: 437.64
                 Mean success rate: 88.50
                  Mean reward/step: 28.88
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9183232
                    Iteration time: 0.47s
                        Total time: 553.13s
                               ETA: 434.2s

################################################################################
                     [1m Learning iteration 1121/2000 [0m

                       Computation: 17917 steps/s (collection: 0.256s, learning 0.201s)
               Value function loss: 83856.7090
                    Surrogate loss: -0.0035
             Mean action noise std: 0.96
                       Mean reward: 11935.28
               Mean episode length: 437.85
                 Mean success rate: 88.50
                  Mean reward/step: 29.67
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 0.46s
                        Total time: 553.59s
                               ETA: 433.7s

################################################################################
                     [1m Learning iteration 1122/2000 [0m

                       Computation: 17780 steps/s (collection: 0.251s, learning 0.210s)
               Value function loss: 85750.1365
                    Surrogate loss: 0.0016
             Mean action noise std: 0.96
                       Mean reward: 11957.15
               Mean episode length: 437.85
                 Mean success rate: 88.50
                  Mean reward/step: 29.36
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9199616
                    Iteration time: 0.46s
                        Total time: 554.05s
                               ETA: 433.2s

################################################################################
                     [1m Learning iteration 1123/2000 [0m

                       Computation: 17249 steps/s (collection: 0.268s, learning 0.207s)
               Value function loss: 107743.9337
                    Surrogate loss: -0.0054
             Mean action noise std: 0.95
                       Mean reward: 12030.67
               Mean episode length: 437.01
                 Mean success rate: 89.00
                  Mean reward/step: 29.26
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 0.47s
                        Total time: 554.52s
                               ETA: 432.7s

################################################################################
                     [1m Learning iteration 1124/2000 [0m

                       Computation: 16827 steps/s (collection: 0.272s, learning 0.215s)
               Value function loss: 134366.6293
                    Surrogate loss: -0.0050
             Mean action noise std: 0.95
                       Mean reward: 12060.96
               Mean episode length: 435.76
                 Mean success rate: 89.00
                  Mean reward/step: 29.05
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9216000
                    Iteration time: 0.49s
                        Total time: 555.01s
                               ETA: 432.2s

################################################################################
                     [1m Learning iteration 1125/2000 [0m

                       Computation: 17501 steps/s (collection: 0.261s, learning 0.207s)
               Value function loss: 72888.1952
                    Surrogate loss: -0.0056
             Mean action noise std: 0.95
                       Mean reward: 12372.97
               Mean episode length: 444.38
                 Mean success rate: 90.50
                  Mean reward/step: 28.54
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 0.47s
                        Total time: 555.48s
                               ETA: 431.7s

################################################################################
                     [1m Learning iteration 1126/2000 [0m

                       Computation: 17619 steps/s (collection: 0.252s, learning 0.213s)
               Value function loss: 113359.3771
                    Surrogate loss: -0.0047
             Mean action noise std: 0.95
                       Mean reward: 12202.32
               Mean episode length: 436.77
                 Mean success rate: 89.00
                  Mean reward/step: 28.60
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9232384
                    Iteration time: 0.46s
                        Total time: 555.94s
                               ETA: 431.1s

################################################################################
                     [1m Learning iteration 1127/2000 [0m

                       Computation: 16815 steps/s (collection: 0.275s, learning 0.212s)
               Value function loss: 69491.5810
                    Surrogate loss: -0.0055
             Mean action noise std: 0.95
                       Mean reward: 12069.95
               Mean episode length: 431.86
                 Mean success rate: 88.00
                  Mean reward/step: 28.34
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.49s
                        Total time: 556.43s
                               ETA: 430.6s

################################################################################
                     [1m Learning iteration 1128/2000 [0m

                       Computation: 16872 steps/s (collection: 0.271s, learning 0.214s)
               Value function loss: 134798.1041
                    Surrogate loss: -0.0034
             Mean action noise std: 0.95
                       Mean reward: 12280.94
               Mean episode length: 436.29
                 Mean success rate: 88.50
                  Mean reward/step: 28.00
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9248768
                    Iteration time: 0.49s
                        Total time: 556.92s
                               ETA: 430.1s

################################################################################
                     [1m Learning iteration 1129/2000 [0m

                       Computation: 17628 steps/s (collection: 0.256s, learning 0.209s)
               Value function loss: 96042.7200
                    Surrogate loss: 0.0055
             Mean action noise std: 0.95
                       Mean reward: 12524.99
               Mean episode length: 441.01
                 Mean success rate: 89.50
                  Mean reward/step: 27.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 0.46s
                        Total time: 557.38s
                               ETA: 429.6s

################################################################################
                     [1m Learning iteration 1130/2000 [0m

                       Computation: 17703 steps/s (collection: 0.254s, learning 0.209s)
               Value function loss: 91068.1874
                    Surrogate loss: -0.0020
             Mean action noise std: 0.95
                       Mean reward: 12569.16
               Mean episode length: 440.29
                 Mean success rate: 89.50
                  Mean reward/step: 27.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9265152
                    Iteration time: 0.46s
                        Total time: 557.84s
                               ETA: 429.1s

################################################################################
                     [1m Learning iteration 1131/2000 [0m

                       Computation: 17302 steps/s (collection: 0.261s, learning 0.212s)
               Value function loss: 74265.7084
                    Surrogate loss: -0.0040
             Mean action noise std: 0.95
                       Mean reward: 12900.25
               Mean episode length: 450.94
                 Mean success rate: 91.50
                  Mean reward/step: 28.17
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 0.47s
                        Total time: 558.32s
                               ETA: 428.6s

################################################################################
                     [1m Learning iteration 1132/2000 [0m

                       Computation: 17439 steps/s (collection: 0.263s, learning 0.207s)
               Value function loss: 87103.9050
                    Surrogate loss: -0.0033
             Mean action noise std: 0.95
                       Mean reward: 12820.42
               Mean episode length: 448.80
                 Mean success rate: 91.50
                  Mean reward/step: 28.63
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9281536
                    Iteration time: 0.47s
                        Total time: 558.79s
                               ETA: 428.1s

################################################################################
                     [1m Learning iteration 1133/2000 [0m

                       Computation: 18088 steps/s (collection: 0.253s, learning 0.200s)
               Value function loss: 88614.2491
                    Surrogate loss: -0.0055
             Mean action noise std: 0.95
                       Mean reward: 12782.42
               Mean episode length: 446.48
                 Mean success rate: 91.00
                  Mean reward/step: 28.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 0.45s
                        Total time: 559.24s
                               ETA: 427.6s

################################################################################
                     [1m Learning iteration 1134/2000 [0m

                       Computation: 17686 steps/s (collection: 0.260s, learning 0.203s)
               Value function loss: 110325.5609
                    Surrogate loss: -0.0047
             Mean action noise std: 0.95
                       Mean reward: 13063.28
               Mean episode length: 454.06
                 Mean success rate: 91.50
                  Mean reward/step: 27.52
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9297920
                    Iteration time: 0.46s
                        Total time: 559.70s
                               ETA: 427.1s

################################################################################
                     [1m Learning iteration 1135/2000 [0m

                       Computation: 17564 steps/s (collection: 0.256s, learning 0.211s)
               Value function loss: 84154.0056
                    Surrogate loss: -0.0026
             Mean action noise std: 0.95
                       Mean reward: 12835.22
               Mean episode length: 446.62
                 Mean success rate: 90.50
                  Mean reward/step: 27.91
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 0.47s
                        Total time: 560.17s
                               ETA: 426.5s

################################################################################
                     [1m Learning iteration 1136/2000 [0m

                       Computation: 17165 steps/s (collection: 0.259s, learning 0.218s)
               Value function loss: 77419.4078
                    Surrogate loss: -0.0001
             Mean action noise std: 0.95
                       Mean reward: 12809.84
               Mean episode length: 446.62
                 Mean success rate: 90.50
                  Mean reward/step: 28.91
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9314304
                    Iteration time: 0.48s
                        Total time: 560.65s
                               ETA: 426.0s

################################################################################
                     [1m Learning iteration 1137/2000 [0m

                       Computation: 18050 steps/s (collection: 0.247s, learning 0.207s)
               Value function loss: 65758.4001
                    Surrogate loss: -0.0031
             Mean action noise std: 0.95
                       Mean reward: 13019.39
               Mean episode length: 454.23
                 Mean success rate: 92.00
                  Mean reward/step: 28.43
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 0.45s
                        Total time: 561.10s
                               ETA: 425.5s

################################################################################
                     [1m Learning iteration 1138/2000 [0m

                       Computation: 17835 steps/s (collection: 0.254s, learning 0.205s)
               Value function loss: 83708.8834
                    Surrogate loss: -0.0027
             Mean action noise std: 0.95
                       Mean reward: 13216.35
               Mean episode length: 459.10
                 Mean success rate: 93.00
                  Mean reward/step: 26.44
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9330688
                    Iteration time: 0.46s
                        Total time: 561.56s
                               ETA: 425.0s

################################################################################
                     [1m Learning iteration 1139/2000 [0m

                       Computation: 17675 steps/s (collection: 0.258s, learning 0.206s)
               Value function loss: 77147.5532
                    Surrogate loss: 0.0036
             Mean action noise std: 0.96
                       Mean reward: 13446.00
               Mean episode length: 467.42
                 Mean success rate: 94.50
                  Mean reward/step: 25.87
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.46s
                        Total time: 562.02s
                               ETA: 424.5s

################################################################################
                     [1m Learning iteration 1140/2000 [0m

                       Computation: 17049 steps/s (collection: 0.274s, learning 0.206s)
               Value function loss: 110054.8007
                    Surrogate loss: -0.0016
             Mean action noise std: 0.96
                       Mean reward: 13521.41
               Mean episode length: 471.78
                 Mean success rate: 95.50
                  Mean reward/step: 27.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9347072
                    Iteration time: 0.48s
                        Total time: 562.50s
                               ETA: 424.0s

################################################################################
                     [1m Learning iteration 1141/2000 [0m

                       Computation: 17892 steps/s (collection: 0.255s, learning 0.203s)
               Value function loss: 83492.2852
                    Surrogate loss: 0.0022
             Mean action noise std: 0.96
                       Mean reward: 13468.01
               Mean episode length: 471.78
                 Mean success rate: 95.50
                  Mean reward/step: 28.51
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 0.46s
                        Total time: 562.96s
                               ETA: 423.5s

################################################################################
                     [1m Learning iteration 1142/2000 [0m

                       Computation: 17355 steps/s (collection: 0.262s, learning 0.210s)
               Value function loss: 112327.9068
                    Surrogate loss: -0.0049
             Mean action noise std: 0.96
                       Mean reward: 13563.28
               Mean episode length: 476.69
                 Mean success rate: 96.50
                  Mean reward/step: 28.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9363456
                    Iteration time: 0.47s
                        Total time: 563.43s
                               ETA: 422.9s

################################################################################
                     [1m Learning iteration 1143/2000 [0m

                       Computation: 17531 steps/s (collection: 0.262s, learning 0.206s)
               Value function loss: 138328.9695
                    Surrogate loss: -0.0041
             Mean action noise std: 0.96
                       Mean reward: 13581.63
               Mean episode length: 477.89
                 Mean success rate: 96.50
                  Mean reward/step: 28.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 0.47s
                        Total time: 563.90s
                               ETA: 422.4s

################################################################################
                     [1m Learning iteration 1144/2000 [0m

                       Computation: 17277 steps/s (collection: 0.263s, learning 0.211s)
               Value function loss: 103444.3375
                    Surrogate loss: -0.0036
             Mean action noise std: 0.96
                       Mean reward: 13603.54
               Mean episode length: 481.19
                 Mean success rate: 97.00
                  Mean reward/step: 27.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9379840
                    Iteration time: 0.47s
                        Total time: 564.38s
                               ETA: 421.9s

################################################################################
                     [1m Learning iteration 1145/2000 [0m

                       Computation: 16906 steps/s (collection: 0.276s, learning 0.209s)
               Value function loss: 84191.5150
                    Surrogate loss: -0.0027
             Mean action noise std: 0.96
                       Mean reward: 13385.63
               Mean episode length: 476.57
                 Mean success rate: 96.00
                  Mean reward/step: 27.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 0.48s
                        Total time: 564.86s
                               ETA: 421.4s

################################################################################
                     [1m Learning iteration 1146/2000 [0m

                       Computation: 16820 steps/s (collection: 0.274s, learning 0.213s)
               Value function loss: 71852.7326
                    Surrogate loss: -0.0032
             Mean action noise std: 0.96
                       Mean reward: 13622.74
               Mean episode length: 484.37
                 Mean success rate: 97.50
                  Mean reward/step: 28.96
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9396224
                    Iteration time: 0.49s
                        Total time: 565.35s
                               ETA: 420.9s

################################################################################
                     [1m Learning iteration 1147/2000 [0m

                       Computation: 16420 steps/s (collection: 0.279s, learning 0.220s)
               Value function loss: 65636.5048
                    Surrogate loss: -0.0014
             Mean action noise std: 0.96
                       Mean reward: 13592.16
               Mean episode length: 484.37
                 Mean success rate: 97.50
                  Mean reward/step: 29.16
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 0.50s
                        Total time: 565.85s
                               ETA: 420.4s

################################################################################
                     [1m Learning iteration 1148/2000 [0m

                       Computation: 16299 steps/s (collection: 0.284s, learning 0.218s)
               Value function loss: 91118.7123
                    Surrogate loss: -0.0028
             Mean action noise std: 0.96
                       Mean reward: 13472.99
               Mean episode length: 484.12
                 Mean success rate: 97.50
                  Mean reward/step: 29.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9412608
                    Iteration time: 0.50s
                        Total time: 566.35s
                               ETA: 420.0s

################################################################################
                     [1m Learning iteration 1149/2000 [0m

                       Computation: 16532 steps/s (collection: 0.282s, learning 0.214s)
               Value function loss: 132954.5885
                    Surrogate loss: -0.0025
             Mean action noise std: 0.96
                       Mean reward: 13258.92
               Mean episode length: 478.22
                 Mean success rate: 96.50
                  Mean reward/step: 28.81
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 0.50s
                        Total time: 566.84s
                               ETA: 419.5s

################################################################################
                     [1m Learning iteration 1150/2000 [0m

                       Computation: 16333 steps/s (collection: 0.290s, learning 0.212s)
               Value function loss: 100968.0471
                    Surrogate loss: -0.0054
             Mean action noise std: 0.96
                       Mean reward: 12976.51
               Mean episode length: 469.44
                 Mean success rate: 95.00
                  Mean reward/step: 27.96
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9428992
                    Iteration time: 0.50s
                        Total time: 567.35s
                               ETA: 419.0s

################################################################################
                     [1m Learning iteration 1151/2000 [0m

                       Computation: 16187 steps/s (collection: 0.287s, learning 0.219s)
               Value function loss: 90680.4502
                    Surrogate loss: -0.0055
             Mean action noise std: 0.96
                       Mean reward: 12839.47
               Mean episode length: 462.50
                 Mean success rate: 94.00
                  Mean reward/step: 28.63
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.51s
                        Total time: 567.85s
                               ETA: 418.5s

################################################################################
                     [1m Learning iteration 1152/2000 [0m

                       Computation: 17589 steps/s (collection: 0.257s, learning 0.208s)
               Value function loss: 59821.5343
                    Surrogate loss: -0.0057
             Mean action noise std: 0.96
                       Mean reward: 12733.97
               Mean episode length: 460.01
                 Mean success rate: 93.50
                  Mean reward/step: 29.21
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9445376
                    Iteration time: 0.47s
                        Total time: 568.32s
                               ETA: 418.0s

################################################################################
                     [1m Learning iteration 1153/2000 [0m

                       Computation: 16637 steps/s (collection: 0.279s, learning 0.214s)
               Value function loss: 92589.1659
                    Surrogate loss: -0.0007
             Mean action noise std: 0.96
                       Mean reward: 12495.41
               Mean episode length: 450.82
                 Mean success rate: 92.00
                  Mean reward/step: 30.05
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 0.49s
                        Total time: 568.81s
                               ETA: 417.5s

################################################################################
                     [1m Learning iteration 1154/2000 [0m

                       Computation: 17575 steps/s (collection: 0.255s, learning 0.211s)
               Value function loss: 99323.6523
                    Surrogate loss: 0.0003
             Mean action noise std: 0.96
                       Mean reward: 12565.53
               Mean episode length: 450.82
                 Mean success rate: 92.00
                  Mean reward/step: 29.99
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9461760
                    Iteration time: 0.47s
                        Total time: 569.28s
                               ETA: 417.0s

################################################################################
                     [1m Learning iteration 1155/2000 [0m

                       Computation: 17193 steps/s (collection: 0.264s, learning 0.212s)
               Value function loss: 109441.0955
                    Surrogate loss: -0.0060
             Mean action noise std: 0.96
                       Mean reward: 12662.11
               Mean episode length: 450.82
                 Mean success rate: 92.00
                  Mean reward/step: 30.06
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 0.48s
                        Total time: 569.75s
                               ETA: 416.5s

################################################################################
                     [1m Learning iteration 1156/2000 [0m

                       Computation: 17671 steps/s (collection: 0.257s, learning 0.206s)
               Value function loss: 79525.7390
                    Surrogate loss: -0.0031
             Mean action noise std: 0.96
                       Mean reward: 12807.79
               Mean episode length: 453.46
                 Mean success rate: 92.50
                  Mean reward/step: 29.78
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9478144
                    Iteration time: 0.46s
                        Total time: 570.22s
                               ETA: 416.0s

################################################################################
                     [1m Learning iteration 1157/2000 [0m

                       Computation: 17337 steps/s (collection: 0.266s, learning 0.206s)
               Value function loss: 93082.5803
                    Surrogate loss: -0.0038
             Mean action noise std: 0.96
                       Mean reward: 13041.63
               Mean episode length: 459.75
                 Mean success rate: 93.50
                  Mean reward/step: 30.55
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 0.47s
                        Total time: 570.69s
                               ETA: 415.4s

################################################################################
                     [1m Learning iteration 1158/2000 [0m

                       Computation: 16636 steps/s (collection: 0.270s, learning 0.222s)
               Value function loss: 87783.5467
                    Surrogate loss: -0.0020
             Mean action noise std: 0.96
                       Mean reward: 13113.10
               Mean episode length: 459.75
                 Mean success rate: 93.50
                  Mean reward/step: 29.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9494528
                    Iteration time: 0.49s
                        Total time: 571.18s
                               ETA: 415.0s

################################################################################
                     [1m Learning iteration 1159/2000 [0m

                       Computation: 16704 steps/s (collection: 0.272s, learning 0.218s)
               Value function loss: 113300.2213
                    Surrogate loss: 0.0039
             Mean action noise std: 0.96
                       Mean reward: 13250.86
               Mean episode length: 459.41
                 Mean success rate: 93.50
                  Mean reward/step: 29.29
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 0.49s
                        Total time: 571.67s
                               ETA: 414.5s

################################################################################
                     [1m Learning iteration 1160/2000 [0m

                       Computation: 16651 steps/s (collection: 0.281s, learning 0.210s)
               Value function loss: 125971.8969
                    Surrogate loss: -0.0022
             Mean action noise std: 0.96
                       Mean reward: 13488.68
               Mean episode length: 465.31
                 Mean success rate: 94.50
                  Mean reward/step: 29.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9510912
                    Iteration time: 0.49s
                        Total time: 572.16s
                               ETA: 414.0s

################################################################################
                     [1m Learning iteration 1161/2000 [0m

                       Computation: 16908 steps/s (collection: 0.274s, learning 0.210s)
               Value function loss: 100421.7741
                    Surrogate loss: 0.0016
             Mean action noise std: 0.96
                       Mean reward: 13570.31
               Mean episode length: 466.36
                 Mean success rate: 94.50
                  Mean reward/step: 28.57
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 0.48s
                        Total time: 572.65s
                               ETA: 413.5s

################################################################################
                     [1m Learning iteration 1162/2000 [0m

                       Computation: 16939 steps/s (collection: 0.266s, learning 0.218s)
               Value function loss: 84092.9535
                    Surrogate loss: 0.0006
             Mean action noise std: 0.95
                       Mean reward: 13553.68
               Mean episode length: 466.36
                 Mean success rate: 94.50
                  Mean reward/step: 29.45
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9527296
                    Iteration time: 0.48s
                        Total time: 573.13s
                               ETA: 413.0s

################################################################################
                     [1m Learning iteration 1163/2000 [0m

                       Computation: 17119 steps/s (collection: 0.257s, learning 0.221s)
               Value function loss: 90404.5553
                    Surrogate loss: 0.0040
             Mean action noise std: 0.95
                       Mean reward: 13874.27
               Mean episode length: 475.80
                 Mean success rate: 96.00
                  Mean reward/step: 29.79
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.48s
                        Total time: 573.61s
                               ETA: 412.5s

################################################################################
                     [1m Learning iteration 1164/2000 [0m

                       Computation: 16546 steps/s (collection: 0.279s, learning 0.216s)
               Value function loss: 73682.3957
                    Surrogate loss: -0.0026
             Mean action noise std: 0.95
                       Mean reward: 13915.70
               Mean episode length: 475.80
                 Mean success rate: 96.00
                  Mean reward/step: 29.95
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9543680
                    Iteration time: 0.50s
                        Total time: 574.11s
                               ETA: 412.0s

################################################################################
                     [1m Learning iteration 1165/2000 [0m

                       Computation: 14958 steps/s (collection: 0.293s, learning 0.254s)
               Value function loss: 116259.1971
                    Surrogate loss: -0.0027
             Mean action noise std: 0.95
                       Mean reward: 14153.85
               Mean episode length: 483.27
                 Mean success rate: 97.50
                  Mean reward/step: 29.17
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 0.55s
                        Total time: 574.65s
                               ETA: 411.5s

################################################################################
                     [1m Learning iteration 1166/2000 [0m

                       Computation: 16158 steps/s (collection: 0.289s, learning 0.218s)
               Value function loss: 96128.3368
                    Surrogate loss: -0.0014
             Mean action noise std: 0.95
                       Mean reward: 14165.04
               Mean episode length: 483.27
                 Mean success rate: 97.50
                  Mean reward/step: 28.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9560064
                    Iteration time: 0.51s
                        Total time: 575.16s
                               ETA: 411.0s

################################################################################
                     [1m Learning iteration 1167/2000 [0m

                       Computation: 17054 steps/s (collection: 0.275s, learning 0.205s)
               Value function loss: 85036.4951
                    Surrogate loss: -0.0023
             Mean action noise std: 0.95
                       Mean reward: 14197.11
               Mean episode length: 483.27
                 Mean success rate: 97.50
                  Mean reward/step: 28.73
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 0.48s
                        Total time: 575.64s
                               ETA: 410.5s

################################################################################
                     [1m Learning iteration 1168/2000 [0m

                       Computation: 16772 steps/s (collection: 0.268s, learning 0.220s)
               Value function loss: 81036.1063
                    Surrogate loss: -0.0026
             Mean action noise std: 0.96
                       Mean reward: 14075.38
               Mean episode length: 478.56
                 Mean success rate: 96.50
                  Mean reward/step: 29.70
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9576448
                    Iteration time: 0.49s
                        Total time: 576.13s
                               ETA: 410.0s

################################################################################
                     [1m Learning iteration 1169/2000 [0m

                       Computation: 16111 steps/s (collection: 0.282s, learning 0.226s)
               Value function loss: 100479.1382
                    Surrogate loss: -0.0051
             Mean action noise std: 0.96
                       Mean reward: 13519.60
               Mean episode length: 462.07
                 Mean success rate: 93.00
                  Mean reward/step: 29.55
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 0.51s
                        Total time: 576.64s
                               ETA: 409.6s

################################################################################
                     [1m Learning iteration 1170/2000 [0m

                       Computation: 15467 steps/s (collection: 0.279s, learning 0.250s)
               Value function loss: 114001.5676
                    Surrogate loss: -0.0033
             Mean action noise std: 0.95
                       Mean reward: 13565.22
               Mean episode length: 462.07
                 Mean success rate: 93.00
                  Mean reward/step: 29.98
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9592832
                    Iteration time: 0.53s
                        Total time: 577.17s
                               ETA: 409.1s

################################################################################
                     [1m Learning iteration 1171/2000 [0m

                       Computation: 16299 steps/s (collection: 0.298s, learning 0.205s)
               Value function loss: 120653.5102
                    Surrogate loss: -0.0027
             Mean action noise std: 0.95
                       Mean reward: 13716.06
               Mean episode length: 466.67
                 Mean success rate: 93.50
                  Mean reward/step: 30.43
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 0.50s
                        Total time: 577.67s
                               ETA: 408.6s

################################################################################
                     [1m Learning iteration 1172/2000 [0m

                       Computation: 16971 steps/s (collection: 0.272s, learning 0.211s)
               Value function loss: 66803.4983
                    Surrogate loss: 0.0056
             Mean action noise std: 0.95
                       Mean reward: 13622.35
               Mean episode length: 462.88
                 Mean success rate: 92.50
                  Mean reward/step: 30.17
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9609216
                    Iteration time: 0.48s
                        Total time: 578.15s
                               ETA: 408.1s

################################################################################
                     [1m Learning iteration 1173/2000 [0m

                       Computation: 16708 steps/s (collection: 0.266s, learning 0.224s)
               Value function loss: 141396.7526
                    Surrogate loss: 0.0021
             Mean action noise std: 0.96
                       Mean reward: 13524.32
               Mean episode length: 458.37
                 Mean success rate: 92.00
                  Mean reward/step: 30.18
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 0.49s
                        Total time: 578.64s
                               ETA: 407.6s

################################################################################
                     [1m Learning iteration 1174/2000 [0m

                       Computation: 16290 steps/s (collection: 0.290s, learning 0.213s)
               Value function loss: 101259.7275
                    Surrogate loss: -0.0015
             Mean action noise std: 0.96
                       Mean reward: 13338.55
               Mean episode length: 452.75
                 Mean success rate: 91.00
                  Mean reward/step: 28.92
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9625600
                    Iteration time: 0.50s
                        Total time: 579.14s
                               ETA: 407.1s

################################################################################
                     [1m Learning iteration 1175/2000 [0m

                       Computation: 16361 steps/s (collection: 0.285s, learning 0.216s)
               Value function loss: 125241.2117
                    Surrogate loss: -0.0038
             Mean action noise std: 0.96
                       Mean reward: 13327.91
               Mean episode length: 450.67
                 Mean success rate: 90.50
                  Mean reward/step: 28.43
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.50s
                        Total time: 579.65s
                               ETA: 406.6s

################################################################################
                     [1m Learning iteration 1176/2000 [0m

                       Computation: 16858 steps/s (collection: 0.275s, learning 0.211s)
               Value function loss: 145280.9549
                    Surrogate loss: -0.0042
             Mean action noise std: 0.95
                       Mean reward: 13034.98
               Mean episode length: 440.31
                 Mean success rate: 89.00
                  Mean reward/step: 28.64
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9641984
                    Iteration time: 0.49s
                        Total time: 580.13s
                               ETA: 406.1s

################################################################################
                     [1m Learning iteration 1177/2000 [0m

                       Computation: 16436 steps/s (collection: 0.292s, learning 0.206s)
               Value function loss: 105524.1117
                    Surrogate loss: -0.0055
             Mean action noise std: 0.96
                       Mean reward: 12759.57
               Mean episode length: 432.53
                 Mean success rate: 87.50
                  Mean reward/step: 28.50
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 0.50s
                        Total time: 580.63s
                               ETA: 405.7s

################################################################################
                     [1m Learning iteration 1178/2000 [0m

                       Computation: 16903 steps/s (collection: 0.278s, learning 0.207s)
               Value function loss: 81139.7856
                    Surrogate loss: -0.0027
             Mean action noise std: 0.96
                       Mean reward: 12386.05
               Mean episode length: 419.45
                 Mean success rate: 86.00
                  Mean reward/step: 28.97
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9658368
                    Iteration time: 0.48s
                        Total time: 581.11s
                               ETA: 405.2s

################################################################################
                     [1m Learning iteration 1179/2000 [0m

                       Computation: 16022 steps/s (collection: 0.305s, learning 0.206s)
               Value function loss: 89036.6798
                    Surrogate loss: 0.0014
             Mean action noise std: 0.96
                       Mean reward: 12619.59
               Mean episode length: 426.05
                 Mean success rate: 87.50
                  Mean reward/step: 29.60
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 0.51s
                        Total time: 581.63s
                               ETA: 404.7s

################################################################################
                     [1m Learning iteration 1180/2000 [0m

                       Computation: 17014 steps/s (collection: 0.280s, learning 0.202s)
               Value function loss: 92582.4387
                    Surrogate loss: -0.0040
             Mean action noise std: 0.96
                       Mean reward: 12716.36
               Mean episode length: 429.25
                 Mean success rate: 88.00
                  Mean reward/step: 29.90
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9674752
                    Iteration time: 0.48s
                        Total time: 582.11s
                               ETA: 404.2s

################################################################################
                     [1m Learning iteration 1181/2000 [0m

                       Computation: 16589 steps/s (collection: 0.281s, learning 0.213s)
               Value function loss: 107373.3048
                    Surrogate loss: -0.0053
             Mean action noise std: 0.96
                       Mean reward: 12605.80
               Mean episode length: 429.25
                 Mean success rate: 88.00
                  Mean reward/step: 29.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 0.49s
                        Total time: 582.60s
                               ETA: 403.7s

################################################################################
                     [1m Learning iteration 1182/2000 [0m

                       Computation: 17095 steps/s (collection: 0.267s, learning 0.213s)
               Value function loss: 86464.9224
                    Surrogate loss: -0.0042
             Mean action noise std: 0.96
                       Mean reward: 12380.41
               Mean episode length: 420.96
                 Mean success rate: 87.00
                  Mean reward/step: 29.46
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9691136
                    Iteration time: 0.48s
                        Total time: 583.08s
                               ETA: 403.2s

################################################################################
                     [1m Learning iteration 1183/2000 [0m

                       Computation: 17106 steps/s (collection: 0.276s, learning 0.203s)
               Value function loss: 93826.9599
                    Surrogate loss: -0.0018
             Mean action noise std: 0.96
                       Mean reward: 12260.29
               Mean episode length: 418.67
                 Mean success rate: 86.50
                  Mean reward/step: 29.78
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 0.48s
                        Total time: 583.56s
                               ETA: 402.7s

################################################################################
                     [1m Learning iteration 1184/2000 [0m

                       Computation: 17839 steps/s (collection: 0.249s, learning 0.210s)
               Value function loss: 104354.5713
                    Surrogate loss: -0.0034
             Mean action noise std: 0.96
                       Mean reward: 12439.41
               Mean episode length: 423.90
                 Mean success rate: 87.00
                  Mean reward/step: 30.19
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9707520
                    Iteration time: 0.46s
                        Total time: 584.02s
                               ETA: 402.2s

################################################################################
                     [1m Learning iteration 1185/2000 [0m

                       Computation: 18083 steps/s (collection: 0.247s, learning 0.206s)
               Value function loss: 113052.8184
                    Surrogate loss: -0.0049
             Mean action noise std: 0.96
                       Mean reward: 12382.53
               Mean episode length: 421.62
                 Mean success rate: 86.50
                  Mean reward/step: 29.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 0.45s
                        Total time: 584.47s
                               ETA: 401.6s

################################################################################
                     [1m Learning iteration 1186/2000 [0m

                       Computation: 17462 steps/s (collection: 0.260s, learning 0.209s)
               Value function loss: 101058.2752
                    Surrogate loss: -0.0023
             Mean action noise std: 0.96
                       Mean reward: 12398.56
               Mean episode length: 423.39
                 Mean success rate: 86.50
                  Mean reward/step: 29.39
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9723904
                    Iteration time: 0.47s
                        Total time: 584.94s
                               ETA: 401.1s

################################################################################
                     [1m Learning iteration 1187/2000 [0m

                       Computation: 15555 steps/s (collection: 0.310s, learning 0.217s)
               Value function loss: 108099.7570
                    Surrogate loss: -0.0034
             Mean action noise std: 0.96
                       Mean reward: 12341.56
               Mean episode length: 422.18
                 Mean success rate: 86.00
                  Mean reward/step: 28.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.53s
                        Total time: 585.47s
                               ETA: 400.7s

################################################################################
                     [1m Learning iteration 1188/2000 [0m

                       Computation: 17173 steps/s (collection: 0.262s, learning 0.215s)
               Value function loss: 91360.4047
                    Surrogate loss: -0.0050
             Mean action noise std: 0.96
                       Mean reward: 12352.65
               Mean episode length: 424.36
                 Mean success rate: 86.50
                  Mean reward/step: 28.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9740288
                    Iteration time: 0.48s
                        Total time: 585.94s
                               ETA: 400.2s

################################################################################
                     [1m Learning iteration 1189/2000 [0m

                       Computation: 16927 steps/s (collection: 0.274s, learning 0.210s)
               Value function loss: 118412.7144
                    Surrogate loss: -0.0028
             Mean action noise std: 0.96
                       Mean reward: 12785.67
               Mean episode length: 438.39
                 Mean success rate: 89.00
                  Mean reward/step: 29.07
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 0.48s
                        Total time: 586.43s
                               ETA: 399.7s

################################################################################
                     [1m Learning iteration 1190/2000 [0m

                       Computation: 17686 steps/s (collection: 0.255s, learning 0.209s)
               Value function loss: 106342.0619
                    Surrogate loss: -0.0039
             Mean action noise std: 0.96
                       Mean reward: 12585.68
               Mean episode length: 431.63
                 Mean success rate: 88.00
                  Mean reward/step: 29.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9756672
                    Iteration time: 0.46s
                        Total time: 586.89s
                               ETA: 399.1s

################################################################################
                     [1m Learning iteration 1191/2000 [0m

                       Computation: 17389 steps/s (collection: 0.262s, learning 0.209s)
               Value function loss: 136074.1742
                    Surrogate loss: -0.0018
             Mean action noise std: 0.96
                       Mean reward: 12272.93
               Mean episode length: 418.87
                 Mean success rate: 86.00
                  Mean reward/step: 29.33
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 0.47s
                        Total time: 587.36s
                               ETA: 398.6s

################################################################################
                     [1m Learning iteration 1192/2000 [0m

                       Computation: 17487 steps/s (collection: 0.264s, learning 0.204s)
               Value function loss: 100621.3315
                    Surrogate loss: 0.0009
             Mean action noise std: 0.96
                       Mean reward: 12187.77
               Mean episode length: 418.46
                 Mean success rate: 86.00
                  Mean reward/step: 28.84
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9773056
                    Iteration time: 0.47s
                        Total time: 587.83s
                               ETA: 398.1s

################################################################################
                     [1m Learning iteration 1193/2000 [0m

                       Computation: 17507 steps/s (collection: 0.258s, learning 0.210s)
               Value function loss: 70730.2211
                    Surrogate loss: -0.0027
             Mean action noise std: 0.96
                       Mean reward: 12651.11
               Mean episode length: 430.50
                 Mean success rate: 88.50
                  Mean reward/step: 29.36
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 0.47s
                        Total time: 588.30s
                               ETA: 397.6s

################################################################################
                     [1m Learning iteration 1194/2000 [0m

                       Computation: 18100 steps/s (collection: 0.249s, learning 0.204s)
               Value function loss: 79410.7282
                    Surrogate loss: 0.0003
             Mean action noise std: 0.96
                       Mean reward: 12547.47
               Mean episode length: 427.73
                 Mean success rate: 88.00
                  Mean reward/step: 30.08
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9789440
                    Iteration time: 0.45s
                        Total time: 588.75s
                               ETA: 397.1s

################################################################################
                     [1m Learning iteration 1195/2000 [0m

                       Computation: 17760 steps/s (collection: 0.254s, learning 0.207s)
               Value function loss: 107628.6240
                    Surrogate loss: -0.0044
             Mean action noise std: 0.96
                       Mean reward: 12257.70
               Mean episode length: 419.88
                 Mean success rate: 87.50
                  Mean reward/step: 29.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 0.46s
                        Total time: 589.21s
                               ETA: 396.6s

################################################################################
                     [1m Learning iteration 1196/2000 [0m

                       Computation: 17480 steps/s (collection: 0.255s, learning 0.214s)
               Value function loss: 127483.7131
                    Surrogate loss: -0.0009
             Mean action noise std: 0.96
                       Mean reward: 12426.34
               Mean episode length: 422.55
                 Mean success rate: 88.50
                  Mean reward/step: 29.15
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9805824
                    Iteration time: 0.47s
                        Total time: 589.68s
                               ETA: 396.1s

################################################################################
                     [1m Learning iteration 1197/2000 [0m

                       Computation: 17405 steps/s (collection: 0.268s, learning 0.203s)
               Value function loss: 114300.6681
                    Surrogate loss: 0.0007
             Mean action noise std: 0.96
                       Mean reward: 12567.06
               Mean episode length: 426.85
                 Mean success rate: 89.50
                  Mean reward/step: 28.45
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 0.47s
                        Total time: 590.15s
                               ETA: 395.6s

################################################################################
                     [1m Learning iteration 1198/2000 [0m

                       Computation: 17211 steps/s (collection: 0.255s, learning 0.221s)
               Value function loss: 103737.8867
                    Surrogate loss: -0.0031
             Mean action noise std: 0.96
                       Mean reward: 12702.25
               Mean episode length: 430.05
                 Mean success rate: 90.00
                  Mean reward/step: 29.20
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9822208
                    Iteration time: 0.48s
                        Total time: 590.63s
                               ETA: 395.1s

################################################################################
                     [1m Learning iteration 1199/2000 [0m

                       Computation: 16537 steps/s (collection: 0.268s, learning 0.228s)
               Value function loss: 89204.2369
                    Surrogate loss: -0.0015
             Mean action noise std: 0.96
                       Mean reward: 12191.83
               Mean episode length: 414.91
                 Mean success rate: 88.00
                  Mean reward/step: 29.40
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.50s
                        Total time: 591.12s
                               ETA: 394.6s

################################################################################
                     [1m Learning iteration 1200/2000 [0m

                       Computation: 16810 steps/s (collection: 0.279s, learning 0.208s)
               Value function loss: 99417.2744
                    Surrogate loss: -0.0040
             Mean action noise std: 0.96
                       Mean reward: 11914.52
               Mean episode length: 406.91
                 Mean success rate: 86.50
                  Mean reward/step: 29.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9838592
                    Iteration time: 0.49s
                        Total time: 591.61s
                               ETA: 394.1s

################################################################################
                     [1m Learning iteration 1201/2000 [0m

                       Computation: 17322 steps/s (collection: 0.264s, learning 0.209s)
               Value function loss: 114043.4108
                    Surrogate loss: -0.0007
             Mean action noise std: 0.96
                       Mean reward: 12214.17
               Mean episode length: 417.64
                 Mean success rate: 88.50
                  Mean reward/step: 29.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 0.47s
                        Total time: 592.08s
                               ETA: 393.6s

################################################################################
                     [1m Learning iteration 1202/2000 [0m

                       Computation: 16745 steps/s (collection: 0.274s, learning 0.215s)
               Value function loss: 115404.6247
                    Surrogate loss: -0.0049
             Mean action noise std: 0.97
                       Mean reward: 12225.75
               Mean episode length: 413.93
                 Mean success rate: 87.50
                  Mean reward/step: 28.90
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9854976
                    Iteration time: 0.49s
                        Total time: 592.57s
                               ETA: 393.1s

################################################################################
                     [1m Learning iteration 1203/2000 [0m

                       Computation: 17490 steps/s (collection: 0.260s, learning 0.209s)
               Value function loss: 98133.2414
                    Surrogate loss: -0.0046
             Mean action noise std: 0.97
                       Mean reward: 12039.76
               Mean episode length: 409.80
                 Mean success rate: 87.00
                  Mean reward/step: 29.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 0.47s
                        Total time: 593.04s
                               ETA: 392.6s

################################################################################
                     [1m Learning iteration 1204/2000 [0m

                       Computation: 17018 steps/s (collection: 0.266s, learning 0.216s)
               Value function loss: 144831.3504
                    Surrogate loss: 0.0033
             Mean action noise std: 0.97
                       Mean reward: 11839.68
               Mean episode length: 405.90
                 Mean success rate: 86.00
                  Mean reward/step: 29.45
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9871360
                    Iteration time: 0.48s
                        Total time: 593.52s
                               ETA: 392.1s

################################################################################
                     [1m Learning iteration 1205/2000 [0m

                       Computation: 17258 steps/s (collection: 0.272s, learning 0.202s)
               Value function loss: 66586.3953
                    Surrogate loss: 0.0087
             Mean action noise std: 0.97
                       Mean reward: 11976.63
               Mean episode length: 410.60
                 Mean success rate: 85.50
                  Mean reward/step: 27.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 0.47s
                        Total time: 594.00s
                               ETA: 391.6s

################################################################################
                     [1m Learning iteration 1206/2000 [0m

                       Computation: 17236 steps/s (collection: 0.265s, learning 0.210s)
               Value function loss: 139049.3018
                    Surrogate loss: -0.0031
             Mean action noise std: 0.97
                       Mean reward: 12133.36
               Mean episode length: 417.47
                 Mean success rate: 86.50
                  Mean reward/step: 28.78
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9887744
                    Iteration time: 0.48s
                        Total time: 594.47s
                               ETA: 391.1s

################################################################################
                     [1m Learning iteration 1207/2000 [0m

                       Computation: 17011 steps/s (collection: 0.257s, learning 0.224s)
               Value function loss: 109299.1240
                    Surrogate loss: -0.0003
             Mean action noise std: 0.97
                       Mean reward: 12158.38
               Mean episode length: 417.47
                 Mean success rate: 86.50
                  Mean reward/step: 28.59
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 0.48s
                        Total time: 594.95s
                               ETA: 390.6s

################################################################################
                     [1m Learning iteration 1208/2000 [0m

                       Computation: 16117 steps/s (collection: 0.290s, learning 0.218s)
               Value function loss: 88705.9921
                    Surrogate loss: -0.0028
             Mean action noise std: 0.97
                       Mean reward: 12318.20
               Mean episode length: 422.25
                 Mean success rate: 87.50
                  Mean reward/step: 28.83
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9904128
                    Iteration time: 0.51s
                        Total time: 595.46s
                               ETA: 390.1s

################################################################################
                     [1m Learning iteration 1209/2000 [0m

                       Computation: 16679 steps/s (collection: 0.281s, learning 0.210s)
               Value function loss: 66134.1997
                    Surrogate loss: 0.0005
             Mean action noise std: 0.97
                       Mean reward: 12288.00
               Mean episode length: 423.04
                 Mean success rate: 87.00
                  Mean reward/step: 29.62
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 0.49s
                        Total time: 595.95s
                               ETA: 389.6s

################################################################################
                     [1m Learning iteration 1210/2000 [0m

                       Computation: 17336 steps/s (collection: 0.265s, learning 0.208s)
               Value function loss: 92838.1737
                    Surrogate loss: -0.0025
             Mean action noise std: 0.97
                       Mean reward: 12589.50
               Mean episode length: 431.43
                 Mean success rate: 87.50
                  Mean reward/step: 29.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9920512
                    Iteration time: 0.47s
                        Total time: 596.43s
                               ETA: 389.1s

################################################################################
                     [1m Learning iteration 1211/2000 [0m

                       Computation: 16958 steps/s (collection: 0.261s, learning 0.222s)
               Value function loss: 90891.6244
                    Surrogate loss: -0.0025
             Mean action noise std: 0.97
                       Mean reward: 12403.34
               Mean episode length: 426.73
                 Mean success rate: 86.50
                  Mean reward/step: 29.32
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.48s
                        Total time: 596.91s
                               ETA: 388.6s

################################################################################
                     [1m Learning iteration 1212/2000 [0m

                       Computation: 16420 steps/s (collection: 0.283s, learning 0.215s)
               Value function loss: 122476.9882
                    Surrogate loss: -0.0035
             Mean action noise std: 0.96
                       Mean reward: 12678.02
               Mean episode length: 435.31
                 Mean success rate: 88.00
                  Mean reward/step: 28.31
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9936896
                    Iteration time: 0.50s
                        Total time: 597.41s
                               ETA: 388.1s

################################################################################
                     [1m Learning iteration 1213/2000 [0m

                       Computation: 16508 steps/s (collection: 0.272s, learning 0.225s)
               Value function loss: 75897.3293
                    Surrogate loss: -0.0033
             Mean action noise std: 0.96
                       Mean reward: 12469.29
               Mean episode length: 429.25
                 Mean success rate: 87.50
                  Mean reward/step: 28.97
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 0.50s
                        Total time: 597.90s
                               ETA: 387.6s

################################################################################
                     [1m Learning iteration 1214/2000 [0m

                       Computation: 16248 steps/s (collection: 0.287s, learning 0.217s)
               Value function loss: 103153.1248
                    Surrogate loss: -0.0031
             Mean action noise std: 0.96
                       Mean reward: 12594.42
               Mean episode length: 431.67
                 Mean success rate: 88.50
                  Mean reward/step: 30.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9953280
                    Iteration time: 0.50s
                        Total time: 598.41s
                               ETA: 387.1s

################################################################################
                     [1m Learning iteration 1215/2000 [0m

                       Computation: 17494 steps/s (collection: 0.256s, learning 0.212s)
               Value function loss: 90924.0867
                    Surrogate loss: -0.0042
             Mean action noise std: 0.96
                       Mean reward: 12392.61
               Mean episode length: 424.75
                 Mean success rate: 88.00
                  Mean reward/step: 29.48
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 0.47s
                        Total time: 598.88s
                               ETA: 386.6s

################################################################################
                     [1m Learning iteration 1216/2000 [0m

                       Computation: 17282 steps/s (collection: 0.261s, learning 0.213s)
               Value function loss: 84411.1209
                    Surrogate loss: -0.0025
             Mean action noise std: 0.96
                       Mean reward: 12574.66
               Mean episode length: 430.12
                 Mean success rate: 89.00
                  Mean reward/step: 29.42
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9969664
                    Iteration time: 0.47s
                        Total time: 599.35s
                               ETA: 386.1s

################################################################################
                     [1m Learning iteration 1217/2000 [0m

                       Computation: 16578 steps/s (collection: 0.279s, learning 0.216s)
               Value function loss: 124485.6243
                    Surrogate loss: -0.0022
             Mean action noise std: 0.96
                       Mean reward: 12296.58
               Mean episode length: 420.57
                 Mean success rate: 87.50
                  Mean reward/step: 29.50
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 0.49s
                        Total time: 599.84s
                               ETA: 385.6s

################################################################################
                     [1m Learning iteration 1218/2000 [0m

                       Computation: 14714 steps/s (collection: 0.294s, learning 0.262s)
               Value function loss: 116505.4863
                    Surrogate loss: -0.0034
             Mean action noise std: 0.96
                       Mean reward: 12023.26
               Mean episode length: 412.10
                 Mean success rate: 86.50
                  Mean reward/step: 29.14
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9986048
                    Iteration time: 0.56s
                        Total time: 600.40s
                               ETA: 385.2s

################################################################################
                     [1m Learning iteration 1219/2000 [0m

                       Computation: 14995 steps/s (collection: 0.287s, learning 0.259s)
               Value function loss: 91868.2277
                    Surrogate loss: 0.0066
             Mean action noise std: 0.96
                       Mean reward: 12103.70
               Mean episode length: 412.77
                 Mean success rate: 87.00
                  Mean reward/step: 28.61
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 0.55s
                        Total time: 600.95s
                               ETA: 384.7s

################################################################################
                     [1m Learning iteration 1220/2000 [0m

                       Computation: 15965 steps/s (collection: 0.296s, learning 0.217s)
               Value function loss: 124234.8946
                    Surrogate loss: 0.0058
             Mean action noise std: 0.96
                       Mean reward: 12161.38
               Mean episode length: 415.88
                 Mean success rate: 88.00
                  Mean reward/step: 28.47
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10002432
                    Iteration time: 0.51s
                        Total time: 601.46s
                               ETA: 384.2s

################################################################################
                     [1m Learning iteration 1221/2000 [0m

                       Computation: 17234 steps/s (collection: 0.269s, learning 0.206s)
               Value function loss: 96092.0385
                    Surrogate loss: -0.0022
             Mean action noise std: 0.96
                       Mean reward: 12244.99
               Mean episode length: 418.24
                 Mean success rate: 89.00
                  Mean reward/step: 28.48
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 0.48s
                        Total time: 601.94s
                               ETA: 383.7s

################################################################################
                     [1m Learning iteration 1222/2000 [0m

                       Computation: 17067 steps/s (collection: 0.265s, learning 0.215s)
               Value function loss: 122940.9593
                    Surrogate loss: -0.0016
             Mean action noise std: 0.96
                       Mean reward: 11889.11
               Mean episode length: 408.50
                 Mean success rate: 87.00
                  Mean reward/step: 28.07
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10018816
                    Iteration time: 0.48s
                        Total time: 602.42s
                               ETA: 383.2s

################################################################################
                     [1m Learning iteration 1223/2000 [0m

                       Computation: 17672 steps/s (collection: 0.264s, learning 0.200s)
               Value function loss: 96052.5667
                    Surrogate loss: -0.0022
             Mean action noise std: 0.96
                       Mean reward: 12161.82
               Mean episode length: 417.02
                 Mean success rate: 88.00
                  Mean reward/step: 28.23
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.46s
                        Total time: 602.88s
                               ETA: 382.7s

################################################################################
                     [1m Learning iteration 1224/2000 [0m

                       Computation: 17792 steps/s (collection: 0.254s, learning 0.207s)
               Value function loss: 58345.3127
                    Surrogate loss: -0.0036
             Mean action noise std: 0.96
                       Mean reward: 12125.73
               Mean episode length: 415.43
                 Mean success rate: 87.50
                  Mean reward/step: 28.87
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10035200
                    Iteration time: 0.46s
                        Total time: 603.34s
                               ETA: 382.2s

################################################################################
                     [1m Learning iteration 1225/2000 [0m

                       Computation: 18140 steps/s (collection: 0.246s, learning 0.206s)
               Value function loss: 87652.8101
                    Surrogate loss: -0.0041
             Mean action noise std: 0.96
                       Mean reward: 12220.22
               Mean episode length: 418.75
                 Mean success rate: 88.00
                  Mean reward/step: 29.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 0.45s
                        Total time: 603.79s
                               ETA: 381.7s

################################################################################
                     [1m Learning iteration 1226/2000 [0m

                       Computation: 18199 steps/s (collection: 0.249s, learning 0.202s)
               Value function loss: 84836.8074
                    Surrogate loss: -0.0044
             Mean action noise std: 0.96
                       Mean reward: 12316.59
               Mean episode length: 423.45
                 Mean success rate: 89.00
                  Mean reward/step: 28.74
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10051584
                    Iteration time: 0.45s
                        Total time: 604.24s
                               ETA: 381.2s

################################################################################
                     [1m Learning iteration 1227/2000 [0m

                       Computation: 18372 steps/s (collection: 0.238s, learning 0.208s)
               Value function loss: 131519.4791
                    Surrogate loss: -0.0041
             Mean action noise std: 0.96
                       Mean reward: 12594.85
               Mean episode length: 433.00
                 Mean success rate: 90.50
                  Mean reward/step: 28.82
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 0.45s
                        Total time: 604.69s
                               ETA: 380.6s

################################################################################
                     [1m Learning iteration 1228/2000 [0m

                       Computation: 17304 steps/s (collection: 0.265s, learning 0.208s)
               Value function loss: 81855.0674
                    Surrogate loss: -0.0030
             Mean action noise std: 0.96
                       Mean reward: 12407.38
               Mean episode length: 427.10
                 Mean success rate: 89.50
                  Mean reward/step: 27.74
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10067968
                    Iteration time: 0.47s
                        Total time: 605.16s
                               ETA: 380.1s

################################################################################
                     [1m Learning iteration 1229/2000 [0m

                       Computation: 17969 steps/s (collection: 0.254s, learning 0.202s)
               Value function loss: 60833.4634
                    Surrogate loss: -0.0036
             Mean action noise std: 0.96
                       Mean reward: 12386.82
               Mean episode length: 427.81
                 Mean success rate: 89.50
                  Mean reward/step: 29.09
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 0.46s
                        Total time: 605.62s
                               ETA: 379.6s

################################################################################
                     [1m Learning iteration 1230/2000 [0m

                       Computation: 17401 steps/s (collection: 0.265s, learning 0.206s)
               Value function loss: 93386.1056
                    Surrogate loss: 0.0029
             Mean action noise std: 0.96
                       Mean reward: 12601.95
               Mean episode length: 434.48
                 Mean success rate: 90.50
                  Mean reward/step: 29.61
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10084352
                    Iteration time: 0.47s
                        Total time: 606.09s
                               ETA: 379.1s

################################################################################
                     [1m Learning iteration 1231/2000 [0m

                       Computation: 16980 steps/s (collection: 0.271s, learning 0.212s)
               Value function loss: 110988.7025
                    Surrogate loss: 0.0012
             Mean action noise std: 0.96
                       Mean reward: 12314.24
               Mean episode length: 425.99
                 Mean success rate: 89.50
                  Mean reward/step: 28.21
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 0.48s
                        Total time: 606.57s
                               ETA: 378.6s

################################################################################
                     [1m Learning iteration 1232/2000 [0m

                       Computation: 16460 steps/s (collection: 0.273s, learning 0.224s)
               Value function loss: 61060.6505
                    Surrogate loss: 0.0033
             Mean action noise std: 0.96
                       Mean reward: 12240.20
               Mean episode length: 424.61
                 Mean success rate: 89.50
                  Mean reward/step: 29.19
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10100736
                    Iteration time: 0.50s
                        Total time: 607.07s
                               ETA: 378.1s

################################################################################
                     [1m Learning iteration 1233/2000 [0m

                       Computation: 17335 steps/s (collection: 0.253s, learning 0.220s)
               Value function loss: 107868.5785
                    Surrogate loss: 0.0020
             Mean action noise std: 0.96
                       Mean reward: 12597.43
               Mean episode length: 434.81
                 Mean success rate: 91.50
                  Mean reward/step: 29.48
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 0.47s
                        Total time: 607.54s
                               ETA: 377.6s

################################################################################
                     [1m Learning iteration 1234/2000 [0m

                       Computation: 17071 steps/s (collection: 0.270s, learning 0.210s)
               Value function loss: 91269.5189
                    Surrogate loss: -0.0011
             Mean action noise std: 0.96
                       Mean reward: 12726.98
               Mean episode length: 439.99
                 Mean success rate: 92.00
                  Mean reward/step: 28.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10117120
                    Iteration time: 0.48s
                        Total time: 608.02s
                               ETA: 377.1s

################################################################################
                     [1m Learning iteration 1235/2000 [0m

                       Computation: 16496 steps/s (collection: 0.266s, learning 0.231s)
               Value function loss: 106690.4889
                    Surrogate loss: -0.0033
             Mean action noise std: 0.96
                       Mean reward: 12749.20
               Mean episode length: 441.35
                 Mean success rate: 92.00
                  Mean reward/step: 29.08
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.50s
                        Total time: 608.52s
                               ETA: 376.6s

################################################################################
                     [1m Learning iteration 1236/2000 [0m

                       Computation: 17370 steps/s (collection: 0.259s, learning 0.213s)
               Value function loss: 83198.0864
                    Surrogate loss: -0.0036
             Mean action noise std: 0.96
                       Mean reward: 13072.69
               Mean episode length: 451.94
                 Mean success rate: 93.50
                  Mean reward/step: 28.52
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10133504
                    Iteration time: 0.47s
                        Total time: 608.99s
                               ETA: 376.1s

################################################################################
                     [1m Learning iteration 1237/2000 [0m

                       Computation: 16493 steps/s (collection: 0.275s, learning 0.222s)
               Value function loss: 130412.1579
                    Surrogate loss: -0.0032
             Mean action noise std: 0.96
                       Mean reward: 13061.38
               Mean episode length: 451.94
                 Mean success rate: 93.50
                  Mean reward/step: 29.48
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 0.50s
                        Total time: 609.49s
                               ETA: 375.6s

################################################################################
                     [1m Learning iteration 1238/2000 [0m

                       Computation: 16814 steps/s (collection: 0.258s, learning 0.229s)
               Value function loss: 86908.7578
                    Surrogate loss: -0.0053
             Mean action noise std: 0.96
                       Mean reward: 13039.04
               Mean episode length: 452.04
                 Mean success rate: 93.50
                  Mean reward/step: 28.45
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10149888
                    Iteration time: 0.49s
                        Total time: 609.97s
                               ETA: 375.1s

################################################################################
                     [1m Learning iteration 1239/2000 [0m

                       Computation: 14375 steps/s (collection: 0.305s, learning 0.265s)
               Value function loss: 114953.4199
                    Surrogate loss: -0.0033
             Mean action noise std: 0.96
                       Mean reward: 13202.85
               Mean episode length: 457.99
                 Mean success rate: 94.50
                  Mean reward/step: 29.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 0.57s
                        Total time: 610.54s
                               ETA: 374.7s

################################################################################
                     [1m Learning iteration 1240/2000 [0m

                       Computation: 15709 steps/s (collection: 0.268s, learning 0.254s)
               Value function loss: 34640.9716
                    Surrogate loss: -0.0034
             Mean action noise std: 0.96
                       Mean reward: 13314.83
               Mean episode length: 461.86
                 Mean success rate: 95.00
                  Mean reward/step: 29.30
       Mean episode length/episode: 31.39
--------------------------------------------------------------------------------
                   Total timesteps: 10166272
                    Iteration time: 0.52s
                        Total time: 611.06s
                               ETA: 374.2s

################################################################################
                     [1m Learning iteration 1241/2000 [0m

                       Computation: 16047 steps/s (collection: 0.296s, learning 0.214s)
               Value function loss: 74013.6057
                    Surrogate loss: -0.0047
             Mean action noise std: 0.96
                       Mean reward: 13346.10
               Mean episode length: 461.86
                 Mean success rate: 95.00
                  Mean reward/step: 30.36
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 0.51s
                        Total time: 611.57s
                               ETA: 373.7s

################################################################################
                     [1m Learning iteration 1242/2000 [0m

                       Computation: 15021 steps/s (collection: 0.296s, learning 0.249s)
               Value function loss: 72232.4121
                    Surrogate loss: -0.0056
             Mean action noise std: 0.96
                       Mean reward: 13574.83
               Mean episode length: 468.44
                 Mean success rate: 96.00
                  Mean reward/step: 29.90
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10182656
                    Iteration time: 0.55s
                        Total time: 612.12s
                               ETA: 373.3s

################################################################################
                     [1m Learning iteration 1243/2000 [0m

                       Computation: 16643 steps/s (collection: 0.268s, learning 0.224s)
               Value function loss: 145272.4717
                    Surrogate loss: -0.0016
             Mean action noise std: 0.96
                       Mean reward: 13992.27
               Mean episode length: 481.40
                 Mean success rate: 97.50
                  Mean reward/step: 28.85
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 0.49s
                        Total time: 612.61s
                               ETA: 372.8s

################################################################################
                     [1m Learning iteration 1244/2000 [0m

                       Computation: 17056 steps/s (collection: 0.266s, learning 0.214s)
               Value function loss: 78256.1951
                    Surrogate loss: -0.0045
             Mean action noise std: 0.96
                       Mean reward: 14121.69
               Mean episode length: 484.81
                 Mean success rate: 98.00
                  Mean reward/step: 28.65
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10199040
                    Iteration time: 0.48s
                        Total time: 613.09s
                               ETA: 372.3s

################################################################################
                     [1m Learning iteration 1245/2000 [0m

                       Computation: 17037 steps/s (collection: 0.257s, learning 0.224s)
               Value function loss: 67041.7865
                    Surrogate loss: -0.0027
             Mean action noise std: 0.96
                       Mean reward: 13968.83
               Mean episode length: 481.06
                 Mean success rate: 97.50
                  Mean reward/step: 29.57
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 0.48s
                        Total time: 613.57s
                               ETA: 371.8s

################################################################################
                     [1m Learning iteration 1246/2000 [0m

                       Computation: 15856 steps/s (collection: 0.277s, learning 0.240s)
               Value function loss: 114015.3807
                    Surrogate loss: -0.0017
             Mean action noise std: 0.96
                       Mean reward: 14076.92
               Mean episode length: 484.13
                 Mean success rate: 98.00
                  Mean reward/step: 29.72
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10215424
                    Iteration time: 0.52s
                        Total time: 614.09s
                               ETA: 371.3s

################################################################################
                     [1m Learning iteration 1247/2000 [0m

                       Computation: 14550 steps/s (collection: 0.295s, learning 0.268s)
               Value function loss: 109994.2672
                    Surrogate loss: 0.0040
             Mean action noise std: 0.96
                       Mean reward: 13993.86
               Mean episode length: 480.69
                 Mean success rate: 97.50
                  Mean reward/step: 29.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.56s
                        Total time: 614.65s
                               ETA: 370.9s

################################################################################
                     [1m Learning iteration 1248/2000 [0m

                       Computation: 14359 steps/s (collection: 0.314s, learning 0.256s)
               Value function loss: 100076.2756
                    Surrogate loss: -0.0026
             Mean action noise std: 0.96
                       Mean reward: 13489.01
               Mean episode length: 463.53
                 Mean success rate: 94.50
                  Mean reward/step: 29.21
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10231808
                    Iteration time: 0.57s
                        Total time: 615.22s
                               ETA: 370.4s

################################################################################
                     [1m Learning iteration 1249/2000 [0m

                       Computation: 14990 steps/s (collection: 0.313s, learning 0.233s)
               Value function loss: 118739.9551
                    Surrogate loss: -0.0036
             Mean action noise std: 0.96
                       Mean reward: 13380.04
               Mean episode length: 459.54
                 Mean success rate: 94.00
                  Mean reward/step: 29.06
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 0.55s
                        Total time: 615.77s
                               ETA: 370.0s

################################################################################
                     [1m Learning iteration 1250/2000 [0m

                       Computation: 15087 steps/s (collection: 0.303s, learning 0.240s)
               Value function loss: 60492.7916
                    Surrogate loss: -0.0033
             Mean action noise std: 0.96
                       Mean reward: 13395.65
               Mean episode length: 460.70
                 Mean success rate: 94.00
                  Mean reward/step: 28.51
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10248192
                    Iteration time: 0.54s
                        Total time: 616.31s
                               ETA: 369.5s

################################################################################
                     [1m Learning iteration 1251/2000 [0m

                       Computation: 16750 steps/s (collection: 0.271s, learning 0.218s)
               Value function loss: 128276.5804
                    Surrogate loss: -0.0041
             Mean action noise std: 0.96
                       Mean reward: 13370.05
               Mean episode length: 457.96
                 Mean success rate: 93.50
                  Mean reward/step: 28.48
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 0.49s
                        Total time: 616.80s
                               ETA: 369.0s

################################################################################
                     [1m Learning iteration 1252/2000 [0m

                       Computation: 17253 steps/s (collection: 0.256s, learning 0.219s)
               Value function loss: 79258.2944
                    Surrogate loss: -0.0021
             Mean action noise std: 0.96
                       Mean reward: 13199.97
               Mean episode length: 453.11
                 Mean success rate: 93.00
                  Mean reward/step: 27.95
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10264576
                    Iteration time: 0.47s
                        Total time: 617.28s
                               ETA: 368.5s

################################################################################
                     [1m Learning iteration 1253/2000 [0m

                       Computation: 17615 steps/s (collection: 0.257s, learning 0.208s)
               Value function loss: 129784.4410
                    Surrogate loss: -0.0043
             Mean action noise std: 0.96
                       Mean reward: 12920.29
               Mean episode length: 444.77
                 Mean success rate: 92.00
                  Mean reward/step: 28.68
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 0.47s
                        Total time: 617.74s
                               ETA: 368.0s

################################################################################
                     [1m Learning iteration 1254/2000 [0m

                       Computation: 17113 steps/s (collection: 0.263s, learning 0.216s)
               Value function loss: 92423.6449
                    Surrogate loss: -0.0038
             Mean action noise std: 0.96
                       Mean reward: 12868.00
               Mean episode length: 443.42
                 Mean success rate: 92.00
                  Mean reward/step: 28.33
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10280960
                    Iteration time: 0.48s
                        Total time: 618.22s
                               ETA: 367.5s

################################################################################
                     [1m Learning iteration 1255/2000 [0m

                       Computation: 16478 steps/s (collection: 0.272s, learning 0.225s)
               Value function loss: 82114.3316
                    Surrogate loss: -0.0032
             Mean action noise std: 0.96
                       Mean reward: 12826.69
               Mean episode length: 440.96
                 Mean success rate: 91.00
                  Mean reward/step: 28.45
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 0.50s
                        Total time: 618.72s
                               ETA: 367.0s

################################################################################
                     [1m Learning iteration 1256/2000 [0m

                       Computation: 17197 steps/s (collection: 0.266s, learning 0.211s)
               Value function loss: 68897.2346
                    Surrogate loss: -0.0045
             Mean action noise std: 0.96
                       Mean reward: 12835.10
               Mean episode length: 440.96
                 Mean success rate: 91.00
                  Mean reward/step: 28.27
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 10297344
                    Iteration time: 0.48s
                        Total time: 619.19s
                               ETA: 366.5s

################################################################################
                     [1m Learning iteration 1257/2000 [0m

                       Computation: 17205 steps/s (collection: 0.267s, learning 0.209s)
               Value function loss: 85281.8173
                    Surrogate loss: -0.0028
             Mean action noise std: 0.96
                       Mean reward: 12365.92
               Mean episode length: 427.11
                 Mean success rate: 88.50
                  Mean reward/step: 28.54
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 0.48s
                        Total time: 619.67s
                               ETA: 366.0s

################################################################################
                     [1m Learning iteration 1258/2000 [0m

                       Computation: 17044 steps/s (collection: 0.257s, learning 0.224s)
               Value function loss: 122304.3217
                    Surrogate loss: -0.0034
             Mean action noise std: 0.96
                       Mean reward: 12501.42
               Mean episode length: 433.85
                 Mean success rate: 90.00
                  Mean reward/step: 28.22
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10313728
                    Iteration time: 0.48s
                        Total time: 620.15s
                               ETA: 365.5s

################################################################################
                     [1m Learning iteration 1259/2000 [0m

                       Computation: 17399 steps/s (collection: 0.266s, learning 0.205s)
               Value function loss: 102174.4225
                    Surrogate loss: 0.0025
             Mean action noise std: 0.96
                       Mean reward: 12353.48
               Mean episode length: 430.10
                 Mean success rate: 89.00
                  Mean reward/step: 26.98
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.47s
                        Total time: 620.62s
                               ETA: 365.0s

################################################################################
                     [1m Learning iteration 1260/2000 [0m

                       Computation: 17791 steps/s (collection: 0.253s, learning 0.207s)
               Value function loss: 75938.5283
                    Surrogate loss: -0.0048
             Mean action noise std: 0.96
                       Mean reward: 12173.62
               Mean episode length: 425.40
                 Mean success rate: 88.00
                  Mean reward/step: 27.65
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10330112
                    Iteration time: 0.46s
                        Total time: 621.08s
                               ETA: 364.5s

################################################################################
                     [1m Learning iteration 1261/2000 [0m

                       Computation: 17291 steps/s (collection: 0.266s, learning 0.208s)
               Value function loss: 91779.6744
                    Surrogate loss: -0.0047
             Mean action noise std: 0.96
                       Mean reward: 12185.85
               Mean episode length: 427.26
                 Mean success rate: 88.50
                  Mean reward/step: 28.63
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 0.47s
                        Total time: 621.56s
                               ETA: 364.0s

################################################################################
                     [1m Learning iteration 1262/2000 [0m

                       Computation: 17353 steps/s (collection: 0.263s, learning 0.209s)
               Value function loss: 82037.6857
                    Surrogate loss: -0.0047
             Mean action noise std: 0.97
                       Mean reward: 11986.02
               Mean episode length: 421.61
                 Mean success rate: 87.50
                  Mean reward/step: 28.24
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10346496
                    Iteration time: 0.47s
                        Total time: 622.03s
                               ETA: 363.5s

################################################################################
                     [1m Learning iteration 1263/2000 [0m

                       Computation: 17027 steps/s (collection: 0.272s, learning 0.209s)
               Value function loss: 87496.3635
                    Surrogate loss: -0.0036
             Mean action noise std: 0.97
                       Mean reward: 11885.73
               Mean episode length: 418.58
                 Mean success rate: 86.50
                  Mean reward/step: 27.78
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 0.48s
                        Total time: 622.51s
                               ETA: 363.0s

################################################################################
                     [1m Learning iteration 1264/2000 [0m

                       Computation: 16304 steps/s (collection: 0.278s, learning 0.225s)
               Value function loss: 87053.0790
                    Surrogate loss: -0.0043
             Mean action noise std: 0.97
                       Mean reward: 12029.31
               Mean episode length: 424.42
                 Mean success rate: 87.00
                  Mean reward/step: 27.16
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10362880
                    Iteration time: 0.50s
                        Total time: 623.01s
                               ETA: 362.5s

################################################################################
                     [1m Learning iteration 1265/2000 [0m

                       Computation: 17082 steps/s (collection: 0.274s, learning 0.206s)
               Value function loss: 122931.6445
                    Surrogate loss: -0.0037
             Mean action noise std: 0.97
                       Mean reward: 11998.23
               Mean episode length: 424.59
                 Mean success rate: 87.00
                  Mean reward/step: 26.87
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 0.48s
                        Total time: 623.49s
                               ETA: 362.0s

################################################################################
                     [1m Learning iteration 1266/2000 [0m

                       Computation: 16767 steps/s (collection: 0.282s, learning 0.207s)
               Value function loss: 98172.3014
                    Surrogate loss: -0.0042
             Mean action noise std: 0.97
                       Mean reward: 11756.18
               Mean episode length: 418.00
                 Mean success rate: 86.50
                  Mean reward/step: 27.03
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10379264
                    Iteration time: 0.49s
                        Total time: 623.98s
                               ETA: 361.5s

################################################################################
                     [1m Learning iteration 1267/2000 [0m

                       Computation: 16123 steps/s (collection: 0.286s, learning 0.222s)
               Value function loss: 90146.3884
                    Surrogate loss: -0.0035
             Mean action noise std: 0.97
                       Mean reward: 11874.97
               Mean episode length: 421.71
                 Mean success rate: 87.50
                  Mean reward/step: 27.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 0.51s
                        Total time: 624.49s
                               ETA: 361.0s

################################################################################
                     [1m Learning iteration 1268/2000 [0m

                       Computation: 14730 steps/s (collection: 0.299s, learning 0.257s)
               Value function loss: 102857.2541
                    Surrogate loss: -0.0043
             Mean action noise std: 0.97
                       Mean reward: 11944.91
               Mean episode length: 424.65
                 Mean success rate: 88.00
                  Mean reward/step: 27.83
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10395648
                    Iteration time: 0.56s
                        Total time: 625.04s
                               ETA: 360.5s

################################################################################
                     [1m Learning iteration 1269/2000 [0m

                       Computation: 16010 steps/s (collection: 0.294s, learning 0.217s)
               Value function loss: 85376.9098
                    Surrogate loss: -0.0042
             Mean action noise std: 0.97
                       Mean reward: 11889.62
               Mean episode length: 423.35
                 Mean success rate: 88.00
                  Mean reward/step: 27.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 0.51s
                        Total time: 625.55s
                               ETA: 360.1s

################################################################################
                     [1m Learning iteration 1270/2000 [0m

                       Computation: 15175 steps/s (collection: 0.303s, learning 0.237s)
               Value function loss: 97361.1305
                    Surrogate loss: -0.0033
             Mean action noise std: 0.97
                       Mean reward: 11866.00
               Mean episode length: 424.24
                 Mean success rate: 88.50
                  Mean reward/step: 27.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10412032
                    Iteration time: 0.54s
                        Total time: 626.09s
                               ETA: 359.6s

################################################################################
                     [1m Learning iteration 1271/2000 [0m

                       Computation: 14188 steps/s (collection: 0.307s, learning 0.271s)
               Value function loss: 67475.0197
                    Surrogate loss: -0.0045
             Mean action noise std: 0.97
                       Mean reward: 11920.80
               Mean episode length: 425.47
                 Mean success rate: 88.50
                  Mean reward/step: 28.21
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.58s
                        Total time: 626.67s
                               ETA: 359.2s

################################################################################
                     [1m Learning iteration 1272/2000 [0m

                       Computation: 14102 steps/s (collection: 0.331s, learning 0.250s)
               Value function loss: 100361.8840
                    Surrogate loss: -0.0052
             Mean action noise std: 0.97
                       Mean reward: 11795.44
               Mean episode length: 422.98
                 Mean success rate: 88.00
                  Mean reward/step: 28.87
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10428416
                    Iteration time: 0.58s
                        Total time: 627.25s
                               ETA: 358.7s

################################################################################
                     [1m Learning iteration 1273/2000 [0m

                       Computation: 15336 steps/s (collection: 0.290s, learning 0.244s)
               Value function loss: 66951.5100
                    Surrogate loss: -0.0047
             Mean action noise std: 0.97
                       Mean reward: 11683.30
               Mean episode length: 419.65
                 Mean success rate: 87.50
                  Mean reward/step: 28.74
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 0.53s
                        Total time: 627.79s
                               ETA: 358.2s

################################################################################
                     [1m Learning iteration 1274/2000 [0m

                       Computation: 15012 steps/s (collection: 0.318s, learning 0.228s)
               Value function loss: 148732.0279
                    Surrogate loss: -0.0040
             Mean action noise std: 0.97
                       Mean reward: 11463.35
               Mean episode length: 412.33
                 Mean success rate: 86.00
                  Mean reward/step: 28.87
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10444800
                    Iteration time: 0.55s
                        Total time: 628.33s
                               ETA: 357.8s

################################################################################
                     [1m Learning iteration 1275/2000 [0m

                       Computation: 15801 steps/s (collection: 0.293s, learning 0.225s)
               Value function loss: 86621.1776
                    Surrogate loss: 0.0015
             Mean action noise std: 0.97
                       Mean reward: 11579.89
               Mean episode length: 417.38
                 Mean success rate: 87.00
                  Mean reward/step: 28.45
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 0.52s
                        Total time: 628.85s
                               ETA: 357.3s

################################################################################
                     [1m Learning iteration 1276/2000 [0m

                       Computation: 17292 steps/s (collection: 0.266s, learning 0.207s)
               Value function loss: 82039.3946
                    Surrogate loss: 0.0117
             Mean action noise std: 0.97
                       Mean reward: 11838.98
               Mean episode length: 425.29
                 Mean success rate: 88.00
                  Mean reward/step: 28.90
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10461184
                    Iteration time: 0.47s
                        Total time: 629.32s
                               ETA: 356.8s

################################################################################
                     [1m Learning iteration 1277/2000 [0m

                       Computation: 16752 steps/s (collection: 0.263s, learning 0.226s)
               Value function loss: 88351.2119
                    Surrogate loss: -0.0026
             Mean action noise std: 0.97
                       Mean reward: 11894.86
               Mean episode length: 427.86
                 Mean success rate: 88.50
                  Mean reward/step: 29.07
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 0.49s
                        Total time: 629.81s
                               ETA: 356.3s

################################################################################
                     [1m Learning iteration 1278/2000 [0m

                       Computation: 17397 steps/s (collection: 0.263s, learning 0.208s)
               Value function loss: 112749.3100
                    Surrogate loss: -0.0052
             Mean action noise std: 0.97
                       Mean reward: 11951.82
               Mean episode length: 429.08
                 Mean success rate: 88.00
                  Mean reward/step: 28.86
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10477568
                    Iteration time: 0.47s
                        Total time: 630.28s
                               ETA: 355.8s

################################################################################
                     [1m Learning iteration 1279/2000 [0m

                       Computation: 18628 steps/s (collection: 0.236s, learning 0.204s)
               Value function loss: 87761.7246
                    Surrogate loss: -0.0059
             Mean action noise std: 0.97
                       Mean reward: 11748.24
               Mean episode length: 422.29
                 Mean success rate: 86.50
                  Mean reward/step: 28.52
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 0.44s
                        Total time: 630.72s
                               ETA: 355.3s

################################################################################
                     [1m Learning iteration 1280/2000 [0m

                       Computation: 17483 steps/s (collection: 0.261s, learning 0.208s)
               Value function loss: 136367.3881
                    Surrogate loss: -0.0039
             Mean action noise std: 0.97
                       Mean reward: 12110.45
               Mean episode length: 433.83
                 Mean success rate: 88.50
                  Mean reward/step: 28.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10493952
                    Iteration time: 0.47s
                        Total time: 631.19s
                               ETA: 354.8s

################################################################################
                     [1m Learning iteration 1281/2000 [0m

                       Computation: 17770 steps/s (collection: 0.258s, learning 0.203s)
               Value function loss: 93074.5146
                    Surrogate loss: -0.0040
             Mean action noise std: 0.97
                       Mean reward: 12085.53
               Mean episode length: 429.60
                 Mean success rate: 87.50
                  Mean reward/step: 28.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 0.46s
                        Total time: 631.65s
                               ETA: 354.3s

################################################################################
                     [1m Learning iteration 1282/2000 [0m

                       Computation: 17531 steps/s (collection: 0.250s, learning 0.217s)
               Value function loss: 128720.5271
                    Surrogate loss: -0.0038
             Mean action noise std: 0.97
                       Mean reward: 12307.91
               Mean episode length: 435.74
                 Mean success rate: 88.50
                  Mean reward/step: 28.64
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10510336
                    Iteration time: 0.47s
                        Total time: 632.12s
                               ETA: 353.8s

################################################################################
                     [1m Learning iteration 1283/2000 [0m

                       Computation: 17680 steps/s (collection: 0.254s, learning 0.209s)
               Value function loss: 72027.9110
                    Surrogate loss: -0.0049
             Mean action noise std: 0.97
                       Mean reward: 12470.14
               Mean episode length: 441.19
                 Mean success rate: 89.50
                  Mean reward/step: 28.57
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.46s
                        Total time: 632.58s
                               ETA: 353.2s

################################################################################
                     [1m Learning iteration 1284/2000 [0m

                       Computation: 18148 steps/s (collection: 0.250s, learning 0.202s)
               Value function loss: 101633.3114
                    Surrogate loss: -0.0056
             Mean action noise std: 0.97
                       Mean reward: 12694.27
               Mean episode length: 446.80
                 Mean success rate: 90.50
                  Mean reward/step: 29.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10526720
                    Iteration time: 0.45s
                        Total time: 633.04s
                               ETA: 352.7s

################################################################################
                     [1m Learning iteration 1285/2000 [0m

                       Computation: 18035 steps/s (collection: 0.251s, learning 0.203s)
               Value function loss: 73583.5381
                    Surrogate loss: -0.0031
             Mean action noise std: 0.97
                       Mean reward: 12969.15
               Mean episode length: 455.04
                 Mean success rate: 92.00
                  Mean reward/step: 29.20
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 0.45s
                        Total time: 633.49s
                               ETA: 352.2s

################################################################################
                     [1m Learning iteration 1286/2000 [0m

                       Computation: 17446 steps/s (collection: 0.259s, learning 0.211s)
               Value function loss: 95245.3573
                    Surrogate loss: -0.0039
             Mean action noise std: 0.97
                       Mean reward: 13148.27
               Mean episode length: 458.94
                 Mean success rate: 92.50
                  Mean reward/step: 29.44
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10543104
                    Iteration time: 0.47s
                        Total time: 633.96s
                               ETA: 351.7s

################################################################################
                     [1m Learning iteration 1287/2000 [0m

                       Computation: 17181 steps/s (collection: 0.264s, learning 0.212s)
               Value function loss: 50339.7398
                    Surrogate loss: 0.0005
             Mean action noise std: 0.97
                       Mean reward: 13012.89
               Mean episode length: 454.70
                 Mean success rate: 91.50
                  Mean reward/step: 30.19
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 0.48s
                        Total time: 634.44s
                               ETA: 351.2s

################################################################################
                     [1m Learning iteration 1288/2000 [0m

                       Computation: 17498 steps/s (collection: 0.270s, learning 0.198s)
               Value function loss: 104171.8240
                    Surrogate loss: -0.0038
             Mean action noise std: 0.97
                       Mean reward: 13053.55
               Mean episode length: 455.33
                 Mean success rate: 91.50
                  Mean reward/step: 29.95
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10559488
                    Iteration time: 0.47s
                        Total time: 634.90s
                               ETA: 350.7s

################################################################################
                     [1m Learning iteration 1289/2000 [0m

                       Computation: 17026 steps/s (collection: 0.275s, learning 0.206s)
               Value function loss: 77241.0303
                    Surrogate loss: -0.0038
             Mean action noise std: 0.97
                       Mean reward: 13242.27
               Mean episode length: 459.33
                 Mean success rate: 92.50
                  Mean reward/step: 29.06
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 0.48s
                        Total time: 635.39s
                               ETA: 350.2s

################################################################################
                     [1m Learning iteration 1290/2000 [0m

                       Computation: 17544 steps/s (collection: 0.265s, learning 0.202s)
               Value function loss: 138568.0581
                    Surrogate loss: -0.0027
             Mean action noise std: 0.97
                       Mean reward: 13774.95
               Mean episode length: 473.48
                 Mean success rate: 95.00
                  Mean reward/step: 28.78
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10575872
                    Iteration time: 0.47s
                        Total time: 635.85s
                               ETA: 349.7s

################################################################################
                     [1m Learning iteration 1291/2000 [0m

                       Computation: 17410 steps/s (collection: 0.269s, learning 0.201s)
               Value function loss: 90623.0508
                    Surrogate loss: -0.0041
             Mean action noise std: 0.97
                       Mean reward: 13816.44
               Mean episode length: 473.48
                 Mean success rate: 95.00
                  Mean reward/step: 28.87
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 0.47s
                        Total time: 636.32s
                               ETA: 349.2s

################################################################################
                     [1m Learning iteration 1292/2000 [0m

                       Computation: 15321 steps/s (collection: 0.309s, learning 0.226s)
               Value function loss: 67820.2317
                    Surrogate loss: -0.0046
             Mean action noise std: 0.97
                       Mean reward: 13731.12
               Mean episode length: 470.75
                 Mean success rate: 94.50
                  Mean reward/step: 29.58
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10592256
                    Iteration time: 0.53s
                        Total time: 636.86s
                               ETA: 348.7s

################################################################################
                     [1m Learning iteration 1293/2000 [0m

                       Computation: 16649 steps/s (collection: 0.270s, learning 0.222s)
               Value function loss: 65621.3935
                    Surrogate loss: -0.0025
             Mean action noise std: 0.97
                       Mean reward: 13707.37
               Mean episode length: 470.83
                 Mean success rate: 94.50
                  Mean reward/step: 29.95
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 0.49s
                        Total time: 637.35s
                               ETA: 348.2s

################################################################################
                     [1m Learning iteration 1294/2000 [0m

                       Computation: 16059 steps/s (collection: 0.293s, learning 0.217s)
               Value function loss: 112892.1496
                    Surrogate loss: -0.0028
             Mean action noise std: 0.97
                       Mean reward: 13722.54
               Mean episode length: 472.41
                 Mean success rate: 94.50
                  Mean reward/step: 30.07
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10608640
                    Iteration time: 0.51s
                        Total time: 637.86s
                               ETA: 347.7s

################################################################################
                     [1m Learning iteration 1295/2000 [0m

                       Computation: 17219 steps/s (collection: 0.267s, learning 0.209s)
               Value function loss: 112371.0455
                    Surrogate loss: -0.0004
             Mean action noise std: 0.97
                       Mean reward: 13761.13
               Mean episode length: 472.41
                 Mean success rate: 94.50
                  Mean reward/step: 29.29
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.48s
                        Total time: 638.34s
                               ETA: 347.2s

################################################################################
                     [1m Learning iteration 1296/2000 [0m

                       Computation: 17475 steps/s (collection: 0.261s, learning 0.208s)
               Value function loss: 138926.0014
                    Surrogate loss: 0.0006
             Mean action noise std: 0.97
                       Mean reward: 13602.50
               Mean episode length: 465.84
                 Mean success rate: 93.00
                  Mean reward/step: 29.02
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10625024
                    Iteration time: 0.47s
                        Total time: 638.80s
                               ETA: 346.7s

################################################################################
                     [1m Learning iteration 1297/2000 [0m

                       Computation: 17316 steps/s (collection: 0.267s, learning 0.206s)
               Value function loss: 88662.3346
                    Surrogate loss: -0.0052
             Mean action noise std: 0.97
                       Mean reward: 13333.13
               Mean episode length: 456.61
                 Mean success rate: 91.00
                  Mean reward/step: 28.87
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 0.47s
                        Total time: 639.28s
                               ETA: 346.2s

################################################################################
                     [1m Learning iteration 1298/2000 [0m

                       Computation: 16520 steps/s (collection: 0.284s, learning 0.212s)
               Value function loss: 124930.4111
                    Surrogate loss: -0.0027
             Mean action noise std: 0.97
                       Mean reward: 13456.84
               Mean episode length: 460.85
                 Mean success rate: 92.00
                  Mean reward/step: 29.22
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10641408
                    Iteration time: 0.50s
                        Total time: 639.77s
                               ETA: 345.7s

################################################################################
                     [1m Learning iteration 1299/2000 [0m

                       Computation: 16052 steps/s (collection: 0.295s, learning 0.215s)
               Value function loss: 76701.6000
                    Surrogate loss: -0.0029
             Mean action noise std: 0.97
                       Mean reward: 13533.45
               Mean episode length: 464.38
                 Mean success rate: 92.50
                  Mean reward/step: 29.14
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 0.51s
                        Total time: 640.28s
                               ETA: 345.3s

################################################################################
                     [1m Learning iteration 1300/2000 [0m

                       Computation: 16259 steps/s (collection: 0.284s, learning 0.219s)
               Value function loss: 113342.2944
                    Surrogate loss: -0.0036
             Mean action noise std: 0.97
                       Mean reward: 13608.28
               Mean episode length: 467.06
                 Mean success rate: 93.00
                  Mean reward/step: 29.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10657792
                    Iteration time: 0.50s
                        Total time: 640.79s
                               ETA: 344.8s

################################################################################
                     [1m Learning iteration 1301/2000 [0m

                       Computation: 17350 steps/s (collection: 0.265s, learning 0.207s)
               Value function loss: 103372.7771
                    Surrogate loss: -0.0040
             Mean action noise std: 0.97
                       Mean reward: 13437.84
               Mean episode length: 462.50
                 Mean success rate: 92.50
                  Mean reward/step: 29.33
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 0.47s
                        Total time: 641.26s
                               ETA: 344.3s

################################################################################
                     [1m Learning iteration 1302/2000 [0m

                       Computation: 17628 steps/s (collection: 0.256s, learning 0.209s)
               Value function loss: 74314.7595
                    Surrogate loss: -0.0048
             Mean action noise std: 0.97
                       Mean reward: 13341.07
               Mean episode length: 458.04
                 Mean success rate: 91.50
                  Mean reward/step: 29.20
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10674176
                    Iteration time: 0.46s
                        Total time: 641.72s
                               ETA: 343.8s

################################################################################
                     [1m Learning iteration 1303/2000 [0m

                       Computation: 17766 steps/s (collection: 0.259s, learning 0.202s)
               Value function loss: 54672.3113
                    Surrogate loss: -0.0014
             Mean action noise std: 0.97
                       Mean reward: 13231.94
               Mean episode length: 453.66
                 Mean success rate: 91.00
                  Mean reward/step: 30.33
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 0.46s
                        Total time: 642.19s
                               ETA: 343.3s

################################################################################
                     [1m Learning iteration 1304/2000 [0m

                       Computation: 17410 steps/s (collection: 0.247s, learning 0.224s)
               Value function loss: 91210.6569
                    Surrogate loss: -0.0040
             Mean action noise std: 0.97
                       Mean reward: 13516.92
               Mean episode length: 461.54
                 Mean success rate: 92.50
                  Mean reward/step: 29.07
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10690560
                    Iteration time: 0.47s
                        Total time: 642.66s
                               ETA: 342.7s

################################################################################
                     [1m Learning iteration 1305/2000 [0m

                       Computation: 15160 steps/s (collection: 0.284s, learning 0.257s)
               Value function loss: 139575.0258
                    Surrogate loss: -0.0040
             Mean action noise std: 0.97
                       Mean reward: 13521.17
               Mean episode length: 461.39
                 Mean success rate: 92.50
                  Mean reward/step: 28.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 0.54s
                        Total time: 643.20s
                               ETA: 342.3s

################################################################################
                     [1m Learning iteration 1306/2000 [0m

                       Computation: 16164 steps/s (collection: 0.293s, learning 0.214s)
               Value function loss: 89334.9500
                    Surrogate loss: -0.0044
             Mean action noise std: 0.97
                       Mean reward: 13373.62
               Mean episode length: 456.84
                 Mean success rate: 92.00
                  Mean reward/step: 27.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10706944
                    Iteration time: 0.51s
                        Total time: 643.70s
                               ETA: 341.8s

################################################################################
                     [1m Learning iteration 1307/2000 [0m

                       Computation: 15372 steps/s (collection: 0.279s, learning 0.254s)
               Value function loss: 72689.3580
                    Surrogate loss: -0.0053
             Mean action noise std: 0.97
                       Mean reward: 13548.29
               Mean episode length: 463.72
                 Mean success rate: 93.50
                  Mean reward/step: 27.89
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.53s
                        Total time: 644.24s
                               ETA: 341.3s

################################################################################
                     [1m Learning iteration 1308/2000 [0m

                       Computation: 15018 steps/s (collection: 0.285s, learning 0.260s)
               Value function loss: 85977.5807
                    Surrogate loss: -0.0045
             Mean action noise std: 0.97
                       Mean reward: 13596.14
               Mean episode length: 465.39
                 Mean success rate: 94.00
                  Mean reward/step: 29.23
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10723328
                    Iteration time: 0.55s
                        Total time: 644.78s
                               ETA: 340.9s

################################################################################
                     [1m Learning iteration 1309/2000 [0m

                       Computation: 16454 steps/s (collection: 0.285s, learning 0.213s)
               Value function loss: 103855.6232
                    Surrogate loss: -0.0052
             Mean action noise std: 0.98
                       Mean reward: 13688.09
               Mean episode length: 469.73
                 Mean success rate: 95.00
                  Mean reward/step: 29.36
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 0.50s
                        Total time: 645.28s
                               ETA: 340.4s

################################################################################
                     [1m Learning iteration 1310/2000 [0m

                       Computation: 16635 steps/s (collection: 0.279s, learning 0.213s)
               Value function loss: 104987.9096
                    Surrogate loss: -0.0050
             Mean action noise std: 0.98
                       Mean reward: 13602.23
               Mean episode length: 465.52
                 Mean success rate: 94.50
                  Mean reward/step: 28.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10739712
                    Iteration time: 0.49s
                        Total time: 645.77s
                               ETA: 339.9s

################################################################################
                     [1m Learning iteration 1311/2000 [0m

                       Computation: 17044 steps/s (collection: 0.274s, learning 0.206s)
               Value function loss: 107770.9053
                    Surrogate loss: -0.0015
             Mean action noise std: 0.98
                       Mean reward: 13249.87
               Mean episode length: 453.76
                 Mean success rate: 92.00
                  Mean reward/step: 28.61
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 0.48s
                        Total time: 646.25s
                               ETA: 339.4s

################################################################################
                     [1m Learning iteration 1312/2000 [0m

                       Computation: 16850 steps/s (collection: 0.280s, learning 0.206s)
               Value function loss: 120874.9672
                    Surrogate loss: -0.0045
             Mean action noise std: 0.97
                       Mean reward: 13198.16
               Mean episode length: 452.38
                 Mean success rate: 91.50
                  Mean reward/step: 28.01
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10756096
                    Iteration time: 0.49s
                        Total time: 646.74s
                               ETA: 338.9s

################################################################################
                     [1m Learning iteration 1313/2000 [0m

                       Computation: 17149 steps/s (collection: 0.270s, learning 0.208s)
               Value function loss: 135351.7879
                    Surrogate loss: -0.0040
             Mean action noise std: 0.97
                       Mean reward: 13410.49
               Mean episode length: 459.86
                 Mean success rate: 93.00
                  Mean reward/step: 28.34
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 0.48s
                        Total time: 647.22s
                               ETA: 338.4s

################################################################################
                     [1m Learning iteration 1314/2000 [0m

                       Computation: 15691 steps/s (collection: 0.299s, learning 0.223s)
               Value function loss: 73641.2387
                    Surrogate loss: -0.0039
             Mean action noise std: 0.97
                       Mean reward: 13364.49
               Mean episode length: 459.63
                 Mean success rate: 93.00
                  Mean reward/step: 28.83
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10772480
                    Iteration time: 0.52s
                        Total time: 647.74s
                               ETA: 337.9s

################################################################################
                     [1m Learning iteration 1315/2000 [0m

                       Computation: 15969 steps/s (collection: 0.280s, learning 0.233s)
               Value function loss: 84466.1457
                    Surrogate loss: -0.0006
             Mean action noise std: 0.97
                       Mean reward: 13350.89
               Mean episode length: 459.63
                 Mean success rate: 93.00
                  Mean reward/step: 29.76
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 0.51s
                        Total time: 648.25s
                               ETA: 337.4s

################################################################################
                     [1m Learning iteration 1316/2000 [0m

                       Computation: 14754 steps/s (collection: 0.315s, learning 0.240s)
               Value function loss: 75550.6804
                    Surrogate loss: -0.0031
             Mean action noise std: 0.97
                       Mean reward: 13512.50
               Mean episode length: 463.73
                 Mean success rate: 94.00
                  Mean reward/step: 29.30
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10788864
                    Iteration time: 0.56s
                        Total time: 648.81s
                               ETA: 337.0s

################################################################################
                     [1m Learning iteration 1317/2000 [0m

                       Computation: 15183 steps/s (collection: 0.303s, learning 0.236s)
               Value function loss: 107555.9595
                    Surrogate loss: -0.0041
             Mean action noise std: 0.98
                       Mean reward: 13727.81
               Mean episode length: 470.49
                 Mean success rate: 95.00
                  Mean reward/step: 29.50
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 0.54s
                        Total time: 649.35s
                               ETA: 336.5s

################################################################################
                     [1m Learning iteration 1318/2000 [0m

                       Computation: 14479 steps/s (collection: 0.308s, learning 0.258s)
               Value function loss: 49224.0235
                    Surrogate loss: -0.0035
             Mean action noise std: 0.98
                       Mean reward: 13683.11
               Mean episode length: 470.49
                 Mean success rate: 95.00
                  Mean reward/step: 29.29
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10805248
                    Iteration time: 0.57s
                        Total time: 649.91s
                               ETA: 336.0s

################################################################################
                     [1m Learning iteration 1319/2000 [0m

                       Computation: 14988 steps/s (collection: 0.310s, learning 0.237s)
               Value function loss: 96982.4713
                    Surrogate loss: -0.0044
             Mean action noise std: 0.98
                       Mean reward: 13575.71
               Mean episode length: 466.94
                 Mean success rate: 94.50
                  Mean reward/step: 30.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.55s
                        Total time: 650.46s
                               ETA: 335.6s

################################################################################
                     [1m Learning iteration 1320/2000 [0m

                       Computation: 15120 steps/s (collection: 0.282s, learning 0.260s)
               Value function loss: 83574.1854
                    Surrogate loss: -0.0052
             Mean action noise std: 0.98
                       Mean reward: 13646.66
               Mean episode length: 470.16
                 Mean success rate: 95.00
                  Mean reward/step: 29.14
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10821632
                    Iteration time: 0.54s
                        Total time: 651.00s
                               ETA: 335.1s

################################################################################
                     [1m Learning iteration 1321/2000 [0m

                       Computation: 13914 steps/s (collection: 0.310s, learning 0.279s)
               Value function loss: 169760.9160
                    Surrogate loss: -0.0028
             Mean action noise std: 0.98
                       Mean reward: 13753.19
               Mean episode length: 474.15
                 Mean success rate: 95.50
                  Mean reward/step: 28.62
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 0.59s
                        Total time: 651.59s
                               ETA: 334.7s

################################################################################
                     [1m Learning iteration 1322/2000 [0m

                       Computation: 15989 steps/s (collection: 0.284s, learning 0.229s)
               Value function loss: 90978.6370
                    Surrogate loss: -0.0039
             Mean action noise std: 0.98
                       Mean reward: 14020.72
               Mean episode length: 483.51
                 Mean success rate: 97.50
                  Mean reward/step: 27.41
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10838016
                    Iteration time: 0.51s
                        Total time: 652.10s
                               ETA: 334.2s

################################################################################
                     [1m Learning iteration 1323/2000 [0m

                       Computation: 15196 steps/s (collection: 0.268s, learning 0.271s)
               Value function loss: 60012.5028
                    Surrogate loss: -0.0033
             Mean action noise std: 0.98
                       Mean reward: 14109.95
               Mean episode length: 485.91
                 Mean success rate: 98.00
                  Mean reward/step: 28.98
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 0.54s
                        Total time: 652.64s
                               ETA: 333.7s

################################################################################
                     [1m Learning iteration 1324/2000 [0m

                       Computation: 15697 steps/s (collection: 0.295s, learning 0.227s)
               Value function loss: 80615.0313
                    Surrogate loss: -0.0037
             Mean action noise std: 0.98
                       Mean reward: 14074.82
               Mean episode length: 484.96
                 Mean success rate: 98.00
                  Mean reward/step: 29.66
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10854400
                    Iteration time: 0.52s
                        Total time: 653.16s
                               ETA: 333.2s

################################################################################
                     [1m Learning iteration 1325/2000 [0m

                       Computation: 16529 steps/s (collection: 0.282s, learning 0.213s)
               Value function loss: 92405.6592
                    Surrogate loss: -0.0050
             Mean action noise std: 0.98
                       Mean reward: 13948.77
               Mean episode length: 480.20
                 Mean success rate: 97.00
                  Mean reward/step: 29.44
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 0.50s
                        Total time: 653.66s
                               ETA: 332.7s

################################################################################
                     [1m Learning iteration 1326/2000 [0m

                       Computation: 18110 steps/s (collection: 0.251s, learning 0.201s)
               Value function loss: 107621.1287
                    Surrogate loss: -0.0041
             Mean action noise std: 0.98
                       Mean reward: 13975.07
               Mean episode length: 481.81
                 Mean success rate: 97.00
                  Mean reward/step: 29.32
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10870784
                    Iteration time: 0.45s
                        Total time: 654.11s
                               ETA: 332.2s

################################################################################
                     [1m Learning iteration 1327/2000 [0m

                       Computation: 16780 steps/s (collection: 0.259s, learning 0.229s)
               Value function loss: 144742.7084
                    Surrogate loss: -0.0023
             Mean action noise std: 0.98
                       Mean reward: 14041.20
               Mean episode length: 484.52
                 Mean success rate: 97.50
                  Mean reward/step: 29.34
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 0.49s
                        Total time: 654.60s
                               ETA: 331.7s

################################################################################
                     [1m Learning iteration 1328/2000 [0m

                       Computation: 14644 steps/s (collection: 0.300s, learning 0.260s)
               Value function loss: 96938.9600
                    Surrogate loss: -0.0043
             Mean action noise std: 0.98
                       Mean reward: 13907.05
               Mean episode length: 480.57
                 Mean success rate: 97.00
                  Mean reward/step: 28.64
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10887168
                    Iteration time: 0.56s
                        Total time: 655.16s
                               ETA: 331.3s

################################################################################
                     [1m Learning iteration 1329/2000 [0m

                       Computation: 16280 steps/s (collection: 0.292s, learning 0.211s)
               Value function loss: 160522.2473
                    Surrogate loss: -0.0032
             Mean action noise std: 0.98
                       Mean reward: 13724.37
               Mean episode length: 474.38
                 Mean success rate: 96.00
                  Mean reward/step: 28.64
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 0.50s
                        Total time: 655.66s
                               ETA: 330.8s

################################################################################
                     [1m Learning iteration 1330/2000 [0m

                       Computation: 16246 steps/s (collection: 0.280s, learning 0.224s)
               Value function loss: 50774.8422
                    Surrogate loss: -0.0035
             Mean action noise std: 0.98
                       Mean reward: 13708.35
               Mean episode length: 473.21
                 Mean success rate: 95.50
                  Mean reward/step: 28.30
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10903552
                    Iteration time: 0.50s
                        Total time: 656.17s
                               ETA: 330.3s

################################################################################
                     [1m Learning iteration 1331/2000 [0m

                       Computation: 17704 steps/s (collection: 0.258s, learning 0.205s)
               Value function loss: 117297.9508
                    Surrogate loss: -0.0032
             Mean action noise std: 0.98
                       Mean reward: 13714.60
               Mean episode length: 473.21
                 Mean success rate: 95.50
                  Mean reward/step: 29.85
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.46s
                        Total time: 656.63s
                               ETA: 329.8s

################################################################################
                     [1m Learning iteration 1332/2000 [0m

                       Computation: 17479 steps/s (collection: 0.259s, learning 0.209s)
               Value function loss: 88500.5445
                    Surrogate loss: -0.0034
             Mean action noise std: 0.98
                       Mean reward: 13747.31
               Mean episode length: 473.43
                 Mean success rate: 95.50
                  Mean reward/step: 29.64
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10919936
                    Iteration time: 0.47s
                        Total time: 657.10s
                               ETA: 329.3s

################################################################################
                     [1m Learning iteration 1333/2000 [0m

                       Computation: 16807 steps/s (collection: 0.273s, learning 0.214s)
               Value function loss: 93683.3252
                    Surrogate loss: -0.0054
             Mean action noise std: 0.98
                       Mean reward: 13594.19
               Mean episode length: 468.54
                 Mean success rate: 94.50
                  Mean reward/step: 29.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 0.49s
                        Total time: 657.58s
                               ETA: 328.8s

################################################################################
                     [1m Learning iteration 1334/2000 [0m

                       Computation: 17127 steps/s (collection: 0.270s, learning 0.209s)
               Value function loss: 57622.4248
                    Surrogate loss: -0.0049
             Mean action noise std: 0.98
                       Mean reward: 13472.14
               Mean episode length: 464.22
                 Mean success rate: 94.00
                  Mean reward/step: 30.42
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 10936320
                    Iteration time: 0.48s
                        Total time: 658.06s
                               ETA: 328.3s

################################################################################
                     [1m Learning iteration 1335/2000 [0m

                       Computation: 16415 steps/s (collection: 0.270s, learning 0.229s)
               Value function loss: 93671.6927
                    Surrogate loss: -0.0067
             Mean action noise std: 0.98
                       Mean reward: 13467.83
               Mean episode length: 463.81
                 Mean success rate: 93.50
                  Mean reward/step: 30.23
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 0.50s
                        Total time: 658.56s
                               ETA: 327.8s

################################################################################
                     [1m Learning iteration 1336/2000 [0m

                       Computation: 16624 steps/s (collection: 0.285s, learning 0.208s)
               Value function loss: 95817.7383
                    Surrogate loss: -0.0031
             Mean action noise std: 0.98
                       Mean reward: 13529.89
               Mean episode length: 464.10
                 Mean success rate: 93.50
                  Mean reward/step: 29.72
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10952704
                    Iteration time: 0.49s
                        Total time: 659.05s
                               ETA: 327.3s

################################################################################
                     [1m Learning iteration 1337/2000 [0m

                       Computation: 16593 steps/s (collection: 0.282s, learning 0.212s)
               Value function loss: 146670.1719
                    Surrogate loss: -0.0032
             Mean action noise std: 0.98
                       Mean reward: 13470.44
               Mean episode length: 462.61
                 Mean success rate: 93.50
                  Mean reward/step: 28.76
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 0.49s
                        Total time: 659.55s
                               ETA: 326.8s

################################################################################
                     [1m Learning iteration 1338/2000 [0m

                       Computation: 16422 steps/s (collection: 0.274s, learning 0.225s)
               Value function loss: 86474.4896
                    Surrogate loss: -0.0046
             Mean action noise std: 0.98
                       Mean reward: 13273.92
               Mean episode length: 456.31
                 Mean success rate: 92.00
                  Mean reward/step: 28.68
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10969088
                    Iteration time: 0.50s
                        Total time: 660.05s
                               ETA: 326.3s

################################################################################
                     [1m Learning iteration 1339/2000 [0m

                       Computation: 16748 steps/s (collection: 0.276s, learning 0.213s)
               Value function loss: 69703.1628
                    Surrogate loss: -0.0045
             Mean action noise std: 0.98
                       Mean reward: 13250.97
               Mean episode length: 454.69
                 Mean success rate: 91.50
                  Mean reward/step: 30.21
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 0.49s
                        Total time: 660.54s
                               ETA: 325.8s

################################################################################
                     [1m Learning iteration 1340/2000 [0m

                       Computation: 17480 steps/s (collection: 0.264s, learning 0.205s)
               Value function loss: 68207.7173
                    Surrogate loss: -0.0025
             Mean action noise std: 0.98
                       Mean reward: 13223.96
               Mean episode length: 454.69
                 Mean success rate: 91.50
                  Mean reward/step: 30.73
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 10985472
                    Iteration time: 0.47s
                        Total time: 661.00s
                               ETA: 325.3s

################################################################################
                     [1m Learning iteration 1341/2000 [0m

                       Computation: 16747 steps/s (collection: 0.264s, learning 0.225s)
               Value function loss: 127355.8668
                    Surrogate loss: -0.0040
             Mean action noise std: 0.99
                       Mean reward: 13222.86
               Mean episode length: 455.02
                 Mean success rate: 92.00
                  Mean reward/step: 29.85
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 0.49s
                        Total time: 661.49s
                               ETA: 324.8s

################################################################################
                     [1m Learning iteration 1342/2000 [0m

                       Computation: 16451 steps/s (collection: 0.284s, learning 0.214s)
               Value function loss: 97837.8983
                    Surrogate loss: -0.0040
             Mean action noise std: 0.99
                       Mean reward: 13480.15
               Mean episode length: 461.40
                 Mean success rate: 93.00
                  Mean reward/step: 29.57
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11001856
                    Iteration time: 0.50s
                        Total time: 661.99s
                               ETA: 324.3s

################################################################################
                     [1m Learning iteration 1343/2000 [0m

                       Computation: 17080 steps/s (collection: 0.274s, learning 0.206s)
               Value function loss: 172243.3299
                    Surrogate loss: -0.0027
             Mean action noise std: 0.99
                       Mean reward: 13527.54
               Mean episode length: 461.40
                 Mean success rate: 93.00
                  Mean reward/step: 28.78
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.48s
                        Total time: 662.47s
                               ETA: 323.8s

################################################################################
                     [1m Learning iteration 1344/2000 [0m

                       Computation: 17360 steps/s (collection: 0.265s, learning 0.207s)
               Value function loss: 91436.0221
                    Surrogate loss: -0.0044
             Mean action noise std: 0.99
                       Mean reward: 13392.61
               Mean episode length: 456.27
                 Mean success rate: 92.50
                  Mean reward/step: 28.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11018240
                    Iteration time: 0.47s
                        Total time: 662.94s
                               ETA: 323.3s

################################################################################
                     [1m Learning iteration 1345/2000 [0m

                       Computation: 16936 steps/s (collection: 0.278s, learning 0.206s)
               Value function loss: 122809.6118
                    Surrogate loss: -0.0032
             Mean action noise std: 0.99
                       Mean reward: 13640.42
               Mean episode length: 463.37
                 Mean success rate: 93.50
                  Mean reward/step: 28.31
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 0.48s
                        Total time: 663.43s
                               ETA: 322.8s

################################################################################
                     [1m Learning iteration 1346/2000 [0m

                       Computation: 17478 steps/s (collection: 0.267s, learning 0.202s)
               Value function loss: 59700.4253
                    Surrogate loss: -0.0045
             Mean action noise std: 0.99
                       Mean reward: 13553.85
               Mean episode length: 461.73
                 Mean success rate: 93.50
                  Mean reward/step: 29.20
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11034624
                    Iteration time: 0.47s
                        Total time: 663.90s
                               ETA: 322.3s

################################################################################
                     [1m Learning iteration 1347/2000 [0m

                       Computation: 16648 steps/s (collection: 0.280s, learning 0.212s)
               Value function loss: 97289.5450
                    Surrogate loss: -0.0048
             Mean action noise std: 0.99
                       Mean reward: 13421.08
               Mean episode length: 457.67
                 Mean success rate: 93.00
                  Mean reward/step: 29.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 0.49s
                        Total time: 664.39s
                               ETA: 321.8s

################################################################################
                     [1m Learning iteration 1348/2000 [0m

                       Computation: 16565 steps/s (collection: 0.293s, learning 0.202s)
               Value function loss: 96958.9510
                    Surrogate loss: -0.0010
             Mean action noise std: 0.99
                       Mean reward: 13424.95
               Mean episode length: 457.71
                 Mean success rate: 92.50
                  Mean reward/step: 29.62
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11051008
                    Iteration time: 0.49s
                        Total time: 664.88s
                               ETA: 321.4s

################################################################################
                     [1m Learning iteration 1349/2000 [0m

                       Computation: 17168 steps/s (collection: 0.265s, learning 0.212s)
               Value function loss: 62460.9268
                    Surrogate loss: -0.0053
             Mean action noise std: 0.99
                       Mean reward: 13324.91
               Mean episode length: 453.42
                 Mean success rate: 92.50
                  Mean reward/step: 29.09
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 0.48s
                        Total time: 665.36s
                               ETA: 320.9s

################################################################################
                     [1m Learning iteration 1350/2000 [0m

                       Computation: 16748 steps/s (collection: 0.275s, learning 0.214s)
               Value function loss: 90280.6828
                    Surrogate loss: -0.0043
             Mean action noise std: 0.99
                       Mean reward: 13460.96
               Mean episode length: 457.11
                 Mean success rate: 93.50
                  Mean reward/step: 29.85
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11067392
                    Iteration time: 0.49s
                        Total time: 665.85s
                               ETA: 320.4s

################################################################################
                     [1m Learning iteration 1351/2000 [0m

                       Computation: 17762 steps/s (collection: 0.251s, learning 0.210s)
               Value function loss: 76054.8070
                    Surrogate loss: -0.0055
             Mean action noise std: 0.99
                       Mean reward: 13514.24
               Mean episode length: 458.23
                 Mean success rate: 93.50
                  Mean reward/step: 29.28
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 0.46s
                        Total time: 666.31s
                               ETA: 319.8s

################################################################################
                     [1m Learning iteration 1352/2000 [0m

                       Computation: 17283 steps/s (collection: 0.266s, learning 0.208s)
               Value function loss: 145812.4760
                    Surrogate loss: -0.0029
             Mean action noise std: 0.99
                       Mean reward: 13498.18
               Mean episode length: 457.50
                 Mean success rate: 93.50
                  Mean reward/step: 29.21
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11083776
                    Iteration time: 0.47s
                        Total time: 666.78s
                               ETA: 319.3s

################################################################################
                     [1m Learning iteration 1353/2000 [0m

                       Computation: 17914 steps/s (collection: 0.248s, learning 0.209s)
               Value function loss: 97958.5393
                    Surrogate loss: -0.0037
             Mean action noise std: 0.99
                       Mean reward: 13451.55
               Mean episode length: 457.50
                 Mean success rate: 93.50
                  Mean reward/step: 27.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 0.46s
                        Total time: 667.24s
                               ETA: 318.8s

################################################################################
                     [1m Learning iteration 1354/2000 [0m

                       Computation: 17961 steps/s (collection: 0.252s, learning 0.204s)
               Value function loss: 69931.0243
                    Surrogate loss: -0.0024
             Mean action noise std: 0.99
                       Mean reward: 13312.34
               Mean episode length: 453.98
                 Mean success rate: 93.00
                  Mean reward/step: 28.45
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11100160
                    Iteration time: 0.46s
                        Total time: 667.70s
                               ETA: 318.3s

################################################################################
                     [1m Learning iteration 1355/2000 [0m

                       Computation: 16923 steps/s (collection: 0.270s, learning 0.214s)
               Value function loss: 85756.4766
                    Surrogate loss: -0.0016
             Mean action noise std: 0.98
                       Mean reward: 13227.83
               Mean episode length: 451.75
                 Mean success rate: 92.50
                  Mean reward/step: 28.35
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.48s
                        Total time: 668.18s
                               ETA: 317.8s

################################################################################
                     [1m Learning iteration 1356/2000 [0m

                       Computation: 16568 steps/s (collection: 0.278s, learning 0.216s)
               Value function loss: 92769.2984
                    Surrogate loss: -0.0037
             Mean action noise std: 0.98
                       Mean reward: 13329.04
               Mean episode length: 454.82
                 Mean success rate: 93.00
                  Mean reward/step: 27.81
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11116544
                    Iteration time: 0.49s
                        Total time: 668.68s
                               ETA: 317.3s

################################################################################
                     [1m Learning iteration 1357/2000 [0m

                       Computation: 16741 steps/s (collection: 0.286s, learning 0.204s)
               Value function loss: 89955.8984
                    Surrogate loss: -0.0042
             Mean action noise std: 0.99
                       Mean reward: 13299.23
               Mean episode length: 454.82
                 Mean success rate: 93.00
                  Mean reward/step: 28.41
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 0.49s
                        Total time: 669.16s
                               ETA: 316.8s

################################################################################
                     [1m Learning iteration 1358/2000 [0m

                       Computation: 16703 steps/s (collection: 0.277s, learning 0.213s)
               Value function loss: 131591.2316
                    Surrogate loss: -0.0050
             Mean action noise std: 0.99
                       Mean reward: 13635.59
               Mean episode length: 465.45
                 Mean success rate: 95.00
                  Mean reward/step: 28.80
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11132928
                    Iteration time: 0.49s
                        Total time: 669.65s
                               ETA: 316.3s

################################################################################
                     [1m Learning iteration 1359/2000 [0m

                       Computation: 16977 steps/s (collection: 0.277s, learning 0.206s)
               Value function loss: 109441.7610
                    Surrogate loss: -0.0049
             Mean action noise std: 0.99
                       Mean reward: 13717.82
               Mean episode length: 468.91
                 Mean success rate: 96.00
                  Mean reward/step: 28.11
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 0.48s
                        Total time: 670.14s
                               ETA: 315.9s

################################################################################
                     [1m Learning iteration 1360/2000 [0m

                       Computation: 17811 steps/s (collection: 0.258s, learning 0.202s)
               Value function loss: 156688.4391
                    Surrogate loss: -0.0039
             Mean action noise std: 0.99
                       Mean reward: 13791.99
               Mean episode length: 471.11
                 Mean success rate: 96.00
                  Mean reward/step: 28.50
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11149312
                    Iteration time: 0.46s
                        Total time: 670.60s
                               ETA: 315.3s

################################################################################
                     [1m Learning iteration 1361/2000 [0m

                       Computation: 17479 steps/s (collection: 0.260s, learning 0.209s)
               Value function loss: 89690.2915
                    Surrogate loss: -0.0043
             Mean action noise std: 0.99
                       Mean reward: 13859.89
               Mean episode length: 474.70
                 Mean success rate: 96.00
                  Mean reward/step: 28.38
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 0.47s
                        Total time: 671.07s
                               ETA: 314.8s

################################################################################
                     [1m Learning iteration 1362/2000 [0m

                       Computation: 17907 steps/s (collection: 0.249s, learning 0.208s)
               Value function loss: 89616.8399
                    Surrogate loss: -0.0022
             Mean action noise std: 0.99
                       Mean reward: 13678.48
               Mean episode length: 471.65
                 Mean success rate: 95.50
                  Mean reward/step: 30.24
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11165696
                    Iteration time: 0.46s
                        Total time: 671.52s
                               ETA: 314.3s

################################################################################
                     [1m Learning iteration 1363/2000 [0m

                       Computation: 17110 steps/s (collection: 0.259s, learning 0.220s)
               Value function loss: 70945.1715
                    Surrogate loss: -0.0026
             Mean action noise std: 0.99
                       Mean reward: 13528.97
               Mean episode length: 468.52
                 Mean success rate: 95.00
                  Mean reward/step: 29.94
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 0.48s
                        Total time: 672.00s
                               ETA: 313.8s

################################################################################
                     [1m Learning iteration 1364/2000 [0m

                       Computation: 15101 steps/s (collection: 0.324s, learning 0.218s)
               Value function loss: 122818.3457
                    Surrogate loss: -0.0046
             Mean action noise std: 0.99
                       Mean reward: 13378.27
               Mean episode length: 463.61
                 Mean success rate: 94.00
                  Mean reward/step: 30.28
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11182080
                    Iteration time: 0.54s
                        Total time: 672.54s
                               ETA: 313.4s

################################################################################
                     [1m Learning iteration 1365/2000 [0m

                       Computation: 16851 steps/s (collection: 0.269s, learning 0.217s)
               Value function loss: 74888.6261
                    Surrogate loss: -0.0042
             Mean action noise std: 0.99
                       Mean reward: 13270.54
               Mean episode length: 460.05
                 Mean success rate: 93.00
                  Mean reward/step: 29.83
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 0.49s
                        Total time: 673.03s
                               ETA: 312.9s

################################################################################
                     [1m Learning iteration 1366/2000 [0m

                       Computation: 16670 steps/s (collection: 0.266s, learning 0.226s)
               Value function loss: 98751.8997
                    Surrogate loss: -0.0052
             Mean action noise std: 0.99
                       Mean reward: 13381.11
               Mean episode length: 464.69
                 Mean success rate: 93.50
                  Mean reward/step: 29.99
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11198464
                    Iteration time: 0.49s
                        Total time: 673.52s
                               ETA: 312.4s

################################################################################
                     [1m Learning iteration 1367/2000 [0m

                       Computation: 16914 steps/s (collection: 0.275s, learning 0.210s)
               Value function loss: 67754.9441
                    Surrogate loss: -0.0035
             Mean action noise std: 0.99
                       Mean reward: 13328.55
               Mean episode length: 464.69
                 Mean success rate: 93.50
                  Mean reward/step: 30.12
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.48s
                        Total time: 674.01s
                               ETA: 311.9s

################################################################################
                     [1m Learning iteration 1368/2000 [0m

                       Computation: 15993 steps/s (collection: 0.299s, learning 0.213s)
               Value function loss: 183935.8279
                    Surrogate loss: -0.0039
             Mean action noise std: 0.99
                       Mean reward: 13228.46
               Mean episode length: 459.81
                 Mean success rate: 92.50
                  Mean reward/step: 29.93
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11214848
                    Iteration time: 0.51s
                        Total time: 674.52s
                               ETA: 311.4s

################################################################################
                     [1m Learning iteration 1369/2000 [0m

                       Computation: 16479 steps/s (collection: 0.296s, learning 0.201s)
               Value function loss: 82984.6762
                    Surrogate loss: -0.0045
             Mean action noise std: 0.99
                       Mean reward: 13047.67
               Mean episode length: 453.29
                 Mean success rate: 91.50
                  Mean reward/step: 29.28
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 0.50s
                        Total time: 675.02s
                               ETA: 310.9s

################################################################################
                     [1m Learning iteration 1370/2000 [0m

                       Computation: 16864 steps/s (collection: 0.275s, learning 0.210s)
               Value function loss: 74951.4082
                    Surrogate loss: -0.0035
             Mean action noise std: 0.99
                       Mean reward: 13210.23
               Mean episode length: 457.88
                 Mean success rate: 92.00
                  Mean reward/step: 30.34
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11231232
                    Iteration time: 0.49s
                        Total time: 675.50s
                               ETA: 310.4s

################################################################################
                     [1m Learning iteration 1371/2000 [0m

                       Computation: 16402 steps/s (collection: 0.292s, learning 0.208s)
               Value function loss: 92723.0616
                    Surrogate loss: -0.0042
             Mean action noise std: 0.99
                       Mean reward: 12944.24
               Mean episode length: 449.13
                 Mean success rate: 90.50
                  Mean reward/step: 30.02
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 0.50s
                        Total time: 676.00s
                               ETA: 309.9s

################################################################################
                     [1m Learning iteration 1372/2000 [0m

                       Computation: 16897 steps/s (collection: 0.270s, learning 0.215s)
               Value function loss: 100369.9150
                    Surrogate loss: -0.0020
             Mean action noise std: 0.99
                       Mean reward: 13011.72
               Mean episode length: 451.20
                 Mean success rate: 91.00
                  Mean reward/step: 30.43
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11247616
                    Iteration time: 0.48s
                        Total time: 676.49s
                               ETA: 309.4s

################################################################################
                     [1m Learning iteration 1373/2000 [0m

                       Computation: 16101 steps/s (collection: 0.293s, learning 0.216s)
               Value function loss: 104742.8871
                    Surrogate loss: -0.0030
             Mean action noise std: 0.99
                       Mean reward: 13181.12
               Mean episode length: 455.17
                 Mean success rate: 91.50
                  Mean reward/step: 30.17
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 0.51s
                        Total time: 676.99s
                               ETA: 308.9s

################################################################################
                     [1m Learning iteration 1374/2000 [0m

                       Computation: 15735 steps/s (collection: 0.275s, learning 0.245s)
               Value function loss: 162822.2350
                    Surrogate loss: -0.0037
             Mean action noise std: 0.98
                       Mean reward: 13156.08
               Mean episode length: 450.57
                 Mean success rate: 91.00
                  Mean reward/step: 29.57
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11264000
                    Iteration time: 0.52s
                        Total time: 677.52s
                               ETA: 308.5s

################################################################################
                     [1m Learning iteration 1375/2000 [0m

                       Computation: 13488 steps/s (collection: 0.321s, learning 0.286s)
               Value function loss: 77983.3412
                    Surrogate loss: -0.0039
             Mean action noise std: 0.98
                       Mean reward: 13298.75
               Mean episode length: 453.06
                 Mean success rate: 91.50
                  Mean reward/step: 28.82
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 0.61s
                        Total time: 678.12s
                               ETA: 308.0s

################################################################################
                     [1m Learning iteration 1376/2000 [0m

                       Computation: 17135 steps/s (collection: 0.260s, learning 0.218s)
               Value function loss: 155581.1352
                    Surrogate loss: -0.0041
             Mean action noise std: 0.98
                       Mean reward: 13528.26
               Mean episode length: 458.42
                 Mean success rate: 93.00
                  Mean reward/step: 28.95
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11280384
                    Iteration time: 0.48s
                        Total time: 678.60s
                               ETA: 307.5s

################################################################################
                     [1m Learning iteration 1377/2000 [0m

                       Computation: 17483 steps/s (collection: 0.263s, learning 0.206s)
               Value function loss: 73863.9652
                    Surrogate loss: -0.0054
             Mean action noise std: 0.99
                       Mean reward: 13653.72
               Mean episode length: 462.86
                 Mean success rate: 94.00
                  Mean reward/step: 29.13
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 0.47s
                        Total time: 679.07s
                               ETA: 307.0s

################################################################################
                     [1m Learning iteration 1378/2000 [0m

                       Computation: 17610 steps/s (collection: 0.259s, learning 0.207s)
               Value function loss: 129714.8119
                    Surrogate loss: -0.0040
             Mean action noise std: 0.99
                       Mean reward: 13724.04
               Mean episode length: 463.15
                 Mean success rate: 94.00
                  Mean reward/step: 29.32
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11296768
                    Iteration time: 0.47s
                        Total time: 679.53s
                               ETA: 306.5s

################################################################################
                     [1m Learning iteration 1379/2000 [0m

                       Computation: 16319 steps/s (collection: 0.255s, learning 0.247s)
               Value function loss: 70500.4332
                    Surrogate loss: -0.0046
             Mean action noise std: 0.99
                       Mean reward: 13594.57
               Mean episode length: 458.57
                 Mean success rate: 93.00
                  Mean reward/step: 29.49
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.50s
                        Total time: 680.04s
                               ETA: 306.0s

################################################################################
                     [1m Learning iteration 1380/2000 [0m

                       Computation: 17712 steps/s (collection: 0.254s, learning 0.208s)
               Value function loss: 87071.3909
                    Surrogate loss: -0.0035
             Mean action noise std: 0.99
                       Mean reward: 13708.37
               Mean episode length: 460.96
                 Mean success rate: 93.50
                  Mean reward/step: 30.39
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11313152
                    Iteration time: 0.46s
                        Total time: 680.50s
                               ETA: 305.5s

################################################################################
                     [1m Learning iteration 1381/2000 [0m

                       Computation: 17234 steps/s (collection: 0.261s, learning 0.214s)
               Value function loss: 100421.2100
                    Surrogate loss: -0.0001
             Mean action noise std: 0.99
                       Mean reward: 13739.93
               Mean episode length: 460.88
                 Mean success rate: 93.50
                  Mean reward/step: 30.54
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 0.48s
                        Total time: 680.97s
                               ETA: 305.0s

################################################################################
                     [1m Learning iteration 1382/2000 [0m

                       Computation: 16516 steps/s (collection: 0.277s, learning 0.219s)
               Value function loss: 98978.8441
                    Surrogate loss: 0.0001
             Mean action noise std: 0.99
                       Mean reward: 13815.85
               Mean episode length: 462.46
                 Mean success rate: 93.50
                  Mean reward/step: 30.26
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11329536
                    Iteration time: 0.50s
                        Total time: 681.47s
                               ETA: 304.5s

################################################################################
                     [1m Learning iteration 1383/2000 [0m

                       Computation: 17442 steps/s (collection: 0.259s, learning 0.210s)
               Value function loss: 111346.5986
                    Surrogate loss: -0.0012
             Mean action noise std: 0.99
                       Mean reward: 13935.81
               Mean episode length: 464.96
                 Mean success rate: 94.00
                  Mean reward/step: 30.27
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 0.47s
                        Total time: 681.94s
                               ETA: 304.0s

################################################################################
                     [1m Learning iteration 1384/2000 [0m

                       Computation: 16658 steps/s (collection: 0.274s, learning 0.217s)
               Value function loss: 123855.5905
                    Surrogate loss: -0.0034
             Mean action noise std: 0.99
                       Mean reward: 14152.53
               Mean episode length: 471.64
                 Mean success rate: 95.00
                  Mean reward/step: 29.06
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11345920
                    Iteration time: 0.49s
                        Total time: 682.43s
                               ETA: 303.5s

################################################################################
                     [1m Learning iteration 1385/2000 [0m

                       Computation: 17538 steps/s (collection: 0.255s, learning 0.212s)
               Value function loss: 83443.0512
                    Surrogate loss: -0.0039
             Mean action noise std: 0.99
                       Mean reward: 14294.36
               Mean episode length: 476.54
                 Mean success rate: 96.00
                  Mean reward/step: 29.25
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 0.47s
                        Total time: 682.90s
                               ETA: 303.0s

################################################################################
                     [1m Learning iteration 1386/2000 [0m

                       Computation: 17252 steps/s (collection: 0.255s, learning 0.220s)
               Value function loss: 97749.9617
                    Surrogate loss: -0.0033
             Mean action noise std: 0.99
                       Mean reward: 14188.20
               Mean episode length: 473.10
                 Mean success rate: 95.50
                  Mean reward/step: 30.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11362304
                    Iteration time: 0.47s
                        Total time: 683.37s
                               ETA: 302.5s

################################################################################
                     [1m Learning iteration 1387/2000 [0m

                       Computation: 16749 steps/s (collection: 0.279s, learning 0.210s)
               Value function loss: 82689.3898
                    Surrogate loss: -0.0044
             Mean action noise std: 0.99
                       Mean reward: 13990.55
               Mean episode length: 468.69
                 Mean success rate: 95.00
                  Mean reward/step: 30.53
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 0.49s
                        Total time: 683.86s
                               ETA: 302.0s

################################################################################
                     [1m Learning iteration 1388/2000 [0m

                       Computation: 16923 steps/s (collection: 0.264s, learning 0.221s)
               Value function loss: 94140.7772
                    Surrogate loss: -0.0044
             Mean action noise std: 0.99
                       Mean reward: 14130.42
               Mean episode length: 472.94
                 Mean success rate: 95.50
                  Mean reward/step: 30.55
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11378688
                    Iteration time: 0.48s
                        Total time: 684.35s
                               ETA: 301.5s

################################################################################
                     [1m Learning iteration 1389/2000 [0m

                       Computation: 15216 steps/s (collection: 0.300s, learning 0.238s)
               Value function loss: 97378.8666
                    Surrogate loss: -0.0031
             Mean action noise std: 0.99
                       Mean reward: 14077.78
               Mean episode length: 469.05
                 Mean success rate: 95.00
                  Mean reward/step: 30.83
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 0.54s
                        Total time: 684.89s
                               ETA: 301.1s

################################################################################
                     [1m Learning iteration 1390/2000 [0m

                       Computation: 15825 steps/s (collection: 0.307s, learning 0.211s)
               Value function loss: 152636.7328
                    Surrogate loss: -0.0002
             Mean action noise std: 0.99
                       Mean reward: 14106.73
               Mean episode length: 469.04
                 Mean success rate: 95.00
                  Mean reward/step: 29.77
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11395072
                    Iteration time: 0.52s
                        Total time: 685.40s
                               ETA: 300.6s

################################################################################
                     [1m Learning iteration 1391/2000 [0m

                       Computation: 16547 steps/s (collection: 0.283s, learning 0.212s)
               Value function loss: 124893.1205
                    Surrogate loss: -0.0037
             Mean action noise std: 0.99
                       Mean reward: 14149.70
               Mean episode length: 473.62
                 Mean success rate: 96.00
                  Mean reward/step: 29.94
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.50s
                        Total time: 685.90s
                               ETA: 300.1s

################################################################################
                     [1m Learning iteration 1392/2000 [0m

                       Computation: 17104 steps/s (collection: 0.266s, learning 0.213s)
               Value function loss: 97576.8642
                    Surrogate loss: -0.0040
             Mean action noise std: 0.99
                       Mean reward: 14286.03
               Mean episode length: 477.82
                 Mean success rate: 96.50
                  Mean reward/step: 30.15
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11411456
                    Iteration time: 0.48s
                        Total time: 686.38s
                               ETA: 299.6s

################################################################################
                     [1m Learning iteration 1393/2000 [0m

                       Computation: 15146 steps/s (collection: 0.270s, learning 0.271s)
               Value function loss: 93215.7502
                    Surrogate loss: -0.0037
             Mean action noise std: 0.99
                       Mean reward: 14417.61
               Mean episode length: 482.50
                 Mean success rate: 97.50
                  Mean reward/step: 30.78
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 0.54s
                        Total time: 686.92s
                               ETA: 299.1s

################################################################################
                     [1m Learning iteration 1394/2000 [0m

                       Computation: 16286 steps/s (collection: 0.284s, learning 0.219s)
               Value function loss: 77894.2715
                    Surrogate loss: -0.0039
             Mean action noise std: 0.99
                       Mean reward: 14426.85
               Mean episode length: 482.50
                 Mean success rate: 97.50
                  Mean reward/step: 30.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11427840
                    Iteration time: 0.50s
                        Total time: 687.42s
                               ETA: 298.6s

################################################################################
                     [1m Learning iteration 1395/2000 [0m

                       Computation: 15454 steps/s (collection: 0.313s, learning 0.217s)
               Value function loss: 77385.4525
                    Surrogate loss: -0.0035
             Mean action noise std: 0.99
                       Mean reward: 14443.06
               Mean episode length: 482.50
                 Mean success rate: 97.50
                  Mean reward/step: 31.60
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 0.53s
                        Total time: 687.95s
                               ETA: 298.1s

################################################################################
                     [1m Learning iteration 1396/2000 [0m

                       Computation: 16299 steps/s (collection: 0.287s, learning 0.216s)
               Value function loss: 78910.6874
                    Surrogate loss: -0.0037
             Mean action noise std: 0.99
                       Mean reward: 14448.48
               Mean episode length: 482.50
                 Mean success rate: 97.50
                  Mean reward/step: 31.36
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11444224
                    Iteration time: 0.50s
                        Total time: 688.45s
                               ETA: 297.7s

################################################################################
                     [1m Learning iteration 1397/2000 [0m

                       Computation: 15214 steps/s (collection: 0.288s, learning 0.250s)
               Value function loss: 115650.5805
                    Surrogate loss: -0.0033
             Mean action noise std: 0.99
                       Mean reward: 14454.95
               Mean episode length: 482.50
                 Mean success rate: 97.50
                  Mean reward/step: 31.22
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 0.54s
                        Total time: 688.99s
                               ETA: 297.2s

################################################################################
                     [1m Learning iteration 1398/2000 [0m

                       Computation: 16612 steps/s (collection: 0.277s, learning 0.216s)
               Value function loss: 82913.5763
                    Surrogate loss: -0.0011
             Mean action noise std: 0.99
                       Mean reward: 14515.66
               Mean episode length: 482.76
                 Mean success rate: 97.50
                  Mean reward/step: 31.32
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11460608
                    Iteration time: 0.49s
                        Total time: 689.48s
                               ETA: 296.7s

################################################################################
                     [1m Learning iteration 1399/2000 [0m

                       Computation: 16869 steps/s (collection: 0.276s, learning 0.209s)
               Value function loss: 172364.2727
                    Surrogate loss: -0.0029
             Mean action noise std: 0.99
                       Mean reward: 14778.24
               Mean episode length: 487.17
                 Mean success rate: 98.00
                  Mean reward/step: 31.93
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 0.49s
                        Total time: 689.97s
                               ETA: 296.2s

################################################################################
                     [1m Learning iteration 1400/2000 [0m

                       Computation: 16950 steps/s (collection: 0.263s, learning 0.220s)
               Value function loss: 98935.4918
                    Surrogate loss: -0.0043
             Mean action noise std: 0.99
                       Mean reward: 14847.74
               Mean episode length: 489.19
                 Mean success rate: 98.00
                  Mean reward/step: 30.37
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11476992
                    Iteration time: 0.48s
                        Total time: 690.45s
                               ETA: 295.7s

################################################################################
                     [1m Learning iteration 1401/2000 [0m

                       Computation: 17875 steps/s (collection: 0.245s, learning 0.214s)
               Value function loss: 77802.5089
                    Surrogate loss: -0.0038
             Mean action noise std: 0.99
                       Mean reward: 14846.75
               Mean episode length: 489.19
                 Mean success rate: 98.00
                  Mean reward/step: 31.10
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 0.46s
                        Total time: 690.91s
                               ETA: 295.2s

################################################################################
                     [1m Learning iteration 1402/2000 [0m

                       Computation: 16708 steps/s (collection: 0.268s, learning 0.222s)
               Value function loss: 104926.7258
                    Surrogate loss: -0.0037
             Mean action noise std: 0.99
                       Mean reward: 14554.59
               Mean episode length: 480.81
                 Mean success rate: 97.00
                  Mean reward/step: 30.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11493376
                    Iteration time: 0.49s
                        Total time: 691.40s
                               ETA: 294.7s

################################################################################
                     [1m Learning iteration 1403/2000 [0m

                       Computation: 18192 steps/s (collection: 0.238s, learning 0.212s)
               Value function loss: 91675.0975
                    Surrogate loss: -0.0029
             Mean action noise std: 0.99
                       Mean reward: 14603.20
               Mean episode length: 480.81
                 Mean success rate: 97.00
                  Mean reward/step: 30.54
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.45s
                        Total time: 691.85s
                               ETA: 294.2s

################################################################################
                     [1m Learning iteration 1404/2000 [0m

                       Computation: 15592 steps/s (collection: 0.273s, learning 0.252s)
               Value function loss: 71102.4140
                    Surrogate loss: -0.0014
             Mean action noise std: 0.99
                       Mean reward: 14656.84
               Mean episode length: 480.81
                 Mean success rate: 97.00
                  Mean reward/step: 30.61
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11509760
                    Iteration time: 0.53s
                        Total time: 692.38s
                               ETA: 293.7s

################################################################################
                     [1m Learning iteration 1405/2000 [0m

                       Computation: 17101 steps/s (collection: 0.270s, learning 0.209s)
               Value function loss: 170714.7738
                    Surrogate loss: -0.0025
             Mean action noise std: 0.99
                       Mean reward: 14645.60
               Mean episode length: 478.76
                 Mean success rate: 97.00
                  Mean reward/step: 30.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 0.48s
                        Total time: 692.86s
                               ETA: 293.2s

################################################################################
                     [1m Learning iteration 1406/2000 [0m

                       Computation: 17173 steps/s (collection: 0.265s, learning 0.212s)
               Value function loss: 110567.4354
                    Surrogate loss: -0.0041
             Mean action noise std: 0.99
                       Mean reward: 14308.24
               Mean episode length: 467.06
                 Mean success rate: 95.00
                  Mean reward/step: 28.86
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11526144
                    Iteration time: 0.48s
                        Total time: 693.33s
                               ETA: 292.7s

################################################################################
                     [1m Learning iteration 1407/2000 [0m

                       Computation: 16668 steps/s (collection: 0.276s, learning 0.215s)
               Value function loss: 172370.5501
                    Surrogate loss: -0.0023
             Mean action noise std: 0.99
                       Mean reward: 14210.52
               Mean episode length: 462.61
                 Mean success rate: 94.50
                  Mean reward/step: 29.59
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 0.49s
                        Total time: 693.83s
                               ETA: 292.2s

################################################################################
                     [1m Learning iteration 1408/2000 [0m

                       Computation: 16856 steps/s (collection: 0.265s, learning 0.221s)
               Value function loss: 96230.4527
                    Surrogate loss: -0.0055
             Mean action noise std: 0.99
                       Mean reward: 14033.22
               Mean episode length: 456.69
                 Mean success rate: 93.00
                  Mean reward/step: 29.44
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11542528
                    Iteration time: 0.49s
                        Total time: 694.31s
                               ETA: 291.7s

################################################################################
                     [1m Learning iteration 1409/2000 [0m

                       Computation: 15746 steps/s (collection: 0.268s, learning 0.252s)
               Value function loss: 122519.4256
                    Surrogate loss: -0.0037
             Mean action noise std: 0.99
                       Mean reward: 13730.57
               Mean episode length: 449.24
                 Mean success rate: 92.00
                  Mean reward/step: 30.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 0.52s
                        Total time: 694.83s
                               ETA: 291.2s

################################################################################
                     [1m Learning iteration 1410/2000 [0m

                       Computation: 16838 steps/s (collection: 0.280s, learning 0.206s)
               Value function loss: 54866.2410
                    Surrogate loss: -0.0032
             Mean action noise std: 0.99
                       Mean reward: 13770.34
               Mean episode length: 451.11
                 Mean success rate: 92.50
                  Mean reward/step: 30.58
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 11558912
                    Iteration time: 0.49s
                        Total time: 695.32s
                               ETA: 290.7s

################################################################################
                     [1m Learning iteration 1411/2000 [0m

                       Computation: 17433 steps/s (collection: 0.262s, learning 0.207s)
               Value function loss: 80567.5972
                    Surrogate loss: -0.0051
             Mean action noise std: 0.99
                       Mean reward: 13508.40
               Mean episode length: 443.19
                 Mean success rate: 91.50
                  Mean reward/step: 30.87
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 0.47s
                        Total time: 695.79s
                               ETA: 290.2s

################################################################################
                     [1m Learning iteration 1412/2000 [0m

                       Computation: 17385 steps/s (collection: 0.262s, learning 0.209s)
               Value function loss: 105607.2219
                    Surrogate loss: -0.0027
             Mean action noise std: 0.99
                       Mean reward: 13708.27
               Mean episode length: 448.52
                 Mean success rate: 92.50
                  Mean reward/step: 31.18
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11575296
                    Iteration time: 0.47s
                        Total time: 696.26s
                               ETA: 289.7s

################################################################################
                     [1m Learning iteration 1413/2000 [0m

                       Computation: 16609 steps/s (collection: 0.256s, learning 0.237s)
               Value function loss: 117697.4882
                    Surrogate loss: -0.0006
             Mean action noise std: 0.99
                       Mean reward: 13883.54
               Mean episode length: 452.27
                 Mean success rate: 93.00
                  Mean reward/step: 30.94
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 0.49s
                        Total time: 696.75s
                               ETA: 289.2s

################################################################################
                     [1m Learning iteration 1414/2000 [0m

                       Computation: 17102 steps/s (collection: 0.256s, learning 0.223s)
               Value function loss: 59715.9218
                    Surrogate loss: -0.0017
             Mean action noise std: 0.99
                       Mean reward: 13888.82
               Mean episode length: 452.27
                 Mean success rate: 93.00
                  Mean reward/step: 31.15
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 11591680
                    Iteration time: 0.48s
                        Total time: 697.23s
                               ETA: 288.7s

################################################################################
                     [1m Learning iteration 1415/2000 [0m

                       Computation: 17232 steps/s (collection: 0.266s, learning 0.210s)
               Value function loss: 149315.5750
                    Surrogate loss: -0.0011
             Mean action noise std: 0.99
                       Mean reward: 13900.58
               Mean episode length: 452.27
                 Mean success rate: 93.00
                  Mean reward/step: 31.13
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.48s
                        Total time: 697.71s
                               ETA: 288.2s

################################################################################
                     [1m Learning iteration 1416/2000 [0m

                       Computation: 18061 steps/s (collection: 0.252s, learning 0.202s)
               Value function loss: 81312.5439
                    Surrogate loss: -0.0030
             Mean action noise std: 0.99
                       Mean reward: 13921.72
               Mean episode length: 454.31
                 Mean success rate: 93.00
                  Mean reward/step: 30.65
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11608064
                    Iteration time: 0.45s
                        Total time: 698.16s
                               ETA: 287.7s

################################################################################
                     [1m Learning iteration 1417/2000 [0m

                       Computation: 16979 steps/s (collection: 0.244s, learning 0.239s)
               Value function loss: 89024.9104
                    Surrogate loss: -0.0030
             Mean action noise std: 0.99
                       Mean reward: 14073.27
               Mean episode length: 459.20
                 Mean success rate: 94.00
                  Mean reward/step: 31.41
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 0.48s
                        Total time: 698.64s
                               ETA: 287.2s

################################################################################
                     [1m Learning iteration 1418/2000 [0m

                       Computation: 16401 steps/s (collection: 0.296s, learning 0.204s)
               Value function loss: 84110.6110
                    Surrogate loss: -0.0031
             Mean action noise std: 0.99
                       Mean reward: 14237.54
               Mean episode length: 465.30
                 Mean success rate: 94.50
                  Mean reward/step: 31.06
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11624448
                    Iteration time: 0.50s
                        Total time: 699.14s
                               ETA: 286.8s

################################################################################
                     [1m Learning iteration 1419/2000 [0m

                       Computation: 16618 steps/s (collection: 0.281s, learning 0.212s)
               Value function loss: 97646.0480
                    Surrogate loss: -0.0045
             Mean action noise std: 0.99
                       Mean reward: 14318.89
               Mean episode length: 467.79
                 Mean success rate: 95.00
                  Mean reward/step: 31.18
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 0.49s
                        Total time: 699.64s
                               ETA: 286.3s

################################################################################
                     [1m Learning iteration 1420/2000 [0m

                       Computation: 17735 steps/s (collection: 0.261s, learning 0.201s)
               Value function loss: 83591.1631
                    Surrogate loss: -0.0036
             Mean action noise std: 0.99
                       Mean reward: 14244.47
               Mean episode length: 466.06
                 Mean success rate: 94.50
                  Mean reward/step: 31.21
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11640832
                    Iteration time: 0.46s
                        Total time: 700.10s
                               ETA: 285.8s

################################################################################
                     [1m Learning iteration 1421/2000 [0m

                       Computation: 17329 steps/s (collection: 0.268s, learning 0.205s)
               Value function loss: 176170.2988
                    Surrogate loss: -0.0031
             Mean action noise std: 0.99
                       Mean reward: 14578.70
               Mean episode length: 475.96
                 Mean success rate: 96.00
                  Mean reward/step: 30.69
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 0.47s
                        Total time: 700.57s
                               ETA: 285.3s

################################################################################
                     [1m Learning iteration 1422/2000 [0m

                       Computation: 16650 steps/s (collection: 0.274s, learning 0.218s)
               Value function loss: 84943.5567
                    Surrogate loss: -0.0035
             Mean action noise std: 0.99
                       Mean reward: 14541.31
               Mean episode length: 474.62
                 Mean success rate: 95.50
                  Mean reward/step: 29.83
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11657216
                    Iteration time: 0.49s
                        Total time: 701.06s
                               ETA: 284.8s

################################################################################
                     [1m Learning iteration 1423/2000 [0m

                       Computation: 17498 steps/s (collection: 0.262s, learning 0.206s)
               Value function loss: 148879.1932
                    Surrogate loss: -0.0026
             Mean action noise std: 0.99
                       Mean reward: 14725.58
               Mean episode length: 479.23
                 Mean success rate: 95.50
                  Mean reward/step: 29.76
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 0.47s
                        Total time: 701.53s
                               ETA: 284.3s

################################################################################
                     [1m Learning iteration 1424/2000 [0m

                       Computation: 16507 steps/s (collection: 0.263s, learning 0.233s)
               Value function loss: 91842.4195
                    Surrogate loss: -0.0032
             Mean action noise std: 0.99
                       Mean reward: 14664.54
               Mean episode length: 477.57
                 Mean success rate: 95.50
                  Mean reward/step: 29.82
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11673600
                    Iteration time: 0.50s
                        Total time: 702.03s
                               ETA: 283.8s

################################################################################
                     [1m Learning iteration 1425/2000 [0m

                       Computation: 17054 steps/s (collection: 0.265s, learning 0.215s)
               Value function loss: 123855.4100
                    Surrogate loss: -0.0040
             Mean action noise std: 0.99
                       Mean reward: 14219.38
               Mean episode length: 464.33
                 Mean success rate: 93.00
                  Mean reward/step: 30.53
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 0.48s
                        Total time: 702.51s
                               ETA: 283.3s

################################################################################
                     [1m Learning iteration 1426/2000 [0m

                       Computation: 17598 steps/s (collection: 0.246s, learning 0.219s)
               Value function loss: 87119.5769
                    Surrogate loss: -0.0036
             Mean action noise std: 0.99
                       Mean reward: 14066.70
               Mean episode length: 459.77
                 Mean success rate: 92.00
                  Mean reward/step: 30.74
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11689984
                    Iteration time: 0.47s
                        Total time: 702.97s
                               ETA: 282.8s

################################################################################
                     [1m Learning iteration 1427/2000 [0m

                       Computation: 16852 steps/s (collection: 0.270s, learning 0.217s)
               Value function loss: 85208.2650
                    Surrogate loss: -0.0048
             Mean action noise std: 0.99
                       Mean reward: 13979.61
               Mean episode length: 455.79
                 Mean success rate: 91.50
                  Mean reward/step: 30.87
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.49s
                        Total time: 703.46s
                               ETA: 282.3s

################################################################################
                     [1m Learning iteration 1428/2000 [0m

                       Computation: 17055 steps/s (collection: 0.266s, learning 0.214s)
               Value function loss: 94107.0604
                    Surrogate loss: -0.0027
             Mean action noise std: 0.99
                       Mean reward: 13963.83
               Mean episode length: 455.79
                 Mean success rate: 91.50
                  Mean reward/step: 31.12
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11706368
                    Iteration time: 0.48s
                        Total time: 703.94s
                               ETA: 281.8s

################################################################################
                     [1m Learning iteration 1429/2000 [0m

                       Computation: 17438 steps/s (collection: 0.264s, learning 0.206s)
               Value function loss: 108284.0410
                    Surrogate loss: -0.0042
             Mean action noise std: 0.99
                       Mean reward: 14039.70
               Mean episode length: 457.43
                 Mean success rate: 92.00
                  Mean reward/step: 30.84
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 0.47s
                        Total time: 704.41s
                               ETA: 281.3s

################################################################################
                     [1m Learning iteration 1430/2000 [0m

                       Computation: 17749 steps/s (collection: 0.250s, learning 0.212s)
               Value function loss: 123497.6711
                    Surrogate loss: -0.0021
             Mean action noise std: 0.99
                       Mean reward: 14059.67
               Mean episode length: 457.43
                 Mean success rate: 92.00
                  Mean reward/step: 31.31
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11722752
                    Iteration time: 0.46s
                        Total time: 704.87s
                               ETA: 280.8s

################################################################################
                     [1m Learning iteration 1431/2000 [0m

                       Computation: 17133 steps/s (collection: 0.273s, learning 0.205s)
               Value function loss: 118808.7667
                    Surrogate loss: -0.0034
             Mean action noise std: 0.99
                       Mean reward: 14141.24
               Mean episode length: 458.26
                 Mean success rate: 92.50
                  Mean reward/step: 29.93
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 0.48s
                        Total time: 705.35s
                               ETA: 280.3s

################################################################################
                     [1m Learning iteration 1432/2000 [0m

                       Computation: 17686 steps/s (collection: 0.249s, learning 0.214s)
               Value function loss: 74927.0576
                    Surrogate loss: -0.0055
             Mean action noise std: 0.99
                       Mean reward: 14141.87
               Mean episode length: 458.26
                 Mean success rate: 92.50
                  Mean reward/step: 30.50
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11739136
                    Iteration time: 0.46s
                        Total time: 705.81s
                               ETA: 279.8s

################################################################################
                     [1m Learning iteration 1433/2000 [0m

                       Computation: 17765 steps/s (collection: 0.251s, learning 0.211s)
               Value function loss: 129694.1029
                    Surrogate loss: -0.0041
             Mean action noise std: 0.99
                       Mean reward: 14251.24
               Mean episode length: 461.12
                 Mean success rate: 93.50
                  Mean reward/step: 31.20
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 0.46s
                        Total time: 706.27s
                               ETA: 279.3s

################################################################################
                     [1m Learning iteration 1434/2000 [0m

                       Computation: 17483 steps/s (collection: 0.259s, learning 0.209s)
               Value function loss: 73230.2774
                    Surrogate loss: -0.0038
             Mean action noise std: 0.99
                       Mean reward: 14251.69
               Mean episode length: 461.12
                 Mean success rate: 93.50
                  Mean reward/step: 30.64
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 11755520
                    Iteration time: 0.47s
                        Total time: 706.74s
                               ETA: 278.8s

################################################################################
                     [1m Learning iteration 1435/2000 [0m

                       Computation: 16340 steps/s (collection: 0.275s, learning 0.226s)
               Value function loss: 66624.8439
                    Surrogate loss: -0.0042
             Mean action noise std: 0.99
                       Mean reward: 14254.61
               Mean episode length: 461.12
                 Mean success rate: 93.50
                  Mean reward/step: 31.34
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 0.50s
                        Total time: 707.24s
                               ETA: 278.3s

################################################################################
                     [1m Learning iteration 1436/2000 [0m

                       Computation: 16543 steps/s (collection: 0.269s, learning 0.226s)
               Value function loss: 98286.4821
                    Surrogate loss: -0.0044
             Mean action noise std: 0.99
                       Mean reward: 14544.33
               Mean episode length: 469.99
                 Mean success rate: 95.00
                  Mean reward/step: 31.71
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11771904
                    Iteration time: 0.50s
                        Total time: 707.74s
                               ETA: 277.8s

################################################################################
                     [1m Learning iteration 1437/2000 [0m

                       Computation: 16690 steps/s (collection: 0.276s, learning 0.215s)
               Value function loss: 129400.3318
                    Surrogate loss: -0.0022
             Mean action noise std: 0.99
                       Mean reward: 14512.45
               Mean episode length: 469.99
                 Mean success rate: 95.00
                  Mean reward/step: 30.15
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 0.49s
                        Total time: 708.23s
                               ETA: 277.3s

################################################################################
                     [1m Learning iteration 1438/2000 [0m

                       Computation: 16942 steps/s (collection: 0.276s, learning 0.208s)
               Value function loss: 177068.3861
                    Surrogate loss: -0.0028
             Mean action noise std: 0.99
                       Mean reward: 15081.12
               Mean episode length: 487.79
                 Mean success rate: 98.50
                  Mean reward/step: 30.29
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11788288
                    Iteration time: 0.48s
                        Total time: 708.71s
                               ETA: 276.8s

################################################################################
                     [1m Learning iteration 1439/2000 [0m

                       Computation: 17072 steps/s (collection: 0.261s, learning 0.219s)
               Value function loss: 130359.8736
                    Surrogate loss: -0.0038
             Mean action noise std: 0.99
                       Mean reward: 14777.77
               Mean episode length: 479.24
                 Mean success rate: 96.50
                  Mean reward/step: 29.29
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.48s
                        Total time: 709.19s
                               ETA: 276.3s

################################################################################
                     [1m Learning iteration 1440/2000 [0m

                       Computation: 15863 steps/s (collection: 0.282s, learning 0.235s)
               Value function loss: 100921.3318
                    Surrogate loss: -0.0027
             Mean action noise std: 0.99
                       Mean reward: 14326.48
               Mean episode length: 465.92
                 Mean success rate: 94.00
                  Mean reward/step: 30.52
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11804672
                    Iteration time: 0.52s
                        Total time: 709.71s
                               ETA: 275.8s

################################################################################
                     [1m Learning iteration 1441/2000 [0m

                       Computation: 17197 steps/s (collection: 0.258s, learning 0.218s)
               Value function loss: 64295.8607
                    Surrogate loss: -0.0040
             Mean action noise std: 0.99
                       Mean reward: 14312.28
               Mean episode length: 466.95
                 Mean success rate: 94.00
                  Mean reward/step: 30.30
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 0.48s
                        Total time: 710.18s
                               ETA: 275.3s

################################################################################
                     [1m Learning iteration 1442/2000 [0m

                       Computation: 17132 steps/s (collection: 0.269s, learning 0.210s)
               Value function loss: 99681.0090
                    Surrogate loss: -0.0034
             Mean action noise std: 0.99
                       Mean reward: 14290.43
               Mean episode length: 468.28
                 Mean success rate: 94.50
                  Mean reward/step: 31.29
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11821056
                    Iteration time: 0.48s
                        Total time: 710.66s
                               ETA: 274.8s

################################################################################
                     [1m Learning iteration 1443/2000 [0m

                       Computation: 16057 steps/s (collection: 0.281s, learning 0.230s)
               Value function loss: 68504.2983
                    Surrogate loss: -0.0023
             Mean action noise std: 0.99
                       Mean reward: 14202.45
               Mean episode length: 465.79
                 Mean success rate: 94.00
                  Mean reward/step: 31.22
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 0.51s
                        Total time: 711.17s
                               ETA: 274.3s

################################################################################
                     [1m Learning iteration 1444/2000 [0m

                       Computation: 16814 steps/s (collection: 0.276s, learning 0.211s)
               Value function loss: 118092.6492
                    Surrogate loss: -0.0033
             Mean action noise std: 0.99
                       Mean reward: 14211.87
               Mean episode length: 465.79
                 Mean success rate: 94.00
                  Mean reward/step: 31.32
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11837440
                    Iteration time: 0.49s
                        Total time: 711.66s
                               ETA: 273.8s

################################################################################
                     [1m Learning iteration 1445/2000 [0m

                       Computation: 17623 steps/s (collection: 0.253s, learning 0.212s)
               Value function loss: 92072.8479
                    Surrogate loss: -0.0019
             Mean action noise std: 0.99
                       Mean reward: 14301.38
               Mean episode length: 469.46
                 Mean success rate: 94.50
                  Mean reward/step: 30.96
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 0.46s
                        Total time: 712.12s
                               ETA: 273.3s

################################################################################
                     [1m Learning iteration 1446/2000 [0m

                       Computation: 16480 steps/s (collection: 0.273s, learning 0.224s)
               Value function loss: 149477.1170
                    Surrogate loss: -0.0008
             Mean action noise std: 0.99
                       Mean reward: 14128.96
               Mean episode length: 464.82
                 Mean success rate: 94.00
                  Mean reward/step: 31.08
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11853824
                    Iteration time: 0.50s
                        Total time: 712.62s
                               ETA: 272.8s

################################################################################
                     [1m Learning iteration 1447/2000 [0m

                       Computation: 16578 steps/s (collection: 0.279s, learning 0.215s)
               Value function loss: 98471.0899
                    Surrogate loss: -0.0013
             Mean action noise std: 0.99
                       Mean reward: 14147.51
               Mean episode length: 464.82
                 Mean success rate: 94.00
                  Mean reward/step: 29.94
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 0.49s
                        Total time: 713.12s
                               ETA: 272.3s

################################################################################
                     [1m Learning iteration 1448/2000 [0m

                       Computation: 17075 steps/s (collection: 0.268s, learning 0.212s)
               Value function loss: 90993.4707
                    Surrogate loss: -0.0029
             Mean action noise std: 0.99
                       Mean reward: 14133.59
               Mean episode length: 464.82
                 Mean success rate: 94.00
                  Mean reward/step: 30.55
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11870208
                    Iteration time: 0.48s
                        Total time: 713.60s
                               ETA: 271.8s

################################################################################
                     [1m Learning iteration 1449/2000 [0m

                       Computation: 16281 steps/s (collection: 0.273s, learning 0.231s)
               Value function loss: 97494.3216
                    Surrogate loss: -0.0006
             Mean action noise std: 0.99
                       Mean reward: 14095.16
               Mean episode length: 463.68
                 Mean success rate: 94.00
                  Mean reward/step: 30.73
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 0.50s
                        Total time: 714.10s
                               ETA: 271.4s

################################################################################
                     [1m Learning iteration 1450/2000 [0m

                       Computation: 15257 steps/s (collection: 0.290s, learning 0.247s)
               Value function loss: 73815.2679
                    Surrogate loss: -0.0039
             Mean action noise std: 0.99
                       Mean reward: 14101.95
               Mean episode length: 463.68
                 Mean success rate: 94.00
                  Mean reward/step: 30.72
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11886592
                    Iteration time: 0.54s
                        Total time: 714.64s
                               ETA: 270.9s

################################################################################
                     [1m Learning iteration 1451/2000 [0m

                       Computation: 16863 steps/s (collection: 0.264s, learning 0.221s)
               Value function loss: 69694.1409
                    Surrogate loss: -0.0018
             Mean action noise std: 0.99
                       Mean reward: 14256.21
               Mean episode length: 468.57
                 Mean success rate: 95.00
                  Mean reward/step: 31.47
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.49s
                        Total time: 715.12s
                               ETA: 270.4s

################################################################################
                     [1m Learning iteration 1452/2000 [0m

                       Computation: 16434 steps/s (collection: 0.281s, learning 0.218s)
               Value function loss: 143529.8166
                    Surrogate loss: 0.0004
             Mean action noise std: 0.99
                       Mean reward: 14793.47
               Mean episode length: 485.10
                 Mean success rate: 98.00
                  Mean reward/step: 30.77
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11902976
                    Iteration time: 0.50s
                        Total time: 715.62s
                               ETA: 269.9s

################################################################################
                     [1m Learning iteration 1453/2000 [0m

                       Computation: 16777 steps/s (collection: 0.273s, learning 0.215s)
               Value function loss: 86810.9006
                    Surrogate loss: 0.0070
             Mean action noise std: 0.99
                       Mean reward: 14953.85
               Mean episode length: 489.54
                 Mean success rate: 99.00
                  Mean reward/step: 30.40
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 0.49s
                        Total time: 716.11s
                               ETA: 269.4s

################################################################################
                     [1m Learning iteration 1454/2000 [0m

                       Computation: 16026 steps/s (collection: 0.296s, learning 0.215s)
               Value function loss: 140609.7602
                    Surrogate loss: -0.0018
             Mean action noise std: 0.99
                       Mean reward: 15013.17
               Mean episode length: 489.54
                 Mean success rate: 99.00
                  Mean reward/step: 30.93
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11919360
                    Iteration time: 0.51s
                        Total time: 716.62s
                               ETA: 268.9s

################################################################################
                     [1m Learning iteration 1455/2000 [0m

                       Computation: 16809 steps/s (collection: 0.274s, learning 0.213s)
               Value function loss: 126469.6245
                    Surrogate loss: -0.0030
             Mean action noise std: 0.99
                       Mean reward: 15186.82
               Mean episode length: 494.21
                 Mean success rate: 99.50
                  Mean reward/step: 30.08
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 0.49s
                        Total time: 717.11s
                               ETA: 268.4s

################################################################################
                     [1m Learning iteration 1456/2000 [0m

                       Computation: 16515 steps/s (collection: 0.278s, learning 0.218s)
               Value function loss: 161678.0111
                    Surrogate loss: -0.0026
             Mean action noise std: 0.99
                       Mean reward: 15180.67
               Mean episode length: 494.21
                 Mean success rate: 99.50
                  Mean reward/step: 30.23
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11935744
                    Iteration time: 0.50s
                        Total time: 717.60s
                               ETA: 267.9s

################################################################################
                     [1m Learning iteration 1457/2000 [0m

                       Computation: 16260 steps/s (collection: 0.282s, learning 0.222s)
               Value function loss: 70177.5609
                    Surrogate loss: -0.0030
             Mean action noise std: 0.99
                       Mean reward: 15226.97
               Mean episode length: 494.21
                 Mean success rate: 99.50
                  Mean reward/step: 30.08
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 0.50s
                        Total time: 718.11s
                               ETA: 267.4s

################################################################################
                     [1m Learning iteration 1458/2000 [0m

                       Computation: 17015 steps/s (collection: 0.269s, learning 0.212s)
               Value function loss: 89323.5691
                    Surrogate loss: -0.0043
             Mean action noise std: 0.99
                       Mean reward: 15381.21
               Mean episode length: 498.86
                 Mean success rate: 100.00
                  Mean reward/step: 30.89
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11952128
                    Iteration time: 0.48s
                        Total time: 718.59s
                               ETA: 266.9s

################################################################################
                     [1m Learning iteration 1459/2000 [0m

                       Computation: 17967 steps/s (collection: 0.254s, learning 0.202s)
               Value function loss: 79249.6651
                    Surrogate loss: -0.0035
             Mean action noise std: 0.99
                       Mean reward: 15374.02
               Mean episode length: 498.86
                 Mean success rate: 100.00
                  Mean reward/step: 30.90
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 0.46s
                        Total time: 719.04s
                               ETA: 266.4s

################################################################################
                     [1m Learning iteration 1460/2000 [0m

                       Computation: 17203 steps/s (collection: 0.267s, learning 0.209s)
               Value function loss: 109037.7919
                    Surrogate loss: -0.0050
             Mean action noise std: 0.99
                       Mean reward: 15200.29
               Mean episode length: 493.96
                 Mean success rate: 99.00
                  Mean reward/step: 31.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11968512
                    Iteration time: 0.48s
                        Total time: 719.52s
                               ETA: 265.9s

################################################################################
                     [1m Learning iteration 1461/2000 [0m

                       Computation: 17873 steps/s (collection: 0.250s, learning 0.208s)
               Value function loss: 83025.5045
                    Surrogate loss: -0.0015
             Mean action noise std: 0.99
                       Mean reward: 15213.52
               Mean episode length: 493.96
                 Mean success rate: 99.00
                  Mean reward/step: 31.07
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 0.46s
                        Total time: 719.98s
                               ETA: 265.4s

################################################################################
                     [1m Learning iteration 1462/2000 [0m

                       Computation: 17447 steps/s (collection: 0.255s, learning 0.215s)
               Value function loss: 163135.9310
                    Surrogate loss: 0.0003
             Mean action noise std: 0.99
                       Mean reward: 15233.12
               Mean episode length: 495.11
                 Mean success rate: 99.00
                  Mean reward/step: 30.60
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11984896
                    Iteration time: 0.47s
                        Total time: 720.45s
                               ETA: 264.9s

################################################################################
                     [1m Learning iteration 1463/2000 [0m

                       Computation: 17260 steps/s (collection: 0.262s, learning 0.212s)
               Value function loss: 86450.8823
                    Surrogate loss: -0.0040
             Mean action noise std: 0.99
                       Mean reward: 15214.11
               Mean episode length: 495.11
                 Mean success rate: 99.00
                  Mean reward/step: 29.88
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.47s
                        Total time: 720.92s
                               ETA: 264.4s

################################################################################
                     [1m Learning iteration 1464/2000 [0m

                       Computation: 15588 steps/s (collection: 0.305s, learning 0.221s)
               Value function loss: 103666.5048
                    Surrogate loss: -0.0030
             Mean action noise std: 0.99
                       Mean reward: 14602.57
               Mean episode length: 476.50
                 Mean success rate: 95.00
                  Mean reward/step: 30.69
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12001280
                    Iteration time: 0.53s
                        Total time: 721.45s
                               ETA: 264.0s

################################################################################
                     [1m Learning iteration 1465/2000 [0m

                       Computation: 16576 steps/s (collection: 0.269s, learning 0.225s)
               Value function loss: 82744.9737
                    Surrogate loss: -0.0031
             Mean action noise std: 0.99
                       Mean reward: 14626.03
               Mean episode length: 476.50
                 Mean success rate: 95.00
                  Mean reward/step: 30.20
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 0.49s
                        Total time: 721.94s
                               ETA: 263.5s

################################################################################
                     [1m Learning iteration 1466/2000 [0m

                       Computation: 16126 steps/s (collection: 0.289s, learning 0.219s)
               Value function loss: 78995.6421
                    Surrogate loss: -0.0047
             Mean action noise std: 0.99
                       Mean reward: 14480.67
               Mean episode length: 471.82
                 Mean success rate: 94.00
                  Mean reward/step: 31.07
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12017664
                    Iteration time: 0.51s
                        Total time: 722.45s
                               ETA: 263.0s

################################################################################
                     [1m Learning iteration 1467/2000 [0m

                       Computation: 16966 steps/s (collection: 0.274s, learning 0.209s)
               Value function loss: 86584.3003
                    Surrogate loss: -0.0027
             Mean action noise std: 0.99
                       Mean reward: 14473.19
               Mean episode length: 471.82
                 Mean success rate: 94.00
                  Mean reward/step: 31.53
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 0.48s
                        Total time: 722.93s
                               ETA: 262.5s

################################################################################
                     [1m Learning iteration 1468/2000 [0m

                       Computation: 16260 steps/s (collection: 0.269s, learning 0.234s)
               Value function loss: 159048.6020
                    Surrogate loss: -0.0030
             Mean action noise std: 0.99
                       Mean reward: 14496.71
               Mean episode length: 471.82
                 Mean success rate: 94.00
                  Mean reward/step: 30.74
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12034048
                    Iteration time: 0.50s
                        Total time: 723.44s
                               ETA: 262.0s

################################################################################
                     [1m Learning iteration 1469/2000 [0m

                       Computation: 16940 steps/s (collection: 0.276s, learning 0.208s)
               Value function loss: 124705.3186
                    Surrogate loss: -0.0028
             Mean action noise std: 0.99
                       Mean reward: 14291.65
               Mean episode length: 467.12
                 Mean success rate: 93.00
                  Mean reward/step: 30.25
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 0.48s
                        Total time: 723.92s
                               ETA: 261.5s

################################################################################
                     [1m Learning iteration 1470/2000 [0m

                       Computation: 17269 steps/s (collection: 0.260s, learning 0.215s)
               Value function loss: 136959.7049
                    Surrogate loss: -0.0032
             Mean action noise std: 0.99
                       Mean reward: 14195.64
               Mean episode length: 464.62
                 Mean success rate: 92.50
                  Mean reward/step: 29.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12050432
                    Iteration time: 0.47s
                        Total time: 724.39s
                               ETA: 261.0s

################################################################################
                     [1m Learning iteration 1471/2000 [0m

                       Computation: 17006 steps/s (collection: 0.273s, learning 0.208s)
               Value function loss: 77981.4923
                    Surrogate loss: -0.0039
             Mean action noise std: 0.99
                       Mean reward: 14236.37
               Mean episode length: 465.68
                 Mean success rate: 93.00
                  Mean reward/step: 29.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 0.48s
                        Total time: 724.88s
                               ETA: 260.5s

################################################################################
                     [1m Learning iteration 1472/2000 [0m

                       Computation: 17127 steps/s (collection: 0.268s, learning 0.210s)
               Value function loss: 123968.9060
                    Surrogate loss: -0.0028
             Mean action noise std: 0.99
                       Mean reward: 14168.08
               Mean episode length: 462.75
                 Mean success rate: 92.50
                  Mean reward/step: 29.78
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12066816
                    Iteration time: 0.48s
                        Total time: 725.35s
                               ETA: 260.0s

################################################################################
                     [1m Learning iteration 1473/2000 [0m

                       Computation: 17908 steps/s (collection: 0.252s, learning 0.205s)
               Value function loss: 79930.9325
                    Surrogate loss: -0.0049
             Mean action noise std: 0.99
                       Mean reward: 13982.22
               Mean episode length: 457.86
                 Mean success rate: 91.50
                  Mean reward/step: 30.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 0.46s
                        Total time: 725.81s
                               ETA: 259.5s

################################################################################
                     [1m Learning iteration 1474/2000 [0m

                       Computation: 17970 steps/s (collection: 0.253s, learning 0.203s)
               Value function loss: 63733.0717
                    Surrogate loss: -0.0037
             Mean action noise std: 0.99
                       Mean reward: 14002.05
               Mean episode length: 457.86
                 Mean success rate: 91.50
                  Mean reward/step: 31.02
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 12083200
                    Iteration time: 0.46s
                        Total time: 726.27s
                               ETA: 259.0s

################################################################################
                     [1m Learning iteration 1475/2000 [0m

                       Computation: 17845 steps/s (collection: 0.257s, learning 0.202s)
               Value function loss: 107002.1680
                    Surrogate loss: -0.0044
             Mean action noise std: 0.99
                       Mean reward: 14011.32
               Mean episode length: 458.71
                 Mean success rate: 92.00
                  Mean reward/step: 31.18
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.46s
                        Total time: 726.73s
                               ETA: 258.5s

################################################################################
                     [1m Learning iteration 1476/2000 [0m

                       Computation: 17864 steps/s (collection: 0.256s, learning 0.202s)
               Value function loss: 101614.9041
                    Surrogate loss: -0.0045
             Mean action noise std: 0.99
                       Mean reward: 13976.53
               Mean episode length: 458.85
                 Mean success rate: 92.00
                  Mean reward/step: 30.29
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12099584
                    Iteration time: 0.46s
                        Total time: 727.19s
                               ETA: 258.0s

################################################################################
                     [1m Learning iteration 1477/2000 [0m

                       Computation: 17841 steps/s (collection: 0.258s, learning 0.201s)
               Value function loss: 135610.0488
                    Surrogate loss: -0.0036
             Mean action noise std: 0.99
                       Mean reward: 13656.24
               Mean episode length: 451.24
                 Mean success rate: 90.50
                  Mean reward/step: 30.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 0.46s
                        Total time: 727.64s
                               ETA: 257.5s

################################################################################
                     [1m Learning iteration 1478/2000 [0m

                       Computation: 17111 steps/s (collection: 0.260s, learning 0.218s)
               Value function loss: 108455.5621
                    Surrogate loss: -0.0039
             Mean action noise std: 0.99
                       Mean reward: 13644.93
               Mean episode length: 451.24
                 Mean success rate: 90.50
                  Mean reward/step: 30.22
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12115968
                    Iteration time: 0.48s
                        Total time: 728.12s
                               ETA: 257.0s

################################################################################
                     [1m Learning iteration 1479/2000 [0m

                       Computation: 17060 steps/s (collection: 0.270s, learning 0.210s)
               Value function loss: 72294.4586
                    Surrogate loss: -0.0036
             Mean action noise std: 0.99
                       Mean reward: 13660.07
               Mean episode length: 451.24
                 Mean success rate: 90.50
                  Mean reward/step: 31.15
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 0.48s
                        Total time: 728.60s
                               ETA: 256.5s

################################################################################
                     [1m Learning iteration 1480/2000 [0m

                       Computation: 17350 steps/s (collection: 0.269s, learning 0.203s)
               Value function loss: 142595.9771
                    Surrogate loss: -0.0027
             Mean action noise std: 0.99
                       Mean reward: 13943.02
               Mean episode length: 458.44
                 Mean success rate: 92.00
                  Mean reward/step: 30.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12132352
                    Iteration time: 0.47s
                        Total time: 729.08s
                               ETA: 256.0s

################################################################################
                     [1m Learning iteration 1481/2000 [0m

                       Computation: 16645 steps/s (collection: 0.286s, learning 0.206s)
               Value function loss: 85268.3252
                    Surrogate loss: -0.0040
             Mean action noise std: 0.99
                       Mean reward: 13778.83
               Mean episode length: 453.54
                 Mean success rate: 91.00
                  Mean reward/step: 30.29
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 0.49s
                        Total time: 729.57s
                               ETA: 255.5s

################################################################################
                     [1m Learning iteration 1482/2000 [0m

                       Computation: 18256 steps/s (collection: 0.244s, learning 0.205s)
               Value function loss: 69267.4542
                    Surrogate loss: -0.0044
             Mean action noise std: 0.99
                       Mean reward: 13580.22
               Mean episode length: 447.69
                 Mean success rate: 90.00
                  Mean reward/step: 30.85
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12148736
                    Iteration time: 0.45s
                        Total time: 730.02s
                               ETA: 255.0s

################################################################################
                     [1m Learning iteration 1483/2000 [0m

                       Computation: 16960 steps/s (collection: 0.262s, learning 0.221s)
               Value function loss: 95196.0795
                    Surrogate loss: -0.0033
             Mean action noise std: 0.99
                       Mean reward: 13710.56
               Mean episode length: 451.52
                 Mean success rate: 90.50
                  Mean reward/step: 31.07
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 0.48s
                        Total time: 730.50s
                               ETA: 254.5s

################################################################################
                     [1m Learning iteration 1484/2000 [0m

                       Computation: 17092 steps/s (collection: 0.263s, learning 0.216s)
               Value function loss: 115775.8959
                    Surrogate loss: -0.0041
             Mean action noise std: 0.99
                       Mean reward: 13778.18
               Mean episode length: 454.44
                 Mean success rate: 91.00
                  Mean reward/step: 30.84
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12165120
                    Iteration time: 0.48s
                        Total time: 730.98s
                               ETA: 254.0s

################################################################################
                     [1m Learning iteration 1485/2000 [0m

                       Computation: 17025 steps/s (collection: 0.272s, learning 0.209s)
               Value function loss: 169150.2041
                    Surrogate loss: -0.0036
             Mean action noise std: 0.99
                       Mean reward: 13938.64
               Mean episode length: 459.34
                 Mean success rate: 92.00
                  Mean reward/step: 30.67
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 0.48s
                        Total time: 731.46s
                               ETA: 253.5s

################################################################################
                     [1m Learning iteration 1486/2000 [0m

                       Computation: 17937 steps/s (collection: 0.251s, learning 0.205s)
               Value function loss: 116271.9996
                    Surrogate loss: -0.0032
             Mean action noise std: 0.99
                       Mean reward: 14381.89
               Mean episode length: 472.19
                 Mean success rate: 94.50
                  Mean reward/step: 30.08
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12181504
                    Iteration time: 0.46s
                        Total time: 731.92s
                               ETA: 253.0s

################################################################################
                     [1m Learning iteration 1487/2000 [0m

                       Computation: 17635 steps/s (collection: 0.258s, learning 0.207s)
               Value function loss: 140147.4840
                    Surrogate loss: -0.0035
             Mean action noise std: 0.99
                       Mean reward: 14625.14
               Mean episode length: 476.96
                 Mean success rate: 95.50
                  Mean reward/step: 31.30
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.46s
                        Total time: 732.38s
                               ETA: 252.5s

################################################################################
                     [1m Learning iteration 1488/2000 [0m

                       Computation: 17754 steps/s (collection: 0.247s, learning 0.215s)
               Value function loss: 81759.9712
                    Surrogate loss: -0.0027
             Mean action noise std: 0.99
                       Mean reward: 14864.63
               Mean episode length: 481.86
                 Mean success rate: 96.50
                  Mean reward/step: 31.03
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12197888
                    Iteration time: 0.46s
                        Total time: 732.84s
                               ETA: 252.0s

################################################################################
                     [1m Learning iteration 1489/2000 [0m

                       Computation: 16971 steps/s (collection: 0.266s, learning 0.217s)
               Value function loss: 93691.1999
                    Surrogate loss: -0.0035
             Mean action noise std: 0.99
                       Mean reward: 14820.97
               Mean episode length: 480.20
                 Mean success rate: 96.50
                  Mean reward/step: 31.75
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 0.48s
                        Total time: 733.33s
                               ETA: 251.5s

################################################################################
                     [1m Learning iteration 1490/2000 [0m

                       Computation: 17877 steps/s (collection: 0.246s, learning 0.212s)
               Value function loss: 61349.6316
                    Surrogate loss: -0.0021
             Mean action noise std: 0.99
                       Mean reward: 14860.15
               Mean episode length: 480.20
                 Mean success rate: 96.50
                  Mean reward/step: 32.28
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 12214272
                    Iteration time: 0.46s
                        Total time: 733.78s
                               ETA: 251.0s

################################################################################
                     [1m Learning iteration 1491/2000 [0m

                       Computation: 17205 steps/s (collection: 0.267s, learning 0.210s)
               Value function loss: 113525.6002
                    Surrogate loss: -0.0042
             Mean action noise std: 0.99
                       Mean reward: 14800.45
               Mean episode length: 480.20
                 Mean success rate: 96.50
                  Mean reward/step: 31.83
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 0.48s
                        Total time: 734.26s
                               ETA: 250.5s

################################################################################
                     [1m Learning iteration 1492/2000 [0m

                       Computation: 17433 steps/s (collection: 0.271s, learning 0.199s)
               Value function loss: 80004.0665
                    Surrogate loss: -0.0045
             Mean action noise std: 0.99
                       Mean reward: 14712.28
               Mean episode length: 477.50
                 Mean success rate: 96.00
                  Mean reward/step: 31.15
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12230656
                    Iteration time: 0.47s
                        Total time: 734.73s
                               ETA: 250.0s

################################################################################
                     [1m Learning iteration 1493/2000 [0m

                       Computation: 17850 steps/s (collection: 0.253s, learning 0.206s)
               Value function loss: 177726.4068
                    Surrogate loss: -0.0026
             Mean action noise std: 0.99
                       Mean reward: 14944.89
               Mean episode length: 484.27
                 Mean success rate: 97.50
                  Mean reward/step: 30.91
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 0.46s
                        Total time: 735.19s
                               ETA: 249.5s

################################################################################
                     [1m Learning iteration 1494/2000 [0m

                       Computation: 18069 steps/s (collection: 0.254s, learning 0.200s)
               Value function loss: 71924.1584
                    Surrogate loss: -0.0038
             Mean action noise std: 0.99
                       Mean reward: 14881.00
               Mean episode length: 482.20
                 Mean success rate: 97.50
                  Mean reward/step: 30.58
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12247040
                    Iteration time: 0.45s
                        Total time: 735.64s
                               ETA: 249.0s

################################################################################
                     [1m Learning iteration 1495/2000 [0m

                       Computation: 17357 steps/s (collection: 0.257s, learning 0.215s)
               Value function loss: 68900.7214
                    Surrogate loss: -0.0036
             Mean action noise std: 0.99
                       Mean reward: 14691.70
               Mean episode length: 477.45
                 Mean success rate: 96.50
                  Mean reward/step: 32.15
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 0.47s
                        Total time: 736.11s
                               ETA: 248.5s

################################################################################
                     [1m Learning iteration 1496/2000 [0m

                       Computation: 16931 steps/s (collection: 0.269s, learning 0.215s)
               Value function loss: 91730.2370
                    Surrogate loss: -0.0031
             Mean action noise std: 0.99
                       Mean reward: 14651.91
               Mean episode length: 473.24
                 Mean success rate: 96.00
                  Mean reward/step: 31.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12263424
                    Iteration time: 0.48s
                        Total time: 736.60s
                               ETA: 248.0s

################################################################################
                     [1m Learning iteration 1497/2000 [0m

                       Computation: 16293 steps/s (collection: 0.281s, learning 0.222s)
               Value function loss: 117344.2824
                    Surrogate loss: -0.0023
             Mean action noise std: 0.99
                       Mean reward: 14676.16
               Mean episode length: 473.24
                 Mean success rate: 96.00
                  Mean reward/step: 32.20
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 0.50s
                        Total time: 737.10s
                               ETA: 247.5s

################################################################################
                     [1m Learning iteration 1498/2000 [0m

                       Computation: 16882 steps/s (collection: 0.269s, learning 0.217s)
               Value function loss: 83466.6832
                    Surrogate loss: -0.0028
             Mean action noise std: 0.99
                       Mean reward: 14715.93
               Mean episode length: 473.24
                 Mean success rate: 96.00
                  Mean reward/step: 32.04
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12279808
                    Iteration time: 0.49s
                        Total time: 737.59s
                               ETA: 247.0s

################################################################################
                     [1m Learning iteration 1499/2000 [0m

                       Computation: 16635 steps/s (collection: 0.279s, learning 0.213s)
               Value function loss: 132944.7965
                    Surrogate loss: -0.0027
             Mean action noise std: 0.99
                       Mean reward: 14727.09
               Mean episode length: 473.24
                 Mean success rate: 96.00
                  Mean reward/step: 31.95
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.49s
                        Total time: 738.08s
                               ETA: 246.5s

################################################################################
                     [1m Learning iteration 1500/2000 [0m

                       Computation: 16971 steps/s (collection: 0.274s, learning 0.208s)
               Value function loss: 78367.1063
                    Surrogate loss: -0.0031
             Mean action noise std: 0.99
                       Mean reward: 14752.59
               Mean episode length: 473.24
                 Mean success rate: 96.00
                  Mean reward/step: 31.47
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12296192
                    Iteration time: 0.48s
                        Total time: 738.56s
                               ETA: 246.0s

################################################################################
                     [1m Learning iteration 1501/2000 [0m

                       Computation: 16445 steps/s (collection: 0.271s, learning 0.227s)
               Value function loss: 167779.1506
                    Surrogate loss: -0.0022
             Mean action noise std: 0.99
                       Mean reward: 14991.84
               Mean episode length: 479.89
                 Mean success rate: 97.00
                  Mean reward/step: 30.58
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 0.50s
                        Total time: 739.06s
                               ETA: 245.5s

################################################################################
                     [1m Learning iteration 1502/2000 [0m

                       Computation: 16637 steps/s (collection: 0.269s, learning 0.224s)
               Value function loss: 103772.1333
                    Surrogate loss: -0.0042
             Mean action noise std: 0.99
                       Mean reward: 15006.03
               Mean episode length: 479.89
                 Mean success rate: 97.00
                  Mean reward/step: 30.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12312576
                    Iteration time: 0.49s
                        Total time: 739.55s
                               ETA: 245.0s

################################################################################
                     [1m Learning iteration 1503/2000 [0m

                       Computation: 16680 steps/s (collection: 0.274s, learning 0.217s)
               Value function loss: 143313.9033
                    Surrogate loss: -0.0031
             Mean action noise std: 0.99
                       Mean reward: 14980.41
               Mean episode length: 478.77
                 Mean success rate: 96.50
                  Mean reward/step: 31.35
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 0.49s
                        Total time: 740.04s
                               ETA: 244.5s

################################################################################
                     [1m Learning iteration 1504/2000 [0m

                       Computation: 18031 steps/s (collection: 0.255s, learning 0.199s)
               Value function loss: 83283.5658
                    Surrogate loss: -0.0014
             Mean action noise std: 0.99
                       Mean reward: 14880.08
               Mean episode length: 474.51
                 Mean success rate: 96.00
                  Mean reward/step: 31.54
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12328960
                    Iteration time: 0.45s
                        Total time: 740.50s
                               ETA: 244.0s

################################################################################
                     [1m Learning iteration 1505/2000 [0m

                       Computation: 17948 steps/s (collection: 0.248s, learning 0.208s)
               Value function loss: 81305.6322
                    Surrogate loss: -0.0040
             Mean action noise std: 0.99
                       Mean reward: 15010.47
               Mean episode length: 478.49
                 Mean success rate: 96.50
                  Mean reward/step: 31.85
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 0.46s
                        Total time: 740.95s
                               ETA: 243.5s

################################################################################
                     [1m Learning iteration 1506/2000 [0m

                       Computation: 17385 steps/s (collection: 0.264s, learning 0.207s)
               Value function loss: 87569.3895
                    Surrogate loss: -0.0040
             Mean action noise std: 0.99
                       Mean reward: 15136.09
               Mean episode length: 480.56
                 Mean success rate: 96.50
                  Mean reward/step: 32.23
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12345344
                    Iteration time: 0.47s
                        Total time: 741.42s
                               ETA: 243.0s

################################################################################
                     [1m Learning iteration 1507/2000 [0m

                       Computation: 17389 steps/s (collection: 0.250s, learning 0.221s)
               Value function loss: 109673.1745
                    Surrogate loss: -0.0046
             Mean action noise std: 0.99
                       Mean reward: 15316.11
               Mean episode length: 485.31
                 Mean success rate: 97.50
                  Mean reward/step: 31.87
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 0.47s
                        Total time: 741.90s
                               ETA: 242.5s

################################################################################
                     [1m Learning iteration 1508/2000 [0m

                       Computation: 16622 steps/s (collection: 0.281s, learning 0.211s)
               Value function loss: 131176.6926
                    Surrogate loss: -0.0027
             Mean action noise std: 0.99
                       Mean reward: 15295.31
               Mean episode length: 484.71
                 Mean success rate: 97.00
                  Mean reward/step: 31.81
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12361728
                    Iteration time: 0.49s
                        Total time: 742.39s
                               ETA: 242.1s

################################################################################
                     [1m Learning iteration 1509/2000 [0m

                       Computation: 17176 steps/s (collection: 0.274s, learning 0.203s)
               Value function loss: 127336.8961
                    Surrogate loss: -0.0040
             Mean action noise std: 0.99
                       Mean reward: 15283.88
               Mean episode length: 484.71
                 Mean success rate: 97.00
                  Mean reward/step: 30.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 0.48s
                        Total time: 742.87s
                               ETA: 241.6s

################################################################################
                     [1m Learning iteration 1510/2000 [0m

                       Computation: 16876 steps/s (collection: 0.281s, learning 0.205s)
               Value function loss: 78762.7004
                    Surrogate loss: -0.0037
             Mean action noise std: 0.99
                       Mean reward: 15270.45
               Mean episode length: 484.71
                 Mean success rate: 97.00
                  Mean reward/step: 31.37
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12378112
                    Iteration time: 0.49s
                        Total time: 743.35s
                               ETA: 241.1s

################################################################################
                     [1m Learning iteration 1511/2000 [0m

                       Computation: 15954 steps/s (collection: 0.274s, learning 0.240s)
               Value function loss: 128689.8223
                    Surrogate loss: -0.0031
             Mean action noise std: 0.99
                       Mean reward: 15082.66
               Mean episode length: 480.02
                 Mean success rate: 96.00
                  Mean reward/step: 32.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.51s
                        Total time: 743.86s
                               ETA: 240.6s

################################################################################
                     [1m Learning iteration 1512/2000 [0m

                       Computation: 15046 steps/s (collection: 0.307s, learning 0.237s)
               Value function loss: 68350.4084
                    Surrogate loss: -0.0003
             Mean action noise std: 0.99
                       Mean reward: 14919.89
               Mean episode length: 475.37
                 Mean success rate: 95.00
                  Mean reward/step: 31.72
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12394496
                    Iteration time: 0.54s
                        Total time: 744.41s
                               ETA: 240.1s

################################################################################
                     [1m Learning iteration 1513/2000 [0m

                       Computation: 16862 steps/s (collection: 0.275s, learning 0.211s)
               Value function loss: 107365.9292
                    Surrogate loss: -0.0049
             Mean action noise std: 0.99
                       Mean reward: 14857.19
               Mean episode length: 473.61
                 Mean success rate: 95.00
                  Mean reward/step: 31.86
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 0.49s
                        Total time: 744.89s
                               ETA: 239.6s

################################################################################
                     [1m Learning iteration 1514/2000 [0m

                       Computation: 16441 steps/s (collection: 0.283s, learning 0.215s)
               Value function loss: 82031.8311
                    Surrogate loss: -0.0041
             Mean action noise std: 0.99
                       Mean reward: 14713.36
               Mean episode length: 468.16
                 Mean success rate: 94.00
                  Mean reward/step: 31.81
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12410880
                    Iteration time: 0.50s
                        Total time: 745.39s
                               ETA: 239.1s

################################################################################
                     [1m Learning iteration 1515/2000 [0m

                       Computation: 17774 steps/s (collection: 0.257s, learning 0.203s)
               Value function loss: 133247.1860
                    Surrogate loss: -0.0028
             Mean action noise std: 1.00
                       Mean reward: 14914.09
               Mean episode length: 472.00
                 Mean success rate: 95.00
                  Mean reward/step: 31.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 0.46s
                        Total time: 745.85s
                               ETA: 238.6s

################################################################################
                     [1m Learning iteration 1516/2000 [0m

                       Computation: 16662 steps/s (collection: 0.264s, learning 0.227s)
               Value function loss: 153401.7477
                    Surrogate loss: -0.0030
             Mean action noise std: 1.00
                       Mean reward: 14924.28
               Mean episode length: 471.37
                 Mean success rate: 94.50
                  Mean reward/step: 30.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12427264
                    Iteration time: 0.49s
                        Total time: 746.35s
                               ETA: 238.1s

################################################################################
                     [1m Learning iteration 1517/2000 [0m

                       Computation: 16534 steps/s (collection: 0.281s, learning 0.214s)
               Value function loss: 133078.4393
                    Surrogate loss: -0.0044
             Mean action noise std: 1.00
                       Mean reward: 14839.64
               Mean episode length: 468.98
                 Mean success rate: 94.00
                  Mean reward/step: 29.89
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 0.50s
                        Total time: 746.84s
                               ETA: 237.6s

################################################################################
                     [1m Learning iteration 1518/2000 [0m

                       Computation: 17134 steps/s (collection: 0.262s, learning 0.216s)
               Value function loss: 95200.1694
                    Surrogate loss: -0.0045
             Mean action noise std: 1.00
                       Mean reward: 14775.30
               Mean episode length: 466.38
                 Mean success rate: 93.50
                  Mean reward/step: 30.22
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12443648
                    Iteration time: 0.48s
                        Total time: 747.32s
                               ETA: 237.1s

################################################################################
                     [1m Learning iteration 1519/2000 [0m

                       Computation: 15990 steps/s (collection: 0.270s, learning 0.242s)
               Value function loss: 108562.6104
                    Surrogate loss: -0.0035
             Mean action noise std: 1.00
                       Mean reward: 14622.71
               Mean episode length: 461.46
                 Mean success rate: 92.50
                  Mean reward/step: 30.66
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 0.51s
                        Total time: 747.83s
                               ETA: 236.6s

################################################################################
                     [1m Learning iteration 1520/2000 [0m

                       Computation: 16391 steps/s (collection: 0.280s, learning 0.219s)
               Value function loss: 98158.4920
                    Surrogate loss: -0.0051
             Mean action noise std: 1.00
                       Mean reward: 14519.26
               Mean episode length: 458.30
                 Mean success rate: 92.00
                  Mean reward/step: 31.21
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12460032
                    Iteration time: 0.50s
                        Total time: 748.33s
                               ETA: 236.2s

################################################################################
                     [1m Learning iteration 1521/2000 [0m

                       Computation: 16554 steps/s (collection: 0.270s, learning 0.225s)
               Value function loss: 89094.5917
                    Surrogate loss: -0.0048
             Mean action noise std: 1.00
                       Mean reward: 13966.51
               Mean episode length: 442.48
                 Mean success rate: 89.00
                  Mean reward/step: 31.37
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 0.49s
                        Total time: 748.83s
                               ETA: 235.7s

################################################################################
                     [1m Learning iteration 1522/2000 [0m

                       Computation: 16742 steps/s (collection: 0.279s, learning 0.210s)
               Value function loss: 105373.1793
                    Surrogate loss: -0.0034
             Mean action noise std: 1.00
                       Mean reward: 13975.51
               Mean episode length: 442.67
                 Mean success rate: 89.50
                  Mean reward/step: 31.60
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12476416
                    Iteration time: 0.49s
                        Total time: 749.31s
                               ETA: 235.2s

################################################################################
                     [1m Learning iteration 1523/2000 [0m

                       Computation: 16421 steps/s (collection: 0.278s, learning 0.221s)
               Value function loss: 107521.2950
                    Surrogate loss: -0.0030
             Mean action noise std: 1.00
                       Mean reward: 14094.41
               Mean episode length: 447.31
                 Mean success rate: 90.50
                  Mean reward/step: 31.59
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.50s
                        Total time: 749.81s
                               ETA: 234.7s

################################################################################
                     [1m Learning iteration 1524/2000 [0m

                       Computation: 16193 steps/s (collection: 0.283s, learning 0.223s)
               Value function loss: 174055.0611
                    Surrogate loss: -0.0016
             Mean action noise std: 1.00
                       Mean reward: 14407.22
               Mean episode length: 456.90
                 Mean success rate: 92.00
                  Mean reward/step: 30.96
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12492800
                    Iteration time: 0.51s
                        Total time: 750.32s
                               ETA: 234.2s

################################################################################
                     [1m Learning iteration 1525/2000 [0m

                       Computation: 15926 steps/s (collection: 0.303s, learning 0.212s)
               Value function loss: 96228.9502
                    Surrogate loss: -0.0042
             Mean action noise std: 1.00
                       Mean reward: 14298.33
               Mean episode length: 454.02
                 Mean success rate: 91.50
                  Mean reward/step: 30.47
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 0.51s
                        Total time: 750.83s
                               ETA: 233.7s

################################################################################
                     [1m Learning iteration 1526/2000 [0m

                       Computation: 15372 steps/s (collection: 0.295s, learning 0.238s)
               Value function loss: 61615.2359
                    Surrogate loss: -0.0039
             Mean action noise std: 1.00
                       Mean reward: 14211.17
               Mean episode length: 453.38
                 Mean success rate: 91.50
                  Mean reward/step: 31.43
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 12509184
                    Iteration time: 0.53s
                        Total time: 751.37s
                               ETA: 233.2s

################################################################################
                     [1m Learning iteration 1527/2000 [0m

                       Computation: 16490 steps/s (collection: 0.289s, learning 0.207s)
               Value function loss: 142517.4999
                    Surrogate loss: -0.0030
             Mean action noise std: 1.00
                       Mean reward: 14092.38
               Mean episode length: 450.98
                 Mean success rate: 91.00
                  Mean reward/step: 31.33
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 0.50s
                        Total time: 751.86s
                               ETA: 232.7s

################################################################################
                     [1m Learning iteration 1528/2000 [0m

                       Computation: 16826 steps/s (collection: 0.275s, learning 0.211s)
               Value function loss: 78970.7488
                    Surrogate loss: -0.0047
             Mean action noise std: 1.00
                       Mean reward: 13897.40
               Mean episode length: 445.26
                 Mean success rate: 90.00
                  Mean reward/step: 31.02
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12525568
                    Iteration time: 0.49s
                        Total time: 752.35s
                               ETA: 232.2s

################################################################################
                     [1m Learning iteration 1529/2000 [0m

                       Computation: 17229 steps/s (collection: 0.267s, learning 0.209s)
               Value function loss: 80090.9267
                    Surrogate loss: -0.0035
             Mean action noise std: 1.00
                       Mean reward: 13860.41
               Mean episode length: 445.26
                 Mean success rate: 90.00
                  Mean reward/step: 31.44
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 0.48s
                        Total time: 752.83s
                               ETA: 231.8s

################################################################################
                     [1m Learning iteration 1530/2000 [0m

                       Computation: 17489 steps/s (collection: 0.262s, learning 0.206s)
               Value function loss: 106289.1367
                    Surrogate loss: -0.0033
             Mean action noise std: 1.00
                       Mean reward: 13927.66
               Mean episode length: 447.87
                 Mean success rate: 90.50
                  Mean reward/step: 31.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12541952
                    Iteration time: 0.47s
                        Total time: 753.29s
                               ETA: 231.3s

################################################################################
                     [1m Learning iteration 1531/2000 [0m

                       Computation: 17560 steps/s (collection: 0.263s, learning 0.203s)
               Value function loss: 101210.9940
                    Surrogate loss: -0.0046
             Mean action noise std: 1.00
                       Mean reward: 14012.48
               Mean episode length: 451.10
                 Mean success rate: 91.50
                  Mean reward/step: 31.13
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 0.47s
                        Total time: 753.76s
                               ETA: 230.8s

################################################################################
                     [1m Learning iteration 1532/2000 [0m

                       Computation: 16213 steps/s (collection: 0.294s, learning 0.212s)
               Value function loss: 147343.4708
                    Surrogate loss: -0.0023
             Mean action noise std: 1.00
                       Mean reward: 14306.44
               Mean episode length: 461.06
                 Mean success rate: 93.50
                  Mean reward/step: 31.32
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12558336
                    Iteration time: 0.51s
                        Total time: 754.27s
                               ETA: 230.3s

################################################################################
                     [1m Learning iteration 1533/2000 [0m

                       Computation: 16077 steps/s (collection: 0.274s, learning 0.235s)
               Value function loss: 136283.8300
                    Surrogate loss: -0.0039
             Mean action noise std: 1.00
                       Mean reward: 14761.51
               Mean episode length: 474.59
                 Mean success rate: 95.50
                  Mean reward/step: 30.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 0.51s
                        Total time: 754.78s
                               ETA: 229.8s

################################################################################
                     [1m Learning iteration 1534/2000 [0m

                       Computation: 16931 steps/s (collection: 0.275s, learning 0.208s)
               Value function loss: 123833.3209
                    Surrogate loss: -0.0042
             Mean action noise std: 1.00
                       Mean reward: 14522.01
               Mean episode length: 466.38
                 Mean success rate: 94.00
                  Mean reward/step: 30.54
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12574720
                    Iteration time: 0.48s
                        Total time: 755.26s
                               ETA: 229.3s

################################################################################
                     [1m Learning iteration 1535/2000 [0m

                       Computation: 17809 steps/s (collection: 0.252s, learning 0.208s)
               Value function loss: 103349.4301
                    Surrogate loss: -0.0039
             Mean action noise std: 1.00
                       Mean reward: 14342.80
               Mean episode length: 461.86
                 Mean success rate: 93.00
                  Mean reward/step: 30.21
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.46s
                        Total time: 755.72s
                               ETA: 228.8s

################################################################################
                     [1m Learning iteration 1536/2000 [0m

                       Computation: 16887 steps/s (collection: 0.269s, learning 0.216s)
               Value function loss: 115253.6034
                    Surrogate loss: -0.0032
             Mean action noise std: 1.00
                       Mean reward: 13959.04
               Mean episode length: 451.58
                 Mean success rate: 91.50
                  Mean reward/step: 30.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12591104
                    Iteration time: 0.49s
                        Total time: 756.20s
                               ETA: 228.3s

################################################################################
                     [1m Learning iteration 1537/2000 [0m

                       Computation: 17279 steps/s (collection: 0.261s, learning 0.213s)
               Value function loss: 97184.7730
                    Surrogate loss: -0.0033
             Mean action noise std: 1.00
                       Mean reward: 14071.91
               Mean episode length: 453.13
                 Mean success rate: 92.00
                  Mean reward/step: 30.90
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 0.47s
                        Total time: 756.68s
                               ETA: 227.8s

################################################################################
                     [1m Learning iteration 1538/2000 [0m

                       Computation: 16417 steps/s (collection: 0.259s, learning 0.240s)
               Value function loss: 109108.3832
                    Surrogate loss: -0.0037
             Mean action noise std: 1.00
                       Mean reward: 14174.97
               Mean episode length: 455.62
                 Mean success rate: 92.50
                  Mean reward/step: 31.35
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12607488
                    Iteration time: 0.50s
                        Total time: 757.18s
                               ETA: 227.3s

################################################################################
                     [1m Learning iteration 1539/2000 [0m

                       Computation: 16029 steps/s (collection: 0.268s, learning 0.244s)
               Value function loss: 98306.9796
                    Surrogate loss: -0.0043
             Mean action noise std: 1.00
                       Mean reward: 14269.66
               Mean episode length: 458.83
                 Mean success rate: 93.00
                  Mean reward/step: 31.42
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 0.51s
                        Total time: 757.69s
                               ETA: 226.8s

################################################################################
                     [1m Learning iteration 1540/2000 [0m

                       Computation: 17776 steps/s (collection: 0.261s, learning 0.200s)
               Value function loss: 149296.7657
                    Surrogate loss: -0.0023
             Mean action noise std: 1.00
                       Mean reward: 14090.37
               Mean episode length: 454.32
                 Mean success rate: 92.00
                  Mean reward/step: 30.74
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12623872
                    Iteration time: 0.46s
                        Total time: 758.15s
                               ETA: 226.3s

################################################################################
                     [1m Learning iteration 1541/2000 [0m

                       Computation: 17750 steps/s (collection: 0.253s, learning 0.208s)
               Value function loss: 76874.5167
                    Surrogate loss: -0.0033
             Mean action noise std: 1.00
                       Mean reward: 14142.83
               Mean episode length: 456.18
                 Mean success rate: 92.00
                  Mean reward/step: 30.89
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 0.46s
                        Total time: 758.61s
                               ETA: 225.8s

################################################################################
                     [1m Learning iteration 1542/2000 [0m

                       Computation: 17507 steps/s (collection: 0.265s, learning 0.203s)
               Value function loss: 96499.2273
                    Surrogate loss: -0.0030
             Mean action noise std: 1.00
                       Mean reward: 14181.74
               Mean episode length: 456.73
                 Mean success rate: 92.50
                  Mean reward/step: 32.09
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12640256
                    Iteration time: 0.47s
                        Total time: 759.08s
                               ETA: 225.3s

################################################################################
                     [1m Learning iteration 1543/2000 [0m

                       Computation: 16736 steps/s (collection: 0.269s, learning 0.221s)
               Value function loss: 86476.8166
                    Surrogate loss: -0.0025
             Mean action noise std: 1.00
                       Mean reward: 13966.04
               Mean episode length: 449.00
                 Mean success rate: 91.50
                  Mean reward/step: 31.06
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 0.49s
                        Total time: 759.57s
                               ETA: 224.8s

################################################################################
                     [1m Learning iteration 1544/2000 [0m

                       Computation: 16697 steps/s (collection: 0.286s, learning 0.205s)
               Value function loss: 118616.5688
                    Surrogate loss: -0.0042
             Mean action noise std: 1.00
                       Mean reward: 13832.08
               Mean episode length: 444.11
                 Mean success rate: 90.50
                  Mean reward/step: 30.95
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12656640
                    Iteration time: 0.49s
                        Total time: 760.06s
                               ETA: 224.3s

################################################################################
                     [1m Learning iteration 1545/2000 [0m

                       Computation: 17620 steps/s (collection: 0.255s, learning 0.210s)
               Value function loss: 93308.7706
                    Surrogate loss: -0.0037
             Mean action noise std: 1.00
                       Mean reward: 13974.65
               Mean episode length: 448.65
                 Mean success rate: 91.50
                  Mean reward/step: 31.52
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 0.46s
                        Total time: 760.52s
                               ETA: 223.8s

################################################################################
                     [1m Learning iteration 1546/2000 [0m

                       Computation: 16516 steps/s (collection: 0.283s, learning 0.213s)
               Value function loss: 106719.4667
                    Surrogate loss: -0.0039
             Mean action noise std: 1.00
                       Mean reward: 13823.72
               Mean episode length: 444.34
                 Mean success rate: 90.50
                  Mean reward/step: 31.57
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12673024
                    Iteration time: 0.50s
                        Total time: 761.02s
                               ETA: 223.3s

################################################################################
                     [1m Learning iteration 1547/2000 [0m

                       Computation: 17308 steps/s (collection: 0.265s, learning 0.209s)
               Value function loss: 96394.0305
                    Surrogate loss: -0.0047
             Mean action noise std: 1.00
                       Mean reward: 14129.31
               Mean episode length: 453.26
                 Mean success rate: 92.50
                  Mean reward/step: 31.30
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.47s
                        Total time: 761.49s
                               ETA: 222.8s

################################################################################
                     [1m Learning iteration 1548/2000 [0m

                       Computation: 17315 steps/s (collection: 0.263s, learning 0.210s)
               Value function loss: 136376.5918
                    Surrogate loss: -0.0033
             Mean action noise std: 1.00
                       Mean reward: 14372.23
               Mean episode length: 462.03
                 Mean success rate: 93.50
                  Mean reward/step: 30.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12689408
                    Iteration time: 0.47s
                        Total time: 761.97s
                               ETA: 222.3s

################################################################################
                     [1m Learning iteration 1549/2000 [0m

                       Computation: 17369 steps/s (collection: 0.257s, learning 0.215s)
               Value function loss: 105492.7561
                    Surrogate loss: -0.0035
             Mean action noise std: 1.00
                       Mean reward: 14423.87
               Mean episode length: 465.92
                 Mean success rate: 94.00
                  Mean reward/step: 30.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 0.47s
                        Total time: 762.44s
                               ETA: 221.8s

################################################################################
                     [1m Learning iteration 1550/2000 [0m

                       Computation: 16321 steps/s (collection: 0.279s, learning 0.223s)
               Value function loss: 132168.1485
                    Surrogate loss: -0.0039
             Mean action noise std: 1.00
                       Mean reward: 14377.21
               Mean episode length: 463.85
                 Mean success rate: 93.50
                  Mean reward/step: 30.45
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12705792
                    Iteration time: 0.50s
                        Total time: 762.94s
                               ETA: 221.4s

################################################################################
                     [1m Learning iteration 1551/2000 [0m

                       Computation: 16569 steps/s (collection: 0.270s, learning 0.225s)
               Value function loss: 106588.7830
                    Surrogate loss: -0.0038
             Mean action noise std: 1.00
                       Mean reward: 14325.24
               Mean episode length: 461.94
                 Mean success rate: 93.50
                  Mean reward/step: 30.07
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 0.49s
                        Total time: 763.43s
                               ETA: 220.9s

################################################################################
                     [1m Learning iteration 1552/2000 [0m

                       Computation: 14912 steps/s (collection: 0.289s, learning 0.261s)
               Value function loss: 96755.5160
                    Surrogate loss: -0.0032
             Mean action noise std: 1.00
                       Mean reward: 14488.92
               Mean episode length: 466.44
                 Mean success rate: 94.50
                  Mean reward/step: 30.24
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12722176
                    Iteration time: 0.55s
                        Total time: 763.98s
                               ETA: 220.4s

################################################################################
                     [1m Learning iteration 1553/2000 [0m

                       Computation: 14423 steps/s (collection: 0.301s, learning 0.267s)
               Value function loss: 86959.8339
                    Surrogate loss: -0.0049
             Mean action noise std: 1.00
                       Mean reward: 14332.54
               Mean episode length: 463.34
                 Mean success rate: 94.00
                  Mean reward/step: 30.87
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 0.57s
                        Total time: 764.55s
                               ETA: 219.9s

################################################################################
                     [1m Learning iteration 1554/2000 [0m

                       Computation: 14873 steps/s (collection: 0.314s, learning 0.237s)
               Value function loss: 114234.5328
                    Surrogate loss: -0.0037
             Mean action noise std: 1.00
                       Mean reward: 14538.92
               Mean episode length: 470.51
                 Mean success rate: 95.00
                  Mean reward/step: 30.44
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12738560
                    Iteration time: 0.55s
                        Total time: 765.10s
                               ETA: 219.4s

################################################################################
                     [1m Learning iteration 1555/2000 [0m

                       Computation: 14733 steps/s (collection: 0.290s, learning 0.266s)
               Value function loss: 163331.3957
                    Surrogate loss: -0.0027
             Mean action noise std: 1.00
                       Mean reward: 14844.30
               Mean episode length: 480.05
                 Mean success rate: 96.50
                  Mean reward/step: 29.98
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 0.56s
                        Total time: 765.66s
                               ETA: 219.0s

################################################################################
                     [1m Learning iteration 1556/2000 [0m

                       Computation: 14538 steps/s (collection: 0.300s, learning 0.264s)
               Value function loss: 101442.1599
                    Surrogate loss: -0.0037
             Mean action noise std: 1.00
                       Mean reward: 14817.40
               Mean episode length: 479.16
                 Mean success rate: 96.00
                  Mean reward/step: 29.21
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12754944
                    Iteration time: 0.56s
                        Total time: 766.22s
                               ETA: 218.5s

################################################################################
                     [1m Learning iteration 1557/2000 [0m

                       Computation: 15473 steps/s (collection: 0.289s, learning 0.241s)
               Value function loss: 57964.8080
                    Surrogate loss: -0.0038
             Mean action noise std: 1.00
                       Mean reward: 14801.24
               Mean episode length: 479.16
                 Mean success rate: 96.00
                  Mean reward/step: 30.52
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 0.53s
                        Total time: 766.75s
                               ETA: 218.0s

################################################################################
                     [1m Learning iteration 1558/2000 [0m

                       Computation: 15747 steps/s (collection: 0.294s, learning 0.226s)
               Value function loss: 157077.5465
                    Surrogate loss: -0.0029
             Mean action noise std: 1.00
                       Mean reward: 14804.17
               Mean episode length: 478.60
                 Mean success rate: 96.00
                  Mean reward/step: 31.17
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12771328
                    Iteration time: 0.52s
                        Total time: 767.27s
                               ETA: 217.5s

################################################################################
                     [1m Learning iteration 1559/2000 [0m

                       Computation: 14236 steps/s (collection: 0.309s, learning 0.266s)
               Value function loss: 80890.9582
                    Surrogate loss: -0.0030
             Mean action noise std: 1.00
                       Mean reward: 14580.54
               Mean episode length: 470.86
                 Mean success rate: 95.00
                  Mean reward/step: 30.34
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.58s
                        Total time: 767.85s
                               ETA: 217.1s

################################################################################
                     [1m Learning iteration 1560/2000 [0m

                       Computation: 16865 steps/s (collection: 0.276s, learning 0.210s)
               Value function loss: 107488.2616
                    Surrogate loss: -0.0042
             Mean action noise std: 1.00
                       Mean reward: 14428.40
               Mean episode length: 465.96
                 Mean success rate: 94.00
                  Mean reward/step: 30.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12787712
                    Iteration time: 0.49s
                        Total time: 768.33s
                               ETA: 216.6s

################################################################################
                     [1m Learning iteration 1561/2000 [0m

                       Computation: 17183 steps/s (collection: 0.268s, learning 0.208s)
               Value function loss: 81284.2559
                    Surrogate loss: -0.0044
             Mean action noise std: 1.00
                       Mean reward: 14225.45
               Mean episode length: 459.63
                 Mean success rate: 93.00
                  Mean reward/step: 30.86
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 0.48s
                        Total time: 768.81s
                               ETA: 216.1s

################################################################################
                     [1m Learning iteration 1562/2000 [0m

                       Computation: 16399 steps/s (collection: 0.297s, learning 0.203s)
               Value function loss: 95636.6342
                    Surrogate loss: -0.0037
             Mean action noise std: 1.00
                       Mean reward: 14459.74
               Mean episode length: 466.60
                 Mean success rate: 94.50
                  Mean reward/step: 31.04
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12804096
                    Iteration time: 0.50s
                        Total time: 769.31s
                               ETA: 215.6s

################################################################################
                     [1m Learning iteration 1563/2000 [0m

                       Computation: 16893 steps/s (collection: 0.275s, learning 0.210s)
               Value function loss: 129499.0322
                    Surrogate loss: -0.0039
             Mean action noise std: 1.00
                       Mean reward: 14520.84
               Mean episode length: 468.50
                 Mean success rate: 94.50
                  Mean reward/step: 31.01
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 0.48s
                        Total time: 769.79s
                               ETA: 215.1s

################################################################################
                     [1m Learning iteration 1564/2000 [0m

                       Computation: 16926 steps/s (collection: 0.276s, learning 0.208s)
               Value function loss: 129933.4391
                    Surrogate loss: -0.0039
             Mean action noise std: 1.00
                       Mean reward: 14566.83
               Mean episode length: 469.12
                 Mean success rate: 94.50
                  Mean reward/step: 30.51
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12820480
                    Iteration time: 0.48s
                        Total time: 770.28s
                               ETA: 214.6s

################################################################################
                     [1m Learning iteration 1565/2000 [0m

                       Computation: 16882 steps/s (collection: 0.273s, learning 0.213s)
               Value function loss: 89032.1788
                    Surrogate loss: -0.0030
             Mean action noise std: 1.00
                       Mean reward: 14590.78
               Mean episode length: 471.61
                 Mean success rate: 95.00
                  Mean reward/step: 30.42
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 0.49s
                        Total time: 770.76s
                               ETA: 214.1s

################################################################################
                     [1m Learning iteration 1566/2000 [0m

                       Computation: 16970 steps/s (collection: 0.279s, learning 0.204s)
               Value function loss: 114193.5935
                    Surrogate loss: -0.0043
             Mean action noise std: 1.00
                       Mean reward: 14343.08
               Mean episode length: 465.24
                 Mean success rate: 94.00
                  Mean reward/step: 30.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12836864
                    Iteration time: 0.48s
                        Total time: 771.25s
                               ETA: 213.6s

################################################################################
                     [1m Learning iteration 1567/2000 [0m

                       Computation: 17872 steps/s (collection: 0.247s, learning 0.212s)
               Value function loss: 103898.3857
                    Surrogate loss: -0.0032
             Mean action noise std: 1.00
                       Mean reward: 14258.13
               Mean episode length: 462.75
                 Mean success rate: 93.50
                  Mean reward/step: 30.34
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 0.46s
                        Total time: 771.70s
                               ETA: 213.1s

################################################################################
                     [1m Learning iteration 1568/2000 [0m

                       Computation: 15980 steps/s (collection: 0.276s, learning 0.237s)
               Value function loss: 59958.2276
                    Surrogate loss: -0.0027
             Mean action noise std: 1.00
                       Mean reward: 14390.10
               Mean episode length: 467.29
                 Mean success rate: 94.50
                  Mean reward/step: 31.10
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12853248
                    Iteration time: 0.51s
                        Total time: 772.22s
                               ETA: 212.6s

################################################################################
                     [1m Learning iteration 1569/2000 [0m

                       Computation: 15784 steps/s (collection: 0.256s, learning 0.263s)
               Value function loss: 95324.3212
                    Surrogate loss: -0.0035
             Mean action noise std: 1.00
                       Mean reward: 14451.05
               Mean episode length: 472.17
                 Mean success rate: 95.50
                  Mean reward/step: 31.72
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 0.52s
                        Total time: 772.74s
                               ETA: 212.1s

################################################################################
                     [1m Learning iteration 1570/2000 [0m

                       Computation: 17156 steps/s (collection: 0.265s, learning 0.213s)
               Value function loss: 91119.5184
                    Surrogate loss: -0.0036
             Mean action noise std: 1.00
                       Mean reward: 14416.79
               Mean episode length: 472.17
                 Mean success rate: 95.50
                  Mean reward/step: 31.88
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12869632
                    Iteration time: 0.48s
                        Total time: 773.21s
                               ETA: 211.6s

################################################################################
                     [1m Learning iteration 1571/2000 [0m

                       Computation: 16350 steps/s (collection: 0.264s, learning 0.237s)
               Value function loss: 164143.9146
                    Surrogate loss: -0.0021
             Mean action noise std: 1.00
                       Mean reward: 14643.90
               Mean episode length: 479.91
                 Mean success rate: 96.50
                  Mean reward/step: 31.09
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.50s
                        Total time: 773.71s
                               ETA: 211.1s

################################################################################
                     [1m Learning iteration 1572/2000 [0m

                       Computation: 15625 steps/s (collection: 0.271s, learning 0.253s)
               Value function loss: 79942.3337
                    Surrogate loss: -0.0044
             Mean action noise std: 1.00
                       Mean reward: 14814.01
               Mean episode length: 484.81
                 Mean success rate: 97.50
                  Mean reward/step: 30.37
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12886016
                    Iteration time: 0.52s
                        Total time: 774.24s
                               ETA: 210.7s

################################################################################
                     [1m Learning iteration 1573/2000 [0m

                       Computation: 16201 steps/s (collection: 0.259s, learning 0.247s)
               Value function loss: 58487.9234
                    Surrogate loss: -0.0027
             Mean action noise std: 1.00
                       Mean reward: 14898.37
               Mean episode length: 486.43
                 Mean success rate: 97.50
                  Mean reward/step: 31.87
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 0.51s
                        Total time: 774.74s
                               ETA: 210.2s

################################################################################
                     [1m Learning iteration 1574/2000 [0m

                       Computation: 17499 steps/s (collection: 0.262s, learning 0.206s)
               Value function loss: 131134.5703
                    Surrogate loss: -0.0026
             Mean action noise std: 1.00
                       Mean reward: 14887.42
               Mean episode length: 486.43
                 Mean success rate: 97.50
                  Mean reward/step: 31.62
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12902400
                    Iteration time: 0.47s
                        Total time: 775.21s
                               ETA: 209.7s

################################################################################
                     [1m Learning iteration 1575/2000 [0m

                       Computation: 17242 steps/s (collection: 0.262s, learning 0.213s)
               Value function loss: 91830.3944
                    Surrogate loss: -0.0039
             Mean action noise std: 1.00
                       Mean reward: 14925.88
               Mean episode length: 486.43
                 Mean success rate: 97.50
                  Mean reward/step: 31.41
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 0.48s
                        Total time: 775.69s
                               ETA: 209.2s

################################################################################
                     [1m Learning iteration 1576/2000 [0m

                       Computation: 17352 steps/s (collection: 0.266s, learning 0.206s)
               Value function loss: 75499.0027
                    Surrogate loss: -0.0032
             Mean action noise std: 1.00
                       Mean reward: 14939.78
               Mean episode length: 486.43
                 Mean success rate: 97.50
                  Mean reward/step: 31.55
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12918784
                    Iteration time: 0.47s
                        Total time: 776.16s
                               ETA: 208.7s

################################################################################
                     [1m Learning iteration 1577/2000 [0m

                       Computation: 17429 steps/s (collection: 0.256s, learning 0.214s)
               Value function loss: 117654.2719
                    Surrogate loss: -0.0039
             Mean action noise std: 1.00
                       Mean reward: 14964.58
               Mean episode length: 486.43
                 Mean success rate: 97.50
                  Mean reward/step: 31.52
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 0.47s
                        Total time: 776.63s
                               ETA: 208.2s

################################################################################
                     [1m Learning iteration 1578/2000 [0m

                       Computation: 18618 steps/s (collection: 0.243s, learning 0.197s)
               Value function loss: 85863.0719
                    Surrogate loss: -0.0039
             Mean action noise std: 1.00
                       Mean reward: 14993.55
               Mean episode length: 486.43
                 Mean success rate: 97.50
                  Mean reward/step: 30.66
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12935168
                    Iteration time: 0.44s
                        Total time: 777.07s
                               ETA: 207.7s

################################################################################
                     [1m Learning iteration 1579/2000 [0m

                       Computation: 18303 steps/s (collection: 0.250s, learning 0.198s)
               Value function loss: 120127.1539
                    Surrogate loss: -0.0010
             Mean action noise std: 1.00
                       Mean reward: 15288.29
               Mean episode length: 495.30
                 Mean success rate: 99.00
                  Mean reward/step: 30.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 0.45s
                        Total time: 777.52s
                               ETA: 207.2s

################################################################################
                     [1m Learning iteration 1580/2000 [0m

                       Computation: 17346 steps/s (collection: 0.272s, learning 0.200s)
               Value function loss: 121376.8450
                    Surrogate loss: -0.0024
             Mean action noise std: 1.00
                       Mean reward: 15296.26
               Mean episode length: 495.30
                 Mean success rate: 99.00
                  Mean reward/step: 30.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12951552
                    Iteration time: 0.47s
                        Total time: 777.99s
                               ETA: 206.7s

################################################################################
                     [1m Learning iteration 1581/2000 [0m

                       Computation: 18082 steps/s (collection: 0.252s, learning 0.201s)
               Value function loss: 130068.0309
                    Surrogate loss: -0.0032
             Mean action noise std: 1.00
                       Mean reward: 15352.02
               Mean episode length: 495.30
                 Mean success rate: 99.00
                  Mean reward/step: 31.13
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 0.45s
                        Total time: 778.44s
                               ETA: 206.2s

################################################################################
                     [1m Learning iteration 1582/2000 [0m

                       Computation: 17919 steps/s (collection: 0.256s, learning 0.201s)
               Value function loss: 134928.7881
                    Surrogate loss: -0.0022
             Mean action noise std: 1.00
                       Mean reward: 15435.21
               Mean episode length: 495.30
                 Mean success rate: 99.00
                  Mean reward/step: 30.81
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12967936
                    Iteration time: 0.46s
                        Total time: 778.90s
                               ETA: 205.7s

################################################################################
                     [1m Learning iteration 1583/2000 [0m

                       Computation: 17183 steps/s (collection: 0.243s, learning 0.234s)
               Value function loss: 106231.1782
                    Surrogate loss: -0.0033
             Mean action noise std: 1.00
                       Mean reward: 15450.92
               Mean episode length: 495.30
                 Mean success rate: 99.00
                  Mean reward/step: 31.10
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.48s
                        Total time: 779.38s
                               ETA: 205.2s

################################################################################
                     [1m Learning iteration 1584/2000 [0m

                       Computation: 17879 steps/s (collection: 0.241s, learning 0.217s)
               Value function loss: 80600.9375
                    Surrogate loss: -0.0018
             Mean action noise std: 1.00
                       Mean reward: 15444.73
               Mean episode length: 495.30
                 Mean success rate: 99.00
                  Mean reward/step: 31.34
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12984320
                    Iteration time: 0.46s
                        Total time: 779.83s
                               ETA: 204.7s

################################################################################
                     [1m Learning iteration 1585/2000 [0m

                       Computation: 16443 steps/s (collection: 0.260s, learning 0.238s)
               Value function loss: 89285.6153
                    Surrogate loss: -0.0038
             Mean action noise std: 1.00
                       Mean reward: 15466.94
               Mean episode length: 496.58
                 Mean success rate: 99.50
                  Mean reward/step: 31.24
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 0.50s
                        Total time: 780.33s
                               ETA: 204.2s

################################################################################
                     [1m Learning iteration 1586/2000 [0m

                       Computation: 16339 steps/s (collection: 0.284s, learning 0.218s)
               Value function loss: 91348.5946
                    Surrogate loss: -0.0027
             Mean action noise std: 1.00
                       Mean reward: 15448.19
               Mean episode length: 496.58
                 Mean success rate: 99.50
                  Mean reward/step: 30.88
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13000704
                    Iteration time: 0.50s
                        Total time: 780.83s
                               ETA: 203.7s

################################################################################
                     [1m Learning iteration 1587/2000 [0m

                       Computation: 17976 steps/s (collection: 0.251s, learning 0.204s)
               Value function loss: 145338.1848
                    Surrogate loss: -0.0039
             Mean action noise std: 1.00
                       Mean reward: 15433.15
               Mean episode length: 496.58
                 Mean success rate: 99.50
                  Mean reward/step: 29.76
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 0.46s
                        Total time: 781.29s
                               ETA: 203.2s

################################################################################
                     [1m Learning iteration 1588/2000 [0m

                       Computation: 17140 steps/s (collection: 0.274s, learning 0.204s)
               Value function loss: 73432.0546
                    Surrogate loss: -0.0036
             Mean action noise std: 1.00
                       Mean reward: 15298.53
               Mean episode length: 491.85
                 Mean success rate: 98.50
                  Mean reward/step: 30.21
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13017088
                    Iteration time: 0.48s
                        Total time: 781.77s
                               ETA: 202.7s

################################################################################
                     [1m Learning iteration 1589/2000 [0m

                       Computation: 17142 steps/s (collection: 0.270s, learning 0.208s)
               Value function loss: 105530.2601
                    Surrogate loss: -0.0024
             Mean action noise std: 1.00
                       Mean reward: 15317.69
               Mean episode length: 491.85
                 Mean success rate: 98.50
                  Mean reward/step: 31.55
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 0.48s
                        Total time: 782.25s
                               ETA: 202.2s

################################################################################
                     [1m Learning iteration 1590/2000 [0m

                       Computation: 17218 steps/s (collection: 0.262s, learning 0.214s)
               Value function loss: 90532.4891
                    Surrogate loss: -0.0036
             Mean action noise std: 1.00
                       Mean reward: 15151.24
               Mean episode length: 488.23
                 Mean success rate: 98.00
                  Mean reward/step: 30.53
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13033472
                    Iteration time: 0.48s
                        Total time: 782.72s
                               ETA: 201.7s

################################################################################
                     [1m Learning iteration 1591/2000 [0m

                       Computation: 16537 steps/s (collection: 0.271s, learning 0.224s)
               Value function loss: 94207.7858
                    Surrogate loss: -0.0027
             Mean action noise std: 1.00
                       Mean reward: 15143.73
               Mean episode length: 488.23
                 Mean success rate: 98.00
                  Mean reward/step: 31.00
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 0.50s
                        Total time: 783.22s
                               ETA: 201.2s

################################################################################
                     [1m Learning iteration 1592/2000 [0m

                       Computation: 16515 steps/s (collection: 0.292s, learning 0.204s)
               Value function loss: 106304.6945
                    Surrogate loss: -0.0023
             Mean action noise std: 1.00
                       Mean reward: 15093.84
               Mean episode length: 488.23
                 Mean success rate: 98.00
                  Mean reward/step: 31.09
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13049856
                    Iteration time: 0.50s
                        Total time: 783.71s
                               ETA: 200.7s

################################################################################
                     [1m Learning iteration 1593/2000 [0m

                       Computation: 16371 steps/s (collection: 0.282s, learning 0.219s)
               Value function loss: 115652.4910
                    Surrogate loss: -0.0046
             Mean action noise std: 1.00
                       Mean reward: 14988.88
               Mean episode length: 484.15
                 Mean success rate: 97.50
                  Mean reward/step: 30.78
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 0.50s
                        Total time: 784.21s
                               ETA: 200.2s

################################################################################
                     [1m Learning iteration 1594/2000 [0m

                       Computation: 16767 steps/s (collection: 0.280s, learning 0.209s)
               Value function loss: 102109.6687
                    Surrogate loss: -0.0044
             Mean action noise std: 1.00
                       Mean reward: 14640.94
               Mean episode length: 475.02
                 Mean success rate: 95.50
                  Mean reward/step: 30.98
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13066240
                    Iteration time: 0.49s
                        Total time: 784.70s
                               ETA: 199.7s

################################################################################
                     [1m Learning iteration 1595/2000 [0m

                       Computation: 17077 steps/s (collection: 0.280s, learning 0.200s)
               Value function loss: 116079.7655
                    Surrogate loss: -0.0032
             Mean action noise std: 1.00
                       Mean reward: 14518.74
               Mean episode length: 470.29
                 Mean success rate: 94.50
                  Mean reward/step: 30.37
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.48s
                        Total time: 785.18s
                               ETA: 199.2s

################################################################################
                     [1m Learning iteration 1596/2000 [0m

                       Computation: 17090 steps/s (collection: 0.275s, learning 0.204s)
               Value function loss: 98593.8529
                    Surrogate loss: -0.0047
             Mean action noise std: 1.00
                       Mean reward: 14532.50
               Mean episode length: 470.29
                 Mean success rate: 94.50
                  Mean reward/step: 30.15
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13082624
                    Iteration time: 0.48s
                        Total time: 785.66s
                               ETA: 198.8s

################################################################################
                     [1m Learning iteration 1597/2000 [0m

                       Computation: 17370 steps/s (collection: 0.267s, learning 0.205s)
               Value function loss: 125668.2709
                    Surrogate loss: -0.0041
             Mean action noise std: 1.00
                       Mean reward: 14541.52
               Mean episode length: 468.81
                 Mean success rate: 94.00
                  Mean reward/step: 29.91
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 0.47s
                        Total time: 786.13s
                               ETA: 198.3s

################################################################################
                     [1m Learning iteration 1598/2000 [0m

                       Computation: 17629 steps/s (collection: 0.265s, learning 0.199s)
               Value function loss: 124486.4167
                    Surrogate loss: -0.0037
             Mean action noise std: 1.00
                       Mean reward: 14507.82
               Mean episode length: 468.81
                 Mean success rate: 94.00
                  Mean reward/step: 29.98
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13099008
                    Iteration time: 0.46s
                        Total time: 786.60s
                               ETA: 197.8s

################################################################################
                     [1m Learning iteration 1599/2000 [0m

                       Computation: 17647 steps/s (collection: 0.266s, learning 0.198s)
               Value function loss: 83893.6595
                    Surrogate loss: -0.0031
             Mean action noise std: 1.00
                       Mean reward: 14307.23
               Mean episode length: 464.12
                 Mean success rate: 93.00
                  Mean reward/step: 30.34
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 0.46s
                        Total time: 787.06s
                               ETA: 197.3s

################################################################################
                     [1m Learning iteration 1600/2000 [0m

                       Computation: 17056 steps/s (collection: 0.267s, learning 0.214s)
               Value function loss: 98467.1951
                    Surrogate loss: -0.0031
             Mean action noise std: 1.00
                       Mean reward: 14341.19
               Mean episode length: 466.34
                 Mean success rate: 93.50
                  Mean reward/step: 31.07
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13115392
                    Iteration time: 0.48s
                        Total time: 787.54s
                               ETA: 196.8s

################################################################################
                     [1m Learning iteration 1601/2000 [0m

                       Computation: 16974 steps/s (collection: 0.282s, learning 0.200s)
               Value function loss: 100228.4257
                    Surrogate loss: -0.0020
             Mean action noise std: 1.00
                       Mean reward: 14387.18
               Mean episode length: 467.47
                 Mean success rate: 93.50
                  Mean reward/step: 30.82
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 0.48s
                        Total time: 788.02s
                               ETA: 196.3s

################################################################################
                     [1m Learning iteration 1602/2000 [0m

                       Computation: 17161 steps/s (collection: 0.270s, learning 0.207s)
               Value function loss: 137463.5504
                    Surrogate loss: -0.0036
             Mean action noise std: 1.00
                       Mean reward: 14310.44
               Mean episode length: 465.43
                 Mean success rate: 93.00
                  Mean reward/step: 30.39
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13131776
                    Iteration time: 0.48s
                        Total time: 788.50s
                               ETA: 195.8s

################################################################################
                     [1m Learning iteration 1603/2000 [0m

                       Computation: 17421 steps/s (collection: 0.268s, learning 0.203s)
               Value function loss: 77725.5371
                    Surrogate loss: -0.0043
             Mean action noise std: 1.00
                       Mean reward: 14349.71
               Mean episode length: 465.43
                 Mean success rate: 93.00
                  Mean reward/step: 29.55
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 0.47s
                        Total time: 788.97s
                               ETA: 195.3s

################################################################################
                     [1m Learning iteration 1604/2000 [0m

                       Computation: 17456 steps/s (collection: 0.267s, learning 0.202s)
               Value function loss: 58839.3910
                    Surrogate loss: -0.0036
             Mean action noise std: 1.00
                       Mean reward: 14301.94
               Mean episode length: 464.14
                 Mean success rate: 92.50
                  Mean reward/step: 31.15
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13148160
                    Iteration time: 0.47s
                        Total time: 789.44s
                               ETA: 194.8s

################################################################################
                     [1m Learning iteration 1605/2000 [0m

                       Computation: 17235 steps/s (collection: 0.269s, learning 0.206s)
               Value function loss: 153556.5014
                    Surrogate loss: -0.0023
             Mean action noise std: 1.00
                       Mean reward: 14450.67
               Mean episode length: 467.82
                 Mean success rate: 93.50
                  Mean reward/step: 31.40
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 0.48s
                        Total time: 789.92s
                               ETA: 194.3s

################################################################################
                     [1m Learning iteration 1606/2000 [0m

                       Computation: 17730 steps/s (collection: 0.262s, learning 0.200s)
               Value function loss: 103148.1906
                    Surrogate loss: -0.0032
             Mean action noise std: 1.00
                       Mean reward: 14538.85
               Mean episode length: 472.56
                 Mean success rate: 94.50
                  Mean reward/step: 30.37
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13164544
                    Iteration time: 0.46s
                        Total time: 790.38s
                               ETA: 193.8s

################################################################################
                     [1m Learning iteration 1607/2000 [0m

                       Computation: 16971 steps/s (collection: 0.268s, learning 0.214s)
               Value function loss: 79423.1687
                    Surrogate loss: -0.0038
             Mean action noise std: 1.00
                       Mean reward: 14220.32
               Mean episode length: 466.07
                 Mean success rate: 93.00
                  Mean reward/step: 31.00
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.48s
                        Total time: 790.86s
                               ETA: 193.3s

################################################################################
                     [1m Learning iteration 1608/2000 [0m

                       Computation: 16946 steps/s (collection: 0.277s, learning 0.206s)
               Value function loss: 92906.0815
                    Surrogate loss: -0.0022
             Mean action noise std: 1.00
                       Mean reward: 14135.04
               Mean episode length: 466.07
                 Mean success rate: 93.00
                  Mean reward/step: 31.66
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13180928
                    Iteration time: 0.48s
                        Total time: 791.34s
                               ETA: 192.8s

################################################################################
                     [1m Learning iteration 1609/2000 [0m

                       Computation: 17239 steps/s (collection: 0.270s, learning 0.205s)
               Value function loss: 109519.5293
                    Surrogate loss: -0.0039
             Mean action noise std: 1.00
                       Mean reward: 14292.37
               Mean episode length: 470.96
                 Mean success rate: 94.00
                  Mean reward/step: 31.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 0.48s
                        Total time: 791.82s
                               ETA: 192.3s

################################################################################
                     [1m Learning iteration 1610/2000 [0m

                       Computation: 17525 steps/s (collection: 0.254s, learning 0.214s)
               Value function loss: 132952.9924
                    Surrogate loss: -0.0024
             Mean action noise std: 1.00
                       Mean reward: 14185.04
               Mean episode length: 468.10
                 Mean success rate: 93.50
                  Mean reward/step: 30.97
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13197312
                    Iteration time: 0.47s
                        Total time: 792.29s
                               ETA: 191.8s

################################################################################
                     [1m Learning iteration 1611/2000 [0m

                       Computation: 16929 steps/s (collection: 0.289s, learning 0.195s)
               Value function loss: 126382.2335
                    Surrogate loss: -0.0028
             Mean action noise std: 1.00
                       Mean reward: 14201.43
               Mean episode length: 467.81
                 Mean success rate: 93.50
                  Mean reward/step: 30.16
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 0.48s
                        Total time: 792.77s
                               ETA: 191.3s

################################################################################
                     [1m Learning iteration 1612/2000 [0m

                       Computation: 17697 steps/s (collection: 0.260s, learning 0.203s)
               Value function loss: 103322.7209
                    Surrogate loss: -0.0032
             Mean action noise std: 1.00
                       Mean reward: 14235.63
               Mean episode length: 467.81
                 Mean success rate: 93.50
                  Mean reward/step: 30.53
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13213696
                    Iteration time: 0.46s
                        Total time: 793.23s
                               ETA: 190.8s

################################################################################
                     [1m Learning iteration 1613/2000 [0m

                       Computation: 17299 steps/s (collection: 0.264s, learning 0.209s)
               Value function loss: 123764.0771
                    Surrogate loss: -0.0032
             Mean action noise std: 1.00
                       Mean reward: 14361.45
               Mean episode length: 469.56
                 Mean success rate: 94.00
                  Mean reward/step: 30.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 0.47s
                        Total time: 793.71s
                               ETA: 190.3s

################################################################################
                     [1m Learning iteration 1614/2000 [0m

                       Computation: 18153 steps/s (collection: 0.247s, learning 0.204s)
               Value function loss: 101736.0139
                    Surrogate loss: -0.0032
             Mean action noise std: 1.00
                       Mean reward: 14216.24
               Mean episode length: 466.04
                 Mean success rate: 93.50
                  Mean reward/step: 30.39
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13230080
                    Iteration time: 0.45s
                        Total time: 794.16s
                               ETA: 189.8s

################################################################################
                     [1m Learning iteration 1615/2000 [0m

                       Computation: 17823 steps/s (collection: 0.251s, learning 0.209s)
               Value function loss: 64062.9254
                    Surrogate loss: -0.0034
             Mean action noise std: 1.00
                       Mean reward: 14348.16
               Mean episode length: 469.63
                 Mean success rate: 94.00
                  Mean reward/step: 31.17
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 0.46s
                        Total time: 794.62s
                               ETA: 189.3s

################################################################################
                     [1m Learning iteration 1616/2000 [0m

                       Computation: 17406 steps/s (collection: 0.258s, learning 0.213s)
               Value function loss: 84243.0935
                    Surrogate loss: -0.0030
             Mean action noise std: 1.00
                       Mean reward: 14392.09
               Mean episode length: 470.87
                 Mean success rate: 94.50
                  Mean reward/step: 31.48
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13246464
                    Iteration time: 0.47s
                        Total time: 795.09s
                               ETA: 188.8s

################################################################################
                     [1m Learning iteration 1617/2000 [0m

                       Computation: 18132 steps/s (collection: 0.245s, learning 0.207s)
               Value function loss: 93007.3818
                    Surrogate loss: -0.0040
             Mean action noise std: 1.00
                       Mean reward: 14469.97
               Mean episode length: 473.83
                 Mean success rate: 95.00
                  Mean reward/step: 31.53
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 0.45s
                        Total time: 795.54s
                               ETA: 188.3s

################################################################################
                     [1m Learning iteration 1618/2000 [0m

                       Computation: 17528 steps/s (collection: 0.261s, learning 0.207s)
               Value function loss: 167892.1777
                    Surrogate loss: -0.0029
             Mean action noise std: 1.00
                       Mean reward: 14695.42
               Mean episode length: 475.96
                 Mean success rate: 96.00
                  Mean reward/step: 30.82
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13262848
                    Iteration time: 0.47s
                        Total time: 796.01s
                               ETA: 187.8s

################################################################################
                     [1m Learning iteration 1619/2000 [0m

                       Computation: 18118 steps/s (collection: 0.243s, learning 0.209s)
               Value function loss: 60488.0961
                    Surrogate loss: -0.0033
             Mean action noise std: 1.00
                       Mean reward: 14606.40
               Mean episode length: 472.19
                 Mean success rate: 95.50
                  Mean reward/step: 30.70
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.45s
                        Total time: 796.46s
                               ETA: 187.3s

################################################################################
                     [1m Learning iteration 1620/2000 [0m

                       Computation: 17582 steps/s (collection: 0.257s, learning 0.209s)
               Value function loss: 87539.1146
                    Surrogate loss: -0.0035
             Mean action noise std: 1.00
                       Mean reward: 14627.88
               Mean episode length: 472.19
                 Mean success rate: 95.50
                  Mean reward/step: 32.07
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13279232
                    Iteration time: 0.47s
                        Total time: 796.93s
                               ETA: 186.8s

################################################################################
                     [1m Learning iteration 1621/2000 [0m

                       Computation: 17212 steps/s (collection: 0.262s, learning 0.214s)
               Value function loss: 119459.0813
                    Surrogate loss: -0.0033
             Mean action noise std: 1.00
                       Mean reward: 14599.69
               Mean episode length: 470.58
                 Mean success rate: 95.00
                  Mean reward/step: 31.02
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 0.48s
                        Total time: 797.40s
                               ETA: 186.3s

################################################################################
                     [1m Learning iteration 1622/2000 [0m

                       Computation: 16910 steps/s (collection: 0.265s, learning 0.219s)
               Value function loss: 107656.7545
                    Surrogate loss: -0.0037
             Mean action noise std: 1.00
                       Mean reward: 14772.27
               Mean episode length: 474.68
                 Mean success rate: 96.00
                  Mean reward/step: 31.10
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13295616
                    Iteration time: 0.48s
                        Total time: 797.89s
                               ETA: 185.8s

################################################################################
                     [1m Learning iteration 1623/2000 [0m

                       Computation: 17230 steps/s (collection: 0.264s, learning 0.211s)
               Value function loss: 100942.4896
                    Surrogate loss: -0.0033
             Mean action noise std: 1.00
                       Mean reward: 14966.97
               Mean episode length: 478.07
                 Mean success rate: 96.50
                  Mean reward/step: 31.63
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 0.48s
                        Total time: 798.36s
                               ETA: 185.3s

################################################################################
                     [1m Learning iteration 1624/2000 [0m

                       Computation: 17148 steps/s (collection: 0.271s, learning 0.206s)
               Value function loss: 118535.3918
                    Surrogate loss: -0.0029
             Mean action noise std: 1.00
                       Mean reward: 14896.49
               Mean episode length: 478.07
                 Mean success rate: 96.50
                  Mean reward/step: 31.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13312000
                    Iteration time: 0.48s
                        Total time: 798.84s
                               ETA: 184.8s

################################################################################
                     [1m Learning iteration 1625/2000 [0m

                       Computation: 17745 steps/s (collection: 0.257s, learning 0.204s)
               Value function loss: 77719.6826
                    Surrogate loss: -0.0036
             Mean action noise std: 1.01
                       Mean reward: 14953.62
               Mean episode length: 480.86
                 Mean success rate: 97.00
                  Mean reward/step: 31.20
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 0.46s
                        Total time: 799.30s
                               ETA: 184.3s

################################################################################
                     [1m Learning iteration 1626/2000 [0m

                       Computation: 17258 steps/s (collection: 0.260s, learning 0.214s)
               Value function loss: 126167.1942
                    Surrogate loss: -0.0014
             Mean action noise std: 1.01
                       Mean reward: 15082.04
               Mean episode length: 484.38
                 Mean success rate: 97.50
                  Mean reward/step: 30.76
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13328384
                    Iteration time: 0.47s
                        Total time: 799.78s
                               ETA: 183.8s

################################################################################
                     [1m Learning iteration 1627/2000 [0m

                       Computation: 16857 steps/s (collection: 0.271s, learning 0.215s)
               Value function loss: 94600.4986
                    Surrogate loss: -0.0032
             Mean action noise std: 1.01
                       Mean reward: 14780.10
               Mean episode length: 474.75
                 Mean success rate: 95.50
                  Mean reward/step: 30.19
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 0.49s
                        Total time: 800.26s
                               ETA: 183.4s

################################################################################
                     [1m Learning iteration 1628/2000 [0m

                       Computation: 15126 steps/s (collection: 0.288s, learning 0.254s)
               Value function loss: 123992.9359
                    Surrogate loss: -0.0027
             Mean action noise std: 1.01
                       Mean reward: 14890.05
               Mean episode length: 477.76
                 Mean success rate: 96.00
                  Mean reward/step: 30.76
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13344768
                    Iteration time: 0.54s
                        Total time: 800.80s
                               ETA: 182.9s

################################################################################
                     [1m Learning iteration 1629/2000 [0m

                       Computation: 16128 steps/s (collection: 0.287s, learning 0.221s)
               Value function loss: 121113.6211
                    Surrogate loss: -0.0032
             Mean action noise std: 1.01
                       Mean reward: 14927.73
               Mean episode length: 478.81
                 Mean success rate: 96.00
                  Mean reward/step: 30.48
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 0.51s
                        Total time: 801.31s
                               ETA: 182.4s

################################################################################
                     [1m Learning iteration 1630/2000 [0m

                       Computation: 16847 steps/s (collection: 0.268s, learning 0.218s)
               Value function loss: 97507.0127
                    Surrogate loss: -0.0044
             Mean action noise std: 1.00
                       Mean reward: 14968.86
               Mean episode length: 480.08
                 Mean success rate: 96.00
                  Mean reward/step: 30.54
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13361152
                    Iteration time: 0.49s
                        Total time: 801.80s
                               ETA: 181.9s

################################################################################
                     [1m Learning iteration 1631/2000 [0m

                       Computation: 17263 steps/s (collection: 0.273s, learning 0.202s)
               Value function loss: 81689.7469
                    Surrogate loss: -0.0037
             Mean action noise std: 1.00
                       Mean reward: 15032.37
               Mean episode length: 482.57
                 Mean success rate: 96.50
                  Mean reward/step: 31.06
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.47s
                        Total time: 802.27s
                               ETA: 181.4s

################################################################################
                     [1m Learning iteration 1632/2000 [0m

                       Computation: 18255 steps/s (collection: 0.248s, learning 0.201s)
               Value function loss: 96180.8162
                    Surrogate loss: -0.0038
             Mean action noise std: 1.00
                       Mean reward: 14843.81
               Mean episode length: 479.46
                 Mean success rate: 95.50
                  Mean reward/step: 31.05
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13377536
                    Iteration time: 0.45s
                        Total time: 802.72s
                               ETA: 180.9s

################################################################################
                     [1m Learning iteration 1633/2000 [0m

                       Computation: 17329 steps/s (collection: 0.268s, learning 0.205s)
               Value function loss: 139696.6602
                    Surrogate loss: -0.0031
             Mean action noise std: 1.00
                       Mean reward: 14833.13
               Mean episode length: 479.50
                 Mean success rate: 95.50
                  Mean reward/step: 30.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 0.47s
                        Total time: 803.19s
                               ETA: 180.4s

################################################################################
                     [1m Learning iteration 1634/2000 [0m

                       Computation: 17850 steps/s (collection: 0.242s, learning 0.217s)
               Value function loss: 106386.1658
                    Surrogate loss: -0.0051
             Mean action noise std: 1.00
                       Mean reward: 14802.59
               Mean episode length: 479.50
                 Mean success rate: 95.50
                  Mean reward/step: 30.04
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13393920
                    Iteration time: 0.46s
                        Total time: 803.65s
                               ETA: 179.9s

################################################################################
                     [1m Learning iteration 1635/2000 [0m

                       Computation: 17693 steps/s (collection: 0.257s, learning 0.206s)
               Value function loss: 74942.2810
                    Surrogate loss: -0.0034
             Mean action noise std: 1.01
                       Mean reward: 14851.88
               Mean episode length: 479.50
                 Mean success rate: 95.50
                  Mean reward/step: 31.27
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 0.46s
                        Total time: 804.12s
                               ETA: 179.4s

################################################################################
                     [1m Learning iteration 1636/2000 [0m

                       Computation: 17095 steps/s (collection: 0.260s, learning 0.219s)
               Value function loss: 135856.0389
                    Surrogate loss: -0.0030
             Mean action noise std: 1.01
                       Mean reward: 14691.76
               Mean episode length: 475.06
                 Mean success rate: 94.50
                  Mean reward/step: 31.82
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13410304
                    Iteration time: 0.48s
                        Total time: 804.59s
                               ETA: 178.9s

################################################################################
                     [1m Learning iteration 1637/2000 [0m

                       Computation: 17000 steps/s (collection: 0.271s, learning 0.211s)
               Value function loss: 101072.7264
                    Surrogate loss: -0.0038
             Mean action noise std: 1.01
                       Mean reward: 14580.64
               Mean episode length: 471.10
                 Mean success rate: 94.00
                  Mean reward/step: 30.27
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 0.48s
                        Total time: 805.08s
                               ETA: 178.4s

################################################################################
                     [1m Learning iteration 1638/2000 [0m

                       Computation: 17301 steps/s (collection: 0.266s, learning 0.207s)
               Value function loss: 114034.6607
                    Surrogate loss: -0.0038
             Mean action noise std: 1.00
                       Mean reward: 14476.91
               Mean episode length: 467.79
                 Mean success rate: 93.50
                  Mean reward/step: 30.39
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13426688
                    Iteration time: 0.47s
                        Total time: 805.55s
                               ETA: 177.9s

################################################################################
                     [1m Learning iteration 1639/2000 [0m

                       Computation: 16512 steps/s (collection: 0.258s, learning 0.238s)
               Value function loss: 87214.0980
                    Surrogate loss: -0.0032
             Mean action noise std: 1.00
                       Mean reward: 14653.88
               Mean episode length: 473.65
                 Mean success rate: 95.00
                  Mean reward/step: 31.05
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 0.50s
                        Total time: 806.05s
                               ETA: 177.4s

################################################################################
                     [1m Learning iteration 1640/2000 [0m

                       Computation: 17386 steps/s (collection: 0.259s, learning 0.212s)
               Value function loss: 99356.5179
                    Surrogate loss: -0.0033
             Mean action noise std: 1.00
                       Mean reward: 14412.15
               Mean episode length: 467.17
                 Mean success rate: 94.00
                  Mean reward/step: 31.29
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13443072
                    Iteration time: 0.47s
                        Total time: 806.52s
                               ETA: 176.9s

################################################################################
                     [1m Learning iteration 1641/2000 [0m

                       Computation: 16934 steps/s (collection: 0.270s, learning 0.213s)
               Value function loss: 110898.3832
                    Surrogate loss: -0.0027
             Mean action noise std: 1.01
                       Mean reward: 14342.11
               Mean episode length: 466.07
                 Mean success rate: 94.00
                  Mean reward/step: 30.98
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 0.48s
                        Total time: 807.00s
                               ETA: 176.4s

################################################################################
                     [1m Learning iteration 1642/2000 [0m

                       Computation: 17690 steps/s (collection: 0.251s, learning 0.212s)
               Value function loss: 137277.2809
                    Surrogate loss: -0.0029
             Mean action noise std: 1.01
                       Mean reward: 14330.97
               Mean episode length: 466.07
                 Mean success rate: 94.00
                  Mean reward/step: 30.44
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13459456
                    Iteration time: 0.46s
                        Total time: 807.46s
                               ETA: 175.9s

################################################################################
                     [1m Learning iteration 1643/2000 [0m

                       Computation: 16908 steps/s (collection: 0.279s, learning 0.205s)
               Value function loss: 98894.7471
                    Surrogate loss: -0.0030
             Mean action noise std: 1.01
                       Mean reward: 14458.36
               Mean episode length: 469.19
                 Mean success rate: 95.00
                  Mean reward/step: 30.53
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.48s
                        Total time: 807.95s
                               ETA: 175.4s

################################################################################
                     [1m Learning iteration 1644/2000 [0m

                       Computation: 17390 steps/s (collection: 0.263s, learning 0.208s)
               Value function loss: 114453.9115
                    Surrogate loss: -0.0035
             Mean action noise std: 1.01
                       Mean reward: 14430.93
               Mean episode length: 469.19
                 Mean success rate: 95.00
                  Mean reward/step: 31.29
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13475840
                    Iteration time: 0.47s
                        Total time: 808.42s
                               ETA: 175.0s

################################################################################
                     [1m Learning iteration 1645/2000 [0m

                       Computation: 16734 steps/s (collection: 0.265s, learning 0.224s)
               Value function loss: 112829.8234
                    Surrogate loss: -0.0029
             Mean action noise std: 1.01
                       Mean reward: 14593.99
               Mean episode length: 473.63
                 Mean success rate: 96.00
                  Mean reward/step: 30.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 0.49s
                        Total time: 808.91s
                               ETA: 174.5s

################################################################################
                     [1m Learning iteration 1646/2000 [0m

                       Computation: 17570 steps/s (collection: 0.257s, learning 0.209s)
               Value function loss: 58207.7894
                    Surrogate loss: -0.0023
             Mean action noise std: 1.01
                       Mean reward: 14592.08
               Mean episode length: 473.63
                 Mean success rate: 96.00
                  Mean reward/step: 31.36
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 13492224
                    Iteration time: 0.47s
                        Total time: 809.38s
                               ETA: 174.0s

################################################################################
                     [1m Learning iteration 1647/2000 [0m

                       Computation: 17068 steps/s (collection: 0.272s, learning 0.208s)
               Value function loss: 111853.6760
                    Surrogate loss: -0.0038
             Mean action noise std: 1.01
                       Mean reward: 14562.50
               Mean episode length: 473.63
                 Mean success rate: 96.00
                  Mean reward/step: 32.25
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 0.48s
                        Total time: 809.86s
                               ETA: 173.5s

################################################################################
                     [1m Learning iteration 1648/2000 [0m

                       Computation: 16351 steps/s (collection: 0.286s, learning 0.215s)
               Value function loss: 87866.6012
                    Surrogate loss: -0.0036
             Mean action noise std: 1.01
                       Mean reward: 14730.49
               Mean episode length: 478.07
                 Mean success rate: 97.00
                  Mean reward/step: 31.98
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13508608
                    Iteration time: 0.50s
                        Total time: 810.36s
                               ETA: 173.0s

################################################################################
                     [1m Learning iteration 1649/2000 [0m

                       Computation: 16856 steps/s (collection: 0.267s, learning 0.219s)
               Value function loss: 145288.5400
                    Surrogate loss: -0.0013
             Mean action noise std: 1.01
                       Mean reward: 14963.76
               Mean episode length: 485.40
                 Mean success rate: 98.00
                  Mean reward/step: 31.79
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 0.49s
                        Total time: 810.84s
                               ETA: 172.5s

################################################################################
                     [1m Learning iteration 1650/2000 [0m

                       Computation: 17232 steps/s (collection: 0.262s, learning 0.213s)
               Value function loss: 92108.5899
                    Surrogate loss: -0.0033
             Mean action noise std: 1.01
                       Mean reward: 14945.12
               Mean episode length: 485.40
                 Mean success rate: 98.00
                  Mean reward/step: 30.93
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13524992
                    Iteration time: 0.48s
                        Total time: 811.32s
                               ETA: 172.0s

################################################################################
                     [1m Learning iteration 1651/2000 [0m

                       Computation: 17359 steps/s (collection: 0.257s, learning 0.215s)
               Value function loss: 48189.5495
                    Surrogate loss: -0.0020
             Mean action noise std: 1.01
                       Mean reward: 15081.51
               Mean episode length: 489.11
                 Mean success rate: 98.50
                  Mean reward/step: 31.89
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 0.47s
                        Total time: 811.79s
                               ETA: 171.5s

################################################################################
                     [1m Learning iteration 1652/2000 [0m

                       Computation: 16926 steps/s (collection: 0.281s, learning 0.203s)
               Value function loss: 139934.4069
                    Surrogate loss: -0.0021
             Mean action noise std: 1.01
                       Mean reward: 15226.51
               Mean episode length: 492.00
                 Mean success rate: 99.00
                  Mean reward/step: 31.81
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13541376
                    Iteration time: 0.48s
                        Total time: 812.27s
                               ETA: 171.0s

################################################################################
                     [1m Learning iteration 1653/2000 [0m

                       Computation: 17495 steps/s (collection: 0.261s, learning 0.208s)
               Value function loss: 102878.3765
                    Surrogate loss: -0.0032
             Mean action noise std: 1.01
                       Mean reward: 15373.96
               Mean episode length: 495.60
                 Mean success rate: 99.50
                  Mean reward/step: 30.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 0.47s
                        Total time: 812.74s
                               ETA: 170.5s

################################################################################
                     [1m Learning iteration 1654/2000 [0m

                       Computation: 17340 steps/s (collection: 0.268s, learning 0.204s)
               Value function loss: 96967.8702
                    Surrogate loss: -0.0029
             Mean action noise std: 1.01
                       Mean reward: 15365.82
               Mean episode length: 495.60
                 Mean success rate: 99.50
                  Mean reward/step: 30.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13557760
                    Iteration time: 0.47s
                        Total time: 813.21s
                               ETA: 170.0s

################################################################################
                     [1m Learning iteration 1655/2000 [0m

                       Computation: 17797 steps/s (collection: 0.259s, learning 0.201s)
               Value function loss: 109571.8195
                    Surrogate loss: -0.0030
             Mean action noise std: 1.01
                       Mean reward: 15132.27
               Mean episode length: 486.33
                 Mean success rate: 98.50
                  Mean reward/step: 30.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.46s
                        Total time: 813.67s
                               ETA: 169.5s

################################################################################
                     [1m Learning iteration 1656/2000 [0m

                       Computation: 17808 steps/s (collection: 0.255s, learning 0.205s)
               Value function loss: 114360.0623
                    Surrogate loss: -0.0034
             Mean action noise std: 1.01
                       Mean reward: 15157.42
               Mean episode length: 486.33
                 Mean success rate: 98.50
                  Mean reward/step: 30.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13574144
                    Iteration time: 0.46s
                        Total time: 814.13s
                               ETA: 169.0s

################################################################################
                     [1m Learning iteration 1657/2000 [0m

                       Computation: 17326 steps/s (collection: 0.256s, learning 0.217s)
               Value function loss: 130808.3189
                    Surrogate loss: -0.0023
             Mean action noise std: 1.01
                       Mean reward: 15152.26
               Mean episode length: 486.33
                 Mean success rate: 98.50
                  Mean reward/step: 30.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 0.47s
                        Total time: 814.61s
                               ETA: 168.5s

################################################################################
                     [1m Learning iteration 1658/2000 [0m

                       Computation: 17163 steps/s (collection: 0.265s, learning 0.212s)
               Value function loss: 121280.1174
                    Surrogate loss: -0.0032
             Mean action noise std: 1.01
                       Mean reward: 15171.48
               Mean episode length: 486.33
                 Mean success rate: 98.50
                  Mean reward/step: 30.22
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13590528
                    Iteration time: 0.48s
                        Total time: 815.08s
                               ETA: 168.0s

################################################################################
                     [1m Learning iteration 1659/2000 [0m

                       Computation: 17002 steps/s (collection: 0.251s, learning 0.231s)
               Value function loss: 107024.6201
                    Surrogate loss: -0.0038
             Mean action noise std: 1.01
                       Mean reward: 15192.90
               Mean episode length: 486.33
                 Mean success rate: 98.50
                  Mean reward/step: 30.70
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 0.48s
                        Total time: 815.57s
                               ETA: 167.5s

################################################################################
                     [1m Learning iteration 1660/2000 [0m

                       Computation: 16629 steps/s (collection: 0.255s, learning 0.237s)
               Value function loss: 129984.4230
                    Surrogate loss: -0.0029
             Mean action noise std: 1.01
                       Mean reward: 15040.48
               Mean episode length: 481.92
                 Mean success rate: 97.50
                  Mean reward/step: 30.59
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13606912
                    Iteration time: 0.49s
                        Total time: 816.06s
                               ETA: 167.0s

################################################################################
                     [1m Learning iteration 1661/2000 [0m

                       Computation: 17796 steps/s (collection: 0.244s, learning 0.216s)
               Value function loss: 98187.4944
                    Surrogate loss: -0.0036
             Mean action noise std: 1.01
                       Mean reward: 14969.73
               Mean episode length: 479.37
                 Mean success rate: 97.00
                  Mean reward/step: 30.44
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 0.46s
                        Total time: 816.52s
                               ETA: 166.5s

################################################################################
                     [1m Learning iteration 1662/2000 [0m

                       Computation: 17722 steps/s (collection: 0.250s, learning 0.212s)
               Value function loss: 65466.5758
                    Surrogate loss: -0.0029
             Mean action noise std: 1.01
                       Mean reward: 14933.27
               Mean episode length: 479.37
                 Mean success rate: 97.00
                  Mean reward/step: 31.76
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 13623296
                    Iteration time: 0.46s
                        Total time: 816.98s
                               ETA: 166.0s

################################################################################
                     [1m Learning iteration 1663/2000 [0m

                       Computation: 16577 steps/s (collection: 0.261s, learning 0.233s)
               Value function loss: 103464.1691
                    Surrogate loss: -0.0038
             Mean action noise std: 1.01
                       Mean reward: 15052.54
               Mean episode length: 483.77
                 Mean success rate: 97.50
                  Mean reward/step: 31.73
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 0.49s
                        Total time: 817.48s
                               ETA: 165.6s

################################################################################
                     [1m Learning iteration 1664/2000 [0m

                       Computation: 16881 steps/s (collection: 0.265s, learning 0.220s)
               Value function loss: 83308.9344
                    Surrogate loss: -0.0035
             Mean action noise std: 1.01
                       Mean reward: 15048.61
               Mean episode length: 483.77
                 Mean success rate: 97.50
                  Mean reward/step: 30.79
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13639680
                    Iteration time: 0.49s
                        Total time: 817.96s
                               ETA: 165.1s

################################################################################
                     [1m Learning iteration 1665/2000 [0m

                       Computation: 17078 steps/s (collection: 0.269s, learning 0.211s)
               Value function loss: 139840.1696
                    Surrogate loss: -0.0024
             Mean action noise std: 1.01
                       Mean reward: 14812.68
               Mean episode length: 477.65
                 Mean success rate: 96.50
                  Mean reward/step: 30.29
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 0.48s
                        Total time: 818.44s
                               ETA: 164.6s

################################################################################
                     [1m Learning iteration 1666/2000 [0m

                       Computation: 17471 steps/s (collection: 0.258s, learning 0.211s)
               Value function loss: 78737.9489
                    Surrogate loss: -0.0041
             Mean action noise std: 1.01
                       Mean reward: 14961.38
               Mean episode length: 482.25
                 Mean success rate: 97.00
                  Mean reward/step: 30.19
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13656064
                    Iteration time: 0.47s
                        Total time: 818.91s
                               ETA: 164.1s

################################################################################
                     [1m Learning iteration 1667/2000 [0m

                       Computation: 17620 steps/s (collection: 0.251s, learning 0.214s)
               Value function loss: 83685.6936
                    Surrogate loss: -0.0019
             Mean action noise std: 1.01
                       Mean reward: 15096.78
               Mean episode length: 486.92
                 Mean success rate: 97.50
                  Mean reward/step: 31.43
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.46s
                        Total time: 819.37s
                               ETA: 163.6s

################################################################################
                     [1m Learning iteration 1668/2000 [0m

                       Computation: 17437 steps/s (collection: 0.269s, learning 0.201s)
               Value function loss: 110927.2866
                    Surrogate loss: -0.0027
             Mean action noise std: 1.01
                       Mean reward: 15104.08
               Mean episode length: 486.92
                 Mean success rate: 97.50
                  Mean reward/step: 30.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13672448
                    Iteration time: 0.47s
                        Total time: 819.84s
                               ETA: 163.1s

################################################################################
                     [1m Learning iteration 1669/2000 [0m

                       Computation: 17062 steps/s (collection: 0.264s, learning 0.216s)
               Value function loss: 103306.6300
                    Surrogate loss: -0.0040
             Mean action noise std: 1.01
                       Mean reward: 15061.78
               Mean episode length: 486.92
                 Mean success rate: 97.50
                  Mean reward/step: 30.59
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 0.48s
                        Total time: 820.32s
                               ETA: 162.6s

################################################################################
                     [1m Learning iteration 1670/2000 [0m

                       Computation: 17045 steps/s (collection: 0.265s, learning 0.216s)
               Value function loss: 99452.6137
                    Surrogate loss: -0.0043
             Mean action noise std: 1.01
                       Mean reward: 15014.21
               Mean episode length: 486.92
                 Mean success rate: 97.50
                  Mean reward/step: 30.43
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13688832
                    Iteration time: 0.48s
                        Total time: 820.80s
                               ETA: 162.1s

################################################################################
                     [1m Learning iteration 1671/2000 [0m

                       Computation: 15914 steps/s (collection: 0.290s, learning 0.224s)
               Value function loss: 138696.0611
                    Surrogate loss: -0.0028
             Mean action noise std: 1.01
                       Mean reward: 14814.85
               Mean episode length: 482.12
                 Mean success rate: 96.50
                  Mean reward/step: 30.12
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 0.51s
                        Total time: 821.32s
                               ETA: 161.6s

################################################################################
                     [1m Learning iteration 1672/2000 [0m

                       Computation: 16701 steps/s (collection: 0.279s, learning 0.211s)
               Value function loss: 104064.6152
                    Surrogate loss: -0.0034
             Mean action noise std: 1.01
                       Mean reward: 14645.29
               Mean episode length: 477.00
                 Mean success rate: 96.00
                  Mean reward/step: 29.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13705216
                    Iteration time: 0.49s
                        Total time: 821.81s
                               ETA: 161.1s

################################################################################
                     [1m Learning iteration 1673/2000 [0m

                       Computation: 16741 steps/s (collection: 0.288s, learning 0.201s)
               Value function loss: 143111.6588
                    Surrogate loss: -0.0021
             Mean action noise std: 1.01
                       Mean reward: 14731.59
               Mean episode length: 479.55
                 Mean success rate: 96.50
                  Mean reward/step: 29.08
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 0.49s
                        Total time: 822.30s
                               ETA: 160.6s

################################################################################
                     [1m Learning iteration 1674/2000 [0m

                       Computation: 16560 steps/s (collection: 0.283s, learning 0.212s)
               Value function loss: 113247.4664
                    Surrogate loss: -0.0034
             Mean action noise std: 1.01
                       Mean reward: 14750.58
               Mean episode length: 479.55
                 Mean success rate: 96.50
                  Mean reward/step: 29.05
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13721600
                    Iteration time: 0.49s
                        Total time: 822.79s
                               ETA: 160.1s

################################################################################
                     [1m Learning iteration 1675/2000 [0m

                       Computation: 17307 steps/s (collection: 0.263s, learning 0.210s)
               Value function loss: 119473.1989
                    Surrogate loss: -0.0042
             Mean action noise std: 1.01
                       Mean reward: 14435.28
               Mean episode length: 470.40
                 Mean success rate: 94.50
                  Mean reward/step: 29.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 0.47s
                        Total time: 823.27s
                               ETA: 159.6s

################################################################################
                     [1m Learning iteration 1676/2000 [0m

                       Computation: 15469 steps/s (collection: 0.324s, learning 0.205s)
               Value function loss: 128117.8446
                    Surrogate loss: -0.0023
             Mean action noise std: 1.01
                       Mean reward: 14450.35
               Mean episode length: 472.73
                 Mean success rate: 94.50
                  Mean reward/step: 29.21
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13737984
                    Iteration time: 0.53s
                        Total time: 823.80s
                               ETA: 159.2s

################################################################################
                     [1m Learning iteration 1677/2000 [0m

                       Computation: 16861 steps/s (collection: 0.271s, learning 0.215s)
               Value function loss: 57395.2002
                    Surrogate loss: -0.0028
             Mean action noise std: 1.01
                       Mean reward: 14447.16
               Mean episode length: 472.73
                 Mean success rate: 94.50
                  Mean reward/step: 29.71
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 0.49s
                        Total time: 824.28s
                               ETA: 158.7s

################################################################################
                     [1m Learning iteration 1678/2000 [0m

                       Computation: 17744 steps/s (collection: 0.252s, learning 0.209s)
               Value function loss: 76874.2420
                    Surrogate loss: -0.0045
             Mean action noise std: 1.01
                       Mean reward: 14429.41
               Mean episode length: 472.73
                 Mean success rate: 94.50
                  Mean reward/step: 30.89
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13754368
                    Iteration time: 0.46s
                        Total time: 824.74s
                               ETA: 158.2s

################################################################################
                     [1m Learning iteration 1679/2000 [0m

                       Computation: 17402 steps/s (collection: 0.263s, learning 0.208s)
               Value function loss: 91050.1943
                    Surrogate loss: -0.0031
             Mean action noise std: 1.01
                       Mean reward: 14425.15
               Mean episode length: 472.73
                 Mean success rate: 94.50
                  Mean reward/step: 30.43
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.47s
                        Total time: 825.22s
                               ETA: 157.7s

################################################################################
                     [1m Learning iteration 1680/2000 [0m

                       Computation: 16737 steps/s (collection: 0.279s, learning 0.211s)
               Value function loss: 144111.8971
                    Surrogate loss: -0.0024
             Mean action noise std: 1.01
                       Mean reward: 14415.91
               Mean episode length: 472.73
                 Mean success rate: 94.50
                  Mean reward/step: 30.32
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13770752
                    Iteration time: 0.49s
                        Total time: 825.70s
                               ETA: 157.2s

################################################################################
                     [1m Learning iteration 1681/2000 [0m

                       Computation: 16431 steps/s (collection: 0.271s, learning 0.228s)
               Value function loss: 103412.2781
                    Surrogate loss: -0.0024
             Mean action noise std: 1.01
                       Mean reward: 14076.39
               Mean episode length: 465.27
                 Mean success rate: 93.00
                  Mean reward/step: 29.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 0.50s
                        Total time: 826.20s
                               ETA: 156.7s

################################################################################
                     [1m Learning iteration 1682/2000 [0m

                       Computation: 14132 steps/s (collection: 0.308s, learning 0.272s)
               Value function loss: 58240.3940
                    Surrogate loss: -0.0036
             Mean action noise std: 1.01
                       Mean reward: 14222.36
               Mean episode length: 470.06
                 Mean success rate: 94.00
                  Mean reward/step: 29.93
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13787136
                    Iteration time: 0.58s
                        Total time: 826.78s
                               ETA: 156.2s

################################################################################
                     [1m Learning iteration 1683/2000 [0m

                       Computation: 15627 steps/s (collection: 0.312s, learning 0.212s)
               Value function loss: 148377.2828
                    Surrogate loss: -0.0013
             Mean action noise std: 1.01
                       Mean reward: 14510.56
               Mean episode length: 479.61
                 Mean success rate: 95.50
                  Mean reward/step: 30.81
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 0.52s
                        Total time: 827.31s
                               ETA: 155.7s

################################################################################
                     [1m Learning iteration 1684/2000 [0m

                       Computation: 15900 steps/s (collection: 0.268s, learning 0.247s)
               Value function loss: 90458.2330
                    Surrogate loss: -0.0040
             Mean action noise std: 1.01
                       Mean reward: 14472.36
               Mean episode length: 479.61
                 Mean success rate: 95.50
                  Mean reward/step: 29.82
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13803520
                    Iteration time: 0.52s
                        Total time: 827.82s
                               ETA: 155.2s

################################################################################
                     [1m Learning iteration 1685/2000 [0m

                       Computation: 14822 steps/s (collection: 0.311s, learning 0.242s)
               Value function loss: 83413.9735
                    Surrogate loss: -0.0032
             Mean action noise std: 1.01
                       Mean reward: 14295.50
               Mean episode length: 475.96
                 Mean success rate: 94.50
                  Mean reward/step: 30.32
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 0.55s
                        Total time: 828.37s
                               ETA: 154.8s

################################################################################
                     [1m Learning iteration 1686/2000 [0m

                       Computation: 14559 steps/s (collection: 0.328s, learning 0.235s)
               Value function loss: 92769.1503
                    Surrogate loss: -0.0034
             Mean action noise std: 1.01
                       Mean reward: 14270.47
               Mean episode length: 475.96
                 Mean success rate: 94.50
                  Mean reward/step: 30.90
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13819904
                    Iteration time: 0.56s
                        Total time: 828.94s
                               ETA: 154.3s

################################################################################
                     [1m Learning iteration 1687/2000 [0m

                       Computation: 16242 steps/s (collection: 0.294s, learning 0.211s)
               Value function loss: 111054.8648
                    Surrogate loss: -0.0024
             Mean action noise std: 1.01
                       Mean reward: 14528.00
               Mean episode length: 485.11
                 Mean success rate: 96.50
                  Mean reward/step: 30.27
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 0.50s
                        Total time: 829.44s
                               ETA: 153.8s

################################################################################
                     [1m Learning iteration 1688/2000 [0m

                       Computation: 15966 steps/s (collection: 0.300s, learning 0.213s)
               Value function loss: 136271.0104
                    Surrogate loss: -0.0021
             Mean action noise std: 1.01
                       Mean reward: 14378.07
               Mean episode length: 481.26
                 Mean success rate: 95.50
                  Mean reward/step: 30.09
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13836288
                    Iteration time: 0.51s
                        Total time: 829.95s
                               ETA: 153.3s

################################################################################
                     [1m Learning iteration 1689/2000 [0m

                       Computation: 15108 steps/s (collection: 0.293s, learning 0.249s)
               Value function loss: 114893.5131
                    Surrogate loss: -0.0032
             Mean action noise std: 1.01
                       Mean reward: 14345.44
               Mean episode length: 480.74
                 Mean success rate: 96.00
                  Mean reward/step: 29.98
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 0.54s
                        Total time: 830.50s
                               ETA: 152.8s

################################################################################
                     [1m Learning iteration 1690/2000 [0m

                       Computation: 16486 steps/s (collection: 0.285s, learning 0.212s)
               Value function loss: 94897.9414
                    Surrogate loss: -0.0032
             Mean action noise std: 1.01
                       Mean reward: 13914.42
               Mean episode length: 470.42
                 Mean success rate: 94.00
                  Mean reward/step: 29.98
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13852672
                    Iteration time: 0.50s
                        Total time: 830.99s
                               ETA: 152.3s

################################################################################
                     [1m Learning iteration 1691/2000 [0m

                       Computation: 17021 steps/s (collection: 0.276s, learning 0.206s)
               Value function loss: 119732.2606
                    Surrogate loss: -0.0020
             Mean action noise std: 1.01
                       Mean reward: 13946.21
               Mean episode length: 470.42
                 Mean success rate: 94.00
                  Mean reward/step: 29.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.48s
                        Total time: 831.48s
                               ETA: 151.8s

################################################################################
                     [1m Learning iteration 1692/2000 [0m

                       Computation: 15869 steps/s (collection: 0.303s, learning 0.213s)
               Value function loss: 83788.0389
                    Surrogate loss: -0.0037
             Mean action noise std: 1.01
                       Mean reward: 14125.89
               Mean episode length: 474.15
                 Mean success rate: 94.50
                  Mean reward/step: 29.85
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13869056
                    Iteration time: 0.52s
                        Total time: 831.99s
                               ETA: 151.4s

################################################################################
                     [1m Learning iteration 1693/2000 [0m

                       Computation: 17096 steps/s (collection: 0.270s, learning 0.209s)
               Value function loss: 39123.8507
                    Surrogate loss: -0.0024
             Mean action noise std: 1.01
                       Mean reward: 14120.40
               Mean episode length: 473.59
                 Mean success rate: 95.00
                  Mean reward/step: 30.92
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 0.48s
                        Total time: 832.47s
                               ETA: 150.9s

################################################################################
                     [1m Learning iteration 1694/2000 [0m

                       Computation: 16049 steps/s (collection: 0.291s, learning 0.219s)
               Value function loss: 107492.5372
                    Surrogate loss: -0.0032
             Mean action noise std: 1.01
                       Mean reward: 14173.48
               Mean episode length: 473.59
                 Mean success rate: 95.00
                  Mean reward/step: 31.25
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13885440
                    Iteration time: 0.51s
                        Total time: 832.98s
                               ETA: 150.4s

################################################################################
                     [1m Learning iteration 1695/2000 [0m

                       Computation: 17343 steps/s (collection: 0.250s, learning 0.223s)
               Value function loss: 70952.4368
                    Surrogate loss: -0.0029
             Mean action noise std: 1.01
                       Mean reward: 14187.83
               Mean episode length: 473.59
                 Mean success rate: 95.00
                  Mean reward/step: 31.09
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 0.47s
                        Total time: 833.45s
                               ETA: 149.9s

################################################################################
                     [1m Learning iteration 1696/2000 [0m

                       Computation: 16676 steps/s (collection: 0.282s, learning 0.210s)
               Value function loss: 151643.0408
                    Surrogate loss: -0.0017
             Mean action noise std: 1.01
                       Mean reward: 14339.88
               Mean episode length: 477.24
                 Mean success rate: 96.00
                  Mean reward/step: 30.63
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13901824
                    Iteration time: 0.49s
                        Total time: 833.94s
                               ETA: 149.4s

################################################################################
                     [1m Learning iteration 1697/2000 [0m

                       Computation: 16939 steps/s (collection: 0.273s, learning 0.211s)
               Value function loss: 97615.3263
                    Surrogate loss: -0.0035
             Mean action noise std: 1.01
                       Mean reward: 14379.54
               Mean episode length: 477.24
                 Mean success rate: 96.00
                  Mean reward/step: 30.24
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 0.48s
                        Total time: 834.43s
                               ETA: 148.9s

################################################################################
                     [1m Learning iteration 1698/2000 [0m

                       Computation: 17439 steps/s (collection: 0.259s, learning 0.211s)
               Value function loss: 60945.8565
                    Surrogate loss: -0.0021
             Mean action noise std: 1.01
                       Mean reward: 14405.02
               Mean episode length: 477.24
                 Mean success rate: 96.00
                  Mean reward/step: 31.51
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 13918208
                    Iteration time: 0.47s
                        Total time: 834.90s
                               ETA: 148.4s

################################################################################
                     [1m Learning iteration 1699/2000 [0m

                       Computation: 16328 steps/s (collection: 0.286s, learning 0.216s)
               Value function loss: 133150.0150
                    Surrogate loss: -0.0020
             Mean action noise std: 1.01
                       Mean reward: 14384.44
               Mean episode length: 474.56
                 Mean success rate: 96.00
                  Mean reward/step: 31.28
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 0.50s
                        Total time: 835.40s
                               ETA: 147.9s

################################################################################
                     [1m Learning iteration 1700/2000 [0m

                       Computation: 16383 steps/s (collection: 0.276s, learning 0.224s)
               Value function loss: 84484.8769
                    Surrogate loss: -0.0025
             Mean action noise std: 1.01
                       Mean reward: 14350.57
               Mean episode length: 472.61
                 Mean success rate: 95.50
                  Mean reward/step: 31.06
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13934592
                    Iteration time: 0.50s
                        Total time: 835.90s
                               ETA: 147.4s

################################################################################
                     [1m Learning iteration 1701/2000 [0m

                       Computation: 17135 steps/s (collection: 0.274s, learning 0.204s)
               Value function loss: 85857.4498
                    Surrogate loss: -0.0022
             Mean action noise std: 1.01
                       Mean reward: 14309.57
               Mean episode length: 472.72
                 Mean success rate: 95.00
                  Mean reward/step: 31.52
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 0.48s
                        Total time: 836.38s
                               ETA: 146.9s

################################################################################
                     [1m Learning iteration 1702/2000 [0m

                       Computation: 17073 steps/s (collection: 0.270s, learning 0.210s)
               Value function loss: 117859.9148
                    Surrogate loss: -0.0022
             Mean action noise std: 1.01
                       Mean reward: 14709.10
               Mean episode length: 480.55
                 Mean success rate: 96.50
                  Mean reward/step: 31.22
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13950976
                    Iteration time: 0.48s
                        Total time: 836.86s
                               ETA: 146.4s

################################################################################
                     [1m Learning iteration 1703/2000 [0m

                       Computation: 17220 steps/s (collection: 0.270s, learning 0.206s)
               Value function loss: 113639.1147
                    Surrogate loss: -0.0027
             Mean action noise std: 1.01
                       Mean reward: 14781.47
               Mean episode length: 483.04
                 Mean success rate: 97.00
                  Mean reward/step: 31.09
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.48s
                        Total time: 837.33s
                               ETA: 145.9s

################################################################################
                     [1m Learning iteration 1704/2000 [0m

                       Computation: 16992 steps/s (collection: 0.265s, learning 0.217s)
               Value function loss: 128327.8024
                    Surrogate loss: -0.0022
             Mean action noise std: 1.01
                       Mean reward: 14837.37
               Mean episode length: 483.04
                 Mean success rate: 97.00
                  Mean reward/step: 30.99
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13967360
                    Iteration time: 0.48s
                        Total time: 837.82s
                               ETA: 145.5s

################################################################################
                     [1m Learning iteration 1705/2000 [0m

                       Computation: 17145 steps/s (collection: 0.262s, learning 0.216s)
               Value function loss: 128163.9070
                    Surrogate loss: -0.0014
             Mean action noise std: 1.01
                       Mean reward: 15010.11
               Mean episode length: 487.32
                 Mean success rate: 97.50
                  Mean reward/step: 30.21
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 0.48s
                        Total time: 838.29s
                               ETA: 145.0s

################################################################################
                     [1m Learning iteration 1706/2000 [0m

                       Computation: 17134 steps/s (collection: 0.272s, learning 0.206s)
               Value function loss: 156850.0871
                    Surrogate loss: -0.0024
             Mean action noise std: 1.01
                       Mean reward: 14864.89
               Mean episode length: 483.24
                 Mean success rate: 97.00
                  Mean reward/step: 29.83
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13983744
                    Iteration time: 0.48s
                        Total time: 838.77s
                               ETA: 144.5s

################################################################################
                     [1m Learning iteration 1707/2000 [0m

                       Computation: 14571 steps/s (collection: 0.289s, learning 0.273s)
               Value function loss: 124184.7043
                    Surrogate loss: -0.0031
             Mean action noise std: 1.01
                       Mean reward: 14595.84
               Mean episode length: 474.55
                 Mean success rate: 95.00
                  Mean reward/step: 29.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 0.56s
                        Total time: 839.33s
                               ETA: 144.0s

################################################################################
                     [1m Learning iteration 1708/2000 [0m

                       Computation: 16615 steps/s (collection: 0.262s, learning 0.231s)
               Value function loss: 78774.3262
                    Surrogate loss: -0.0029
             Mean action noise std: 1.01
                       Mean reward: 14350.93
               Mean episode length: 468.50
                 Mean success rate: 93.50
                  Mean reward/step: 30.53
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14000128
                    Iteration time: 0.49s
                        Total time: 839.83s
                               ETA: 143.5s

################################################################################
                     [1m Learning iteration 1709/2000 [0m

                       Computation: 17481 steps/s (collection: 0.246s, learning 0.222s)
               Value function loss: 74250.1484
                    Surrogate loss: -0.0018
             Mean action noise std: 1.01
                       Mean reward: 14382.23
               Mean episode length: 468.50
                 Mean success rate: 93.50
                  Mean reward/step: 32.05
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 0.47s
                        Total time: 840.30s
                               ETA: 143.0s

################################################################################
                     [1m Learning iteration 1710/2000 [0m

                       Computation: 16985 steps/s (collection: 0.269s, learning 0.213s)
               Value function loss: 90924.8165
                    Surrogate loss: -0.0031
             Mean action noise std: 1.01
                       Mean reward: 14535.01
               Mean episode length: 472.52
                 Mean success rate: 94.00
                  Mean reward/step: 31.91
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14016512
                    Iteration time: 0.48s
                        Total time: 840.78s
                               ETA: 142.5s

################################################################################
                     [1m Learning iteration 1711/2000 [0m

                       Computation: 17542 steps/s (collection: 0.256s, learning 0.211s)
               Value function loss: 97280.1256
                    Surrogate loss: -0.0012
             Mean action noise std: 1.01
                       Mean reward: 14514.91
               Mean episode length: 472.52
                 Mean success rate: 94.00
                  Mean reward/step: 31.86
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 0.47s
                        Total time: 841.24s
                               ETA: 142.0s

################################################################################
                     [1m Learning iteration 1712/2000 [0m

                       Computation: 15879 steps/s (collection: 0.290s, learning 0.226s)
               Value function loss: 114431.3302
                    Surrogate loss: -0.0028
             Mean action noise std: 1.01
                       Mean reward: 14910.50
               Mean episode length: 481.18
                 Mean success rate: 96.00
                  Mean reward/step: 31.00
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14032896
                    Iteration time: 0.52s
                        Total time: 841.76s
                               ETA: 141.5s

################################################################################
                     [1m Learning iteration 1713/2000 [0m

                       Computation: 16305 steps/s (collection: 0.287s, learning 0.215s)
               Value function loss: 84874.5661
                    Surrogate loss: -0.0028
             Mean action noise std: 1.01
                       Mean reward: 14913.25
               Mean episode length: 481.18
                 Mean success rate: 96.00
                  Mean reward/step: 31.50
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 0.50s
                        Total time: 842.26s
                               ETA: 141.0s

################################################################################
                     [1m Learning iteration 1714/2000 [0m

                       Computation: 16279 steps/s (collection: 0.275s, learning 0.228s)
               Value function loss: 127943.5879
                    Surrogate loss: -0.0015
             Mean action noise std: 1.01
                       Mean reward: 14924.95
               Mean episode length: 481.18
                 Mean success rate: 96.00
                  Mean reward/step: 32.43
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14049280
                    Iteration time: 0.50s
                        Total time: 842.77s
                               ETA: 140.5s

################################################################################
                     [1m Learning iteration 1715/2000 [0m

                       Computation: 16123 steps/s (collection: 0.298s, learning 0.210s)
               Value function loss: 94620.6964
                    Surrogate loss: -0.0036
             Mean action noise std: 1.01
                       Mean reward: 14958.40
               Mean episode length: 481.18
                 Mean success rate: 96.00
                  Mean reward/step: 31.26
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.51s
                        Total time: 843.27s
                               ETA: 140.1s

################################################################################
                     [1m Learning iteration 1716/2000 [0m

                       Computation: 17136 steps/s (collection: 0.274s, learning 0.204s)
               Value function loss: 93142.3514
                    Surrogate loss: -0.0031
             Mean action noise std: 1.01
                       Mean reward: 14650.61
               Mean episode length: 472.20
                 Mean success rate: 94.50
                  Mean reward/step: 31.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14065664
                    Iteration time: 0.48s
                        Total time: 843.75s
                               ETA: 139.6s

################################################################################
                     [1m Learning iteration 1717/2000 [0m

                       Computation: 17752 steps/s (collection: 0.261s, learning 0.201s)
               Value function loss: 114688.8795
                    Surrogate loss: -0.0025
             Mean action noise std: 1.01
                       Mean reward: 14509.64
               Mean episode length: 467.61
                 Mean success rate: 93.50
                  Mean reward/step: 31.33
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 0.46s
                        Total time: 844.21s
                               ETA: 139.1s

################################################################################
                     [1m Learning iteration 1718/2000 [0m

                       Computation: 17066 steps/s (collection: 0.273s, learning 0.207s)
               Value function loss: 99450.6110
                    Surrogate loss: -0.0020
             Mean action noise std: 1.01
                       Mean reward: 14652.29
               Mean episode length: 471.69
                 Mean success rate: 94.00
                  Mean reward/step: 31.09
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14082048
                    Iteration time: 0.48s
                        Total time: 844.69s
                               ETA: 138.6s

################################################################################
                     [1m Learning iteration 1719/2000 [0m

                       Computation: 17096 steps/s (collection: 0.272s, learning 0.207s)
               Value function loss: 125544.1705
                    Surrogate loss: -0.0022
             Mean action noise std: 1.01
                       Mean reward: 14945.74
               Mean episode length: 480.38
                 Mean success rate: 96.00
                  Mean reward/step: 31.16
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 0.48s
                        Total time: 845.17s
                               ETA: 138.1s

################################################################################
                     [1m Learning iteration 1720/2000 [0m

                       Computation: 16271 steps/s (collection: 0.290s, learning 0.214s)
               Value function loss: 120820.8477
                    Surrogate loss: -0.0026
             Mean action noise std: 1.01
                       Mean reward: 15055.67
               Mean episode length: 481.69
                 Mean success rate: 96.50
                  Mean reward/step: 30.44
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14098432
                    Iteration time: 0.50s
                        Total time: 845.68s
                               ETA: 137.6s

################################################################################
                     [1m Learning iteration 1721/2000 [0m

                       Computation: 15599 steps/s (collection: 0.296s, learning 0.229s)
               Value function loss: 110929.4465
                    Surrogate loss: -0.0030
             Mean action noise std: 1.01
                       Mean reward: 15010.20
               Mean episode length: 481.00
                 Mean success rate: 97.00
                  Mean reward/step: 30.58
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 0.53s
                        Total time: 846.20s
                               ETA: 137.1s

################################################################################
                     [1m Learning iteration 1722/2000 [0m

                       Computation: 15770 steps/s (collection: 0.285s, learning 0.235s)
               Value function loss: 157734.5781
                    Surrogate loss: -0.0016
             Mean action noise std: 1.01
                       Mean reward: 15047.54
               Mean episode length: 481.00
                 Mean success rate: 97.00
                  Mean reward/step: 30.12
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14114816
                    Iteration time: 0.52s
                        Total time: 846.72s
                               ETA: 136.6s

################################################################################
                     [1m Learning iteration 1723/2000 [0m

                       Computation: 15627 steps/s (collection: 0.287s, learning 0.237s)
               Value function loss: 122888.8300
                    Surrogate loss: -0.0026
             Mean action noise std: 1.01
                       Mean reward: 14821.68
               Mean episode length: 474.69
                 Mean success rate: 96.00
                  Mean reward/step: 29.63
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 0.52s
                        Total time: 847.24s
                               ETA: 136.1s

################################################################################
                     [1m Learning iteration 1724/2000 [0m

                       Computation: 16480 steps/s (collection: 0.282s, learning 0.215s)
               Value function loss: 48795.0310
                    Surrogate loss: -0.0024
             Mean action noise std: 1.01
                       Mean reward: 14664.64
               Mean episode length: 470.00
                 Mean success rate: 95.00
                  Mean reward/step: 30.78
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14131200
                    Iteration time: 0.50s
                        Total time: 847.74s
                               ETA: 135.6s

################################################################################
                     [1m Learning iteration 1725/2000 [0m

                       Computation: 17196 steps/s (collection: 0.266s, learning 0.211s)
               Value function loss: 104738.5643
                    Surrogate loss: -0.0020
             Mean action noise std: 1.01
                       Mean reward: 14652.09
               Mean episode length: 470.00
                 Mean success rate: 95.00
                  Mean reward/step: 32.27
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 0.48s
                        Total time: 848.22s
                               ETA: 135.1s

################################################################################
                     [1m Learning iteration 1726/2000 [0m

                       Computation: 17513 steps/s (collection: 0.258s, learning 0.210s)
               Value function loss: 68979.6731
                    Surrogate loss: -0.0025
             Mean action noise std: 1.01
                       Mean reward: 14541.89
               Mean episode length: 467.43
                 Mean success rate: 94.50
                  Mean reward/step: 31.74
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14147584
                    Iteration time: 0.47s
                        Total time: 848.69s
                               ETA: 134.6s

################################################################################
                     [1m Learning iteration 1727/2000 [0m

                       Computation: 16495 steps/s (collection: 0.276s, learning 0.220s)
               Value function loss: 129108.0038
                    Surrogate loss: -0.0018
             Mean action noise std: 1.01
                       Mean reward: 14702.30
               Mean episode length: 473.54
                 Mean success rate: 95.50
                  Mean reward/step: 31.95
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.50s
                        Total time: 849.18s
                               ETA: 134.2s

################################################################################
                     [1m Learning iteration 1728/2000 [0m

                       Computation: 16770 steps/s (collection: 0.279s, learning 0.210s)
               Value function loss: 94773.0833
                    Surrogate loss: -0.0033
             Mean action noise std: 1.01
                       Mean reward: 14755.60
               Mean episode length: 474.89
                 Mean success rate: 96.00
                  Mean reward/step: 31.18
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14163968
                    Iteration time: 0.49s
                        Total time: 849.67s
                               ETA: 133.7s

################################################################################
                     [1m Learning iteration 1729/2000 [0m

                       Computation: 16415 steps/s (collection: 0.282s, learning 0.217s)
               Value function loss: 50851.8339
                    Surrogate loss: -0.0028
             Mean action noise std: 1.01
                       Mean reward: 14610.06
               Mean episode length: 470.01
                 Mean success rate: 95.00
                  Mean reward/step: 32.07
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 0.50s
                        Total time: 850.17s
                               ETA: 133.2s

################################################################################
                     [1m Learning iteration 1730/2000 [0m

                       Computation: 16013 steps/s (collection: 0.279s, learning 0.232s)
               Value function loss: 151709.5935
                    Surrogate loss: -0.0016
             Mean action noise std: 1.01
                       Mean reward: 14474.82
               Mean episode length: 465.82
                 Mean success rate: 94.00
                  Mean reward/step: 32.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14180352
                    Iteration time: 0.51s
                        Total time: 850.68s
                               ETA: 132.7s

################################################################################
                     [1m Learning iteration 1731/2000 [0m

                       Computation: 17991 steps/s (collection: 0.250s, learning 0.206s)
               Value function loss: 100712.0219
                    Surrogate loss: -0.0034
             Mean action noise std: 1.01
                       Mean reward: 14470.54
               Mean episode length: 464.67
                 Mean success rate: 94.00
                  Mean reward/step: 31.03
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 0.46s
                        Total time: 851.14s
                               ETA: 132.2s

################################################################################
                     [1m Learning iteration 1732/2000 [0m

                       Computation: 17012 steps/s (collection: 0.255s, learning 0.227s)
               Value function loss: 99765.2959
                    Surrogate loss: -0.0025
             Mean action noise std: 1.01
                       Mean reward: 14399.64
               Mean episode length: 463.01
                 Mean success rate: 93.50
                  Mean reward/step: 31.48
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14196736
                    Iteration time: 0.48s
                        Total time: 851.62s
                               ETA: 131.7s

################################################################################
                     [1m Learning iteration 1733/2000 [0m

                       Computation: 16692 steps/s (collection: 0.277s, learning 0.214s)
               Value function loss: 101759.8400
                    Surrogate loss: -0.0028
             Mean action noise std: 1.01
                       Mean reward: 14542.13
               Mean episode length: 467.59
                 Mean success rate: 94.00
                  Mean reward/step: 31.38
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 0.49s
                        Total time: 852.11s
                               ETA: 131.2s

################################################################################
                     [1m Learning iteration 1734/2000 [0m

                       Computation: 17386 steps/s (collection: 0.266s, learning 0.205s)
               Value function loss: 105936.7490
                    Surrogate loss: -0.0024
             Mean action noise std: 1.01
                       Mean reward: 14482.26
               Mean episode length: 467.59
                 Mean success rate: 94.00
                  Mean reward/step: 31.49
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14213120
                    Iteration time: 0.47s
                        Total time: 852.58s
                               ETA: 130.7s

################################################################################
                     [1m Learning iteration 1735/2000 [0m

                       Computation: 16854 steps/s (collection: 0.277s, learning 0.209s)
               Value function loss: 136641.7689
                    Surrogate loss: -0.0016
             Mean action noise std: 1.01
                       Mean reward: 14732.55
               Mean episode length: 473.91
                 Mean success rate: 95.00
                  Mean reward/step: 31.36
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 0.49s
                        Total time: 853.07s
                               ETA: 130.2s

################################################################################
                     [1m Learning iteration 1736/2000 [0m

                       Computation: 16220 steps/s (collection: 0.289s, learning 0.216s)
               Value function loss: 130797.0841
                    Surrogate loss: -0.0021
             Mean action noise std: 1.01
                       Mean reward: 14567.84
               Mean episode length: 470.33
                 Mean success rate: 95.00
                  Mean reward/step: 30.43
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14229504
                    Iteration time: 0.51s
                        Total time: 853.57s
                               ETA: 129.7s

################################################################################
                     [1m Learning iteration 1737/2000 [0m

                       Computation: 16525 steps/s (collection: 0.273s, learning 0.223s)
               Value function loss: 216552.2108
                    Surrogate loss: -0.0018
             Mean action noise std: 1.01
                       Mean reward: 14701.04
               Mean episode length: 472.91
                 Mean success rate: 95.50
                  Mean reward/step: 30.68
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 0.50s
                        Total time: 854.07s
                               ETA: 129.2s

################################################################################
                     [1m Learning iteration 1738/2000 [0m

                       Computation: 16205 steps/s (collection: 0.297s, learning 0.208s)
               Value function loss: 186211.7984
                    Surrogate loss: -0.0018
             Mean action noise std: 1.01
                       Mean reward: 14821.68
               Mean episode length: 475.77
                 Mean success rate: 96.00
                  Mean reward/step: 30.30
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14245888
                    Iteration time: 0.51s
                        Total time: 854.57s
                               ETA: 128.8s

################################################################################
                     [1m Learning iteration 1739/2000 [0m

                       Computation: 16464 steps/s (collection: 0.283s, learning 0.214s)
               Value function loss: 108620.9277
                    Surrogate loss: -0.0032
             Mean action noise std: 1.01
                       Mean reward: 14964.05
               Mean episode length: 479.01
                 Mean success rate: 96.50
                  Mean reward/step: 29.77
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.50s
                        Total time: 855.07s
                               ETA: 128.3s

################################################################################
                     [1m Learning iteration 1740/2000 [0m

                       Computation: 17428 steps/s (collection: 0.254s, learning 0.216s)
               Value function loss: 60963.7828
                    Surrogate loss: -0.0026
             Mean action noise std: 1.01
                       Mean reward: 14970.50
               Mean episode length: 479.01
                 Mean success rate: 96.50
                  Mean reward/step: 31.10
       Mean episode length/episode: 31.27
--------------------------------------------------------------------------------
                   Total timesteps: 14262272
                    Iteration time: 0.47s
                        Total time: 855.54s
                               ETA: 127.8s

################################################################################
                     [1m Learning iteration 1741/2000 [0m

                       Computation: 16903 steps/s (collection: 0.267s, learning 0.218s)
               Value function loss: 112199.3848
                    Surrogate loss: -0.0025
             Mean action noise std: 1.01
                       Mean reward: 14972.75
               Mean episode length: 479.96
                 Mean success rate: 97.00
                  Mean reward/step: 31.28
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 0.48s
                        Total time: 856.03s
                               ETA: 127.3s

################################################################################
                     [1m Learning iteration 1742/2000 [0m

                       Computation: 17719 steps/s (collection: 0.253s, learning 0.209s)
               Value function loss: 83897.4250
                    Surrogate loss: -0.0026
             Mean action noise std: 1.01
                       Mean reward: 14958.37
               Mean episode length: 479.96
                 Mean success rate: 97.00
                  Mean reward/step: 31.44
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14278656
                    Iteration time: 0.46s
                        Total time: 856.49s
                               ETA: 126.8s

################################################################################
                     [1m Learning iteration 1743/2000 [0m

                       Computation: 17860 steps/s (collection: 0.253s, learning 0.205s)
               Value function loss: 133203.4360
                    Surrogate loss: -0.0018
             Mean action noise std: 1.01
                       Mean reward: 15077.99
               Mean episode length: 483.61
                 Mean success rate: 97.50
                  Mean reward/step: 31.04
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 0.46s
                        Total time: 856.95s
                               ETA: 126.3s

################################################################################
                     [1m Learning iteration 1744/2000 [0m

                       Computation: 17908 steps/s (collection: 0.254s, learning 0.203s)
               Value function loss: 71597.6603
                    Surrogate loss: -0.0031
             Mean action noise std: 1.01
                       Mean reward: 15064.82
               Mean episode length: 483.61
                 Mean success rate: 97.50
                  Mean reward/step: 30.61
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14295040
                    Iteration time: 0.46s
                        Total time: 857.40s
                               ETA: 125.8s

################################################################################
                     [1m Learning iteration 1745/2000 [0m

                       Computation: 17445 steps/s (collection: 0.261s, learning 0.208s)
               Value function loss: 82759.8531
                    Surrogate loss: -0.0017
             Mean action noise std: 1.01
                       Mean reward: 14916.74
               Mean episode length: 479.25
                 Mean success rate: 97.00
                  Mean reward/step: 31.79
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 0.47s
                        Total time: 857.87s
                               ETA: 125.3s

################################################################################
                     [1m Learning iteration 1746/2000 [0m

                       Computation: 17289 steps/s (collection: 0.272s, learning 0.202s)
               Value function loss: 119480.4338
                    Surrogate loss: -0.0020
             Mean action noise std: 1.01
                       Mean reward: 14965.85
               Mean episode length: 479.25
                 Mean success rate: 97.00
                  Mean reward/step: 30.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14311424
                    Iteration time: 0.47s
                        Total time: 858.35s
                               ETA: 124.8s

################################################################################
                     [1m Learning iteration 1747/2000 [0m

                       Computation: 17600 steps/s (collection: 0.255s, learning 0.210s)
               Value function loss: 82481.6278
                    Surrogate loss: -0.0033
             Mean action noise std: 1.01
                       Mean reward: 14834.37
               Mean episode length: 475.10
                 Mean success rate: 96.50
                  Mean reward/step: 30.94
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 0.47s
                        Total time: 858.81s
                               ETA: 124.3s

################################################################################
                     [1m Learning iteration 1748/2000 [0m

                       Computation: 17301 steps/s (collection: 0.265s, learning 0.208s)
               Value function loss: 109531.8100
                    Surrogate loss: -0.0027
             Mean action noise std: 1.01
                       Mean reward: 14666.45
               Mean episode length: 470.09
                 Mean success rate: 95.50
                  Mean reward/step: 31.47
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14327808
                    Iteration time: 0.47s
                        Total time: 859.29s
                               ETA: 123.8s

################################################################################
                     [1m Learning iteration 1749/2000 [0m

                       Computation: 16772 steps/s (collection: 0.278s, learning 0.210s)
               Value function loss: 114319.5920
                    Surrogate loss: -0.0022
             Mean action noise std: 1.01
                       Mean reward: 14672.21
               Mean episode length: 470.09
                 Mean success rate: 95.50
                  Mean reward/step: 31.46
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 0.49s
                        Total time: 859.77s
                               ETA: 123.3s

################################################################################
                     [1m Learning iteration 1750/2000 [0m

                       Computation: 17254 steps/s (collection: 0.269s, learning 0.206s)
               Value function loss: 92491.8770
                    Surrogate loss: -0.0015
             Mean action noise std: 1.01
                       Mean reward: 14624.22
               Mean episode length: 470.09
                 Mean success rate: 95.50
                  Mean reward/step: 31.63
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14344192
                    Iteration time: 0.47s
                        Total time: 860.25s
                               ETA: 122.8s

################################################################################
                     [1m Learning iteration 1751/2000 [0m

                       Computation: 16409 steps/s (collection: 0.286s, learning 0.214s)
               Value function loss: 137914.7668
                    Surrogate loss: -0.0018
             Mean action noise std: 1.01
                       Mean reward: 14622.84
               Mean episode length: 470.09
                 Mean success rate: 95.50
                  Mean reward/step: 31.09
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.50s
                        Total time: 860.75s
                               ETA: 122.3s

################################################################################
                     [1m Learning iteration 1752/2000 [0m

                       Computation: 17461 steps/s (collection: 0.265s, learning 0.204s)
               Value function loss: 119679.1444
                    Surrogate loss: -0.0019
             Mean action noise std: 1.01
                       Mean reward: 14777.90
               Mean episode length: 473.70
                 Mean success rate: 96.00
                  Mean reward/step: 30.37
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14360576
                    Iteration time: 0.47s
                        Total time: 861.22s
                               ETA: 121.8s

################################################################################
                     [1m Learning iteration 1753/2000 [0m

                       Computation: 16990 steps/s (collection: 0.276s, learning 0.206s)
               Value function loss: 158522.0129
                    Surrogate loss: -0.0020
             Mean action noise std: 1.01
                       Mean reward: 14910.99
               Mean episode length: 478.22
                 Mean success rate: 97.00
                  Mean reward/step: 30.13
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 0.48s
                        Total time: 861.70s
                               ETA: 121.3s

################################################################################
                     [1m Learning iteration 1754/2000 [0m

                       Computation: 16433 steps/s (collection: 0.287s, learning 0.212s)
               Value function loss: 146464.7658
                    Surrogate loss: -0.0024
             Mean action noise std: 1.01
                       Mean reward: 14754.94
               Mean episode length: 473.46
                 Mean success rate: 96.00
                  Mean reward/step: 29.75
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14376960
                    Iteration time: 0.50s
                        Total time: 862.20s
                               ETA: 120.9s

################################################################################
                     [1m Learning iteration 1755/2000 [0m

                       Computation: 16629 steps/s (collection: 0.283s, learning 0.209s)
               Value function loss: 76962.8850
                    Surrogate loss: -0.0022
             Mean action noise std: 1.01
                       Mean reward: 14424.21
               Mean episode length: 464.68
                 Mean success rate: 94.00
                  Mean reward/step: 30.09
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 0.49s
                        Total time: 862.69s
                               ETA: 120.4s

################################################################################
                     [1m Learning iteration 1756/2000 [0m

                       Computation: 17430 steps/s (collection: 0.267s, learning 0.203s)
               Value function loss: 69816.4325
                    Surrogate loss: -0.0029
             Mean action noise std: 1.01
                       Mean reward: 14284.19
               Mean episode length: 460.31
                 Mean success rate: 93.00
                  Mean reward/step: 31.78
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14393344
                    Iteration time: 0.47s
                        Total time: 863.16s
                               ETA: 119.9s

################################################################################
                     [1m Learning iteration 1757/2000 [0m

                       Computation: 17812 steps/s (collection: 0.251s, learning 0.209s)
               Value function loss: 72788.7817
                    Surrogate loss: -0.0021
             Mean action noise std: 1.01
                       Mean reward: 14099.03
               Mean episode length: 455.63
                 Mean success rate: 92.00
                  Mean reward/step: 31.51
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 0.46s
                        Total time: 863.62s
                               ETA: 119.4s

################################################################################
                     [1m Learning iteration 1758/2000 [0m

                       Computation: 16396 steps/s (collection: 0.293s, learning 0.207s)
               Value function loss: 141767.8687
                    Surrogate loss: -0.0010
             Mean action noise std: 1.01
                       Mean reward: 14387.15
               Mean episode length: 464.50
                 Mean success rate: 93.50
                  Mean reward/step: 31.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14409728
                    Iteration time: 0.50s
                        Total time: 864.12s
                               ETA: 118.9s

################################################################################
                     [1m Learning iteration 1759/2000 [0m

                       Computation: 17362 steps/s (collection: 0.267s, learning 0.205s)
               Value function loss: 91233.5403
                    Surrogate loss: -0.0031
             Mean action noise std: 1.01
                       Mean reward: 14625.94
               Mean episode length: 470.56
                 Mean success rate: 94.00
                  Mean reward/step: 30.09
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 0.47s
                        Total time: 864.59s
                               ETA: 118.4s

################################################################################
                     [1m Learning iteration 1760/2000 [0m

                       Computation: 16686 steps/s (collection: 0.286s, learning 0.205s)
               Value function loss: 64293.0157
                    Surrogate loss: -0.0029
             Mean action noise std: 1.01
                       Mean reward: 14125.55
               Mean episode length: 459.48
                 Mean success rate: 92.50
                  Mean reward/step: 31.23
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14426112
                    Iteration time: 0.49s
                        Total time: 865.08s
                               ETA: 117.9s

################################################################################
                     [1m Learning iteration 1761/2000 [0m

                       Computation: 17268 steps/s (collection: 0.264s, learning 0.211s)
               Value function loss: 132337.5012
                    Surrogate loss: -0.0015
             Mean action noise std: 1.01
                       Mean reward: 14044.81
               Mean episode length: 454.75
                 Mean success rate: 91.50
                  Mean reward/step: 31.99
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 0.47s
                        Total time: 865.56s
                               ETA: 117.4s

################################################################################
                     [1m Learning iteration 1762/2000 [0m

                       Computation: 17427 steps/s (collection: 0.261s, learning 0.209s)
               Value function loss: 105629.7121
                    Surrogate loss: -0.0018
             Mean action noise std: 1.01
                       Mean reward: 13977.83
               Mean episode length: 452.60
                 Mean success rate: 91.00
                  Mean reward/step: 31.15
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14442496
                    Iteration time: 0.47s
                        Total time: 866.03s
                               ETA: 116.9s

################################################################################
                     [1m Learning iteration 1763/2000 [0m

                       Computation: 17155 steps/s (collection: 0.272s, learning 0.205s)
               Value function loss: 89032.0894
                    Surrogate loss: -0.0028
             Mean action noise std: 1.01
                       Mean reward: 13966.40
               Mean episode length: 452.60
                 Mean success rate: 91.00
                  Mean reward/step: 31.69
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.48s
                        Total time: 866.50s
                               ETA: 116.4s

################################################################################
                     [1m Learning iteration 1764/2000 [0m

                       Computation: 17353 steps/s (collection: 0.264s, learning 0.208s)
               Value function loss: 110751.9468
                    Surrogate loss: -0.0024
             Mean action noise std: 1.01
                       Mean reward: 13953.58
               Mean episode length: 452.60
                 Mean success rate: 91.00
                  Mean reward/step: 31.82
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14458880
                    Iteration time: 0.47s
                        Total time: 866.98s
                               ETA: 115.9s

################################################################################
                     [1m Learning iteration 1765/2000 [0m

                       Computation: 17411 steps/s (collection: 0.263s, learning 0.207s)
               Value function loss: 94345.1983
                    Surrogate loss: -0.0021
             Mean action noise std: 1.01
                       Mean reward: 13956.05
               Mean episode length: 452.60
                 Mean success rate: 91.00
                  Mean reward/step: 31.54
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 0.47s
                        Total time: 867.45s
                               ETA: 115.4s

################################################################################
                     [1m Learning iteration 1766/2000 [0m

                       Computation: 17552 steps/s (collection: 0.258s, learning 0.209s)
               Value function loss: 120192.2297
                    Surrogate loss: -0.0025
             Mean action noise std: 1.01
                       Mean reward: 14168.88
               Mean episode length: 457.36
                 Mean success rate: 92.00
                  Mean reward/step: 31.30
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14475264
                    Iteration time: 0.47s
                        Total time: 867.91s
                               ETA: 114.9s

################################################################################
                     [1m Learning iteration 1767/2000 [0m

                       Computation: 16918 steps/s (collection: 0.283s, learning 0.201s)
               Value function loss: 132660.0451
                    Surrogate loss: -0.0019
             Mean action noise std: 1.01
                       Mean reward: 14633.82
               Mean episode length: 470.50
                 Mean success rate: 94.50
                  Mean reward/step: 30.97
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 0.48s
                        Total time: 868.40s
                               ETA: 114.4s

################################################################################
                     [1m Learning iteration 1768/2000 [0m

                       Computation: 17623 steps/s (collection: 0.261s, learning 0.204s)
               Value function loss: 120631.3203
                    Surrogate loss: -0.0017
             Mean action noise std: 1.01
                       Mean reward: 14624.34
               Mean episode length: 470.84
                 Mean success rate: 94.50
                  Mean reward/step: 31.10
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14491648
                    Iteration time: 0.46s
                        Total time: 868.86s
                               ETA: 113.9s

################################################################################
                     [1m Learning iteration 1769/2000 [0m

                       Computation: 17217 steps/s (collection: 0.265s, learning 0.211s)
               Value function loss: 113467.9643
                    Surrogate loss: -0.0027
             Mean action noise std: 1.01
                       Mean reward: 14772.03
               Mean episode length: 475.51
                 Mean success rate: 95.50
                  Mean reward/step: 30.78
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 0.48s
                        Total time: 869.34s
                               ETA: 113.5s

################################################################################
                     [1m Learning iteration 1770/2000 [0m

                       Computation: 17268 steps/s (collection: 0.269s, learning 0.206s)
               Value function loss: 108529.0276
                    Surrogate loss: -0.0016
             Mean action noise std: 1.01
                       Mean reward: 14723.86
               Mean episode length: 475.51
                 Mean success rate: 95.50
                  Mean reward/step: 30.78
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14508032
                    Iteration time: 0.47s
                        Total time: 869.81s
                               ETA: 113.0s

################################################################################
                     [1m Learning iteration 1771/2000 [0m

                       Computation: 17750 steps/s (collection: 0.258s, learning 0.204s)
               Value function loss: 68233.2309
                    Surrogate loss: -0.0021
             Mean action noise std: 1.01
                       Mean reward: 14985.55
               Mean episode length: 482.21
                 Mean success rate: 96.50
                  Mean reward/step: 31.82
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 0.46s
                        Total time: 870.27s
                               ETA: 112.5s

################################################################################
                     [1m Learning iteration 1772/2000 [0m

                       Computation: 18176 steps/s (collection: 0.240s, learning 0.210s)
               Value function loss: 115744.8779
                    Surrogate loss: -0.0024
             Mean action noise std: 1.01
                       Mean reward: 15111.89
               Mean episode length: 484.94
                 Mean success rate: 97.00
                  Mean reward/step: 32.41
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14524416
                    Iteration time: 0.45s
                        Total time: 870.73s
                               ETA: 112.0s

################################################################################
                     [1m Learning iteration 1773/2000 [0m

                       Computation: 17306 steps/s (collection: 0.262s, learning 0.211s)
               Value function loss: 73073.9658
                    Surrogate loss: -0.0018
             Mean action noise std: 1.01
                       Mean reward: 15268.28
               Mean episode length: 489.67
                 Mean success rate: 98.00
                  Mean reward/step: 32.08
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 0.47s
                        Total time: 871.20s
                               ETA: 111.5s

################################################################################
                     [1m Learning iteration 1774/2000 [0m

                       Computation: 18161 steps/s (collection: 0.244s, learning 0.207s)
               Value function loss: 151804.0740
                    Surrogate loss: -0.0027
             Mean action noise std: 1.01
                       Mean reward: 15358.37
               Mean episode length: 491.81
                 Mean success rate: 98.50
                  Mean reward/step: 31.90
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14540800
                    Iteration time: 0.45s
                        Total time: 871.65s
                               ETA: 111.0s

################################################################################
                     [1m Learning iteration 1775/2000 [0m

                       Computation: 17706 steps/s (collection: 0.239s, learning 0.223s)
               Value function loss: 78596.8926
                    Surrogate loss: -0.0022
             Mean action noise std: 1.01
                       Mean reward: 15369.26
               Mean episode length: 491.81
                 Mean success rate: 98.50
                  Mean reward/step: 30.94
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.46s
                        Total time: 872.11s
                               ETA: 110.5s

################################################################################
                     [1m Learning iteration 1776/2000 [0m

                       Computation: 16841 steps/s (collection: 0.276s, learning 0.210s)
               Value function loss: 61791.0508
                    Surrogate loss: -0.0013
             Mean action noise std: 1.01
                       Mean reward: 15398.92
               Mean episode length: 491.81
                 Mean success rate: 98.50
                  Mean reward/step: 32.11
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 14557184
                    Iteration time: 0.49s
                        Total time: 872.60s
                               ETA: 110.0s

################################################################################
                     [1m Learning iteration 1777/2000 [0m

                       Computation: 17775 steps/s (collection: 0.252s, learning 0.209s)
               Value function loss: 122565.8786
                    Surrogate loss: -0.0015
             Mean action noise std: 1.01
                       Mean reward: 15396.67
               Mean episode length: 491.81
                 Mean success rate: 98.50
                  Mean reward/step: 31.81
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 0.46s
                        Total time: 873.06s
                               ETA: 109.5s

################################################################################
                     [1m Learning iteration 1778/2000 [0m

                       Computation: 17530 steps/s (collection: 0.258s, learning 0.209s)
               Value function loss: 99322.8127
                    Surrogate loss: -0.0023
             Mean action noise std: 1.01
                       Mean reward: 15383.10
               Mean episode length: 491.81
                 Mean success rate: 98.50
                  Mean reward/step: 31.33
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14573568
                    Iteration time: 0.47s
                        Total time: 873.53s
                               ETA: 109.0s

################################################################################
                     [1m Learning iteration 1779/2000 [0m

                       Computation: 17691 steps/s (collection: 0.256s, learning 0.207s)
               Value function loss: 93171.1910
                    Surrogate loss: -0.0026
             Mean action noise std: 1.01
                       Mean reward: 15388.65
               Mean episode length: 491.81
                 Mean success rate: 98.50
                  Mean reward/step: 31.43
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 0.46s
                        Total time: 873.99s
                               ETA: 108.5s

################################################################################
                     [1m Learning iteration 1780/2000 [0m

                       Computation: 17185 steps/s (collection: 0.249s, learning 0.227s)
               Value function loss: 110080.4791
                    Surrogate loss: -0.0020
             Mean action noise std: 1.01
                       Mean reward: 15418.81
               Mean episode length: 491.81
                 Mean success rate: 98.50
                  Mean reward/step: 31.32
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14589952
                    Iteration time: 0.48s
                        Total time: 874.47s
                               ETA: 108.0s

################################################################################
                     [1m Learning iteration 1781/2000 [0m

                       Computation: 17564 steps/s (collection: 0.252s, learning 0.215s)
               Value function loss: 111312.8537
                    Surrogate loss: -0.0015
             Mean action noise std: 1.01
                       Mean reward: 15612.51
               Mean episode length: 495.85
                 Mean success rate: 99.50
                  Mean reward/step: 31.46
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 0.47s
                        Total time: 874.93s
                               ETA: 107.5s

################################################################################
                     [1m Learning iteration 1782/2000 [0m

                       Computation: 17390 steps/s (collection: 0.256s, learning 0.215s)
               Value function loss: 126088.1619
                    Surrogate loss: -0.0019
             Mean action noise std: 1.01
                       Mean reward: 15504.65
               Mean episode length: 491.48
                 Mean success rate: 98.50
                  Mean reward/step: 31.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14606336
                    Iteration time: 0.47s
                        Total time: 875.40s
                               ETA: 107.0s

################################################################################
                     [1m Learning iteration 1783/2000 [0m

                       Computation: 17592 steps/s (collection: 0.266s, learning 0.200s)
               Value function loss: 139558.3887
                    Surrogate loss: -0.0016
             Mean action noise std: 1.01
                       Mean reward: 15288.40
               Mean episode length: 483.79
                 Mean success rate: 97.00
                  Mean reward/step: 31.04
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 0.47s
                        Total time: 875.87s
                               ETA: 106.5s

################################################################################
                     [1m Learning iteration 1784/2000 [0m

                       Computation: 15979 steps/s (collection: 0.264s, learning 0.249s)
               Value function loss: 125374.9006
                    Surrogate loss: -0.0024
             Mean action noise std: 1.01
                       Mean reward: 15467.14
               Mean episode length: 487.94
                 Mean success rate: 97.50
                  Mean reward/step: 31.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14622720
                    Iteration time: 0.51s
                        Total time: 876.38s
                               ETA: 106.0s

################################################################################
                     [1m Learning iteration 1785/2000 [0m

                       Computation: 16544 steps/s (collection: 0.280s, learning 0.215s)
               Value function loss: 138704.6840
                    Surrogate loss: -0.0024
             Mean action noise std: 1.01
                       Mean reward: 15402.56
               Mean episode length: 485.55
                 Mean success rate: 97.00
                  Mean reward/step: 31.06
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 0.50s
                        Total time: 876.88s
                               ETA: 105.6s

################################################################################
                     [1m Learning iteration 1786/2000 [0m

                       Computation: 17860 steps/s (collection: 0.257s, learning 0.201s)
               Value function loss: 101043.1313
                    Surrogate loss: -0.0020
             Mean action noise std: 1.01
                       Mean reward: 15403.07
               Mean episode length: 485.55
                 Mean success rate: 97.00
                  Mean reward/step: 30.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14639104
                    Iteration time: 0.46s
                        Total time: 877.34s
                               ETA: 105.1s

################################################################################
                     [1m Learning iteration 1787/2000 [0m

                       Computation: 17779 steps/s (collection: 0.252s, learning 0.209s)
               Value function loss: 70652.8646
                    Surrogate loss: -0.0017
             Mean action noise std: 1.01
                       Mean reward: 15383.63
               Mean episode length: 485.55
                 Mean success rate: 97.00
                  Mean reward/step: 31.98
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.46s
                        Total time: 877.80s
                               ETA: 104.6s

################################################################################
                     [1m Learning iteration 1788/2000 [0m

                       Computation: 17339 steps/s (collection: 0.266s, learning 0.206s)
               Value function loss: 101271.2354
                    Surrogate loss: -0.0023
             Mean action noise std: 1.01
                       Mean reward: 15092.85
               Mean episode length: 477.79
                 Mean success rate: 95.50
                  Mean reward/step: 31.79
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14655488
                    Iteration time: 0.47s
                        Total time: 878.27s
                               ETA: 104.1s

################################################################################
                     [1m Learning iteration 1789/2000 [0m

                       Computation: 17262 steps/s (collection: 0.253s, learning 0.222s)
               Value function loss: 90782.1814
                    Surrogate loss: -0.0021
             Mean action noise std: 1.01
                       Mean reward: 14945.05
               Mean episode length: 474.88
                 Mean success rate: 95.00
                  Mean reward/step: 32.19
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 0.47s
                        Total time: 878.74s
                               ETA: 103.6s

################################################################################
                     [1m Learning iteration 1790/2000 [0m

                       Computation: 17602 steps/s (collection: 0.264s, learning 0.201s)
               Value function loss: 121289.0569
                    Surrogate loss: -0.0009
             Mean action noise std: 1.01
                       Mean reward: 14957.93
               Mean episode length: 474.88
                 Mean success rate: 95.00
                  Mean reward/step: 31.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14671872
                    Iteration time: 0.47s
                        Total time: 879.21s
                               ETA: 103.1s

################################################################################
                     [1m Learning iteration 1791/2000 [0m

                       Computation: 17498 steps/s (collection: 0.268s, learning 0.201s)
               Value function loss: 74968.5721
                    Surrogate loss: -0.0018
             Mean action noise std: 1.01
                       Mean reward: 14952.88
               Mean episode length: 474.88
                 Mean success rate: 95.00
                  Mean reward/step: 32.36
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 0.47s
                        Total time: 879.68s
                               ETA: 102.6s

################################################################################
                     [1m Learning iteration 1792/2000 [0m

                       Computation: 17782 steps/s (collection: 0.251s, learning 0.210s)
               Value function loss: 90198.2806
                    Surrogate loss: -0.0017
             Mean action noise std: 1.01
                       Mean reward: 14941.72
               Mean episode length: 474.88
                 Mean success rate: 95.00
                  Mean reward/step: 33.03
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14688256
                    Iteration time: 0.46s
                        Total time: 880.14s
                               ETA: 102.1s

################################################################################
                     [1m Learning iteration 1793/2000 [0m

                       Computation: 18283 steps/s (collection: 0.238s, learning 0.210s)
               Value function loss: 101591.5188
                    Surrogate loss: -0.0020
             Mean action noise std: 1.01
                       Mean reward: 14931.78
               Mean episode length: 474.88
                 Mean success rate: 95.00
                  Mean reward/step: 31.92
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 0.45s
                        Total time: 880.59s
                               ETA: 101.6s

################################################################################
                     [1m Learning iteration 1794/2000 [0m

                       Computation: 16683 steps/s (collection: 0.265s, learning 0.226s)
               Value function loss: 90228.7546
                    Surrogate loss: -0.0013
             Mean action noise std: 1.01
                       Mean reward: 15092.33
               Mean episode length: 479.25
                 Mean success rate: 96.00
                  Mean reward/step: 32.07
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14704640
                    Iteration time: 0.49s
                        Total time: 881.08s
                               ETA: 101.1s

################################################################################
                     [1m Learning iteration 1795/2000 [0m

                       Computation: 16436 steps/s (collection: 0.284s, learning 0.214s)
               Value function loss: 93716.8134
                    Surrogate loss: -0.0026
             Mean action noise std: 1.01
                       Mean reward: 15310.98
               Mean episode length: 486.94
                 Mean success rate: 97.50
                  Mean reward/step: 31.86
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 0.50s
                        Total time: 881.58s
                               ETA: 100.6s

################################################################################
                     [1m Learning iteration 1796/2000 [0m

                       Computation: 16318 steps/s (collection: 0.269s, learning 0.233s)
               Value function loss: 107091.8900
                    Surrogate loss: -0.0026
             Mean action noise std: 1.01
                       Mean reward: 15322.81
               Mean episode length: 486.94
                 Mean success rate: 97.50
                  Mean reward/step: 31.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14721024
                    Iteration time: 0.50s
                        Total time: 882.08s
                               ETA: 100.1s

################################################################################
                     [1m Learning iteration 1797/2000 [0m

                       Computation: 17903 steps/s (collection: 0.247s, learning 0.211s)
               Value function loss: 107724.5461
                    Surrogate loss: -0.0017
             Mean action noise std: 1.01
                       Mean reward: 15319.79
               Mean episode length: 486.94
                 Mean success rate: 97.50
                  Mean reward/step: 32.26
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 0.46s
                        Total time: 882.54s
                               ETA: 99.6s

################################################################################
                     [1m Learning iteration 1798/2000 [0m

                       Computation: 16919 steps/s (collection: 0.267s, learning 0.217s)
               Value function loss: 137869.8213
                    Surrogate loss: -0.0014
             Mean action noise std: 1.01
                       Mean reward: 15431.61
               Mean episode length: 489.33
                 Mean success rate: 98.00
                  Mean reward/step: 31.82
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14737408
                    Iteration time: 0.48s
                        Total time: 883.02s
                               ETA: 99.1s

################################################################################
                     [1m Learning iteration 1799/2000 [0m

                       Computation: 16467 steps/s (collection: 0.288s, learning 0.209s)
               Value function loss: 148585.2225
                    Surrogate loss: -0.0015
             Mean action noise std: 1.01
                       Mean reward: 15488.45
               Mean episode length: 490.69
                 Mean success rate: 98.50
                  Mean reward/step: 31.10
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.50s
                        Total time: 883.52s
                               ETA: 98.7s

################################################################################
                     [1m Learning iteration 1800/2000 [0m

                       Computation: 17267 steps/s (collection: 0.266s, learning 0.208s)
               Value function loss: 135938.7219
                    Surrogate loss: -0.0018
             Mean action noise std: 1.01
                       Mean reward: 15465.61
               Mean episode length: 489.27
                 Mean success rate: 98.00
                  Mean reward/step: 30.61
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14753792
                    Iteration time: 0.47s
                        Total time: 883.99s
                               ETA: 98.2s

################################################################################
                     [1m Learning iteration 1801/2000 [0m

                       Computation: 16148 steps/s (collection: 0.259s, learning 0.248s)
               Value function loss: 140973.2766
                    Surrogate loss: -0.0014
             Mean action noise std: 1.01
                       Mean reward: 15645.98
               Mean episode length: 492.18
                 Mean success rate: 98.50
                  Mean reward/step: 30.36
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 0.51s
                        Total time: 884.50s
                               ETA: 97.7s

################################################################################
                     [1m Learning iteration 1802/2000 [0m

                       Computation: 13946 steps/s (collection: 0.325s, learning 0.262s)
               Value function loss: 72315.3769
                    Surrogate loss: -0.0021
             Mean action noise std: 1.01
                       Mean reward: 15618.50
               Mean episode length: 492.18
                 Mean success rate: 98.50
                  Mean reward/step: 31.01
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14770176
                    Iteration time: 0.59s
                        Total time: 885.09s
                               ETA: 97.2s

################################################################################
                     [1m Learning iteration 1803/2000 [0m

                       Computation: 16064 steps/s (collection: 0.283s, learning 0.227s)
               Value function loss: 98759.7678
                    Surrogate loss: -0.0013
             Mean action noise std: 1.01
                       Mean reward: 15478.48
               Mean episode length: 488.09
                 Mean success rate: 97.50
                  Mean reward/step: 32.20
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 0.51s
                        Total time: 885.60s
                               ETA: 96.7s

################################################################################
                     [1m Learning iteration 1804/2000 [0m

                       Computation: 16698 steps/s (collection: 0.280s, learning 0.211s)
               Value function loss: 92252.8217
                    Surrogate loss: -0.0028
             Mean action noise std: 1.01
                       Mean reward: 15452.51
               Mean episode length: 488.09
                 Mean success rate: 97.50
                  Mean reward/step: 31.57
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14786560
                    Iteration time: 0.49s
                        Total time: 886.09s
                               ETA: 96.2s

################################################################################
                     [1m Learning iteration 1805/2000 [0m

                       Computation: 17384 steps/s (collection: 0.260s, learning 0.211s)
               Value function loss: 154459.1736
                    Surrogate loss: -0.0008
             Mean action noise std: 1.01
                       Mean reward: 15494.78
               Mean episode length: 488.09
                 Mean success rate: 97.50
                  Mean reward/step: 31.90
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 0.47s
                        Total time: 886.56s
                               ETA: 95.7s

################################################################################
                     [1m Learning iteration 1806/2000 [0m

                       Computation: 17285 steps/s (collection: 0.256s, learning 0.218s)
               Value function loss: 86695.1731
                    Surrogate loss: -0.0016
             Mean action noise std: 1.01
                       Mean reward: 15369.39
               Mean episode length: 483.54
                 Mean success rate: 96.50
                  Mean reward/step: 30.96
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14802944
                    Iteration time: 0.47s
                        Total time: 887.03s
                               ETA: 95.2s

################################################################################
                     [1m Learning iteration 1807/2000 [0m

                       Computation: 16849 steps/s (collection: 0.270s, learning 0.217s)
               Value function loss: 67748.0165
                    Surrogate loss: -0.0017
             Mean action noise std: 1.01
                       Mean reward: 15182.51
               Mean episode length: 478.75
                 Mean success rate: 95.50
                  Mean reward/step: 32.23
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 0.49s
                        Total time: 887.52s
                               ETA: 94.7s

################################################################################
                     [1m Learning iteration 1808/2000 [0m

                       Computation: 17320 steps/s (collection: 0.256s, learning 0.217s)
               Value function loss: 143603.9799
                    Surrogate loss: -0.0015
             Mean action noise std: 1.01
                       Mean reward: 15193.44
               Mean episode length: 478.75
                 Mean success rate: 95.50
                  Mean reward/step: 32.37
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14819328
                    Iteration time: 0.47s
                        Total time: 887.99s
                               ETA: 94.2s

################################################################################
                     [1m Learning iteration 1809/2000 [0m

                       Computation: 17752 steps/s (collection: 0.250s, learning 0.212s)
               Value function loss: 107068.0725
                    Surrogate loss: -0.0019
             Mean action noise std: 1.01
                       Mean reward: 15184.17
               Mean episode length: 478.75
                 Mean success rate: 95.50
                  Mean reward/step: 31.70
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 0.46s
                        Total time: 888.45s
                               ETA: 93.8s

################################################################################
                     [1m Learning iteration 1810/2000 [0m

                       Computation: 17359 steps/s (collection: 0.263s, learning 0.209s)
               Value function loss: 100311.7137
                    Surrogate loss: -0.0019
             Mean action noise std: 1.01
                       Mean reward: 15179.98
               Mean episode length: 478.75
                 Mean success rate: 95.50
                  Mean reward/step: 32.24
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14835712
                    Iteration time: 0.47s
                        Total time: 888.92s
                               ETA: 93.3s

################################################################################
                     [1m Learning iteration 1811/2000 [0m

                       Computation: 17184 steps/s (collection: 0.266s, learning 0.211s)
               Value function loss: 100293.7577
                    Surrogate loss: -0.0013
             Mean action noise std: 1.01
                       Mean reward: 15038.19
               Mean episode length: 475.50
                 Mean success rate: 95.00
                  Mean reward/step: 32.23
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.48s
                        Total time: 889.40s
                               ETA: 92.8s

################################################################################
                     [1m Learning iteration 1812/2000 [0m

                       Computation: 17773 steps/s (collection: 0.255s, learning 0.206s)
               Value function loss: 95001.3908
                    Surrogate loss: -0.0013
             Mean action noise std: 1.01
                       Mean reward: 15061.77
               Mean episode length: 475.50
                 Mean success rate: 95.00
                  Mean reward/step: 32.08
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14852096
                    Iteration time: 0.46s
                        Total time: 889.86s
                               ETA: 92.3s

################################################################################
                     [1m Learning iteration 1813/2000 [0m

                       Computation: 17843 steps/s (collection: 0.244s, learning 0.215s)
               Value function loss: 120151.0623
                    Surrogate loss: -0.0016
             Mean action noise std: 1.01
                       Mean reward: 15219.91
               Mean episode length: 480.06
                 Mean success rate: 96.00
                  Mean reward/step: 31.91
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 0.46s
                        Total time: 890.32s
                               ETA: 91.8s

################################################################################
                     [1m Learning iteration 1814/2000 [0m

                       Computation: 17807 steps/s (collection: 0.257s, learning 0.203s)
               Value function loss: 150631.1516
                    Surrogate loss: -0.0007
             Mean action noise std: 1.01
                       Mean reward: 15092.24
               Mean episode length: 475.66
                 Mean success rate: 95.50
                  Mean reward/step: 31.61
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14868480
                    Iteration time: 0.46s
                        Total time: 890.78s
                               ETA: 91.3s

################################################################################
                     [1m Learning iteration 1815/2000 [0m

                       Computation: 17417 steps/s (collection: 0.265s, learning 0.205s)
               Value function loss: 107677.9147
                    Surrogate loss: -0.0017
             Mean action noise std: 1.01
                       Mean reward: 15223.04
               Mean episode length: 479.75
                 Mean success rate: 96.50
                  Mean reward/step: 31.32
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 0.47s
                        Total time: 891.25s
                               ETA: 90.8s

################################################################################
                     [1m Learning iteration 1816/2000 [0m

                       Computation: 16534 steps/s (collection: 0.280s, learning 0.215s)
               Value function loss: 124230.4979
                    Surrogate loss: -0.0019
             Mean action noise std: 1.01
                       Mean reward: 15179.37
               Mean episode length: 479.75
                 Mean success rate: 96.50
                  Mean reward/step: 31.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14884864
                    Iteration time: 0.50s
                        Total time: 891.75s
                               ETA: 90.3s

################################################################################
                     [1m Learning iteration 1817/2000 [0m

                       Computation: 17694 steps/s (collection: 0.243s, learning 0.219s)
               Value function loss: 99086.0533
                    Surrogate loss: -0.0020
             Mean action noise std: 1.01
                       Mean reward: 15209.18
               Mean episode length: 479.75
                 Mean success rate: 96.50
                  Mean reward/step: 31.10
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 0.46s
                        Total time: 892.21s
                               ETA: 89.8s

################################################################################
                     [1m Learning iteration 1818/2000 [0m

                       Computation: 17308 steps/s (collection: 0.264s, learning 0.209s)
               Value function loss: 45449.8244
                    Surrogate loss: -0.0016
             Mean action noise std: 1.01
                       Mean reward: 15354.83
               Mean episode length: 484.31
                 Mean success rate: 97.50
                  Mean reward/step: 32.18
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14901248
                    Iteration time: 0.47s
                        Total time: 892.68s
                               ETA: 89.3s

################################################################################
                     [1m Learning iteration 1819/2000 [0m

                       Computation: 16094 steps/s (collection: 0.251s, learning 0.258s)
               Value function loss: 127455.2401
                    Surrogate loss: -0.0011
             Mean action noise std: 1.01
                       Mean reward: 15564.44
               Mean episode length: 489.08
                 Mean success rate: 98.50
                  Mean reward/step: 32.30
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 0.51s
                        Total time: 893.19s
                               ETA: 88.8s

################################################################################
                     [1m Learning iteration 1820/2000 [0m

                       Computation: 17006 steps/s (collection: 0.272s, learning 0.210s)
               Value function loss: 80491.1918
                    Surrogate loss: -0.0020
             Mean action noise std: 1.01
                       Mean reward: 15524.43
               Mean episode length: 488.88
                 Mean success rate: 98.50
                  Mean reward/step: 31.87
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14917632
                    Iteration time: 0.48s
                        Total time: 893.67s
                               ETA: 88.3s

################################################################################
                     [1m Learning iteration 1821/2000 [0m

                       Computation: 17886 steps/s (collection: 0.254s, learning 0.204s)
               Value function loss: 139179.4857
                    Surrogate loss: -0.0011
             Mean action noise std: 1.01
                       Mean reward: 15509.44
               Mean episode length: 488.88
                 Mean success rate: 98.50
                  Mean reward/step: 31.42
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 0.46s
                        Total time: 894.13s
                               ETA: 87.8s

################################################################################
                     [1m Learning iteration 1822/2000 [0m

                       Computation: 17654 steps/s (collection: 0.251s, learning 0.213s)
               Value function loss: 81203.9951
                    Surrogate loss: -0.0013
             Mean action noise std: 1.01
                       Mean reward: 15651.67
               Mean episode length: 492.50
                 Mean success rate: 99.00
                  Mean reward/step: 31.88
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14934016
                    Iteration time: 0.46s
                        Total time: 894.60s
                               ETA: 87.3s

################################################################################
                     [1m Learning iteration 1823/2000 [0m

                       Computation: 16938 steps/s (collection: 0.258s, learning 0.226s)
               Value function loss: 63939.3221
                    Surrogate loss: -0.0007
             Mean action noise std: 1.01
                       Mean reward: 15573.86
               Mean episode length: 490.87
                 Mean success rate: 98.50
                  Mean reward/step: 32.53
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.48s
                        Total time: 895.08s
                               ETA: 86.9s

################################################################################
                     [1m Learning iteration 1824/2000 [0m

                       Computation: 16768 steps/s (collection: 0.258s, learning 0.230s)
               Value function loss: 123615.1422
                    Surrogate loss: -0.0010
             Mean action noise std: 1.01
                       Mean reward: 15519.59
               Mean episode length: 490.87
                 Mean success rate: 98.50
                  Mean reward/step: 32.19
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14950400
                    Iteration time: 0.49s
                        Total time: 895.57s
                               ETA: 86.4s

################################################################################
                     [1m Learning iteration 1825/2000 [0m

                       Computation: 16862 steps/s (collection: 0.272s, learning 0.214s)
               Value function loss: 84276.0838
                    Surrogate loss: -0.0022
             Mean action noise std: 1.01
                       Mean reward: 15541.62
               Mean episode length: 490.87
                 Mean success rate: 98.50
                  Mean reward/step: 31.84
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 0.49s
                        Total time: 896.05s
                               ETA: 85.9s

################################################################################
                     [1m Learning iteration 1826/2000 [0m

                       Computation: 17199 steps/s (collection: 0.262s, learning 0.215s)
               Value function loss: 92811.7184
                    Surrogate loss: -0.0016
             Mean action noise std: 1.01
                       Mean reward: 15604.87
               Mean episode length: 492.76
                 Mean success rate: 98.50
                  Mean reward/step: 31.76
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14966784
                    Iteration time: 0.48s
                        Total time: 896.53s
                               ETA: 85.4s

################################################################################
                     [1m Learning iteration 1827/2000 [0m

                       Computation: 17268 steps/s (collection: 0.265s, learning 0.210s)
               Value function loss: 126710.1420
                    Surrogate loss: -0.0015
             Mean action noise std: 1.01
                       Mean reward: 15629.47
               Mean episode length: 492.76
                 Mean success rate: 98.50
                  Mean reward/step: 30.96
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 0.47s
                        Total time: 897.00s
                               ETA: 84.9s

################################################################################
                     [1m Learning iteration 1828/2000 [0m

                       Computation: 16506 steps/s (collection: 0.277s, learning 0.220s)
               Value function loss: 112712.2239
                    Surrogate loss: -0.0014
             Mean action noise std: 1.01
                       Mean reward: 15690.48
               Mean episode length: 492.76
                 Mean success rate: 98.50
                  Mean reward/step: 30.97
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14983168
                    Iteration time: 0.50s
                        Total time: 897.50s
                               ETA: 84.4s

################################################################################
                     [1m Learning iteration 1829/2000 [0m

                       Computation: 17205 steps/s (collection: 0.272s, learning 0.204s)
               Value function loss: 116799.7130
                    Surrogate loss: -0.0016
             Mean action noise std: 1.01
                       Mean reward: 15430.59
               Mean episode length: 483.43
                 Mean success rate: 96.50
                  Mean reward/step: 31.13
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 0.48s
                        Total time: 897.98s
                               ETA: 83.9s

################################################################################
                     [1m Learning iteration 1830/2000 [0m

                       Computation: 16821 steps/s (collection: 0.268s, learning 0.219s)
               Value function loss: 167674.7861
                    Surrogate loss: -0.0005
             Mean action noise std: 1.01
                       Mean reward: 15400.08
               Mean episode length: 483.43
                 Mean success rate: 96.50
                  Mean reward/step: 30.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14999552
                    Iteration time: 0.49s
                        Total time: 898.46s
                               ETA: 83.4s

################################################################################
                     [1m Learning iteration 1831/2000 [0m

                       Computation: 17603 steps/s (collection: 0.251s, learning 0.215s)
               Value function loss: 131551.8127
                    Surrogate loss: -0.0023
             Mean action noise std: 1.01
                       Mean reward: 15403.78
               Mean episode length: 483.43
                 Mean success rate: 96.50
                  Mean reward/step: 30.24
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 0.47s
                        Total time: 898.93s
                               ETA: 82.9s

################################################################################
                     [1m Learning iteration 1832/2000 [0m

                       Computation: 16798 steps/s (collection: 0.266s, learning 0.222s)
               Value function loss: 138171.4186
                    Surrogate loss: -0.0016
             Mean action noise std: 1.01
                       Mean reward: 15219.45
               Mean episode length: 479.29
                 Mean success rate: 95.50
                  Mean reward/step: 30.49
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15015936
                    Iteration time: 0.49s
                        Total time: 899.42s
                               ETA: 82.4s

################################################################################
                     [1m Learning iteration 1833/2000 [0m

                       Computation: 16180 steps/s (collection: 0.288s, learning 0.218s)
               Value function loss: 86822.4551
                    Surrogate loss: -0.0022
             Mean action noise std: 1.01
                       Mean reward: 15053.63
               Mean episode length: 476.05
                 Mean success rate: 95.00
                  Mean reward/step: 30.61
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 0.51s
                        Total time: 899.92s
                               ETA: 81.9s

################################################################################
                     [1m Learning iteration 1834/2000 [0m

                       Computation: 17345 steps/s (collection: 0.247s, learning 0.226s)
               Value function loss: 94702.0779
                    Surrogate loss: -0.0012
             Mean action noise std: 1.01
                       Mean reward: 15229.73
               Mean episode length: 480.58
                 Mean success rate: 96.00
                  Mean reward/step: 31.92
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15032320
                    Iteration time: 0.47s
                        Total time: 900.40s
                               ETA: 81.5s

################################################################################
                     [1m Learning iteration 1835/2000 [0m

                       Computation: 17916 steps/s (collection: 0.252s, learning 0.206s)
               Value function loss: 77250.5712
                    Surrogate loss: -0.0012
             Mean action noise std: 1.01
                       Mean reward: 15131.75
               Mean episode length: 476.64
                 Mean success rate: 95.00
                  Mean reward/step: 31.16
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.46s
                        Total time: 900.85s
                               ETA: 81.0s

################################################################################
                     [1m Learning iteration 1836/2000 [0m

                       Computation: 16900 steps/s (collection: 0.264s, learning 0.221s)
               Value function loss: 193432.4553
                    Surrogate loss: -0.0021
             Mean action noise std: 1.01
                       Mean reward: 15106.17
               Mean episode length: 476.64
                 Mean success rate: 95.00
                  Mean reward/step: 31.88
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15048704
                    Iteration time: 0.48s
                        Total time: 901.34s
                               ETA: 80.5s

################################################################################
                     [1m Learning iteration 1837/2000 [0m

                       Computation: 16906 steps/s (collection: 0.277s, learning 0.208s)
               Value function loss: 153305.7900
                    Surrogate loss: -0.0022
             Mean action noise std: 1.01
                       Mean reward: 14650.23
               Mean episode length: 467.12
                 Mean success rate: 93.50
                  Mean reward/step: 30.78
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 0.48s
                        Total time: 901.82s
                               ETA: 80.0s

################################################################################
                     [1m Learning iteration 1838/2000 [0m

                       Computation: 17533 steps/s (collection: 0.259s, learning 0.208s)
               Value function loss: 85614.7797
                    Surrogate loss: -0.0009
             Mean action noise std: 1.01
                       Mean reward: 14642.85
               Mean episode length: 467.12
                 Mean success rate: 93.50
                  Mean reward/step: 32.04
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15065088
                    Iteration time: 0.47s
                        Total time: 902.29s
                               ETA: 79.5s

################################################################################
                     [1m Learning iteration 1839/2000 [0m

                       Computation: 17474 steps/s (collection: 0.259s, learning 0.209s)
               Value function loss: 138362.3096
                    Surrogate loss: -0.0010
             Mean action noise std: 1.01
                       Mean reward: 14560.33
               Mean episode length: 467.12
                 Mean success rate: 93.50
                  Mean reward/step: 32.08
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 0.47s
                        Total time: 902.76s
                               ETA: 79.0s

################################################################################
                     [1m Learning iteration 1840/2000 [0m

                       Computation: 17011 steps/s (collection: 0.268s, learning 0.213s)
               Value function loss: 85779.3053
                    Surrogate loss: -0.0018
             Mean action noise std: 1.01
                       Mean reward: 14734.16
               Mean episode length: 472.80
                 Mean success rate: 95.00
                  Mean reward/step: 31.16
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15081472
                    Iteration time: 0.48s
                        Total time: 903.24s
                               ETA: 78.5s

################################################################################
                     [1m Learning iteration 1841/2000 [0m

                       Computation: 17505 steps/s (collection: 0.265s, learning 0.203s)
               Value function loss: 84647.3591
                    Surrogate loss: -0.0020
             Mean action noise std: 1.01
                       Mean reward: 14624.76
               Mean episode length: 470.72
                 Mean success rate: 94.50
                  Mean reward/step: 31.55
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 0.47s
                        Total time: 903.71s
                               ETA: 78.0s

################################################################################
                     [1m Learning iteration 1842/2000 [0m

                       Computation: 16874 steps/s (collection: 0.280s, learning 0.206s)
               Value function loss: 123039.7963
                    Surrogate loss: -0.0017
             Mean action noise std: 1.01
                       Mean reward: 14262.75
               Mean episode length: 461.75
                 Mean success rate: 93.00
                  Mean reward/step: 31.76
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15097856
                    Iteration time: 0.49s
                        Total time: 904.19s
                               ETA: 77.5s

################################################################################
                     [1m Learning iteration 1843/2000 [0m

                       Computation: 17335 steps/s (collection: 0.256s, learning 0.216s)
               Value function loss: 83910.2515
                    Surrogate loss: -0.0018
             Mean action noise std: 1.01
                       Mean reward: 14252.71
               Mean episode length: 461.75
                 Mean success rate: 93.00
                  Mean reward/step: 31.19
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 0.47s
                        Total time: 904.67s
                               ETA: 77.0s

################################################################################
                     [1m Learning iteration 1844/2000 [0m

                       Computation: 17213 steps/s (collection: 0.268s, learning 0.208s)
               Value function loss: 106549.4121
                    Surrogate loss: -0.0015
             Mean action noise std: 1.01
                       Mean reward: 14456.54
               Mean episode length: 466.09
                 Mean success rate: 94.00
                  Mean reward/step: 31.91
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15114240
                    Iteration time: 0.48s
                        Total time: 905.14s
                               ETA: 76.5s

################################################################################
                     [1m Learning iteration 1845/2000 [0m

                       Computation: 16348 steps/s (collection: 0.259s, learning 0.242s)
               Value function loss: 144160.6227
                    Surrogate loss: -0.0009
             Mean action noise std: 1.01
                       Mean reward: 14331.98
               Mean episode length: 461.51
                 Mean success rate: 93.00
                  Mean reward/step: 31.62
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 0.50s
                        Total time: 905.64s
                               ETA: 76.0s

################################################################################
                     [1m Learning iteration 1846/2000 [0m

                       Computation: 16411 steps/s (collection: 0.279s, learning 0.220s)
               Value function loss: 109019.6511
                    Surrogate loss: -0.0014
             Mean action noise std: 1.01
                       Mean reward: 14314.10
               Mean episode length: 461.51
                 Mean success rate: 93.00
                  Mean reward/step: 30.96
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15130624
                    Iteration time: 0.50s
                        Total time: 906.14s
                               ETA: 75.6s

################################################################################
                     [1m Learning iteration 1847/2000 [0m

                       Computation: 16665 steps/s (collection: 0.274s, learning 0.217s)
               Value function loss: 129551.0351
                    Surrogate loss: -0.0012
             Mean action noise std: 1.01
                       Mean reward: 14338.98
               Mean episode length: 461.54
                 Mean success rate: 93.00
                  Mean reward/step: 31.39
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.49s
                        Total time: 906.63s
                               ETA: 75.1s

################################################################################
                     [1m Learning iteration 1848/2000 [0m

                       Computation: 17144 steps/s (collection: 0.274s, learning 0.204s)
               Value function loss: 133586.3365
                    Surrogate loss: -0.0014
             Mean action noise std: 1.01
                       Mean reward: 14729.70
               Mean episode length: 470.12
                 Mean success rate: 94.50
                  Mean reward/step: 30.96
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15147008
                    Iteration time: 0.48s
                        Total time: 907.11s
                               ETA: 74.6s

################################################################################
                     [1m Learning iteration 1849/2000 [0m

                       Computation: 16909 steps/s (collection: 0.275s, learning 0.209s)
               Value function loss: 58503.5741
                    Surrogate loss: -0.0017
             Mean action noise std: 1.01
                       Mean reward: 14849.16
               Mean episode length: 473.57
                 Mean success rate: 95.00
                  Mean reward/step: 31.55
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 0.48s
                        Total time: 907.60s
                               ETA: 74.1s

################################################################################
                     [1m Learning iteration 1850/2000 [0m

                       Computation: 16865 steps/s (collection: 0.262s, learning 0.224s)
               Value function loss: 128242.6020
                    Surrogate loss: -0.0021
             Mean action noise std: 1.01
                       Mean reward: 14913.01
               Mean episode length: 473.57
                 Mean success rate: 95.00
                  Mean reward/step: 32.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15163392
                    Iteration time: 0.49s
                        Total time: 908.08s
                               ETA: 73.6s

################################################################################
                     [1m Learning iteration 1851/2000 [0m

                       Computation: 16974 steps/s (collection: 0.269s, learning 0.214s)
               Value function loss: 82642.9427
                    Surrogate loss: -0.0012
             Mean action noise std: 1.01
                       Mean reward: 14765.67
               Mean episode length: 469.61
                 Mean success rate: 94.50
                  Mean reward/step: 31.84
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 0.48s
                        Total time: 908.56s
                               ETA: 73.1s

################################################################################
                     [1m Learning iteration 1852/2000 [0m

                       Computation: 17248 steps/s (collection: 0.276s, learning 0.199s)
               Value function loss: 149737.4518
                    Surrogate loss: -0.0011
             Mean action noise std: 1.01
                       Mean reward: 14809.96
               Mean episode length: 471.06
                 Mean success rate: 94.50
                  Mean reward/step: 31.94
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15179776
                    Iteration time: 0.47s
                        Total time: 909.04s
                               ETA: 72.6s

################################################################################
                     [1m Learning iteration 1853/2000 [0m

                       Computation: 17767 steps/s (collection: 0.256s, learning 0.205s)
               Value function loss: 70562.9094
                    Surrogate loss: -0.0015
             Mean action noise std: 1.01
                       Mean reward: 14884.52
               Mean episode length: 471.39
                 Mean success rate: 94.50
                  Mean reward/step: 31.48
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 0.46s
                        Total time: 909.50s
                               ETA: 72.1s

################################################################################
                     [1m Learning iteration 1854/2000 [0m

                       Computation: 17033 steps/s (collection: 0.271s, learning 0.210s)
               Value function loss: 57681.3851
                    Surrogate loss: -0.0012
             Mean action noise std: 1.01
                       Mean reward: 15025.40
               Mean episode length: 476.04
                 Mean success rate: 95.00
                  Mean reward/step: 32.81
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 15196160
                    Iteration time: 0.48s
                        Total time: 909.98s
                               ETA: 71.6s

################################################################################
                     [1m Learning iteration 1855/2000 [0m

                       Computation: 17662 steps/s (collection: 0.265s, learning 0.199s)
               Value function loss: 139434.4555
                    Surrogate loss: -0.0012
             Mean action noise std: 1.01
                       Mean reward: 14826.45
               Mean episode length: 471.30
                 Mean success rate: 94.00
                  Mean reward/step: 32.53
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 0.46s
                        Total time: 910.44s
                               ETA: 71.1s

################################################################################
                     [1m Learning iteration 1856/2000 [0m

                       Computation: 17329 steps/s (collection: 0.267s, learning 0.206s)
               Value function loss: 97969.7507
                    Surrogate loss: -0.0019
             Mean action noise std: 1.01
                       Mean reward: 14993.61
               Mean episode length: 474.94
                 Mean success rate: 94.50
                  Mean reward/step: 32.12
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15212544
                    Iteration time: 0.47s
                        Total time: 910.92s
                               ETA: 70.6s

################################################################################
                     [1m Learning iteration 1857/2000 [0m

                       Computation: 17891 steps/s (collection: 0.254s, learning 0.204s)
               Value function loss: 96365.2049
                    Surrogate loss: -0.0019
             Mean action noise std: 1.01
                       Mean reward: 15166.83
               Mean episode length: 479.11
                 Mean success rate: 95.50
                  Mean reward/step: 32.82
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 0.46s
                        Total time: 911.38s
                               ETA: 70.1s

################################################################################
                     [1m Learning iteration 1858/2000 [0m

                       Computation: 17528 steps/s (collection: 0.262s, learning 0.205s)
               Value function loss: 111669.3835
                    Surrogate loss: -0.0013
             Mean action noise std: 1.01
                       Mean reward: 15172.95
               Mean episode length: 479.11
                 Mean success rate: 95.50
                  Mean reward/step: 32.26
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15228928
                    Iteration time: 0.47s
                        Total time: 911.84s
                               ETA: 69.7s

################################################################################
                     [1m Learning iteration 1859/2000 [0m

                       Computation: 16587 steps/s (collection: 0.284s, learning 0.210s)
               Value function loss: 91801.8512
                    Surrogate loss: -0.0013
             Mean action noise std: 1.01
                       Mean reward: 15164.63
               Mean episode length: 479.11
                 Mean success rate: 95.50
                  Mean reward/step: 32.08
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.49s
                        Total time: 912.34s
                               ETA: 69.2s

################################################################################
                     [1m Learning iteration 1860/2000 [0m

                       Computation: 16325 steps/s (collection: 0.262s, learning 0.240s)
               Value function loss: 125390.2559
                    Surrogate loss: -0.0011
             Mean action noise std: 1.01
                       Mean reward: 15329.18
               Mean episode length: 483.03
                 Mean success rate: 96.50
                  Mean reward/step: 32.02
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15245312
                    Iteration time: 0.50s
                        Total time: 912.84s
                               ETA: 68.7s

################################################################################
                     [1m Learning iteration 1861/2000 [0m

                       Computation: 16448 steps/s (collection: 0.280s, learning 0.218s)
               Value function loss: 170172.4689
                    Surrogate loss: -0.0007
             Mean action noise std: 1.01
                       Mean reward: 14969.10
               Mean episode length: 471.84
                 Mean success rate: 94.50
                  Mean reward/step: 30.84
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 0.50s
                        Total time: 913.34s
                               ETA: 68.2s

################################################################################
                     [1m Learning iteration 1862/2000 [0m

                       Computation: 17298 steps/s (collection: 0.268s, learning 0.205s)
               Value function loss: 116191.4662
                    Surrogate loss: -0.0013
             Mean action noise std: 1.01
                       Mean reward: 15084.34
               Mean episode length: 475.81
                 Mean success rate: 95.00
                  Mean reward/step: 30.47
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15261696
                    Iteration time: 0.47s
                        Total time: 913.81s
                               ETA: 67.7s

################################################################################
                     [1m Learning iteration 1863/2000 [0m

                       Computation: 16873 steps/s (collection: 0.280s, learning 0.205s)
               Value function loss: 146248.8445
                    Surrogate loss: -0.0010
             Mean action noise std: 1.01
                       Mean reward: 15023.13
               Mean episode length: 473.40
                 Mean success rate: 94.50
                  Mean reward/step: 31.35
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 0.49s
                        Total time: 914.30s
                               ETA: 67.2s

################################################################################
                     [1m Learning iteration 1864/2000 [0m

                       Computation: 17564 steps/s (collection: 0.263s, learning 0.203s)
               Value function loss: 122812.2838
                    Surrogate loss: -0.0011
             Mean action noise std: 1.01
                       Mean reward: 15394.51
               Mean episode length: 481.67
                 Mean success rate: 96.50
                  Mean reward/step: 31.09
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15278080
                    Iteration time: 0.47s
                        Total time: 914.76s
                               ETA: 66.7s

################################################################################
                     [1m Learning iteration 1865/2000 [0m

                       Computation: 18128 steps/s (collection: 0.249s, learning 0.203s)
               Value function loss: 44737.7830
                    Surrogate loss: -0.0007
             Mean action noise std: 1.01
                       Mean reward: 15263.85
               Mean episode length: 479.10
                 Mean success rate: 96.00
                  Mean reward/step: 31.92
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 0.45s
                        Total time: 915.21s
                               ETA: 66.2s

################################################################################
                     [1m Learning iteration 1866/2000 [0m

                       Computation: 17274 steps/s (collection: 0.272s, learning 0.203s)
               Value function loss: 109939.6936
                    Surrogate loss: -0.0015
             Mean action noise std: 1.01
                       Mean reward: 15280.60
               Mean episode length: 479.10
                 Mean success rate: 96.00
                  Mean reward/step: 31.88
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15294464
                    Iteration time: 0.47s
                        Total time: 915.69s
                               ETA: 65.7s

################################################################################
                     [1m Learning iteration 1867/2000 [0m

                       Computation: 18121 steps/s (collection: 0.245s, learning 0.207s)
               Value function loss: 103549.6617
                    Surrogate loss: -0.0013
             Mean action noise std: 1.01
                       Mean reward: 15341.86
               Mean episode length: 480.51
                 Mean success rate: 96.50
                  Mean reward/step: 31.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 0.45s
                        Total time: 916.14s
                               ETA: 65.2s

################################################################################
                     [1m Learning iteration 1868/2000 [0m

                       Computation: 16590 steps/s (collection: 0.289s, learning 0.205s)
               Value function loss: 141725.4443
                    Surrogate loss: -0.0007
             Mean action noise std: 1.01
                       Mean reward: 15346.78
               Mean episode length: 480.51
                 Mean success rate: 96.50
                  Mean reward/step: 30.77
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15310848
                    Iteration time: 0.49s
                        Total time: 916.63s
                               ETA: 64.7s

################################################################################
                     [1m Learning iteration 1869/2000 [0m

                       Computation: 16127 steps/s (collection: 0.279s, learning 0.229s)
               Value function loss: 58155.0440
                    Surrogate loss: -0.0016
             Mean action noise std: 1.01
                       Mean reward: 15101.08
               Mean episode length: 474.85
                 Mean success rate: 95.00
                  Mean reward/step: 30.89
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 0.51s
                        Total time: 917.14s
                               ETA: 64.2s

################################################################################
                     [1m Learning iteration 1870/2000 [0m

                       Computation: 15545 steps/s (collection: 0.277s, learning 0.250s)
               Value function loss: 98938.5818
                    Surrogate loss: -0.0008
             Mean action noise std: 1.01
                       Mean reward: 15149.87
               Mean episode length: 474.85
                 Mean success rate: 95.00
                  Mean reward/step: 32.31
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15327232
                    Iteration time: 0.53s
                        Total time: 917.67s
                               ETA: 63.8s

################################################################################
                     [1m Learning iteration 1871/2000 [0m

                       Computation: 16158 steps/s (collection: 0.290s, learning 0.217s)
               Value function loss: 105638.0376
                    Surrogate loss: -0.0011
             Mean action noise std: 1.01
                       Mean reward: 15197.07
               Mean episode length: 474.85
                 Mean success rate: 95.00
                  Mean reward/step: 31.39
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.51s
                        Total time: 918.18s
                               ETA: 63.3s

################################################################################
                     [1m Learning iteration 1872/2000 [0m

                       Computation: 17394 steps/s (collection: 0.260s, learning 0.211s)
               Value function loss: 77767.4721
                    Surrogate loss: -0.0010
             Mean action noise std: 1.01
                       Mean reward: 15366.84
               Mean episode length: 479.66
                 Mean success rate: 96.00
                  Mean reward/step: 31.70
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15343616
                    Iteration time: 0.47s
                        Total time: 918.65s
                               ETA: 62.8s

################################################################################
                     [1m Learning iteration 1873/2000 [0m

                       Computation: 16947 steps/s (collection: 0.267s, learning 0.217s)
               Value function loss: 112371.1053
                    Surrogate loss: -0.0012
             Mean action noise std: 1.01
                       Mean reward: 15248.55
               Mean episode length: 476.65
                 Mean success rate: 95.00
                  Mean reward/step: 32.18
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 0.48s
                        Total time: 919.13s
                               ETA: 62.3s

################################################################################
                     [1m Learning iteration 1874/2000 [0m

                       Computation: 16403 steps/s (collection: 0.275s, learning 0.224s)
               Value function loss: 127956.2158
                    Surrogate loss: -0.0009
             Mean action noise std: 1.01
                       Mean reward: 15228.88
               Mean episode length: 477.68
                 Mean success rate: 95.50
                  Mean reward/step: 31.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15360000
                    Iteration time: 0.50s
                        Total time: 919.63s
                               ETA: 61.8s

################################################################################
                     [1m Learning iteration 1875/2000 [0m

                       Computation: 17702 steps/s (collection: 0.252s, learning 0.211s)
               Value function loss: 77142.6697
                    Surrogate loss: -0.0014
             Mean action noise std: 1.01
                       Mean reward: 15152.95
               Mean episode length: 476.50
                 Mean success rate: 95.50
                  Mean reward/step: 32.03
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 0.46s
                        Total time: 920.09s
                               ETA: 61.3s

################################################################################
                     [1m Learning iteration 1876/2000 [0m

                       Computation: 16988 steps/s (collection: 0.264s, learning 0.218s)
               Value function loss: 150700.2871
                    Surrogate loss: -0.0008
             Mean action noise std: 1.01
                       Mean reward: 14973.00
               Mean episode length: 471.68
                 Mean success rate: 94.50
                  Mean reward/step: 31.62
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15376384
                    Iteration time: 0.48s
                        Total time: 920.57s
                               ETA: 60.8s

################################################################################
                     [1m Learning iteration 1877/2000 [0m

                       Computation: 16708 steps/s (collection: 0.279s, learning 0.212s)
               Value function loss: 159470.5165
                    Surrogate loss: -0.0010
             Mean action noise std: 1.01
                       Mean reward: 15115.28
               Mean episode length: 474.25
                 Mean success rate: 95.00
                  Mean reward/step: 30.50
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 0.49s
                        Total time: 921.06s
                               ETA: 60.3s

################################################################################
                     [1m Learning iteration 1878/2000 [0m

                       Computation: 16937 steps/s (collection: 0.274s, learning 0.210s)
               Value function loss: 114623.1297
                    Surrogate loss: -0.0012
             Mean action noise std: 1.01
                       Mean reward: 15084.78
               Mean episode length: 474.25
                 Mean success rate: 95.00
                  Mean reward/step: 30.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15392768
                    Iteration time: 0.48s
                        Total time: 921.55s
                               ETA: 59.8s

################################################################################
                     [1m Learning iteration 1879/2000 [0m

                       Computation: 15685 steps/s (collection: 0.301s, learning 0.222s)
               Value function loss: 145817.9543
                    Surrogate loss: -0.0012
             Mean action noise std: 1.01
                       Mean reward: 15003.04
               Mean episode length: 473.57
                 Mean success rate: 95.00
                  Mean reward/step: 31.11
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 0.52s
                        Total time: 922.07s
                               ETA: 59.3s

################################################################################
                     [1m Learning iteration 1880/2000 [0m

                       Computation: 16377 steps/s (collection: 0.285s, learning 0.216s)
               Value function loss: 95533.6311
                    Surrogate loss: -0.0016
             Mean action noise std: 1.01
                       Mean reward: 15235.14
               Mean episode length: 479.24
                 Mean success rate: 96.50
                  Mean reward/step: 31.19
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15409152
                    Iteration time: 0.50s
                        Total time: 922.57s
                               ETA: 58.9s

################################################################################
                     [1m Learning iteration 1881/2000 [0m

                       Computation: 16914 steps/s (collection: 0.282s, learning 0.202s)
               Value function loss: 84640.4975
                    Surrogate loss: -0.0012
             Mean action noise std: 1.01
                       Mean reward: 15222.68
               Mean episode length: 479.24
                 Mean success rate: 96.50
                  Mean reward/step: 32.44
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 0.48s
                        Total time: 923.06s
                               ETA: 58.4s

################################################################################
                     [1m Learning iteration 1882/2000 [0m

                       Computation: 16790 steps/s (collection: 0.280s, learning 0.208s)
               Value function loss: 81467.2854
                    Surrogate loss: -0.0008
             Mean action noise std: 1.01
                       Mean reward: 15099.72
               Mean episode length: 479.24
                 Mean success rate: 96.50
                  Mean reward/step: 32.03
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15425536
                    Iteration time: 0.49s
                        Total time: 923.54s
                               ETA: 57.9s

################################################################################
                     [1m Learning iteration 1883/2000 [0m

                       Computation: 16265 steps/s (collection: 0.290s, learning 0.213s)
               Value function loss: 161092.0027
                    Surrogate loss: -0.0008
             Mean action noise std: 1.01
                       Mean reward: 14599.25
               Mean episode length: 465.76
                 Mean success rate: 94.00
                  Mean reward/step: 31.97
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.50s
                        Total time: 924.05s
                               ETA: 57.4s

################################################################################
                     [1m Learning iteration 1884/2000 [0m

                       Computation: 16868 steps/s (collection: 0.275s, learning 0.210s)
               Value function loss: 118115.8020
                    Surrogate loss: -0.0012
             Mean action noise std: 1.01
                       Mean reward: 14650.58
               Mean episode length: 467.00
                 Mean success rate: 94.50
                  Mean reward/step: 30.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15441920
                    Iteration time: 0.49s
                        Total time: 924.53s
                               ETA: 56.9s

################################################################################
                     [1m Learning iteration 1885/2000 [0m

                       Computation: 16126 steps/s (collection: 0.285s, learning 0.223s)
               Value function loss: 56410.1817
                    Surrogate loss: -0.0008
             Mean action noise std: 1.01
                       Mean reward: 14450.79
               Mean episode length: 459.25
                 Mean success rate: 93.00
                  Mean reward/step: 31.63
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 0.51s
                        Total time: 925.04s
                               ETA: 56.4s

################################################################################
                     [1m Learning iteration 1886/2000 [0m

                       Computation: 15708 steps/s (collection: 0.297s, learning 0.225s)
               Value function loss: 147294.5066
                    Surrogate loss: -0.0005
             Mean action noise std: 1.01
                       Mean reward: 14660.55
               Mean episode length: 465.42
                 Mean success rate: 94.00
                  Mean reward/step: 32.06
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15458304
                    Iteration time: 0.52s
                        Total time: 925.56s
                               ETA: 55.9s

################################################################################
                     [1m Learning iteration 1887/2000 [0m

                       Computation: 16015 steps/s (collection: 0.283s, learning 0.229s)
               Value function loss: 111426.7904
                    Surrogate loss: -0.0012
             Mean action noise std: 1.01
                       Mean reward: 14519.22
               Mean episode length: 460.76
                 Mean success rate: 93.00
                  Mean reward/step: 31.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 0.51s
                        Total time: 926.07s
                               ETA: 55.4s

################################################################################
                     [1m Learning iteration 1888/2000 [0m

                       Computation: 16592 steps/s (collection: 0.276s, learning 0.218s)
               Value function loss: 61671.4074
                    Surrogate loss: -0.0004
             Mean action noise std: 1.01
                       Mean reward: 14489.33
               Mean episode length: 460.76
                 Mean success rate: 93.00
                  Mean reward/step: 31.86
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 15474688
                    Iteration time: 0.49s
                        Total time: 926.57s
                               ETA: 54.9s

################################################################################
                     [1m Learning iteration 1889/2000 [0m

                       Computation: 16247 steps/s (collection: 0.271s, learning 0.233s)
               Value function loss: 135507.1625
                    Surrogate loss: -0.0013
             Mean action noise std: 1.01
                       Mean reward: 14483.83
               Mean episode length: 460.76
                 Mean success rate: 93.00
                  Mean reward/step: 31.93
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 0.50s
                        Total time: 927.07s
                               ETA: 54.4s

################################################################################
                     [1m Learning iteration 1890/2000 [0m

                       Computation: 17724 steps/s (collection: 0.258s, learning 0.204s)
               Value function loss: 67713.2754
                    Surrogate loss: -0.0006
             Mean action noise std: 1.01
                       Mean reward: 14458.95
               Mean episode length: 460.76
                 Mean success rate: 93.00
                  Mean reward/step: 31.36
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15491072
                    Iteration time: 0.46s
                        Total time: 927.53s
                               ETA: 54.0s

################################################################################
                     [1m Learning iteration 1891/2000 [0m

                       Computation: 16841 steps/s (collection: 0.270s, learning 0.216s)
               Value function loss: 105331.9488
                    Surrogate loss: -0.0010
             Mean action noise std: 1.01
                       Mean reward: 14445.18
               Mean episode length: 460.53
                 Mean success rate: 92.50
                  Mean reward/step: 32.11
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 0.49s
                        Total time: 928.02s
                               ETA: 53.5s

################################################################################
                     [1m Learning iteration 1892/2000 [0m

                       Computation: 16652 steps/s (collection: 0.272s, learning 0.220s)
               Value function loss: 160835.7609
                    Surrogate loss: -0.0006
             Mean action noise std: 1.01
                       Mean reward: 14440.41
               Mean episode length: 460.53
                 Mean success rate: 92.50
                  Mean reward/step: 31.04
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15507456
                    Iteration time: 0.49s
                        Total time: 928.51s
                               ETA: 53.0s

################################################################################
                     [1m Learning iteration 1893/2000 [0m

                       Computation: 17062 steps/s (collection: 0.267s, learning 0.213s)
               Value function loss: 121590.9271
                    Surrogate loss: -0.0012
             Mean action noise std: 1.01
                       Mean reward: 14516.72
               Mean episode length: 460.53
                 Mean success rate: 92.50
                  Mean reward/step: 30.07
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 0.48s
                        Total time: 928.99s
                               ETA: 52.5s

################################################################################
                     [1m Learning iteration 1894/2000 [0m

                       Computation: 16610 steps/s (collection: 0.285s, learning 0.208s)
               Value function loss: 123699.1898
                    Surrogate loss: -0.0007
             Mean action noise std: 1.01
                       Mean reward: 14674.07
               Mean episode length: 465.30
                 Mean success rate: 93.50
                  Mean reward/step: 30.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15523840
                    Iteration time: 0.49s
                        Total time: 929.49s
                               ETA: 52.0s

################################################################################
                     [1m Learning iteration 1895/2000 [0m

                       Computation: 17030 steps/s (collection: 0.262s, learning 0.219s)
               Value function loss: 117892.7873
                    Surrogate loss: -0.0007
             Mean action noise std: 1.01
                       Mean reward: 14437.98
               Mean episode length: 459.78
                 Mean success rate: 92.00
                  Mean reward/step: 30.70
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.48s
                        Total time: 929.97s
                               ETA: 51.5s

################################################################################
                     [1m Learning iteration 1896/2000 [0m

                       Computation: 16681 steps/s (collection: 0.265s, learning 0.226s)
               Value function loss: 64882.7192
                    Surrogate loss: -0.0004
             Mean action noise std: 1.01
                       Mean reward: 14445.41
               Mean episode length: 459.85
                 Mean success rate: 92.00
                  Mean reward/step: 31.43
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15540224
                    Iteration time: 0.49s
                        Total time: 930.46s
                               ETA: 51.0s

################################################################################
                     [1m Learning iteration 1897/2000 [0m

                       Computation: 17209 steps/s (collection: 0.258s, learning 0.218s)
               Value function loss: 125592.8910
                    Surrogate loss: -0.0008
             Mean action noise std: 1.01
                       Mean reward: 14237.14
               Mean episode length: 455.09
                 Mean success rate: 91.00
                  Mean reward/step: 32.41
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 0.48s
                        Total time: 930.93s
                               ETA: 50.5s

################################################################################
                     [1m Learning iteration 1898/2000 [0m

                       Computation: 16270 steps/s (collection: 0.273s, learning 0.230s)
               Value function loss: 73844.7524
                    Surrogate loss: -0.0009
             Mean action noise std: 1.01
                       Mean reward: 14390.13
               Mean episode length: 459.58
                 Mean success rate: 92.00
                  Mean reward/step: 31.77
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 15556608
                    Iteration time: 0.50s
                        Total time: 931.44s
                               ETA: 50.0s

################################################################################
                     [1m Learning iteration 1899/2000 [0m

                       Computation: 17325 steps/s (collection: 0.261s, learning 0.212s)
               Value function loss: 157017.9543
                    Surrogate loss: -0.0007
             Mean action noise std: 1.01
                       Mean reward: 14525.19
               Mean episode length: 464.47
                 Mean success rate: 93.00
                  Mean reward/step: 31.94
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 0.47s
                        Total time: 931.91s
                               ETA: 49.5s

################################################################################
                     [1m Learning iteration 1900/2000 [0m

                       Computation: 16734 steps/s (collection: 0.267s, learning 0.222s)
               Value function loss: 80629.7614
                    Surrogate loss: -0.0006
             Mean action noise std: 1.01
                       Mean reward: 14354.69
               Mean episode length: 459.75
                 Mean success rate: 92.00
                  Mean reward/step: 31.30
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15572992
                    Iteration time: 0.49s
                        Total time: 932.40s
                               ETA: 49.0s

################################################################################
                     [1m Learning iteration 1901/2000 [0m

                       Computation: 16545 steps/s (collection: 0.267s, learning 0.228s)
               Value function loss: 66444.8447
                    Surrogate loss: -0.0006
             Mean action noise std: 1.01
                       Mean reward: 14393.08
               Mean episode length: 459.75
                 Mean success rate: 92.00
                  Mean reward/step: 32.35
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 0.50s
                        Total time: 932.89s
                               ETA: 48.6s

################################################################################
                     [1m Learning iteration 1902/2000 [0m

                       Computation: 16209 steps/s (collection: 0.279s, learning 0.226s)
               Value function loss: 124557.6239
                    Surrogate loss: -0.0006
             Mean action noise std: 1.01
                       Mean reward: 14587.94
               Mean episode length: 463.98
                 Mean success rate: 93.00
                  Mean reward/step: 31.68
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15589376
                    Iteration time: 0.51s
                        Total time: 933.40s
                               ETA: 48.1s

################################################################################
                     [1m Learning iteration 1903/2000 [0m

                       Computation: 17326 steps/s (collection: 0.256s, learning 0.217s)
               Value function loss: 93056.2879
                    Surrogate loss: -0.0017
             Mean action noise std: 1.01
                       Mean reward: 14477.05
               Mean episode length: 461.20
                 Mean success rate: 92.50
                  Mean reward/step: 31.27
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 0.47s
                        Total time: 933.87s
                               ETA: 47.6s

################################################################################
                     [1m Learning iteration 1904/2000 [0m

                       Computation: 16794 steps/s (collection: 0.274s, learning 0.213s)
               Value function loss: 80823.8975
                    Surrogate loss: -0.0016
             Mean action noise std: 1.01
                       Mean reward: 14172.00
               Mean episode length: 451.42
                 Mean success rate: 90.50
                  Mean reward/step: 32.19
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15605760
                    Iteration time: 0.49s
                        Total time: 934.36s
                               ETA: 47.1s

################################################################################
                     [1m Learning iteration 1905/2000 [0m

                       Computation: 17237 steps/s (collection: 0.260s, learning 0.215s)
               Value function loss: 105646.1847
                    Surrogate loss: -0.0009
             Mean action noise std: 1.01
                       Mean reward: 14329.63
               Mean episode length: 456.01
                 Mean success rate: 91.50
                  Mean reward/step: 32.02
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 0.48s
                        Total time: 934.84s
                               ETA: 46.6s

################################################################################
                     [1m Learning iteration 1906/2000 [0m

                       Computation: 15571 steps/s (collection: 0.292s, learning 0.234s)
               Value function loss: 97250.5451
                    Surrogate loss: -0.0005
             Mean action noise std: 1.01
                       Mean reward: 14306.76
               Mean episode length: 456.01
                 Mean success rate: 91.50
                  Mean reward/step: 32.14
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15622144
                    Iteration time: 0.53s
                        Total time: 935.36s
                               ETA: 46.1s

################################################################################
                     [1m Learning iteration 1907/2000 [0m

                       Computation: 16744 steps/s (collection: 0.277s, learning 0.212s)
               Value function loss: 141988.2805
                    Surrogate loss: -0.0003
             Mean action noise std: 1.01
                       Mean reward: 14665.77
               Mean episode length: 464.70
                 Mean success rate: 93.50
                  Mean reward/step: 32.12
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.49s
                        Total time: 935.85s
                               ETA: 45.6s

################################################################################
                     [1m Learning iteration 1908/2000 [0m

                       Computation: 16933 steps/s (collection: 0.272s, learning 0.212s)
               Value function loss: 147793.4163
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15131.32
               Mean episode length: 477.96
                 Mean success rate: 95.50
                  Mean reward/step: 30.82
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15638528
                    Iteration time: 0.48s
                        Total time: 936.33s
                               ETA: 45.1s

################################################################################
                     [1m Learning iteration 1909/2000 [0m

                       Computation: 17032 steps/s (collection: 0.270s, learning 0.211s)
               Value function loss: 107491.9391
                    Surrogate loss: -0.0009
             Mean action noise std: 1.01
                       Mean reward: 15208.27
               Mean episode length: 478.48
                 Mean success rate: 95.50
                  Mean reward/step: 30.83
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 0.48s
                        Total time: 936.82s
                               ETA: 44.6s

################################################################################
                     [1m Learning iteration 1910/2000 [0m

                       Computation: 16802 steps/s (collection: 0.278s, learning 0.210s)
               Value function loss: 149117.0393
                    Surrogate loss: -0.0002
             Mean action noise std: 1.01
                       Mean reward: 15191.99
               Mean episode length: 478.48
                 Mean success rate: 95.50
                  Mean reward/step: 31.00
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15654912
                    Iteration time: 0.49s
                        Total time: 937.30s
                               ETA: 44.1s

################################################################################
                     [1m Learning iteration 1911/2000 [0m

                       Computation: 18059 steps/s (collection: 0.245s, learning 0.209s)
               Value function loss: 100915.4847
                    Surrogate loss: -0.0008
             Mean action noise std: 1.01
                       Mean reward: 15214.65
               Mean episode length: 478.48
                 Mean success rate: 95.50
                  Mean reward/step: 30.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 0.45s
                        Total time: 937.76s
                               ETA: 43.7s

################################################################################
                     [1m Learning iteration 1912/2000 [0m

                       Computation: 17633 steps/s (collection: 0.250s, learning 0.215s)
               Value function loss: 68186.6071
                    Surrogate loss: -0.0007
             Mean action noise std: 1.01
                       Mean reward: 15205.94
               Mean episode length: 478.96
                 Mean success rate: 95.50
                  Mean reward/step: 31.88
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15671296
                    Iteration time: 0.46s
                        Total time: 938.22s
                               ETA: 43.2s

################################################################################
                     [1m Learning iteration 1913/2000 [0m

                       Computation: 16932 steps/s (collection: 0.267s, learning 0.216s)
               Value function loss: 100551.0413
                    Surrogate loss: -0.0020
             Mean action noise std: 1.01
                       Mean reward: 15194.76
               Mean episode length: 478.96
                 Mean success rate: 95.50
                  Mean reward/step: 31.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 0.48s
                        Total time: 938.70s
                               ETA: 42.7s

################################################################################
                     [1m Learning iteration 1914/2000 [0m

                       Computation: 17605 steps/s (collection: 0.248s, learning 0.218s)
               Value function loss: 94835.4717
                    Surrogate loss: -0.0010
             Mean action noise std: 1.01
                       Mean reward: 15162.83
               Mean episode length: 478.96
                 Mean success rate: 95.50
                  Mean reward/step: 31.71
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15687680
                    Iteration time: 0.47s
                        Total time: 939.17s
                               ETA: 42.2s

################################################################################
                     [1m Learning iteration 1915/2000 [0m

                       Computation: 16617 steps/s (collection: 0.281s, learning 0.212s)
               Value function loss: 125372.5746
                    Surrogate loss: -0.0009
             Mean action noise std: 1.01
                       Mean reward: 15461.89
               Mean episode length: 487.38
                 Mean success rate: 97.50
                  Mean reward/step: 30.79
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 0.49s
                        Total time: 939.66s
                               ETA: 41.7s

################################################################################
                     [1m Learning iteration 1916/2000 [0m

                       Computation: 17176 steps/s (collection: 0.264s, learning 0.213s)
               Value function loss: 81379.4953
                    Surrogate loss: -0.0006
             Mean action noise std: 1.01
                       Mean reward: 15406.73
               Mean episode length: 487.38
                 Mean success rate: 97.50
                  Mean reward/step: 30.89
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15704064
                    Iteration time: 0.48s
                        Total time: 940.14s
                               ETA: 41.2s

################################################################################
                     [1m Learning iteration 1917/2000 [0m

                       Computation: 16695 steps/s (collection: 0.267s, learning 0.223s)
               Value function loss: 90922.0000
                    Surrogate loss: -0.0008
             Mean action noise std: 1.01
                       Mean reward: 15405.26
               Mean episode length: 487.38
                 Mean success rate: 97.50
                  Mean reward/step: 31.67
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 0.49s
                        Total time: 940.63s
                               ETA: 40.7s

################################################################################
                     [1m Learning iteration 1918/2000 [0m

                       Computation: 17438 steps/s (collection: 0.257s, learning 0.213s)
               Value function loss: 99813.9696
                    Surrogate loss: -0.0008
             Mean action noise std: 1.01
                       Mean reward: 15264.51
               Mean episode length: 483.60
                 Mean success rate: 97.00
                  Mean reward/step: 31.48
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15720448
                    Iteration time: 0.47s
                        Total time: 941.10s
                               ETA: 40.2s

################################################################################
                     [1m Learning iteration 1919/2000 [0m

                       Computation: 16569 steps/s (collection: 0.266s, learning 0.228s)
               Value function loss: 91370.1686
                    Surrogate loss: -0.0013
             Mean action noise std: 1.01
                       Mean reward: 15224.22
               Mean episode length: 483.60
                 Mean success rate: 97.00
                  Mean reward/step: 31.47
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.49s
                        Total time: 941.59s
                               ETA: 39.7s

################################################################################
                     [1m Learning iteration 1920/2000 [0m

                       Computation: 16864 steps/s (collection: 0.262s, learning 0.224s)
               Value function loss: 79112.4969
                    Surrogate loss: -0.0008
             Mean action noise std: 1.01
                       Mean reward: 15225.34
               Mean episode length: 483.60
                 Mean success rate: 97.00
                  Mean reward/step: 32.10
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15736832
                    Iteration time: 0.49s
                        Total time: 942.08s
                               ETA: 39.2s

################################################################################
                     [1m Learning iteration 1921/2000 [0m

                       Computation: 17283 steps/s (collection: 0.264s, learning 0.210s)
               Value function loss: 91924.0521
                    Surrogate loss: -0.0011
             Mean action noise std: 1.01
                       Mean reward: 15217.69
               Mean episode length: 483.60
                 Mean success rate: 97.00
                  Mean reward/step: 31.77
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 0.47s
                        Total time: 942.55s
                               ETA: 38.7s

################################################################################
                     [1m Learning iteration 1922/2000 [0m

                       Computation: 15497 steps/s (collection: 0.292s, learning 0.237s)
               Value function loss: 87545.2489
                    Surrogate loss: -0.0003
             Mean action noise std: 1.01
                       Mean reward: 15228.69
               Mean episode length: 484.05
                 Mean success rate: 97.50
                  Mean reward/step: 32.03
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15753216
                    Iteration time: 0.53s
                        Total time: 943.08s
                               ETA: 38.3s

################################################################################
                     [1m Learning iteration 1923/2000 [0m

                       Computation: 17673 steps/s (collection: 0.256s, learning 0.207s)
               Value function loss: 156318.8553
                    Surrogate loss: -0.0004
             Mean action noise std: 1.01
                       Mean reward: 15161.28
               Mean episode length: 481.66
                 Mean success rate: 97.00
                  Mean reward/step: 31.34
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 0.46s
                        Total time: 943.55s
                               ETA: 37.8s

################################################################################
                     [1m Learning iteration 1924/2000 [0m

                       Computation: 17013 steps/s (collection: 0.282s, learning 0.200s)
               Value function loss: 138612.0369
                    Surrogate loss: -0.0005
             Mean action noise std: 1.01
                       Mean reward: 15244.86
               Mean episode length: 485.90
                 Mean success rate: 98.00
                  Mean reward/step: 30.33
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15769600
                    Iteration time: 0.48s
                        Total time: 944.03s
                               ETA: 37.3s

################################################################################
                     [1m Learning iteration 1925/2000 [0m

                       Computation: 16938 steps/s (collection: 0.254s, learning 0.230s)
               Value function loss: 123451.4154
                    Surrogate loss: -0.0007
             Mean action noise std: 1.01
                       Mean reward: 15242.89
               Mean episode length: 485.90
                 Mean success rate: 98.00
                  Mean reward/step: 30.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 0.48s
                        Total time: 944.51s
                               ETA: 36.8s

################################################################################
                     [1m Learning iteration 1926/2000 [0m

                       Computation: 16890 steps/s (collection: 0.279s, learning 0.206s)
               Value function loss: 150553.0240
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 15306.82
               Mean episode length: 487.55
                 Mean success rate: 98.00
                  Mean reward/step: 30.54
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15785984
                    Iteration time: 0.48s
                        Total time: 945.00s
                               ETA: 36.3s

################################################################################
                     [1m Learning iteration 1927/2000 [0m

                       Computation: 17646 steps/s (collection: 0.255s, learning 0.209s)
               Value function loss: 90789.4802
                    Surrogate loss: -0.0009
             Mean action noise std: 1.01
                       Mean reward: 15284.67
               Mean episode length: 486.31
                 Mean success rate: 98.00
                  Mean reward/step: 30.77
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 0.46s
                        Total time: 945.46s
                               ETA: 35.8s

################################################################################
                     [1m Learning iteration 1928/2000 [0m

                       Computation: 17206 steps/s (collection: 0.274s, learning 0.202s)
               Value function loss: 102357.9376
                    Surrogate loss: -0.0005
             Mean action noise std: 1.01
                       Mean reward: 14985.10
               Mean episode length: 477.18
                 Mean success rate: 96.00
                  Mean reward/step: 31.98
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15802368
                    Iteration time: 0.48s
                        Total time: 945.94s
                               ETA: 35.3s

################################################################################
                     [1m Learning iteration 1929/2000 [0m

                       Computation: 17894 steps/s (collection: 0.257s, learning 0.201s)
               Value function loss: 78348.9205
                    Surrogate loss: -0.0004
             Mean action noise std: 1.01
                       Mean reward: 14985.67
               Mean episode length: 477.18
                 Mean success rate: 96.00
                  Mean reward/step: 31.68
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 0.46s
                        Total time: 946.39s
                               ETA: 34.8s

################################################################################
                     [1m Learning iteration 1930/2000 [0m

                       Computation: 16837 steps/s (collection: 0.251s, learning 0.235s)
               Value function loss: 152184.1146
                    Surrogate loss: -0.0003
             Mean action noise std: 1.01
                       Mean reward: 15066.41
               Mean episode length: 478.56
                 Mean success rate: 96.00
                  Mean reward/step: 32.31
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15818752
                    Iteration time: 0.49s
                        Total time: 946.88s
                               ETA: 34.3s

################################################################################
                     [1m Learning iteration 1931/2000 [0m

                       Computation: 16865 steps/s (collection: 0.264s, learning 0.221s)
               Value function loss: 109282.1305
                    Surrogate loss: -0.0009
             Mean action noise std: 1.01
                       Mean reward: 15052.16
               Mean episode length: 478.56
                 Mean success rate: 96.00
                  Mean reward/step: 31.28
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.49s
                        Total time: 947.37s
                               ETA: 33.8s

################################################################################
                     [1m Learning iteration 1932/2000 [0m

                       Computation: 16745 steps/s (collection: 0.267s, learning 0.222s)
               Value function loss: 71842.9913
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 15039.02
               Mean episode length: 478.56
                 Mean success rate: 96.00
                  Mean reward/step: 31.58
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 15835136
                    Iteration time: 0.49s
                        Total time: 947.86s
                               ETA: 33.3s

################################################################################
                     [1m Learning iteration 1933/2000 [0m

                       Computation: 16149 steps/s (collection: 0.293s, learning 0.215s)
               Value function loss: 144913.0014
                    Surrogate loss: 0.0000
             Mean action noise std: 1.01
                       Mean reward: 15188.16
               Mean episode length: 482.35
                 Mean success rate: 96.50
                  Mean reward/step: 31.86
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 0.51s
                        Total time: 948.36s
                               ETA: 32.9s

################################################################################
                     [1m Learning iteration 1934/2000 [0m

                       Computation: 17819 steps/s (collection: 0.258s, learning 0.202s)
               Value function loss: 102828.4123
                    Surrogate loss: -0.0009
             Mean action noise std: 1.01
                       Mean reward: 14907.06
               Mean episode length: 475.04
                 Mean success rate: 95.00
                  Mean reward/step: 31.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15851520
                    Iteration time: 0.46s
                        Total time: 948.82s
                               ETA: 32.4s

################################################################################
                     [1m Learning iteration 1935/2000 [0m

                       Computation: 15945 steps/s (collection: 0.277s, learning 0.237s)
               Value function loss: 90116.3373
                    Surrogate loss: -0.0004
             Mean action noise std: 1.01
                       Mean reward: 14870.19
               Mean episode length: 472.75
                 Mean success rate: 94.50
                  Mean reward/step: 31.72
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 0.51s
                        Total time: 949.34s
                               ETA: 31.9s

################################################################################
                     [1m Learning iteration 1936/2000 [0m

                       Computation: 14978 steps/s (collection: 0.287s, learning 0.259s)
               Value function loss: 94177.3705
                    Surrogate loss: -0.0003
             Mean action noise std: 1.01
                       Mean reward: 14890.18
               Mean episode length: 472.75
                 Mean success rate: 94.50
                  Mean reward/step: 31.65
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15867904
                    Iteration time: 0.55s
                        Total time: 949.88s
                               ETA: 31.4s

################################################################################
                     [1m Learning iteration 1937/2000 [0m

                       Computation: 16165 steps/s (collection: 0.285s, learning 0.221s)
               Value function loss: 73703.8809
                    Surrogate loss: -0.0003
             Mean action noise std: 1.01
                       Mean reward: 14712.28
               Mean episode length: 468.35
                 Mean success rate: 93.50
                  Mean reward/step: 31.83
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 0.51s
                        Total time: 950.39s
                               ETA: 30.9s

################################################################################
                     [1m Learning iteration 1938/2000 [0m

                       Computation: 17670 steps/s (collection: 0.263s, learning 0.201s)
               Value function loss: 122014.0824
                    Surrogate loss: -0.0002
             Mean action noise std: 1.01
                       Mean reward: 14583.45
               Mean episode length: 463.71
                 Mean success rate: 92.50
                  Mean reward/step: 31.78
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15884288
                    Iteration time: 0.46s
                        Total time: 950.85s
                               ETA: 30.4s

################################################################################
                     [1m Learning iteration 1939/2000 [0m

                       Computation: 17420 steps/s (collection: 0.269s, learning 0.201s)
               Value function loss: 160917.5320
                    Surrogate loss: -0.0002
             Mean action noise std: 1.01
                       Mean reward: 14661.58
               Mean episode length: 467.32
                 Mean success rate: 93.00
                  Mean reward/step: 30.72
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 0.47s
                        Total time: 951.32s
                               ETA: 29.9s

################################################################################
                     [1m Learning iteration 1940/2000 [0m

                       Computation: 17402 steps/s (collection: 0.253s, learning 0.218s)
               Value function loss: 96199.9438
                    Surrogate loss: -0.0004
             Mean action noise std: 1.01
                       Mean reward: 14798.89
               Mean episode length: 472.02
                 Mean success rate: 94.00
                  Mean reward/step: 30.32
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15900672
                    Iteration time: 0.47s
                        Total time: 951.80s
                               ETA: 29.4s

################################################################################
                     [1m Learning iteration 1941/2000 [0m

                       Computation: 17471 steps/s (collection: 0.260s, learning 0.209s)
               Value function loss: 120375.1356
                    Surrogate loss: -0.0002
             Mean action noise std: 1.01
                       Mean reward: 14771.08
               Mean episode length: 471.27
                 Mean success rate: 94.00
                  Mean reward/step: 30.75
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 0.47s
                        Total time: 952.26s
                               ETA: 28.9s

################################################################################
                     [1m Learning iteration 1942/2000 [0m

                       Computation: 17561 steps/s (collection: 0.254s, learning 0.212s)
               Value function loss: 109828.8156
                    Surrogate loss: -0.0004
             Mean action noise std: 1.01
                       Mean reward: 14781.59
               Mean episode length: 471.27
                 Mean success rate: 94.00
                  Mean reward/step: 30.44
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15917056
                    Iteration time: 0.47s
                        Total time: 952.73s
                               ETA: 28.4s

################################################################################
                     [1m Learning iteration 1943/2000 [0m

                       Computation: 18091 steps/s (collection: 0.254s, learning 0.199s)
               Value function loss: 72436.1398
                    Surrogate loss: -0.0002
             Mean action noise std: 1.01
                       Mean reward: 14775.63
               Mean episode length: 471.27
                 Mean success rate: 94.00
                  Mean reward/step: 31.52
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.45s
                        Total time: 953.18s
                               ETA: 27.9s

################################################################################
                     [1m Learning iteration 1944/2000 [0m

                       Computation: 17731 steps/s (collection: 0.252s, learning 0.210s)
               Value function loss: 131747.4959
                    Surrogate loss: -0.0002
             Mean action noise std: 1.01
                       Mean reward: 14809.03
               Mean episode length: 471.27
                 Mean success rate: 94.00
                  Mean reward/step: 31.55
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15933440
                    Iteration time: 0.46s
                        Total time: 953.65s
                               ETA: 27.5s

################################################################################
                     [1m Learning iteration 1945/2000 [0m

                       Computation: 18435 steps/s (collection: 0.238s, learning 0.206s)
               Value function loss: 57192.9042
                    Surrogate loss: -0.0002
             Mean action noise std: 1.01
                       Mean reward: 14804.58
               Mean episode length: 471.27
                 Mean success rate: 94.00
                  Mean reward/step: 31.42
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 0.44s
                        Total time: 954.09s
                               ETA: 27.0s

################################################################################
                     [1m Learning iteration 1946/2000 [0m

                       Computation: 17390 steps/s (collection: 0.259s, learning 0.212s)
               Value function loss: 149901.1475
                    Surrogate loss: -0.0004
             Mean action noise std: 1.01
                       Mean reward: 15082.92
               Mean episode length: 480.30
                 Mean success rate: 96.00
                  Mean reward/step: 31.21
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15949824
                    Iteration time: 0.47s
                        Total time: 954.56s
                               ETA: 26.5s

################################################################################
                     [1m Learning iteration 1947/2000 [0m

                       Computation: 18980 steps/s (collection: 0.235s, learning 0.197s)
               Value function loss: 75108.8571
                    Surrogate loss: -0.0002
             Mean action noise std: 1.01
                       Mean reward: 15097.23
               Mean episode length: 480.30
                 Mean success rate: 96.00
                  Mean reward/step: 31.06
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 0.43s
                        Total time: 954.99s
                               ETA: 26.0s

################################################################################
                     [1m Learning iteration 1948/2000 [0m

                       Computation: 17246 steps/s (collection: 0.266s, learning 0.209s)
               Value function loss: 70440.6032
                    Surrogate loss: -0.0004
             Mean action noise std: 1.01
                       Mean reward: 15095.89
               Mean episode length: 480.30
                 Mean success rate: 96.00
                  Mean reward/step: 32.17
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15966208
                    Iteration time: 0.47s
                        Total time: 955.47s
                               ETA: 25.5s

################################################################################
                     [1m Learning iteration 1949/2000 [0m

                       Computation: 18158 steps/s (collection: 0.244s, learning 0.207s)
               Value function loss: 99188.1516
                    Surrogate loss: -0.0003
             Mean action noise std: 1.01
                       Mean reward: 15198.29
               Mean episode length: 484.70
                 Mean success rate: 97.00
                  Mean reward/step: 32.06
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 0.45s
                        Total time: 955.92s
                               ETA: 25.0s

################################################################################
                     [1m Learning iteration 1950/2000 [0m

                       Computation: 18456 steps/s (collection: 0.239s, learning 0.205s)
               Value function loss: 86714.8843
                    Surrogate loss: -0.0004
             Mean action noise std: 1.01
                       Mean reward: 15312.72
               Mean episode length: 489.33
                 Mean success rate: 98.00
                  Mean reward/step: 31.62
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15982592
                    Iteration time: 0.44s
                        Total time: 956.36s
                               ETA: 24.5s

################################################################################
                     [1m Learning iteration 1951/2000 [0m

                       Computation: 18112 steps/s (collection: 0.254s, learning 0.198s)
               Value function loss: 93194.1967
                    Surrogate loss: -0.0003
             Mean action noise std: 1.01
                       Mean reward: 15325.88
               Mean episode length: 489.33
                 Mean success rate: 98.00
                  Mean reward/step: 31.82
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 0.45s
                        Total time: 956.81s
                               ETA: 24.0s

################################################################################
                     [1m Learning iteration 1952/2000 [0m

                       Computation: 17758 steps/s (collection: 0.257s, learning 0.204s)
               Value function loss: 82878.1048
                    Surrogate loss: -0.0003
             Mean action noise std: 1.01
                       Mean reward: 15496.37
               Mean episode length: 493.89
                 Mean success rate: 99.00
                  Mean reward/step: 31.96
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15998976
                    Iteration time: 0.46s
                        Total time: 957.28s
                               ETA: 23.5s

################################################################################
                     [1m Learning iteration 1953/2000 [0m

                       Computation: 17459 steps/s (collection: 0.270s, learning 0.199s)
               Value function loss: 96316.3173
                    Surrogate loss: -0.0006
             Mean action noise std: 1.01
                       Mean reward: 15522.00
               Mean episode length: 493.89
                 Mean success rate: 99.00
                  Mean reward/step: 32.21
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 0.47s
                        Total time: 957.75s
                               ETA: 23.0s

################################################################################
                     [1m Learning iteration 1954/2000 [0m

                       Computation: 18720 steps/s (collection: 0.244s, learning 0.193s)
               Value function loss: 132683.9871
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 15601.05
               Mean episode length: 497.04
                 Mean success rate: 99.50
                  Mean reward/step: 32.04
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16015360
                    Iteration time: 0.44s
                        Total time: 958.18s
                               ETA: 22.5s

################################################################################
                     [1m Learning iteration 1955/2000 [0m

                       Computation: 17878 steps/s (collection: 0.264s, learning 0.194s)
               Value function loss: 152995.9323
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 15578.57
               Mean episode length: 497.04
                 Mean success rate: 99.50
                  Mean reward/step: 30.83
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.46s
                        Total time: 958.64s
                               ETA: 22.1s

################################################################################
                     [1m Learning iteration 1956/2000 [0m

                       Computation: 17085 steps/s (collection: 0.273s, learning 0.207s)
               Value function loss: 118898.4357
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 15563.44
               Mean episode length: 497.04
                 Mean success rate: 99.50
                  Mean reward/step: 31.05
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16031744
                    Iteration time: 0.48s
                        Total time: 959.12s
                               ETA: 21.6s

################################################################################
                     [1m Learning iteration 1957/2000 [0m

                       Computation: 16016 steps/s (collection: 0.292s, learning 0.219s)
               Value function loss: 152485.5641
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 15422.87
               Mean episode length: 492.81
                 Mean success rate: 99.00
                  Mean reward/step: 31.16
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 0.51s
                        Total time: 959.63s
                               ETA: 21.1s

################################################################################
                     [1m Learning iteration 1958/2000 [0m

                       Computation: 17111 steps/s (collection: 0.278s, learning 0.200s)
               Value function loss: 103285.5399
                    Surrogate loss: -0.0009
             Mean action noise std: 1.01
                       Mean reward: 15555.29
               Mean episode length: 495.77
                 Mean success rate: 99.50
                  Mean reward/step: 31.13
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16048128
                    Iteration time: 0.48s
                        Total time: 960.11s
                               ETA: 20.6s

################################################################################
                     [1m Learning iteration 1959/2000 [0m

                       Computation: 18026 steps/s (collection: 0.253s, learning 0.201s)
               Value function loss: 100298.9418
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 15534.61
               Mean episode length: 495.77
                 Mean success rate: 99.50
                  Mean reward/step: 31.91
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 0.45s
                        Total time: 960.57s
                               ETA: 20.1s

################################################################################
                     [1m Learning iteration 1960/2000 [0m

                       Computation: 17766 steps/s (collection: 0.254s, learning 0.207s)
               Value function loss: 84888.0580
                    Surrogate loss: -0.0002
             Mean action noise std: 1.01
                       Mean reward: 15411.20
               Mean episode length: 491.71
                 Mean success rate: 99.00
                  Mean reward/step: 31.57
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16064512
                    Iteration time: 0.46s
                        Total time: 961.03s
                               ETA: 19.6s

################################################################################
                     [1m Learning iteration 1961/2000 [0m

                       Computation: 17399 steps/s (collection: 0.256s, learning 0.215s)
               Value function loss: 104268.5701
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 15466.92
               Mean episode length: 491.71
                 Mean success rate: 99.00
                  Mean reward/step: 32.16
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 0.47s
                        Total time: 961.50s
                               ETA: 19.1s

################################################################################
                     [1m Learning iteration 1962/2000 [0m

                       Computation: 16366 steps/s (collection: 0.277s, learning 0.224s)
               Value function loss: 121112.6113
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 15506.07
               Mean episode length: 491.71
                 Mean success rate: 99.00
                  Mean reward/step: 31.30
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16080896
                    Iteration time: 0.50s
                        Total time: 962.00s
                               ETA: 18.6s

################################################################################
                     [1m Learning iteration 1963/2000 [0m

                       Computation: 17255 steps/s (collection: 0.266s, learning 0.209s)
               Value function loss: 81784.7161
                    Surrogate loss: -0.0002
             Mean action noise std: 1.01
                       Mean reward: 15362.38
               Mean episode length: 487.44
                 Mean success rate: 98.00
                  Mean reward/step: 31.27
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 0.47s
                        Total time: 962.47s
                               ETA: 18.1s

################################################################################
                     [1m Learning iteration 1964/2000 [0m

                       Computation: 17764 steps/s (collection: 0.250s, learning 0.211s)
               Value function loss: 116625.2598
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15334.78
               Mean episode length: 487.44
                 Mean success rate: 98.00
                  Mean reward/step: 32.16
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16097280
                    Iteration time: 0.46s
                        Total time: 962.93s
                               ETA: 17.6s

################################################################################
                     [1m Learning iteration 1965/2000 [0m

                       Computation: 16791 steps/s (collection: 0.267s, learning 0.221s)
               Value function loss: 98507.2342
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 15356.55
               Mean episode length: 487.44
                 Mean success rate: 98.00
                  Mean reward/step: 31.59
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 0.49s
                        Total time: 963.42s
                               ETA: 17.2s

################################################################################
                     [1m Learning iteration 1966/2000 [0m

                       Computation: 16746 steps/s (collection: 0.273s, learning 0.216s)
               Value function loss: 94335.3436
                    Surrogate loss: -0.0002
             Mean action noise std: 1.01
                       Mean reward: 15239.11
               Mean episode length: 484.62
                 Mean success rate: 97.50
                  Mean reward/step: 31.68
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16113664
                    Iteration time: 0.49s
                        Total time: 963.91s
                               ETA: 16.7s

################################################################################
                     [1m Learning iteration 1967/2000 [0m

                       Computation: 17145 steps/s (collection: 0.262s, learning 0.216s)
               Value function loss: 99116.9695
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 15130.24
               Mean episode length: 481.17
                 Mean success rate: 97.00
                  Mean reward/step: 31.83
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.48s
                        Total time: 964.39s
                               ETA: 16.2s

################################################################################
                     [1m Learning iteration 1968/2000 [0m

                       Computation: 17068 steps/s (collection: 0.267s, learning 0.213s)
               Value function loss: 56048.6365
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 14966.91
               Mean episode length: 476.42
                 Mean success rate: 96.00
                  Mean reward/step: 32.20
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 16130048
                    Iteration time: 0.48s
                        Total time: 964.87s
                               ETA: 15.7s

################################################################################
                     [1m Learning iteration 1969/2000 [0m

                       Computation: 17597 steps/s (collection: 0.263s, learning 0.203s)
               Value function loss: 108270.3654
                    Surrogate loss: -0.0002
             Mean action noise std: 1.01
                       Mean reward: 15059.27
               Mean episode length: 478.68
                 Mean success rate: 96.00
                  Mean reward/step: 32.26
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 0.47s
                        Total time: 965.33s
                               ETA: 15.2s

################################################################################
                     [1m Learning iteration 1970/2000 [0m

                       Computation: 17149 steps/s (collection: 0.268s, learning 0.210s)
               Value function loss: 154115.3037
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 15041.62
               Mean episode length: 478.68
                 Mean success rate: 96.00
                  Mean reward/step: 31.16
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16146432
                    Iteration time: 0.48s
                        Total time: 965.81s
                               ETA: 14.7s

################################################################################
                     [1m Learning iteration 1971/2000 [0m

                       Computation: 16393 steps/s (collection: 0.287s, learning 0.212s)
               Value function loss: 111827.6686
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 14995.47
               Mean episode length: 475.88
                 Mean success rate: 95.50
                  Mean reward/step: 30.26
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 0.50s
                        Total time: 966.31s
                               ETA: 14.2s

################################################################################
                     [1m Learning iteration 1972/2000 [0m

                       Computation: 16378 steps/s (collection: 0.286s, learning 0.214s)
               Value function loss: 141299.0053
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15004.97
               Mean episode length: 475.49
                 Mean success rate: 95.00
                  Mean reward/step: 31.19
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16162816
                    Iteration time: 0.50s
                        Total time: 966.81s
                               ETA: 13.7s

################################################################################
                     [1m Learning iteration 1973/2000 [0m

                       Computation: 16554 steps/s (collection: 0.283s, learning 0.211s)
               Value function loss: 121579.3226
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 14975.43
               Mean episode length: 475.49
                 Mean success rate: 95.00
                  Mean reward/step: 30.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 0.49s
                        Total time: 967.31s
                               ETA: 13.2s

################################################################################
                     [1m Learning iteration 1974/2000 [0m

                       Computation: 17476 steps/s (collection: 0.259s, learning 0.210s)
               Value function loss: 93199.6160
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 14847.61
               Mean episode length: 470.77
                 Mean success rate: 94.00
                  Mean reward/step: 31.38
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16179200
                    Iteration time: 0.47s
                        Total time: 967.78s
                               ETA: 12.7s

################################################################################
                     [1m Learning iteration 1975/2000 [0m

                       Computation: 17453 steps/s (collection: 0.259s, learning 0.210s)
               Value function loss: 126667.4937
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 14967.09
               Mean episode length: 475.04
                 Mean success rate: 95.00
                  Mean reward/step: 31.94
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 0.47s
                        Total time: 968.24s
                               ETA: 12.3s

################################################################################
                     [1m Learning iteration 1976/2000 [0m

                       Computation: 17199 steps/s (collection: 0.276s, learning 0.200s)
               Value function loss: 68490.6800
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 14997.82
               Mean episode length: 475.04
                 Mean success rate: 95.00
                  Mean reward/step: 31.66
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 16195584
                    Iteration time: 0.48s
                        Total time: 968.72s
                               ETA: 11.8s

################################################################################
                     [1m Learning iteration 1977/2000 [0m

                       Computation: 17393 steps/s (collection: 0.260s, learning 0.211s)
               Value function loss: 151471.6644
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15125.99
               Mean episode length: 477.86
                 Mean success rate: 95.50
                  Mean reward/step: 31.85
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 0.47s
                        Total time: 969.19s
                               ETA: 11.3s

################################################################################
                     [1m Learning iteration 1978/2000 [0m

                       Computation: 16888 steps/s (collection: 0.277s, learning 0.208s)
               Value function loss: 101270.5675
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 15228.20
               Mean episode length: 481.31
                 Mean success rate: 96.00
                  Mean reward/step: 30.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16211968
                    Iteration time: 0.49s
                        Total time: 969.68s
                               ETA: 10.8s

################################################################################
                     [1m Learning iteration 1979/2000 [0m

                       Computation: 17037 steps/s (collection: 0.258s, learning 0.223s)
               Value function loss: 73187.6175
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15238.29
               Mean episode length: 481.31
                 Mean success rate: 96.00
                  Mean reward/step: 31.64
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.48s
                        Total time: 970.16s
                               ETA: 10.3s

################################################################################
                     [1m Learning iteration 1980/2000 [0m

                       Computation: 16137 steps/s (collection: 0.296s, learning 0.211s)
               Value function loss: 114245.2662
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15316.69
               Mean episode length: 483.90
                 Mean success rate: 96.50
                  Mean reward/step: 31.69
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16228352
                    Iteration time: 0.51s
                        Total time: 970.67s
                               ETA: 9.8s

################################################################################
                     [1m Learning iteration 1981/2000 [0m

                       Computation: 17328 steps/s (collection: 0.252s, learning 0.221s)
               Value function loss: 102435.1978
                    Surrogate loss: -0.0002
             Mean action noise std: 1.01
                       Mean reward: 15330.56
               Mean episode length: 483.90
                 Mean success rate: 96.50
                  Mean reward/step: 30.94
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 0.47s
                        Total time: 971.14s
                               ETA: 9.3s

################################################################################
                     [1m Learning iteration 1982/2000 [0m

                       Computation: 17347 steps/s (collection: 0.250s, learning 0.222s)
               Value function loss: 96322.3597
                    Surrogate loss: -0.0001
             Mean action noise std: 1.01
                       Mean reward: 15328.07
               Mean episode length: 483.90
                 Mean success rate: 96.50
                  Mean reward/step: 31.52
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16244736
                    Iteration time: 0.47s
                        Total time: 971.61s
                               ETA: 8.8s

################################################################################
                     [1m Learning iteration 1983/2000 [0m

                       Computation: 18041 steps/s (collection: 0.247s, learning 0.207s)
               Value function loss: 87392.8191
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15427.91
               Mean episode length: 486.71
                 Mean success rate: 97.00
                  Mean reward/step: 32.16
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 0.45s
                        Total time: 972.06s
                               ETA: 8.3s

################################################################################
                     [1m Learning iteration 1984/2000 [0m

                       Computation: 18001 steps/s (collection: 0.250s, learning 0.205s)
               Value function loss: 95575.0813
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15384.07
               Mean episode length: 486.71
                 Mean success rate: 97.00
                  Mean reward/step: 32.34
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16261120
                    Iteration time: 0.46s
                        Total time: 972.52s
                               ETA: 7.8s

################################################################################
                     [1m Learning iteration 1985/2000 [0m

                       Computation: 17416 steps/s (collection: 0.258s, learning 0.212s)
               Value function loss: 122951.2447
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15516.68
               Mean episode length: 491.16
                 Mean success rate: 98.00
                  Mean reward/step: 32.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 0.47s
                        Total time: 972.99s
                               ETA: 7.3s

################################################################################
                     [1m Learning iteration 1986/2000 [0m

                       Computation: 17261 steps/s (collection: 0.276s, learning 0.199s)
               Value function loss: 176060.5602
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15676.70
               Mean episode length: 495.88
                 Mean success rate: 99.00
                  Mean reward/step: 31.23
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16277504
                    Iteration time: 0.47s
                        Total time: 973.46s
                               ETA: 6.9s

################################################################################
                     [1m Learning iteration 1987/2000 [0m

                       Computation: 17299 steps/s (collection: 0.261s, learning 0.213s)
               Value function loss: 117402.8551
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15729.03
               Mean episode length: 495.88
                 Mean success rate: 99.00
                  Mean reward/step: 30.93
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 0.47s
                        Total time: 973.94s
                               ETA: 6.4s

################################################################################
                     [1m Learning iteration 1988/2000 [0m

                       Computation: 16636 steps/s (collection: 0.278s, learning 0.214s)
               Value function loss: 133532.2451
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15674.72
               Mean episode length: 495.88
                 Mean success rate: 99.00
                  Mean reward/step: 31.16
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16293888
                    Iteration time: 0.49s
                        Total time: 974.43s
                               ETA: 5.9s

################################################################################
                     [1m Learning iteration 1989/2000 [0m

                       Computation: 17423 steps/s (collection: 0.256s, learning 0.214s)
               Value function loss: 123391.2721
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15676.71
               Mean episode length: 495.88
                 Mean success rate: 99.00
                  Mean reward/step: 31.05
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 0.47s
                        Total time: 974.90s
                               ETA: 5.4s

################################################################################
                     [1m Learning iteration 1990/2000 [0m

                       Computation: 18042 steps/s (collection: 0.245s, learning 0.209s)
               Value function loss: 53203.3932
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15664.17
               Mean episode length: 495.88
                 Mean success rate: 99.00
                  Mean reward/step: 31.62
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 16310272
                    Iteration time: 0.45s
                        Total time: 975.35s
                               ETA: 4.9s

################################################################################
                     [1m Learning iteration 1991/2000 [0m

                       Computation: 17367 steps/s (collection: 0.249s, learning 0.223s)
               Value function loss: 116941.3139
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15485.94
               Mean episode length: 491.38
                 Mean success rate: 98.50
                  Mean reward/step: 31.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.47s
                        Total time: 975.83s
                               ETA: 4.4s

################################################################################
                     [1m Learning iteration 1992/2000 [0m

                       Computation: 17509 steps/s (collection: 0.261s, learning 0.207s)
               Value function loss: 78668.0299
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15680.80
               Mean episode length: 495.50
                 Mean success rate: 99.50
                  Mean reward/step: 31.47
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 16326656
                    Iteration time: 0.47s
                        Total time: 976.29s
                               ETA: 3.9s

################################################################################
                     [1m Learning iteration 1993/2000 [0m

                       Computation: 17868 steps/s (collection: 0.248s, learning 0.210s)
               Value function loss: 152499.1436
                    Surrogate loss: 0.0000
             Mean action noise std: 1.01
                       Mean reward: 15512.08
               Mean episode length: 491.46
                 Mean success rate: 99.00
                  Mean reward/step: 31.17
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 0.46s
                        Total time: 976.75s
                               ETA: 3.4s

################################################################################
                     [1m Learning iteration 1994/2000 [0m

                       Computation: 17098 steps/s (collection: 0.270s, learning 0.209s)
               Value function loss: 77554.0661
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15506.57
               Mean episode length: 491.46
                 Mean success rate: 99.00
                  Mean reward/step: 30.64
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16343040
                    Iteration time: 0.48s
                        Total time: 977.23s
                               ETA: 2.9s

################################################################################
                     [1m Learning iteration 1995/2000 [0m

                       Computation: 16337 steps/s (collection: 0.266s, learning 0.235s)
               Value function loss: 89078.5262
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15478.63
               Mean episode length: 491.46
                 Mean success rate: 99.00
                  Mean reward/step: 31.81
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 0.50s
                        Total time: 977.73s
                               ETA: 2.4s

################################################################################
                     [1m Learning iteration 1996/2000 [0m

                       Computation: 15629 steps/s (collection: 0.274s, learning 0.250s)
               Value function loss: 92607.4487
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15466.25
               Mean episode length: 491.46
                 Mean success rate: 99.00
                  Mean reward/step: 31.34
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16359424
                    Iteration time: 0.52s
                        Total time: 978.26s
                               ETA: 2.0s

################################################################################
                     [1m Learning iteration 1997/2000 [0m

                       Computation: 16677 steps/s (collection: 0.272s, learning 0.220s)
               Value function loss: 88865.6623
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15302.61
               Mean episode length: 485.81
                 Mean success rate: 98.00
                  Mean reward/step: 31.28
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 0.49s
                        Total time: 978.75s
                               ETA: 1.5s

################################################################################
                     [1m Learning iteration 1998/2000 [0m

                       Computation: 17368 steps/s (collection: 0.252s, learning 0.219s)
               Value function loss: 89409.1357
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15274.33
               Mean episode length: 485.81
                 Mean success rate: 98.00
                  Mean reward/step: 31.31
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16375808
                    Iteration time: 0.47s
                        Total time: 979.22s
                               ETA: 1.0s

################################################################################
                     [1m Learning iteration 1999/2000 [0m

                       Computation: 17736 steps/s (collection: 0.244s, learning 0.218s)
               Value function loss: 87113.5567
                    Surrogate loss: -0.0000
             Mean action noise std: 1.01
                       Mean reward: 15261.90
               Mean episode length: 485.81
                 Mean success rate: 98.00
                  Mean reward/step: 31.95
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 0.46s
                        Total time: 979.68s
                               ETA: 0.5s
