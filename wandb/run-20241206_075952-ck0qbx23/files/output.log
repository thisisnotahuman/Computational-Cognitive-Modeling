PopSpikeActor(
  (encoder): PopSpikeEncoderRegularSpike()
  (snn): SpikeMLP(
    (hidden_layers): ModuleList(
      (0): Linear(in_features=340, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=64, bias=True)
    )
    (out_pop_layer): Linear(in_features=64, out_features=90, bias=True)
  )
  (decoder): PopSpikeDecoder(
    (decoder): Conv1d(9, 9, kernel_size=(10,), stride=(1,), groups=9)
    (output_activation): ELU(alpha=1.0)
  )
)
Sequential(
  (0): Linear(in_features=34, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=1, bias=True)
)
################################################################################
                      [1m Learning iteration 0/4000 [0m

                       Computation: 3029 steps/s (collection: 0.502s, learning 2.202s)
               Value function loss: 3.1420
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 4.71
               Mean episode length: 14.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 23.81
--------------------------------------------------------------------------------
                   Total timesteps: 8192
                    Iteration time: 2.70s
                        Total time: 2.70s
                               ETA: 10815.0s

################################################################################
                      [1m Learning iteration 1/4000 [0m

                       Computation: 3261 steps/s (collection: 0.464s, learning 2.048s)
               Value function loss: 3.8904
                    Surrogate loss: 0.0079
             Mean action noise std: 1.00
                       Mean reward: 6.51
               Mean episode length: 23.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 13.84
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 2.51s
                        Total time: 5.22s
                               ETA: 10428.4s

################################################################################
                      [1m Learning iteration 2/4000 [0m

                       Computation: 3273 steps/s (collection: 0.413s, learning 2.089s)
               Value function loss: 3.8005
                    Surrogate loss: 0.0071
             Mean action noise std: 1.00
                       Mean reward: 7.02
               Mean episode length: 24.73
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 15.37
--------------------------------------------------------------------------------
                   Total timesteps: 24576
                    Iteration time: 2.50s
                        Total time: 7.72s
                               ETA: 10285.8s

################################################################################
                      [1m Learning iteration 3/4000 [0m

                       Computation: 3197 steps/s (collection: 0.513s, learning 2.049s)
               Value function loss: 4.0549
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 6.82
               Mean episode length: 25.42
                 Mean success rate: 0.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 15.91
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 2.56s
                        Total time: 10.28s
                               ETA: 10272.6s

################################################################################
                      [1m Learning iteration 4/4000 [0m

                       Computation: 3296 steps/s (collection: 0.427s, learning 2.058s)
               Value function loss: 3.5283
                    Surrogate loss: 0.0076
             Mean action noise std: 1.00
                       Mean reward: 7.93
               Mean episode length: 30.00
                 Mean success rate: 0.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 17.69
--------------------------------------------------------------------------------
                   Total timesteps: 40960
                    Iteration time: 2.48s
                        Total time: 12.77s
                               ETA: 10201.9s

################################################################################
                      [1m Learning iteration 5/4000 [0m

                       Computation: 3117 steps/s (collection: 0.483s, learning 2.145s)
               Value function loss: 3.6712
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 8.93
               Mean episode length: 32.72
                 Mean success rate: 0.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 18.83
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 2.63s
                        Total time: 15.39s
                               ETA: 10249.2s

################################################################################
                      [1m Learning iteration 6/4000 [0m

                       Computation: 3078 steps/s (collection: 0.513s, learning 2.148s)
               Value function loss: 3.3835
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 8.45
               Mean episode length: 31.74
                 Mean success rate: 0.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 19.93
--------------------------------------------------------------------------------
                   Total timesteps: 57344
                    Iteration time: 2.66s
                        Total time: 18.05s
                               ETA: 10301.2s

################################################################################
                      [1m Learning iteration 7/4000 [0m

                       Computation: 3051 steps/s (collection: 0.594s, learning 2.090s)
               Value function loss: 2.7916
                    Surrogate loss: 0.0066
             Mean action noise std: 1.00
                       Mean reward: 9.69
               Mean episode length: 35.83
                 Mean success rate: 0.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 22.26
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 2.68s
                        Total time: 20.74s
                               ETA: 10351.1s

################################################################################
                      [1m Learning iteration 8/4000 [0m

                       Computation: 3264 steps/s (collection: 0.465s, learning 2.044s)
               Value function loss: 2.5647
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 10.14
               Mean episode length: 39.16
                 Mean success rate: 0.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 23.54
--------------------------------------------------------------------------------
                   Total timesteps: 73728
                    Iteration time: 2.51s
                        Total time: 23.25s
                               ETA: 10311.7s

################################################################################
                      [1m Learning iteration 9/4000 [0m

                       Computation: 3178 steps/s (collection: 0.489s, learning 2.088s)
               Value function loss: 2.1977
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 10.72
               Mean episode length: 42.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 2.58s
                        Total time: 25.82s
                               ETA: 10306.7s

################################################################################
                      [1m Learning iteration 10/4000 [0m

                       Computation: 3018 steps/s (collection: 0.537s, learning 2.177s)
               Value function loss: 1.5349
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 11.06
               Mean episode length: 45.83
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 90112
                    Iteration time: 2.71s
                        Total time: 28.54s
                               ETA: 10352.0s

################################################################################
                      [1m Learning iteration 11/4000 [0m

                       Computation: 3116 steps/s (collection: 0.523s, learning 2.106s)
               Value function loss: 1.9264
                    Surrogate loss: 0.0064
             Mean action noise std: 1.00
                       Mean reward: 12.52
               Mean episode length: 52.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 2.63s
                        Total time: 31.17s
                               ETA: 10360.8s

################################################################################
                      [1m Learning iteration 12/4000 [0m

                       Computation: 3042 steps/s (collection: 0.479s, learning 2.213s)
               Value function loss: 3.8738
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 16.00
               Mean episode length: 78.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 25.52
--------------------------------------------------------------------------------
                   Total timesteps: 106496
                    Iteration time: 2.69s
                        Total time: 33.86s
                               ETA: 10387.5s

################################################################################
                      [1m Learning iteration 13/4000 [0m

                       Computation: 3169 steps/s (collection: 0.524s, learning 2.061s)
               Value function loss: 3.9520
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 18.67
               Mean episode length: 94.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 2.58s
                        Total time: 36.45s
                               ETA: 10379.2s

################################################################################
                      [1m Learning iteration 14/4000 [0m

                       Computation: 3260 steps/s (collection: 0.466s, learning 2.046s)
               Value function loss: 5.0117
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 23.15
               Mean episode length: 122.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 23.47
--------------------------------------------------------------------------------
                   Total timesteps: 122880
                    Iteration time: 2.51s
                        Total time: 38.96s
                               ETA: 10352.6s

################################################################################
                      [1m Learning iteration 15/4000 [0m

                       Computation: 3194 steps/s (collection: 0.502s, learning 2.062s)
               Value function loss: 3.5593
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 23.35
               Mean episode length: 119.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 24.67
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 2.56s
                        Total time: 41.52s
                               ETA: 10341.9s

################################################################################
                      [1m Learning iteration 16/4000 [0m

                       Computation: 3249 steps/s (collection: 0.465s, learning 2.056s)
               Value function loss: 3.0092
                    Surrogate loss: 0.0053
             Mean action noise std: 1.00
                       Mean reward: 24.45
               Mean episode length: 124.84
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 25.21
--------------------------------------------------------------------------------
                   Total timesteps: 139264
                    Iteration time: 2.52s
                        Total time: 44.04s
                               ETA: 10321.8s

################################################################################
                      [1m Learning iteration 17/4000 [0m

                       Computation: 3243 steps/s (collection: 0.464s, learning 2.061s)
               Value function loss: 2.5411
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 24.20
               Mean episode length: 122.38
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 2.53s
                        Total time: 46.57s
                               ETA: 10304.8s

################################################################################
                      [1m Learning iteration 18/4000 [0m

                       Computation: 3251 steps/s (collection: 0.471s, learning 2.049s)
               Value function loss: 2.0624
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 25.13
               Mean episode length: 126.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 155648
                    Iteration time: 2.52s
                        Total time: 49.09s
                               ETA: 10288.1s

################################################################################
                      [1m Learning iteration 19/4000 [0m

                       Computation: 3264 steps/s (collection: 0.478s, learning 2.031s)
               Value function loss: 3.6971
                    Surrogate loss: 0.0030
             Mean action noise std: 1.00
                       Mean reward: 29.14
               Mean episode length: 147.49
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 2.51s
                        Total time: 51.60s
                               ETA: 10270.7s

################################################################################
                      [1m Learning iteration 20/4000 [0m

                       Computation: 3336 steps/s (collection: 0.422s, learning 2.034s)
               Value function loss: 3.5041
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 29.77
               Mean episode length: 152.26
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 172032
                    Iteration time: 2.46s
                        Total time: 54.05s
                               ETA: 10244.5s

################################################################################
                      [1m Learning iteration 21/4000 [0m

                       Computation: 3323 steps/s (collection: 0.461s, learning 2.004s)
               Value function loss: 6.2966
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 30.26
               Mean episode length: 151.22
                 Mean success rate: 0.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 2.46s
                        Total time: 56.52s
                               ETA: 10222.2s

################################################################################
                      [1m Learning iteration 22/4000 [0m

                       Computation: 3287 steps/s (collection: 0.441s, learning 2.051s)
               Value function loss: 7.6660
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 31.30
               Mean episode length: 156.43
                 Mean success rate: 0.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 188416
                    Iteration time: 2.49s
                        Total time: 59.01s
                               ETA: 10206.2s

################################################################################
                      [1m Learning iteration 23/4000 [0m

                       Computation: 3290 steps/s (collection: 0.442s, learning 2.048s)
               Value function loss: 7.2565
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 30.82
               Mean episode length: 154.65
                 Mean success rate: 0.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 2.49s
                        Total time: 61.50s
                               ETA: 10191.1s

################################################################################
                      [1m Learning iteration 24/4000 [0m

                       Computation: 3283 steps/s (collection: 0.465s, learning 2.030s)
               Value function loss: 6.0118
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 26.97
               Mean episode length: 129.32
                 Mean success rate: 0.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 204800
                    Iteration time: 2.49s
                        Total time: 63.99s
                               ETA: 10177.7s

################################################################################
                      [1m Learning iteration 25/4000 [0m

                       Computation: 3280 steps/s (collection: 0.453s, learning 2.044s)
               Value function loss: 5.6230
                    Surrogate loss: 0.0030
             Mean action noise std: 1.00
                       Mean reward: 27.67
               Mean episode length: 136.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 2.50s
                        Total time: 66.49s
                               ETA: 10165.6s

################################################################################
                      [1m Learning iteration 26/4000 [0m

                       Computation: 3326 steps/s (collection: 0.432s, learning 2.031s)
               Value function loss: 7.7903
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 27.43
               Mean episode length: 131.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 221184
                    Iteration time: 2.46s
                        Total time: 68.96s
                               ETA: 10149.2s

################################################################################
                      [1m Learning iteration 27/4000 [0m

                       Computation: 3286 steps/s (collection: 0.462s, learning 2.031s)
               Value function loss: 8.5843
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 25.98
               Mean episode length: 119.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 2.49s
                        Total time: 71.45s
                               ETA: 10137.9s

################################################################################
                      [1m Learning iteration 28/4000 [0m

                       Computation: 3306 steps/s (collection: 0.459s, learning 2.019s)
               Value function loss: 9.9908
                    Surrogate loss: 0.0063
             Mean action noise std: 1.00
                       Mean reward: 31.01
               Mean episode length: 138.61
                 Mean success rate: 0.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 237568
                    Iteration time: 2.48s
                        Total time: 73.93s
                               ETA: 10125.2s

################################################################################
                      [1m Learning iteration 29/4000 [0m

                       Computation: 3373 steps/s (collection: 0.417s, learning 2.011s)
               Value function loss: 8.9056
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 37.20
               Mean episode length: 164.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 2.43s
                        Total time: 76.35s
                               ETA: 10106.6s

################################################################################
                      [1m Learning iteration 30/4000 [0m

                       Computation: 3284 steps/s (collection: 0.445s, learning 2.050s)
               Value function loss: 11.9198
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 47.67
               Mean episode length: 207.50
                 Mean success rate: 0.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 253952
                    Iteration time: 2.49s
                        Total time: 78.85s
                               ETA: 10097.6s

################################################################################
                      [1m Learning iteration 31/4000 [0m

                       Computation: 3275 steps/s (collection: 0.462s, learning 2.038s)
               Value function loss: 13.4822
                    Surrogate loss: 0.0069
             Mean action noise std: 1.00
                       Mean reward: 58.79
               Mean episode length: 251.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 2.50s
                        Total time: 81.35s
                               ETA: 10089.7s

################################################################################
                      [1m Learning iteration 32/4000 [0m

                       Computation: 3268 steps/s (collection: 0.463s, learning 2.044s)
               Value function loss: 10.6322
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 67.00
               Mean episode length: 281.24
                 Mean success rate: 0.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 270336
                    Iteration time: 2.51s
                        Total time: 83.85s
                               ETA: 10082.8s

################################################################################
                      [1m Learning iteration 33/4000 [0m

                       Computation: 3270 steps/s (collection: 0.472s, learning 2.033s)
               Value function loss: 10.8744
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 71.32
               Mean episode length: 296.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 2.50s
                        Total time: 86.36s
                               ETA: 10076.1s

################################################################################
                      [1m Learning iteration 34/4000 [0m

                       Computation: 3289 steps/s (collection: 0.447s, learning 2.044s)
               Value function loss: 9.8105
                    Surrogate loss: 0.0038
             Mean action noise std: 1.00
                       Mean reward: 73.50
               Mean episode length: 296.95
                 Mean success rate: 0.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 286720
                    Iteration time: 2.49s
                        Total time: 88.85s
                               ETA: 10068.0s

################################################################################
                      [1m Learning iteration 35/4000 [0m

                       Computation: 3290 steps/s (collection: 0.449s, learning 2.040s)
               Value function loss: 8.6905
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 76.09
               Mean episode length: 300.42
                 Mean success rate: 0.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 2.49s
                        Total time: 91.34s
                               ETA: 10060.0s

################################################################################
                      [1m Learning iteration 36/4000 [0m

                       Computation: 3274 steps/s (collection: 0.455s, learning 2.046s)
               Value function loss: 10.4756
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 72.65
               Mean episode length: 273.63
                 Mean success rate: 0.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 303104
                    Iteration time: 2.50s
                        Total time: 93.84s
                               ETA: 10053.6s

################################################################################
                      [1m Learning iteration 37/4000 [0m

                       Computation: 3354 steps/s (collection: 0.411s, learning 2.031s)
               Value function loss: 14.9160
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 72.76
               Mean episode length: 259.01
                 Mean success rate: 0.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 2.44s
                        Total time: 96.28s
                               ETA: 10041.3s

################################################################################
                      [1m Learning iteration 38/4000 [0m

                       Computation: 3344 steps/s (collection: 0.419s, learning 2.030s)
               Value function loss: 14.7615
                    Surrogate loss: 0.0053
             Mean action noise std: 1.00
                       Mean reward: 70.89
               Mean episode length: 240.43
                 Mean success rate: 0.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 319488
                    Iteration time: 2.45s
                        Total time: 98.73s
                               ETA: 10030.2s

################################################################################
                      [1m Learning iteration 39/4000 [0m

                       Computation: 3187 steps/s (collection: 0.493s, learning 2.078s)
               Value function loss: 18.7295
                    Surrogate loss: 0.0042
             Mean action noise std: 1.00
                       Mean reward: 67.09
               Mean episode length: 217.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.37
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 2.57s
                        Total time: 101.30s
                               ETA: 10031.4s

################################################################################
                      [1m Learning iteration 40/4000 [0m

                       Computation: 3167 steps/s (collection: 0.464s, learning 2.122s)
               Value function loss: 10.5164
                    Surrogate loss: 0.0063
             Mean action noise std: 1.00
                       Mean reward: 65.18
               Mean episode length: 209.14
                 Mean success rate: 0.00
                  Mean reward/step: 0.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 335872
                    Iteration time: 2.59s
                        Total time: 103.89s
                               ETA: 10034.1s

################################################################################
                      [1m Learning iteration 41/4000 [0m

                       Computation: 3330 steps/s (collection: 0.443s, learning 2.017s)
               Value function loss: 18.2541
                    Surrogate loss: 0.0035
             Mean action noise std: 1.00
                       Mean reward: 68.43
               Mean episode length: 213.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 2.46s
                        Total time: 106.35s
                               ETA: 10024.6s

################################################################################
                      [1m Learning iteration 42/4000 [0m

                       Computation: 3235 steps/s (collection: 0.507s, learning 2.025s)
               Value function loss: 16.3346
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 71.29
               Mean episode length: 219.57
                 Mean success rate: 0.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 352256
                    Iteration time: 2.53s
                        Total time: 108.88s
                               ETA: 10022.0s

################################################################################
                      [1m Learning iteration 43/4000 [0m

                       Computation: 3328 steps/s (collection: 0.443s, learning 2.018s)
               Value function loss: 13.0186
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 73.56
               Mean episode length: 223.18
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 2.46s
                        Total time: 111.34s
                               ETA: 10013.1s

################################################################################
                      [1m Learning iteration 44/4000 [0m

                       Computation: 3251 steps/s (collection: 0.452s, learning 2.067s)
               Value function loss: 18.8148
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 78.17
               Mean episode length: 229.54
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 368640
                    Iteration time: 2.52s
                        Total time: 113.86s
                               ETA: 10009.6s

################################################################################
                      [1m Learning iteration 45/4000 [0m

                       Computation: 3192 steps/s (collection: 0.508s, learning 2.058s)
               Value function loss: 20.2664
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 85.37
               Mean episode length: 244.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 2.57s
                        Total time: 116.43s
                               ETA: 10010.1s

################################################################################
                      [1m Learning iteration 46/4000 [0m

                       Computation: 3210 steps/s (collection: 0.447s, learning 2.105s)
               Value function loss: 19.7468
                    Surrogate loss: 0.0053
             Mean action noise std: 1.00
                       Mean reward: 96.64
               Mean episode length: 272.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 385024
                    Iteration time: 2.55s
                        Total time: 118.98s
                               ETA: 10009.3s

################################################################################
                      [1m Learning iteration 47/4000 [0m

                       Computation: 3237 steps/s (collection: 0.476s, learning 2.055s)
               Value function loss: 17.2354
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 101.68
               Mean episode length: 280.31
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 2.53s
                        Total time: 121.51s
                               ETA: 10006.7s

################################################################################
                      [1m Learning iteration 48/4000 [0m

                       Computation: 3073 steps/s (collection: 0.490s, learning 2.175s)
               Value function loss: 14.4768
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 107.36
               Mean episode length: 293.49
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 401408
                    Iteration time: 2.67s
                        Total time: 124.17s
                               ETA: 10014.9s

################################################################################
                      [1m Learning iteration 49/4000 [0m

                       Computation: 3066 steps/s (collection: 0.541s, learning 2.131s)
               Value function loss: 21.4454
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 106.67
               Mean episode length: 281.77
                 Mean success rate: 0.00
                  Mean reward/step: 0.44
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 2.67s
                        Total time: 126.84s
                               ETA: 10023.3s

################################################################################
                      [1m Learning iteration 50/4000 [0m

                       Computation: 3113 steps/s (collection: 0.546s, learning 2.085s)
               Value function loss: 34.0880
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 98.85
               Mean episode length: 252.82
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 25.76
--------------------------------------------------------------------------------
                   Total timesteps: 417792
                    Iteration time: 2.63s
                        Total time: 129.48s
                               ETA: 10028.0s

################################################################################
                      [1m Learning iteration 51/4000 [0m

                       Computation: 3257 steps/s (collection: 0.459s, learning 2.056s)
               Value function loss: 20.8360
                    Surrogate loss: 0.0039
             Mean action noise std: 1.00
                       Mean reward: 93.67
               Mean episode length: 232.96
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 2.51s
                        Total time: 131.99s
                               ETA: 10023.6s

################################################################################
                      [1m Learning iteration 52/4000 [0m

                       Computation: 3137 steps/s (collection: 0.517s, learning 2.094s)
               Value function loss: 23.9417
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 80.32
               Mean episode length: 193.84
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 434176
                    Iteration time: 2.61s
                        Total time: 134.60s
                               ETA: 10026.5s

################################################################################
                      [1m Learning iteration 53/4000 [0m

                       Computation: 3205 steps/s (collection: 0.491s, learning 2.065s)
               Value function loss: 25.5686
                    Surrogate loss: 0.0073
             Mean action noise std: 1.00
                       Mean reward: 72.61
               Mean episode length: 173.43
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 2.56s
                        Total time: 137.16s
                               ETA: 10025.1s

################################################################################
                      [1m Learning iteration 54/4000 [0m

                       Computation: 3047 steps/s (collection: 0.549s, learning 2.139s)
               Value function loss: 25.0248
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 77.10
               Mean episode length: 177.79
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 450560
                    Iteration time: 2.69s
                        Total time: 139.85s
                               ETA: 10033.3s

################################################################################
                      [1m Learning iteration 55/4000 [0m

                       Computation: 3190 steps/s (collection: 0.499s, learning 2.069s)
               Value function loss: 22.8424
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 81.30
               Mean episode length: 184.59
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 2.57s
                        Total time: 142.41s
                               ETA: 10032.5s

################################################################################
                      [1m Learning iteration 56/4000 [0m

                       Computation: 3229 steps/s (collection: 0.427s, learning 2.110s)
               Value function loss: 22.3529
                    Surrogate loss: 0.0036
             Mean action noise std: 1.00
                       Mean reward: 79.52
               Mean episode length: 175.47
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 466944
                    Iteration time: 2.54s
                        Total time: 144.95s
                               ETA: 10029.5s

################################################################################
                      [1m Learning iteration 57/4000 [0m

                       Computation: 3231 steps/s (collection: 0.466s, learning 2.069s)
               Value function loss: 21.5357
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 78.36
               Mean episode length: 171.04
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 2.54s
                        Total time: 147.48s
                               ETA: 10026.4s

################################################################################
                      [1m Learning iteration 58/4000 [0m

                       Computation: 3191 steps/s (collection: 0.457s, learning 2.109s)
               Value function loss: 26.5864
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 86.14
               Mean episode length: 186.50
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 483328
                    Iteration time: 2.57s
                        Total time: 150.05s
                               ETA: 10025.4s

################################################################################
                      [1m Learning iteration 59/4000 [0m

                       Computation: 3182 steps/s (collection: 0.487s, learning 2.087s)
               Value function loss: 30.0796
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 91.73
               Mean episode length: 198.01
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 2.57s
                        Total time: 152.62s
                               ETA: 10024.9s

################################################################################
                      [1m Learning iteration 60/4000 [0m

                       Computation: 3157 steps/s (collection: 0.479s, learning 2.115s)
               Value function loss: 28.3445
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 95.20
               Mean episode length: 208.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 499712
                    Iteration time: 2.59s
                        Total time: 155.22s
                               ETA: 10025.6s

################################################################################
                      [1m Learning iteration 61/4000 [0m

                       Computation: 3193 steps/s (collection: 0.459s, learning 2.106s)
               Value function loss: 34.4622
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 94.23
               Mean episode length: 207.07
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 2.57s
                        Total time: 157.78s
                               ETA: 10024.4s

################################################################################
                      [1m Learning iteration 62/4000 [0m

                       Computation: 3113 steps/s (collection: 0.532s, learning 2.099s)
               Value function loss: 29.2824
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 98.37
               Mean episode length: 214.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 516096
                    Iteration time: 2.63s
                        Total time: 160.42s
                               ETA: 10027.2s

################################################################################
                      [1m Learning iteration 63/4000 [0m

                       Computation: 3174 steps/s (collection: 0.478s, learning 2.102s)
               Value function loss: 36.2630
                    Surrogate loss: 0.0069
             Mean action noise std: 1.00
                       Mean reward: 95.86
               Mean episode length: 207.10
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 2.58s
                        Total time: 163.00s
                               ETA: 10026.8s

################################################################################
                      [1m Learning iteration 64/4000 [0m

                       Computation: 3197 steps/s (collection: 0.459s, learning 2.103s)
               Value function loss: 29.1982
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 89.52
               Mean episode length: 188.81
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 532480
                    Iteration time: 2.56s
                        Total time: 165.56s
                               ETA: 10025.1s

################################################################################
                      [1m Learning iteration 65/4000 [0m

                       Computation: 3074 steps/s (collection: 0.460s, learning 2.204s)
               Value function loss: 28.7182
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 93.05
               Mean episode length: 196.29
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 2.66s
                        Total time: 168.22s
                               ETA: 10029.6s

################################################################################
                      [1m Learning iteration 66/4000 [0m

                       Computation: 3024 steps/s (collection: 0.506s, learning 2.203s)
               Value function loss: 26.3384
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 95.26
               Mean episode length: 200.52
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 548864
                    Iteration time: 2.71s
                        Total time: 170.93s
                               ETA: 10036.4s

################################################################################
                      [1m Learning iteration 67/4000 [0m

                       Computation: 3121 steps/s (collection: 0.533s, learning 2.091s)
               Value function loss: 26.4001
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 107.31
               Mean episode length: 222.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 2.62s
                        Total time: 173.55s
                               ETA: 10038.1s

################################################################################
                      [1m Learning iteration 68/4000 [0m

                       Computation: 3174 steps/s (collection: 0.513s, learning 2.067s)
               Value function loss: 35.4319
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 112.36
               Mean episode length: 229.93
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 565248
                    Iteration time: 2.58s
                        Total time: 176.14s
                               ETA: 10037.1s

################################################################################
                      [1m Learning iteration 69/4000 [0m

                       Computation: 3256 steps/s (collection: 0.492s, learning 2.023s)
               Value function loss: 29.2204
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 112.83
               Mean episode length: 228.43
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 2.52s
                        Total time: 178.65s
                               ETA: 10032.5s

################################################################################
                      [1m Learning iteration 70/4000 [0m

                       Computation: 3029 steps/s (collection: 0.589s, learning 2.115s)
               Value function loss: 32.7938
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 117.19
               Mean episode length: 234.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 581632
                    Iteration time: 2.70s
                        Total time: 181.35s
                               ETA: 10038.4s

################################################################################
                      [1m Learning iteration 71/4000 [0m

                       Computation: 3170 steps/s (collection: 0.511s, learning 2.073s)
               Value function loss: 26.1909
                    Surrogate loss: 0.0039
             Mean action noise std: 1.00
                       Mean reward: 114.81
               Mean episode length: 229.95
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 2.58s
                        Total time: 183.94s
                               ETA: 10037.4s

################################################################################
                      [1m Learning iteration 72/4000 [0m

                       Computation: 3096 steps/s (collection: 0.502s, learning 2.144s)
               Value function loss: 23.7283
                    Surrogate loss: 0.0053
             Mean action noise std: 1.00
                       Mean reward: 106.99
               Mean episode length: 212.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.46
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 598016
                    Iteration time: 2.65s
                        Total time: 186.58s
                               ETA: 10039.7s

################################################################################
                      [1m Learning iteration 73/4000 [0m

                       Computation: 3237 steps/s (collection: 0.470s, learning 2.060s)
               Value function loss: 16.9556
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 106.63
               Mean episode length: 215.46
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 2.53s
                        Total time: 189.11s
                               ETA: 10035.8s

################################################################################
                      [1m Learning iteration 74/4000 [0m

                       Computation: 3162 steps/s (collection: 0.460s, learning 2.130s)
               Value function loss: 27.1864
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 106.72
               Mean episode length: 216.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 614400
                    Iteration time: 2.59s
                        Total time: 191.70s
                               ETA: 10035.1s

################################################################################
                      [1m Learning iteration 75/4000 [0m

                       Computation: 3072 steps/s (collection: 0.587s, learning 2.079s)
               Value function loss: 21.1161
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 111.01
               Mean episode length: 225.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.46
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 2.67s
                        Total time: 194.37s
                               ETA: 10038.2s

################################################################################
                      [1m Learning iteration 76/4000 [0m

                       Computation: 3256 steps/s (collection: 0.454s, learning 2.062s)
               Value function loss: 20.4269
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 123.11
               Mean episode length: 250.41
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 630784
                    Iteration time: 2.52s
                        Total time: 196.89s
                               ETA: 10033.5s

################################################################################
                      [1m Learning iteration 77/4000 [0m

                       Computation: 3185 steps/s (collection: 0.523s, learning 2.049s)
               Value function loss: 27.2398
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 124.06
               Mean episode length: 258.30
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 2.57s
                        Total time: 199.46s
                               ETA: 10031.7s

################################################################################
                      [1m Learning iteration 78/4000 [0m

                       Computation: 3263 steps/s (collection: 0.441s, learning 2.069s)
               Value function loss: 29.2195
                    Surrogate loss: 0.0081
             Mean action noise std: 1.00
                       Mean reward: 129.18
               Mean episode length: 271.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 647168
                    Iteration time: 2.51s
                        Total time: 201.97s
                               ETA: 10026.8s

################################################################################
                      [1m Learning iteration 79/4000 [0m

                       Computation: 3223 steps/s (collection: 0.434s, learning 2.107s)
               Value function loss: 29.9381
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 131.50
               Mean episode length: 280.50
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 2.54s
                        Total time: 204.51s
                               ETA: 10023.5s

################################################################################
                      [1m Learning iteration 80/4000 [0m

                       Computation: 3128 steps/s (collection: 0.460s, learning 2.158s)
               Value function loss: 20.9323
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 138.72
               Mean episode length: 294.46
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 663552
                    Iteration time: 2.62s
                        Total time: 207.13s
                               ETA: 10023.9s

################################################################################
                      [1m Learning iteration 81/4000 [0m

                       Computation: 3213 steps/s (collection: 0.460s, learning 2.090s)
               Value function loss: 25.6711
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 149.16
               Mean episode length: 314.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 2.55s
                        Total time: 209.68s
                               ETA: 10021.0s

################################################################################
                      [1m Learning iteration 82/4000 [0m

                       Computation: 3211 steps/s (collection: 0.458s, learning 2.093s)
               Value function loss: 32.0919
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 151.66
               Mean episode length: 316.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 679936
                    Iteration time: 2.55s
                        Total time: 212.23s
                               ETA: 10018.1s

################################################################################
                      [1m Learning iteration 83/4000 [0m

                       Computation: 3194 steps/s (collection: 0.460s, learning 2.104s)
               Value function loss: 31.6164
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 161.75
               Mean episode length: 334.17
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 2.56s
                        Total time: 214.79s
                               ETA: 10015.9s

################################################################################
                      [1m Learning iteration 84/4000 [0m

                       Computation: 3266 steps/s (collection: 0.458s, learning 2.050s)
               Value function loss: 50.2324
                    Surrogate loss: 0.0086
             Mean action noise std: 1.00
                       Mean reward: 167.53
               Mean episode length: 344.49
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 696320
                    Iteration time: 2.51s
                        Total time: 217.30s
                               ETA: 10011.1s

################################################################################
                      [1m Learning iteration 85/4000 [0m

                       Computation: 3154 steps/s (collection: 0.492s, learning 2.105s)
               Value function loss: 21.1213
                    Surrogate loss: 0.0053
             Mean action noise std: 1.00
                       Mean reward: 168.38
               Mean episode length: 347.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 2.60s
                        Total time: 219.90s
                               ETA: 10010.4s

################################################################################
                      [1m Learning iteration 86/4000 [0m

                       Computation: 3249 steps/s (collection: 0.449s, learning 2.072s)
               Value function loss: 28.7734
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 169.74
               Mean episode length: 351.92
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 712704
                    Iteration time: 2.52s
                        Total time: 222.42s
                               ETA: 10006.2s

################################################################################
                      [1m Learning iteration 87/4000 [0m

                       Computation: 3209 steps/s (collection: 0.478s, learning 2.074s)
               Value function loss: 38.2060
                    Surrogate loss: 0.0068
             Mean action noise std: 1.00
                       Mean reward: 179.66
               Mean episode length: 368.27
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 2.55s
                        Total time: 224.97s
                               ETA: 10003.5s

################################################################################
                      [1m Learning iteration 88/4000 [0m

                       Computation: 3205 steps/s (collection: 0.477s, learning 2.079s)
               Value function loss: 36.7512
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 169.99
               Mean episode length: 344.42
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 729088
                    Iteration time: 2.56s
                        Total time: 227.53s
                               ETA: 10000.9s

################################################################################
                      [1m Learning iteration 89/4000 [0m

                       Computation: 3223 steps/s (collection: 0.468s, learning 2.073s)
               Value function loss: 35.8968
                    Surrogate loss: 0.0068
             Mean action noise std: 1.00
                       Mean reward: 170.82
               Mean episode length: 344.01
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 2.54s
                        Total time: 230.07s
                               ETA: 9997.7s

################################################################################
                      [1m Learning iteration 90/4000 [0m

                       Computation: 3210 steps/s (collection: 0.475s, learning 2.077s)
               Value function loss: 49.9333
                    Surrogate loss: 0.0066
             Mean action noise std: 1.00
                       Mean reward: 156.07
               Mean episode length: 316.93
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 745472
                    Iteration time: 2.55s
                        Total time: 232.62s
                               ETA: 9994.9s

################################################################################
                      [1m Learning iteration 91/4000 [0m

                       Computation: 3236 steps/s (collection: 0.464s, learning 2.067s)
               Value function loss: 33.8035
                    Surrogate loss: 0.0071
             Mean action noise std: 1.00
                       Mean reward: 140.60
               Mean episode length: 288.98
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 2.53s
                        Total time: 235.15s
                               ETA: 9991.3s

################################################################################
                      [1m Learning iteration 92/4000 [0m

                       Computation: 3274 steps/s (collection: 0.461s, learning 2.040s)
               Value function loss: 43.9229
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 137.15
               Mean episode length: 275.67
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 761856
                    Iteration time: 2.50s
                        Total time: 237.65s
                               ETA: 9986.4s

################################################################################
                      [1m Learning iteration 93/4000 [0m

                       Computation: 3317 steps/s (collection: 0.435s, learning 2.034s)
               Value function loss: 61.9771
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 136.86
               Mean episode length: 264.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.61
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 2.47s
                        Total time: 240.12s
                               ETA: 9980.3s

################################################################################
                      [1m Learning iteration 94/4000 [0m

                       Computation: 3229 steps/s (collection: 0.487s, learning 2.050s)
               Value function loss: 45.0240
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 142.54
               Mean episode length: 273.26
                 Mean success rate: 0.00
                  Mean reward/step: 0.64
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 778240
                    Iteration time: 2.54s
                        Total time: 242.66s
                               ETA: 9977.0s

################################################################################
                      [1m Learning iteration 95/4000 [0m

                       Computation: 3256 steps/s (collection: 0.482s, learning 2.034s)
               Value function loss: 70.0842
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 138.87
               Mean episode length: 256.73
                 Mean success rate: 0.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 2.52s
                        Total time: 245.17s
                               ETA: 9972.9s

################################################################################
                      [1m Learning iteration 96/4000 [0m

                       Computation: 3217 steps/s (collection: 0.492s, learning 2.054s)
               Value function loss: 57.5605
                    Surrogate loss: 0.0063
             Mean action noise std: 1.00
                       Mean reward: 132.54
               Mean episode length: 240.90
                 Mean success rate: 0.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 794624
                    Iteration time: 2.55s
                        Total time: 247.72s
                               ETA: 9970.0s

################################################################################
                      [1m Learning iteration 97/4000 [0m

                       Computation: 3229 steps/s (collection: 0.492s, learning 2.044s)
               Value function loss: 55.8588
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 117.79
               Mean episode length: 210.33
                 Mean success rate: 0.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 2.54s
                        Total time: 250.25s
                               ETA: 9966.8s

################################################################################
                      [1m Learning iteration 98/4000 [0m

                       Computation: 3198 steps/s (collection: 0.466s, learning 2.095s)
               Value function loss: 55.0242
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 107.69
               Mean episode length: 187.73
                 Mean success rate: 0.00
                  Mean reward/step: 0.68
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 811008
                    Iteration time: 2.56s
                        Total time: 252.82s
                               ETA: 9964.5s

################################################################################
                      [1m Learning iteration 99/4000 [0m

                       Computation: 3188 steps/s (collection: 0.485s, learning 2.084s)
               Value function loss: 77.1763
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 105.64
               Mean episode length: 177.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.69
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 2.57s
                        Total time: 255.38s
                               ETA: 9962.6s

################################################################################
                     [1m Learning iteration 100/4000 [0m

                       Computation: 3172 steps/s (collection: 0.531s, learning 2.051s)
               Value function loss: 67.3387
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 111.88
               Mean episode length: 181.11
                 Mean success rate: 0.00
                  Mean reward/step: 0.69
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 827392
                    Iteration time: 2.58s
                        Total time: 257.97s
                               ETA: 9961.1s

################################################################################
                     [1m Learning iteration 101/4000 [0m

                       Computation: 3210 steps/s (collection: 0.506s, learning 2.045s)
               Value function loss: 63.3881
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 117.32
               Mean episode length: 180.07
                 Mean success rate: 0.00
                  Mean reward/step: 0.68
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 2.55s
                        Total time: 260.52s
                               ETA: 9958.4s

################################################################################
                     [1m Learning iteration 102/4000 [0m

                       Computation: 3270 steps/s (collection: 0.468s, learning 2.036s)
               Value function loss: 73.1705
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 125.71
               Mean episode length: 182.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 843776
                    Iteration time: 2.50s
                        Total time: 263.02s
                               ETA: 9954.0s

################################################################################
                     [1m Learning iteration 103/4000 [0m

                       Computation: 3228 steps/s (collection: 0.468s, learning 2.070s)
               Value function loss: 49.2522
                    Surrogate loss: 0.0053
             Mean action noise std: 1.00
                       Mean reward: 119.38
               Mean episode length: 175.10
                 Mean success rate: 0.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 2.54s
                        Total time: 265.56s
                               ETA: 9950.8s

################################################################################
                     [1m Learning iteration 104/4000 [0m

                       Computation: 3230 steps/s (collection: 0.451s, learning 2.085s)
               Value function loss: 52.4600
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 120.05
               Mean episode length: 173.65
                 Mean success rate: 0.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 860160
                    Iteration time: 2.54s
                        Total time: 268.10s
                               ETA: 9947.6s

################################################################################
                     [1m Learning iteration 105/4000 [0m

                       Computation: 3217 steps/s (collection: 0.478s, learning 2.068s)
               Value function loss: 50.8792
                    Surrogate loss: 0.0069
             Mean action noise std: 1.00
                       Mean reward: 118.24
               Mean episode length: 169.90
                 Mean success rate: 0.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 2.55s
                        Total time: 270.64s
                               ETA: 9944.8s

################################################################################
                     [1m Learning iteration 106/4000 [0m

                       Computation: 3235 steps/s (collection: 0.463s, learning 2.069s)
               Value function loss: 73.3290
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 115.14
               Mean episode length: 174.31
                 Mean success rate: 0.00
                  Mean reward/step: 0.66
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 876544
                    Iteration time: 2.53s
                        Total time: 273.17s
                               ETA: 9941.5s

################################################################################
                     [1m Learning iteration 107/4000 [0m

                       Computation: 3122 steps/s (collection: 0.482s, learning 2.141s)
               Value function loss: 54.7607
                    Surrogate loss: 0.0073
             Mean action noise std: 1.00
                       Mean reward: 107.51
               Mean episode length: 166.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.69
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 2.62s
                        Total time: 275.80s
                               ETA: 9941.4s

################################################################################
                     [1m Learning iteration 108/4000 [0m

                       Computation: 3226 steps/s (collection: 0.503s, learning 2.036s)
               Value function loss: 63.8645
                    Surrogate loss: 0.0066
             Mean action noise std: 1.00
                       Mean reward: 106.26
               Mean episode length: 167.19
                 Mean success rate: 0.00
                  Mean reward/step: 0.70
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 892928
                    Iteration time: 2.54s
                        Total time: 278.34s
                               ETA: 9938.4s

################################################################################
                     [1m Learning iteration 109/4000 [0m

                       Computation: 3189 steps/s (collection: 0.498s, learning 2.070s)
               Value function loss: 80.8000
                    Surrogate loss: 0.0042
             Mean action noise std: 1.00
                       Mean reward: 111.00
               Mean episode length: 170.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.69
       Mean episode length/episode: 25.92
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 2.57s
                        Total time: 280.90s
                               ETA: 9936.3s

################################################################################
                     [1m Learning iteration 110/4000 [0m

                       Computation: 3164 steps/s (collection: 0.467s, learning 2.122s)
               Value function loss: 69.3349
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 107.38
               Mean episode length: 162.32
                 Mean success rate: 0.00
                  Mean reward/step: 0.68
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 909312
                    Iteration time: 2.59s
                        Total time: 283.49s
                               ETA: 9935.0s

################################################################################
                     [1m Learning iteration 111/4000 [0m

                       Computation: 3213 steps/s (collection: 0.470s, learning 2.079s)
               Value function loss: 60.4279
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 107.71
               Mean episode length: 162.03
                 Mean success rate: 0.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 2.55s
                        Total time: 286.04s
                               ETA: 9932.3s

################################################################################
                     [1m Learning iteration 112/4000 [0m

                       Computation: 3225 steps/s (collection: 0.502s, learning 2.038s)
               Value function loss: 76.7925
                    Surrogate loss: 0.0082
             Mean action noise std: 1.00
                       Mean reward: 119.52
               Mean episode length: 177.71
                 Mean success rate: 0.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 925696
                    Iteration time: 2.54s
                        Total time: 288.58s
                               ETA: 9929.3s

################################################################################
                     [1m Learning iteration 113/4000 [0m

                       Computation: 3173 steps/s (collection: 0.485s, learning 2.096s)
               Value function loss: 60.9242
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 114.67
               Mean episode length: 175.07
                 Mean success rate: 0.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 2.58s
                        Total time: 291.16s
                               ETA: 9927.6s

################################################################################
                     [1m Learning iteration 114/4000 [0m

                       Computation: 3241 steps/s (collection: 0.484s, learning 2.043s)
               Value function loss: 63.4474
                    Surrogate loss: 0.0082
             Mean action noise std: 1.00
                       Mean reward: 123.78
               Mean episode length: 189.87
                 Mean success rate: 0.00
                  Mean reward/step: 0.69
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 942080
                    Iteration time: 2.53s
                        Total time: 293.69s
                               ETA: 9924.2s

################################################################################
                     [1m Learning iteration 115/4000 [0m

                       Computation: 3297 steps/s (collection: 0.445s, learning 2.039s)
               Value function loss: 51.0260
                    Surrogate loss: 0.0071
             Mean action noise std: 1.00
                       Mean reward: 125.53
               Mean episode length: 189.84
                 Mean success rate: 0.00
                  Mean reward/step: 0.70
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 2.48s
                        Total time: 296.17s
                               ETA: 9919.3s

################################################################################
                     [1m Learning iteration 116/4000 [0m

                       Computation: 3217 steps/s (collection: 0.469s, learning 2.077s)
               Value function loss: 62.3983
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 124.83
               Mean episode length: 189.71
                 Mean success rate: 0.00
                  Mean reward/step: 0.71
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 958464
                    Iteration time: 2.55s
                        Total time: 298.72s
                               ETA: 9916.5s

################################################################################
                     [1m Learning iteration 117/4000 [0m

                       Computation: 3190 steps/s (collection: 0.473s, learning 2.095s)
               Value function loss: 80.9714
                    Surrogate loss: 0.0084
             Mean action noise std: 1.00
                       Mean reward: 120.65
               Mean episode length: 185.70
                 Mean success rate: 0.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 2.57s
                        Total time: 301.29s
                               ETA: 9914.4s

################################################################################
                     [1m Learning iteration 118/4000 [0m

                       Computation: 3167 steps/s (collection: 0.510s, learning 2.077s)
               Value function loss: 86.6030
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 136.62
               Mean episode length: 202.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.78
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 974848
                    Iteration time: 2.59s
                        Total time: 303.87s
                               ETA: 9913.0s

################################################################################
                     [1m Learning iteration 119/4000 [0m

                       Computation: 3198 steps/s (collection: 0.477s, learning 2.084s)
               Value function loss: 61.0106
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 140.66
               Mean episode length: 204.65
                 Mean success rate: 0.00
                  Mean reward/step: 0.74
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 2.56s
                        Total time: 306.44s
                               ETA: 9910.6s

################################################################################
                     [1m Learning iteration 120/4000 [0m

                       Computation: 3213 steps/s (collection: 0.459s, learning 2.090s)
               Value function loss: 46.8247
                    Surrogate loss: 0.0069
             Mean action noise std: 1.00
                       Mean reward: 144.98
               Mean episode length: 208.95
                 Mean success rate: 0.00
                  Mean reward/step: 0.76
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 991232
                    Iteration time: 2.55s
                        Total time: 308.99s
                               ETA: 9908.0s

################################################################################
                     [1m Learning iteration 121/4000 [0m

                       Computation: 3183 steps/s (collection: 0.505s, learning 2.069s)
               Value function loss: 72.1100
                    Surrogate loss: 0.0082
             Mean action noise std: 1.00
                       Mean reward: 153.43
               Mean episode length: 218.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.78
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 2.57s
                        Total time: 311.56s
                               ETA: 9906.0s

################################################################################
                     [1m Learning iteration 122/4000 [0m

                       Computation: 3208 steps/s (collection: 0.480s, learning 2.073s)
               Value function loss: 93.2220
                    Surrogate loss: 0.0084
             Mean action noise std: 1.00
                       Mean reward: 150.27
               Mean episode length: 209.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 1007616
                    Iteration time: 2.55s
                        Total time: 314.11s
                               ETA: 9903.5s

################################################################################
                     [1m Learning iteration 123/4000 [0m

                       Computation: 3220 steps/s (collection: 0.490s, learning 2.054s)
               Value function loss: 95.2796
                    Surrogate loss: 0.0083
             Mean action noise std: 1.00
                       Mean reward: 149.26
               Mean episode length: 205.49
                 Mean success rate: 0.00
                  Mean reward/step: 0.81
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 2.54s
                        Total time: 316.66s
                               ETA: 9900.6s

################################################################################
                     [1m Learning iteration 124/4000 [0m

                       Computation: 3212 steps/s (collection: 0.481s, learning 2.069s)
               Value function loss: 67.8153
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 148.35
               Mean episode length: 204.65
                 Mean success rate: 0.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1024000
                    Iteration time: 2.55s
                        Total time: 319.21s
                               ETA: 9897.9s

################################################################################
                     [1m Learning iteration 125/4000 [0m

                       Computation: 3159 steps/s (collection: 0.502s, learning 2.091s)
               Value function loss: 103.1335
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 141.15
               Mean episode length: 194.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.82
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 2.59s
                        Total time: 321.80s
                               ETA: 9896.6s

################################################################################
                     [1m Learning iteration 126/4000 [0m

                       Computation: 3207 steps/s (collection: 0.484s, learning 2.070s)
               Value function loss: 67.9363
                    Surrogate loss: 0.0092
             Mean action noise std: 1.00
                       Mean reward: 155.03
               Mean episode length: 209.38
                 Mean success rate: 0.00
                  Mean reward/step: 0.83
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1040384
                    Iteration time: 2.55s
                        Total time: 324.35s
                               ETA: 9894.0s

################################################################################
                     [1m Learning iteration 127/4000 [0m

                       Computation: 3100 steps/s (collection: 0.479s, learning 2.163s)
               Value function loss: 67.1990
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 161.11
               Mean episode length: 214.08
                 Mean success rate: 0.00
                  Mean reward/step: 0.83
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 2.64s
                        Total time: 327.00s
                               ETA: 9894.2s

################################################################################
                     [1m Learning iteration 128/4000 [0m

                       Computation: 3211 steps/s (collection: 0.470s, learning 2.081s)
               Value function loss: 60.1338
                    Surrogate loss: 0.0083
             Mean action noise std: 1.00
                       Mean reward: 166.18
               Mean episode length: 220.40
                 Mean success rate: 0.00
                  Mean reward/step: 0.83
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1056768
                    Iteration time: 2.55s
                        Total time: 329.55s
                               ETA: 9891.5s

################################################################################
                     [1m Learning iteration 129/4000 [0m

                       Computation: 3152 steps/s (collection: 0.489s, learning 2.110s)
               Value function loss: 91.3573
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 178.79
               Mean episode length: 232.63
                 Mean success rate: 0.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 2.60s
                        Total time: 332.15s
                               ETA: 9890.3s

################################################################################
                     [1m Learning iteration 130/4000 [0m

                       Computation: 3196 steps/s (collection: 0.495s, learning 2.068s)
               Value function loss: 114.4458
                    Surrogate loss: 0.0087
             Mean action noise std: 1.00
                       Mean reward: 178.99
               Mean episode length: 223.07
                 Mean success rate: 0.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 1073152
                    Iteration time: 2.56s
                        Total time: 334.71s
                               ETA: 9887.9s

################################################################################
                     [1m Learning iteration 131/4000 [0m

                       Computation: 3168 steps/s (collection: 0.476s, learning 2.109s)
               Value function loss: 122.7360
                    Surrogate loss: 0.0080
             Mean action noise std: 1.00
                       Mean reward: 170.43
               Mean episode length: 213.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 2.59s
                        Total time: 337.29s
                               ETA: 9886.3s

################################################################################
                     [1m Learning iteration 132/4000 [0m

                       Computation: 3200 steps/s (collection: 0.519s, learning 2.040s)
               Value function loss: 88.6749
                    Surrogate loss: 0.0079
             Mean action noise std: 0.99
                       Mean reward: 168.47
               Mean episode length: 209.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1089536
                    Iteration time: 2.56s
                        Total time: 339.85s
                               ETA: 9883.8s

################################################################################
                     [1m Learning iteration 133/4000 [0m

                       Computation: 3197 steps/s (collection: 0.485s, learning 2.077s)
               Value function loss: 110.7002
                    Surrogate loss: 0.0073
             Mean action noise std: 1.00
                       Mean reward: 170.32
               Mean episode length: 207.09
                 Mean success rate: 0.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 2.56s
                        Total time: 342.41s
                               ETA: 9881.5s

################################################################################
                     [1m Learning iteration 134/4000 [0m

                       Computation: 3249 steps/s (collection: 0.453s, learning 2.068s)
               Value function loss: 85.7541
                    Surrogate loss: 0.0069
             Mean action noise std: 1.00
                       Mean reward: 167.81
               Mean episode length: 201.53
                 Mean success rate: 0.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1105920
                    Iteration time: 2.52s
                        Total time: 344.94s
                               ETA: 9877.9s

################################################################################
                     [1m Learning iteration 135/4000 [0m

                       Computation: 3232 steps/s (collection: 0.489s, learning 2.045s)
               Value function loss: 105.2810
                    Surrogate loss: 0.0081
             Mean action noise std: 1.00
                       Mean reward: 174.92
               Mean episode length: 209.03
                 Mean success rate: 0.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 2.53s
                        Total time: 347.47s
                               ETA: 9874.8s

################################################################################
                     [1m Learning iteration 136/4000 [0m

                       Computation: 3178 steps/s (collection: 0.507s, learning 2.070s)
               Value function loss: 89.8162
                    Surrogate loss: 0.0089
             Mean action noise std: 1.00
                       Mean reward: 173.65
               Mean episode length: 207.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.96
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1122304
                    Iteration time: 2.58s
                        Total time: 350.05s
                               ETA: 9872.9s

################################################################################
                     [1m Learning iteration 137/4000 [0m

                       Computation: 3246 steps/s (collection: 0.457s, learning 2.066s)
               Value function loss: 124.7927
                    Surrogate loss: 0.0088
             Mean action noise std: 1.00
                       Mean reward: 196.09
               Mean episode length: 232.29
                 Mean success rate: 0.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 2.52s
                        Total time: 352.57s
                               ETA: 9869.4s

################################################################################
                     [1m Learning iteration 138/4000 [0m

                       Computation: 3180 steps/s (collection: 0.503s, learning 2.073s)
               Value function loss: 122.3450
                    Surrogate loss: 0.0072
             Mean action noise std: 0.99
                       Mean reward: 201.27
               Mean episode length: 234.68
                 Mean success rate: 0.00
                  Mean reward/step: 0.97
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1138688
                    Iteration time: 2.58s
                        Total time: 355.15s
                               ETA: 9867.4s

################################################################################
                     [1m Learning iteration 139/4000 [0m

                       Computation: 3239 steps/s (collection: 0.475s, learning 2.054s)
               Value function loss: 122.9308
                    Surrogate loss: 0.0100
             Mean action noise std: 0.99
                       Mean reward: 199.41
               Mean episode length: 231.29
                 Mean success rate: 0.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 2.53s
                        Total time: 357.68s
                               ETA: 9864.2s

################################################################################
                     [1m Learning iteration 140/4000 [0m

                       Computation: 3226 steps/s (collection: 0.495s, learning 2.044s)
               Value function loss: 110.5018
                    Surrogate loss: 0.0080
             Mean action noise std: 0.99
                       Mean reward: 205.21
               Mean episode length: 236.90
                 Mean success rate: 0.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1155072
                    Iteration time: 2.54s
                        Total time: 360.21s
                               ETA: 9861.2s

################################################################################
                     [1m Learning iteration 141/4000 [0m

                       Computation: 3177 steps/s (collection: 0.524s, learning 2.054s)
               Value function loss: 127.4643
                    Surrogate loss: 0.0095
             Mean action noise std: 0.99
                       Mean reward: 211.93
               Mean episode length: 244.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 2.58s
                        Total time: 362.79s
                               ETA: 9859.3s

################################################################################
                     [1m Learning iteration 142/4000 [0m

                       Computation: 3206 steps/s (collection: 0.460s, learning 2.094s)
               Value function loss: 138.0649
                    Surrogate loss: 0.0109
             Mean action noise std: 0.99
                       Mean reward: 198.56
               Mean episode length: 227.79
                 Mean success rate: 0.00
                  Mean reward/step: 0.96
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1171456
                    Iteration time: 2.55s
                        Total time: 365.35s
                               ETA: 9856.7s

################################################################################
                     [1m Learning iteration 143/4000 [0m

                       Computation: 3192 steps/s (collection: 0.499s, learning 2.067s)
               Value function loss: 121.0009
                    Surrogate loss: 0.0075
             Mean action noise std: 0.99
                       Mean reward: 202.32
               Mean episode length: 227.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 2.57s
                        Total time: 367.91s
                               ETA: 9854.4s

################################################################################
                     [1m Learning iteration 144/4000 [0m

                       Computation: 3175 steps/s (collection: 0.510s, learning 2.070s)
               Value function loss: 127.2671
                    Surrogate loss: 0.0083
             Mean action noise std: 0.99
                       Mean reward: 197.80
               Mean episode length: 221.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1187840
                    Iteration time: 2.58s
                        Total time: 370.49s
                               ETA: 9852.6s

################################################################################
                     [1m Learning iteration 145/4000 [0m

                       Computation: 3203 steps/s (collection: 0.482s, learning 2.075s)
               Value function loss: 110.6038
                    Surrogate loss: 0.0095
             Mean action noise std: 0.99
                       Mean reward: 199.07
               Mean episode length: 220.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 2.56s
                        Total time: 373.05s
                               ETA: 9850.1s

################################################################################
                     [1m Learning iteration 146/4000 [0m

                       Computation: 3205 steps/s (collection: 0.502s, learning 2.054s)
               Value function loss: 189.9671
                    Surrogate loss: 0.0111
             Mean action noise std: 0.99
                       Mean reward: 188.62
               Mean episode length: 204.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 1204224
                    Iteration time: 2.56s
                        Total time: 375.61s
                               ETA: 9847.5s

################################################################################
                     [1m Learning iteration 147/4000 [0m

                       Computation: 3190 steps/s (collection: 0.508s, learning 2.060s)
               Value function loss: 131.8794
                    Surrogate loss: 0.0105
             Mean action noise std: 0.99
                       Mean reward: 196.33
               Mean episode length: 207.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 2.57s
                        Total time: 378.17s
                               ETA: 9845.3s

################################################################################
                     [1m Learning iteration 148/4000 [0m

                       Computation: 3215 steps/s (collection: 0.505s, learning 2.043s)
               Value function loss: 140.7324
                    Surrogate loss: 0.0070
             Mean action noise std: 0.99
                       Mean reward: 196.34
               Mean episode length: 205.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1220608
                    Iteration time: 2.55s
                        Total time: 380.72s
                               ETA: 9842.5s

################################################################################
                     [1m Learning iteration 149/4000 [0m

                       Computation: 3220 steps/s (collection: 0.489s, learning 2.054s)
               Value function loss: 130.6229
                    Surrogate loss: 0.0100
             Mean action noise std: 0.99
                       Mean reward: 199.88
               Mean episode length: 206.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 2.54s
                        Total time: 383.26s
                               ETA: 9839.7s

################################################################################
                     [1m Learning iteration 150/4000 [0m

                       Computation: 3154 steps/s (collection: 0.508s, learning 2.089s)
               Value function loss: 157.5389
                    Surrogate loss: 0.0079
             Mean action noise std: 0.99
                       Mean reward: 207.95
               Mean episode length: 212.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1236992
                    Iteration time: 2.60s
                        Total time: 385.86s
                               ETA: 9838.2s

################################################################################
                     [1m Learning iteration 151/4000 [0m

                       Computation: 3218 steps/s (collection: 0.489s, learning 2.057s)
               Value function loss: 140.5942
                    Surrogate loss: 0.0079
             Mean action noise std: 0.99
                       Mean reward: 215.87
               Mean episode length: 216.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 2.55s
                        Total time: 388.41s
                               ETA: 9835.4s

################################################################################
                     [1m Learning iteration 152/4000 [0m

                       Computation: 3198 steps/s (collection: 0.480s, learning 2.082s)
               Value function loss: 110.3752
                    Surrogate loss: 0.0081
             Mean action noise std: 0.99
                       Mean reward: 211.16
               Mean episode length: 211.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1253376
                    Iteration time: 2.56s
                        Total time: 390.97s
                               ETA: 9833.0s

################################################################################
                     [1m Learning iteration 153/4000 [0m

                       Computation: 3294 steps/s (collection: 0.454s, learning 2.033s)
               Value function loss: 157.6682
                    Surrogate loss: 0.0099
             Mean action noise std: 0.99
                       Mean reward: 207.94
               Mean episode length: 206.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 2.49s
                        Total time: 393.45s
                               ETA: 9828.7s

################################################################################
                     [1m Learning iteration 154/4000 [0m

                       Computation: 3195 steps/s (collection: 0.464s, learning 2.100s)
               Value function loss: 177.1551
                    Surrogate loss: 0.0092
             Mean action noise std: 0.99
                       Mean reward: 217.99
               Mean episode length: 214.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1269760
                    Iteration time: 2.56s
                        Total time: 396.02s
                               ETA: 9826.4s

################################################################################
                     [1m Learning iteration 155/4000 [0m

                       Computation: 3188 steps/s (collection: 0.497s, learning 2.072s)
               Value function loss: 156.4142
                    Surrogate loss: 0.0094
             Mean action noise std: 0.99
                       Mean reward: 220.20
               Mean episode length: 213.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 2.57s
                        Total time: 398.59s
                               ETA: 9824.2s

################################################################################
                     [1m Learning iteration 156/4000 [0m

                       Computation: 3208 steps/s (collection: 0.505s, learning 2.048s)
               Value function loss: 161.1654
                    Surrogate loss: 0.0090
             Mean action noise std: 0.99
                       Mean reward: 227.84
               Mean episode length: 219.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1286144
                    Iteration time: 2.55s
                        Total time: 401.14s
                               ETA: 9821.6s

################################################################################
                     [1m Learning iteration 157/4000 [0m

                       Computation: 3186 steps/s (collection: 0.456s, learning 2.115s)
               Value function loss: 102.3925
                    Surrogate loss: 0.0078
             Mean action noise std: 0.99
                       Mean reward: 230.00
               Mean episode length: 222.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 2.57s
                        Total time: 403.71s
                               ETA: 9819.4s

################################################################################
                     [1m Learning iteration 158/4000 [0m

                       Computation: 3139 steps/s (collection: 0.496s, learning 2.114s)
               Value function loss: 140.6199
                    Surrogate loss: 0.0083
             Mean action noise std: 0.99
                       Mean reward: 249.40
               Mean episode length: 239.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1302528
                    Iteration time: 2.61s
                        Total time: 406.32s
                               ETA: 9818.1s

################################################################################
                     [1m Learning iteration 159/4000 [0m

                       Computation: 3182 steps/s (collection: 0.531s, learning 2.043s)
               Value function loss: 150.3737
                    Surrogate loss: 0.0119
             Mean action noise std: 0.99
                       Mean reward: 266.15
               Mean episode length: 253.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 2.57s
                        Total time: 408.90s
                               ETA: 9816.0s

################################################################################
                     [1m Learning iteration 160/4000 [0m

                       Computation: 3203 steps/s (collection: 0.484s, learning 2.073s)
               Value function loss: 188.5155
                    Surrogate loss: 0.0096
             Mean action noise std: 0.99
                       Mean reward: 270.21
               Mean episode length: 254.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1318912
                    Iteration time: 2.56s
                        Total time: 411.45s
                               ETA: 9813.5s

################################################################################
                     [1m Learning iteration 161/4000 [0m

                       Computation: 3230 steps/s (collection: 0.490s, learning 2.046s)
               Value function loss: 177.1292
                    Surrogate loss: 0.0084
             Mean action noise std: 0.99
                       Mean reward: 272.09
               Mean episode length: 255.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 2.54s
                        Total time: 413.99s
                               ETA: 9810.5s

################################################################################
                     [1m Learning iteration 162/4000 [0m

                       Computation: 3206 steps/s (collection: 0.499s, learning 2.056s)
               Value function loss: 205.2931
                    Surrogate loss: 0.0100
             Mean action noise std: 0.99
                       Mean reward: 263.44
               Mean episode length: 242.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 1335296
                    Iteration time: 2.56s
                        Total time: 416.54s
                               ETA: 9807.9s

################################################################################
                     [1m Learning iteration 163/4000 [0m

                       Computation: 3228 steps/s (collection: 0.484s, learning 2.053s)
               Value function loss: 161.4729
                    Surrogate loss: 0.0084
             Mean action noise std: 0.99
                       Mean reward: 253.76
               Mean episode length: 231.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 2.54s
                        Total time: 419.08s
                               ETA: 9804.9s

################################################################################
                     [1m Learning iteration 164/4000 [0m

                       Computation: 3291 steps/s (collection: 0.459s, learning 2.029s)
               Value function loss: 204.6956
                    Surrogate loss: 0.0085
             Mean action noise std: 0.99
                       Mean reward: 256.11
               Mean episode length: 229.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1351680
                    Iteration time: 2.49s
                        Total time: 421.57s
                               ETA: 9800.8s

################################################################################
                     [1m Learning iteration 165/4000 [0m

                       Computation: 3262 steps/s (collection: 0.457s, learning 2.053s)
               Value function loss: 162.4357
                    Surrogate loss: 0.0098
             Mean action noise std: 0.99
                       Mean reward: 248.10
               Mean episode length: 221.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 2.51s
                        Total time: 424.08s
                               ETA: 9797.3s

################################################################################
                     [1m Learning iteration 166/4000 [0m

                       Computation: 3162 steps/s (collection: 0.502s, learning 2.089s)
               Value function loss: 226.0599
                    Surrogate loss: 0.0071
             Mean action noise std: 0.99
                       Mean reward: 250.62
               Mean episode length: 223.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1368064
                    Iteration time: 2.59s
                        Total time: 426.67s
                               ETA: 9795.5s

################################################################################
                     [1m Learning iteration 167/4000 [0m

                       Computation: 3241 steps/s (collection: 0.472s, learning 2.055s)
               Value function loss: 150.0663
                    Surrogate loss: 0.0075
             Mean action noise std: 0.99
                       Mean reward: 254.75
               Mean episode length: 229.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 2.53s
                        Total time: 429.20s
                               ETA: 9792.3s

################################################################################
                     [1m Learning iteration 168/4000 [0m

                       Computation: 3273 steps/s (collection: 0.450s, learning 2.052s)
               Value function loss: 111.9442
                    Surrogate loss: 0.0113
             Mean action noise std: 0.99
                       Mean reward: 255.29
               Mean episode length: 230.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1384448
                    Iteration time: 2.50s
                        Total time: 431.70s
                               ETA: 9788.6s

################################################################################
                     [1m Learning iteration 169/4000 [0m

                       Computation: 3272 steps/s (collection: 0.461s, learning 2.042s)
               Value function loss: 204.0996
                    Surrogate loss: 0.0101
             Mean action noise std: 0.99
                       Mean reward: 259.01
               Mean episode length: 235.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 2.50s
                        Total time: 434.20s
                               ETA: 9784.9s

################################################################################
                     [1m Learning iteration 170/4000 [0m

                       Computation: 3227 steps/s (collection: 0.495s, learning 2.044s)
               Value function loss: 147.2322
                    Surrogate loss: 0.0089
             Mean action noise std: 0.99
                       Mean reward: 266.84
               Mean episode length: 243.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1400832
                    Iteration time: 2.54s
                        Total time: 436.74s
                               ETA: 9782.0s

################################################################################
                     [1m Learning iteration 171/4000 [0m

                       Computation: 3216 steps/s (collection: 0.507s, learning 2.041s)
               Value function loss: 248.2210
                    Surrogate loss: 0.0080
             Mean action noise std: 0.99
                       Mean reward: 271.98
               Mean episode length: 245.98
                 Mean success rate: 0.50
                  Mean reward/step: 1.21
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 2.55s
                        Total time: 439.29s
                               ETA: 9779.3s

################################################################################
                     [1m Learning iteration 172/4000 [0m

                       Computation: 3271 steps/s (collection: 0.448s, learning 2.056s)
               Value function loss: 148.9933
                    Surrogate loss: 0.0095
             Mean action noise std: 0.99
                       Mean reward: 278.70
               Mean episode length: 252.01
                 Mean success rate: 0.50
                  Mean reward/step: 1.17
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1417216
                    Iteration time: 2.50s
                        Total time: 441.79s
                               ETA: 9775.6s

################################################################################
                     [1m Learning iteration 173/4000 [0m

                       Computation: 3161 steps/s (collection: 0.535s, learning 2.057s)
               Value function loss: 184.3391
                    Surrogate loss: 0.0104
             Mean action noise std: 0.99
                       Mean reward: 278.09
               Mean episode length: 249.47
                 Mean success rate: 0.50
                  Mean reward/step: 1.14
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 2.59s
                        Total time: 444.38s
                               ETA: 9773.9s

################################################################################
                     [1m Learning iteration 174/4000 [0m

                       Computation: 3225 steps/s (collection: 0.463s, learning 2.078s)
               Value function loss: 199.4036
                    Surrogate loss: 0.0115
             Mean action noise std: 0.99
                       Mean reward: 269.21
               Mean episode length: 241.49
                 Mean success rate: 0.50
                  Mean reward/step: 1.18
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1433600
                    Iteration time: 2.54s
                        Total time: 446.92s
                               ETA: 9771.0s

################################################################################
                     [1m Learning iteration 175/4000 [0m

                       Computation: 3233 steps/s (collection: 0.473s, learning 2.061s)
               Value function loss: 136.1981
                    Surrogate loss: 0.0099
             Mean action noise std: 0.99
                       Mean reward: 277.69
               Mean episode length: 244.70
                 Mean success rate: 0.50
                  Mean reward/step: 1.15
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 2.53s
                        Total time: 449.46s
                               ETA: 9768.0s

################################################################################
                     [1m Learning iteration 176/4000 [0m

                       Computation: 3246 steps/s (collection: 0.475s, learning 2.048s)
               Value function loss: 176.8998
                    Surrogate loss: 0.0100
             Mean action noise std: 0.99
                       Mean reward: 276.57
               Mean episode length: 245.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1449984
                    Iteration time: 2.52s
                        Total time: 451.98s
                               ETA: 9764.8s

################################################################################
                     [1m Learning iteration 177/4000 [0m

                       Computation: 3274 steps/s (collection: 0.473s, learning 2.029s)
               Value function loss: 211.4152
                    Surrogate loss: 0.0095
             Mean action noise std: 0.99
                       Mean reward: 270.63
               Mean episode length: 241.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 2.50s
                        Total time: 454.48s
                               ETA: 9761.2s

################################################################################
                     [1m Learning iteration 178/4000 [0m

                       Computation: 3248 steps/s (collection: 0.450s, learning 2.071s)
               Value function loss: 162.0169
                    Surrogate loss: 0.0103
             Mean action noise std: 0.99
                       Mean reward: 292.44
               Mean episode length: 258.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1466368
                    Iteration time: 2.52s
                        Total time: 457.00s
                               ETA: 9757.9s

################################################################################
                     [1m Learning iteration 179/4000 [0m

                       Computation: 3236 steps/s (collection: 0.484s, learning 2.048s)
               Value function loss: 180.3107
                    Surrogate loss: 0.0109
             Mean action noise std: 0.99
                       Mean reward: 292.43
               Mean episode length: 260.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 2.53s
                        Total time: 459.54s
                               ETA: 9754.9s

################################################################################
                     [1m Learning iteration 180/4000 [0m

                       Computation: 3284 steps/s (collection: 0.443s, learning 2.052s)
               Value function loss: 167.0264
                    Surrogate loss: 0.0121
             Mean action noise std: 0.99
                       Mean reward: 290.41
               Mean episode length: 255.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1482752
                    Iteration time: 2.49s
                        Total time: 462.03s
                               ETA: 9751.1s

################################################################################
                     [1m Learning iteration 181/4000 [0m

                       Computation: 3268 steps/s (collection: 0.458s, learning 2.048s)
               Value function loss: 134.7999
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 284.29
               Mean episode length: 251.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 2.51s
                        Total time: 464.54s
                               ETA: 9747.6s

################################################################################
                     [1m Learning iteration 182/4000 [0m

                       Computation: 3195 steps/s (collection: 0.507s, learning 2.057s)
               Value function loss: 228.8773
                    Surrogate loss: 0.0112
             Mean action noise std: 0.99
                       Mean reward: 280.62
               Mean episode length: 245.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 1499136
                    Iteration time: 2.56s
                        Total time: 467.10s
                               ETA: 9745.3s

################################################################################
                     [1m Learning iteration 183/4000 [0m

                       Computation: 3270 steps/s (collection: 0.478s, learning 2.027s)
               Value function loss: 158.1146
                    Surrogate loss: 0.0094
             Mean action noise std: 0.99
                       Mean reward: 274.80
               Mean episode length: 240.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 2.51s
                        Total time: 469.61s
                               ETA: 9741.8s

################################################################################
                     [1m Learning iteration 184/4000 [0m

                       Computation: 3302 steps/s (collection: 0.453s, learning 2.028s)
               Value function loss: 147.4752
                    Surrogate loss: 0.0120
             Mean action noise std: 0.99
                       Mean reward: 268.80
               Mean episode length: 232.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1515520
                    Iteration time: 2.48s
                        Total time: 472.09s
                               ETA: 9737.7s

################################################################################
                     [1m Learning iteration 185/4000 [0m

                       Computation: 3155 steps/s (collection: 0.472s, learning 2.124s)
               Value function loss: 152.3351
                    Surrogate loss: 0.0133
             Mean action noise std: 0.99
                       Mean reward: 268.48
               Mean episode length: 230.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 2.60s
                        Total time: 474.68s
                               ETA: 9736.1s

################################################################################
                     [1m Learning iteration 186/4000 [0m

                       Computation: 3277 steps/s (collection: 0.434s, learning 2.065s)
               Value function loss: 363.9521
                    Surrogate loss: 0.0098
             Mean action noise std: 0.99
                       Mean reward: 271.49
               Mean episode length: 231.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1531904
                    Iteration time: 2.50s
                        Total time: 477.18s
                               ETA: 9732.4s

################################################################################
                     [1m Learning iteration 187/4000 [0m

                       Computation: 3250 steps/s (collection: 0.458s, learning 2.062s)
               Value function loss: 194.5697
                    Surrogate loss: 0.0115
             Mean action noise std: 0.99
                       Mean reward: 271.40
               Mean episode length: 228.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 2.52s
                        Total time: 479.70s
                               ETA: 9729.3s

################################################################################
                     [1m Learning iteration 188/4000 [0m

                       Computation: 3203 steps/s (collection: 0.481s, learning 2.076s)
               Value function loss: 161.9164
                    Surrogate loss: 0.0117
             Mean action noise std: 0.99
                       Mean reward: 268.94
               Mean episode length: 220.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1548288
                    Iteration time: 2.56s
                        Total time: 482.26s
                               ETA: 9726.8s

################################################################################
                     [1m Learning iteration 189/4000 [0m

                       Computation: 3227 steps/s (collection: 0.463s, learning 2.075s)
               Value function loss: 265.1012
                    Surrogate loss: 0.0097
             Mean action noise std: 0.99
                       Mean reward: 264.34
               Mean episode length: 214.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 2.54s
                        Total time: 484.80s
                               ETA: 9724.0s

################################################################################
                     [1m Learning iteration 190/4000 [0m

                       Computation: 3185 steps/s (collection: 0.480s, learning 2.091s)
               Value function loss: 182.8798
                    Surrogate loss: 0.0131
             Mean action noise std: 0.99
                       Mean reward: 284.18
               Mean episode length: 232.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1564672
                    Iteration time: 2.57s
                        Total time: 487.37s
                               ETA: 9721.8s

################################################################################
                     [1m Learning iteration 191/4000 [0m

                       Computation: 3223 steps/s (collection: 0.481s, learning 2.060s)
               Value function loss: 174.3140
                    Surrogate loss: 0.0129
             Mean action noise std: 0.99
                       Mean reward: 287.47
               Mean episode length: 236.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 2.54s
                        Total time: 489.91s
                               ETA: 9719.1s

################################################################################
                     [1m Learning iteration 192/4000 [0m

                       Computation: 3180 steps/s (collection: 0.484s, learning 2.092s)
               Value function loss: 175.1024
                    Surrogate loss: 0.0138
             Mean action noise std: 0.99
                       Mean reward: 301.81
               Mean episode length: 247.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1581056
                    Iteration time: 2.58s
                        Total time: 492.49s
                               ETA: 9717.0s

################################################################################
                     [1m Learning iteration 193/4000 [0m

                       Computation: 3157 steps/s (collection: 0.478s, learning 2.116s)
               Value function loss: 216.0570
                    Surrogate loss: 0.0119
             Mean action noise std: 0.99
                       Mean reward: 304.55
               Mean episode length: 249.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 2.59s
                        Total time: 495.08s
                               ETA: 9715.3s

################################################################################
                     [1m Learning iteration 194/4000 [0m

                       Computation: 3228 steps/s (collection: 0.465s, learning 2.072s)
               Value function loss: 252.5231
                    Surrogate loss: 0.0125
             Mean action noise std: 0.99
                       Mean reward: 310.33
               Mean episode length: 257.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1597440
                    Iteration time: 2.54s
                        Total time: 497.62s
                               ETA: 9712.5s

################################################################################
                     [1m Learning iteration 195/4000 [0m

                       Computation: 3234 steps/s (collection: 0.491s, learning 2.042s)
               Value function loss: 134.1623
                    Surrogate loss: 0.0130
             Mean action noise std: 0.99
                       Mean reward: 337.11
               Mean episode length: 271.98
                 Mean success rate: 0.50
                  Mean reward/step: 1.21
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 2.53s
                        Total time: 500.15s
                               ETA: 9709.5s

################################################################################
                     [1m Learning iteration 196/4000 [0m

                       Computation: 3137 steps/s (collection: 0.479s, learning 2.132s)
               Value function loss: 150.0221
                    Surrogate loss: 0.0139
             Mean action noise std: 0.99
                       Mean reward: 341.12
               Mean episode length: 277.51
                 Mean success rate: 0.50
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1613824
                    Iteration time: 2.61s
                        Total time: 502.76s
                               ETA: 9708.1s

################################################################################
                     [1m Learning iteration 197/4000 [0m

                       Computation: 3078 steps/s (collection: 0.512s, learning 2.149s)
               Value function loss: 162.9938
                    Surrogate loss: 0.0125
             Mean action noise std: 0.99
                       Mean reward: 336.36
               Mean episode length: 269.67
                 Mean success rate: 0.50
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 2.66s
                        Total time: 505.42s
                               ETA: 9707.7s

################################################################################
                     [1m Learning iteration 198/4000 [0m

                       Computation: 3141 steps/s (collection: 0.494s, learning 2.114s)
               Value function loss: 174.6914
                    Surrogate loss: 0.0150
             Mean action noise std: 0.99
                       Mean reward: 339.89
               Mean episode length: 271.81
                 Mean success rate: 0.50
                  Mean reward/step: 1.30
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1630208
                    Iteration time: 2.61s
                        Total time: 508.03s
                               ETA: 9706.2s

################################################################################
                     [1m Learning iteration 199/4000 [0m

                       Computation: 3120 steps/s (collection: 0.515s, learning 2.109s)
               Value function loss: 150.2175
                    Surrogate loss: 0.0122
             Mean action noise std: 0.99
                       Mean reward: 335.42
               Mean episode length: 266.21
                 Mean success rate: 0.50
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 2.62s
                        Total time: 510.65s
                               ETA: 9705.0s

################################################################################
                     [1m Learning iteration 200/4000 [0m

                       Computation: 3076 steps/s (collection: 0.521s, learning 2.142s)
               Value function loss: 168.6770
                    Surrogate loss: 0.0132
             Mean action noise std: 0.99
                       Mean reward: 335.90
               Mean episode length: 264.40
                 Mean success rate: 0.50
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1646592
                    Iteration time: 2.66s
                        Total time: 513.32s
                               ETA: 9704.5s

################################################################################
                     [1m Learning iteration 201/4000 [0m

                       Computation: 3132 steps/s (collection: 0.501s, learning 2.115s)
               Value function loss: 267.7704
                    Surrogate loss: 0.0111
             Mean action noise std: 0.99
                       Mean reward: 315.08
               Mean episode length: 251.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 2.62s
                        Total time: 515.93s
                               ETA: 9703.1s

################################################################################
                     [1m Learning iteration 202/4000 [0m

                       Computation: 3175 steps/s (collection: 0.497s, learning 2.083s)
               Value function loss: 161.8964
                    Surrogate loss: 0.0113
             Mean action noise std: 0.99
                       Mean reward: 303.22
               Mean episode length: 239.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1662976
                    Iteration time: 2.58s
                        Total time: 518.51s
                               ETA: 9701.0s

################################################################################
                     [1m Learning iteration 203/4000 [0m

                       Computation: 3098 steps/s (collection: 0.514s, learning 2.130s)
               Value function loss: 156.6388
                    Surrogate loss: 0.0135
             Mean action noise std: 0.99
                       Mean reward: 287.23
               Mean episode length: 221.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 2.64s
                        Total time: 521.16s
                               ETA: 9700.2s

################################################################################
                     [1m Learning iteration 204/4000 [0m

                       Computation: 3127 steps/s (collection: 0.495s, learning 2.124s)
               Value function loss: 178.9474
                    Surrogate loss: 0.0118
             Mean action noise std: 0.99
                       Mean reward: 290.27
               Mean episode length: 223.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1679360
                    Iteration time: 2.62s
                        Total time: 523.78s
                               ETA: 9698.8s

################################################################################
                     [1m Learning iteration 205/4000 [0m

                       Computation: 3069 steps/s (collection: 0.528s, learning 2.141s)
               Value function loss: 228.1106
                    Surrogate loss: 0.0147
             Mean action noise std: 0.99
                       Mean reward: 286.66
               Mean episode length: 218.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 2.67s
                        Total time: 526.44s
                               ETA: 9698.3s

################################################################################
                     [1m Learning iteration 206/4000 [0m

                       Computation: 3120 steps/s (collection: 0.563s, learning 2.063s)
               Value function loss: 154.7094
                    Surrogate loss: 0.0143
             Mean action noise std: 0.99
                       Mean reward: 291.49
               Mean episode length: 221.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1695744
                    Iteration time: 2.63s
                        Total time: 529.07s
                               ETA: 9697.1s

################################################################################
                     [1m Learning iteration 207/4000 [0m

                       Computation: 3154 steps/s (collection: 0.501s, learning 2.095s)
               Value function loss: 210.8901
                    Surrogate loss: 0.0124
             Mean action noise std: 0.99
                       Mean reward: 286.38
               Mean episode length: 220.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 2.60s
                        Total time: 531.67s
                               ETA: 9695.3s

################################################################################
                     [1m Learning iteration 208/4000 [0m

                       Computation: 3220 steps/s (collection: 0.483s, learning 2.061s)
               Value function loss: 136.8456
                    Surrogate loss: 0.0139
             Mean action noise std: 0.99
                       Mean reward: 274.53
               Mean episode length: 212.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1712128
                    Iteration time: 2.54s
                        Total time: 534.21s
                               ETA: 9692.5s

################################################################################
                     [1m Learning iteration 209/4000 [0m

                       Computation: 3199 steps/s (collection: 0.477s, learning 2.083s)
               Value function loss: 206.1818
                    Surrogate loss: 0.0132
             Mean action noise std: 0.99
                       Mean reward: 254.59
               Mean episode length: 198.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 2.56s
                        Total time: 536.77s
                               ETA: 9690.0s

################################################################################
                     [1m Learning iteration 210/4000 [0m

                       Computation: 3247 steps/s (collection: 0.481s, learning 2.042s)
               Value function loss: 206.6168
                    Surrogate loss: 0.0122
             Mean action noise std: 0.99
                       Mean reward: 246.75
               Mean episode length: 193.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 1728512
                    Iteration time: 2.52s
                        Total time: 539.29s
                               ETA: 9686.8s

################################################################################
                     [1m Learning iteration 211/4000 [0m

                       Computation: 3234 steps/s (collection: 0.505s, learning 2.028s)
               Value function loss: 209.0342
                    Surrogate loss: 0.0122
             Mean action noise std: 0.99
                       Mean reward: 242.94
               Mean episode length: 187.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 2.53s
                        Total time: 541.83s
                               ETA: 9683.9s

################################################################################
                     [1m Learning iteration 212/4000 [0m

                       Computation: 3231 steps/s (collection: 0.483s, learning 2.052s)
               Value function loss: 152.1325
                    Surrogate loss: 0.0138
             Mean action noise std: 0.99
                       Mean reward: 250.33
               Mean episode length: 194.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1744896
                    Iteration time: 2.54s
                        Total time: 544.36s
                               ETA: 9681.0s

################################################################################
                     [1m Learning iteration 213/4000 [0m

                       Computation: 3258 steps/s (collection: 0.469s, learning 2.045s)
               Value function loss: 180.2468
                    Surrogate loss: 0.0129
             Mean action noise std: 0.99
                       Mean reward: 256.87
               Mean episode length: 197.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 2.51s
                        Total time: 546.88s
                               ETA: 9677.7s

################################################################################
                     [1m Learning iteration 214/4000 [0m

                       Computation: 3167 steps/s (collection: 0.521s, learning 2.065s)
               Value function loss: 220.7702
                    Surrogate loss: 0.0093
             Mean action noise std: 0.99
                       Mean reward: 261.22
               Mean episode length: 202.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 1761280
                    Iteration time: 2.59s
                        Total time: 549.46s
                               ETA: 9675.6s

################################################################################
                     [1m Learning iteration 215/4000 [0m

                       Computation: 3267 steps/s (collection: 0.468s, learning 2.039s)
               Value function loss: 154.1484
                    Surrogate loss: 0.0112
             Mean action noise std: 0.99
                       Mean reward: 261.01
               Mean episode length: 200.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 2.51s
                        Total time: 551.97s
                               ETA: 9672.2s

################################################################################
                     [1m Learning iteration 216/4000 [0m

                       Computation: 3206 steps/s (collection: 0.507s, learning 2.048s)
               Value function loss: 192.2891
                    Surrogate loss: 0.0133
             Mean action noise std: 0.99
                       Mean reward: 257.27
               Mean episode length: 196.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 1777664
                    Iteration time: 2.56s
                        Total time: 554.52s
                               ETA: 9669.7s

################################################################################
                     [1m Learning iteration 217/4000 [0m

                       Computation: 3194 steps/s (collection: 0.482s, learning 2.083s)
               Value function loss: 173.9207
                    Surrogate loss: 0.0128
             Mean action noise std: 0.99
                       Mean reward: 263.21
               Mean episode length: 198.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 2.56s
                        Total time: 557.09s
                               ETA: 9667.3s

################################################################################
                     [1m Learning iteration 218/4000 [0m

                       Computation: 3209 steps/s (collection: 0.499s, learning 2.053s)
               Value function loss: 137.1663
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 260.65
               Mean episode length: 195.94
                 Mean success rate: 0.50
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1794048
                    Iteration time: 2.55s
                        Total time: 559.64s
                               ETA: 9664.7s

################################################################################
                     [1m Learning iteration 219/4000 [0m

                       Computation: 3251 steps/s (collection: 0.489s, learning 2.031s)
               Value function loss: 238.9026
                    Surrogate loss: 0.0123
             Mean action noise std: 0.99
                       Mean reward: 254.42
               Mean episode length: 189.10
                 Mean success rate: 0.50
                  Mean reward/step: 1.27
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 2.52s
                        Total time: 562.16s
                               ETA: 9661.5s

################################################################################
                     [1m Learning iteration 220/4000 [0m

                       Computation: 3072 steps/s (collection: 0.557s, learning 2.109s)
               Value function loss: 223.4665
                    Surrogate loss: 0.0144
             Mean action noise std: 0.99
                       Mean reward: 258.75
               Mean episode length: 195.34
                 Mean success rate: 0.50
                  Mean reward/step: 1.37
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 1810432
                    Iteration time: 2.67s
                        Total time: 564.83s
                               ETA: 9660.8s

################################################################################
                     [1m Learning iteration 221/4000 [0m

                       Computation: 3207 steps/s (collection: 0.486s, learning 2.068s)
               Value function loss: 247.6666
                    Surrogate loss: 0.0127
             Mean action noise std: 0.99
                       Mean reward: 250.82
               Mean episode length: 187.94
                 Mean success rate: 0.50
                  Mean reward/step: 1.34
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 2.55s
                        Total time: 567.38s
                               ETA: 9658.3s

################################################################################
                     [1m Learning iteration 222/4000 [0m

                       Computation: 3132 steps/s (collection: 0.496s, learning 2.119s)
               Value function loss: 173.7425
                    Surrogate loss: 0.0152
             Mean action noise std: 0.99
                       Mean reward: 248.62
               Mean episode length: 185.95
                 Mean success rate: 0.50
                  Mean reward/step: 1.35
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1826816
                    Iteration time: 2.62s
                        Total time: 570.00s
                               ETA: 9656.7s

################################################################################
                     [1m Learning iteration 223/4000 [0m

                       Computation: 3074 steps/s (collection: 0.512s, learning 2.153s)
               Value function loss: 199.0112
                    Surrogate loss: 0.0133
             Mean action noise std: 0.99
                       Mean reward: 253.14
               Mean episode length: 190.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 2.66s
                        Total time: 572.66s
                               ETA: 9656.0s

################################################################################
                     [1m Learning iteration 224/4000 [0m

                       Computation: 3084 steps/s (collection: 0.559s, learning 2.097s)
               Value function loss: 227.4314
                    Surrogate loss: 0.0109
             Mean action noise std: 0.99
                       Mean reward: 258.39
               Mean episode length: 196.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1843200
                    Iteration time: 2.66s
                        Total time: 575.32s
                               ETA: 9655.1s

################################################################################
                     [1m Learning iteration 225/4000 [0m

                       Computation: 3195 steps/s (collection: 0.488s, learning 2.075s)
               Value function loss: 200.2462
                    Surrogate loss: 0.0120
             Mean action noise std: 0.99
                       Mean reward: 268.26
               Mean episode length: 199.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 2.56s
                        Total time: 577.88s
                               ETA: 9652.6s

################################################################################
                     [1m Learning iteration 226/4000 [0m

                       Computation: 3143 steps/s (collection: 0.529s, learning 2.077s)
               Value function loss: 199.4543
                    Surrogate loss: 0.0099
             Mean action noise std: 0.99
                       Mean reward: 269.11
               Mean episode length: 204.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 1859584
                    Iteration time: 2.61s
                        Total time: 580.49s
                               ETA: 9650.9s

################################################################################
                     [1m Learning iteration 227/4000 [0m

                       Computation: 3088 steps/s (collection: 0.540s, learning 2.112s)
               Value function loss: 191.3403
                    Surrogate loss: 0.0136
             Mean action noise std: 0.99
                       Mean reward: 289.84
               Mean episode length: 219.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 2.65s
                        Total time: 583.14s
                               ETA: 9649.9s

################################################################################
                     [1m Learning iteration 228/4000 [0m

                       Computation: 3059 steps/s (collection: 0.562s, learning 2.116s)
               Value function loss: 148.1863
                    Surrogate loss: 0.0122
             Mean action noise std: 0.99
                       Mean reward: 291.79
               Mean episode length: 223.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1875968
                    Iteration time: 2.68s
                        Total time: 585.82s
                               ETA: 9649.3s

################################################################################
                     [1m Learning iteration 229/4000 [0m

                       Computation: 3194 steps/s (collection: 0.491s, learning 2.073s)
               Value function loss: 209.8660
                    Surrogate loss: 0.0159
             Mean action noise std: 0.99
                       Mean reward: 306.51
               Mean episode length: 228.88
                 Mean success rate: 1.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 2.56s
                        Total time: 588.38s
                               ETA: 9646.9s

################################################################################
                     [1m Learning iteration 230/4000 [0m

                       Computation: 3179 steps/s (collection: 0.487s, learning 2.089s)
               Value function loss: 190.8790
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 324.48
               Mean episode length: 242.62
                 Mean success rate: 1.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1892352
                    Iteration time: 2.58s
                        Total time: 590.96s
                               ETA: 9644.6s

################################################################################
                     [1m Learning iteration 231/4000 [0m

                       Computation: 3080 steps/s (collection: 0.573s, learning 2.086s)
               Value function loss: 206.7712
                    Surrogate loss: 0.0106
             Mean action noise std: 0.99
                       Mean reward: 298.61
               Mean episode length: 221.68
                 Mean success rate: 1.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 2.66s
                        Total time: 593.62s
                               ETA: 9643.7s

################################################################################
                     [1m Learning iteration 232/4000 [0m

                       Computation: 3022 steps/s (collection: 0.529s, learning 2.182s)
               Value function loss: 239.3108
                    Surrogate loss: 0.0118
             Mean action noise std: 0.99
                       Mean reward: 279.36
               Mean episode length: 209.42
                 Mean success rate: 1.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1908736
                    Iteration time: 2.71s
                        Total time: 596.33s
                               ETA: 9643.6s

################################################################################
                     [1m Learning iteration 233/4000 [0m

                       Computation: 3156 steps/s (collection: 0.501s, learning 2.094s)
               Value function loss: 163.7763
                    Surrogate loss: 0.0141
             Mean action noise std: 0.99
                       Mean reward: 286.87
               Mean episode length: 211.13
                 Mean success rate: 1.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 2.59s
                        Total time: 598.92s
                               ETA: 9641.6s

################################################################################
                     [1m Learning iteration 234/4000 [0m

                       Computation: 3176 steps/s (collection: 0.489s, learning 2.090s)
               Value function loss: 198.8576
                    Surrogate loss: 0.0130
             Mean action noise std: 0.99
                       Mean reward: 250.59
               Mean episode length: 189.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 1925120
                    Iteration time: 2.58s
                        Total time: 601.50s
                               ETA: 9639.4s

################################################################################
                     [1m Learning iteration 235/4000 [0m

                       Computation: 3108 steps/s (collection: 0.520s, learning 2.116s)
               Value function loss: 175.9754
                    Surrogate loss: 0.0143
             Mean action noise std: 0.99
                       Mean reward: 272.98
               Mean episode length: 202.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 2.64s
                        Total time: 604.14s
                               ETA: 9638.0s

################################################################################
                     [1m Learning iteration 236/4000 [0m

                       Computation: 2968 steps/s (collection: 0.590s, learning 2.170s)
               Value function loss: 142.2917
                    Surrogate loss: 0.0149
             Mean action noise std: 0.99
                       Mean reward: 290.41
               Mean episode length: 214.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 1941504
                    Iteration time: 2.76s
                        Total time: 606.90s
                               ETA: 9638.6s

################################################################################
                     [1m Learning iteration 237/4000 [0m

                       Computation: 3177 steps/s (collection: 0.556s, learning 2.022s)
               Value function loss: 186.7638
                    Surrogate loss: 0.0113
             Mean action noise std: 0.99
                       Mean reward: 289.82
               Mean episode length: 212.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 2.58s
                        Total time: 609.47s
                               ETA: 9636.3s

################################################################################
                     [1m Learning iteration 238/4000 [0m

                       Computation: 3147 steps/s (collection: 0.488s, learning 2.115s)
               Value function loss: 131.7688
                    Surrogate loss: 0.0152
             Mean action noise std: 0.99
                       Mean reward: 292.10
               Mean episode length: 214.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1957888
                    Iteration time: 2.60s
                        Total time: 612.08s
                               ETA: 9634.4s

################################################################################
                     [1m Learning iteration 239/4000 [0m

                       Computation: 3022 steps/s (collection: 0.576s, learning 2.134s)
               Value function loss: 155.7131
                    Surrogate loss: 0.0131
             Mean action noise std: 0.99
                       Mean reward: 290.15
               Mean episode length: 214.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 2.71s
                        Total time: 614.79s
                               ETA: 9634.2s

################################################################################
                     [1m Learning iteration 240/4000 [0m

                       Computation: 3202 steps/s (collection: 0.480s, learning 2.078s)
               Value function loss: 150.1285
                    Surrogate loss: 0.0138
             Mean action noise std: 0.99
                       Mean reward: 303.32
               Mean episode length: 222.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1974272
                    Iteration time: 2.56s
                        Total time: 617.34s
                               ETA: 9631.6s

################################################################################
                     [1m Learning iteration 241/4000 [0m

                       Computation: 3198 steps/s (collection: 0.500s, learning 2.062s)
               Value function loss: 161.0828
                    Surrogate loss: 0.0143
             Mean action noise std: 0.99
                       Mean reward: 309.10
               Mean episode length: 227.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 2.56s
                        Total time: 619.91s
                               ETA: 9629.0s

################################################################################
                     [1m Learning iteration 242/4000 [0m

                       Computation: 3171 steps/s (collection: 0.502s, learning 2.081s)
               Value function loss: 202.1962
                    Surrogate loss: 0.0159
             Mean action noise std: 0.99
                       Mean reward: 296.16
               Mean episode length: 220.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1990656
                    Iteration time: 2.58s
                        Total time: 622.49s
                               ETA: 9626.8s

################################################################################
                     [1m Learning iteration 243/4000 [0m

                       Computation: 3168 steps/s (collection: 0.473s, learning 2.113s)
               Value function loss: 137.8430
                    Surrogate loss: 0.0138
             Mean action noise std: 0.99
                       Mean reward: 314.20
               Mean episode length: 232.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 2.59s
                        Total time: 625.07s
                               ETA: 9624.6s

################################################################################
                     [1m Learning iteration 244/4000 [0m

                       Computation: 3116 steps/s (collection: 0.468s, learning 2.160s)
               Value function loss: 250.1091
                    Surrogate loss: 0.0136
             Mean action noise std: 0.99
                       Mean reward: 313.75
               Mean episode length: 231.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2007040
                    Iteration time: 2.63s
                        Total time: 627.70s
                               ETA: 9623.1s

################################################################################
                     [1m Learning iteration 245/4000 [0m

                       Computation: 3193 steps/s (collection: 0.468s, learning 2.097s)
               Value function loss: 195.1733
                    Surrogate loss: 0.0113
             Mean action noise std: 0.99
                       Mean reward: 298.55
               Mean episode length: 219.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 2.57s
                        Total time: 630.27s
                               ETA: 9620.6s

################################################################################
                     [1m Learning iteration 246/4000 [0m

                       Computation: 3171 steps/s (collection: 0.481s, learning 2.102s)
               Value function loss: 145.4470
                    Surrogate loss: 0.0136
             Mean action noise std: 0.99
                       Mean reward: 295.57
               Mean episode length: 219.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2023424
                    Iteration time: 2.58s
                        Total time: 632.85s
                               ETA: 9618.3s

################################################################################
                     [1m Learning iteration 247/4000 [0m

                       Computation: 3171 steps/s (collection: 0.518s, learning 2.066s)
               Value function loss: 168.6562
                    Surrogate loss: 0.0143
             Mean action noise std: 0.99
                       Mean reward: 298.19
               Mean episode length: 222.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 2.58s
                        Total time: 635.43s
                               ETA: 9616.1s

################################################################################
                     [1m Learning iteration 248/4000 [0m

                       Computation: 3212 steps/s (collection: 0.457s, learning 2.093s)
               Value function loss: 234.9014
                    Surrogate loss: 0.0125
             Mean action noise std: 0.99
                       Mean reward: 320.51
               Mean episode length: 239.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2039808
                    Iteration time: 2.55s
                        Total time: 637.98s
                               ETA: 9613.3s

################################################################################
                     [1m Learning iteration 249/4000 [0m

                       Computation: 3167 steps/s (collection: 0.509s, learning 2.077s)
               Value function loss: 214.8483
                    Surrogate loss: 0.0135
             Mean action noise std: 0.99
                       Mean reward: 325.82
               Mean episode length: 242.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 2.59s
                        Total time: 640.57s
                               ETA: 9611.1s

################################################################################
                     [1m Learning iteration 250/4000 [0m

                       Computation: 3132 steps/s (collection: 0.483s, learning 2.132s)
               Value function loss: 191.6213
                    Surrogate loss: 0.0119
             Mean action noise std: 0.99
                       Mean reward: 334.74
               Mean episode length: 248.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2056192
                    Iteration time: 2.62s
                        Total time: 643.19s
                               ETA: 9609.3s

################################################################################
                     [1m Learning iteration 251/4000 [0m

                       Computation: 3156 steps/s (collection: 0.484s, learning 2.111s)
               Value function loss: 139.0854
                    Surrogate loss: 0.0119
             Mean action noise std: 0.99
                       Mean reward: 343.47
               Mean episode length: 259.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 2.60s
                        Total time: 645.78s
                               ETA: 9607.3s

################################################################################
                     [1m Learning iteration 252/4000 [0m

                       Computation: 3137 steps/s (collection: 0.514s, learning 2.097s)
               Value function loss: 177.2716
                    Surrogate loss: 0.0131
             Mean action noise std: 0.99
                       Mean reward: 344.23
               Mean episode length: 257.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2072576
                    Iteration time: 2.61s
                        Total time: 648.39s
                               ETA: 9605.4s

################################################################################
                     [1m Learning iteration 253/4000 [0m

                       Computation: 3239 steps/s (collection: 0.462s, learning 2.067s)
               Value function loss: 204.8236
                    Surrogate loss: 0.0129
             Mean action noise std: 0.99
                       Mean reward: 350.95
               Mean episode length: 257.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 2.53s
                        Total time: 650.92s
                               ETA: 9602.4s

################################################################################
                     [1m Learning iteration 254/4000 [0m

                       Computation: 3180 steps/s (collection: 0.479s, learning 2.097s)
               Value function loss: 301.6026
                    Surrogate loss: 0.0096
             Mean action noise std: 0.99
                       Mean reward: 336.39
               Mean episode length: 244.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 2088960
                    Iteration time: 2.58s
                        Total time: 653.50s
                               ETA: 9600.0s

################################################################################
                     [1m Learning iteration 255/4000 [0m

                       Computation: 3130 steps/s (collection: 0.491s, learning 2.125s)
               Value function loss: 184.9833
                    Surrogate loss: 0.0131
             Mean action noise std: 0.99
                       Mean reward: 328.49
               Mean episode length: 240.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 2.62s
                        Total time: 656.11s
                               ETA: 9598.2s

################################################################################
                     [1m Learning iteration 256/4000 [0m

                       Computation: 3196 steps/s (collection: 0.495s, learning 2.068s)
               Value function loss: 251.3478
                    Surrogate loss: 0.0124
             Mean action noise std: 0.99
                       Mean reward: 324.75
               Mean episode length: 234.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2105344
                    Iteration time: 2.56s
                        Total time: 658.68s
                               ETA: 9595.6s

################################################################################
                     [1m Learning iteration 257/4000 [0m

                       Computation: 3151 steps/s (collection: 0.511s, learning 2.088s)
               Value function loss: 286.3526
                    Surrogate loss: 0.0075
             Mean action noise std: 0.99
                       Mean reward: 322.73
               Mean episode length: 231.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 2.60s
                        Total time: 661.27s
                               ETA: 9593.6s

################################################################################
                     [1m Learning iteration 258/4000 [0m

                       Computation: 3151 steps/s (collection: 0.504s, learning 2.095s)
               Value function loss: 197.4606
                    Surrogate loss: 0.0120
             Mean action noise std: 0.99
                       Mean reward: 320.62
               Mean episode length: 230.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2121728
                    Iteration time: 2.60s
                        Total time: 663.87s
                               ETA: 9591.6s

################################################################################
                     [1m Learning iteration 259/4000 [0m

                       Computation: 3155 steps/s (collection: 0.491s, learning 2.105s)
               Value function loss: 199.0267
                    Surrogate loss: 0.0128
             Mean action noise std: 0.99
                       Mean reward: 315.37
               Mean episode length: 224.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 2.60s
                        Total time: 666.47s
                               ETA: 9589.5s

################################################################################
                     [1m Learning iteration 260/4000 [0m

                       Computation: 3258 steps/s (collection: 0.455s, learning 2.059s)
               Value function loss: 190.3069
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 319.21
               Mean episode length: 225.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2138112
                    Iteration time: 2.51s
                        Total time: 668.98s
                               ETA: 9586.2s

################################################################################
                     [1m Learning iteration 261/4000 [0m

                       Computation: 3214 steps/s (collection: 0.497s, learning 2.051s)
               Value function loss: 265.0185
                    Surrogate loss: 0.0130
             Mean action noise std: 0.99
                       Mean reward: 306.88
               Mean episode length: 216.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 2.55s
                        Total time: 671.53s
                               ETA: 9583.4s

################################################################################
                     [1m Learning iteration 262/4000 [0m

                       Computation: 3180 steps/s (collection: 0.506s, learning 2.069s)
               Value function loss: 166.7433
                    Surrogate loss: 0.0120
             Mean action noise std: 0.99
                       Mean reward: 298.09
               Mean episode length: 210.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2154496
                    Iteration time: 2.58s
                        Total time: 674.11s
                               ETA: 9581.0s

################################################################################
                     [1m Learning iteration 263/4000 [0m

                       Computation: 3141 steps/s (collection: 0.473s, learning 2.135s)
               Value function loss: 208.6999
                    Surrogate loss: 0.0127
             Mean action noise std: 0.99
                       Mean reward: 309.83
               Mean episode length: 217.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 2.61s
                        Total time: 676.72s
                               ETA: 9579.1s

################################################################################
                     [1m Learning iteration 264/4000 [0m

                       Computation: 3203 steps/s (collection: 0.503s, learning 2.054s)
               Value function loss: 218.0140
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 297.13
               Mean episode length: 210.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2170880
                    Iteration time: 2.56s
                        Total time: 679.27s
                               ETA: 9576.5s

################################################################################
                     [1m Learning iteration 265/4000 [0m

                       Computation: 3200 steps/s (collection: 0.486s, learning 2.073s)
               Value function loss: 226.8498
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 312.81
               Mean episode length: 223.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 2.56s
                        Total time: 681.83s
                               ETA: 9573.8s

################################################################################
                     [1m Learning iteration 266/4000 [0m

                       Computation: 3210 steps/s (collection: 0.491s, learning 2.060s)
               Value function loss: 151.8354
                    Surrogate loss: 0.0134
             Mean action noise std: 0.99
                       Mean reward: 309.68
               Mean episode length: 222.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2187264
                    Iteration time: 2.55s
                        Total time: 684.38s
                               ETA: 9571.1s

################################################################################
                     [1m Learning iteration 267/4000 [0m

                       Computation: 3227 steps/s (collection: 0.484s, learning 2.054s)
               Value function loss: 184.6862
                    Surrogate loss: 0.0157
             Mean action noise std: 0.99
                       Mean reward: 309.05
               Mean episode length: 220.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 2.54s
                        Total time: 686.92s
                               ETA: 9568.2s

################################################################################
                     [1m Learning iteration 268/4000 [0m

                       Computation: 3180 steps/s (collection: 0.498s, learning 2.077s)
               Value function loss: 175.8435
                    Surrogate loss: 0.0135
             Mean action noise std: 0.99
                       Mean reward: 316.80
               Mean episode length: 225.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2203648
                    Iteration time: 2.58s
                        Total time: 689.50s
                               ETA: 9565.8s

################################################################################
                     [1m Learning iteration 269/4000 [0m

                       Computation: 3192 steps/s (collection: 0.496s, learning 2.070s)
               Value function loss: 174.8914
                    Surrogate loss: 0.0144
             Mean action noise std: 0.99
                       Mean reward: 310.76
               Mean episode length: 224.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 2.57s
                        Total time: 692.06s
                               ETA: 9563.3s

################################################################################
                     [1m Learning iteration 270/4000 [0m

                       Computation: 3226 steps/s (collection: 0.490s, learning 2.050s)
               Value function loss: 232.5232
                    Surrogate loss: 0.0147
             Mean action noise std: 0.99
                       Mean reward: 333.46
               Mean episode length: 237.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2220032
                    Iteration time: 2.54s
                        Total time: 694.60s
                               ETA: 9560.4s

################################################################################
                     [1m Learning iteration 271/4000 [0m

                       Computation: 3239 steps/s (collection: 0.463s, learning 2.067s)
               Value function loss: 253.4609
                    Surrogate loss: 0.0149
             Mean action noise std: 0.99
                       Mean reward: 337.33
               Mean episode length: 238.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 2.53s
                        Total time: 697.13s
                               ETA: 9557.4s

################################################################################
                     [1m Learning iteration 272/4000 [0m

                       Computation: 3128 steps/s (collection: 0.535s, learning 2.084s)
               Value function loss: 205.3164
                    Surrogate loss: 0.0127
             Mean action noise std: 0.99
                       Mean reward: 329.79
               Mean episode length: 234.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2236416
                    Iteration time: 2.62s
                        Total time: 699.75s
                               ETA: 9555.6s

################################################################################
                     [1m Learning iteration 273/4000 [0m

                       Computation: 3162 steps/s (collection: 0.494s, learning 2.097s)
               Value function loss: 220.9820
                    Surrogate loss: 0.0111
             Mean action noise std: 0.99
                       Mean reward: 350.71
               Mean episode length: 248.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 2.59s
                        Total time: 702.34s
                               ETA: 9553.4s

################################################################################
                     [1m Learning iteration 274/4000 [0m

                       Computation: 3141 steps/s (collection: 0.505s, learning 2.103s)
               Value function loss: 179.1667
                    Surrogate loss: 0.0140
             Mean action noise std: 0.99
                       Mean reward: 351.83
               Mean episode length: 248.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2252800
                    Iteration time: 2.61s
                        Total time: 704.95s
                               ETA: 9551.4s

################################################################################
                     [1m Learning iteration 275/4000 [0m

                       Computation: 3124 steps/s (collection: 0.499s, learning 2.123s)
               Value function loss: 200.9117
                    Surrogate loss: 0.0150
             Mean action noise std: 0.99
                       Mean reward: 341.02
               Mean episode length: 242.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 2.62s
                        Total time: 707.57s
                               ETA: 9549.6s

################################################################################
                     [1m Learning iteration 276/4000 [0m

                       Computation: 3207 steps/s (collection: 0.488s, learning 2.066s)
               Value function loss: 209.1190
                    Surrogate loss: 0.0132
             Mean action noise std: 0.99
                       Mean reward: 346.56
               Mean episode length: 245.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2269184
                    Iteration time: 2.55s
                        Total time: 710.12s
                               ETA: 9546.9s

################################################################################
                     [1m Learning iteration 277/4000 [0m

                       Computation: 3228 steps/s (collection: 0.479s, learning 2.058s)
               Value function loss: 284.7627
                    Surrogate loss: 0.0134
             Mean action noise std: 0.99
                       Mean reward: 347.89
               Mean episode length: 245.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 2.54s
                        Total time: 712.66s
                               ETA: 9544.0s

################################################################################
                     [1m Learning iteration 278/4000 [0m

                       Computation: 3159 steps/s (collection: 0.488s, learning 2.105s)
               Value function loss: 229.4654
                    Surrogate loss: 0.0114
             Mean action noise std: 0.99
                       Mean reward: 345.72
               Mean episode length: 244.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2285568
                    Iteration time: 2.59s
                        Total time: 715.25s
                               ETA: 9541.8s

################################################################################
                     [1m Learning iteration 279/4000 [0m

                       Computation: 3203 steps/s (collection: 0.473s, learning 2.084s)
               Value function loss: 199.5473
                    Surrogate loss: 0.0141
             Mean action noise std: 0.99
                       Mean reward: 352.24
               Mean episode length: 247.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 2.56s
                        Total time: 717.81s
                               ETA: 9539.2s

################################################################################
                     [1m Learning iteration 280/4000 [0m

                       Computation: 3286 steps/s (collection: 0.447s, learning 2.046s)
               Value function loss: 177.2972
                    Surrogate loss: 0.0146
             Mean action noise std: 0.99
                       Mean reward: 350.87
               Mean episode length: 243.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2301952
                    Iteration time: 2.49s
                        Total time: 720.30s
                               ETA: 9535.7s

################################################################################
                     [1m Learning iteration 281/4000 [0m

                       Computation: 3208 steps/s (collection: 0.484s, learning 2.069s)
               Value function loss: 159.9082
                    Surrogate loss: 0.0185
             Mean action noise std: 0.99
                       Mean reward: 369.77
               Mean episode length: 255.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 2.55s
                        Total time: 722.86s
                               ETA: 9533.0s

################################################################################
                     [1m Learning iteration 282/4000 [0m

                       Computation: 3140 steps/s (collection: 0.500s, learning 2.109s)
               Value function loss: 347.3625
                    Surrogate loss: 0.0119
             Mean action noise std: 0.99
                       Mean reward: 364.56
               Mean episode length: 247.77
                 Mean success rate: 1.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2318336
                    Iteration time: 2.61s
                        Total time: 725.47s
                               ETA: 9531.0s

################################################################################
                     [1m Learning iteration 283/4000 [0m

                       Computation: 3178 steps/s (collection: 0.502s, learning 2.075s)
               Value function loss: 258.9570
                    Surrogate loss: 0.0116
             Mean action noise std: 0.99
                       Mean reward: 377.13
               Mean episode length: 256.46
                 Mean success rate: 1.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 2.58s
                        Total time: 728.04s
                               ETA: 9528.6s

################################################################################
                     [1m Learning iteration 284/4000 [0m

                       Computation: 3181 steps/s (collection: 0.515s, learning 2.060s)
               Value function loss: 210.5577
                    Surrogate loss: 0.0124
             Mean action noise std: 0.99
                       Mean reward: 379.50
               Mean episode length: 255.44
                 Mean success rate: 1.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2334720
                    Iteration time: 2.57s
                        Total time: 730.62s
                               ETA: 9526.2s

################################################################################
                     [1m Learning iteration 285/4000 [0m

                       Computation: 3267 steps/s (collection: 0.455s, learning 2.052s)
               Value function loss: 181.1832
                    Surrogate loss: 0.0131
             Mean action noise std: 0.99
                       Mean reward: 390.21
               Mean episode length: 261.85
                 Mean success rate: 1.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 2.51s
                        Total time: 733.12s
                               ETA: 9522.9s

################################################################################
                     [1m Learning iteration 286/4000 [0m

                       Computation: 3268 steps/s (collection: 0.467s, learning 2.040s)
               Value function loss: 238.3475
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 390.43
               Mean episode length: 262.13
                 Mean success rate: 1.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2351104
                    Iteration time: 2.51s
                        Total time: 735.63s
                               ETA: 9519.6s

################################################################################
                     [1m Learning iteration 287/4000 [0m

                       Computation: 3161 steps/s (collection: 0.477s, learning 2.115s)
               Value function loss: 233.0353
                    Surrogate loss: 0.0151
             Mean action noise std: 0.99
                       Mean reward: 404.45
               Mean episode length: 274.90
                 Mean success rate: 1.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 2.59s
                        Total time: 738.22s
                               ETA: 9517.4s

################################################################################
                     [1m Learning iteration 288/4000 [0m

                       Computation: 3243 steps/s (collection: 0.491s, learning 2.035s)
               Value function loss: 234.1594
                    Surrogate loss: 0.0129
             Mean action noise std: 0.99
                       Mean reward: 407.03
               Mean episode length: 271.71
                 Mean success rate: 1.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2367488
                    Iteration time: 2.53s
                        Total time: 740.75s
                               ETA: 9514.4s

################################################################################
                     [1m Learning iteration 289/4000 [0m

                       Computation: 3304 steps/s (collection: 0.441s, learning 2.037s)
               Value function loss: 215.1658
                    Surrogate loss: 0.0141
             Mean action noise std: 0.99
                       Mean reward: 395.33
               Mean episode length: 268.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 2.48s
                        Total time: 743.23s
                               ETA: 9510.7s

################################################################################
                     [1m Learning iteration 290/4000 [0m

                       Computation: 3168 steps/s (collection: 0.517s, learning 2.068s)
               Value function loss: 203.4241
                    Surrogate loss: 0.0138
             Mean action noise std: 0.99
                       Mean reward: 371.34
               Mean episode length: 252.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 2383872
                    Iteration time: 2.59s
                        Total time: 745.81s
                               ETA: 9508.5s

################################################################################
                     [1m Learning iteration 291/4000 [0m

                       Computation: 3211 steps/s (collection: 0.501s, learning 2.050s)
               Value function loss: 194.4917
                    Surrogate loss: 0.0182
             Mean action noise std: 0.99
                       Mean reward: 356.87
               Mean episode length: 242.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 2.55s
                        Total time: 748.36s
                               ETA: 9505.7s

################################################################################
                     [1m Learning iteration 292/4000 [0m

                       Computation: 3160 steps/s (collection: 0.526s, learning 2.066s)
               Value function loss: 236.3602
                    Surrogate loss: 0.0132
             Mean action noise std: 0.99
                       Mean reward: 346.60
               Mean episode length: 232.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2400256
                    Iteration time: 2.59s
                        Total time: 750.95s
                               ETA: 9503.5s

################################################################################
                     [1m Learning iteration 293/4000 [0m

                       Computation: 3213 steps/s (collection: 0.456s, learning 2.093s)
               Value function loss: 218.8236
                    Surrogate loss: 0.0119
             Mean action noise std: 0.99
                       Mean reward: 328.65
               Mean episode length: 222.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 2.55s
                        Total time: 753.50s
                               ETA: 9500.8s

################################################################################
                     [1m Learning iteration 294/4000 [0m

                       Computation: 3229 steps/s (collection: 0.467s, learning 2.070s)
               Value function loss: 195.8255
                    Surrogate loss: 0.0125
             Mean action noise std: 0.99
                       Mean reward: 342.58
               Mean episode length: 230.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2416640
                    Iteration time: 2.54s
                        Total time: 756.04s
                               ETA: 9497.9s

################################################################################
                     [1m Learning iteration 295/4000 [0m

                       Computation: 3236 steps/s (collection: 0.481s, learning 2.050s)
               Value function loss: 268.1295
                    Surrogate loss: 0.0140
             Mean action noise std: 0.99
                       Mean reward: 362.44
               Mean episode length: 243.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 2.53s
                        Total time: 758.57s
                               ETA: 9495.0s

################################################################################
                     [1m Learning iteration 296/4000 [0m

                       Computation: 3179 steps/s (collection: 0.519s, learning 2.057s)
               Value function loss: 266.5890
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 357.43
               Mean episode length: 241.09
                 Mean success rate: 0.50
                  Mean reward/step: 1.57
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2433024
                    Iteration time: 2.58s
                        Total time: 761.15s
                               ETA: 9492.6s

################################################################################
                     [1m Learning iteration 297/4000 [0m

                       Computation: 3223 steps/s (collection: 0.490s, learning 2.051s)
               Value function loss: 252.5601
                    Surrogate loss: 0.0121
             Mean action noise std: 0.99
                       Mean reward: 370.76
               Mean episode length: 248.03
                 Mean success rate: 0.50
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 2.54s
                        Total time: 763.69s
                               ETA: 9489.7s

################################################################################
                     [1m Learning iteration 298/4000 [0m

                       Computation: 3183 steps/s (collection: 0.491s, learning 2.083s)
               Value function loss: 218.3284
                    Surrogate loss: 0.0151
             Mean action noise std: 0.99
                       Mean reward: 380.85
               Mean episode length: 257.70
                 Mean success rate: 0.50
                  Mean reward/step: 1.56
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2449408
                    Iteration time: 2.57s
                        Total time: 766.26s
                               ETA: 9487.3s

################################################################################
                     [1m Learning iteration 299/4000 [0m

                       Computation: 3176 steps/s (collection: 0.511s, learning 2.068s)
               Value function loss: 227.2550
                    Surrogate loss: 0.0136
             Mean action noise std: 0.99
                       Mean reward: 364.34
               Mean episode length: 245.88
                 Mean success rate: 0.50
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 2.58s
                        Total time: 768.84s
                               ETA: 9484.9s

################################################################################
                     [1m Learning iteration 300/4000 [0m

                       Computation: 3276 steps/s (collection: 0.464s, learning 2.036s)
               Value function loss: 221.0838
                    Surrogate loss: 0.0136
             Mean action noise std: 0.99
                       Mean reward: 378.82
               Mean episode length: 254.96
                 Mean success rate: 0.50
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2465792
                    Iteration time: 2.50s
                        Total time: 771.34s
                               ETA: 9481.6s

################################################################################
                     [1m Learning iteration 301/4000 [0m

                       Computation: 3235 steps/s (collection: 0.473s, learning 2.059s)
               Value function loss: 214.6577
                    Surrogate loss: 0.0134
             Mean action noise std: 0.99
                       Mean reward: 361.49
               Mean episode length: 240.95
                 Mean success rate: 0.50
                  Mean reward/step: 1.48
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 2.53s
                        Total time: 773.87s
                               ETA: 9478.7s

################################################################################
                     [1m Learning iteration 302/4000 [0m

                       Computation: 3192 steps/s (collection: 0.507s, learning 2.059s)
               Value function loss: 223.5445
                    Surrogate loss: 0.0152
             Mean action noise std: 0.99
                       Mean reward: 360.82
               Mean episode length: 241.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 2482176
                    Iteration time: 2.57s
                        Total time: 776.44s
                               ETA: 9476.2s

################################################################################
                     [1m Learning iteration 303/4000 [0m

                       Computation: 3169 steps/s (collection: 0.514s, learning 2.070s)
               Value function loss: 192.5609
                    Surrogate loss: 0.0180
             Mean action noise std: 0.99
                       Mean reward: 360.64
               Mean episode length: 240.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 2.58s
                        Total time: 779.02s
                               ETA: 9473.9s

################################################################################
                     [1m Learning iteration 304/4000 [0m

                       Computation: 3264 steps/s (collection: 0.480s, learning 2.029s)
               Value function loss: 162.8429
                    Surrogate loss: 0.0185
             Mean action noise std: 0.99
                       Mean reward: 365.74
               Mean episode length: 243.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2498560
                    Iteration time: 2.51s
                        Total time: 781.53s
                               ETA: 9470.7s

################################################################################
                     [1m Learning iteration 305/4000 [0m

                       Computation: 3183 steps/s (collection: 0.459s, learning 2.114s)
               Value function loss: 255.6764
                    Surrogate loss: 0.0119
             Mean action noise std: 0.99
                       Mean reward: 363.73
               Mean episode length: 241.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 2.57s
                        Total time: 784.11s
                               ETA: 9468.2s

################################################################################
                     [1m Learning iteration 306/4000 [0m

                       Computation: 3178 steps/s (collection: 0.519s, learning 2.059s)
               Value function loss: 196.1920
                    Surrogate loss: 0.0125
             Mean action noise std: 0.99
                       Mean reward: 374.42
               Mean episode length: 245.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2514944
                    Iteration time: 2.58s
                        Total time: 786.68s
                               ETA: 9465.8s

################################################################################
                     [1m Learning iteration 307/4000 [0m

                       Computation: 3201 steps/s (collection: 0.500s, learning 2.058s)
               Value function loss: 191.7454
                    Surrogate loss: 0.0147
             Mean action noise std: 0.99
                       Mean reward: 384.87
               Mean episode length: 253.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 2.56s
                        Total time: 789.24s
                               ETA: 9463.2s

################################################################################
                     [1m Learning iteration 308/4000 [0m

                       Computation: 3203 steps/s (collection: 0.508s, learning 2.049s)
               Value function loss: 300.4403
                    Surrogate loss: 0.0135
             Mean action noise std: 0.99
                       Mean reward: 402.71
               Mean episode length: 263.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2531328
                    Iteration time: 2.56s
                        Total time: 791.80s
                               ETA: 9460.6s

################################################################################
                     [1m Learning iteration 309/4000 [0m

                       Computation: 3259 steps/s (collection: 0.461s, learning 2.052s)
               Value function loss: 214.5690
                    Surrogate loss: 0.0146
             Mean action noise std: 0.99
                       Mean reward: 401.81
               Mean episode length: 264.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 2.51s
                        Total time: 794.31s
                               ETA: 9457.5s

################################################################################
                     [1m Learning iteration 310/4000 [0m

                       Computation: 3232 steps/s (collection: 0.455s, learning 2.079s)
               Value function loss: 250.0000
                    Surrogate loss: 0.0133
             Mean action noise std: 0.99
                       Mean reward: 395.79
               Mean episode length: 259.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2547712
                    Iteration time: 2.53s
                        Total time: 796.85s
                               ETA: 9454.6s

################################################################################
                     [1m Learning iteration 311/4000 [0m

                       Computation: 3240 steps/s (collection: 0.465s, learning 2.063s)
               Value function loss: 213.2855
                    Surrogate loss: 0.0186
             Mean action noise std: 0.99
                       Mean reward: 412.11
               Mean episode length: 269.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 2.53s
                        Total time: 799.38s
                               ETA: 9451.6s

################################################################################
                     [1m Learning iteration 312/4000 [0m

                       Computation: 3165 steps/s (collection: 0.544s, learning 2.044s)
               Value function loss: 270.0388
                    Surrogate loss: 0.0132
             Mean action noise std: 0.99
                       Mean reward: 411.38
               Mean episode length: 272.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2564096
                    Iteration time: 2.59s
                        Total time: 801.96s
                               ETA: 9449.3s

################################################################################
                     [1m Learning iteration 313/4000 [0m

                       Computation: 3269 steps/s (collection: 0.461s, learning 2.045s)
               Value function loss: 210.2203
                    Surrogate loss: 0.0127
             Mean action noise std: 0.99
                       Mean reward: 410.14
               Mean episode length: 271.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 2.51s
                        Total time: 804.47s
                               ETA: 9446.1s

################################################################################
                     [1m Learning iteration 314/4000 [0m

                       Computation: 3180 steps/s (collection: 0.494s, learning 2.082s)
               Value function loss: 276.2864
                    Surrogate loss: 0.0128
             Mean action noise std: 0.99
                       Mean reward: 388.29
               Mean episode length: 258.74
                 Mean success rate: 0.50
                  Mean reward/step: 1.49
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 2580480
                    Iteration time: 2.58s
                        Total time: 807.05s
                               ETA: 9443.7s

################################################################################
                     [1m Learning iteration 315/4000 [0m

                       Computation: 3210 steps/s (collection: 0.454s, learning 2.098s)
               Value function loss: 227.9219
                    Surrogate loss: 0.0145
             Mean action noise std: 0.99
                       Mean reward: 381.48
               Mean episode length: 255.85
                 Mean success rate: 0.50
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 2.55s
                        Total time: 809.60s
                               ETA: 9441.0s

################################################################################
                     [1m Learning iteration 316/4000 [0m

                       Computation: 3232 steps/s (collection: 0.450s, learning 2.085s)
               Value function loss: 260.1618
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 397.13
               Mean episode length: 266.42
                 Mean success rate: 0.50
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2596864
                    Iteration time: 2.53s
                        Total time: 812.13s
                               ETA: 9438.2s

################################################################################
                     [1m Learning iteration 317/4000 [0m

                       Computation: 3251 steps/s (collection: 0.487s, learning 2.032s)
               Value function loss: 267.8165
                    Surrogate loss: 0.0150
             Mean action noise std: 0.99
                       Mean reward: 389.97
               Mean episode length: 259.85
                 Mean success rate: 1.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 2.52s
                        Total time: 814.65s
                               ETA: 9435.1s

################################################################################
                     [1m Learning iteration 318/4000 [0m

                       Computation: 3183 steps/s (collection: 0.461s, learning 2.113s)
               Value function loss: 281.8321
                    Surrogate loss: 0.0158
             Mean action noise std: 0.99
                       Mean reward: 377.29
               Mean episode length: 250.07
                 Mean success rate: 1.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2613248
                    Iteration time: 2.57s
                        Total time: 817.22s
                               ETA: 9432.7s

################################################################################
                     [1m Learning iteration 319/4000 [0m

                       Computation: 3208 steps/s (collection: 0.497s, learning 2.056s)
               Value function loss: 254.3605
                    Surrogate loss: 0.0179
             Mean action noise std: 0.99
                       Mean reward: 389.83
               Mean episode length: 256.32
                 Mean success rate: 1.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 2.55s
                        Total time: 819.78s
                               ETA: 9430.0s

################################################################################
                     [1m Learning iteration 320/4000 [0m

                       Computation: 3146 steps/s (collection: 0.551s, learning 2.053s)
               Value function loss: 213.6615
                    Surrogate loss: 0.0155
             Mean action noise std: 0.99
                       Mean reward: 381.01
               Mean episode length: 253.37
                 Mean success rate: 0.50
                  Mean reward/step: 1.55
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2629632
                    Iteration time: 2.60s
                        Total time: 822.38s
                               ETA: 9427.9s

################################################################################
                     [1m Learning iteration 321/4000 [0m

                       Computation: 3278 steps/s (collection: 0.472s, learning 2.027s)
               Value function loss: 337.2646
                    Surrogate loss: 0.0157
             Mean action noise std: 0.99
                       Mean reward: 368.79
               Mean episode length: 241.43
                 Mean success rate: 0.50
                  Mean reward/step: 1.60
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 2.50s
                        Total time: 824.88s
                               ETA: 9424.6s

################################################################################
                     [1m Learning iteration 322/4000 [0m

                       Computation: 3245 steps/s (collection: 0.451s, learning 2.074s)
               Value function loss: 278.0167
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 357.83
               Mean episode length: 232.74
                 Mean success rate: 0.50
                  Mean reward/step: 1.52
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2646016
                    Iteration time: 2.52s
                        Total time: 827.40s
                               ETA: 9421.7s

################################################################################
                     [1m Learning iteration 323/4000 [0m

                       Computation: 3262 steps/s (collection: 0.462s, learning 2.049s)
               Value function loss: 277.3640
                    Surrogate loss: 0.0173
             Mean action noise std: 0.99
                       Mean reward: 353.12
               Mean episode length: 229.32
                 Mean success rate: 0.50
                  Mean reward/step: 1.56
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 2.51s
                        Total time: 829.92s
                               ETA: 9418.5s

################################################################################
                     [1m Learning iteration 324/4000 [0m

                       Computation: 3222 steps/s (collection: 0.488s, learning 2.053s)
               Value function loss: 342.5823
                    Surrogate loss: 0.0138
             Mean action noise std: 0.99
                       Mean reward: 350.85
               Mean episode length: 225.77
                 Mean success rate: 0.50
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2662400
                    Iteration time: 2.54s
                        Total time: 832.46s
                               ETA: 9415.7s

################################################################################
                     [1m Learning iteration 325/4000 [0m

                       Computation: 3182 steps/s (collection: 0.465s, learning 2.109s)
               Value function loss: 237.3164
                    Surrogate loss: 0.0188
             Mean action noise std: 0.99
                       Mean reward: 351.64
               Mean episode length: 224.81
                 Mean success rate: 0.50
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 2.57s
                        Total time: 835.03s
                               ETA: 9413.3s

################################################################################
                     [1m Learning iteration 326/4000 [0m

                       Computation: 3210 steps/s (collection: 0.490s, learning 2.062s)
               Value function loss: 222.9965
                    Surrogate loss: 0.0174
             Mean action noise std: 0.99
                       Mean reward: 359.97
               Mean episode length: 227.34
                 Mean success rate: 0.50
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2678784
                    Iteration time: 2.55s
                        Total time: 837.58s
                               ETA: 9410.7s

################################################################################
                     [1m Learning iteration 327/4000 [0m

                       Computation: 3225 steps/s (collection: 0.493s, learning 2.046s)
               Value function loss: 273.7814
                    Surrogate loss: 0.0170
             Mean action noise std: 0.99
                       Mean reward: 366.52
               Mean episode length: 230.65
                 Mean success rate: 0.50
                  Mean reward/step: 1.59
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 2.54s
                        Total time: 840.12s
                               ETA: 9407.8s

################################################################################
                     [1m Learning iteration 328/4000 [0m

                       Computation: 3249 steps/s (collection: 0.429s, learning 2.092s)
               Value function loss: 252.0156
                    Surrogate loss: 0.0160
             Mean action noise std: 0.99
                       Mean reward: 367.08
               Mean episode length: 231.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2695168
                    Iteration time: 2.52s
                        Total time: 842.64s
                               ETA: 9404.8s

################################################################################
                     [1m Learning iteration 329/4000 [0m

                       Computation: 3219 steps/s (collection: 0.505s, learning 2.040s)
               Value function loss: 292.5665
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 369.97
               Mean episode length: 232.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 2.54s
                        Total time: 845.19s
                               ETA: 9402.1s

################################################################################
                     [1m Learning iteration 330/4000 [0m

                       Computation: 3211 steps/s (collection: 0.468s, learning 2.083s)
               Value function loss: 277.3350
                    Surrogate loss: 0.0147
             Mean action noise std: 0.99
                       Mean reward: 361.20
               Mean episode length: 228.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2711552
                    Iteration time: 2.55s
                        Total time: 847.74s
                               ETA: 9399.4s

################################################################################
                     [1m Learning iteration 331/4000 [0m

                       Computation: 3196 steps/s (collection: 0.520s, learning 2.042s)
               Value function loss: 226.6686
                    Surrogate loss: 0.0154
             Mean action noise std: 0.99
                       Mean reward: 370.86
               Mean episode length: 232.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 2.56s
                        Total time: 850.30s
                               ETA: 9396.9s

################################################################################
                     [1m Learning iteration 332/4000 [0m

                       Computation: 3169 steps/s (collection: 0.509s, learning 2.076s)
               Value function loss: 244.7182
                    Surrogate loss: 0.0163
             Mean action noise std: 0.99
                       Mean reward: 355.54
               Mean episode length: 224.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2727936
                    Iteration time: 2.58s
                        Total time: 852.89s
                               ETA: 9394.6s

################################################################################
                     [1m Learning iteration 333/4000 [0m

                       Computation: 3192 steps/s (collection: 0.489s, learning 2.077s)
               Value function loss: 248.8143
                    Surrogate loss: 0.0154
             Mean action noise std: 0.99
                       Mean reward: 359.01
               Mean episode length: 230.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 2.57s
                        Total time: 855.45s
                               ETA: 9392.1s

################################################################################
                     [1m Learning iteration 334/4000 [0m

                       Computation: 3254 steps/s (collection: 0.467s, learning 2.050s)
               Value function loss: 237.5111
                    Surrogate loss: 0.0169
             Mean action noise std: 0.98
                       Mean reward: 357.58
               Mean episode length: 229.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2744320
                    Iteration time: 2.52s
                        Total time: 857.97s
                               ETA: 9389.0s

################################################################################
                     [1m Learning iteration 335/4000 [0m

                       Computation: 3218 steps/s (collection: 0.490s, learning 2.055s)
               Value function loss: 337.7720
                    Surrogate loss: 0.0132
             Mean action noise std: 0.98
                       Mean reward: 378.14
               Mean episode length: 244.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 2.55s
                        Total time: 860.52s
                               ETA: 9386.3s

################################################################################
                     [1m Learning iteration 336/4000 [0m

                       Computation: 3121 steps/s (collection: 0.512s, learning 2.112s)
               Value function loss: 270.2447
                    Surrogate loss: 0.0135
             Mean action noise std: 0.98
                       Mean reward: 385.11
               Mean episode length: 247.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2760704
                    Iteration time: 2.62s
                        Total time: 863.14s
                               ETA: 9384.4s

################################################################################
                     [1m Learning iteration 337/4000 [0m

                       Computation: 3108 steps/s (collection: 0.498s, learning 2.137s)
               Value function loss: 326.7788
                    Surrogate loss: 0.0168
             Mean action noise std: 0.98
                       Mean reward: 408.40
               Mean episode length: 262.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 2.64s
                        Total time: 865.78s
                               ETA: 9382.7s

################################################################################
                     [1m Learning iteration 338/4000 [0m

                       Computation: 3015 steps/s (collection: 0.506s, learning 2.211s)
               Value function loss: 246.0395
                    Surrogate loss: 0.0162
             Mean action noise std: 0.98
                       Mean reward: 405.53
               Mean episode length: 263.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2777088
                    Iteration time: 2.72s
                        Total time: 868.49s
                               ETA: 9381.8s

################################################################################
                     [1m Learning iteration 339/4000 [0m

                       Computation: 3146 steps/s (collection: 0.500s, learning 2.104s)
               Value function loss: 309.5105
                    Surrogate loss: 0.0167
             Mean action noise std: 0.98
                       Mean reward: 414.93
               Mean episode length: 264.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 2.60s
                        Total time: 871.10s
                               ETA: 9379.7s

################################################################################
                     [1m Learning iteration 340/4000 [0m

                       Computation: 3091 steps/s (collection: 0.528s, learning 2.122s)
               Value function loss: 333.7904
                    Surrogate loss: 0.0149
             Mean action noise std: 0.98
                       Mean reward: 418.02
               Mean episode length: 263.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2793472
                    Iteration time: 2.65s
                        Total time: 873.75s
                               ETA: 9378.0s

################################################################################
                     [1m Learning iteration 341/4000 [0m

                       Computation: 3138 steps/s (collection: 0.458s, learning 2.152s)
               Value function loss: 272.7955
                    Surrogate loss: 0.0144
             Mean action noise std: 0.98
                       Mean reward: 422.36
               Mean episode length: 265.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 2.61s
                        Total time: 876.36s
                               ETA: 9376.0s

################################################################################
                     [1m Learning iteration 342/4000 [0m

                       Computation: 3112 steps/s (collection: 0.499s, learning 2.132s)
               Value function loss: 275.5676
                    Surrogate loss: 0.0157
             Mean action noise std: 0.98
                       Mean reward: 424.18
               Mean episode length: 266.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2809856
                    Iteration time: 2.63s
                        Total time: 878.99s
                               ETA: 9374.2s

################################################################################
                     [1m Learning iteration 343/4000 [0m

                       Computation: 3107 steps/s (collection: 0.539s, learning 2.097s)
               Value function loss: 274.1733
                    Surrogate loss: 0.0160
             Mean action noise std: 0.98
                       Mean reward: 429.34
               Mean episode length: 266.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 2.64s
                        Total time: 881.62s
                               ETA: 9372.4s

################################################################################
                     [1m Learning iteration 344/4000 [0m

                       Computation: 3178 steps/s (collection: 0.482s, learning 2.095s)
               Value function loss: 181.3405
                    Surrogate loss: 0.0192
             Mean action noise std: 0.98
                       Mean reward: 430.14
               Mean episode length: 269.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2826240
                    Iteration time: 2.58s
                        Total time: 884.20s
                               ETA: 9370.0s

################################################################################
                     [1m Learning iteration 345/4000 [0m

                       Computation: 3192 steps/s (collection: 0.488s, learning 2.079s)
               Value function loss: 230.8334
                    Surrogate loss: 0.0203
             Mean action noise std: 0.98
                       Mean reward: 438.20
               Mean episode length: 273.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 2.57s
                        Total time: 886.77s
                               ETA: 9367.4s

################################################################################
                     [1m Learning iteration 346/4000 [0m

                       Computation: 3177 steps/s (collection: 0.486s, learning 2.092s)
               Value function loss: 258.9406
                    Surrogate loss: 0.0180
             Mean action noise std: 0.98
                       Mean reward: 439.60
               Mean episode length: 272.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2842624
                    Iteration time: 2.58s
                        Total time: 889.35s
                               ETA: 9365.0s

################################################################################
                     [1m Learning iteration 347/4000 [0m

                       Computation: 3183 steps/s (collection: 0.501s, learning 2.072s)
               Value function loss: 418.4132
                    Surrogate loss: 0.0152
             Mean action noise std: 0.98
                       Mean reward: 436.47
               Mean episode length: 271.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 2.57s
                        Total time: 891.92s
                               ETA: 9362.6s

################################################################################
                     [1m Learning iteration 348/4000 [0m

                       Computation: 3198 steps/s (collection: 0.505s, learning 2.057s)
               Value function loss: 275.9895
                    Surrogate loss: 0.0165
             Mean action noise std: 0.98
                       Mean reward: 440.89
               Mean episode length: 274.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2859008
                    Iteration time: 2.56s
                        Total time: 894.48s
                               ETA: 9360.0s

################################################################################
                     [1m Learning iteration 349/4000 [0m

                       Computation: 3128 steps/s (collection: 0.486s, learning 2.132s)
               Value function loss: 233.4013
                    Surrogate loss: 0.0151
             Mean action noise std: 0.98
                       Mean reward: 451.37
               Mean episode length: 281.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 2.62s
                        Total time: 897.10s
                               ETA: 9358.0s

################################################################################
                     [1m Learning iteration 350/4000 [0m

                       Computation: 3264 steps/s (collection: 0.442s, learning 2.067s)
               Value function loss: 384.0665
                    Surrogate loss: 0.0153
             Mean action noise std: 0.98
                       Mean reward: 454.07
               Mean episode length: 282.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2875392
                    Iteration time: 2.51s
                        Total time: 899.61s
                               ETA: 9354.9s

################################################################################
                     [1m Learning iteration 351/4000 [0m

                       Computation: 3206 steps/s (collection: 0.467s, learning 2.089s)
               Value function loss: 381.9138
                    Surrogate loss: 0.0131
             Mean action noise std: 0.98
                       Mean reward: 452.32
               Mean episode length: 276.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 2.56s
                        Total time: 902.16s
                               ETA: 9352.3s

################################################################################
                     [1m Learning iteration 352/4000 [0m

                       Computation: 3161 steps/s (collection: 0.472s, learning 2.120s)
               Value function loss: 341.6346
                    Surrogate loss: 0.0148
             Mean action noise std: 0.98
                       Mean reward: 473.09
               Mean episode length: 289.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2891776
                    Iteration time: 2.59s
                        Total time: 904.75s
                               ETA: 9350.0s

################################################################################
                     [1m Learning iteration 353/4000 [0m

                       Computation: 3199 steps/s (collection: 0.466s, learning 2.094s)
               Value function loss: 257.2995
                    Surrogate loss: 0.0153
             Mean action noise std: 0.98
                       Mean reward: 469.48
               Mean episode length: 285.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 2.56s
                        Total time: 907.32s
                               ETA: 9347.4s

################################################################################
                     [1m Learning iteration 354/4000 [0m

                       Computation: 3199 steps/s (collection: 0.470s, learning 2.090s)
               Value function loss: 303.1949
                    Surrogate loss: 0.0159
             Mean action noise std: 0.98
                       Mean reward: 471.13
               Mean episode length: 285.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2908160
                    Iteration time: 2.56s
                        Total time: 909.88s
                               ETA: 9344.8s

################################################################################
                     [1m Learning iteration 355/4000 [0m

                       Computation: 3222 steps/s (collection: 0.485s, learning 2.056s)
               Value function loss: 314.5215
                    Surrogate loss: 0.0152
             Mean action noise std: 0.98
                       Mean reward: 466.20
               Mean episode length: 281.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 2.54s
                        Total time: 912.42s
                               ETA: 9342.0s

################################################################################
                     [1m Learning iteration 356/4000 [0m

                       Computation: 3264 steps/s (collection: 0.479s, learning 2.030s)
               Value function loss: 359.2607
                    Surrogate loss: 0.0145
             Mean action noise std: 0.98
                       Mean reward: 480.25
               Mean episode length: 285.32
                 Mean success rate: 0.50
                  Mean reward/step: 1.76
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2924544
                    Iteration time: 2.51s
                        Total time: 914.93s
                               ETA: 9338.9s

################################################################################
                     [1m Learning iteration 357/4000 [0m

                       Computation: 3200 steps/s (collection: 0.487s, learning 2.073s)
               Value function loss: 446.2087
                    Surrogate loss: 0.0148
             Mean action noise std: 0.98
                       Mean reward: 471.67
               Mean episode length: 281.70
                 Mean success rate: 0.50
                  Mean reward/step: 1.78
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 2.56s
                        Total time: 917.49s
                               ETA: 9336.3s

################################################################################
                     [1m Learning iteration 358/4000 [0m

                       Computation: 3209 steps/s (collection: 0.494s, learning 2.058s)
               Value function loss: 209.0368
                    Surrogate loss: 0.0190
             Mean action noise std: 0.98
                       Mean reward: 432.52
               Mean episode length: 257.10
                 Mean success rate: 0.50
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2940928
                    Iteration time: 2.55s
                        Total time: 920.04s
                               ETA: 9333.6s

################################################################################
                     [1m Learning iteration 359/4000 [0m

                       Computation: 3169 steps/s (collection: 0.514s, learning 2.071s)
               Value function loss: 397.9505
                    Surrogate loss: 0.0154
             Mean action noise std: 0.98
                       Mean reward: 432.04
               Mean episode length: 255.12
                 Mean success rate: 0.50
                  Mean reward/step: 1.71
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 2.58s
                        Total time: 922.62s
                               ETA: 9331.3s

################################################################################
                     [1m Learning iteration 360/4000 [0m

                       Computation: 3158 steps/s (collection: 0.483s, learning 2.111s)
               Value function loss: 284.6698
                    Surrogate loss: 0.0160
             Mean action noise std: 0.98
                       Mean reward: 405.46
               Mean episode length: 238.50
                 Mean success rate: 0.50
                  Mean reward/step: 1.68
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2957312
                    Iteration time: 2.59s
                        Total time: 925.22s
                               ETA: 9329.1s

################################################################################
                     [1m Learning iteration 361/4000 [0m

                       Computation: 3105 steps/s (collection: 0.524s, learning 2.114s)
               Value function loss: 387.2957
                    Surrogate loss: 0.0156
             Mean action noise std: 0.98
                       Mean reward: 402.23
               Mean episode length: 233.55
                 Mean success rate: 1.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 2.64s
                        Total time: 927.85s
                               ETA: 9327.2s

################################################################################
                     [1m Learning iteration 362/4000 [0m

                       Computation: 3167 steps/s (collection: 0.459s, learning 2.128s)
               Value function loss: 422.0402
                    Surrogate loss: 0.0157
             Mean action noise std: 0.98
                       Mean reward: 373.56
               Mean episode length: 218.70
                 Mean success rate: 1.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2973696
                    Iteration time: 2.59s
                        Total time: 930.44s
                               ETA: 9324.9s

################################################################################
                     [1m Learning iteration 363/4000 [0m

                       Computation: 3233 steps/s (collection: 0.441s, learning 2.092s)
               Value function loss: 297.6285
                    Surrogate loss: 0.0198
             Mean action noise std: 0.98
                       Mean reward: 373.14
               Mean episode length: 218.35
                 Mean success rate: 1.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 2.53s
                        Total time: 932.97s
                               ETA: 9322.1s

################################################################################
                     [1m Learning iteration 364/4000 [0m

                       Computation: 3147 steps/s (collection: 0.485s, learning 2.118s)
               Value function loss: 357.2303
                    Surrogate loss: 0.0155
             Mean action noise std: 0.98
                       Mean reward: 391.08
               Mean episode length: 228.48
                 Mean success rate: 1.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 2990080
                    Iteration time: 2.60s
                        Total time: 935.58s
                               ETA: 9319.9s

################################################################################
                     [1m Learning iteration 365/4000 [0m

                       Computation: 3077 steps/s (collection: 0.531s, learning 2.130s)
               Value function loss: 292.1674
                    Surrogate loss: 0.0154
             Mean action noise std: 0.98
                       Mean reward: 383.71
               Mean episode length: 223.69
                 Mean success rate: 1.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 2.66s
                        Total time: 938.24s
                               ETA: 9318.3s

################################################################################
                     [1m Learning iteration 366/4000 [0m

                       Computation: 3146 steps/s (collection: 0.529s, learning 2.074s)
               Value function loss: 338.4369
                    Surrogate loss: 0.0171
             Mean action noise std: 0.98
                       Mean reward: 408.47
               Mean episode length: 235.92
                 Mean success rate: 1.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3006464
                    Iteration time: 2.60s
                        Total time: 940.84s
                               ETA: 9316.1s

################################################################################
                     [1m Learning iteration 367/4000 [0m

                       Computation: 3127 steps/s (collection: 0.506s, learning 2.113s)
               Value function loss: 257.7211
                    Surrogate loss: 0.0205
             Mean action noise std: 0.98
                       Mean reward: 409.45
               Mean episode length: 237.24
                 Mean success rate: 1.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 2.62s
                        Total time: 943.46s
                               ETA: 9314.1s

################################################################################
                     [1m Learning iteration 368/4000 [0m

                       Computation: 2971 steps/s (collection: 0.597s, learning 2.160s)
               Value function loss: 318.6652
                    Surrogate loss: 0.0184
             Mean action noise std: 0.98
                       Mean reward: 410.70
               Mean episode length: 239.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3022848
                    Iteration time: 2.76s
                        Total time: 946.22s
                               ETA: 9313.5s

################################################################################
                     [1m Learning iteration 369/4000 [0m

                       Computation: 3099 steps/s (collection: 0.522s, learning 2.121s)
               Value function loss: 508.7445
                    Surrogate loss: 0.0126
             Mean action noise std: 0.98
                       Mean reward: 425.51
               Mean episode length: 247.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 2.64s
                        Total time: 948.86s
                               ETA: 9311.7s

################################################################################
                     [1m Learning iteration 370/4000 [0m

                       Computation: 3248 steps/s (collection: 0.483s, learning 2.039s)
               Value function loss: 419.6943
                    Surrogate loss: 0.0142
             Mean action noise std: 0.98
                       Mean reward: 450.62
               Mean episode length: 257.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.92
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3039232
                    Iteration time: 2.52s
                        Total time: 951.38s
                               ETA: 9308.7s

################################################################################
                     [1m Learning iteration 371/4000 [0m

                       Computation: 3212 steps/s (collection: 0.493s, learning 2.057s)
               Value function loss: 425.3056
                    Surrogate loss: 0.0167
             Mean action noise std: 0.98
                       Mean reward: 451.86
               Mean episode length: 255.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 2.55s
                        Total time: 953.93s
                               ETA: 9306.0s

################################################################################
                     [1m Learning iteration 372/4000 [0m

                       Computation: 3118 steps/s (collection: 0.500s, learning 2.128s)
               Value function loss: 392.4490
                    Surrogate loss: 0.0159
             Mean action noise std: 0.98
                       Mean reward: 432.63
               Mean episode length: 245.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 3055616
                    Iteration time: 2.63s
                        Total time: 956.56s
                               ETA: 9304.0s

################################################################################
                     [1m Learning iteration 373/4000 [0m

                       Computation: 3120 steps/s (collection: 0.543s, learning 2.082s)
               Value function loss: 486.3817
                    Surrogate loss: 0.0152
             Mean action noise std: 0.98
                       Mean reward: 444.37
               Mean episode length: 251.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 2.63s
                        Total time: 959.19s
                               ETA: 9302.1s

################################################################################
                     [1m Learning iteration 374/4000 [0m

                       Computation: 3229 steps/s (collection: 0.484s, learning 2.053s)
               Value function loss: 377.8208
                    Surrogate loss: 0.0147
             Mean action noise std: 0.98
                       Mean reward: 454.57
               Mean episode length: 257.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3072000
                    Iteration time: 2.54s
                        Total time: 961.72s
                               ETA: 9299.2s

################################################################################
                     [1m Learning iteration 375/4000 [0m

                       Computation: 3251 steps/s (collection: 0.474s, learning 2.045s)
               Value function loss: 429.6653
                    Surrogate loss: 0.0164
             Mean action noise std: 0.98
                       Mean reward: 440.93
               Mean episode length: 252.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 2.52s
                        Total time: 964.24s
                               ETA: 9296.2s

################################################################################
                     [1m Learning iteration 376/4000 [0m

                       Computation: 3143 steps/s (collection: 0.479s, learning 2.128s)
               Value function loss: 391.6257
                    Surrogate loss: 0.0116
             Mean action noise std: 0.98
                       Mean reward: 436.78
               Mean episode length: 248.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 3088384
                    Iteration time: 2.61s
                        Total time: 966.85s
                               ETA: 9294.1s

################################################################################
                     [1m Learning iteration 377/4000 [0m

                       Computation: 3232 steps/s (collection: 0.460s, learning 2.075s)
               Value function loss: 563.2614
                    Surrogate loss: 0.0130
             Mean action noise std: 0.98
                       Mean reward: 442.90
               Mean episode length: 247.75
                 Mean success rate: 0.50
                  Mean reward/step: 1.88
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 2.53s
                        Total time: 969.38s
                               ETA: 9291.2s

################################################################################
                     [1m Learning iteration 378/4000 [0m

                       Computation: 3210 steps/s (collection: 0.462s, learning 2.090s)
               Value function loss: 402.5271
                    Surrogate loss: 0.0132
             Mean action noise std: 0.98
                       Mean reward: 423.52
               Mean episode length: 237.28
                 Mean success rate: 0.50
                  Mean reward/step: 1.74
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3104768
                    Iteration time: 2.55s
                        Total time: 971.93s
                               ETA: 9288.5s

################################################################################
                     [1m Learning iteration 379/4000 [0m

                       Computation: 3196 steps/s (collection: 0.476s, learning 2.086s)
               Value function loss: 325.2137
                    Surrogate loss: 0.0163
             Mean action noise std: 0.98
                       Mean reward: 410.71
               Mean episode length: 226.12
                 Mean success rate: 0.50
                  Mean reward/step: 1.78
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 2.56s
                        Total time: 974.50s
                               ETA: 9285.9s

################################################################################
                     [1m Learning iteration 380/4000 [0m

                       Computation: 3210 steps/s (collection: 0.480s, learning 2.072s)
               Value function loss: 397.6593
                    Surrogate loss: 0.0135
             Mean action noise std: 0.98
                       Mean reward: 408.68
               Mean episode length: 221.13
                 Mean success rate: 0.50
                  Mean reward/step: 1.83
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3121152
                    Iteration time: 2.55s
                        Total time: 977.05s
                               ETA: 9283.2s

################################################################################
                     [1m Learning iteration 381/4000 [0m

                       Computation: 3281 steps/s (collection: 0.463s, learning 2.034s)
               Value function loss: 359.3324
                    Surrogate loss: 0.0153
             Mean action noise std: 0.98
                       Mean reward: 391.86
               Mean episode length: 208.34
                 Mean success rate: 0.50
                  Mean reward/step: 1.83
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 2.50s
                        Total time: 979.55s
                               ETA: 9280.0s

################################################################################
                     [1m Learning iteration 382/4000 [0m

                       Computation: 3255 steps/s (collection: 0.473s, learning 2.043s)
               Value function loss: 376.4516
                    Surrogate loss: 0.0163
             Mean action noise std: 0.98
                       Mean reward: 386.80
               Mean episode length: 208.49
                 Mean success rate: 0.50
                  Mean reward/step: 1.83
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3137536
                    Iteration time: 2.52s
                        Total time: 982.06s
                               ETA: 9277.0s

################################################################################
                     [1m Learning iteration 383/4000 [0m

                       Computation: 3134 steps/s (collection: 0.513s, learning 2.100s)
               Value function loss: 448.2267
                    Surrogate loss: 0.0144
             Mean action noise std: 0.98
                       Mean reward: 378.47
               Mean episode length: 208.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 2.61s
                        Total time: 984.68s
                               ETA: 9274.9s

################################################################################
                     [1m Learning iteration 384/4000 [0m

                       Computation: 3173 steps/s (collection: 0.463s, learning 2.118s)
               Value function loss: 653.3476
                    Surrogate loss: 0.0141
             Mean action noise std: 0.98
                       Mean reward: 366.58
               Mean episode length: 202.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.89
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3153920
                    Iteration time: 2.58s
                        Total time: 987.26s
                               ETA: 9272.5s

################################################################################
                     [1m Learning iteration 385/4000 [0m

                       Computation: 3255 steps/s (collection: 0.470s, learning 2.046s)
               Value function loss: 565.2636
                    Surrogate loss: 0.0144
             Mean action noise std: 0.98
                       Mean reward: 363.64
               Mean episode length: 203.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 2.52s
                        Total time: 989.77s
                               ETA: 9269.5s

################################################################################
                     [1m Learning iteration 386/4000 [0m

                       Computation: 3223 steps/s (collection: 0.502s, learning 2.039s)
               Value function loss: 476.4542
                    Surrogate loss: 0.0144
             Mean action noise std: 0.98
                       Mean reward: 365.95
               Mean episode length: 208.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.93
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3170304
                    Iteration time: 2.54s
                        Total time: 992.31s
                               ETA: 9266.7s

################################################################################
                     [1m Learning iteration 387/4000 [0m

                       Computation: 3224 steps/s (collection: 0.455s, learning 2.085s)
               Value function loss: 542.1492
                    Surrogate loss: 0.0131
             Mean action noise std: 0.98
                       Mean reward: 371.67
               Mean episode length: 214.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.92
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 2.54s
                        Total time: 994.85s
                               ETA: 9263.9s

################################################################################
                     [1m Learning iteration 388/4000 [0m

                       Computation: 3230 steps/s (collection: 0.477s, learning 2.058s)
               Value function loss: 469.3654
                    Surrogate loss: 0.0127
             Mean action noise std: 0.98
                       Mean reward: 391.34
               Mean episode length: 216.79
                 Mean success rate: 1.50
                  Mean reward/step: 1.76
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3186688
                    Iteration time: 2.54s
                        Total time: 997.39s
                               ETA: 9261.1s

################################################################################
                     [1m Learning iteration 389/4000 [0m

                       Computation: 3203 steps/s (collection: 0.488s, learning 2.070s)
               Value function loss: 476.8228
                    Surrogate loss: 0.0159
             Mean action noise std: 0.98
                       Mean reward: 414.93
               Mean episode length: 227.33
                 Mean success rate: 1.50
                  Mean reward/step: 1.85
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 2.56s
                        Total time: 999.95s
                               ETA: 9258.5s

################################################################################
                     [1m Learning iteration 390/4000 [0m

                       Computation: 3173 steps/s (collection: 0.470s, learning 2.111s)
               Value function loss: 764.1154
                    Surrogate loss: 0.0137
             Mean action noise std: 0.98
                       Mean reward: 419.58
               Mean episode length: 230.84
                 Mean success rate: 1.50
                  Mean reward/step: 1.93
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3203072
                    Iteration time: 2.58s
                        Total time: 1002.53s
                               ETA: 9256.1s

################################################################################
                     [1m Learning iteration 391/4000 [0m

                       Computation: 3130 steps/s (collection: 0.495s, learning 2.121s)
               Value function loss: 686.0402
                    Surrogate loss: 0.0127
             Mean action noise std: 0.98
                       Mean reward: 445.24
               Mean episode length: 245.04
                 Mean success rate: 1.50
                  Mean reward/step: 1.94
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 2.62s
                        Total time: 1005.15s
                               ETA: 9254.0s

################################################################################
                     [1m Learning iteration 392/4000 [0m

                       Computation: 3105 steps/s (collection: 0.522s, learning 2.115s)
               Value function loss: 1003.4939
                    Surrogate loss: 0.0141
             Mean action noise std: 0.98
                       Mean reward: 465.05
               Mean episode length: 253.41
                 Mean success rate: 1.50
                  Mean reward/step: 2.07
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3219456
                    Iteration time: 2.64s
                        Total time: 1007.78s
                               ETA: 9252.1s

################################################################################
                     [1m Learning iteration 393/4000 [0m

                       Computation: 3223 steps/s (collection: 0.478s, learning 2.063s)
               Value function loss: 954.2493
                    Surrogate loss: 0.0098
             Mean action noise std: 0.98
                       Mean reward: 452.31
               Mean episode length: 237.49
                 Mean success rate: 1.50
                  Mean reward/step: 2.02
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 2.54s
                        Total time: 1010.32s
                               ETA: 9249.3s

################################################################################
                     [1m Learning iteration 394/4000 [0m

                       Computation: 3209 steps/s (collection: 0.493s, learning 2.059s)
               Value function loss: 834.9729
                    Surrogate loss: 0.0132
             Mean action noise std: 0.98
                       Mean reward: 469.37
               Mean episode length: 247.32
                 Mean success rate: 1.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3235840
                    Iteration time: 2.55s
                        Total time: 1012.88s
                               ETA: 9246.7s

################################################################################
                     [1m Learning iteration 395/4000 [0m

                       Computation: 3127 steps/s (collection: 0.516s, learning 2.104s)
               Value function loss: 785.4640
                    Surrogate loss: 0.0152
             Mean action noise std: 0.98
                       Mean reward: 434.88
               Mean episode length: 231.84
                 Mean success rate: 1.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 2.62s
                        Total time: 1015.50s
                               ETA: 9244.6s

################################################################################
                     [1m Learning iteration 396/4000 [0m

                       Computation: 3189 steps/s (collection: 0.480s, learning 2.088s)
               Value function loss: 945.0053
                    Surrogate loss: 0.0116
             Mean action noise std: 0.98
                       Mean reward: 463.13
               Mean episode length: 243.24
                 Mean success rate: 1.50
                  Mean reward/step: 1.91
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3252224
                    Iteration time: 2.57s
                        Total time: 1018.06s
                               ETA: 9242.1s

################################################################################
                     [1m Learning iteration 397/4000 [0m

                       Computation: 3189 steps/s (collection: 0.475s, learning 2.093s)
               Value function loss: 696.2780
                    Surrogate loss: 0.0163
             Mean action noise std: 0.98
                       Mean reward: 453.99
               Mean episode length: 234.16
                 Mean success rate: 2.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 2.57s
                        Total time: 1020.63s
                               ETA: 9239.6s

################################################################################
                     [1m Learning iteration 398/4000 [0m

                       Computation: 3190 steps/s (collection: 0.495s, learning 2.073s)
               Value function loss: 581.9813
                    Surrogate loss: 0.0143
             Mean action noise std: 0.98
                       Mean reward: 439.28
               Mean episode length: 229.32
                 Mean success rate: 1.50
                  Mean reward/step: 1.75
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3268608
                    Iteration time: 2.57s
                        Total time: 1023.20s
                               ETA: 9237.0s

################################################################################
                     [1m Learning iteration 399/4000 [0m

                       Computation: 3134 steps/s (collection: 0.505s, learning 2.109s)
               Value function loss: 874.5793
                    Surrogate loss: 0.0140
             Mean action noise std: 0.98
                       Mean reward: 452.59
               Mean episode length: 236.83
                 Mean success rate: 1.50
                  Mean reward/step: 1.79
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 2.61s
                        Total time: 1025.81s
                               ETA: 9234.9s

################################################################################
                     [1m Learning iteration 400/4000 [0m

                       Computation: 3191 steps/s (collection: 0.481s, learning 2.086s)
               Value function loss: 711.1100
                    Surrogate loss: 0.0139
             Mean action noise std: 0.98
                       Mean reward: 454.01
               Mean episode length: 241.24
                 Mean success rate: 1.00
                  Mean reward/step: 1.97
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3284992
                    Iteration time: 2.57s
                        Total time: 1028.38s
                               ETA: 9232.4s

################################################################################
                     [1m Learning iteration 401/4000 [0m

                       Computation: 3211 steps/s (collection: 0.473s, learning 2.078s)
               Value function loss: 907.5696
                    Surrogate loss: 0.0128
             Mean action noise std: 0.98
                       Mean reward: 450.66
               Mean episode length: 234.76
                 Mean success rate: 1.50
                  Mean reward/step: 2.13
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 2.55s
                        Total time: 1030.93s
                               ETA: 9229.7s

################################################################################
                     [1m Learning iteration 402/4000 [0m

                       Computation: 3210 steps/s (collection: 0.464s, learning 2.088s)
               Value function loss: 714.7324
                    Surrogate loss: 0.0139
             Mean action noise std: 0.98
                       Mean reward: 433.33
               Mean episode length: 223.79
                 Mean success rate: 1.50
                  Mean reward/step: 2.00
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3301376
                    Iteration time: 2.55s
                        Total time: 1033.48s
                               ETA: 9227.0s

################################################################################
                     [1m Learning iteration 403/4000 [0m

                       Computation: 3158 steps/s (collection: 0.498s, learning 2.095s)
               Value function loss: 707.8395
                    Surrogate loss: 0.0167
             Mean action noise std: 0.98
                       Mean reward: 440.52
               Mean episode length: 222.72
                 Mean success rate: 2.50
                  Mean reward/step: 1.93
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 2.59s
                        Total time: 1036.08s
                               ETA: 9224.7s

################################################################################
                     [1m Learning iteration 404/4000 [0m

                       Computation: 3148 steps/s (collection: 0.540s, learning 2.062s)
               Value function loss: 698.8254
                    Surrogate loss: 0.0143
             Mean action noise std: 0.98
                       Mean reward: 447.25
               Mean episode length: 225.04
                 Mean success rate: 2.50
                  Mean reward/step: 2.00
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3317760
                    Iteration time: 2.60s
                        Total time: 1038.68s
                               ETA: 9222.4s

################################################################################
                     [1m Learning iteration 405/4000 [0m

                       Computation: 3246 steps/s (collection: 0.497s, learning 2.027s)
               Value function loss: 830.4594
                    Surrogate loss: 0.0127
             Mean action noise std: 0.98
                       Mean reward: 440.92
               Mean episode length: 218.07
                 Mean success rate: 3.50
                  Mean reward/step: 1.98
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 2.52s
                        Total time: 1041.20s
                               ETA: 9219.5s

################################################################################
                     [1m Learning iteration 406/4000 [0m

                       Computation: 3208 steps/s (collection: 0.488s, learning 2.065s)
               Value function loss: 816.7817
                    Surrogate loss: 0.0151
             Mean action noise std: 0.98
                       Mean reward: 434.12
               Mean episode length: 212.88
                 Mean success rate: 3.50
                  Mean reward/step: 1.97
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3334144
                    Iteration time: 2.55s
                        Total time: 1043.76s
                               ETA: 9216.8s

################################################################################
                     [1m Learning iteration 407/4000 [0m

                       Computation: 3163 steps/s (collection: 0.512s, learning 2.077s)
               Value function loss: 1054.2214
                    Surrogate loss: 0.0123
             Mean action noise std: 0.98
                       Mean reward: 455.18
               Mean episode length: 223.75
                 Mean success rate: 4.00
                  Mean reward/step: 1.87
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 2.59s
                        Total time: 1046.35s
                               ETA: 9214.5s

################################################################################
                     [1m Learning iteration 408/4000 [0m

                       Computation: 3201 steps/s (collection: 0.474s, learning 2.084s)
               Value function loss: 1012.1387
                    Surrogate loss: 0.0131
             Mean action noise std: 0.98
                       Mean reward: 435.66
               Mean episode length: 214.25
                 Mean success rate: 4.50
                  Mean reward/step: 2.05
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 3350528
                    Iteration time: 2.56s
                        Total time: 1048.90s
                               ETA: 9211.9s

################################################################################
                     [1m Learning iteration 409/4000 [0m

                       Computation: 3243 steps/s (collection: 0.493s, learning 2.033s)
               Value function loss: 896.6526
                    Surrogate loss: 0.0154
             Mean action noise std: 0.98
                       Mean reward: 437.15
               Mean episode length: 216.75
                 Mean success rate: 5.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 2.53s
                        Total time: 1051.43s
                               ETA: 9209.0s

################################################################################
                     [1m Learning iteration 410/4000 [0m

                       Computation: 3195 steps/s (collection: 0.482s, learning 2.081s)
               Value function loss: 1741.1651
                    Surrogate loss: 0.0127
             Mean action noise std: 0.98
                       Mean reward: 400.83
               Mean episode length: 205.44
                 Mean success rate: 4.50
                  Mean reward/step: 2.26
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3366912
                    Iteration time: 2.56s
                        Total time: 1053.99s
                               ETA: 9206.4s

################################################################################
                     [1m Learning iteration 411/4000 [0m

                       Computation: 3130 steps/s (collection: 0.516s, learning 2.101s)
               Value function loss: 1671.6593
                    Surrogate loss: 0.0129
             Mean action noise std: 0.98
                       Mean reward: 413.67
               Mean episode length: 209.31
                 Mean success rate: 4.50
                  Mean reward/step: 2.32
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 2.62s
                        Total time: 1056.61s
                               ETA: 9204.3s

################################################################################
                     [1m Learning iteration 412/4000 [0m

                       Computation: 3221 steps/s (collection: 0.502s, learning 2.040s)
               Value function loss: 1227.9253
                    Surrogate loss: 0.0135
             Mean action noise std: 0.98
                       Mean reward: 378.54
               Mean episode length: 201.65
                 Mean success rate: 2.50
                  Mean reward/step: 2.20
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3383296
                    Iteration time: 2.54s
                        Total time: 1059.15s
                               ETA: 9201.6s

################################################################################
                     [1m Learning iteration 413/4000 [0m

                       Computation: 3245 steps/s (collection: 0.471s, learning 2.052s)
               Value function loss: 1569.6328
                    Surrogate loss: 0.0124
             Mean action noise std: 0.98
                       Mean reward: 386.38
               Mean episode length: 203.61
                 Mean success rate: 2.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 2.52s
                        Total time: 1061.68s
                               ETA: 9198.6s

################################################################################
                     [1m Learning iteration 414/4000 [0m

                       Computation: 3241 steps/s (collection: 0.474s, learning 2.054s)
               Value function loss: 1978.4623
                    Surrogate loss: 0.0101
             Mean action noise std: 0.98
                       Mean reward: 441.20
               Mean episode length: 218.81
                 Mean success rate: 3.50
                  Mean reward/step: 2.18
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3399680
                    Iteration time: 2.53s
                        Total time: 1064.20s
                               ETA: 9195.7s

################################################################################
                     [1m Learning iteration 415/4000 [0m

                       Computation: 3138 steps/s (collection: 0.491s, learning 2.120s)
               Value function loss: 1370.0787
                    Surrogate loss: 0.0140
             Mean action noise std: 0.98
                       Mean reward: 458.80
               Mean episode length: 222.11
                 Mean success rate: 3.50
                  Mean reward/step: 2.12
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 2.61s
                        Total time: 1066.81s
                               ETA: 9193.6s

################################################################################
                     [1m Learning iteration 416/4000 [0m

                       Computation: 3112 steps/s (collection: 0.526s, learning 2.106s)
               Value function loss: 876.1718
                    Surrogate loss: 0.0170
             Mean action noise std: 0.98
                       Mean reward: 474.47
               Mean episode length: 226.96
                 Mean success rate: 3.50
                  Mean reward/step: 1.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3416064
                    Iteration time: 2.63s
                        Total time: 1069.45s
                               ETA: 9191.6s

################################################################################
                     [1m Learning iteration 417/4000 [0m

                       Computation: 3246 steps/s (collection: 0.471s, learning 2.052s)
               Value function loss: 888.2909
                    Surrogate loss: 0.0153
             Mean action noise std: 0.98
                       Mean reward: 473.65
               Mean episode length: 226.73
                 Mean success rate: 4.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 2.52s
                        Total time: 1071.97s
                               ETA: 9188.7s

################################################################################
                     [1m Learning iteration 418/4000 [0m

                       Computation: 3237 steps/s (collection: 0.486s, learning 2.044s)
               Value function loss: 1076.3081
                    Surrogate loss: 0.0163
             Mean action noise std: 0.98
                       Mean reward: 499.73
               Mean episode length: 237.40
                 Mean success rate: 4.50
                  Mean reward/step: 2.09
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3432448
                    Iteration time: 2.53s
                        Total time: 1074.50s
                               ETA: 9185.8s

################################################################################
                     [1m Learning iteration 419/4000 [0m

                       Computation: 3263 steps/s (collection: 0.458s, learning 2.052s)
               Value function loss: 960.8022
                    Surrogate loss: 0.0166
             Mean action noise std: 0.98
                       Mean reward: 509.70
               Mean episode length: 238.85
                 Mean success rate: 5.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 2.51s
                        Total time: 1077.01s
                               ETA: 9182.8s

################################################################################
                     [1m Learning iteration 420/4000 [0m

                       Computation: 3150 steps/s (collection: 0.516s, learning 2.084s)
               Value function loss: 1182.4231
                    Surrogate loss: 0.0152
             Mean action noise std: 0.98
                       Mean reward: 491.59
               Mean episode length: 232.90
                 Mean success rate: 5.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3448832
                    Iteration time: 2.60s
                        Total time: 1079.61s
                               ETA: 9180.5s

################################################################################
                     [1m Learning iteration 421/4000 [0m

                       Computation: 3135 steps/s (collection: 0.547s, learning 2.066s)
               Value function loss: 1169.2603
                    Surrogate loss: 0.0159
             Mean action noise std: 0.98
                       Mean reward: 509.22
               Mean episode length: 234.26
                 Mean success rate: 5.50
                  Mean reward/step: 2.04
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 2.61s
                        Total time: 1082.22s
                               ETA: 9178.4s

################################################################################
                     [1m Learning iteration 422/4000 [0m

                       Computation: 3186 steps/s (collection: 0.507s, learning 2.064s)
               Value function loss: 1581.8775
                    Surrogate loss: 0.0132
             Mean action noise std: 0.98
                       Mean reward: 539.06
               Mean episode length: 240.95
                 Mean success rate: 6.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3465216
                    Iteration time: 2.57s
                        Total time: 1084.79s
                               ETA: 9175.9s

################################################################################
                     [1m Learning iteration 423/4000 [0m

                       Computation: 3184 steps/s (collection: 0.501s, learning 2.072s)
               Value function loss: 1540.0298
                    Surrogate loss: 0.0134
             Mean action noise std: 0.98
                       Mean reward: 513.24
               Mean episode length: 231.68
                 Mean success rate: 5.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 2.57s
                        Total time: 1087.37s
                               ETA: 9173.4s

################################################################################
                     [1m Learning iteration 424/4000 [0m

                       Computation: 3130 steps/s (collection: 0.480s, learning 2.137s)
               Value function loss: 1995.2147
                    Surrogate loss: 0.0163
             Mean action noise std: 0.98
                       Mean reward: 519.37
               Mean episode length: 240.89
                 Mean success rate: 4.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3481600
                    Iteration time: 2.62s
                        Total time: 1089.98s
                               ETA: 9171.2s

################################################################################
                     [1m Learning iteration 425/4000 [0m

                       Computation: 3189 steps/s (collection: 0.525s, learning 2.043s)
               Value function loss: 2083.9967
                    Surrogate loss: 0.0136
             Mean action noise std: 0.98
                       Mean reward: 495.81
               Mean episode length: 234.04
                 Mean success rate: 3.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 2.57s
                        Total time: 1092.55s
                               ETA: 9168.7s

################################################################################
                     [1m Learning iteration 426/4000 [0m

                       Computation: 3222 steps/s (collection: 0.470s, learning 2.072s)
               Value function loss: 2304.9710
                    Surrogate loss: 0.0147
             Mean action noise std: 0.98
                       Mean reward: 506.38
               Mean episode length: 248.35
                 Mean success rate: 3.00
                  Mean reward/step: 2.46
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3497984
                    Iteration time: 2.54s
                        Total time: 1095.09s
                               ETA: 9166.0s

################################################################################
                     [1m Learning iteration 427/4000 [0m

                       Computation: 3229 steps/s (collection: 0.469s, learning 2.068s)
               Value function loss: 2196.2343
                    Surrogate loss: 0.0120
             Mean action noise std: 0.98
                       Mean reward: 500.13
               Mean episode length: 245.59
                 Mean success rate: 3.50
                  Mean reward/step: 2.44
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 2.54s
                        Total time: 1097.63s
                               ETA: 9163.2s

################################################################################
                     [1m Learning iteration 428/4000 [0m

                       Computation: 3204 steps/s (collection: 0.491s, learning 2.065s)
               Value function loss: 1928.7379
                    Surrogate loss: 0.0139
             Mean action noise std: 0.97
                       Mean reward: 481.17
               Mean episode length: 243.85
                 Mean success rate: 3.50
                  Mean reward/step: 2.24
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3514368
                    Iteration time: 2.56s
                        Total time: 1100.19s
                               ETA: 9160.5s

################################################################################
                     [1m Learning iteration 429/4000 [0m

                       Computation: 3195 steps/s (collection: 0.509s, learning 2.055s)
               Value function loss: 2079.1759
                    Surrogate loss: 0.0144
             Mean action noise std: 0.97
                       Mean reward: 510.50
               Mean episode length: 252.19
                 Mean success rate: 4.50
                  Mean reward/step: 2.60
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 2.56s
                        Total time: 1102.75s
                               ETA: 9158.0s

################################################################################
                     [1m Learning iteration 430/4000 [0m

                       Computation: 3215 steps/s (collection: 0.512s, learning 2.036s)
               Value function loss: 3084.4095
                    Surrogate loss: 0.0102
             Mean action noise std: 0.97
                       Mean reward: 520.50
               Mean episode length: 239.82
                 Mean success rate: 6.50
                  Mean reward/step: 2.69
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3530752
                    Iteration time: 2.55s
                        Total time: 1105.30s
                               ETA: 9155.3s

################################################################################
                     [1m Learning iteration 431/4000 [0m

                       Computation: 3236 steps/s (collection: 0.467s, learning 2.064s)
               Value function loss: 3562.7949
                    Surrogate loss: 0.0103
             Mean action noise std: 0.97
                       Mean reward: 505.22
               Mean episode length: 236.39
                 Mean success rate: 6.00
                  Mean reward/step: 2.64
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 2.53s
                        Total time: 1107.83s
                               ETA: 9152.4s

################################################################################
                     [1m Learning iteration 432/4000 [0m

                       Computation: 3200 steps/s (collection: 0.496s, learning 2.064s)
               Value function loss: 3798.8138
                    Surrogate loss: 0.0138
             Mean action noise std: 0.97
                       Mean reward: 501.44
               Mean episode length: 225.05
                 Mean success rate: 6.50
                  Mean reward/step: 2.83
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3547136
                    Iteration time: 2.56s
                        Total time: 1110.39s
                               ETA: 9149.8s

################################################################################
                     [1m Learning iteration 433/4000 [0m

                       Computation: 3188 steps/s (collection: 0.529s, learning 2.040s)
               Value function loss: 4420.4252
                    Surrogate loss: 0.0117
             Mean action noise std: 0.97
                       Mean reward: 504.62
               Mean episode length: 223.64
                 Mean success rate: 7.50
                  Mean reward/step: 2.77
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 2.57s
                        Total time: 1112.96s
                               ETA: 9147.3s

################################################################################
                     [1m Learning iteration 434/4000 [0m

                       Computation: 3154 steps/s (collection: 0.507s, learning 2.090s)
               Value function loss: 4621.4798
                    Surrogate loss: 0.0129
             Mean action noise std: 0.97
                       Mean reward: 556.78
               Mean episode length: 226.79
                 Mean success rate: 9.50
                  Mean reward/step: 2.58
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3563520
                    Iteration time: 2.60s
                        Total time: 1115.56s
                               ETA: 9145.0s

################################################################################
                     [1m Learning iteration 435/4000 [0m

                       Computation: 3207 steps/s (collection: 0.505s, learning 2.049s)
               Value function loss: 3251.3034
                    Surrogate loss: 0.0144
             Mean action noise std: 0.97
                       Mean reward: 560.18
               Mean episode length: 232.88
                 Mean success rate: 9.00
                  Mean reward/step: 2.58
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 2.55s
                        Total time: 1118.11s
                               ETA: 9142.3s

################################################################################
                     [1m Learning iteration 436/4000 [0m

                       Computation: 3172 steps/s (collection: 0.478s, learning 2.104s)
               Value function loss: 4709.4113
                    Surrogate loss: 0.0114
             Mean action noise std: 0.97
                       Mean reward: 633.45
               Mean episode length: 252.00
                 Mean success rate: 12.50
                  Mean reward/step: 2.54
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3579904
                    Iteration time: 2.58s
                        Total time: 1120.69s
                               ETA: 9139.9s

################################################################################
                     [1m Learning iteration 437/4000 [0m

                       Computation: 3239 steps/s (collection: 0.478s, learning 2.051s)
               Value function loss: 3529.3094
                    Surrogate loss: 0.0108
             Mean action noise std: 0.97
                       Mean reward: 669.39
               Mean episode length: 253.19
                 Mean success rate: 14.00
                  Mean reward/step: 2.66
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 2.53s
                        Total time: 1123.22s
                               ETA: 9137.1s

################################################################################
                     [1m Learning iteration 438/4000 [0m

                       Computation: 3190 steps/s (collection: 0.482s, learning 2.086s)
               Value function loss: 3612.7790
                    Surrogate loss: 0.0132
             Mean action noise std: 0.97
                       Mean reward: 675.42
               Mean episode length: 251.33
                 Mean success rate: 14.50
                  Mean reward/step: 2.53
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 3596288
                    Iteration time: 2.57s
                        Total time: 1125.79s
                               ETA: 9134.5s

################################################################################
                     [1m Learning iteration 439/4000 [0m

                       Computation: 3238 steps/s (collection: 0.440s, learning 2.089s)
               Value function loss: 3240.7410
                    Surrogate loss: 0.0148
             Mean action noise std: 0.97
                       Mean reward: 705.13
               Mean episode length: 259.07
                 Mean success rate: 15.50
                  Mean reward/step: 2.56
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 2.53s
                        Total time: 1128.32s
                               ETA: 9131.7s

################################################################################
                     [1m Learning iteration 440/4000 [0m

                       Computation: 3214 steps/s (collection: 0.468s, learning 2.081s)
               Value function loss: 4508.8633
                    Surrogate loss: 0.0132
             Mean action noise std: 0.97
                       Mean reward: 708.40
               Mean episode length: 265.83
                 Mean success rate: 14.50
                  Mean reward/step: 2.60
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3612672
                    Iteration time: 2.55s
                        Total time: 1130.87s
                               ETA: 9129.0s

################################################################################
                     [1m Learning iteration 441/4000 [0m

                       Computation: 3173 steps/s (collection: 0.526s, learning 2.056s)
               Value function loss: 3410.3512
                    Surrogate loss: 0.0123
             Mean action noise std: 0.97
                       Mean reward: 694.22
               Mean episode length: 254.90
                 Mean success rate: 15.00
                  Mean reward/step: 2.48
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 2.58s
                        Total time: 1133.45s
                               ETA: 9126.6s

################################################################################
                     [1m Learning iteration 442/4000 [0m

                       Computation: 3176 steps/s (collection: 0.517s, learning 2.062s)
               Value function loss: 1996.9489
                    Surrogate loss: 0.0135
             Mean action noise std: 0.97
                       Mean reward: 631.21
               Mean episode length: 232.91
                 Mean success rate: 13.50
                  Mean reward/step: 2.31
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3629056
                    Iteration time: 2.58s
                        Total time: 1136.03s
                               ETA: 9124.1s

################################################################################
                     [1m Learning iteration 443/4000 [0m

                       Computation: 3198 steps/s (collection: 0.521s, learning 2.040s)
               Value function loss: 2604.5309
                    Surrogate loss: 0.0093
             Mean action noise std: 0.97
                       Mean reward: 649.09
               Mean episode length: 241.10
                 Mean success rate: 14.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 2.56s
                        Total time: 1138.59s
                               ETA: 9121.5s

################################################################################
                     [1m Learning iteration 444/4000 [0m

                       Computation: 3220 steps/s (collection: 0.483s, learning 2.061s)
               Value function loss: 2405.3078
                    Surrogate loss: 0.0117
             Mean action noise std: 0.97
                       Mean reward: 631.46
               Mean episode length: 239.54
                 Mean success rate: 13.50
                  Mean reward/step: 2.41
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3645440
                    Iteration time: 2.54s
                        Total time: 1141.13s
                               ETA: 9118.8s

################################################################################
                     [1m Learning iteration 445/4000 [0m

                       Computation: 3129 steps/s (collection: 0.540s, learning 2.077s)
               Value function loss: 3941.0082
                    Surrogate loss: 0.0118
             Mean action noise std: 0.97
                       Mean reward: 589.87
               Mean episode length: 226.40
                 Mean success rate: 13.00
                  Mean reward/step: 2.65
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 2.62s
                        Total time: 1143.75s
                               ETA: 9116.7s

################################################################################
                     [1m Learning iteration 446/4000 [0m

                       Computation: 3235 steps/s (collection: 0.497s, learning 2.036s)
               Value function loss: 4837.3841
                    Surrogate loss: 0.0105
             Mean action noise std: 0.97
                       Mean reward: 552.07
               Mean episode length: 217.76
                 Mean success rate: 12.00
                  Mean reward/step: 2.66
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3661824
                    Iteration time: 2.53s
                        Total time: 1146.28s
                               ETA: 9113.8s

################################################################################
                     [1m Learning iteration 447/4000 [0m

                       Computation: 3196 steps/s (collection: 0.509s, learning 2.054s)
               Value function loss: 3375.3967
                    Surrogate loss: 0.0118
             Mean action noise std: 0.97
                       Mean reward: 562.93
               Mean episode length: 224.03
                 Mean success rate: 12.50
                  Mean reward/step: 2.52
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 2.56s
                        Total time: 1148.84s
                               ETA: 9111.3s

################################################################################
                     [1m Learning iteration 448/4000 [0m

                       Computation: 3229 steps/s (collection: 0.476s, learning 2.061s)
               Value function loss: 2798.9078
                    Surrogate loss: 0.0146
             Mean action noise std: 0.97
                       Mean reward: 553.13
               Mean episode length: 225.87
                 Mean success rate: 11.50
                  Mean reward/step: 2.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3678208
                    Iteration time: 2.54s
                        Total time: 1151.38s
                               ETA: 9108.5s

################################################################################
                     [1m Learning iteration 449/4000 [0m

                       Computation: 3243 steps/s (collection: 0.491s, learning 2.035s)
               Value function loss: 3920.0399
                    Surrogate loss: 0.0117
             Mean action noise std: 0.97
                       Mean reward: 526.51
               Mean episode length: 220.79
                 Mean success rate: 10.00
                  Mean reward/step: 2.71
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 2.53s
                        Total time: 1153.91s
                               ETA: 9105.6s

################################################################################
                     [1m Learning iteration 450/4000 [0m

                       Computation: 3235 steps/s (collection: 0.466s, learning 2.066s)
               Value function loss: 3915.7882
                    Surrogate loss: 0.0147
             Mean action noise std: 0.97
                       Mean reward: 554.27
               Mean episode length: 224.54
                 Mean success rate: 12.00
                  Mean reward/step: 2.61
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 3694592
                    Iteration time: 2.53s
                        Total time: 1156.44s
                               ETA: 9102.8s

################################################################################
                     [1m Learning iteration 451/4000 [0m

                       Computation: 3215 steps/s (collection: 0.517s, learning 2.031s)
               Value function loss: 3308.0821
                    Surrogate loss: 0.0147
             Mean action noise std: 0.97
                       Mean reward: 521.57
               Mean episode length: 216.01
                 Mean success rate: 10.50
                  Mean reward/step: 2.50
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 2.55s
                        Total time: 1158.99s
                               ETA: 9100.1s

################################################################################
                     [1m Learning iteration 452/4000 [0m

                       Computation: 3157 steps/s (collection: 0.511s, learning 2.083s)
               Value function loss: 3358.5046
                    Surrogate loss: 0.0133
             Mean action noise std: 0.97
                       Mean reward: 542.24
               Mean episode length: 218.67
                 Mean success rate: 11.50
                  Mean reward/step: 2.58
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3710976
                    Iteration time: 2.59s
                        Total time: 1161.58s
                               ETA: 9097.8s

################################################################################
                     [1m Learning iteration 453/4000 [0m

                       Computation: 3223 steps/s (collection: 0.499s, learning 2.042s)
               Value function loss: 2607.3386
                    Surrogate loss: 0.0149
             Mean action noise std: 0.97
                       Mean reward: 528.45
               Mean episode length: 205.21
                 Mean success rate: 13.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 2.54s
                        Total time: 1164.12s
                               ETA: 9095.0s

################################################################################
                     [1m Learning iteration 454/4000 [0m

                       Computation: 3306 steps/s (collection: 0.436s, learning 2.042s)
               Value function loss: 1996.0364
                    Surrogate loss: 0.0133
             Mean action noise std: 0.97
                       Mean reward: 528.90
               Mean episode length: 212.53
                 Mean success rate: 12.50
                  Mean reward/step: 2.24
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3727360
                    Iteration time: 2.48s
                        Total time: 1166.60s
                               ETA: 9091.8s

################################################################################
                     [1m Learning iteration 455/4000 [0m

                       Computation: 3260 steps/s (collection: 0.475s, learning 2.038s)
               Value function loss: 2808.1977
                    Surrogate loss: 0.0131
             Mean action noise std: 0.97
                       Mean reward: 536.61
               Mean episode length: 216.44
                 Mean success rate: 12.50
                  Mean reward/step: 2.45
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 2.51s
                        Total time: 1169.11s
                               ETA: 9088.8s

################################################################################
                     [1m Learning iteration 456/4000 [0m

                       Computation: 3210 steps/s (collection: 0.494s, learning 2.057s)
               Value function loss: 2259.4593
                    Surrogate loss: 0.0169
             Mean action noise std: 0.97
                       Mean reward: 530.13
               Mean episode length: 218.65
                 Mean success rate: 12.00
                  Mean reward/step: 2.72
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3743744
                    Iteration time: 2.55s
                        Total time: 1171.67s
                               ETA: 9086.2s

################################################################################
                     [1m Learning iteration 457/4000 [0m

                       Computation: 3163 steps/s (collection: 0.553s, learning 2.037s)
               Value function loss: 2720.5531
                    Surrogate loss: 0.0174
             Mean action noise std: 0.97
                       Mean reward: 527.81
               Mean episode length: 212.91
                 Mean success rate: 13.00
                  Mean reward/step: 2.91
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 2.59s
                        Total time: 1174.25s
                               ETA: 9083.8s

################################################################################
                     [1m Learning iteration 458/4000 [0m

                       Computation: 3273 steps/s (collection: 0.459s, learning 2.043s)
               Value function loss: 4036.8707
                    Surrogate loss: 0.0172
             Mean action noise std: 0.97
                       Mean reward: 501.94
               Mean episode length: 212.35
                 Mean success rate: 10.00
                  Mean reward/step: 3.06
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 3760128
                    Iteration time: 2.50s
                        Total time: 1176.76s
                               ETA: 9080.8s

################################################################################
                     [1m Learning iteration 459/4000 [0m

                       Computation: 3236 steps/s (collection: 0.484s, learning 2.047s)
               Value function loss: 3131.3776
                    Surrogate loss: 0.0145
             Mean action noise std: 0.97
                       Mean reward: 500.21
               Mean episode length: 203.75
                 Mean success rate: 11.00
                  Mean reward/step: 2.80
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 2.53s
                        Total time: 1179.29s
                               ETA: 9078.0s

################################################################################
                     [1m Learning iteration 460/4000 [0m

                       Computation: 3226 steps/s (collection: 0.461s, learning 2.078s)
               Value function loss: 2462.9987
                    Surrogate loss: 0.0148
             Mean action noise std: 0.97
                       Mean reward: 496.58
               Mean episode length: 199.57
                 Mean success rate: 10.50
                  Mean reward/step: 2.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3776512
                    Iteration time: 2.54s
                        Total time: 1181.83s
                               ETA: 9075.2s

################################################################################
                     [1m Learning iteration 461/4000 [0m

                       Computation: 3190 steps/s (collection: 0.510s, learning 2.058s)
               Value function loss: 1982.3086
                    Surrogate loss: 0.0142
             Mean action noise std: 0.97
                       Mean reward: 522.55
               Mean episode length: 202.58
                 Mean success rate: 12.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 2.57s
                        Total time: 1184.39s
                               ETA: 9072.7s

################################################################################
                     [1m Learning iteration 462/4000 [0m

                       Computation: 3171 steps/s (collection: 0.472s, learning 2.111s)
               Value function loss: 2336.4929
                    Surrogate loss: 0.0180
             Mean action noise std: 0.97
                       Mean reward: 523.78
               Mean episode length: 197.04
                 Mean success rate: 11.00
                  Mean reward/step: 2.29
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 3792896
                    Iteration time: 2.58s
                        Total time: 1186.98s
                               ETA: 9070.3s

################################################################################
                     [1m Learning iteration 463/4000 [0m

                       Computation: 3165 steps/s (collection: 0.491s, learning 2.098s)
               Value function loss: 2556.1933
                    Surrogate loss: 0.0171
             Mean action noise std: 0.97
                       Mean reward: 522.67
               Mean episode length: 191.28
                 Mean success rate: 12.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 2.59s
                        Total time: 1189.57s
                               ETA: 9067.9s

################################################################################
                     [1m Learning iteration 464/4000 [0m

                       Computation: 3235 steps/s (collection: 0.501s, learning 2.031s)
               Value function loss: 1671.0444
                    Surrogate loss: 0.0145
             Mean action noise std: 0.97
                       Mean reward: 547.86
               Mean episode length: 198.15
                 Mean success rate: 14.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3809280
                    Iteration time: 2.53s
                        Total time: 1192.10s
                               ETA: 9065.1s

################################################################################
                     [1m Learning iteration 465/4000 [0m

                       Computation: 3235 steps/s (collection: 0.472s, learning 2.060s)
               Value function loss: 2731.1636
                    Surrogate loss: 0.0138
             Mean action noise std: 0.97
                       Mean reward: 536.91
               Mean episode length: 197.29
                 Mean success rate: 14.50
                  Mean reward/step: 2.54
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 2.53s
                        Total time: 1194.63s
                               ETA: 9062.3s

################################################################################
                     [1m Learning iteration 466/4000 [0m

                       Computation: 3224 steps/s (collection: 0.458s, learning 2.083s)
               Value function loss: 2041.5971
                    Surrogate loss: 0.0144
             Mean action noise std: 0.97
                       Mean reward: 493.77
               Mean episode length: 192.20
                 Mean success rate: 12.00
                  Mean reward/step: 2.48
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3825664
                    Iteration time: 2.54s
                        Total time: 1197.17s
                               ETA: 9059.5s

################################################################################
                     [1m Learning iteration 467/4000 [0m

                       Computation: 3159 steps/s (collection: 0.499s, learning 2.094s)
               Value function loss: 2852.0080
                    Surrogate loss: 0.0149
             Mean action noise std: 0.97
                       Mean reward: 500.20
               Mean episode length: 206.64
                 Mean success rate: 11.50
                  Mean reward/step: 2.57
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 2.59s
                        Total time: 1199.76s
                               ETA: 9057.2s

################################################################################
                     [1m Learning iteration 468/4000 [0m

                       Computation: 3145 steps/s (collection: 0.532s, learning 2.072s)
               Value function loss: 3046.1603
                    Surrogate loss: 0.0165
             Mean action noise std: 0.97
                       Mean reward: 494.13
               Mean episode length: 203.96
                 Mean success rate: 11.00
                  Mean reward/step: 2.69
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3842048
                    Iteration time: 2.60s
                        Total time: 1202.37s
                               ETA: 9054.9s

################################################################################
                     [1m Learning iteration 469/4000 [0m

                       Computation: 3214 steps/s (collection: 0.487s, learning 2.062s)
               Value function loss: 2033.1598
                    Surrogate loss: 0.0172
             Mean action noise std: 0.97
                       Mean reward: 517.27
               Mean episode length: 211.55
                 Mean success rate: 11.50
                  Mean reward/step: 2.44
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 2.55s
                        Total time: 1204.92s
                               ETA: 9052.3s

################################################################################
                     [1m Learning iteration 470/4000 [0m

                       Computation: 3222 steps/s (collection: 0.487s, learning 2.055s)
               Value function loss: 2382.7009
                    Surrogate loss: 0.0197
             Mean action noise std: 0.97
                       Mean reward: 559.70
               Mean episode length: 221.53
                 Mean success rate: 12.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3858432
                    Iteration time: 2.54s
                        Total time: 1207.46s
                               ETA: 9049.5s

################################################################################
                     [1m Learning iteration 471/4000 [0m

                       Computation: 3160 steps/s (collection: 0.507s, learning 2.085s)
               Value function loss: 2979.0748
                    Surrogate loss: 0.0170
             Mean action noise std: 0.97
                       Mean reward: 523.68
               Mean episode length: 214.29
                 Mean success rate: 10.00
                  Mean reward/step: 2.56
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 2.59s
                        Total time: 1210.05s
                               ETA: 9047.2s

################################################################################
                     [1m Learning iteration 472/4000 [0m

                       Computation: 3257 steps/s (collection: 0.449s, learning 2.067s)
               Value function loss: 2309.5104
                    Surrogate loss: 0.0173
             Mean action noise std: 0.97
                       Mean reward: 537.92
               Mean episode length: 211.94
                 Mean success rate: 12.00
                  Mean reward/step: 2.50
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3874816
                    Iteration time: 2.52s
                        Total time: 1212.57s
                               ETA: 9044.2s

################################################################################
                     [1m Learning iteration 473/4000 [0m

                       Computation: 3224 steps/s (collection: 0.442s, learning 2.098s)
               Value function loss: 2804.1249
                    Surrogate loss: 0.0151
             Mean action noise std: 0.97
                       Mean reward: 522.24
               Mean episode length: 217.48
                 Mean success rate: 10.50
                  Mean reward/step: 2.63
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 2.54s
                        Total time: 1215.11s
                               ETA: 9041.5s

################################################################################
                     [1m Learning iteration 474/4000 [0m

                       Computation: 3210 steps/s (collection: 0.484s, learning 2.067s)
               Value function loss: 2019.6093
                    Surrogate loss: 0.0195
             Mean action noise std: 0.97
                       Mean reward: 484.04
               Mean episode length: 208.50
                 Mean success rate: 9.00
                  Mean reward/step: 2.57
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3891200
                    Iteration time: 2.55s
                        Total time: 1217.66s
                               ETA: 9038.9s

################################################################################
                     [1m Learning iteration 475/4000 [0m

                       Computation: 3161 steps/s (collection: 0.519s, learning 2.072s)
               Value function loss: 2299.3623
                    Surrogate loss: 0.0162
             Mean action noise std: 0.97
                       Mean reward: 454.86
               Mean episode length: 204.69
                 Mean success rate: 7.50
                  Mean reward/step: 2.39
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 2.59s
                        Total time: 1220.25s
                               ETA: 9036.5s

################################################################################
                     [1m Learning iteration 476/4000 [0m

                       Computation: 3161 steps/s (collection: 0.521s, learning 2.070s)
               Value function loss: 2520.0026
                    Surrogate loss: 0.0152
             Mean action noise std: 0.97
                       Mean reward: 450.83
               Mean episode length: 196.84
                 Mean success rate: 8.00
                  Mean reward/step: 2.53
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3907584
                    Iteration time: 2.59s
                        Total time: 1222.84s
                               ETA: 9034.1s

################################################################################
                     [1m Learning iteration 477/4000 [0m

                       Computation: 3181 steps/s (collection: 0.480s, learning 2.095s)
               Value function loss: 2161.9977
                    Surrogate loss: 0.0173
             Mean action noise std: 0.97
                       Mean reward: 444.71
               Mean episode length: 193.36
                 Mean success rate: 9.00
                  Mean reward/step: 2.57
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 2.57s
                        Total time: 1225.41s
                               ETA: 9031.7s

################################################################################
                     [1m Learning iteration 478/4000 [0m

                       Computation: 3069 steps/s (collection: 0.491s, learning 2.177s)
               Value function loss: 3361.2920
                    Surrogate loss: 0.0165
             Mean action noise std: 0.97
                       Mean reward: 517.84
               Mean episode length: 192.22
                 Mean success rate: 11.00
                  Mean reward/step: 2.79
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3923968
                    Iteration time: 2.67s
                        Total time: 1228.08s
                               ETA: 9029.9s

################################################################################
                     [1m Learning iteration 479/4000 [0m

                       Computation: 2995 steps/s (collection: 0.551s, learning 2.184s)
               Value function loss: 2641.1807
                    Surrogate loss: 0.0168
             Mean action noise std: 0.97
                       Mean reward: 499.43
               Mean episode length: 186.68
                 Mean success rate: 10.50
                  Mean reward/step: 2.64
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 2.74s
                        Total time: 1230.82s
                               ETA: 9028.6s

################################################################################
                     [1m Learning iteration 480/4000 [0m

                       Computation: 3175 steps/s (collection: 0.491s, learning 2.089s)
               Value function loss: 3615.2935
                    Surrogate loss: 0.0164
             Mean action noise std: 0.97
                       Mean reward: 523.71
               Mean episode length: 193.94
                 Mean success rate: 11.50
                  Mean reward/step: 2.85
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3940352
                    Iteration time: 2.58s
                        Total time: 1233.40s
                               ETA: 9026.1s

################################################################################
                     [1m Learning iteration 481/4000 [0m

                       Computation: 3174 steps/s (collection: 0.480s, learning 2.101s)
               Value function loss: 3085.3449
                    Surrogate loss: 0.0169
             Mean action noise std: 0.97
                       Mean reward: 521.20
               Mean episode length: 197.31
                 Mean success rate: 10.50
                  Mean reward/step: 2.69
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 2.58s
                        Total time: 1235.98s
                               ETA: 9023.7s

################################################################################
                     [1m Learning iteration 482/4000 [0m

                       Computation: 3072 steps/s (collection: 0.517s, learning 2.149s)
               Value function loss: 2177.3064
                    Surrogate loss: 0.0206
             Mean action noise std: 0.97
                       Mean reward: 517.79
               Mean episode length: 205.59
                 Mean success rate: 8.00
                  Mean reward/step: 2.50
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3956736
                    Iteration time: 2.67s
                        Total time: 1238.64s
                               ETA: 9021.8s

################################################################################
                     [1m Learning iteration 483/4000 [0m

                       Computation: 3145 steps/s (collection: 0.496s, learning 2.108s)
               Value function loss: 2391.3544
                    Surrogate loss: 0.0200
             Mean action noise std: 0.97
                       Mean reward: 435.39
               Mean episode length: 197.13
                 Mean success rate: 6.00
                  Mean reward/step: 2.49
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 2.60s
                        Total time: 1241.25s
                               ETA: 9019.6s

################################################################################
                     [1m Learning iteration 484/4000 [0m

                       Computation: 3167 steps/s (collection: 0.467s, learning 2.119s)
               Value function loss: 1993.8232
                    Surrogate loss: 0.0173
             Mean action noise std: 0.97
                       Mean reward: 446.07
               Mean episode length: 196.22
                 Mean success rate: 6.50
                  Mean reward/step: 2.40
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3973120
                    Iteration time: 2.59s
                        Total time: 1243.84s
                               ETA: 9017.2s

################################################################################
                     [1m Learning iteration 485/4000 [0m

                       Computation: 3109 steps/s (collection: 0.509s, learning 2.126s)
               Value function loss: 1778.2438
                    Surrogate loss: 0.0197
             Mean action noise std: 0.97
                       Mean reward: 467.46
               Mean episode length: 204.07
                 Mean success rate: 7.00
                  Mean reward/step: 2.39
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 2.63s
                        Total time: 1246.47s
                               ETA: 9015.1s

################################################################################
                     [1m Learning iteration 486/4000 [0m

                       Computation: 3065 steps/s (collection: 0.566s, learning 2.106s)
               Value function loss: 2060.0367
                    Surrogate loss: 0.0163
             Mean action noise std: 0.97
                       Mean reward: 455.41
               Mean episode length: 195.09
                 Mean success rate: 6.50
                  Mean reward/step: 2.29
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 3989504
                    Iteration time: 2.67s
                        Total time: 1249.14s
                               ETA: 9013.3s

################################################################################
                     [1m Learning iteration 487/4000 [0m

                       Computation: 3115 steps/s (collection: 0.520s, learning 2.110s)
               Value function loss: 3086.4738
                    Surrogate loss: 0.0151
             Mean action noise std: 0.97
                       Mean reward: 528.81
               Mean episode length: 198.00
                 Mean success rate: 10.50
                  Mean reward/step: 2.50
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 2.63s
                        Total time: 1251.77s
                               ETA: 9011.2s

################################################################################
                     [1m Learning iteration 488/4000 [0m

                       Computation: 3205 steps/s (collection: 0.517s, learning 2.039s)
               Value function loss: 1986.2025
                    Surrogate loss: 0.0194
             Mean action noise std: 0.97
                       Mean reward: 536.89
               Mean episode length: 202.53
                 Mean success rate: 11.00
                  Mean reward/step: 2.31
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 4005888
                    Iteration time: 2.56s
                        Total time: 1254.33s
                               ETA: 9008.6s

################################################################################
                     [1m Learning iteration 489/4000 [0m

                       Computation: 3249 steps/s (collection: 0.447s, learning 2.074s)
               Value function loss: 3037.2272
                    Surrogate loss: 0.0124
             Mean action noise std: 0.97
                       Mean reward: 558.04
               Mean episode length: 205.81
                 Mean success rate: 11.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 2.52s
                        Total time: 1256.85s
                               ETA: 9005.7s

################################################################################
                     [1m Learning iteration 490/4000 [0m

                       Computation: 3197 steps/s (collection: 0.465s, learning 2.097s)
               Value function loss: 2977.8458
                    Surrogate loss: 0.0141
             Mean action noise std: 0.97
                       Mean reward: 593.16
               Mean episode length: 212.34
                 Mean success rate: 11.50
                  Mean reward/step: 2.64
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4022272
                    Iteration time: 2.56s
                        Total time: 1259.41s
                               ETA: 9003.1s

################################################################################
                     [1m Learning iteration 491/4000 [0m

                       Computation: 3226 steps/s (collection: 0.505s, learning 2.034s)
               Value function loss: 3423.1172
                    Surrogate loss: 0.0148
             Mean action noise std: 0.97
                       Mean reward: 513.95
               Mean episode length: 206.99
                 Mean success rate: 8.50
                  Mean reward/step: 2.92
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 2.54s
                        Total time: 1261.95s
                               ETA: 9000.4s

################################################################################
                     [1m Learning iteration 492/4000 [0m

                       Computation: 3256 steps/s (collection: 0.475s, learning 2.040s)
               Value function loss: 4269.8982
                    Surrogate loss: 0.0140
             Mean action noise std: 0.97
                       Mean reward: 533.32
               Mean episode length: 205.15
                 Mean success rate: 8.50
                  Mean reward/step: 3.11
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4038656
                    Iteration time: 2.52s
                        Total time: 1264.47s
                               ETA: 8997.5s

################################################################################
                     [1m Learning iteration 493/4000 [0m

                       Computation: 3279 steps/s (collection: 0.448s, learning 2.049s)
               Value function loss: 4022.3033
                    Surrogate loss: 0.0156
             Mean action noise std: 0.97
                       Mean reward: 535.20
               Mean episode length: 203.66
                 Mean success rate: 8.00
                  Mean reward/step: 3.21
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 2.50s
                        Total time: 1266.96s
                               ETA: 8994.4s

################################################################################
                     [1m Learning iteration 494/4000 [0m

                       Computation: 3245 steps/s (collection: 0.480s, learning 2.044s)
               Value function loss: 5280.9458
                    Surrogate loss: 0.0142
             Mean action noise std: 0.97
                       Mean reward: 516.69
               Mean episode length: 208.59
                 Mean success rate: 7.00
                  Mean reward/step: 3.37
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4055040
                    Iteration time: 2.52s
                        Total time: 1269.49s
                               ETA: 8991.6s

################################################################################
                     [1m Learning iteration 495/4000 [0m

                       Computation: 3206 steps/s (collection: 0.499s, learning 2.056s)
               Value function loss: 4276.2274
                    Surrogate loss: 0.0127
             Mean action noise std: 0.97
                       Mean reward: 532.17
               Mean episode length: 208.53
                 Mean success rate: 8.50
                  Mean reward/step: 3.48
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 2.55s
                        Total time: 1272.04s
                               ETA: 8988.9s

################################################################################
                     [1m Learning iteration 496/4000 [0m

                       Computation: 3254 steps/s (collection: 0.457s, learning 2.060s)
               Value function loss: 3255.0392
                    Surrogate loss: 0.0149
             Mean action noise std: 0.96
                       Mean reward: 520.97
               Mean episode length: 205.96
                 Mean success rate: 8.50
                  Mean reward/step: 3.17
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4071424
                    Iteration time: 2.52s
                        Total time: 1274.56s
                               ETA: 8986.0s

################################################################################
                     [1m Learning iteration 497/4000 [0m

                       Computation: 3303 steps/s (collection: 0.444s, learning 2.036s)
               Value function loss: 4688.4764
                    Surrogate loss: 0.0134
             Mean action noise std: 0.96
                       Mean reward: 529.01
               Mean episode length: 207.31
                 Mean success rate: 8.50
                  Mean reward/step: 3.11
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 2.48s
                        Total time: 1277.04s
                               ETA: 8982.9s

################################################################################
                     [1m Learning iteration 498/4000 [0m

                       Computation: 3238 steps/s (collection: 0.486s, learning 2.044s)
               Value function loss: 4869.7081
                    Surrogate loss: 0.0120
             Mean action noise std: 0.97
                       Mean reward: 535.00
               Mean episode length: 201.99
                 Mean success rate: 10.50
                  Mean reward/step: 3.33
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4087808
                    Iteration time: 2.53s
                        Total time: 1279.57s
                               ETA: 8980.1s

################################################################################
                     [1m Learning iteration 499/4000 [0m

                       Computation: 3214 steps/s (collection: 0.483s, learning 2.066s)
               Value function loss: 4474.6067
                    Surrogate loss: 0.0108
             Mean action noise std: 0.96
                       Mean reward: 548.87
               Mean episode length: 201.09
                 Mean success rate: 11.00
                  Mean reward/step: 3.06
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 2.55s
                        Total time: 1282.12s
                               ETA: 8977.4s

################################################################################
                     [1m Learning iteration 500/4000 [0m

                       Computation: 3221 steps/s (collection: 0.500s, learning 2.043s)
               Value function loss: 4243.5283
                    Surrogate loss: 0.0160
             Mean action noise std: 0.96
                       Mean reward: 546.93
               Mean episode length: 199.91
                 Mean success rate: 12.00
                  Mean reward/step: 2.91
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 4104192
                    Iteration time: 2.54s
                        Total time: 1284.66s
                               ETA: 8974.7s

################################################################################
                     [1m Learning iteration 501/4000 [0m

                       Computation: 3161 steps/s (collection: 0.476s, learning 2.115s)
               Value function loss: 3618.6559
                    Surrogate loss: 0.0152
             Mean action noise std: 0.96
                       Mean reward: 614.03
               Mean episode length: 209.06
                 Mean success rate: 15.00
                  Mean reward/step: 2.84
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 2.59s
                        Total time: 1287.25s
                               ETA: 8972.3s

################################################################################
                     [1m Learning iteration 502/4000 [0m

                       Computation: 3088 steps/s (collection: 0.524s, learning 2.128s)
               Value function loss: 4264.5177
                    Surrogate loss: 0.0138
             Mean action noise std: 0.96
                       Mean reward: 620.20
               Mean episode length: 210.21
                 Mean success rate: 14.50
                  Mean reward/step: 3.16
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4120576
                    Iteration time: 2.65s
                        Total time: 1289.90s
                               ETA: 8970.3s

################################################################################
                     [1m Learning iteration 503/4000 [0m

                       Computation: 3156 steps/s (collection: 0.572s, learning 2.024s)
               Value function loss: 4384.5218
                    Surrogate loss: 0.0140
             Mean action noise std: 0.96
                       Mean reward: 638.75
               Mean episode length: 212.22
                 Mean success rate: 15.00
                  Mean reward/step: 3.21
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 2.60s
                        Total time: 1292.50s
                               ETA: 8968.0s

################################################################################
                     [1m Learning iteration 504/4000 [0m

                       Computation: 3174 steps/s (collection: 0.513s, learning 2.068s)
               Value function loss: 3885.6159
                    Surrogate loss: 0.0144
             Mean action noise std: 0.96
                       Mean reward: 723.59
               Mean episode length: 230.09
                 Mean success rate: 17.50
                  Mean reward/step: 3.04
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4136960
                    Iteration time: 2.58s
                        Total time: 1295.08s
                               ETA: 8965.5s

################################################################################
                     [1m Learning iteration 505/4000 [0m

                       Computation: 3204 steps/s (collection: 0.467s, learning 2.089s)
               Value function loss: 5001.6591
                    Surrogate loss: 0.0130
             Mean action noise std: 0.96
                       Mean reward: 758.28
               Mean episode length: 241.61
                 Mean success rate: 17.50
                  Mean reward/step: 3.00
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 2.56s
                        Total time: 1297.64s
                               ETA: 8962.9s

################################################################################
                     [1m Learning iteration 506/4000 [0m

                       Computation: 3148 steps/s (collection: 0.471s, learning 2.131s)
               Value function loss: 6627.5542
                    Surrogate loss: 0.0097
             Mean action noise std: 0.96
                       Mean reward: 788.91
               Mean episode length: 242.66
                 Mean success rate: 16.50
                  Mean reward/step: 3.01
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 4153344
                    Iteration time: 2.60s
                        Total time: 1300.24s
                               ETA: 8960.6s

################################################################################
                     [1m Learning iteration 507/4000 [0m

                       Computation: 3142 steps/s (collection: 0.488s, learning 2.119s)
               Value function loss: 4160.0570
                    Surrogate loss: 0.0136
             Mean action noise std: 0.96
                       Mean reward: 785.29
               Mean episode length: 238.14
                 Mean success rate: 16.50
                  Mean reward/step: 3.02
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 2.61s
                        Total time: 1302.85s
                               ETA: 8958.3s

################################################################################
                     [1m Learning iteration 508/4000 [0m

                       Computation: 3177 steps/s (collection: 0.485s, learning 2.093s)
               Value function loss: 4695.6600
                    Surrogate loss: 0.0136
             Mean action noise std: 0.96
                       Mean reward: 722.26
               Mean episode length: 222.87
                 Mean success rate: 14.50
                  Mean reward/step: 3.21
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4169728
                    Iteration time: 2.58s
                        Total time: 1305.42s
                               ETA: 8955.9s

################################################################################
                     [1m Learning iteration 509/4000 [0m

                       Computation: 3126 steps/s (collection: 0.468s, learning 2.152s)
               Value function loss: 4867.9790
                    Surrogate loss: 0.0155
             Mean action noise std: 0.96
                       Mean reward: 688.67
               Mean episode length: 218.06
                 Mean success rate: 13.50
                  Mean reward/step: 3.35
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 2.62s
                        Total time: 1308.04s
                               ETA: 8953.7s

################################################################################
                     [1m Learning iteration 510/4000 [0m

                       Computation: 3059 steps/s (collection: 0.549s, learning 2.129s)
               Value function loss: 4709.4164
                    Surrogate loss: 0.0142
             Mean action noise std: 0.96
                       Mean reward: 646.45
               Mean episode length: 204.76
                 Mean success rate: 12.50
                  Mean reward/step: 3.48
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4186112
                    Iteration time: 2.68s
                        Total time: 1310.72s
                               ETA: 8951.9s

################################################################################
                     [1m Learning iteration 511/4000 [0m

                       Computation: 3193 steps/s (collection: 0.514s, learning 2.051s)
               Value function loss: 4938.2935
                    Surrogate loss: 0.0126
             Mean action noise std: 0.96
                       Mean reward: 638.98
               Mean episode length: 196.06
                 Mean success rate: 14.00
                  Mean reward/step: 3.52
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 2.57s
                        Total time: 1313.29s
                               ETA: 8949.3s

################################################################################
                     [1m Learning iteration 512/4000 [0m

                       Computation: 3213 steps/s (collection: 0.474s, learning 2.076s)
               Value function loss: 5361.7818
                    Surrogate loss: 0.0145
             Mean action noise std: 0.96
                       Mean reward: 659.51
               Mean episode length: 198.45
                 Mean success rate: 15.50
                  Mean reward/step: 3.14
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4202496
                    Iteration time: 2.55s
                        Total time: 1315.84s
                               ETA: 8946.7s

################################################################################
                     [1m Learning iteration 513/4000 [0m

                       Computation: 3174 steps/s (collection: 0.486s, learning 2.095s)
               Value function loss: 4706.4194
                    Surrogate loss: 0.0140
             Mean action noise std: 0.96
                       Mean reward: 611.97
               Mean episode length: 198.33
                 Mean success rate: 14.00
                  Mean reward/step: 3.19
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 2.58s
                        Total time: 1318.42s
                               ETA: 8944.2s

################################################################################
                     [1m Learning iteration 514/4000 [0m

                       Computation: 3167 steps/s (collection: 0.500s, learning 2.086s)
               Value function loss: 4370.7074
                    Surrogate loss: 0.0140
             Mean action noise std: 0.96
                       Mean reward: 563.51
               Mean episode length: 198.14
                 Mean success rate: 13.00
                  Mean reward/step: 3.17
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 4218880
                    Iteration time: 2.59s
                        Total time: 1321.00s
                               ETA: 8941.8s

################################################################################
                     [1m Learning iteration 515/4000 [0m

                       Computation: 3209 steps/s (collection: 0.500s, learning 2.052s)
               Value function loss: 6958.0744
                    Surrogate loss: 0.0108
             Mean action noise std: 0.96
                       Mean reward: 619.36
               Mean episode length: 204.16
                 Mean success rate: 14.00
                  Mean reward/step: 3.30
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 2.55s
                        Total time: 1323.56s
                               ETA: 8939.1s

################################################################################
                     [1m Learning iteration 516/4000 [0m

                       Computation: 3231 steps/s (collection: 0.442s, learning 2.093s)
               Value function loss: 6048.2316
                    Surrogate loss: 0.0161
             Mean action noise std: 0.96
                       Mean reward: 665.05
               Mean episode length: 214.52
                 Mean success rate: 15.00
                  Mean reward/step: 3.32
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4235264
                    Iteration time: 2.54s
                        Total time: 1326.09s
                               ETA: 8936.4s

################################################################################
                     [1m Learning iteration 517/4000 [0m

                       Computation: 3250 steps/s (collection: 0.479s, learning 2.042s)
               Value function loss: 5813.2898
                    Surrogate loss: 0.0148
             Mean action noise std: 0.96
                       Mean reward: 673.40
               Mean episode length: 219.56
                 Mean success rate: 15.50
                  Mean reward/step: 3.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 2.52s
                        Total time: 1328.61s
                               ETA: 8933.5s

################################################################################
                     [1m Learning iteration 518/4000 [0m

                       Computation: 3203 steps/s (collection: 0.496s, learning 2.061s)
               Value function loss: 6999.6455
                    Surrogate loss: 0.0143
             Mean action noise std: 0.96
                       Mean reward: 681.82
               Mean episode length: 213.35
                 Mean success rate: 16.00
                  Mean reward/step: 3.75
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 4251648
                    Iteration time: 2.56s
                        Total time: 1331.17s
                               ETA: 8930.9s

################################################################################
                     [1m Learning iteration 519/4000 [0m

                       Computation: 3198 steps/s (collection: 0.486s, learning 2.075s)
               Value function loss: 7367.3649
                    Surrogate loss: 0.0155
             Mean action noise std: 0.96
                       Mean reward: 766.08
               Mean episode length: 216.47
                 Mean success rate: 18.00
                  Mean reward/step: 3.61
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 2.56s
                        Total time: 1333.73s
                               ETA: 8928.3s

################################################################################
                     [1m Learning iteration 520/4000 [0m

                       Computation: 3175 steps/s (collection: 0.486s, learning 2.094s)
               Value function loss: 7928.8355
                    Surrogate loss: 0.0108
             Mean action noise std: 0.96
                       Mean reward: 741.01
               Mean episode length: 208.76
                 Mean success rate: 18.50
                  Mean reward/step: 3.64
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4268032
                    Iteration time: 2.58s
                        Total time: 1336.31s
                               ETA: 8925.8s

################################################################################
                     [1m Learning iteration 521/4000 [0m

                       Computation: 3222 steps/s (collection: 0.489s, learning 2.053s)
               Value function loss: 7701.7586
                    Surrogate loss: 0.0125
             Mean action noise std: 0.96
                       Mean reward: 736.45
               Mean episode length: 206.78
                 Mean success rate: 18.00
                  Mean reward/step: 3.83
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 2.54s
                        Total time: 1338.85s
                               ETA: 8923.1s

################################################################################
                     [1m Learning iteration 522/4000 [0m

                       Computation: 3163 steps/s (collection: 0.483s, learning 2.107s)
               Value function loss: 5999.8358
                    Surrogate loss: 0.0144
             Mean action noise std: 0.96
                       Mean reward: 731.91
               Mean episode length: 210.52
                 Mean success rate: 18.00
                  Mean reward/step: 3.31
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4284416
                    Iteration time: 2.59s
                        Total time: 1341.44s
                               ETA: 8920.7s

################################################################################
                     [1m Learning iteration 523/4000 [0m

                       Computation: 3272 steps/s (collection: 0.479s, learning 2.025s)
               Value function loss: 4395.1465
                    Surrogate loss: 0.0151
             Mean action noise std: 0.96
                       Mean reward: 681.98
               Mean episode length: 204.53
                 Mean success rate: 16.50
                  Mean reward/step: 3.46
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 2.50s
                        Total time: 1343.94s
                               ETA: 8917.7s

################################################################################
                     [1m Learning iteration 524/4000 [0m

                       Computation: 3182 steps/s (collection: 0.477s, learning 2.098s)
               Value function loss: 7094.4687
                    Surrogate loss: 0.0128
             Mean action noise std: 0.96
                       Mean reward: 735.56
               Mean episode length: 214.56
                 Mean success rate: 17.50
                  Mean reward/step: 3.63
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4300800
                    Iteration time: 2.57s
                        Total time: 1346.52s
                               ETA: 8915.2s

################################################################################
                     [1m Learning iteration 525/4000 [0m

                       Computation: 3145 steps/s (collection: 0.488s, learning 2.116s)
               Value function loss: 5852.9570
                    Surrogate loss: 0.0145
             Mean action noise std: 0.96
                       Mean reward: 760.95
               Mean episode length: 218.96
                 Mean success rate: 18.00
                  Mean reward/step: 3.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 2.60s
                        Total time: 1349.12s
                               ETA: 8912.9s

################################################################################
                     [1m Learning iteration 526/4000 [0m

                       Computation: 3144 steps/s (collection: 0.503s, learning 2.103s)
               Value function loss: 6752.7364
                    Surrogate loss: 0.0135
             Mean action noise std: 0.96
                       Mean reward: 727.50
               Mean episode length: 219.72
                 Mean success rate: 15.00
                  Mean reward/step: 3.64
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 4317184
                    Iteration time: 2.61s
                        Total time: 1351.73s
                               ETA: 8910.6s

################################################################################
                     [1m Learning iteration 527/4000 [0m

                       Computation: 3257 steps/s (collection: 0.479s, learning 2.036s)
               Value function loss: 4906.2506
                    Surrogate loss: 0.0132
             Mean action noise std: 0.96
                       Mean reward: 757.88
               Mean episode length: 223.26
                 Mean success rate: 14.50
                  Mean reward/step: 3.67
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 2.51s
                        Total time: 1354.24s
                               ETA: 8907.7s

################################################################################
                     [1m Learning iteration 528/4000 [0m

                       Computation: 3194 steps/s (collection: 0.467s, learning 2.097s)
               Value function loss: 9016.1820
                    Surrogate loss: 0.0090
             Mean action noise std: 0.96
                       Mean reward: 805.35
               Mean episode length: 222.59
                 Mean success rate: 15.50
                  Mean reward/step: 3.26
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4333568
                    Iteration time: 2.56s
                        Total time: 1356.81s
                               ETA: 8905.2s

################################################################################
                     [1m Learning iteration 529/4000 [0m

                       Computation: 3107 steps/s (collection: 0.477s, learning 2.160s)
               Value function loss: 5577.2616
                    Surrogate loss: 0.0115
             Mean action noise std: 0.96
                       Mean reward: 796.87
               Mean episode length: 217.83
                 Mean success rate: 17.00
                  Mean reward/step: 2.85
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 2.64s
                        Total time: 1359.44s
                               ETA: 8903.1s

################################################################################
                     [1m Learning iteration 530/4000 [0m

                       Computation: 3119 steps/s (collection: 0.509s, learning 2.117s)
               Value function loss: 5580.5722
                    Surrogate loss: 0.0138
             Mean action noise std: 0.96
                       Mean reward: 785.81
               Mean episode length: 214.75
                 Mean success rate: 18.00
                  Mean reward/step: 3.13
       Mean episode length/episode: 25.68
--------------------------------------------------------------------------------
                   Total timesteps: 4349952
                    Iteration time: 2.63s
                        Total time: 1362.07s
                               ETA: 8900.9s

################################################################################
                     [1m Learning iteration 531/4000 [0m

                       Computation: 3222 steps/s (collection: 0.485s, learning 2.058s)
               Value function loss: 4738.4430
                    Surrogate loss: 0.0125
             Mean action noise std: 0.96
                       Mean reward: 787.95
               Mean episode length: 220.41
                 Mean success rate: 20.00
                  Mean reward/step: 3.28
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 2.54s
                        Total time: 1364.61s
                               ETA: 8898.2s

################################################################################
                     [1m Learning iteration 532/4000 [0m

                       Computation: 3222 steps/s (collection: 0.462s, learning 2.080s)
               Value function loss: 6704.8013
                    Surrogate loss: 0.0137
             Mean action noise std: 0.96
                       Mean reward: 787.77
               Mean episode length: 217.66
                 Mean success rate: 20.00
                  Mean reward/step: 3.12
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4366336
                    Iteration time: 2.54s
                        Total time: 1367.15s
                               ETA: 8895.5s

################################################################################
                     [1m Learning iteration 533/4000 [0m

                       Computation: 3143 steps/s (collection: 0.511s, learning 2.095s)
               Value function loss: 4137.8190
                    Surrogate loss: 0.0136
             Mean action noise std: 0.96
                       Mean reward: 777.53
               Mean episode length: 217.04
                 Mean success rate: 19.00
                  Mean reward/step: 3.45
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 2.61s
                        Total time: 1369.76s
                               ETA: 8893.2s

################################################################################
                     [1m Learning iteration 534/4000 [0m

                       Computation: 3172 steps/s (collection: 0.500s, learning 2.083s)
               Value function loss: 8339.1292
                    Surrogate loss: 0.0126
             Mean action noise std: 0.96
                       Mean reward: 783.11
               Mean episode length: 218.50
                 Mean success rate: 17.50
                  Mean reward/step: 3.93
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4382720
                    Iteration time: 2.58s
                        Total time: 1372.34s
                               ETA: 8890.7s

################################################################################
                     [1m Learning iteration 535/4000 [0m

                       Computation: 3223 steps/s (collection: 0.491s, learning 2.050s)
               Value function loss: 5726.7522
                    Surrogate loss: 0.0141
             Mean action noise std: 0.96
                       Mean reward: 758.39
               Mean episode length: 217.84
                 Mean success rate: 15.50
                  Mean reward/step: 3.84
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 2.54s
                        Total time: 1374.88s
                               ETA: 8888.0s

################################################################################
                     [1m Learning iteration 536/4000 [0m

                       Computation: 3202 steps/s (collection: 0.504s, learning 2.055s)
               Value function loss: 5471.2580
                    Surrogate loss: 0.0152
             Mean action noise std: 0.96
                       Mean reward: 706.98
               Mean episode length: 218.31
                 Mean success rate: 12.50
                  Mean reward/step: 3.89
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4399104
                    Iteration time: 2.56s
                        Total time: 1377.44s
                               ETA: 8885.4s

################################################################################
                     [1m Learning iteration 537/4000 [0m

                       Computation: 3200 steps/s (collection: 0.482s, learning 2.078s)
               Value function loss: 10774.3635
                    Surrogate loss: 0.0131
             Mean action noise std: 0.96
                       Mean reward: 721.01
               Mean episode length: 225.72
                 Mean success rate: 11.50
                  Mean reward/step: 3.87
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 2.56s
                        Total time: 1380.00s
                               ETA: 8882.8s

################################################################################
                     [1m Learning iteration 538/4000 [0m

                       Computation: 3230 steps/s (collection: 0.480s, learning 2.056s)
               Value function loss: 8646.5502
                    Surrogate loss: 0.0124
             Mean action noise std: 0.96
                       Mean reward: 728.03
               Mean episode length: 219.03
                 Mean success rate: 12.00
                  Mean reward/step: 3.67
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4415488
                    Iteration time: 2.54s
                        Total time: 1382.54s
                               ETA: 8880.0s

################################################################################
                     [1m Learning iteration 539/4000 [0m

                       Computation: 3183 steps/s (collection: 0.519s, learning 2.055s)
               Value function loss: 5357.0212
                    Surrogate loss: 0.0168
             Mean action noise std: 0.96
                       Mean reward: 656.89
               Mean episode length: 210.43
                 Mean success rate: 10.50
                  Mean reward/step: 3.72
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 2.57s
                        Total time: 1385.11s
                               ETA: 8877.5s

################################################################################
                     [1m Learning iteration 540/4000 [0m

                       Computation: 3276 steps/s (collection: 0.451s, learning 2.049s)
               Value function loss: 7954.1090
                    Surrogate loss: 0.0149
             Mean action noise std: 0.96
                       Mean reward: 650.50
               Mean episode length: 207.40
                 Mean success rate: 11.00
                  Mean reward/step: 3.85
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4431872
                    Iteration time: 2.50s
                        Total time: 1387.61s
                               ETA: 8874.6s

################################################################################
                     [1m Learning iteration 541/4000 [0m

                       Computation: 3136 steps/s (collection: 0.525s, learning 2.086s)
               Value function loss: 11784.3417
                    Surrogate loss: 0.0106
             Mean action noise std: 0.96
                       Mean reward: 665.75
               Mean episode length: 202.72
                 Mean success rate: 12.00
                  Mean reward/step: 4.09
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 2.61s
                        Total time: 1390.22s
                               ETA: 8872.3s

################################################################################
                     [1m Learning iteration 542/4000 [0m

                       Computation: 3117 steps/s (collection: 0.559s, learning 2.069s)
               Value function loss: 8745.2177
                    Surrogate loss: 0.0135
             Mean action noise std: 0.96
                       Mean reward: 626.87
               Mean episode length: 192.97
                 Mean success rate: 12.00
                  Mean reward/step: 3.67
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 4448256
                    Iteration time: 2.63s
                        Total time: 1392.85s
                               ETA: 8870.1s

################################################################################
                     [1m Learning iteration 543/4000 [0m

                       Computation: 3253 steps/s (collection: 0.456s, learning 2.061s)
               Value function loss: 8449.3264
                    Surrogate loss: 0.0156
             Mean action noise std: 0.96
                       Mean reward: 647.56
               Mean episode length: 195.20
                 Mean success rate: 13.00
                  Mean reward/step: 3.76
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 2.52s
                        Total time: 1395.37s
                               ETA: 8867.3s

################################################################################
                     [1m Learning iteration 544/4000 [0m

                       Computation: 3094 steps/s (collection: 0.452s, learning 2.195s)
               Value function loss: 7310.1886
                    Surrogate loss: 0.0128
             Mean action noise std: 0.96
                       Mean reward: 675.59
               Mean episode length: 200.53
                 Mean success rate: 13.00
                  Mean reward/step: 3.67
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 4464640
                    Iteration time: 2.65s
                        Total time: 1398.02s
                               ETA: 8865.2s

################################################################################
                     [1m Learning iteration 545/4000 [0m

                       Computation: 3207 steps/s (collection: 0.467s, learning 2.086s)
               Value function loss: 5849.9215
                    Surrogate loss: 0.0175
             Mean action noise std: 0.96
                       Mean reward: 763.82
               Mean episode length: 206.66
                 Mean success rate: 15.50
                  Mean reward/step: 3.68
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 2.55s
                        Total time: 1400.57s
                               ETA: 8862.6s

################################################################################
                     [1m Learning iteration 546/4000 [0m

                       Computation: 3205 steps/s (collection: 0.492s, learning 2.064s)
               Value function loss: 9450.7305
                    Surrogate loss: 0.0148
             Mean action noise std: 0.96
                       Mean reward: 829.03
               Mean episode length: 208.94
                 Mean success rate: 17.00
                  Mean reward/step: 3.50
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4481024
                    Iteration time: 2.56s
                        Total time: 1403.12s
                               ETA: 8860.0s

################################################################################
                     [1m Learning iteration 547/4000 [0m

                       Computation: 3255 steps/s (collection: 0.463s, learning 2.054s)
               Value function loss: 5584.6072
                    Surrogate loss: 0.0204
             Mean action noise std: 0.96
                       Mean reward: 889.52
               Mean episode length: 212.82
                 Mean success rate: 19.00
                  Mean reward/step: 3.85
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 2.52s
                        Total time: 1405.64s
                               ETA: 8857.1s

################################################################################
                     [1m Learning iteration 548/4000 [0m

                       Computation: 3176 steps/s (collection: 0.481s, learning 2.098s)
               Value function loss: 10035.6090
                    Surrogate loss: 0.0142
             Mean action noise std: 0.96
                       Mean reward: 853.97
               Mean episode length: 205.38
                 Mean success rate: 18.00
                  Mean reward/step: 4.04
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4497408
                    Iteration time: 2.58s
                        Total time: 1408.22s
                               ETA: 8854.6s

################################################################################
                     [1m Learning iteration 549/4000 [0m

                       Computation: 3255 steps/s (collection: 0.460s, learning 2.056s)
               Value function loss: 7561.5382
                    Surrogate loss: 0.0169
             Mean action noise std: 0.96
                       Mean reward: 844.92
               Mean episode length: 198.24
                 Mean success rate: 17.50
                  Mean reward/step: 3.41
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 2.52s
                        Total time: 1410.74s
                               ETA: 8851.7s

################################################################################
                     [1m Learning iteration 550/4000 [0m

                       Computation: 3205 steps/s (collection: 0.458s, learning 2.098s)
               Value function loss: 5295.6385
                    Surrogate loss: 0.0239
             Mean action noise std: 0.96
                       Mean reward: 828.82
               Mean episode length: 196.15
                 Mean success rate: 18.00
                  Mean reward/step: 2.98
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4513792
                    Iteration time: 2.56s
                        Total time: 1413.29s
                               ETA: 8849.1s

################################################################################
                     [1m Learning iteration 551/4000 [0m

                       Computation: 3231 steps/s (collection: 0.456s, learning 2.079s)
               Value function loss: 6330.5654
                    Surrogate loss: 0.0182
             Mean action noise std: 0.96
                       Mean reward: 678.26
               Mean episode length: 180.80
                 Mean success rate: 14.50
                  Mean reward/step: 3.04
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 2.54s
                        Total time: 1415.83s
                               ETA: 8846.4s

################################################################################
                     [1m Learning iteration 552/4000 [0m

                       Computation: 3172 steps/s (collection: 0.473s, learning 2.109s)
               Value function loss: 6927.7706
                    Surrogate loss: 0.0163
             Mean action noise std: 0.96
                       Mean reward: 806.73
               Mean episode length: 184.33
                 Mean success rate: 16.00
                  Mean reward/step: 3.12
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4530176
                    Iteration time: 2.58s
                        Total time: 1418.41s
                               ETA: 8843.9s

################################################################################
                     [1m Learning iteration 553/4000 [0m

                       Computation: 3137 steps/s (collection: 0.482s, learning 2.129s)
               Value function loss: 5669.7090
                    Surrogate loss: 0.0141
             Mean action noise std: 0.96
                       Mean reward: 811.86
               Mean episode length: 199.08
                 Mean success rate: 15.00
                  Mean reward/step: 3.29
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 2.61s
                        Total time: 1421.02s
                               ETA: 8841.6s

################################################################################
                     [1m Learning iteration 554/4000 [0m

                       Computation: 3196 steps/s (collection: 0.490s, learning 2.073s)
               Value function loss: 5106.3376
                    Surrogate loss: 0.0182
             Mean action noise std: 0.96
                       Mean reward: 802.55
               Mean episode length: 213.34
                 Mean success rate: 15.00
                  Mean reward/step: 3.00
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4546560
                    Iteration time: 2.56s
                        Total time: 1423.58s
                               ETA: 8839.0s

################################################################################
                     [1m Learning iteration 555/4000 [0m

                       Computation: 3291 steps/s (collection: 0.442s, learning 2.047s)
               Value function loss: 5095.1384
                    Surrogate loss: 0.0157
             Mean action noise std: 0.96
                       Mean reward: 752.43
               Mean episode length: 216.43
                 Mean success rate: 14.00
                  Mean reward/step: 2.94
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 2.49s
                        Total time: 1426.07s
                               ETA: 8836.0s

################################################################################
                     [1m Learning iteration 556/4000 [0m

                       Computation: 3204 steps/s (collection: 0.451s, learning 2.106s)
               Value function loss: 5093.0578
                    Surrogate loss: 0.0163
             Mean action noise std: 0.96
                       Mean reward: 727.98
               Mean episode length: 218.15
                 Mean success rate: 12.50
                  Mean reward/step: 3.11
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4562944
                    Iteration time: 2.56s
                        Total time: 1428.63s
                               ETA: 8833.4s

################################################################################
                     [1m Learning iteration 557/4000 [0m

                       Computation: 3227 steps/s (collection: 0.471s, learning 2.068s)
               Value function loss: 8518.4600
                    Surrogate loss: 0.0150
             Mean action noise std: 0.96
                       Mean reward: 697.43
               Mean episode length: 231.00
                 Mean success rate: 11.50
                  Mean reward/step: 3.72
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 2.54s
                        Total time: 1431.17s
                               ETA: 8830.7s

################################################################################
                     [1m Learning iteration 558/4000 [0m

                       Computation: 3191 steps/s (collection: 0.501s, learning 2.066s)
               Value function loss: 10197.0312
                    Surrogate loss: 0.0119
             Mean action noise std: 0.96
                       Mean reward: 625.78
               Mean episode length: 222.12
                 Mean success rate: 11.00
                  Mean reward/step: 4.23
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4579328
                    Iteration time: 2.57s
                        Total time: 1433.73s
                               ETA: 8828.1s

################################################################################
                     [1m Learning iteration 559/4000 [0m

                       Computation: 3141 steps/s (collection: 0.533s, learning 2.075s)
               Value function loss: 10832.4194
                    Surrogate loss: 0.0121
             Mean action noise std: 0.96
                       Mean reward: 662.07
               Mean episode length: 205.28
                 Mean success rate: 13.00
                  Mean reward/step: 4.39
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 2.61s
                        Total time: 1436.34s
                               ETA: 8825.8s

################################################################################
                     [1m Learning iteration 560/4000 [0m

                       Computation: 3223 steps/s (collection: 0.484s, learning 2.057s)
               Value function loss: 6823.5267
                    Surrogate loss: 0.0145
             Mean action noise std: 0.96
                       Mean reward: 738.91
               Mean episode length: 211.99
                 Mean success rate: 14.50
                  Mean reward/step: 4.29
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4595712
                    Iteration time: 2.54s
                        Total time: 1438.88s
                               ETA: 8823.1s

################################################################################
                     [1m Learning iteration 561/4000 [0m

                       Computation: 3139 steps/s (collection: 0.540s, learning 2.069s)
               Value function loss: 11672.3973
                    Surrogate loss: 0.0115
             Mean action noise std: 0.96
                       Mean reward: 759.02
               Mean episode length: 212.94
                 Mean success rate: 16.50
                  Mean reward/step: 3.92
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 2.61s
                        Total time: 1441.49s
                               ETA: 8820.8s

################################################################################
                     [1m Learning iteration 562/4000 [0m

                       Computation: 3267 steps/s (collection: 0.453s, learning 2.054s)
               Value function loss: 12592.3325
                    Surrogate loss: 0.0115
             Mean action noise std: 0.96
                       Mean reward: 864.37
               Mean episode length: 222.94
                 Mean success rate: 19.00
                  Mean reward/step: 4.47
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4612096
                    Iteration time: 2.51s
                        Total time: 1444.00s
                               ETA: 8817.9s

################################################################################
                     [1m Learning iteration 563/4000 [0m

                       Computation: 3204 steps/s (collection: 0.475s, learning 2.081s)
               Value function loss: 6386.9603
                    Surrogate loss: 0.0144
             Mean action noise std: 0.96
                       Mean reward: 879.40
               Mean episode length: 229.56
                 Mean success rate: 18.50
                  Mean reward/step: 4.33
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 2.56s
                        Total time: 1446.56s
                               ETA: 8815.3s

################################################################################
                     [1m Learning iteration 564/4000 [0m

                       Computation: 3183 steps/s (collection: 0.455s, learning 2.119s)
               Value function loss: 8073.1787
                    Surrogate loss: 0.0135
             Mean action noise std: 0.96
                       Mean reward: 817.43
               Mean episode length: 229.31
                 Mean success rate: 17.00
                  Mean reward/step: 4.03
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4628480
                    Iteration time: 2.57s
                        Total time: 1449.13s
                               ETA: 8812.8s

################################################################################
                     [1m Learning iteration 565/4000 [0m

                       Computation: 3228 steps/s (collection: 0.482s, learning 2.056s)
               Value function loss: 11460.7448
                    Surrogate loss: 0.0145
             Mean action noise std: 0.96
                       Mean reward: 857.23
               Mean episode length: 228.49
                 Mean success rate: 17.00
                  Mean reward/step: 4.07
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 2.54s
                        Total time: 1451.67s
                               ETA: 8810.0s

################################################################################
                     [1m Learning iteration 566/4000 [0m

                       Computation: 3186 steps/s (collection: 0.517s, learning 2.053s)
               Value function loss: 12626.1621
                    Surrogate loss: 0.0128
             Mean action noise std: 0.96
                       Mean reward: 837.36
               Mean episode length: 229.90
                 Mean success rate: 15.00
                  Mean reward/step: 4.19
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4644864
                    Iteration time: 2.57s
                        Total time: 1454.24s
                               ETA: 8807.5s

################################################################################
                     [1m Learning iteration 567/4000 [0m

                       Computation: 3132 steps/s (collection: 0.529s, learning 2.087s)
               Value function loss: 11857.8095
                    Surrogate loss: 0.0142
             Mean action noise std: 0.96
                       Mean reward: 798.43
               Mean episode length: 219.99
                 Mean success rate: 13.50
                  Mean reward/step: 4.52
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 2.62s
                        Total time: 1456.85s
                               ETA: 8805.2s

################################################################################
                     [1m Learning iteration 568/4000 [0m

                       Computation: 3181 steps/s (collection: 0.488s, learning 2.087s)
               Value function loss: 11344.6371
                    Surrogate loss: 0.0146
             Mean action noise std: 0.96
                       Mean reward: 845.76
               Mean episode length: 217.87
                 Mean success rate: 13.50
                  Mean reward/step: 4.44
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4661248
                    Iteration time: 2.58s
                        Total time: 1459.43s
                               ETA: 8802.7s

################################################################################
                     [1m Learning iteration 569/4000 [0m

                       Computation: 3235 steps/s (collection: 0.446s, learning 2.086s)
               Value function loss: 13876.4296
                    Surrogate loss: 0.0154
             Mean action noise std: 0.96
                       Mean reward: 778.89
               Mean episode length: 205.58
                 Mean success rate: 12.00
                  Mean reward/step: 4.67
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 2.53s
                        Total time: 1461.96s
                               ETA: 8800.0s

################################################################################
                     [1m Learning iteration 570/4000 [0m

                       Computation: 3081 steps/s (collection: 0.554s, learning 2.104s)
               Value function loss: 11483.9272
                    Surrogate loss: 0.0152
             Mean action noise std: 0.96
                       Mean reward: 825.59
               Mean episode length: 212.95
                 Mean success rate: 12.00
                  Mean reward/step: 4.58
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4677632
                    Iteration time: 2.66s
                        Total time: 1464.62s
                               ETA: 8798.0s

################################################################################
                     [1m Learning iteration 571/4000 [0m

                       Computation: 3177 steps/s (collection: 0.504s, learning 2.075s)
               Value function loss: 11589.7118
                    Surrogate loss: 0.0194
             Mean action noise std: 0.96
                       Mean reward: 859.39
               Mean episode length: 206.99
                 Mean success rate: 12.00
                  Mean reward/step: 4.44
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 2.58s
                        Total time: 1467.20s
                               ETA: 8795.5s

################################################################################
                     [1m Learning iteration 572/4000 [0m

                       Computation: 3209 steps/s (collection: 0.460s, learning 2.092s)
               Value function loss: 10620.5326
                    Surrogate loss: 0.0121
             Mean action noise std: 0.96
                       Mean reward: 1015.74
               Mean episode length: 216.22
                 Mean success rate: 15.00
                  Mean reward/step: 4.23
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4694016
                    Iteration time: 2.55s
                        Total time: 1469.75s
                               ETA: 8792.8s

################################################################################
                     [1m Learning iteration 573/4000 [0m

                       Computation: 3285 steps/s (collection: 0.470s, learning 2.023s)
               Value function loss: 7619.9105
                    Surrogate loss: 0.0155
             Mean action noise std: 0.95
                       Mean reward: 964.35
               Mean episode length: 210.94
                 Mean success rate: 15.00
                  Mean reward/step: 4.26
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 2.49s
                        Total time: 1472.24s
                               ETA: 8789.9s

################################################################################
                     [1m Learning iteration 574/4000 [0m

                       Computation: 3156 steps/s (collection: 0.506s, learning 2.090s)
               Value function loss: 9332.3890
                    Surrogate loss: 0.0161
             Mean action noise std: 0.95
                       Mean reward: 987.36
               Mean episode length: 213.81
                 Mean success rate: 16.50
                  Mean reward/step: 4.58
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4710400
                    Iteration time: 2.60s
                        Total time: 1474.84s
                               ETA: 8787.5s

################################################################################
                     [1m Learning iteration 575/4000 [0m

                       Computation: 3144 steps/s (collection: 0.509s, learning 2.096s)
               Value function loss: 11978.2201
                    Surrogate loss: 0.0143
             Mean action noise std: 0.95
                       Mean reward: 1008.07
               Mean episode length: 222.21
                 Mean success rate: 17.50
                  Mean reward/step: 4.41
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 2.61s
                        Total time: 1477.44s
                               ETA: 8785.1s

################################################################################
                     [1m Learning iteration 576/4000 [0m

                       Computation: 3195 steps/s (collection: 0.460s, learning 2.104s)
               Value function loss: 12900.3358
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 1000.88
               Mean episode length: 222.75
                 Mean success rate: 19.50
                  Mean reward/step: 4.71
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4726784
                    Iteration time: 2.56s
                        Total time: 1480.01s
                               ETA: 8782.6s

################################################################################
                     [1m Learning iteration 577/4000 [0m

                       Computation: 3211 steps/s (collection: 0.496s, learning 2.055s)
               Value function loss: 15177.5164
                    Surrogate loss: 0.0125
             Mean action noise std: 0.95
                       Mean reward: 792.72
               Mean episode length: 208.75
                 Mean success rate: 17.00
                  Mean reward/step: 4.46
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 2.55s
                        Total time: 1482.56s
                               ETA: 8779.9s

################################################################################
                     [1m Learning iteration 578/4000 [0m

                       Computation: 3129 steps/s (collection: 0.506s, learning 2.112s)
               Value function loss: 16638.8257
                    Surrogate loss: 0.0111
             Mean action noise std: 0.95
                       Mean reward: 875.07
               Mean episode length: 213.00
                 Mean success rate: 17.50
                  Mean reward/step: 4.68
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 4743168
                    Iteration time: 2.62s
                        Total time: 1485.18s
                               ETA: 8777.7s

################################################################################
                     [1m Learning iteration 579/4000 [0m

                       Computation: 3162 steps/s (collection: 0.493s, learning 2.098s)
               Value function loss: 10292.9714
                    Surrogate loss: 0.0167
             Mean action noise std: 0.95
                       Mean reward: 855.51
               Mean episode length: 205.57
                 Mean success rate: 19.00
                  Mean reward/step: 4.78
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 2.59s
                        Total time: 1487.77s
                               ETA: 8775.3s

################################################################################
                     [1m Learning iteration 580/4000 [0m

                       Computation: 3164 steps/s (collection: 0.523s, learning 2.065s)
               Value function loss: 13730.3413
                    Surrogate loss: 0.0131
             Mean action noise std: 0.95
                       Mean reward: 940.19
               Mean episode length: 211.09
                 Mean success rate: 19.00
                  Mean reward/step: 4.92
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4759552
                    Iteration time: 2.59s
                        Total time: 1490.35s
                               ETA: 8772.8s

################################################################################
                     [1m Learning iteration 581/4000 [0m

                       Computation: 3168 steps/s (collection: 0.499s, learning 2.086s)
               Value function loss: 16965.4643
                    Surrogate loss: 0.0123
             Mean action noise std: 0.95
                       Mean reward: 982.10
               Mean episode length: 208.75
                 Mean success rate: 18.50
                  Mean reward/step: 4.93
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 2.59s
                        Total time: 1492.94s
                               ETA: 8770.4s

################################################################################
                     [1m Learning iteration 582/4000 [0m

                       Computation: 3237 steps/s (collection: 0.464s, learning 2.066s)
               Value function loss: 16291.9111
                    Surrogate loss: 0.0170
             Mean action noise std: 0.95
                       Mean reward: 1046.77
               Mean episode length: 213.64
                 Mean success rate: 20.00
                  Mean reward/step: 5.39
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4775936
                    Iteration time: 2.53s
                        Total time: 1495.47s
                               ETA: 8767.6s

################################################################################
                     [1m Learning iteration 583/4000 [0m

                       Computation: 3239 steps/s (collection: 0.484s, learning 2.045s)
               Value function loss: 16273.2782
                    Surrogate loss: 0.0162
             Mean action noise std: 0.95
                       Mean reward: 1061.52
               Mean episode length: 218.29
                 Mean success rate: 21.00
                  Mean reward/step: 5.54
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 2.53s
                        Total time: 1498.00s
                               ETA: 8764.8s

################################################################################
                     [1m Learning iteration 584/4000 [0m

                       Computation: 3261 steps/s (collection: 0.463s, learning 2.048s)
               Value function loss: 21006.2813
                    Surrogate loss: 0.0162
             Mean action noise std: 0.95
                       Mean reward: 1098.04
               Mean episode length: 221.87
                 Mean success rate: 20.00
                  Mean reward/step: 5.50
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4792320
                    Iteration time: 2.51s
                        Total time: 1500.51s
                               ETA: 8762.0s

################################################################################
                     [1m Learning iteration 585/4000 [0m

                       Computation: 3228 steps/s (collection: 0.472s, learning 2.065s)
               Value function loss: 13913.6799
                    Surrogate loss: 0.0174
             Mean action noise std: 0.95
                       Mean reward: 1052.33
               Mean episode length: 218.87
                 Mean success rate: 18.50
                  Mean reward/step: 5.78
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 2.54s
                        Total time: 1503.05s
                               ETA: 8759.2s

################################################################################
                     [1m Learning iteration 586/4000 [0m

                       Computation: 3125 steps/s (collection: 0.532s, learning 2.089s)
               Value function loss: 16241.6817
                    Surrogate loss: 0.0152
             Mean action noise std: 0.95
                       Mean reward: 1003.88
               Mean episode length: 219.18
                 Mean success rate: 18.00
                  Mean reward/step: 6.14
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4808704
                    Iteration time: 2.62s
                        Total time: 1505.67s
                               ETA: 8757.0s

################################################################################
                     [1m Learning iteration 587/4000 [0m

                       Computation: 3234 steps/s (collection: 0.482s, learning 2.051s)
               Value function loss: 17229.1738
                    Surrogate loss: 0.0135
             Mean action noise std: 0.95
                       Mean reward: 897.47
               Mean episode length: 206.89
                 Mean success rate: 16.50
                  Mean reward/step: 5.93
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 2.53s
                        Total time: 1508.20s
                               ETA: 8754.2s

################################################################################
                     [1m Learning iteration 588/4000 [0m

                       Computation: 3274 steps/s (collection: 0.469s, learning 2.033s)
               Value function loss: 22192.9370
                    Surrogate loss: 0.0116
             Mean action noise std: 0.95
                       Mean reward: 1014.30
               Mean episode length: 213.75
                 Mean success rate: 19.50
                  Mean reward/step: 5.78
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4825088
                    Iteration time: 2.50s
                        Total time: 1510.70s
                               ETA: 8751.3s

################################################################################
                     [1m Learning iteration 589/4000 [0m

                       Computation: 3258 steps/s (collection: 0.477s, learning 2.037s)
               Value function loss: 16375.3125
                    Surrogate loss: 0.0153
             Mean action noise std: 0.95
                       Mean reward: 946.71
               Mean episode length: 202.88
                 Mean success rate: 19.00
                  Mean reward/step: 6.22
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 2.51s
                        Total time: 1513.22s
                               ETA: 8748.4s

################################################################################
                     [1m Learning iteration 590/4000 [0m

                       Computation: 3222 steps/s (collection: 0.492s, learning 2.050s)
               Value function loss: 20242.5280
                    Surrogate loss: 0.0152
             Mean action noise std: 0.95
                       Mean reward: 866.39
               Mean episode length: 196.68
                 Mean success rate: 17.50
                  Mean reward/step: 6.22
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4841472
                    Iteration time: 2.54s
                        Total time: 1515.76s
                               ETA: 8745.8s

################################################################################
                     [1m Learning iteration 591/4000 [0m

                       Computation: 3194 steps/s (collection: 0.499s, learning 2.065s)
               Value function loss: 19906.5587
                    Surrogate loss: 0.0128
             Mean action noise std: 0.95
                       Mean reward: 1133.20
               Mean episode length: 201.35
                 Mean success rate: 23.50
                  Mean reward/step: 6.15
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 2.56s
                        Total time: 1518.32s
                               ETA: 8743.2s

################################################################################
                     [1m Learning iteration 592/4000 [0m

                       Computation: 3211 steps/s (collection: 0.479s, learning 2.072s)
               Value function loss: 16240.7722
                    Surrogate loss: 0.0165
             Mean action noise std: 0.95
                       Mean reward: 1149.29
               Mean episode length: 198.62
                 Mean success rate: 22.50
                  Mean reward/step: 6.41
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4857856
                    Iteration time: 2.55s
                        Total time: 1520.87s
                               ETA: 8740.5s

################################################################################
                     [1m Learning iteration 593/4000 [0m

                       Computation: 3262 steps/s (collection: 0.458s, learning 2.053s)
               Value function loss: 36032.1590
                    Surrogate loss: 0.0130
             Mean action noise std: 0.95
                       Mean reward: 1386.88
               Mean episode length: 221.60
                 Mean success rate: 25.00
                  Mean reward/step: 6.13
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 2.51s
                        Total time: 1523.39s
                               ETA: 8737.7s

################################################################################
                     [1m Learning iteration 594/4000 [0m

                       Computation: 3262 steps/s (collection: 0.474s, learning 2.036s)
               Value function loss: 15760.3167
                    Surrogate loss: 0.0171
             Mean action noise std: 0.95
                       Mean reward: 1421.31
               Mean episode length: 226.80
                 Mean success rate: 25.50
                  Mean reward/step: 5.71
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4874240
                    Iteration time: 2.51s
                        Total time: 1525.90s
                               ETA: 8734.8s

################################################################################
                     [1m Learning iteration 595/4000 [0m

                       Computation: 3233 steps/s (collection: 0.463s, learning 2.071s)
               Value function loss: 23464.2533
                    Surrogate loss: 0.0148
             Mean action noise std: 0.95
                       Mean reward: 1489.02
               Mean episode length: 235.29
                 Mean success rate: 26.00
                  Mean reward/step: 5.89
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 2.53s
                        Total time: 1528.43s
                               ETA: 8732.1s

################################################################################
                     [1m Learning iteration 596/4000 [0m

                       Computation: 3263 steps/s (collection: 0.433s, learning 2.077s)
               Value function loss: 28853.8133
                    Surrogate loss: 0.0126
             Mean action noise std: 0.95
                       Mean reward: 1392.70
               Mean episode length: 225.55
                 Mean success rate: 24.00
                  Mean reward/step: 5.95
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 4890624
                    Iteration time: 2.51s
                        Total time: 1530.94s
                               ETA: 8729.2s

################################################################################
                     [1m Learning iteration 597/4000 [0m

                       Computation: 3317 steps/s (collection: 0.443s, learning 2.026s)
               Value function loss: 19909.4624
                    Surrogate loss: 0.0168
             Mean action noise std: 0.95
                       Mean reward: 1314.34
               Mean episode length: 216.19
                 Mean success rate: 23.00
                  Mean reward/step: 5.75
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 2.47s
                        Total time: 1533.41s
                               ETA: 8726.1s

################################################################################
                     [1m Learning iteration 598/4000 [0m

                       Computation: 3249 steps/s (collection: 0.472s, learning 2.049s)
               Value function loss: 24106.9352
                    Surrogate loss: 0.0122
             Mean action noise std: 0.95
                       Mean reward: 1298.26
               Mean episode length: 207.58
                 Mean success rate: 24.50
                  Mean reward/step: 6.10
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4907008
                    Iteration time: 2.52s
                        Total time: 1535.93s
                               ETA: 8723.3s

################################################################################
                     [1m Learning iteration 599/4000 [0m

                       Computation: 3262 steps/s (collection: 0.448s, learning 2.062s)
               Value function loss: 17216.6209
                    Surrogate loss: 0.0169
             Mean action noise std: 0.95
                       Mean reward: 1259.98
               Mean episode length: 204.76
                 Mean success rate: 22.50
                  Mean reward/step: 6.15
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 2.51s
                        Total time: 1538.44s
                               ETA: 8720.4s

################################################################################
                     [1m Learning iteration 600/4000 [0m

                       Computation: 3273 steps/s (collection: 0.452s, learning 2.050s)
               Value function loss: 18618.1438
                    Surrogate loss: 0.0163
             Mean action noise std: 0.95
                       Mean reward: 1262.99
               Mean episode length: 198.81
                 Mean success rate: 23.00
                  Mean reward/step: 6.45
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 4923392
                    Iteration time: 2.50s
                        Total time: 1540.94s
                               ETA: 8717.5s

################################################################################
                     [1m Learning iteration 601/4000 [0m

                       Computation: 3259 steps/s (collection: 0.483s, learning 2.030s)
               Value function loss: 23561.0074
                    Surrogate loss: 0.0193
             Mean action noise std: 0.95
                       Mean reward: 1211.46
               Mean episode length: 193.78
                 Mean success rate: 22.50
                  Mean reward/step: 6.50
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 2.51s
                        Total time: 1543.46s
                               ETA: 8714.6s

################################################################################
                     [1m Learning iteration 602/4000 [0m

                       Computation: 3253 steps/s (collection: 0.474s, learning 2.044s)
               Value function loss: 23905.2915
                    Surrogate loss: 0.0219
             Mean action noise std: 0.95
                       Mean reward: 1290.78
               Mean episode length: 206.09
                 Mean success rate: 24.00
                  Mean reward/step: 6.84
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4939776
                    Iteration time: 2.52s
                        Total time: 1545.98s
                               ETA: 8711.8s

################################################################################
                     [1m Learning iteration 603/4000 [0m

                       Computation: 3226 steps/s (collection: 0.478s, learning 2.061s)
               Value function loss: 16375.7464
                    Surrogate loss: 0.0197
             Mean action noise std: 0.95
                       Mean reward: 1075.90
               Mean episode length: 194.63
                 Mean success rate: 21.00
                  Mean reward/step: 6.87
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 2.54s
                        Total time: 1548.51s
                               ETA: 8709.1s

################################################################################
                     [1m Learning iteration 604/4000 [0m

                       Computation: 3242 steps/s (collection: 0.464s, learning 2.062s)
               Value function loss: 19027.8554
                    Surrogate loss: 0.0134
             Mean action noise std: 0.95
                       Mean reward: 1029.20
               Mean episode length: 184.61
                 Mean success rate: 20.00
                  Mean reward/step: 6.96
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4956160
                    Iteration time: 2.53s
                        Total time: 1551.04s
                               ETA: 8706.3s

################################################################################
                     [1m Learning iteration 605/4000 [0m

                       Computation: 3231 steps/s (collection: 0.450s, learning 2.086s)
               Value function loss: 14128.8455
                    Surrogate loss: 0.0179
             Mean action noise std: 0.95
                       Mean reward: 1064.87
               Mean episode length: 187.15
                 Mean success rate: 20.50
                  Mean reward/step: 7.00
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 2.54s
                        Total time: 1553.58s
                               ETA: 8703.6s

################################################################################
                     [1m Learning iteration 606/4000 [0m

                       Computation: 3115 steps/s (collection: 0.521s, learning 2.108s)
               Value function loss: 16606.5658
                    Surrogate loss: 0.0183
             Mean action noise std: 0.95
                       Mean reward: 1016.80
               Mean episode length: 193.96
                 Mean success rate: 19.50
                  Mean reward/step: 7.17
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4972544
                    Iteration time: 2.63s
                        Total time: 1556.20s
                               ETA: 8701.4s

################################################################################
                     [1m Learning iteration 607/4000 [0m

                       Computation: 3214 steps/s (collection: 0.489s, learning 2.060s)
               Value function loss: 21782.0804
                    Surrogate loss: 0.0138
             Mean action noise std: 0.95
                       Mean reward: 1101.32
               Mean episode length: 203.71
                 Mean success rate: 19.50
                  Mean reward/step: 7.10
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 2.55s
                        Total time: 1558.75s
                               ETA: 8698.8s

################################################################################
                     [1m Learning iteration 608/4000 [0m

                       Computation: 3190 steps/s (collection: 0.483s, learning 2.084s)
               Value function loss: 33290.4667
                    Surrogate loss: 0.0126
             Mean action noise std: 0.95
                       Mean reward: 1313.70
               Mean episode length: 207.99
                 Mean success rate: 22.00
                  Mean reward/step: 6.71
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 4988928
                    Iteration time: 2.57s
                        Total time: 1561.32s
                               ETA: 8696.2s

################################################################################
                     [1m Learning iteration 609/4000 [0m

                       Computation: 3172 steps/s (collection: 0.504s, learning 2.078s)
               Value function loss: 22726.1894
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 1388.49
               Mean episode length: 212.29
                 Mean success rate: 22.50
                  Mean reward/step: 6.32
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 2.58s
                        Total time: 1563.90s
                               ETA: 8693.8s

################################################################################
                     [1m Learning iteration 610/4000 [0m

                       Computation: 3199 steps/s (collection: 0.489s, learning 2.072s)
               Value function loss: 18008.2540
                    Surrogate loss: 0.0155
             Mean action noise std: 0.95
                       Mean reward: 1413.91
               Mean episode length: 218.50
                 Mean success rate: 23.00
                  Mean reward/step: 6.78
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5005312
                    Iteration time: 2.56s
                        Total time: 1566.46s
                               ETA: 8691.2s

################################################################################
                     [1m Learning iteration 611/4000 [0m

                       Computation: 3165 steps/s (collection: 0.505s, learning 2.083s)
               Value function loss: 28459.2369
                    Surrogate loss: 0.0144
             Mean action noise std: 0.95
                       Mean reward: 1573.33
               Mean episode length: 218.05
                 Mean success rate: 23.50
                  Mean reward/step: 6.93
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 2.59s
                        Total time: 1569.05s
                               ETA: 8688.7s

################################################################################
                     [1m Learning iteration 612/4000 [0m

                       Computation: 3240 steps/s (collection: 0.445s, learning 2.083s)
               Value function loss: 33535.9655
                    Surrogate loss: 0.0125
             Mean action noise std: 0.95
                       Mean reward: 1720.90
               Mean episode length: 223.54
                 Mean success rate: 26.50
                  Mean reward/step: 6.93
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5021696
                    Iteration time: 2.53s
                        Total time: 1571.58s
                               ETA: 8686.0s

################################################################################
                     [1m Learning iteration 613/4000 [0m

                       Computation: 3252 steps/s (collection: 0.446s, learning 2.073s)
               Value function loss: 29124.6344
                    Surrogate loss: 0.0135
             Mean action noise std: 0.95
                       Mean reward: 1654.31
               Mean episode length: 216.64
                 Mean success rate: 25.00
                  Mean reward/step: 7.33
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 2.52s
                        Total time: 1574.10s
                               ETA: 8683.2s

################################################################################
                     [1m Learning iteration 614/4000 [0m

                       Computation: 3116 steps/s (collection: 0.511s, learning 2.117s)
               Value function loss: 34363.1296
                    Surrogate loss: 0.0149
             Mean action noise std: 0.95
                       Mean reward: 1537.18
               Mean episode length: 212.75
                 Mean success rate: 23.00
                  Mean reward/step: 6.95
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 5038080
                    Iteration time: 2.63s
                        Total time: 1576.73s
                               ETA: 8681.0s

################################################################################
                     [1m Learning iteration 615/4000 [0m

                       Computation: 3161 steps/s (collection: 0.459s, learning 2.132s)
               Value function loss: 22898.6188
                    Surrogate loss: 0.0148
             Mean action noise std: 0.95
                       Mean reward: 1645.53
               Mean episode length: 218.65
                 Mean success rate: 23.50
                  Mean reward/step: 7.03
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 2.59s
                        Total time: 1579.32s
                               ETA: 8678.5s

################################################################################
                     [1m Learning iteration 616/4000 [0m

                       Computation: 3134 steps/s (collection: 0.508s, learning 2.106s)
               Value function loss: 24587.0629
                    Surrogate loss: 0.0156
             Mean action noise std: 0.95
                       Mean reward: 1373.40
               Mean episode length: 199.00
                 Mean success rate: 20.00
                  Mean reward/step: 6.46
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 5054464
                    Iteration time: 2.61s
                        Total time: 1581.93s
                               ETA: 8676.3s

################################################################################
                     [1m Learning iteration 617/4000 [0m

                       Computation: 3125 steps/s (collection: 0.508s, learning 2.113s)
               Value function loss: 25370.6272
                    Surrogate loss: 0.0157
             Mean action noise std: 0.95
                       Mean reward: 1507.96
               Mean episode length: 200.47
                 Mean success rate: 22.00
                  Mean reward/step: 6.76
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 2.62s
                        Total time: 1584.55s
                               ETA: 8674.0s

################################################################################
                     [1m Learning iteration 618/4000 [0m

                       Computation: 3142 steps/s (collection: 0.491s, learning 2.116s)
               Value function loss: 26601.8713
                    Surrogate loss: 0.0144
             Mean action noise std: 0.95
                       Mean reward: 1602.62
               Mean episode length: 209.12
                 Mean success rate: 24.00
                  Mean reward/step: 6.86
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5070848
                    Iteration time: 2.61s
                        Total time: 1587.16s
                               ETA: 8671.7s

################################################################################
                     [1m Learning iteration 619/4000 [0m

                       Computation: 3154 steps/s (collection: 0.480s, learning 2.117s)
               Value function loss: 19477.7580
                    Surrogate loss: 0.0147
             Mean action noise std: 0.95
                       Mean reward: 1528.81
               Mean episode length: 209.70
                 Mean success rate: 24.00
                  Mean reward/step: 6.83
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 2.60s
                        Total time: 1589.76s
                               ETA: 8669.3s

################################################################################
                     [1m Learning iteration 620/4000 [0m

                       Computation: 3145 steps/s (collection: 0.477s, learning 2.127s)
               Value function loss: 26210.5471
                    Surrogate loss: 0.0147
             Mean action noise std: 0.95
                       Mean reward: 1477.88
               Mean episode length: 215.24
                 Mean success rate: 24.00
                  Mean reward/step: 7.20
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5087232
                    Iteration time: 2.60s
                        Total time: 1592.36s
                               ETA: 8667.0s

################################################################################
                     [1m Learning iteration 621/4000 [0m

                       Computation: 3090 steps/s (collection: 0.518s, learning 2.133s)
               Value function loss: 22816.4903
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 1321.20
               Mean episode length: 206.31
                 Mean success rate: 22.50
                  Mean reward/step: 8.00
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 2.65s
                        Total time: 1595.01s
                               ETA: 8664.9s

################################################################################
                     [1m Learning iteration 622/4000 [0m

                       Computation: 3141 steps/s (collection: 0.518s, learning 2.090s)
               Value function loss: 27373.8432
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 1542.07
               Mean episode length: 219.31
                 Mean success rate: 24.50
                  Mean reward/step: 8.10
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 5103616
                    Iteration time: 2.61s
                        Total time: 1597.62s
                               ETA: 8662.5s

################################################################################
                     [1m Learning iteration 623/4000 [0m

                       Computation: 3159 steps/s (collection: 0.469s, learning 2.124s)
               Value function loss: 36517.8707
                    Surrogate loss: 0.0124
             Mean action noise std: 0.95
                       Mean reward: 1416.89
               Mean episode length: 209.54
                 Mean success rate: 23.00
                  Mean reward/step: 7.57
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 2.59s
                        Total time: 1600.21s
                               ETA: 8660.1s

################################################################################
                     [1m Learning iteration 624/4000 [0m

                       Computation: 3108 steps/s (collection: 0.506s, learning 2.130s)
               Value function loss: 26768.9498
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 1540.62
               Mean episode length: 210.93
                 Mean success rate: 23.50
                  Mean reward/step: 7.22
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5120000
                    Iteration time: 2.64s
                        Total time: 1602.85s
                               ETA: 8657.9s

################################################################################
                     [1m Learning iteration 625/4000 [0m

                       Computation: 3140 steps/s (collection: 0.485s, learning 2.124s)
               Value function loss: 20912.4553
                    Surrogate loss: 0.0157
             Mean action noise std: 0.95
                       Mean reward: 1540.34
               Mean episode length: 202.41
                 Mean success rate: 25.00
                  Mean reward/step: 7.19
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 2.61s
                        Total time: 1605.46s
                               ETA: 8655.6s

################################################################################
                     [1m Learning iteration 626/4000 [0m

                       Computation: 3102 steps/s (collection: 0.513s, learning 2.127s)
               Value function loss: 24594.1859
                    Surrogate loss: 0.0153
             Mean action noise std: 0.95
                       Mean reward: 1594.35
               Mean episode length: 211.04
                 Mean success rate: 25.00
                  Mean reward/step: 7.40
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5136384
                    Iteration time: 2.64s
                        Total time: 1608.10s
                               ETA: 8653.5s

################################################################################
                     [1m Learning iteration 627/4000 [0m

                       Computation: 3198 steps/s (collection: 0.461s, learning 2.099s)
               Value function loss: 24120.5470
                    Surrogate loss: 0.0152
             Mean action noise std: 0.95
                       Mean reward: 1552.92
               Mean episode length: 215.24
                 Mean success rate: 24.00
                  Mean reward/step: 7.81
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 2.56s
                        Total time: 1610.66s
                               ETA: 8650.9s

################################################################################
                     [1m Learning iteration 628/4000 [0m

                       Computation: 3215 steps/s (collection: 0.462s, learning 2.086s)
               Value function loss: 23960.1708
                    Surrogate loss: 0.0142
             Mean action noise std: 0.95
                       Mean reward: 1459.42
               Mean episode length: 214.82
                 Mean success rate: 23.50
                  Mean reward/step: 7.50
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 5152768
                    Iteration time: 2.55s
                        Total time: 1613.20s
                               ETA: 8648.2s

################################################################################
                     [1m Learning iteration 629/4000 [0m

                       Computation: 3181 steps/s (collection: 0.490s, learning 2.085s)
               Value function loss: 30591.5089
                    Surrogate loss: 0.0147
             Mean action noise std: 0.95
                       Mean reward: 1355.95
               Mean episode length: 208.29
                 Mean success rate: 21.50
                  Mean reward/step: 7.71
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 2.57s
                        Total time: 1615.78s
                               ETA: 8645.7s

################################################################################
                     [1m Learning iteration 630/4000 [0m

                       Computation: 3225 steps/s (collection: 0.470s, learning 2.070s)
               Value function loss: 31415.0030
                    Surrogate loss: 0.0171
             Mean action noise std: 0.95
                       Mean reward: 1421.48
               Mean episode length: 212.81
                 Mean success rate: 22.00
                  Mean reward/step: 8.06
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 5169152
                    Iteration time: 2.54s
                        Total time: 1618.32s
                               ETA: 8643.0s

################################################################################
                     [1m Learning iteration 631/4000 [0m

                       Computation: 3277 steps/s (collection: 0.457s, learning 2.042s)
               Value function loss: 42156.3040
                    Surrogate loss: 0.0131
             Mean action noise std: 0.95
                       Mean reward: 1691.27
               Mean episode length: 218.48
                 Mean success rate: 26.50
                  Mean reward/step: 8.07
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 2.50s
                        Total time: 1620.82s
                               ETA: 8640.1s

################################################################################
                     [1m Learning iteration 632/4000 [0m

                       Computation: 3272 steps/s (collection: 0.435s, learning 2.068s)
               Value function loss: 25767.8131
                    Surrogate loss: 0.0164
             Mean action noise std: 0.95
                       Mean reward: 1781.14
               Mean episode length: 214.85
                 Mean success rate: 27.00
                  Mean reward/step: 7.37
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5185536
                    Iteration time: 2.50s
                        Total time: 1623.32s
                               ETA: 8637.2s

################################################################################
                     [1m Learning iteration 633/4000 [0m

                       Computation: 3205 steps/s (collection: 0.484s, learning 2.072s)
               Value function loss: 30830.6781
                    Surrogate loss: 0.0147
             Mean action noise std: 0.95
                       Mean reward: 1819.11
               Mean episode length: 217.63
                 Mean success rate: 29.00
                  Mean reward/step: 7.68
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 2.56s
                        Total time: 1625.88s
                               ETA: 8634.6s

################################################################################
                     [1m Learning iteration 634/4000 [0m

                       Computation: 3236 steps/s (collection: 0.474s, learning 2.058s)
               Value function loss: 24677.6952
                    Surrogate loss: 0.0160
             Mean action noise std: 0.95
                       Mean reward: 1661.52
               Mean episode length: 202.35
                 Mean success rate: 26.50
                  Mean reward/step: 7.45
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 5201920
                    Iteration time: 2.53s
                        Total time: 1628.41s
                               ETA: 8631.9s

################################################################################
                     [1m Learning iteration 635/4000 [0m

                       Computation: 3296 steps/s (collection: 0.438s, learning 2.047s)
               Value function loss: 19643.3855
                    Surrogate loss: 0.0168
             Mean action noise std: 0.95
                       Mean reward: 1542.70
               Mean episode length: 192.79
                 Mean success rate: 24.50
                  Mean reward/step: 7.25
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 2.48s
                        Total time: 1630.89s
                               ETA: 8628.9s

################################################################################
                     [1m Learning iteration 636/4000 [0m

                       Computation: 3225 steps/s (collection: 0.448s, learning 2.092s)
               Value function loss: 24781.4398
                    Surrogate loss: 0.0165
             Mean action noise std: 0.95
                       Mean reward: 1337.77
               Mean episode length: 186.31
                 Mean success rate: 22.00
                  Mean reward/step: 7.90
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5218304
                    Iteration time: 2.54s
                        Total time: 1633.43s
                               ETA: 8626.2s

################################################################################
                     [1m Learning iteration 637/4000 [0m

                       Computation: 3268 steps/s (collection: 0.454s, learning 2.052s)
               Value function loss: 31727.1912
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 1383.42
               Mean episode length: 190.20
                 Mean success rate: 21.50
                  Mean reward/step: 7.44
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 2.51s
                        Total time: 1635.94s
                               ETA: 8623.3s

################################################################################
                     [1m Learning iteration 638/4000 [0m

                       Computation: 3253 steps/s (collection: 0.450s, learning 2.068s)
               Value function loss: 31993.1661
                    Surrogate loss: 0.0112
             Mean action noise std: 0.95
                       Mean reward: 1597.95
               Mean episode length: 206.96
                 Mean success rate: 24.00
                  Mean reward/step: 7.30
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 5234688
                    Iteration time: 2.52s
                        Total time: 1638.46s
                               ETA: 8620.5s

################################################################################
                     [1m Learning iteration 639/4000 [0m

                       Computation: 3166 steps/s (collection: 0.475s, learning 2.112s)
               Value function loss: 31336.9762
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 1670.09
               Mean episode length: 213.92
                 Mean success rate: 25.50
                  Mean reward/step: 6.99
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 2.59s
                        Total time: 1641.05s
                               ETA: 8618.1s

################################################################################
                     [1m Learning iteration 640/4000 [0m

                       Computation: 3093 steps/s (collection: 0.517s, learning 2.131s)
               Value function loss: 25718.1965
                    Surrogate loss: 0.0167
             Mean action noise std: 0.95
                       Mean reward: 1667.95
               Mean episode length: 226.81
                 Mean success rate: 25.50
                  Mean reward/step: 6.78
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 5251072
                    Iteration time: 2.65s
                        Total time: 1643.69s
                               ETA: 8615.9s

################################################################################
                     [1m Learning iteration 641/4000 [0m

                       Computation: 3074 steps/s (collection: 0.510s, learning 2.155s)
               Value function loss: 23424.8943
                    Surrogate loss: 0.0146
             Mean action noise std: 0.95
                       Mean reward: 1595.78
               Mean episode length: 219.72
                 Mean success rate: 23.00
                  Mean reward/step: 7.28
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 2.66s
                        Total time: 1646.36s
                               ETA: 8613.9s

################################################################################
                     [1m Learning iteration 642/4000 [0m

                       Computation: 3068 steps/s (collection: 0.512s, learning 2.158s)
               Value function loss: 25601.1752
                    Surrogate loss: 0.0149
             Mean action noise std: 0.95
                       Mean reward: 1353.03
               Mean episode length: 205.58
                 Mean success rate: 19.50
                  Mean reward/step: 7.29
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 5267456
                    Iteration time: 2.67s
                        Total time: 1649.03s
                               ETA: 8611.9s

################################################################################
                     [1m Learning iteration 643/4000 [0m

                       Computation: 3194 steps/s (collection: 0.511s, learning 2.053s)
               Value function loss: 20866.8057
                    Surrogate loss: 0.0162
             Mean action noise std: 0.95
                       Mean reward: 1164.69
               Mean episode length: 188.16
                 Mean success rate: 15.50
                  Mean reward/step: 7.84
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 2.56s
                        Total time: 1651.59s
                               ETA: 8609.3s

################################################################################
                     [1m Learning iteration 644/4000 [0m

                       Computation: 3184 steps/s (collection: 0.493s, learning 2.080s)
               Value function loss: 35589.5926
                    Surrogate loss: 0.0155
             Mean action noise std: 0.95
                       Mean reward: 1270.84
               Mean episode length: 189.90
                 Mean success rate: 17.00
                  Mean reward/step: 7.95
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 5283840
                    Iteration time: 2.57s
                        Total time: 1654.16s
                               ETA: 8606.8s

################################################################################
                     [1m Learning iteration 645/4000 [0m

                       Computation: 3070 steps/s (collection: 0.541s, learning 2.126s)
               Value function loss: 31637.5176
                    Surrogate loss: 0.0138
             Mean action noise std: 0.95
                       Mean reward: 1567.37
               Mean episode length: 202.83
                 Mean success rate: 22.00
                  Mean reward/step: 7.58
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 2.67s
                        Total time: 1656.83s
                               ETA: 8604.8s

################################################################################
                     [1m Learning iteration 646/4000 [0m

                       Computation: 3016 steps/s (collection: 0.574s, learning 2.142s)
               Value function loss: 29592.4166
                    Surrogate loss: 0.0159
             Mean action noise std: 0.95
                       Mean reward: 1561.81
               Mean episode length: 202.71
                 Mean success rate: 22.00
                  Mean reward/step: 6.86
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 5300224
                    Iteration time: 2.72s
                        Total time: 1659.55s
                               ETA: 8603.0s

################################################################################
                     [1m Learning iteration 647/4000 [0m

                       Computation: 3141 steps/s (collection: 0.491s, learning 2.116s)
               Value function loss: 26501.9059
                    Surrogate loss: 0.0130
             Mean action noise std: 0.95
                       Mean reward: 1699.98
               Mean episode length: 215.38
                 Mean success rate: 25.50
                  Mean reward/step: 6.63
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 2.61s
                        Total time: 1662.16s
                               ETA: 8600.6s

################################################################################
                     [1m Learning iteration 648/4000 [0m

                       Computation: 3168 steps/s (collection: 0.473s, learning 2.113s)
               Value function loss: 19838.0884
                    Surrogate loss: 0.0168
             Mean action noise std: 0.95
                       Mean reward: 1725.24
               Mean episode length: 220.56
                 Mean success rate: 26.50
                  Mean reward/step: 6.76
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5316608
                    Iteration time: 2.59s
                        Total time: 1664.74s
                               ETA: 8598.2s

################################################################################
                     [1m Learning iteration 649/4000 [0m

                       Computation: 3069 steps/s (collection: 0.538s, learning 2.131s)
               Value function loss: 23916.8427
                    Surrogate loss: 0.0127
             Mean action noise std: 0.95
                       Mean reward: 1522.65
               Mean episode length: 208.35
                 Mean success rate: 22.50
                  Mean reward/step: 7.11
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 2.67s
                        Total time: 1667.41s
                               ETA: 8596.1s

################################################################################
                     [1m Learning iteration 650/4000 [0m

                       Computation: 3100 steps/s (collection: 0.518s, learning 2.124s)
               Value function loss: 20240.4414
                    Surrogate loss: 0.0166
             Mean action noise std: 0.95
                       Mean reward: 1465.04
               Mean episode length: 206.64
                 Mean success rate: 21.50
                  Mean reward/step: 7.62
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 5332992
                    Iteration time: 2.64s
                        Total time: 1670.05s
                               ETA: 8594.0s

################################################################################
                     [1m Learning iteration 651/4000 [0m

                       Computation: 3067 steps/s (collection: 0.595s, learning 2.075s)
               Value function loss: 24786.9211
                    Surrogate loss: 0.0136
             Mean action noise std: 0.95
                       Mean reward: 1251.47
               Mean episode length: 202.51
                 Mean success rate: 17.50
                  Mean reward/step: 7.27
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 2.67s
                        Total time: 1672.72s
                               ETA: 8591.9s

################################################################################
                     [1m Learning iteration 652/4000 [0m

                       Computation: 3256 steps/s (collection: 0.441s, learning 2.075s)
               Value function loss: 20416.4912
                    Surrogate loss: 0.0167
             Mean action noise std: 0.95
                       Mean reward: 1153.39
               Mean episode length: 190.07
                 Mean success rate: 16.50
                  Mean reward/step: 7.67
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 5349376
                    Iteration time: 2.52s
                        Total time: 1675.24s
                               ETA: 8589.1s

################################################################################
                     [1m Learning iteration 653/4000 [0m

                       Computation: 3181 steps/s (collection: 0.469s, learning 2.105s)
               Value function loss: 29826.1774
                    Surrogate loss: 0.0157
             Mean action noise std: 0.95
                       Mean reward: 1181.85
               Mean episode length: 183.66
                 Mean success rate: 15.50
                  Mean reward/step: 8.12
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 2.57s
                        Total time: 1677.81s
                               ETA: 8586.6s

################################################################################
                     [1m Learning iteration 654/4000 [0m

                       Computation: 3175 steps/s (collection: 0.486s, learning 2.093s)
               Value function loss: 37127.0894
                    Surrogate loss: 0.0125
             Mean action noise std: 0.95
                       Mean reward: 1586.77
               Mean episode length: 203.75
                 Mean success rate: 22.00
                  Mean reward/step: 7.78
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 5365760
                    Iteration time: 2.58s
                        Total time: 1680.39s
                               ETA: 8584.1s

################################################################################
                     [1m Learning iteration 655/4000 [0m

                       Computation: 3114 steps/s (collection: 0.493s, learning 2.137s)
               Value function loss: 40178.7751
                    Surrogate loss: 0.0142
             Mean action noise std: 0.95
                       Mean reward: 1729.39
               Mean episode length: 209.15
                 Mean success rate: 24.00
                  Mean reward/step: 7.16
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 2.63s
                        Total time: 1683.02s
                               ETA: 8581.9s

################################################################################
                     [1m Learning iteration 656/4000 [0m

                       Computation: 3172 steps/s (collection: 0.498s, learning 2.085s)
               Value function loss: 23725.8448
                    Surrogate loss: 0.0132
             Mean action noise std: 0.95
                       Mean reward: 1929.43
               Mean episode length: 219.09
                 Mean success rate: 26.50
                  Mean reward/step: 6.39
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 5382144
                    Iteration time: 2.58s
                        Total time: 1685.60s
                               ETA: 8579.4s

################################################################################
                     [1m Learning iteration 657/4000 [0m

                       Computation: 3134 steps/s (collection: 0.504s, learning 2.109s)
               Value function loss: 18095.5722
                    Surrogate loss: 0.0185
             Mean action noise std: 0.95
                       Mean reward: 1991.85
               Mean episode length: 231.00
                 Mean success rate: 27.50
                  Mean reward/step: 6.44
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 2.61s
                        Total time: 1688.22s
                               ETA: 8577.1s

################################################################################
                     [1m Learning iteration 658/4000 [0m

                       Computation: 3148 steps/s (collection: 0.474s, learning 2.128s)
               Value function loss: 23064.2265
                    Surrogate loss: 0.0126
             Mean action noise std: 0.95
                       Mean reward: 1965.25
               Mean episode length: 230.88
                 Mean success rate: 28.00
                  Mean reward/step: 7.05
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5398528
                    Iteration time: 2.60s
                        Total time: 1690.82s
                               ETA: 8574.7s

################################################################################
                     [1m Learning iteration 659/4000 [0m

                       Computation: 3151 steps/s (collection: 0.502s, learning 2.098s)
               Value function loss: 26633.6739
                    Surrogate loss: 0.0128
             Mean action noise std: 0.95
                       Mean reward: 1718.94
               Mean episode length: 222.69
                 Mean success rate: 24.50
                  Mean reward/step: 7.44
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 2.60s
                        Total time: 1693.42s
                               ETA: 8572.3s

################################################################################
                     [1m Learning iteration 660/4000 [0m

                       Computation: 3090 steps/s (collection: 0.509s, learning 2.141s)
               Value function loss: 28432.9557
                    Surrogate loss: 0.0136
             Mean action noise std: 0.94
                       Mean reward: 1502.09
               Mean episode length: 218.95
                 Mean success rate: 21.00
                  Mean reward/step: 7.42
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5414912
                    Iteration time: 2.65s
                        Total time: 1696.07s
                               ETA: 8570.2s

################################################################################
                     [1m Learning iteration 661/4000 [0m

                       Computation: 3181 steps/s (collection: 0.493s, learning 2.082s)
               Value function loss: 27722.0672
                    Surrogate loss: 0.0138
             Mean action noise std: 0.95
                       Mean reward: 1450.63
               Mean episode length: 218.92
                 Mean success rate: 21.00
                  Mean reward/step: 8.18
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 2.58s
                        Total time: 1698.65s
                               ETA: 8567.6s

################################################################################
                     [1m Learning iteration 662/4000 [0m

                       Computation: 3229 steps/s (collection: 0.458s, learning 2.079s)
               Value function loss: 22781.3222
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 1552.39
               Mean episode length: 222.24
                 Mean success rate: 22.50
                  Mean reward/step: 8.57
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5431296
                    Iteration time: 2.54s
                        Total time: 1701.18s
                               ETA: 8564.9s

################################################################################
                     [1m Learning iteration 663/4000 [0m

                       Computation: 3191 steps/s (collection: 0.479s, learning 2.088s)
               Value function loss: 29769.8308
                    Surrogate loss: 0.0232
             Mean action noise std: 0.94
                       Mean reward: 1635.29
               Mean episode length: 232.24
                 Mean success rate: 23.50
                  Mean reward/step: 8.59
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 2.57s
                        Total time: 1703.75s
                               ETA: 8562.4s

################################################################################
                     [1m Learning iteration 664/4000 [0m

                       Computation: 3180 steps/s (collection: 0.489s, learning 2.087s)
               Value function loss: 38809.3190
                    Surrogate loss: 0.0143
             Mean action noise std: 0.94
                       Mean reward: 1873.15
               Mean episode length: 236.41
                 Mean success rate: 25.50
                  Mean reward/step: 8.41
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5447680
                    Iteration time: 2.58s
                        Total time: 1706.33s
                               ETA: 8559.9s

################################################################################
                     [1m Learning iteration 665/4000 [0m

                       Computation: 3144 steps/s (collection: 0.511s, learning 2.094s)
               Value function loss: 31689.5496
                    Surrogate loss: 0.0132
             Mean action noise std: 0.94
                       Mean reward: 1896.24
               Mean episode length: 236.54
                 Mean success rate: 26.50
                  Mean reward/step: 8.22
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 2.61s
                        Total time: 1708.93s
                               ETA: 8557.5s

################################################################################
                     [1m Learning iteration 666/4000 [0m

                       Computation: 3142 steps/s (collection: 0.509s, learning 2.097s)
               Value function loss: 16762.0032
                    Surrogate loss: 0.0173
             Mean action noise std: 0.94
                       Mean reward: 1746.33
               Mean episode length: 233.46
                 Mean success rate: 25.00
                  Mean reward/step: 8.04
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5464064
                    Iteration time: 2.61s
                        Total time: 1711.54s
                               ETA: 8555.1s

################################################################################
                     [1m Learning iteration 667/4000 [0m

                       Computation: 3142 steps/s (collection: 0.491s, learning 2.116s)
               Value function loss: 32620.8944
                    Surrogate loss: 0.0143
             Mean action noise std: 0.94
                       Mean reward: 1745.29
               Mean episode length: 236.03
                 Mean success rate: 24.00
                  Mean reward/step: 7.88
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 2.61s
                        Total time: 1714.14s
                               ETA: 8552.8s

################################################################################
                     [1m Learning iteration 668/4000 [0m

                       Computation: 3091 steps/s (collection: 0.500s, learning 2.150s)
               Value function loss: 32356.9650
                    Surrogate loss: 0.0159
             Mean action noise std: 0.94
                       Mean reward: 1623.04
               Mean episode length: 227.97
                 Mean success rate: 22.00
                  Mean reward/step: 7.67
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 5480448
                    Iteration time: 2.65s
                        Total time: 1716.79s
                               ETA: 8550.6s

################################################################################
                     [1m Learning iteration 669/4000 [0m

                       Computation: 3133 steps/s (collection: 0.500s, learning 2.115s)
               Value function loss: 38089.5876
                    Surrogate loss: 0.0158
             Mean action noise std: 0.94
                       Mean reward: 1532.12
               Mean episode length: 222.61
                 Mean success rate: 21.50
                  Mean reward/step: 7.95
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 2.61s
                        Total time: 1719.41s
                               ETA: 8548.3s

################################################################################
                     [1m Learning iteration 670/4000 [0m

                       Computation: 3217 steps/s (collection: 0.474s, learning 2.072s)
               Value function loss: 36115.7811
                    Surrogate loss: 0.0141
             Mean action noise std: 0.94
                       Mean reward: 1752.74
               Mean episode length: 230.24
                 Mean success rate: 24.00
                  Mean reward/step: 7.68
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 5496832
                    Iteration time: 2.55s
                        Total time: 1721.95s
                               ETA: 8545.6s

################################################################################
                     [1m Learning iteration 671/4000 [0m

                       Computation: 3230 steps/s (collection: 0.481s, learning 2.055s)
               Value function loss: 30214.2553
                    Surrogate loss: 0.0178
             Mean action noise std: 0.94
                       Mean reward: 1726.04
               Mean episode length: 222.76
                 Mean success rate: 24.00
                  Mean reward/step: 7.62
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 2.54s
                        Total time: 1724.49s
                               ETA: 8542.9s

################################################################################
                     [1m Learning iteration 672/4000 [0m

                       Computation: 3177 steps/s (collection: 0.492s, learning 2.086s)
               Value function loss: 33019.6623
                    Surrogate loss: 0.0190
             Mean action noise std: 0.94
                       Mean reward: 1909.45
               Mean episode length: 237.71
                 Mean success rate: 26.00
                  Mean reward/step: 8.22
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5513216
                    Iteration time: 2.58s
                        Total time: 1727.07s
                               ETA: 8540.4s

################################################################################
                     [1m Learning iteration 673/4000 [0m

                       Computation: 3157 steps/s (collection: 0.505s, learning 2.090s)
               Value function loss: 27448.2505
                    Surrogate loss: 0.0178
             Mean action noise std: 0.94
                       Mean reward: 1923.24
               Mean episode length: 233.94
                 Mean success rate: 26.50
                  Mean reward/step: 8.16
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 2.59s
                        Total time: 1729.66s
                               ETA: 8538.0s

################################################################################
                     [1m Learning iteration 674/4000 [0m

                       Computation: 3185 steps/s (collection: 0.506s, learning 2.066s)
               Value function loss: 31434.1461
                    Surrogate loss: 0.0153
             Mean action noise std: 0.94
                       Mean reward: 1825.73
               Mean episode length: 224.63
                 Mean success rate: 23.50
                  Mean reward/step: 8.34
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 5529600
                    Iteration time: 2.57s
                        Total time: 1732.23s
                               ETA: 8535.4s

################################################################################
                     [1m Learning iteration 675/4000 [0m

                       Computation: 3177 steps/s (collection: 0.497s, learning 2.082s)
               Value function loss: 27040.7305
                    Surrogate loss: 0.0148
             Mean action noise std: 0.94
                       Mean reward: 1692.36
               Mean episode length: 215.01
                 Mean success rate: 21.50
                  Mean reward/step: 8.32
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 2.58s
                        Total time: 1734.81s
                               ETA: 8532.9s

################################################################################
                     [1m Learning iteration 676/4000 [0m

                       Computation: 3160 steps/s (collection: 0.510s, learning 2.082s)
               Value function loss: 32070.2212
                    Surrogate loss: 0.0161
             Mean action noise std: 0.94
                       Mean reward: 1592.12
               Mean episode length: 211.81
                 Mean success rate: 22.00
                  Mean reward/step: 8.33
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 5545984
                    Iteration time: 2.59s
                        Total time: 1737.41s
                               ETA: 8530.5s

################################################################################
                     [1m Learning iteration 677/4000 [0m

                       Computation: 3205 steps/s (collection: 0.502s, learning 2.054s)
               Value function loss: 24441.7500
                    Surrogate loss: 0.0145
             Mean action noise std: 0.94
                       Mean reward: 1331.94
               Mean episode length: 186.49
                 Mean success rate: 19.00
                  Mean reward/step: 8.59
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 2.56s
                        Total time: 1739.96s
                               ETA: 8527.9s

################################################################################
                     [1m Learning iteration 678/4000 [0m

                       Computation: 3171 steps/s (collection: 0.504s, learning 2.079s)
               Value function loss: 31920.5953
                    Surrogate loss: 0.0140
             Mean action noise std: 0.94
                       Mean reward: 1466.19
               Mean episode length: 194.48
                 Mean success rate: 21.50
                  Mean reward/step: 8.82
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 5562368
                    Iteration time: 2.58s
                        Total time: 1742.54s
                               ETA: 8525.4s

################################################################################
                     [1m Learning iteration 679/4000 [0m

                       Computation: 3201 steps/s (collection: 0.478s, learning 2.080s)
               Value function loss: 30078.9231
                    Surrogate loss: 0.0171
             Mean action noise std: 0.94
                       Mean reward: 1556.87
               Mean episode length: 199.62
                 Mean success rate: 23.50
                  Mean reward/step: 9.31
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 2.56s
                        Total time: 1745.10s
                               ETA: 8522.8s

################################################################################
                     [1m Learning iteration 680/4000 [0m

                       Computation: 3178 steps/s (collection: 0.496s, learning 2.081s)
               Value function loss: 47513.0265
                    Surrogate loss: 0.0132
             Mean action noise std: 0.94
                       Mean reward: 1595.63
               Mean episode length: 201.26
                 Mean success rate: 23.50
                  Mean reward/step: 9.48
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5578752
                    Iteration time: 2.58s
                        Total time: 1747.68s
                               ETA: 8520.3s

################################################################################
                     [1m Learning iteration 681/4000 [0m

                       Computation: 3114 steps/s (collection: 0.528s, learning 2.102s)
               Value function loss: 38542.7741
                    Surrogate loss: 0.0143
             Mean action noise std: 0.94
                       Mean reward: 1677.20
               Mean episode length: 198.57
                 Mean success rate: 22.50
                  Mean reward/step: 9.78
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 2.63s
                        Total time: 1750.31s
                               ETA: 8518.0s

################################################################################
                     [1m Learning iteration 682/4000 [0m

                       Computation: 3214 steps/s (collection: 0.474s, learning 2.074s)
               Value function loss: 37677.1386
                    Surrogate loss: 0.0121
             Mean action noise std: 0.94
                       Mean reward: 1905.66
               Mean episode length: 210.87
                 Mean success rate: 25.50
                  Mean reward/step: 9.73
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5595136
                    Iteration time: 2.55s
                        Total time: 1752.86s
                               ETA: 8515.4s

################################################################################
                     [1m Learning iteration 683/4000 [0m

                       Computation: 3152 steps/s (collection: 0.502s, learning 2.097s)
               Value function loss: 38332.9904
                    Surrogate loss: 0.0105
             Mean action noise std: 0.94
                       Mean reward: 2025.40
               Mean episode length: 235.79
                 Mean success rate: 27.00
                  Mean reward/step: 9.40
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 2.60s
                        Total time: 1755.46s
                               ETA: 8512.9s

################################################################################
                     [1m Learning iteration 684/4000 [0m

                       Computation: 3151 steps/s (collection: 0.528s, learning 2.071s)
               Value function loss: 50140.5334
                    Surrogate loss: 0.0104
             Mean action noise std: 0.94
                       Mean reward: 2352.32
               Mean episode length: 255.31
                 Mean success rate: 31.50
                  Mean reward/step: 9.03
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 5611520
                    Iteration time: 2.60s
                        Total time: 1758.06s
                               ETA: 8510.5s

################################################################################
                     [1m Learning iteration 685/4000 [0m

                       Computation: 3110 steps/s (collection: 0.549s, learning 2.084s)
               Value function loss: 29390.5292
                    Surrogate loss: 0.0158
             Mean action noise std: 0.94
                       Mean reward: 2387.29
               Mean episode length: 252.42
                 Mean success rate: 32.00
                  Mean reward/step: 9.00
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 2.63s
                        Total time: 1760.69s
                               ETA: 8508.3s

################################################################################
                     [1m Learning iteration 686/4000 [0m

                       Computation: 3169 steps/s (collection: 0.515s, learning 2.069s)
               Value function loss: 32552.5174
                    Surrogate loss: 0.0143
             Mean action noise std: 0.94
                       Mean reward: 2200.99
               Mean episode length: 251.34
                 Mean success rate: 30.50
                  Mean reward/step: 9.19
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 5627904
                    Iteration time: 2.58s
                        Total time: 1763.27s
                               ETA: 8505.8s

################################################################################
                     [1m Learning iteration 687/4000 [0m

                       Computation: 3243 steps/s (collection: 0.468s, learning 2.057s)
               Value function loss: 34027.9727
                    Surrogate loss: 0.0124
             Mean action noise std: 0.94
                       Mean reward: 2178.92
               Mean episode length: 250.35
                 Mean success rate: 31.00
                  Mean reward/step: 9.12
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 2.53s
                        Total time: 1765.80s
                               ETA: 8503.0s

################################################################################
                     [1m Learning iteration 688/4000 [0m

                       Computation: 3114 steps/s (collection: 0.522s, learning 2.108s)
               Value function loss: 39979.7527
                    Surrogate loss: 0.0143
             Mean action noise std: 0.94
                       Mean reward: 2066.59
               Mean episode length: 239.53
                 Mean success rate: 28.00
                  Mean reward/step: 9.34
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5644288
                    Iteration time: 2.63s
                        Total time: 1768.43s
                               ETA: 8500.8s

################################################################################
                     [1m Learning iteration 689/4000 [0m

                       Computation: 3180 steps/s (collection: 0.514s, learning 2.061s)
               Value function loss: 33893.4780
                    Surrogate loss: 0.0152
             Mean action noise std: 0.94
                       Mean reward: 1867.70
               Mean episode length: 217.94
                 Mean success rate: 25.00
                  Mean reward/step: 9.44
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 2.58s
                        Total time: 1771.01s
                               ETA: 8498.3s

################################################################################
                     [1m Learning iteration 690/4000 [0m

                       Computation: 3214 steps/s (collection: 0.496s, learning 2.053s)
               Value function loss: 33023.4272
                    Surrogate loss: 0.0163
             Mean action noise std: 0.94
                       Mean reward: 1808.10
               Mean episode length: 212.90
                 Mean success rate: 23.50
                  Mean reward/step: 9.37
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 5660672
                    Iteration time: 2.55s
                        Total time: 1773.55s
                               ETA: 8495.6s

################################################################################
                     [1m Learning iteration 691/4000 [0m

                       Computation: 3230 steps/s (collection: 0.466s, learning 2.070s)
               Value function loss: 37603.3467
                    Surrogate loss: 0.0138
             Mean action noise std: 0.94
                       Mean reward: 1829.22
               Mean episode length: 215.76
                 Mean success rate: 24.00
                  Mean reward/step: 9.25
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 2.54s
                        Total time: 1776.09s
                               ETA: 8492.9s

################################################################################
                     [1m Learning iteration 692/4000 [0m

                       Computation: 3204 steps/s (collection: 0.513s, learning 2.044s)
               Value function loss: 38668.0212
                    Surrogate loss: 0.0163
             Mean action noise std: 0.94
                       Mean reward: 1983.75
               Mean episode length: 215.52
                 Mean success rate: 26.00
                  Mean reward/step: 9.44
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5677056
                    Iteration time: 2.56s
                        Total time: 1778.65s
                               ETA: 8490.3s

################################################################################
                     [1m Learning iteration 693/4000 [0m

                       Computation: 3114 steps/s (collection: 0.538s, learning 2.092s)
               Value function loss: 41405.6581
                    Surrogate loss: 0.0169
             Mean action noise std: 0.94
                       Mean reward: 2079.68
               Mean episode length: 215.79
                 Mean success rate: 27.50
                  Mean reward/step: 9.42
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 2.63s
                        Total time: 1781.28s
                               ETA: 8488.0s

################################################################################
                     [1m Learning iteration 694/4000 [0m

                       Computation: 3239 steps/s (collection: 0.481s, learning 2.047s)
               Value function loss: 38709.7020
                    Surrogate loss: 0.0145
             Mean action noise std: 0.94
                       Mean reward: 2123.26
               Mean episode length: 225.76
                 Mean success rate: 29.00
                  Mean reward/step: 8.96
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5693440
                    Iteration time: 2.53s
                        Total time: 1783.81s
                               ETA: 8485.3s

################################################################################
                     [1m Learning iteration 695/4000 [0m

                       Computation: 3193 steps/s (collection: 0.457s, learning 2.108s)
               Value function loss: 34860.7844
                    Surrogate loss: 0.0150
             Mean action noise std: 0.94
                       Mean reward: 2203.33
               Mean episode length: 235.25
                 Mean success rate: 30.50
                  Mean reward/step: 9.09
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 2.57s
                        Total time: 1786.37s
                               ETA: 8482.7s

################################################################################
                     [1m Learning iteration 696/4000 [0m

                       Computation: 3207 steps/s (collection: 0.474s, learning 2.080s)
               Value function loss: 28647.3738
                    Surrogate loss: 0.0127
             Mean action noise std: 0.94
                       Mean reward: 2243.29
               Mean episode length: 238.91
                 Mean success rate: 32.00
                  Mean reward/step: 9.34
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5709824
                    Iteration time: 2.55s
                        Total time: 1788.92s
                               ETA: 8480.1s

################################################################################
                     [1m Learning iteration 697/4000 [0m

                       Computation: 3119 steps/s (collection: 0.538s, learning 2.088s)
               Value function loss: 25854.8479
                    Surrogate loss: 0.0163
             Mean action noise std: 0.94
                       Mean reward: 2306.96
               Mean episode length: 243.16
                 Mean success rate: 33.50
                  Mean reward/step: 10.03
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 2.63s
                        Total time: 1791.55s
                               ETA: 8477.8s

################################################################################
                     [1m Learning iteration 698/4000 [0m

                       Computation: 3234 steps/s (collection: 0.473s, learning 2.060s)
               Value function loss: 41843.6866
                    Surrogate loss: 0.0149
             Mean action noise std: 0.94
                       Mean reward: 2344.59
               Mean episode length: 243.56
                 Mean success rate: 34.00
                  Mean reward/step: 10.45
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5726208
                    Iteration time: 2.53s
                        Total time: 1794.08s
                               ETA: 8475.1s

################################################################################
                     [1m Learning iteration 699/4000 [0m

                       Computation: 3210 steps/s (collection: 0.463s, learning 2.089s)
               Value function loss: 30193.7929
                    Surrogate loss: 0.0153
             Mean action noise std: 0.94
                       Mean reward: 2291.26
               Mean episode length: 244.06
                 Mean success rate: 34.00
                  Mean reward/step: 10.18
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 2.55s
                        Total time: 1796.64s
                               ETA: 8472.4s

################################################################################
                     [1m Learning iteration 700/4000 [0m

                       Computation: 3138 steps/s (collection: 0.493s, learning 2.116s)
               Value function loss: 43711.0774
                    Surrogate loss: 0.0142
             Mean action noise std: 0.94
                       Mean reward: 2398.40
               Mean episode length: 250.28
                 Mean success rate: 33.50
                  Mean reward/step: 10.39
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 5742592
                    Iteration time: 2.61s
                        Total time: 1799.25s
                               ETA: 8470.1s

################################################################################
                     [1m Learning iteration 701/4000 [0m

                       Computation: 3165 steps/s (collection: 0.508s, learning 2.080s)
               Value function loss: 51714.7642
                    Surrogate loss: 0.0118
             Mean action noise std: 0.94
                       Mean reward: 2431.45
               Mean episode length: 247.62
                 Mean success rate: 34.00
                  Mean reward/step: 10.11
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 2.59s
                        Total time: 1801.83s
                               ETA: 8467.6s

################################################################################
                     [1m Learning iteration 702/4000 [0m

                       Computation: 3207 steps/s (collection: 0.466s, learning 2.087s)
               Value function loss: 25960.2800
                    Surrogate loss: 0.0133
             Mean action noise std: 0.94
                       Mean reward: 2483.93
               Mean episode length: 251.31
                 Mean success rate: 35.50
                  Mean reward/step: 9.86
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5758976
                    Iteration time: 2.55s
                        Total time: 1804.39s
                               ETA: 8465.0s

################################################################################
                     [1m Learning iteration 703/4000 [0m

                       Computation: 3162 steps/s (collection: 0.482s, learning 2.109s)
               Value function loss: 41706.9106
                    Surrogate loss: 0.0145
             Mean action noise std: 0.94
                       Mean reward: 2534.66
               Mean episode length: 246.47
                 Mean success rate: 36.00
                  Mean reward/step: 10.26
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 2.59s
                        Total time: 1806.98s
                               ETA: 8462.5s

################################################################################
                     [1m Learning iteration 704/4000 [0m

                       Computation: 3215 steps/s (collection: 0.479s, learning 2.069s)
               Value function loss: 41164.4295
                    Surrogate loss: 0.0129
             Mean action noise std: 0.94
                       Mean reward: 2496.20
               Mean episode length: 246.81
                 Mean success rate: 35.00
                  Mean reward/step: 10.45
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5775360
                    Iteration time: 2.55s
                        Total time: 1809.52s
                               ETA: 8459.8s

################################################################################
                     [1m Learning iteration 705/4000 [0m

                       Computation: 3172 steps/s (collection: 0.485s, learning 2.097s)
               Value function loss: 48039.4485
                    Surrogate loss: 0.0115
             Mean action noise std: 0.94
                       Mean reward: 2612.02
               Mean episode length: 247.98
                 Mean success rate: 36.00
                  Mean reward/step: 10.35
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 2.58s
                        Total time: 1812.11s
                               ETA: 8457.4s

################################################################################
                     [1m Learning iteration 706/4000 [0m

                       Computation: 3141 steps/s (collection: 0.498s, learning 2.109s)
               Value function loss: 29627.7884
                    Surrogate loss: 0.0141
             Mean action noise std: 0.94
                       Mean reward: 2362.46
               Mean episode length: 240.34
                 Mean success rate: 34.50
                  Mean reward/step: 10.20
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 5791744
                    Iteration time: 2.61s
                        Total time: 1814.71s
                               ETA: 8455.0s

################################################################################
                     [1m Learning iteration 707/4000 [0m

                       Computation: 3202 steps/s (collection: 0.486s, learning 2.072s)
               Value function loss: 47153.3996
                    Surrogate loss: 0.0130
             Mean action noise std: 0.94
                       Mean reward: 2448.94
               Mean episode length: 241.56
                 Mean success rate: 36.00
                  Mean reward/step: 10.06
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 2.56s
                        Total time: 1817.27s
                               ETA: 8452.4s

################################################################################
                     [1m Learning iteration 708/4000 [0m

                       Computation: 3158 steps/s (collection: 0.494s, learning 2.100s)
               Value function loss: 38200.3664
                    Surrogate loss: 0.0130
             Mean action noise std: 0.94
                       Mean reward: 2406.60
               Mean episode length: 250.67
                 Mean success rate: 36.00
                  Mean reward/step: 9.95
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5808128
                    Iteration time: 2.59s
                        Total time: 1819.87s
                               ETA: 8449.9s

################################################################################
                     [1m Learning iteration 709/4000 [0m

                       Computation: 3173 steps/s (collection: 0.475s, learning 2.106s)
               Value function loss: 42713.8619
                    Surrogate loss: 0.0151
             Mean action noise std: 0.94
                       Mean reward: 2533.87
               Mean episode length: 263.11
                 Mean success rate: 36.50
                  Mean reward/step: 10.33
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 2.58s
                        Total time: 1822.45s
                               ETA: 8447.4s

################################################################################
                     [1m Learning iteration 710/4000 [0m

                       Computation: 3204 steps/s (collection: 0.468s, learning 2.088s)
               Value function loss: 39670.5655
                    Surrogate loss: 0.0132
             Mean action noise std: 0.94
                       Mean reward: 2498.21
               Mean episode length: 259.69
                 Mean success rate: 35.00
                  Mean reward/step: 10.43
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5824512
                    Iteration time: 2.56s
                        Total time: 1825.00s
                               ETA: 8444.8s

################################################################################
                     [1m Learning iteration 711/4000 [0m

                       Computation: 3175 steps/s (collection: 0.474s, learning 2.106s)
               Value function loss: 40884.9938
                    Surrogate loss: 0.0166
             Mean action noise std: 0.94
                       Mean reward: 2715.86
               Mean episode length: 269.71
                 Mean success rate: 37.00
                  Mean reward/step: 10.36
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 2.58s
                        Total time: 1827.58s
                               ETA: 8442.3s

################################################################################
                     [1m Learning iteration 712/4000 [0m

                       Computation: 3160 steps/s (collection: 0.489s, learning 2.103s)
               Value function loss: 29379.4314
                    Surrogate loss: 0.0162
             Mean action noise std: 0.94
                       Mean reward: 2788.80
               Mean episode length: 284.63
                 Mean success rate: 38.00
                  Mean reward/step: 10.50
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 5840896
                    Iteration time: 2.59s
                        Total time: 1830.18s
                               ETA: 8439.9s

################################################################################
                     [1m Learning iteration 713/4000 [0m

                       Computation: 3168 steps/s (collection: 0.513s, learning 2.072s)
               Value function loss: 41938.1198
                    Surrogate loss: 0.0183
             Mean action noise std: 0.94
                       Mean reward: 2863.04
               Mean episode length: 281.62
                 Mean success rate: 38.00
                  Mean reward/step: 10.95
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 2.59s
                        Total time: 1832.76s
                               ETA: 8437.4s

################################################################################
                     [1m Learning iteration 714/4000 [0m

                       Computation: 3217 steps/s (collection: 0.487s, learning 2.059s)
               Value function loss: 42794.9396
                    Surrogate loss: 0.0119
             Mean action noise std: 0.94
                       Mean reward: 2789.95
               Mean episode length: 279.02
                 Mean success rate: 37.50
                  Mean reward/step: 11.41
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5857280
                    Iteration time: 2.55s
                        Total time: 1835.31s
                               ETA: 8434.7s

################################################################################
                     [1m Learning iteration 715/4000 [0m

                       Computation: 3248 steps/s (collection: 0.450s, learning 2.072s)
               Value function loss: 42315.4713
                    Surrogate loss: 0.0122
             Mean action noise std: 0.94
                       Mean reward: 2947.23
               Mean episode length: 276.62
                 Mean success rate: 38.50
                  Mean reward/step: 11.61
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 2.52s
                        Total time: 1837.83s
                               ETA: 8431.9s

################################################################################
                     [1m Learning iteration 716/4000 [0m

                       Computation: 3203 steps/s (collection: 0.475s, learning 2.083s)
               Value function loss: 33138.1385
                    Surrogate loss: 0.0152
             Mean action noise std: 0.94
                       Mean reward: 2870.96
               Mean episode length: 266.96
                 Mean success rate: 37.00
                  Mean reward/step: 11.64
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5873664
                    Iteration time: 2.56s
                        Total time: 1840.39s
                               ETA: 8429.3s

################################################################################
                     [1m Learning iteration 717/4000 [0m

                       Computation: 3198 steps/s (collection: 0.503s, learning 2.059s)
               Value function loss: 48711.0062
                    Surrogate loss: 0.0155
             Mean action noise std: 0.94
                       Mean reward: 2923.54
               Mean episode length: 269.38
                 Mean success rate: 38.50
                  Mean reward/step: 11.78
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 2.56s
                        Total time: 1842.95s
                               ETA: 8426.7s

################################################################################
                     [1m Learning iteration 718/4000 [0m

                       Computation: 3155 steps/s (collection: 0.485s, learning 2.112s)
               Value function loss: 36274.1156
                    Surrogate loss: 0.0157
             Mean action noise std: 0.94
                       Mean reward: 2642.50
               Mean episode length: 259.50
                 Mean success rate: 35.00
                  Mean reward/step: 11.94
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5890048
                    Iteration time: 2.60s
                        Total time: 1845.54s
                               ETA: 8424.3s

################################################################################
                     [1m Learning iteration 719/4000 [0m

                       Computation: 3139 steps/s (collection: 0.508s, learning 2.102s)
               Value function loss: 42690.0342
                    Surrogate loss: 0.0155
             Mean action noise std: 0.94
                       Mean reward: 2763.61
               Mean episode length: 260.66
                 Mean success rate: 36.00
                  Mean reward/step: 11.42
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 2.61s
                        Total time: 1848.15s
                               ETA: 8421.9s

################################################################################
                     [1m Learning iteration 720/4000 [0m

                       Computation: 3201 steps/s (collection: 0.498s, learning 2.061s)
               Value function loss: 26846.0356
                    Surrogate loss: 0.0216
             Mean action noise std: 0.94
                       Mean reward: 2663.68
               Mean episode length: 262.23
                 Mean success rate: 35.00
                  Mean reward/step: 11.07
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5906432
                    Iteration time: 2.56s
                        Total time: 1850.71s
                               ETA: 8419.3s

################################################################################
                     [1m Learning iteration 721/4000 [0m

                       Computation: 3184 steps/s (collection: 0.508s, learning 2.065s)
               Value function loss: 44647.5092
                    Surrogate loss: 0.0211
             Mean action noise std: 0.94
                       Mean reward: 2625.69
               Mean episode length: 259.67
                 Mean success rate: 33.50
                  Mean reward/step: 11.23
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 2.57s
                        Total time: 1853.29s
                               ETA: 8416.8s

################################################################################
                     [1m Learning iteration 722/4000 [0m

                       Computation: 3194 steps/s (collection: 0.492s, learning 2.073s)
               Value function loss: 58590.8101
                    Surrogate loss: 0.0197
             Mean action noise std: 0.94
                       Mean reward: 2828.76
               Mean episode length: 269.75
                 Mean success rate: 34.50
                  Mean reward/step: 10.42
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 5922816
                    Iteration time: 2.56s
                        Total time: 1855.85s
                               ETA: 8414.2s

################################################################################
                     [1m Learning iteration 723/4000 [0m

                       Computation: 3221 steps/s (collection: 0.497s, learning 2.045s)
               Value function loss: 49479.2560
                    Surrogate loss: 0.0157
             Mean action noise std: 0.94
                       Mean reward: 2882.98
               Mean episode length: 270.38
                 Mean success rate: 34.50
                  Mean reward/step: 10.40
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 2.54s
                        Total time: 1858.39s
                               ETA: 8411.5s

################################################################################
                     [1m Learning iteration 724/4000 [0m

                       Computation: 3217 steps/s (collection: 0.478s, learning 2.068s)
               Value function loss: 28939.2631
                    Surrogate loss: 0.0156
             Mean action noise std: 0.94
                       Mean reward: 2966.66
               Mean episode length: 271.44
                 Mean success rate: 36.50
                  Mean reward/step: 10.23
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5939200
                    Iteration time: 2.55s
                        Total time: 1860.94s
                               ETA: 8408.9s

################################################################################
                     [1m Learning iteration 725/4000 [0m

                       Computation: 3266 steps/s (collection: 0.474s, learning 2.033s)
               Value function loss: 38576.9307
                    Surrogate loss: 0.0148
             Mean action noise std: 0.94
                       Mean reward: 3080.44
               Mean episode length: 266.33
                 Mean success rate: 36.50
                  Mean reward/step: 10.06
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 2.51s
                        Total time: 1863.45s
                               ETA: 8406.0s

################################################################################
                     [1m Learning iteration 726/4000 [0m

                       Computation: 3199 steps/s (collection: 0.493s, learning 2.067s)
               Value function loss: 41714.0135
                    Surrogate loss: 0.0158
             Mean action noise std: 0.94
                       Mean reward: 3113.17
               Mean episode length: 270.76
                 Mean success rate: 37.50
                  Mean reward/step: 9.73
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5955584
                    Iteration time: 2.56s
                        Total time: 1866.01s
                               ETA: 8403.4s

################################################################################
                     [1m Learning iteration 727/4000 [0m

                       Computation: 3232 steps/s (collection: 0.497s, learning 2.038s)
               Value function loss: 25825.5224
                    Surrogate loss: 0.0139
             Mean action noise std: 0.94
                       Mean reward: 2815.73
               Mean episode length: 254.13
                 Mean success rate: 34.00
                  Mean reward/step: 9.99
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 2.53s
                        Total time: 1868.54s
                               ETA: 8400.7s

################################################################################
                     [1m Learning iteration 728/4000 [0m

                       Computation: 3210 steps/s (collection: 0.505s, learning 2.047s)
               Value function loss: 39871.4067
                    Surrogate loss: 0.0157
             Mean action noise std: 0.94
                       Mean reward: 2620.75
               Mean episode length: 236.82
                 Mean success rate: 30.50
                  Mean reward/step: 10.30
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5971968
                    Iteration time: 2.55s
                        Total time: 1871.09s
                               ETA: 8398.1s

################################################################################
                     [1m Learning iteration 729/4000 [0m

                       Computation: 3159 steps/s (collection: 0.493s, learning 2.100s)
               Value function loss: 50586.5343
                    Surrogate loss: 0.0138
             Mean action noise std: 0.94
                       Mean reward: 2569.46
               Mean episode length: 231.73
                 Mean success rate: 30.00
                  Mean reward/step: 10.60
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 2.59s
                        Total time: 1873.69s
                               ETA: 8395.7s

################################################################################
                     [1m Learning iteration 730/4000 [0m

                       Computation: 3136 steps/s (collection: 0.496s, learning 2.116s)
               Value function loss: 29644.0656
                    Surrogate loss: 0.0164
             Mean action noise std: 0.94
                       Mean reward: 2520.84
               Mean episode length: 240.22
                 Mean success rate: 29.00
                  Mean reward/step: 10.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5988352
                    Iteration time: 2.61s
                        Total time: 1876.30s
                               ETA: 8393.3s

################################################################################
                     [1m Learning iteration 731/4000 [0m

                       Computation: 3192 steps/s (collection: 0.486s, learning 2.080s)
               Value function loss: 38216.7122
                    Surrogate loss: 0.0155
             Mean action noise std: 0.94
                       Mean reward: 2581.75
               Mean episode length: 243.30
                 Mean success rate: 30.50
                  Mean reward/step: 11.43
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 2.57s
                        Total time: 1878.86s
                               ETA: 8390.7s

################################################################################
                     [1m Learning iteration 732/4000 [0m

                       Computation: 3284 steps/s (collection: 0.433s, learning 2.061s)
               Value function loss: 23832.5701
                    Surrogate loss: 0.0198
             Mean action noise std: 0.94
                       Mean reward: 2265.03
               Mean episode length: 223.41
                 Mean success rate: 28.00
                  Mean reward/step: 11.74
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6004736
                    Iteration time: 2.49s
                        Total time: 1881.36s
                               ETA: 8387.8s

################################################################################
                     [1m Learning iteration 733/4000 [0m

                       Computation: 3282 steps/s (collection: 0.464s, learning 2.031s)
               Value function loss: 33789.0931
                    Surrogate loss: 0.0172
             Mean action noise std: 0.94
                       Mean reward: 2215.60
               Mean episode length: 216.77
                 Mean success rate: 26.50
                  Mean reward/step: 12.75
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 2.50s
                        Total time: 1883.85s
                               ETA: 8384.9s

################################################################################
                     [1m Learning iteration 734/4000 [0m

                       Computation: 3180 steps/s (collection: 0.507s, learning 2.069s)
               Value function loss: 54751.1577
                    Surrogate loss: 0.0163
             Mean action noise std: 0.94
                       Mean reward: 2301.89
               Mean episode length: 224.09
                 Mean success rate: 28.50
                  Mean reward/step: 12.15
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6021120
                    Iteration time: 2.58s
                        Total time: 1886.43s
                               ETA: 8382.4s

################################################################################
                     [1m Learning iteration 735/4000 [0m

                       Computation: 3274 steps/s (collection: 0.480s, learning 2.022s)
               Value function loss: 38865.3061
                    Surrogate loss: 0.0160
             Mean action noise std: 0.94
                       Mean reward: 2287.36
               Mean episode length: 227.03
                 Mean success rate: 31.00
                  Mean reward/step: 11.75
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 2.50s
                        Total time: 1888.93s
                               ETA: 8379.6s

################################################################################
                     [1m Learning iteration 736/4000 [0m

                       Computation: 3247 steps/s (collection: 0.481s, learning 2.042s)
               Value function loss: 54185.4020
                    Surrogate loss: 0.0153
             Mean action noise std: 0.94
                       Mean reward: 2447.10
               Mean episode length: 232.72
                 Mean success rate: 33.00
                  Mean reward/step: 11.34
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 6037504
                    Iteration time: 2.52s
                        Total time: 1891.45s
                               ETA: 8376.8s

################################################################################
                     [1m Learning iteration 737/4000 [0m

                       Computation: 3171 steps/s (collection: 0.510s, learning 2.073s)
               Value function loss: 40956.3902
                    Surrogate loss: 0.0163
             Mean action noise std: 0.94
                       Mean reward: 2536.44
               Mean episode length: 236.00
                 Mean success rate: 32.50
                  Mean reward/step: 10.69
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 2.58s
                        Total time: 1894.04s
                               ETA: 8374.3s

################################################################################
                     [1m Learning iteration 738/4000 [0m

                       Computation: 3203 steps/s (collection: 0.507s, learning 2.050s)
               Value function loss: 58534.3882
                    Surrogate loss: 0.0148
             Mean action noise std: 0.94
                       Mean reward: 2933.01
               Mean episode length: 261.99
                 Mean success rate: 37.00
                  Mean reward/step: 10.36
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6053888
                    Iteration time: 2.56s
                        Total time: 1896.59s
                               ETA: 8371.7s

################################################################################
                     [1m Learning iteration 739/4000 [0m

                       Computation: 3314 steps/s (collection: 0.446s, learning 2.025s)
               Value function loss: 47959.8861
                    Surrogate loss: 0.0161
             Mean action noise std: 0.94
                       Mean reward: 3250.93
               Mean episode length: 280.37
                 Mean success rate: 42.00
                  Mean reward/step: 10.45
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 2.47s
                        Total time: 1899.06s
                               ETA: 8368.7s

################################################################################
                     [1m Learning iteration 740/4000 [0m

                       Computation: 3274 steps/s (collection: 0.455s, learning 2.047s)
               Value function loss: 42492.6287
                    Surrogate loss: 0.0190
             Mean action noise std: 0.94
                       Mean reward: 3242.58
               Mean episode length: 284.52
                 Mean success rate: 41.50
                  Mean reward/step: 10.30
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6070272
                    Iteration time: 2.50s
                        Total time: 1901.57s
                               ETA: 8365.9s

################################################################################
                     [1m Learning iteration 741/4000 [0m

                       Computation: 3220 steps/s (collection: 0.481s, learning 2.063s)
               Value function loss: 57953.8500
                    Surrogate loss: 0.0138
             Mean action noise std: 0.94
                       Mean reward: 3158.15
               Mean episode length: 279.19
                 Mean success rate: 38.50
                  Mean reward/step: 10.42
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 2.54s
                        Total time: 1904.11s
                               ETA: 8363.2s

################################################################################
                     [1m Learning iteration 742/4000 [0m

                       Computation: 3265 steps/s (collection: 0.474s, learning 2.035s)
               Value function loss: 47743.6247
                    Surrogate loss: 0.0140
             Mean action noise std: 0.94
                       Mean reward: 3158.65
               Mean episode length: 280.44
                 Mean success rate: 39.50
                  Mean reward/step: 10.06
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6086656
                    Iteration time: 2.51s
                        Total time: 1906.62s
                               ETA: 8360.4s

################################################################################
                     [1m Learning iteration 743/4000 [0m

                       Computation: 3270 steps/s (collection: 0.490s, learning 2.015s)
               Value function loss: 22393.9994
                    Surrogate loss: 0.0182
             Mean action noise std: 0.94
                       Mean reward: 2809.16
               Mean episode length: 262.95
                 Mean success rate: 36.50
                  Mean reward/step: 10.37
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 2.51s
                        Total time: 1909.12s
                               ETA: 8357.5s

################################################################################
                     [1m Learning iteration 744/4000 [0m

                       Computation: 3258 steps/s (collection: 0.450s, learning 2.064s)
               Value function loss: 31037.9420
                    Surrogate loss: 0.0154
             Mean action noise std: 0.94
                       Mean reward: 2486.40
               Mean episode length: 242.59
                 Mean success rate: 33.00
                  Mean reward/step: 11.41
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6103040
                    Iteration time: 2.51s
                        Total time: 1911.64s
                               ETA: 8354.8s

################################################################################
                     [1m Learning iteration 745/4000 [0m

                       Computation: 3286 steps/s (collection: 0.428s, learning 2.065s)
               Value function loss: 41832.8172
                    Surrogate loss: 0.0133
             Mean action noise std: 0.94
                       Mean reward: 2362.64
               Mean episode length: 233.31
                 Mean success rate: 32.00
                  Mean reward/step: 11.66
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 2.49s
                        Total time: 1914.13s
                               ETA: 8351.9s

################################################################################
                     [1m Learning iteration 746/4000 [0m

                       Computation: 3112 steps/s (collection: 0.482s, learning 2.150s)
               Value function loss: 35916.0768
                    Surrogate loss: 0.0130
             Mean action noise std: 0.94
                       Mean reward: 2075.93
               Mean episode length: 214.78
                 Mean success rate: 30.50
                  Mean reward/step: 11.96
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6119424
                    Iteration time: 2.63s
                        Total time: 1916.76s
                               ETA: 8349.6s

################################################################################
                     [1m Learning iteration 747/4000 [0m

                       Computation: 3114 steps/s (collection: 0.530s, learning 2.100s)
               Value function loss: 40591.2069
                    Surrogate loss: 0.0136
             Mean action noise std: 0.94
                       Mean reward: 2224.80
               Mean episode length: 218.05
                 Mean success rate: 32.50
                  Mean reward/step: 12.49
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 2.63s
                        Total time: 1919.39s
                               ETA: 8347.3s

################################################################################
                     [1m Learning iteration 748/4000 [0m

                       Computation: 3141 steps/s (collection: 0.489s, learning 2.118s)
               Value function loss: 41569.2627
                    Surrogate loss: 0.0188
             Mean action noise std: 0.94
                       Mean reward: 2309.81
               Mean episode length: 217.46
                 Mean success rate: 32.00
                  Mean reward/step: 12.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6135808
                    Iteration time: 2.61s
                        Total time: 1922.00s
                               ETA: 8344.9s

################################################################################
                     [1m Learning iteration 749/4000 [0m

                       Computation: 3126 steps/s (collection: 0.531s, learning 2.089s)
               Value function loss: 42365.4151
                    Surrogate loss: 0.0153
             Mean action noise std: 0.94
                       Mean reward: 2271.38
               Mean episode length: 217.86
                 Mean success rate: 32.50
                  Mean reward/step: 12.82
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 2.62s
                        Total time: 1924.62s
                               ETA: 8342.6s

################################################################################
                     [1m Learning iteration 750/4000 [0m

                       Computation: 3163 steps/s (collection: 0.517s, learning 2.073s)
               Value function loss: 40457.6712
                    Surrogate loss: 0.0157
             Mean action noise std: 0.94
                       Mean reward: 2599.15
               Mean episode length: 235.53
                 Mean success rate: 36.00
                  Mean reward/step: 12.77
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6152192
                    Iteration time: 2.59s
                        Total time: 1927.21s
                               ETA: 8340.1s

################################################################################
                     [1m Learning iteration 751/4000 [0m

                       Computation: 3179 steps/s (collection: 0.475s, learning 2.101s)
               Value function loss: 46996.5578
                    Surrogate loss: 0.0147
             Mean action noise std: 0.94
                       Mean reward: 2693.44
               Mean episode length: 243.40
                 Mean success rate: 36.50
                  Mean reward/step: 13.03
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 2.58s
                        Total time: 1929.79s
                               ETA: 8337.6s

################################################################################
                     [1m Learning iteration 752/4000 [0m

                       Computation: 3289 steps/s (collection: 0.423s, learning 2.067s)
               Value function loss: 61412.8618
                    Surrogate loss: 0.0119
             Mean action noise std: 0.94
                       Mean reward: 2900.95
               Mean episode length: 243.49
                 Mean success rate: 37.50
                  Mean reward/step: 13.11
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6168576
                    Iteration time: 2.49s
                        Total time: 1932.28s
                               ETA: 8334.7s

################################################################################
                     [1m Learning iteration 753/4000 [0m

                       Computation: 3226 steps/s (collection: 0.478s, learning 2.062s)
               Value function loss: 70658.9267
                    Surrogate loss: 0.0145
             Mean action noise std: 0.94
                       Mean reward: 3563.28
               Mean episode length: 275.25
                 Mean success rate: 44.50
                  Mean reward/step: 12.81
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 2.54s
                        Total time: 1934.82s
                               ETA: 8332.0s

################################################################################
                     [1m Learning iteration 754/4000 [0m

                       Computation: 3141 steps/s (collection: 0.533s, learning 2.075s)
               Value function loss: 60945.7403
                    Surrogate loss: 0.0131
             Mean action noise std: 0.94
                       Mean reward: 3514.60
               Mean episode length: 272.73
                 Mean success rate: 43.00
                  Mean reward/step: 12.64
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6184960
                    Iteration time: 2.61s
                        Total time: 1937.42s
                               ETA: 8329.6s

################################################################################
                     [1m Learning iteration 755/4000 [0m

                       Computation: 3188 steps/s (collection: 0.507s, learning 2.062s)
               Value function loss: 41580.0273
                    Surrogate loss: 0.0193
             Mean action noise std: 0.94
                       Mean reward: 3536.41
               Mean episode length: 274.44
                 Mean success rate: 43.00
                  Mean reward/step: 12.11
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 2.57s
                        Total time: 1939.99s
                               ETA: 8327.1s

################################################################################
                     [1m Learning iteration 756/4000 [0m

                       Computation: 3231 steps/s (collection: 0.430s, learning 2.105s)
               Value function loss: 61849.1037
                    Surrogate loss: 0.0170
             Mean action noise std: 0.94
                       Mean reward: 3763.47
               Mean episode length: 283.03
                 Mean success rate: 44.50
                  Mean reward/step: 11.49
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6201344
                    Iteration time: 2.54s
                        Total time: 1942.53s
                               ETA: 8324.4s

################################################################################
                     [1m Learning iteration 757/4000 [0m

                       Computation: 3137 steps/s (collection: 0.508s, learning 2.103s)
               Value function loss: 49849.2986
                    Surrogate loss: 0.0172
             Mean action noise std: 0.94
                       Mean reward: 3850.42
               Mean episode length: 282.93
                 Mean success rate: 45.00
                  Mean reward/step: 10.94
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 2.61s
                        Total time: 1945.14s
                               ETA: 8322.0s

################################################################################
                     [1m Learning iteration 758/4000 [0m

                       Computation: 3254 steps/s (collection: 0.446s, learning 2.071s)
               Value function loss: 46866.8482
                    Surrogate loss: 0.0175
             Mean action noise std: 0.94
                       Mean reward: 3691.01
               Mean episode length: 278.53
                 Mean success rate: 44.00
                  Mean reward/step: 10.79
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6217728
                    Iteration time: 2.52s
                        Total time: 1947.66s
                               ETA: 8319.2s

################################################################################
                     [1m Learning iteration 759/4000 [0m

                       Computation: 3205 steps/s (collection: 0.460s, learning 2.095s)
               Value function loss: 42103.2143
                    Surrogate loss: 0.0179
             Mean action noise std: 0.94
                       Mean reward: 3277.38
               Mean episode length: 261.15
                 Mean success rate: 39.50
                  Mean reward/step: 10.84
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 2.56s
                        Total time: 1950.21s
                               ETA: 8316.6s

################################################################################
                     [1m Learning iteration 760/4000 [0m

                       Computation: 3132 steps/s (collection: 0.503s, learning 2.113s)
               Value function loss: 46906.2301
                    Surrogate loss: 0.0179
             Mean action noise std: 0.94
                       Mean reward: 3201.36
               Mean episode length: 271.30
                 Mean success rate: 38.50
                  Mean reward/step: 10.66
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6234112
                    Iteration time: 2.62s
                        Total time: 1952.83s
                               ETA: 8314.3s

################################################################################
                     [1m Learning iteration 761/4000 [0m

                       Computation: 3147 steps/s (collection: 0.522s, learning 2.081s)
               Value function loss: 43083.3014
                    Surrogate loss: 0.0200
             Mean action noise std: 0.94
                       Mean reward: 3099.68
               Mean episode length: 269.44
                 Mean success rate: 38.00
                  Mean reward/step: 10.89
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 2.60s
                        Total time: 1955.43s
                               ETA: 8311.9s

################################################################################
                     [1m Learning iteration 762/4000 [0m

                       Computation: 3153 steps/s (collection: 0.481s, learning 2.116s)
               Value function loss: 38991.8398
                    Surrogate loss: 0.0232
             Mean action noise std: 0.94
                       Mean reward: 2462.39
               Mean episode length: 241.97
                 Mean success rate: 32.00
                  Mean reward/step: 11.34
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6250496
                    Iteration time: 2.60s
                        Total time: 1958.03s
                               ETA: 8309.4s

################################################################################
                     [1m Learning iteration 763/4000 [0m

                       Computation: 3177 steps/s (collection: 0.473s, learning 2.105s)
               Value function loss: 47543.1832
                    Surrogate loss: 0.0124
             Mean action noise std: 0.94
                       Mean reward: 2453.99
               Mean episode length: 242.38
                 Mean success rate: 32.00
                  Mean reward/step: 11.81
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 2.58s
                        Total time: 1960.60s
                               ETA: 8306.9s

################################################################################
                     [1m Learning iteration 764/4000 [0m

                       Computation: 3101 steps/s (collection: 0.516s, learning 2.125s)
               Value function loss: 43444.5797
                    Surrogate loss: 0.0145
             Mean action noise std: 0.94
                       Mean reward: 2186.51
               Mean episode length: 231.56
                 Mean success rate: 28.50
                  Mean reward/step: 11.83
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6266880
                    Iteration time: 2.64s
                        Total time: 1963.25s
                               ETA: 8304.7s

################################################################################
                     [1m Learning iteration 765/4000 [0m

                       Computation: 3182 steps/s (collection: 0.458s, learning 2.116s)
               Value function loss: 38439.9636
                    Surrogate loss: 0.0152
             Mean action noise std: 0.94
                       Mean reward: 2274.15
               Mean episode length: 227.16
                 Mean success rate: 27.50
                  Mean reward/step: 11.51
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 2.57s
                        Total time: 1965.82s
                               ETA: 8302.1s

################################################################################
                     [1m Learning iteration 766/4000 [0m

                       Computation: 3224 steps/s (collection: 0.454s, learning 2.087s)
               Value function loss: 58581.8131
                    Surrogate loss: 0.0129
             Mean action noise std: 0.94
                       Mean reward: 2487.76
               Mean episode length: 224.72
                 Mean success rate: 30.50
                  Mean reward/step: 10.92
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6283264
                    Iteration time: 2.54s
                        Total time: 1968.36s
                               ETA: 8299.5s

################################################################################
                     [1m Learning iteration 767/4000 [0m

                       Computation: 3140 steps/s (collection: 0.500s, learning 2.108s)
               Value function loss: 55500.8316
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 2750.96
               Mean episode length: 234.22
                 Mean success rate: 32.50
                  Mean reward/step: 10.78
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 2.61s
                        Total time: 1970.97s
                               ETA: 8297.1s

################################################################################
                     [1m Learning iteration 768/4000 [0m

                       Computation: 3238 steps/s (collection: 0.469s, learning 2.060s)
               Value function loss: 24291.3596
                    Surrogate loss: 0.0187
             Mean action noise std: 0.94
                       Mean reward: 2534.75
               Mean episode length: 223.28
                 Mean success rate: 29.00
                  Mean reward/step: 10.64
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6299648
                    Iteration time: 2.53s
                        Total time: 1973.50s
                               ETA: 8294.3s

################################################################################
                     [1m Learning iteration 769/4000 [0m

                       Computation: 3276 steps/s (collection: 0.448s, learning 2.052s)
               Value function loss: 42820.9260
                    Surrogate loss: 0.0177
             Mean action noise std: 0.94
                       Mean reward: 2652.91
               Mean episode length: 235.96
                 Mean success rate: 32.50
                  Mean reward/step: 11.09
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 2.50s
                        Total time: 1976.00s
                               ETA: 8291.5s

################################################################################
                     [1m Learning iteration 770/4000 [0m

                       Computation: 3316 steps/s (collection: 0.446s, learning 2.024s)
               Value function loss: 44037.9121
                    Surrogate loss: 0.0136
             Mean action noise std: 0.94
                       Mean reward: 2977.34
               Mean episode length: 244.91
                 Mean success rate: 36.00
                  Mean reward/step: 10.84
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6316032
                    Iteration time: 2.47s
                        Total time: 1978.47s
                               ETA: 8288.5s

################################################################################
                     [1m Learning iteration 771/4000 [0m

                       Computation: 3236 steps/s (collection: 0.452s, learning 2.079s)
               Value function loss: 48300.6748
                    Surrogate loss: 0.0128
             Mean action noise std: 0.94
                       Mean reward: 3233.15
               Mean episode length: 263.75
                 Mean success rate: 39.00
                  Mean reward/step: 11.05
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 2.53s
                        Total time: 1981.00s
                               ETA: 8285.8s

################################################################################
                     [1m Learning iteration 772/4000 [0m

                       Computation: 3194 steps/s (collection: 0.478s, learning 2.087s)
               Value function loss: 36723.7248
                    Surrogate loss: 0.0142
             Mean action noise std: 0.94
                       Mean reward: 2646.46
               Mean episode length: 245.58
                 Mean success rate: 33.50
                  Mean reward/step: 10.97
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6332416
                    Iteration time: 2.56s
                        Total time: 1983.56s
                               ETA: 8283.2s

################################################################################
                     [1m Learning iteration 773/4000 [0m

                       Computation: 3228 steps/s (collection: 0.460s, learning 2.077s)
               Value function loss: 41766.3074
                    Surrogate loss: 0.0157
             Mean action noise std: 0.94
                       Mean reward: 2533.69
               Mean episode length: 242.60
                 Mean success rate: 32.00
                  Mean reward/step: 11.64
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 2.54s
                        Total time: 1986.10s
                               ETA: 8280.6s

################################################################################
                     [1m Learning iteration 774/4000 [0m

                       Computation: 3275 steps/s (collection: 0.454s, learning 2.047s)
               Value function loss: 50925.3351
                    Surrogate loss: 0.0132
             Mean action noise std: 0.94
                       Mean reward: 3015.99
               Mean episode length: 262.42
                 Mean success rate: 37.00
                  Mean reward/step: 11.79
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6348800
                    Iteration time: 2.50s
                        Total time: 1988.60s
                               ETA: 8277.7s

################################################################################
                     [1m Learning iteration 775/4000 [0m

                       Computation: 3188 steps/s (collection: 0.451s, learning 2.118s)
               Value function loss: 58563.5657
                    Surrogate loss: 0.0120
             Mean action noise std: 0.94
                       Mean reward: 3135.11
               Mean episode length: 264.25
                 Mean success rate: 37.00
                  Mean reward/step: 11.13
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 2.57s
                        Total time: 1991.17s
                               ETA: 8275.2s

################################################################################
                     [1m Learning iteration 776/4000 [0m

                       Computation: 3206 steps/s (collection: 0.470s, learning 2.084s)
               Value function loss: 46506.2153
                    Surrogate loss: 0.0135
             Mean action noise std: 0.94
                       Mean reward: 3085.40
               Mean episode length: 264.50
                 Mean success rate: 36.00
                  Mean reward/step: 10.30
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 6365184
                    Iteration time: 2.55s
                        Total time: 1993.73s
                               ETA: 8272.6s

################################################################################
                     [1m Learning iteration 777/4000 [0m

                       Computation: 3222 steps/s (collection: 0.484s, learning 2.059s)
               Value function loss: 48375.2037
                    Surrogate loss: 0.0146
             Mean action noise std: 0.94
                       Mean reward: 2898.54
               Mean episode length: 255.93
                 Mean success rate: 34.50
                  Mean reward/step: 11.17
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 2.54s
                        Total time: 1996.27s
                               ETA: 8269.9s

################################################################################
                     [1m Learning iteration 778/4000 [0m

                       Computation: 3195 steps/s (collection: 0.455s, learning 2.108s)
               Value function loss: 39762.7213
                    Surrogate loss: 0.0143
             Mean action noise std: 0.94
                       Mean reward: 2933.93
               Mean episode length: 254.84
                 Mean success rate: 35.00
                  Mean reward/step: 11.86
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6381568
                    Iteration time: 2.56s
                        Total time: 1998.83s
                               ETA: 8267.3s

################################################################################
                     [1m Learning iteration 779/4000 [0m

                       Computation: 3229 steps/s (collection: 0.454s, learning 2.083s)
               Value function loss: 36177.5861
                    Surrogate loss: 0.0149
             Mean action noise std: 0.94
                       Mean reward: 2695.14
               Mean episode length: 242.94
                 Mean success rate: 32.50
                  Mean reward/step: 11.95
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 2.54s
                        Total time: 2001.37s
                               ETA: 8264.6s

################################################################################
                     [1m Learning iteration 780/4000 [0m

                       Computation: 3019 steps/s (collection: 0.526s, learning 2.188s)
               Value function loss: 61201.7794
                    Surrogate loss: 0.0138
             Mean action noise std: 0.94
                       Mean reward: 2672.85
               Mean episode length: 242.81
                 Mean success rate: 32.50
                  Mean reward/step: 11.66
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6397952
                    Iteration time: 2.71s
                        Total time: 2004.08s
                               ETA: 8262.7s

################################################################################
                     [1m Learning iteration 781/4000 [0m

                       Computation: 3068 steps/s (collection: 0.527s, learning 2.142s)
               Value function loss: 37215.0940
                    Surrogate loss: 0.0154
             Mean action noise std: 0.94
                       Mean reward: 2239.06
               Mean episode length: 218.96
                 Mean success rate: 26.50
                  Mean reward/step: 11.06
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 2.67s
                        Total time: 2006.75s
                               ETA: 8260.5s

################################################################################
                     [1m Learning iteration 782/4000 [0m

                       Computation: 3076 steps/s (collection: 0.523s, learning 2.140s)
               Value function loss: 54726.9465
                    Surrogate loss: 0.0161
             Mean action noise std: 0.94
                       Mean reward: 2274.44
               Mean episode length: 221.31
                 Mean success rate: 27.00
                  Mean reward/step: 10.70
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6414336
                    Iteration time: 2.66s
                        Total time: 2009.41s
                               ETA: 8258.4s

################################################################################
                     [1m Learning iteration 783/4000 [0m

                       Computation: 3227 steps/s (collection: 0.467s, learning 2.072s)
               Value function loss: 55083.2968
                    Surrogate loss: 0.0144
             Mean action noise std: 0.94
                       Mean reward: 2358.52
               Mean episode length: 224.63
                 Mean success rate: 28.50
                  Mean reward/step: 10.95
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 2.54s
                        Total time: 2011.95s
                               ETA: 8255.7s

################################################################################
                     [1m Learning iteration 784/4000 [0m

                       Computation: 3161 steps/s (collection: 0.482s, learning 2.110s)
               Value function loss: 41670.7048
                    Surrogate loss: 0.0154
             Mean action noise std: 0.94
                       Mean reward: 2441.23
               Mean episode length: 226.47
                 Mean success rate: 28.50
                  Mean reward/step: 11.21
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6430720
                    Iteration time: 2.59s
                        Total time: 2014.54s
                               ETA: 8253.2s

################################################################################
                     [1m Learning iteration 785/4000 [0m

                       Computation: 3160 steps/s (collection: 0.439s, learning 2.153s)
               Value function loss: 38630.9154
                    Surrogate loss: 0.0226
             Mean action noise std: 0.94
                       Mean reward: 2662.31
               Mean episode length: 229.75
                 Mean success rate: 31.50
                  Mean reward/step: 11.32
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 2.59s
                        Total time: 2017.14s
                               ETA: 8250.8s

################################################################################
                     [1m Learning iteration 786/4000 [0m

                       Computation: 3096 steps/s (collection: 0.526s, learning 2.120s)
               Value function loss: 44597.3600
                    Surrogate loss: 0.0158
             Mean action noise std: 0.94
                       Mean reward: 2824.44
               Mean episode length: 238.59
                 Mean success rate: 34.00
                  Mean reward/step: 11.89
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6447104
                    Iteration time: 2.65s
                        Total time: 2019.78s
                               ETA: 8248.5s

################################################################################
                     [1m Learning iteration 787/4000 [0m

                       Computation: 3087 steps/s (collection: 0.546s, learning 2.107s)
               Value function loss: 61421.6904
                    Surrogate loss: 0.0127
             Mean action noise std: 0.94
                       Mean reward: 2882.19
               Mean episode length: 241.70
                 Mean success rate: 35.00
                  Mean reward/step: 11.69
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 2.65s
                        Total time: 2022.44s
                               ETA: 8246.3s

################################################################################
                     [1m Learning iteration 788/4000 [0m

                       Computation: 3141 steps/s (collection: 0.497s, learning 2.110s)
               Value function loss: 51044.3987
                    Surrogate loss: 0.0106
             Mean action noise std: 0.94
                       Mean reward: 2957.80
               Mean episode length: 233.60
                 Mean success rate: 34.50
                  Mean reward/step: 11.20
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 6463488
                    Iteration time: 2.61s
                        Total time: 2025.04s
                               ETA: 8243.9s

################################################################################
                     [1m Learning iteration 789/4000 [0m

                       Computation: 3047 steps/s (collection: 0.534s, learning 2.153s)
               Value function loss: 41074.9092
                    Surrogate loss: 0.0136
             Mean action noise std: 0.94
                       Mean reward: 2978.72
               Mean episode length: 245.00
                 Mean success rate: 34.50
                  Mean reward/step: 10.96
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 2.69s
                        Total time: 2027.73s
                               ETA: 8241.8s

################################################################################
                     [1m Learning iteration 790/4000 [0m

                       Computation: 3091 steps/s (collection: 0.533s, learning 2.117s)
               Value function loss: 46965.9877
                    Surrogate loss: 0.0112
             Mean action noise std: 0.94
                       Mean reward: 2971.02
               Mean episode length: 242.94
                 Mean success rate: 34.00
                  Mean reward/step: 11.61
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6479872
                    Iteration time: 2.65s
                        Total time: 2030.38s
                               ETA: 8239.6s

################################################################################
                     [1m Learning iteration 791/4000 [0m

                       Computation: 3160 steps/s (collection: 0.501s, learning 2.091s)
               Value function loss: 42537.7878
                    Surrogate loss: 0.0126
             Mean action noise std: 0.94
                       Mean reward: 2901.34
               Mean episode length: 247.79
                 Mean success rate: 35.50
                  Mean reward/step: 11.47
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 2.59s
                        Total time: 2032.97s
                               ETA: 8237.1s

################################################################################
                     [1m Learning iteration 792/4000 [0m

                       Computation: 3175 steps/s (collection: 0.465s, learning 2.115s)
               Value function loss: 50124.5532
                    Surrogate loss: 0.0152
             Mean action noise std: 0.94
                       Mean reward: 2921.68
               Mean episode length: 247.28
                 Mean success rate: 35.50
                  Mean reward/step: 11.98
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6496256
                    Iteration time: 2.58s
                        Total time: 2035.55s
                               ETA: 8234.6s

################################################################################
                     [1m Learning iteration 793/4000 [0m

                       Computation: 3133 steps/s (collection: 0.504s, learning 2.110s)
               Value function loss: 41446.2789
                    Surrogate loss: 0.0172
             Mean action noise std: 0.94
                       Mean reward: 2835.66
               Mean episode length: 249.98
                 Mean success rate: 34.00
                  Mean reward/step: 12.49
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 2.61s
                        Total time: 2038.17s
                               ETA: 8232.2s

################################################################################
                     [1m Learning iteration 794/4000 [0m

                       Computation: 3192 steps/s (collection: 0.472s, learning 2.094s)
               Value function loss: 29593.9656
                    Surrogate loss: 0.0186
             Mean action noise std: 0.94
                       Mean reward: 2732.82
               Mean episode length: 252.85
                 Mean success rate: 33.00
                  Mean reward/step: 13.29
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6512640
                    Iteration time: 2.57s
                        Total time: 2040.73s
                               ETA: 8229.7s

################################################################################
                     [1m Learning iteration 795/4000 [0m

                       Computation: 3149 steps/s (collection: 0.477s, learning 2.124s)
               Value function loss: 40966.5979
                    Surrogate loss: 0.0141
             Mean action noise std: 0.94
                       Mean reward: 2680.12
               Mean episode length: 250.85
                 Mean success rate: 33.00
                  Mean reward/step: 13.84
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 2.60s
                        Total time: 2043.33s
                               ETA: 8227.2s

################################################################################
                     [1m Learning iteration 796/4000 [0m

                       Computation: 3160 steps/s (collection: 0.506s, learning 2.086s)
               Value function loss: 49461.5353
                    Surrogate loss: 0.0120
             Mean action noise std: 0.94
                       Mean reward: 2815.08
               Mean episode length: 255.61
                 Mean success rate: 36.00
                  Mean reward/step: 14.03
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6529024
                    Iteration time: 2.59s
                        Total time: 2045.93s
                               ETA: 8224.8s

################################################################################
                     [1m Learning iteration 797/4000 [0m

                       Computation: 3065 steps/s (collection: 0.525s, learning 2.147s)
               Value function loss: 48687.6720
                    Surrogate loss: 0.0168
             Mean action noise std: 0.94
                       Mean reward: 3032.08
               Mean episode length: 264.00
                 Mean success rate: 38.50
                  Mean reward/step: 13.06
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 2.67s
                        Total time: 2048.60s
                               ETA: 8222.6s

################################################################################
                     [1m Learning iteration 798/4000 [0m

                       Computation: 3105 steps/s (collection: 0.518s, learning 2.120s)
               Value function loss: 59056.9985
                    Surrogate loss: 0.0156
             Mean action noise std: 0.94
                       Mean reward: 3219.26
               Mean episode length: 268.18
                 Mean success rate: 38.00
                  Mean reward/step: 12.29
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6545408
                    Iteration time: 2.64s
                        Total time: 2051.24s
                               ETA: 8220.3s

################################################################################
                     [1m Learning iteration 799/4000 [0m

                       Computation: 3134 steps/s (collection: 0.459s, learning 2.154s)
               Value function loss: 56584.6937
                    Surrogate loss: 0.0157
             Mean action noise std: 0.94
                       Mean reward: 3192.54
               Mean episode length: 261.49
                 Mean success rate: 38.50
                  Mean reward/step: 12.23
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 2.61s
                        Total time: 2053.85s
                               ETA: 8218.0s

################################################################################
                     [1m Learning iteration 800/4000 [0m

                       Computation: 3160 steps/s (collection: 0.494s, learning 2.098s)
               Value function loss: 52538.9062
                    Surrogate loss: 0.0137
             Mean action noise std: 0.94
                       Mean reward: 3437.34
               Mean episode length: 272.90
                 Mean success rate: 42.50
                  Mean reward/step: 12.16
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6561792
                    Iteration time: 2.59s
                        Total time: 2056.44s
                               ETA: 8215.5s

################################################################################
                     [1m Learning iteration 801/4000 [0m

                       Computation: 3069 steps/s (collection: 0.549s, learning 2.120s)
               Value function loss: 44863.5488
                    Surrogate loss: 0.0152
             Mean action noise std: 0.94
                       Mean reward: 3461.48
               Mean episode length: 274.99
                 Mean success rate: 44.00
                  Mean reward/step: 11.83
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 2.67s
                        Total time: 2059.11s
                               ETA: 8213.3s

################################################################################
                     [1m Learning iteration 802/4000 [0m

                       Computation: 3173 steps/s (collection: 0.477s, learning 2.104s)
               Value function loss: 46468.5384
                    Surrogate loss: 0.0147
             Mean action noise std: 0.94
                       Mean reward: 3452.82
               Mean episode length: 271.85
                 Mean success rate: 43.00
                  Mean reward/step: 12.84
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6578176
                    Iteration time: 2.58s
                        Total time: 2061.69s
                               ETA: 8210.8s

################################################################################
                     [1m Learning iteration 803/4000 [0m

                       Computation: 3218 steps/s (collection: 0.463s, learning 2.082s)
               Value function loss: 40034.3083
                    Surrogate loss: 0.0162
             Mean action noise std: 0.94
                       Mean reward: 3285.18
               Mean episode length: 269.76
                 Mean success rate: 41.00
                  Mean reward/step: 13.46
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 2.54s
                        Total time: 2064.24s
                               ETA: 8208.2s

################################################################################
                     [1m Learning iteration 804/4000 [0m

                       Computation: 3173 steps/s (collection: 0.468s, learning 2.113s)
               Value function loss: 50299.7422
                    Surrogate loss: 0.0160
             Mean action noise std: 0.94
                       Mean reward: 3401.66
               Mean episode length: 273.81
                 Mean success rate: 42.00
                  Mean reward/step: 13.09
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6594560
                    Iteration time: 2.58s
                        Total time: 2066.82s
                               ETA: 8205.6s

################################################################################
                     [1m Learning iteration 805/4000 [0m

                       Computation: 3131 steps/s (collection: 0.483s, learning 2.133s)
               Value function loss: 55599.4642
                    Surrogate loss: 0.0152
             Mean action noise std: 0.94
                       Mean reward: 3276.45
               Mean episode length: 266.90
                 Mean success rate: 40.00
                  Mean reward/step: 13.53
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 2.62s
                        Total time: 2069.43s
                               ETA: 8203.3s

################################################################################
                     [1m Learning iteration 806/4000 [0m

                       Computation: 3166 steps/s (collection: 0.480s, learning 2.107s)
               Value function loss: 64074.0913
                    Surrogate loss: 0.0165
             Mean action noise std: 0.94
                       Mean reward: 3190.17
               Mean episode length: 260.52
                 Mean success rate: 39.00
                  Mean reward/step: 13.76
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6610944
                    Iteration time: 2.59s
                        Total time: 2072.02s
                               ETA: 8200.8s

################################################################################
                     [1m Learning iteration 807/4000 [0m

                       Computation: 3167 steps/s (collection: 0.472s, learning 2.114s)
               Value function loss: 56448.6833
                    Surrogate loss: 0.0162
             Mean action noise std: 0.94
                       Mean reward: 3426.39
               Mean episode length: 272.34
                 Mean success rate: 41.00
                  Mean reward/step: 13.18
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 2.59s
                        Total time: 2074.61s
                               ETA: 8198.3s

################################################################################
                     [1m Learning iteration 808/4000 [0m

                       Computation: 3133 steps/s (collection: 0.512s, learning 2.102s)
               Value function loss: 52896.9647
                    Surrogate loss: 0.0137
             Mean action noise std: 0.94
                       Mean reward: 3651.27
               Mean episode length: 278.08
                 Mean success rate: 42.50
                  Mean reward/step: 12.86
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6627328
                    Iteration time: 2.61s
                        Total time: 2077.22s
                               ETA: 8195.9s

################################################################################
                     [1m Learning iteration 809/4000 [0m

                       Computation: 3172 steps/s (collection: 0.473s, learning 2.109s)
               Value function loss: 40454.0395
                    Surrogate loss: 0.0155
             Mean action noise std: 0.94
                       Mean reward: 3766.63
               Mean episode length: 287.20
                 Mean success rate: 44.50
                  Mean reward/step: 12.88
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 2.58s
                        Total time: 2079.80s
                               ETA: 8193.4s

################################################################################
                     [1m Learning iteration 810/4000 [0m

                       Computation: 3163 steps/s (collection: 0.462s, learning 2.128s)
               Value function loss: 41135.0953
                    Surrogate loss: 0.0170
             Mean action noise std: 0.94
                       Mean reward: 3731.95
               Mean episode length: 288.08
                 Mean success rate: 43.50
                  Mean reward/step: 12.83
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6643712
                    Iteration time: 2.59s
                        Total time: 2082.39s
                               ETA: 8190.9s

################################################################################
                     [1m Learning iteration 811/4000 [0m

                       Computation: 3168 steps/s (collection: 0.480s, learning 2.106s)
               Value function loss: 36052.4316
                    Surrogate loss: 0.0207
             Mean action noise std: 0.94
                       Mean reward: 3494.68
               Mean episode length: 281.40
                 Mean success rate: 40.50
                  Mean reward/step: 12.53
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 2.59s
                        Total time: 2084.98s
                               ETA: 8188.4s

################################################################################
                     [1m Learning iteration 812/4000 [0m

                       Computation: 3105 steps/s (collection: 0.492s, learning 2.146s)
               Value function loss: 59934.6787
                    Surrogate loss: 0.0178
             Mean action noise std: 0.94
                       Mean reward: 3795.05
               Mean episode length: 292.67
                 Mean success rate: 43.50
                  Mean reward/step: 12.71
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6660096
                    Iteration time: 2.64s
                        Total time: 2087.61s
                               ETA: 8186.1s

################################################################################
                     [1m Learning iteration 813/4000 [0m

                       Computation: 3101 steps/s (collection: 0.518s, learning 2.123s)
               Value function loss: 45886.7875
                    Surrogate loss: 0.0177
             Mean action noise std: 0.94
                       Mean reward: 3532.91
               Mean episode length: 285.63
                 Mean success rate: 41.50
                  Mean reward/step: 12.48
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 2.64s
                        Total time: 2090.26s
                               ETA: 8183.8s

################################################################################
                     [1m Learning iteration 814/4000 [0m

                       Computation: 3209 steps/s (collection: 0.476s, learning 2.077s)
               Value function loss: 54875.9800
                    Surrogate loss: 0.0168
             Mean action noise std: 0.94
                       Mean reward: 3480.40
               Mean episode length: 275.76
                 Mean success rate: 39.00
                  Mean reward/step: 12.30
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6676480
                    Iteration time: 2.55s
                        Total time: 2092.81s
                               ETA: 8181.2s

################################################################################
                     [1m Learning iteration 815/4000 [0m

                       Computation: 3151 steps/s (collection: 0.481s, learning 2.118s)
               Value function loss: 46494.0896
                    Surrogate loss: 0.0186
             Mean action noise std: 0.94
                       Mean reward: 3318.21
               Mean episode length: 266.81
                 Mean success rate: 37.50
                  Mean reward/step: 12.55
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 2.60s
                        Total time: 2095.41s
                               ETA: 8178.8s

################################################################################
                     [1m Learning iteration 816/4000 [0m

                       Computation: 3100 steps/s (collection: 0.502s, learning 2.139s)
               Value function loss: 41554.2552
                    Surrogate loss: 0.0147
             Mean action noise std: 0.94
                       Mean reward: 3364.04
               Mean episode length: 267.02
                 Mean success rate: 37.00
                  Mean reward/step: 12.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6692864
                    Iteration time: 2.64s
                        Total time: 2098.05s
                               ETA: 8176.5s

################################################################################
                     [1m Learning iteration 817/4000 [0m

                       Computation: 3182 steps/s (collection: 0.529s, learning 2.045s)
               Value function loss: 71589.7909
                    Surrogate loss: 0.0116
             Mean action noise std: 0.94
                       Mean reward: 3706.24
               Mean episode length: 277.53
                 Mean success rate: 41.00
                  Mean reward/step: 12.29
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 2.57s
                        Total time: 2100.62s
                               ETA: 8173.9s

################################################################################
                     [1m Learning iteration 818/4000 [0m

                       Computation: 3214 steps/s (collection: 0.460s, learning 2.088s)
               Value function loss: 69078.4542
                    Surrogate loss: 0.0113
             Mean action noise std: 0.94
                       Mean reward: 3865.44
               Mean episode length: 286.78
                 Mean success rate: 43.00
                  Mean reward/step: 12.50
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6709248
                    Iteration time: 2.55s
                        Total time: 2103.17s
                               ETA: 8171.3s

################################################################################
                     [1m Learning iteration 819/4000 [0m

                       Computation: 3192 steps/s (collection: 0.504s, learning 2.062s)
               Value function loss: 69702.0552
                    Surrogate loss: 0.0144
             Mean action noise std: 0.94
                       Mean reward: 4045.21
               Mean episode length: 289.57
                 Mean success rate: 44.50
                  Mean reward/step: 12.83
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 2.57s
                        Total time: 2105.74s
                               ETA: 8168.7s

################################################################################
                     [1m Learning iteration 820/4000 [0m

                       Computation: 3111 steps/s (collection: 0.566s, learning 2.067s)
               Value function loss: 55749.4702
                    Surrogate loss: 0.0195
             Mean action noise std: 0.94
                       Mean reward: 3965.73
               Mean episode length: 287.48
                 Mean success rate: 43.50
                  Mean reward/step: 13.37
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6725632
                    Iteration time: 2.63s
                        Total time: 2108.37s
                               ETA: 8166.4s

################################################################################
                     [1m Learning iteration 821/4000 [0m

                       Computation: 3127 steps/s (collection: 0.519s, learning 2.100s)
               Value function loss: 36286.2328
                    Surrogate loss: 0.0251
             Mean action noise std: 0.94
                       Mean reward: 3792.58
               Mean episode length: 290.36
                 Mean success rate: 44.00
                  Mean reward/step: 13.12
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 2.62s
                        Total time: 2110.99s
                               ETA: 8164.0s

################################################################################
                     [1m Learning iteration 822/4000 [0m

                       Computation: 3225 steps/s (collection: 0.479s, learning 2.060s)
               Value function loss: 62497.9902
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 3870.35
               Mean episode length: 289.06
                 Mean success rate: 44.50
                  Mean reward/step: 13.23
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 6742016
                    Iteration time: 2.54s
                        Total time: 2113.53s
                               ETA: 8161.4s

################################################################################
                     [1m Learning iteration 823/4000 [0m

                       Computation: 3155 steps/s (collection: 0.491s, learning 2.105s)
               Value function loss: 38503.3042
                    Surrogate loss: 0.0139
             Mean action noise std: 0.94
                       Mean reward: 3482.03
               Mean episode length: 277.00
                 Mean success rate: 41.50
                  Mean reward/step: 13.37
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 2.60s
                        Total time: 2116.13s
                               ETA: 8158.9s

################################################################################
                     [1m Learning iteration 824/4000 [0m

                       Computation: 3146 steps/s (collection: 0.506s, learning 2.098s)
               Value function loss: 37613.5722
                    Surrogate loss: 0.0183
             Mean action noise std: 0.94
                       Mean reward: 3292.49
               Mean episode length: 266.93
                 Mean success rate: 39.00
                  Mean reward/step: 13.74
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6758400
                    Iteration time: 2.60s
                        Total time: 2118.73s
                               ETA: 8156.5s

################################################################################
                     [1m Learning iteration 825/4000 [0m

                       Computation: 3169 steps/s (collection: 0.485s, learning 2.099s)
               Value function loss: 39874.5225
                    Surrogate loss: 0.0141
             Mean action noise std: 0.94
                       Mean reward: 3132.89
               Mean episode length: 259.28
                 Mean success rate: 38.50
                  Mean reward/step: 14.39
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 2.58s
                        Total time: 2121.31s
                               ETA: 8154.0s

################################################################################
                     [1m Learning iteration 826/4000 [0m

                       Computation: 3177 steps/s (collection: 0.477s, learning 2.101s)
               Value function loss: 62479.0638
                    Surrogate loss: 0.0143
             Mean action noise std: 0.94
                       Mean reward: 2949.02
               Mean episode length: 253.54
                 Mean success rate: 36.50
                  Mean reward/step: 14.24
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6774784
                    Iteration time: 2.58s
                        Total time: 2123.89s
                               ETA: 8151.4s

################################################################################
                     [1m Learning iteration 827/4000 [0m

                       Computation: 3172 steps/s (collection: 0.505s, learning 2.077s)
               Value function loss: 82682.6816
                    Surrogate loss: 0.0128
             Mean action noise std: 0.94
                       Mean reward: 3010.86
               Mean episode length: 248.46
                 Mean success rate: 37.50
                  Mean reward/step: 14.33
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 2.58s
                        Total time: 2126.47s
                               ETA: 8148.9s

################################################################################
                     [1m Learning iteration 828/4000 [0m

                       Computation: 3164 steps/s (collection: 0.496s, learning 2.092s)
               Value function loss: 68954.3785
                    Surrogate loss: 0.0121
             Mean action noise std: 0.94
                       Mean reward: 3191.75
               Mean episode length: 252.82
                 Mean success rate: 38.50
                  Mean reward/step: 13.89
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6791168
                    Iteration time: 2.59s
                        Total time: 2129.06s
                               ETA: 8146.4s

################################################################################
                     [1m Learning iteration 829/4000 [0m

                       Computation: 3199 steps/s (collection: 0.502s, learning 2.058s)
               Value function loss: 58015.0801
                    Surrogate loss: 0.0151
             Mean action noise std: 0.94
                       Mean reward: 3082.12
               Mean episode length: 248.34
                 Mean success rate: 37.50
                  Mean reward/step: 14.08
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 2.56s
                        Total time: 2131.62s
                               ETA: 8143.8s

################################################################################
                     [1m Learning iteration 830/4000 [0m

                       Computation: 3258 steps/s (collection: 0.469s, learning 2.045s)
               Value function loss: 65617.0022
                    Surrogate loss: 0.0102
             Mean action noise std: 0.94
                       Mean reward: 3554.61
               Mean episode length: 262.51
                 Mean success rate: 43.00
                  Mean reward/step: 13.72
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6807552
                    Iteration time: 2.51s
                        Total time: 2134.14s
                               ETA: 8141.1s

################################################################################
                     [1m Learning iteration 831/4000 [0m

                       Computation: 3237 steps/s (collection: 0.455s, learning 2.075s)
               Value function loss: 62757.0377
                    Surrogate loss: 0.0153
             Mean action noise std: 0.94
                       Mean reward: 3854.40
               Mean episode length: 266.64
                 Mean success rate: 45.00
                  Mean reward/step: 13.52
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 2.53s
                        Total time: 2136.67s
                               ETA: 8138.3s

################################################################################
                     [1m Learning iteration 832/4000 [0m

                       Computation: 3145 steps/s (collection: 0.491s, learning 2.113s)
               Value function loss: 46483.8512
                    Surrogate loss: 0.0200
             Mean action noise std: 0.93
                       Mean reward: 3716.26
               Mean episode length: 257.75
                 Mean success rate: 43.00
                  Mean reward/step: 13.51
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6823936
                    Iteration time: 2.60s
                        Total time: 2139.27s
                               ETA: 8135.9s

################################################################################
                     [1m Learning iteration 833/4000 [0m

                       Computation: 3221 steps/s (collection: 0.501s, learning 2.042s)
               Value function loss: 61826.4186
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 3805.33
               Mean episode length: 260.26
                 Mean success rate: 43.50
                  Mean reward/step: 13.21
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 2.54s
                        Total time: 2141.81s
                               ETA: 8133.2s

################################################################################
                     [1m Learning iteration 834/4000 [0m

                       Computation: 3237 steps/s (collection: 0.469s, learning 2.062s)
               Value function loss: 54216.5943
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 3661.53
               Mean episode length: 258.52
                 Mean success rate: 43.00
                  Mean reward/step: 13.37
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6840320
                    Iteration time: 2.53s
                        Total time: 2144.34s
                               ETA: 8130.5s

################################################################################
                     [1m Learning iteration 835/4000 [0m

                       Computation: 3195 steps/s (collection: 0.481s, learning 2.083s)
               Value function loss: 39326.3057
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 3531.14
               Mean episode length: 258.82
                 Mean success rate: 41.50
                  Mean reward/step: 13.22
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 2.56s
                        Total time: 2146.91s
                               ETA: 8127.9s

################################################################################
                     [1m Learning iteration 836/4000 [0m

                       Computation: 3165 steps/s (collection: 0.496s, learning 2.092s)
               Value function loss: 67435.8952
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 3510.56
               Mean episode length: 262.11
                 Mean success rate: 40.50
                  Mean reward/step: 14.00
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6856704
                    Iteration time: 2.59s
                        Total time: 2149.50s
                               ETA: 8125.5s

################################################################################
                     [1m Learning iteration 837/4000 [0m

                       Computation: 3220 steps/s (collection: 0.519s, learning 2.025s)
               Value function loss: 68074.6349
                    Surrogate loss: 0.0132
             Mean action noise std: 0.94
                       Mean reward: 3499.29
               Mean episode length: 268.66
                 Mean success rate: 40.50
                  Mean reward/step: 14.09
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 2.54s
                        Total time: 2152.04s
                               ETA: 8122.8s

################################################################################
                     [1m Learning iteration 838/4000 [0m

                       Computation: 3260 steps/s (collection: 0.468s, learning 2.045s)
               Value function loss: 39981.2444
                    Surrogate loss: 0.0190
             Mean action noise std: 0.94
                       Mean reward: 3583.85
               Mean episode length: 272.16
                 Mean success rate: 42.00
                  Mean reward/step: 14.37
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 6873088
                    Iteration time: 2.51s
                        Total time: 2154.55s
                               ETA: 8120.0s

################################################################################
                     [1m Learning iteration 839/4000 [0m

                       Computation: 3215 steps/s (collection: 0.448s, learning 2.100s)
               Value function loss: 41113.5004
                    Surrogate loss: 0.0182
             Mean action noise std: 0.94
                       Mean reward: 3227.90
               Mean episode length: 258.16
                 Mean success rate: 39.00
                  Mean reward/step: 14.97
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 2.55s
                        Total time: 2157.10s
                               ETA: 8117.4s

################################################################################
                     [1m Learning iteration 840/4000 [0m

                       Computation: 3184 steps/s (collection: 0.480s, learning 2.092s)
               Value function loss: 59559.3187
                    Surrogate loss: 0.0264
             Mean action noise std: 0.94
                       Mean reward: 3027.38
               Mean episode length: 241.51
                 Mean success rate: 34.50
                  Mean reward/step: 15.03
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 6889472
                    Iteration time: 2.57s
                        Total time: 2159.67s
                               ETA: 8114.8s

################################################################################
                     [1m Learning iteration 841/4000 [0m

                       Computation: 3237 steps/s (collection: 0.493s, learning 2.038s)
               Value function loss: 52625.0770
                    Surrogate loss: 0.0172
             Mean action noise std: 0.94
                       Mean reward: 3046.80
               Mean episode length: 239.87
                 Mean success rate: 34.00
                  Mean reward/step: 15.24
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 2.53s
                        Total time: 2162.20s
                               ETA: 8112.1s

################################################################################
                     [1m Learning iteration 842/4000 [0m

                       Computation: 3169 steps/s (collection: 0.508s, learning 2.077s)
               Value function loss: 65621.4803
                    Surrogate loss: 0.0129
             Mean action noise std: 0.94
                       Mean reward: 3549.82
               Mean episode length: 251.53
                 Mean success rate: 37.50
                  Mean reward/step: 14.72
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6905856
                    Iteration time: 2.58s
                        Total time: 2164.79s
                               ETA: 8109.6s

################################################################################
                     [1m Learning iteration 843/4000 [0m

                       Computation: 3197 steps/s (collection: 0.494s, learning 2.068s)
               Value function loss: 52719.2796
                    Surrogate loss: 0.0150
             Mean action noise std: 0.94
                       Mean reward: 3463.45
               Mean episode length: 250.50
                 Mean success rate: 37.00
                  Mean reward/step: 14.24
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 2.56s
                        Total time: 2167.35s
                               ETA: 8107.0s

################################################################################
                     [1m Learning iteration 844/4000 [0m

                       Computation: 3153 steps/s (collection: 0.491s, learning 2.106s)
               Value function loss: 51826.1426
                    Surrogate loss: 0.0176
             Mean action noise std: 0.94
                       Mean reward: 3403.57
               Mean episode length: 243.26
                 Mean success rate: 36.50
                  Mean reward/step: 14.89
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6922240
                    Iteration time: 2.60s
                        Total time: 2169.95s
                               ETA: 8104.6s

################################################################################
                     [1m Learning iteration 845/4000 [0m

                       Computation: 3227 steps/s (collection: 0.465s, learning 2.073s)
               Value function loss: 60244.3345
                    Surrogate loss: 0.0174
             Mean action noise std: 0.94
                       Mean reward: 3610.33
               Mean episode length: 252.50
                 Mean success rate: 37.50
                  Mean reward/step: 15.06
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 2.54s
                        Total time: 2172.49s
                               ETA: 8101.9s

################################################################################
                     [1m Learning iteration 846/4000 [0m

                       Computation: 3217 steps/s (collection: 0.486s, learning 2.060s)
               Value function loss: 65348.9153
                    Surrogate loss: 0.0169
             Mean action noise std: 0.94
                       Mean reward: 4034.96
               Mean episode length: 272.00
                 Mean success rate: 41.50
                  Mean reward/step: 15.24
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6938624
                    Iteration time: 2.55s
                        Total time: 2175.03s
                               ETA: 8099.2s

################################################################################
                     [1m Learning iteration 847/4000 [0m

                       Computation: 3189 steps/s (collection: 0.495s, learning 2.073s)
               Value function loss: 71398.9233
                    Surrogate loss: 0.0171
             Mean action noise std: 0.94
                       Mean reward: 4361.18
               Mean episode length: 283.11
                 Mean success rate: 45.00
                  Mean reward/step: 15.06
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 2.57s
                        Total time: 2177.60s
                               ETA: 8096.7s

################################################################################
                     [1m Learning iteration 848/4000 [0m

                       Computation: 3230 steps/s (collection: 0.460s, learning 2.076s)
               Value function loss: 64850.3250
                    Surrogate loss: 0.0156
             Mean action noise std: 0.94
                       Mean reward: 4389.66
               Mean episode length: 288.84
                 Mean success rate: 47.00
                  Mean reward/step: 15.00
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6955008
                    Iteration time: 2.54s
                        Total time: 2180.14s
                               ETA: 8094.0s

################################################################################
                     [1m Learning iteration 849/4000 [0m

                       Computation: 3171 steps/s (collection: 0.472s, learning 2.111s)
               Value function loss: 63905.5221
                    Surrogate loss: 0.0159
             Mean action noise std: 0.94
                       Mean reward: 4382.97
               Mean episode length: 287.65
                 Mean success rate: 47.00
                  Mean reward/step: 14.56
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 2.58s
                        Total time: 2182.72s
                               ETA: 8091.5s

################################################################################
                     [1m Learning iteration 850/4000 [0m

                       Computation: 3155 steps/s (collection: 0.515s, learning 2.081s)
               Value function loss: 65994.3567
                    Surrogate loss: 0.0150
             Mean action noise std: 0.94
                       Mean reward: 4379.67
               Mean episode length: 286.21
                 Mean success rate: 48.00
                  Mean reward/step: 14.19
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6971392
                    Iteration time: 2.60s
                        Total time: 2185.32s
                               ETA: 8089.0s

################################################################################
                     [1m Learning iteration 851/4000 [0m

                       Computation: 3207 steps/s (collection: 0.472s, learning 2.081s)
               Value function loss: 57564.6050
                    Surrogate loss: 0.0195
             Mean action noise std: 0.94
                       Mean reward: 4268.44
               Mean episode length: 283.53
                 Mean success rate: 47.00
                  Mean reward/step: 14.59
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 2.55s
                        Total time: 2187.87s
                               ETA: 8086.4s

################################################################################
                     [1m Learning iteration 852/4000 [0m

                       Computation: 3255 steps/s (collection: 0.475s, learning 2.041s)
               Value function loss: 77852.3110
                    Surrogate loss: 0.0201
             Mean action noise std: 0.94
                       Mean reward: 4259.03
               Mean episode length: 287.00
                 Mean success rate: 48.00
                  Mean reward/step: 14.97
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6987776
                    Iteration time: 2.52s
                        Total time: 2190.39s
                               ETA: 8083.6s

################################################################################
                     [1m Learning iteration 853/4000 [0m

                       Computation: 3201 steps/s (collection: 0.482s, learning 2.076s)
               Value function loss: 56840.1555
                    Surrogate loss: 0.0151
             Mean action noise std: 0.94
                       Mean reward: 4369.03
               Mean episode length: 295.93
                 Mean success rate: 50.50
                  Mean reward/step: 14.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 2.56s
                        Total time: 2192.94s
                               ETA: 8081.0s

################################################################################
                     [1m Learning iteration 854/4000 [0m

                       Computation: 3102 steps/s (collection: 0.531s, learning 2.109s)
               Value function loss: 26043.8652
                    Surrogate loss: 0.0176
             Mean action noise std: 0.94
                       Mean reward: 3945.18
               Mean episode length: 279.89
                 Mean success rate: 47.50
                  Mean reward/step: 14.43
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7004160
                    Iteration time: 2.64s
                        Total time: 2195.59s
                               ETA: 8078.7s

################################################################################
                     [1m Learning iteration 855/4000 [0m

                       Computation: 3183 steps/s (collection: 0.492s, learning 2.081s)
               Value function loss: 66311.5940
                    Surrogate loss: 0.0138
             Mean action noise std: 0.94
                       Mean reward: 3949.48
               Mean episode length: 273.01
                 Mean success rate: 45.50
                  Mean reward/step: 15.53
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 2.57s
                        Total time: 2198.16s
                               ETA: 8076.2s

################################################################################
                     [1m Learning iteration 856/4000 [0m

                       Computation: 3251 steps/s (collection: 0.461s, learning 2.058s)
               Value function loss: 65214.4844
                    Surrogate loss: 0.0132
             Mean action noise std: 0.94
                       Mean reward: 3695.69
               Mean episode length: 262.05
                 Mean success rate: 43.00
                  Mean reward/step: 14.77
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7020544
                    Iteration time: 2.52s
                        Total time: 2200.68s
                               ETA: 8073.4s

################################################################################
                     [1m Learning iteration 857/4000 [0m

                       Computation: 3203 steps/s (collection: 0.478s, learning 2.080s)
               Value function loss: 59344.5620
                    Surrogate loss: 0.0144
             Mean action noise std: 0.94
                       Mean reward: 3645.35
               Mean episode length: 254.81
                 Mean success rate: 41.50
                  Mean reward/step: 14.41
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 2.56s
                        Total time: 2203.24s
                               ETA: 8070.8s

################################################################################
                     [1m Learning iteration 858/4000 [0m

                       Computation: 3221 steps/s (collection: 0.475s, learning 2.067s)
               Value function loss: 60553.2405
                    Surrogate loss: 0.0126
             Mean action noise std: 0.93
                       Mean reward: 3461.86
               Mean episode length: 237.66
                 Mean success rate: 38.50
                  Mean reward/step: 14.29
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7036928
                    Iteration time: 2.54s
                        Total time: 2205.78s
                               ETA: 8068.2s

################################################################################
                     [1m Learning iteration 859/4000 [0m

                       Computation: 3241 steps/s (collection: 0.481s, learning 2.046s)
               Value function loss: 52671.3078
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 3400.10
               Mean episode length: 233.66
                 Mean success rate: 38.50
                  Mean reward/step: 14.84
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 2.53s
                        Total time: 2208.31s
                               ETA: 8065.4s

################################################################################
                     [1m Learning iteration 860/4000 [0m

                       Computation: 3202 steps/s (collection: 0.493s, learning 2.065s)
               Value function loss: 60061.4226
                    Surrogate loss: 0.0132
             Mean action noise std: 0.93
                       Mean reward: 3828.42
               Mean episode length: 249.47
                 Mean success rate: 41.50
                  Mean reward/step: 15.94
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7053312
                    Iteration time: 2.56s
                        Total time: 2210.86s
                               ETA: 8062.8s

################################################################################
                     [1m Learning iteration 861/4000 [0m

                       Computation: 3260 steps/s (collection: 0.476s, learning 2.037s)
               Value function loss: 51834.1968
                    Surrogate loss: 0.0168
             Mean action noise std: 0.94
                       Mean reward: 4022.98
               Mean episode length: 258.98
                 Mean success rate: 43.50
                  Mean reward/step: 15.43
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 2.51s
                        Total time: 2213.38s
                               ETA: 8060.1s

################################################################################
                     [1m Learning iteration 862/4000 [0m

                       Computation: 3192 steps/s (collection: 0.497s, learning 2.069s)
               Value function loss: 68980.3121
                    Surrogate loss: 0.0152
             Mean action noise std: 0.94
                       Mean reward: 4180.27
               Mean episode length: 268.72
                 Mean success rate: 45.50
                  Mean reward/step: 15.73
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7069696
                    Iteration time: 2.57s
                        Total time: 2215.94s
                               ETA: 8057.5s

################################################################################
                     [1m Learning iteration 863/4000 [0m

                       Computation: 3305 steps/s (collection: 0.446s, learning 2.032s)
               Value function loss: 47944.9660
                    Surrogate loss: 0.0145
             Mean action noise std: 0.94
                       Mean reward: 4179.70
               Mean episode length: 273.38
                 Mean success rate: 45.00
                  Mean reward/step: 16.20
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 2.48s
                        Total time: 2218.42s
                               ETA: 8054.6s

################################################################################
                     [1m Learning iteration 864/4000 [0m

                       Computation: 3217 steps/s (collection: 0.498s, learning 2.048s)
               Value function loss: 91562.9493
                    Surrogate loss: 0.0137
             Mean action noise std: 0.94
                       Mean reward: 4151.55
               Mean episode length: 274.52
                 Mean success rate: 46.50
                  Mean reward/step: 16.42
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7086080
                    Iteration time: 2.55s
                        Total time: 2220.97s
                               ETA: 8052.0s

################################################################################
                     [1m Learning iteration 865/4000 [0m

                       Computation: 3266 steps/s (collection: 0.456s, learning 2.052s)
               Value function loss: 75377.8926
                    Surrogate loss: 0.0119
             Mean action noise std: 0.94
                       Mean reward: 4477.11
               Mean episode length: 288.52
                 Mean success rate: 49.00
                  Mean reward/step: 14.90
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 2.51s
                        Total time: 2223.47s
                               ETA: 8049.2s

################################################################################
                     [1m Learning iteration 866/4000 [0m

                       Computation: 3231 steps/s (collection: 0.469s, learning 2.066s)
               Value function loss: 57389.7484
                    Surrogate loss: 0.0146
             Mean action noise std: 0.94
                       Mean reward: 4424.90
               Mean episode length: 282.73
                 Mean success rate: 47.00
                  Mean reward/step: 14.74
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7102464
                    Iteration time: 2.53s
                        Total time: 2226.01s
                               ETA: 8046.5s

################################################################################
                     [1m Learning iteration 867/4000 [0m

                       Computation: 3266 steps/s (collection: 0.440s, learning 2.068s)
               Value function loss: 49228.3403
                    Surrogate loss: 0.0155
             Mean action noise std: 0.94
                       Mean reward: 4460.93
               Mean episode length: 285.04
                 Mean success rate: 47.50
                  Mean reward/step: 14.94
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 2.51s
                        Total time: 2228.52s
                               ETA: 8043.7s

################################################################################
                     [1m Learning iteration 868/4000 [0m

                       Computation: 3131 steps/s (collection: 0.505s, learning 2.111s)
               Value function loss: 60565.6919
                    Surrogate loss: 0.0163
             Mean action noise std: 0.94
                       Mean reward: 4319.75
               Mean episode length: 280.30
                 Mean success rate: 47.00
                  Mean reward/step: 14.89
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7118848
                    Iteration time: 2.62s
                        Total time: 2231.13s
                               ETA: 8041.3s

################################################################################
                     [1m Learning iteration 869/4000 [0m

                       Computation: 3232 steps/s (collection: 0.477s, learning 2.057s)
               Value function loss: 44431.1474
                    Surrogate loss: 0.0198
             Mean action noise std: 0.94
                       Mean reward: 3899.79
               Mean episode length: 266.93
                 Mean success rate: 44.00
                  Mean reward/step: 15.35
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 2.53s
                        Total time: 2233.67s
                               ETA: 8038.6s

################################################################################
                     [1m Learning iteration 870/4000 [0m

                       Computation: 3141 steps/s (collection: 0.483s, learning 2.125s)
               Value function loss: 58741.0877
                    Surrogate loss: 0.0176
             Mean action noise std: 0.94
                       Mean reward: 3948.67
               Mean episode length: 263.77
                 Mean success rate: 44.50
                  Mean reward/step: 15.93
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7135232
                    Iteration time: 2.61s
                        Total time: 2236.28s
                               ETA: 8036.2s

################################################################################
                     [1m Learning iteration 871/4000 [0m

                       Computation: 3217 steps/s (collection: 0.496s, learning 2.050s)
               Value function loss: 94379.0117
                    Surrogate loss: 0.0122
             Mean action noise std: 0.94
                       Mean reward: 4223.54
               Mean episode length: 278.71
                 Mean success rate: 47.50
                  Mean reward/step: 15.36
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 2.55s
                        Total time: 2238.82s
                               ETA: 8033.6s

################################################################################
                     [1m Learning iteration 872/4000 [0m

                       Computation: 3217 steps/s (collection: 0.460s, learning 2.085s)
               Value function loss: 54975.0972
                    Surrogate loss: 0.0148
             Mean action noise std: 0.94
                       Mean reward: 3997.00
               Mean episode length: 267.83
                 Mean success rate: 45.50
                  Mean reward/step: 16.00
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7151616
                    Iteration time: 2.55s
                        Total time: 2241.37s
                               ETA: 8030.9s

################################################################################
                     [1m Learning iteration 873/4000 [0m

                       Computation: 3250 steps/s (collection: 0.500s, learning 2.021s)
               Value function loss: 76605.3912
                    Surrogate loss: 0.0157
             Mean action noise std: 0.94
                       Mean reward: 4071.71
               Mean episode length: 272.89
                 Mean success rate: 46.50
                  Mean reward/step: 16.26
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 2.52s
                        Total time: 2243.89s
                               ETA: 8028.2s

################################################################################
                     [1m Learning iteration 874/4000 [0m

                       Computation: 3261 steps/s (collection: 0.485s, learning 2.027s)
               Value function loss: 46235.5644
                    Surrogate loss: 0.0188
             Mean action noise std: 0.94
                       Mean reward: 3845.91
               Mean episode length: 262.17
                 Mean success rate: 42.50
                  Mean reward/step: 16.04
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7168000
                    Iteration time: 2.51s
                        Total time: 2246.40s
                               ETA: 8025.4s

################################################################################
                     [1m Learning iteration 875/4000 [0m

                       Computation: 3276 steps/s (collection: 0.469s, learning 2.031s)
               Value function loss: 84187.7168
                    Surrogate loss: 0.0152
             Mean action noise std: 0.93
                       Mean reward: 4138.17
               Mean episode length: 263.48
                 Mean success rate: 44.00
                  Mean reward/step: 16.37
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 2.50s
                        Total time: 2248.90s
                               ETA: 8022.6s

################################################################################
                     [1m Learning iteration 876/4000 [0m

                       Computation: 3304 steps/s (collection: 0.453s, learning 2.026s)
               Value function loss: 64640.4599
                    Surrogate loss: 0.0181
             Mean action noise std: 0.93
                       Mean reward: 4528.92
               Mean episode length: 280.66
                 Mean success rate: 47.00
                  Mean reward/step: 16.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7184384
                    Iteration time: 2.48s
                        Total time: 2251.38s
                               ETA: 8019.7s

################################################################################
                     [1m Learning iteration 877/4000 [0m

                       Computation: 3267 steps/s (collection: 0.454s, learning 2.053s)
               Value function loss: 59538.0445
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 4577.80
               Mean episode length: 288.01
                 Mean success rate: 47.50
                  Mean reward/step: 15.95
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 2.51s
                        Total time: 2253.89s
                               ETA: 8017.0s

################################################################################
                     [1m Learning iteration 878/4000 [0m

                       Computation: 3185 steps/s (collection: 0.509s, learning 2.063s)
               Value function loss: 52642.0838
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 4463.71
               Mean episode length: 283.81
                 Mean success rate: 45.00
                  Mean reward/step: 16.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7200768
                    Iteration time: 2.57s
                        Total time: 2256.46s
                               ETA: 8014.4s

################################################################################
                     [1m Learning iteration 879/4000 [0m

                       Computation: 3187 steps/s (collection: 0.491s, learning 2.079s)
               Value function loss: 66376.1881
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 4413.08
               Mean episode length: 277.70
                 Mean success rate: 45.00
                  Mean reward/step: 16.26
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 2.57s
                        Total time: 2259.03s
                               ETA: 8011.8s

################################################################################
                     [1m Learning iteration 880/4000 [0m

                       Computation: 3269 steps/s (collection: 0.441s, learning 2.064s)
               Value function loss: 81814.9043
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 4616.72
               Mean episode length: 288.81
                 Mean success rate: 47.00
                  Mean reward/step: 15.23
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7217152
                    Iteration time: 2.51s
                        Total time: 2261.53s
                               ETA: 8009.1s

################################################################################
                     [1m Learning iteration 881/4000 [0m

                       Computation: 3269 steps/s (collection: 0.471s, learning 2.035s)
               Value function loss: 70372.3678
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 4511.71
               Mean episode length: 284.27
                 Mean success rate: 46.50
                  Mean reward/step: 15.02
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 2.51s
                        Total time: 2264.04s
                               ETA: 8006.3s

################################################################################
                     [1m Learning iteration 882/4000 [0m

                       Computation: 3253 steps/s (collection: 0.458s, learning 2.060s)
               Value function loss: 71684.4316
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 4634.59
               Mean episode length: 288.89
                 Mean success rate: 48.50
                  Mean reward/step: 15.06
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7233536
                    Iteration time: 2.52s
                        Total time: 2266.56s
                               ETA: 8003.5s

################################################################################
                     [1m Learning iteration 883/4000 [0m

                       Computation: 3217 steps/s (collection: 0.457s, learning 2.089s)
               Value function loss: 70279.1652
                    Surrogate loss: 0.0162
             Mean action noise std: 0.93
                       Mean reward: 4757.57
               Mean episode length: 296.12
                 Mean success rate: 49.50
                  Mean reward/step: 14.90
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 2.55s
                        Total time: 2269.10s
                               ETA: 8000.9s

################################################################################
                     [1m Learning iteration 884/4000 [0m

                       Computation: 3228 steps/s (collection: 0.446s, learning 2.091s)
               Value function loss: 54901.7798
                    Surrogate loss: 0.0175
             Mean action noise std: 0.93
                       Mean reward: 4660.63
               Mean episode length: 286.05
                 Mean success rate: 48.50
                  Mean reward/step: 14.56
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7249920
                    Iteration time: 2.54s
                        Total time: 2271.64s
                               ETA: 7998.2s

################################################################################
                     [1m Learning iteration 885/4000 [0m

                       Computation: 3216 steps/s (collection: 0.457s, learning 2.090s)
               Value function loss: 42938.6353
                    Surrogate loss: 0.0201
             Mean action noise std: 0.93
                       Mean reward: 4356.65
               Mean episode length: 276.71
                 Mean success rate: 45.00
                  Mean reward/step: 14.81
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 2.55s
                        Total time: 2274.19s
                               ETA: 7995.6s

################################################################################
                     [1m Learning iteration 886/4000 [0m

                       Computation: 3164 steps/s (collection: 0.524s, learning 2.064s)
               Value function loss: 61457.5897
                    Surrogate loss: 0.0188
             Mean action noise std: 0.93
                       Mean reward: 3851.38
               Mean episode length: 255.64
                 Mean success rate: 41.00
                  Mean reward/step: 14.39
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7266304
                    Iteration time: 2.59s
                        Total time: 2276.78s
                               ETA: 7993.1s

################################################################################
                     [1m Learning iteration 887/4000 [0m

                       Computation: 3144 steps/s (collection: 0.484s, learning 2.121s)
               Value function loss: 66979.4396
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 3995.20
               Mean episode length: 262.60
                 Mean success rate: 43.50
                  Mean reward/step: 13.81
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 2.61s
                        Total time: 2279.38s
                               ETA: 7990.7s

################################################################################
                     [1m Learning iteration 888/4000 [0m

                       Computation: 3151 steps/s (collection: 0.485s, learning 2.114s)
               Value function loss: 59186.1459
                    Surrogate loss: 0.0161
             Mean action noise std: 0.93
                       Mean reward: 3476.90
               Mean episode length: 239.19
                 Mean success rate: 37.50
                  Mean reward/step: 13.78
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 7282688
                    Iteration time: 2.60s
                        Total time: 2281.98s
                               ETA: 7988.2s

################################################################################
                     [1m Learning iteration 889/4000 [0m

                       Computation: 3221 steps/s (collection: 0.439s, learning 2.103s)
               Value function loss: 65935.2688
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 3415.84
               Mean episode length: 236.91
                 Mean success rate: 38.00
                  Mean reward/step: 13.96
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 2.54s
                        Total time: 2284.52s
                               ETA: 7985.6s

################################################################################
                     [1m Learning iteration 890/4000 [0m

                       Computation: 3172 steps/s (collection: 0.494s, learning 2.089s)
               Value function loss: 54178.3358
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 3636.11
               Mean episode length: 245.81
                 Mean success rate: 39.50
                  Mean reward/step: 14.18
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7299072
                    Iteration time: 2.58s
                        Total time: 2287.11s
                               ETA: 7983.1s

################################################################################
                     [1m Learning iteration 891/4000 [0m

                       Computation: 3202 steps/s (collection: 0.477s, learning 2.080s)
               Value function loss: 61624.6734
                    Surrogate loss: 0.0193
             Mean action noise std: 0.93
                       Mean reward: 3308.46
               Mean episode length: 231.87
                 Mean success rate: 36.00
                  Mean reward/step: 14.24
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 2.56s
                        Total time: 2289.66s
                               ETA: 7980.5s

################################################################################
                     [1m Learning iteration 892/4000 [0m

                       Computation: 3170 steps/s (collection: 0.474s, learning 2.110s)
               Value function loss: 57240.2266
                    Surrogate loss: 0.0140
             Mean action noise std: 0.93
                       Mean reward: 2988.48
               Mean episode length: 219.26
                 Mean success rate: 32.00
                  Mean reward/step: 13.96
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 7315456
                    Iteration time: 2.58s
                        Total time: 2292.25s
                               ETA: 7977.9s

################################################################################
                     [1m Learning iteration 893/4000 [0m

                       Computation: 3255 steps/s (collection: 0.445s, learning 2.071s)
               Value function loss: 31593.0991
                    Surrogate loss: 0.0183
             Mean action noise std: 0.93
                       Mean reward: 3109.67
               Mean episode length: 225.62
                 Mean success rate: 33.50
                  Mean reward/step: 14.09
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 2.52s
                        Total time: 2294.76s
                               ETA: 7975.2s

################################################################################
                     [1m Learning iteration 894/4000 [0m

                       Computation: 3174 steps/s (collection: 0.490s, learning 2.091s)
               Value function loss: 69514.6054
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 3323.76
               Mean episode length: 235.76
                 Mean success rate: 35.50
                  Mean reward/step: 14.39
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7331840
                    Iteration time: 2.58s
                        Total time: 2297.34s
                               ETA: 7972.7s

################################################################################
                     [1m Learning iteration 895/4000 [0m

                       Computation: 3151 steps/s (collection: 0.503s, learning 2.096s)
               Value function loss: 58250.3746
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 3236.14
               Mean episode length: 227.73
                 Mean success rate: 34.00
                  Mean reward/step: 14.98
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 2.60s
                        Total time: 2299.94s
                               ETA: 7970.2s

################################################################################
                     [1m Learning iteration 896/4000 [0m

                       Computation: 3093 steps/s (collection: 0.514s, learning 2.134s)
               Value function loss: 54533.9354
                    Surrogate loss: 0.0172
             Mean action noise std: 0.93
                       Mean reward: 3157.92
               Mean episode length: 222.34
                 Mean success rate: 33.50
                  Mean reward/step: 15.15
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7348224
                    Iteration time: 2.65s
                        Total time: 2302.59s
                               ETA: 7967.9s

################################################################################
                     [1m Learning iteration 897/4000 [0m

                       Computation: 3049 steps/s (collection: 0.555s, learning 2.132s)
               Value function loss: 61887.4886
                    Surrogate loss: 0.0162
             Mean action noise std: 0.93
                       Mean reward: 3297.87
               Mean episode length: 234.31
                 Mean success rate: 34.50
                  Mean reward/step: 15.31
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 2.69s
                        Total time: 2305.28s
                               ETA: 7965.8s

################################################################################
                     [1m Learning iteration 898/4000 [0m

                       Computation: 3126 steps/s (collection: 0.495s, learning 2.125s)
               Value function loss: 52555.9300
                    Surrogate loss: 0.0170
             Mean action noise std: 0.93
                       Mean reward: 3542.86
               Mean episode length: 240.91
                 Mean success rate: 36.50
                  Mean reward/step: 15.38
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 7364608
                    Iteration time: 2.62s
                        Total time: 2307.90s
                               ETA: 7963.4s

################################################################################
                     [1m Learning iteration 899/4000 [0m

                       Computation: 3181 steps/s (collection: 0.462s, learning 2.112s)
               Value function loss: 39276.0129
                    Surrogate loss: 0.0204
             Mean action noise std: 0.93
                       Mean reward: 3380.59
               Mean episode length: 231.79
                 Mean success rate: 34.50
                  Mean reward/step: 14.88
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 2.57s
                        Total time: 2310.47s
                               ETA: 7960.9s

################################################################################
                     [1m Learning iteration 900/4000 [0m

                       Computation: 3201 steps/s (collection: 0.461s, learning 2.098s)
               Value function loss: 53373.7956
                    Surrogate loss: 0.0191
             Mean action noise std: 0.93
                       Mean reward: 3125.34
               Mean episode length: 222.06
                 Mean success rate: 32.50
                  Mean reward/step: 15.08
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7380992
                    Iteration time: 2.56s
                        Total time: 2313.03s
                               ETA: 7958.3s

################################################################################
                     [1m Learning iteration 901/4000 [0m

                       Computation: 3105 steps/s (collection: 0.519s, learning 2.119s)
               Value function loss: 60220.1821
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 3388.74
               Mean episode length: 232.41
                 Mean success rate: 35.00
                  Mean reward/step: 15.50
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 2.64s
                        Total time: 2315.67s
                               ETA: 7955.9s

################################################################################
                     [1m Learning iteration 902/4000 [0m

                       Computation: 3136 steps/s (collection: 0.486s, learning 2.126s)
               Value function loss: 67462.0634
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 3385.96
               Mean episode length: 231.68
                 Mean success rate: 35.00
                  Mean reward/step: 14.94
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 7397376
                    Iteration time: 2.61s
                        Total time: 2318.28s
                               ETA: 7953.5s

################################################################################
                     [1m Learning iteration 903/4000 [0m

                       Computation: 3243 steps/s (collection: 0.457s, learning 2.069s)
               Value function loss: 58453.2020
                    Surrogate loss: 0.0131
             Mean action noise std: 0.93
                       Mean reward: 3503.16
               Mean episode length: 233.66
                 Mean success rate: 34.50
                  Mean reward/step: 14.30
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 2.53s
                        Total time: 2320.81s
                               ETA: 7950.8s

################################################################################
                     [1m Learning iteration 904/4000 [0m

                       Computation: 3179 steps/s (collection: 0.473s, learning 2.104s)
               Value function loss: 67258.8112
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 3566.03
               Mean episode length: 240.67
                 Mean success rate: 36.50
                  Mean reward/step: 13.61
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7413760
                    Iteration time: 2.58s
                        Total time: 2323.38s
                               ETA: 7948.3s

################################################################################
                     [1m Learning iteration 905/4000 [0m

                       Computation: 3031 steps/s (collection: 0.568s, learning 2.134s)
               Value function loss: 64470.1182
                    Surrogate loss: 0.0143
             Mean action noise std: 0.94
                       Mean reward: 3635.38
               Mean episode length: 242.57
                 Mean success rate: 36.00
                  Mean reward/step: 13.91
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 2.70s
                        Total time: 2326.09s
                               ETA: 7946.2s

################################################################################
                     [1m Learning iteration 906/4000 [0m

                       Computation: 3176 steps/s (collection: 0.471s, learning 2.109s)
               Value function loss: 55553.7453
                    Surrogate loss: 0.0157
             Mean action noise std: 0.94
                       Mean reward: 3536.71
               Mean episode length: 236.47
                 Mean success rate: 35.00
                  Mean reward/step: 13.42
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7430144
                    Iteration time: 2.58s
                        Total time: 2328.66s
                               ETA: 7943.6s

################################################################################
                     [1m Learning iteration 907/4000 [0m

                       Computation: 3233 steps/s (collection: 0.469s, learning 2.064s)
               Value function loss: 58744.5167
                    Surrogate loss: 0.0165
             Mean action noise std: 0.93
                       Mean reward: 3360.89
               Mean episode length: 230.53
                 Mean success rate: 34.00
                  Mean reward/step: 13.18
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 2.53s
                        Total time: 2331.20s
                               ETA: 7941.0s

################################################################################
                     [1m Learning iteration 908/4000 [0m

                       Computation: 3158 steps/s (collection: 0.488s, learning 2.106s)
               Value function loss: 58401.8671
                    Surrogate loss: 0.0166
             Mean action noise std: 0.93
                       Mean reward: 2975.63
               Mean episode length: 209.54
                 Mean success rate: 30.50
                  Mean reward/step: 12.50
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 7446528
                    Iteration time: 2.59s
                        Total time: 2333.79s
                               ETA: 7938.5s

################################################################################
                     [1m Learning iteration 909/4000 [0m

                       Computation: 3227 steps/s (collection: 0.479s, learning 2.059s)
               Value function loss: 78976.1629
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 2787.88
               Mean episode length: 196.02
                 Mean success rate: 29.50
                  Mean reward/step: 12.94
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 2.54s
                        Total time: 2336.33s
                               ETA: 7935.8s

################################################################################
                     [1m Learning iteration 910/4000 [0m

                       Computation: 3299 steps/s (collection: 0.445s, learning 2.038s)
               Value function loss: 66347.3221
                    Surrogate loss: 0.0165
             Mean action noise std: 0.93
                       Mean reward: 2835.64
               Mean episode length: 195.51
                 Mean success rate: 28.50
                  Mean reward/step: 13.14
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7462912
                    Iteration time: 2.48s
                        Total time: 2338.81s
                               ETA: 7933.0s

################################################################################
                     [1m Learning iteration 911/4000 [0m

                       Computation: 3214 steps/s (collection: 0.477s, learning 2.072s)
               Value function loss: 58006.9534
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 2643.66
               Mean episode length: 191.03
                 Mean success rate: 26.50
                  Mean reward/step: 13.68
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 2.55s
                        Total time: 2341.36s
                               ETA: 7930.3s

################################################################################
                     [1m Learning iteration 912/4000 [0m

                       Computation: 3247 steps/s (collection: 0.476s, learning 2.046s)
               Value function loss: 63842.2569
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 3076.62
               Mean episode length: 210.35
                 Mean success rate: 31.00
                  Mean reward/step: 13.95
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7479296
                    Iteration time: 2.52s
                        Total time: 2343.88s
                               ETA: 7927.6s

################################################################################
                     [1m Learning iteration 913/4000 [0m

                       Computation: 3236 steps/s (collection: 0.476s, learning 2.055s)
               Value function loss: 47902.0149
                    Surrogate loss: 0.0173
             Mean action noise std: 0.93
                       Mean reward: 3147.21
               Mean episode length: 216.62
                 Mean success rate: 32.00
                  Mean reward/step: 13.19
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 2.53s
                        Total time: 2346.42s
                               ETA: 7924.9s

################################################################################
                     [1m Learning iteration 914/4000 [0m

                       Computation: 3220 steps/s (collection: 0.502s, learning 2.042s)
               Value function loss: 53035.5426
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 3049.97
               Mean episode length: 216.55
                 Mean success rate: 32.00
                  Mean reward/step: 14.14
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7495680
                    Iteration time: 2.54s
                        Total time: 2348.96s
                               ETA: 7922.3s

################################################################################
                     [1m Learning iteration 915/4000 [0m

                       Computation: 3240 steps/s (collection: 0.483s, learning 2.045s)
               Value function loss: 47307.2730
                    Surrogate loss: 0.0174
             Mean action noise std: 0.93
                       Mean reward: 3029.88
               Mean episode length: 224.07
                 Mean success rate: 31.50
                  Mean reward/step: 14.73
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 2.53s
                        Total time: 2351.49s
                               ETA: 7919.6s

################################################################################
                     [1m Learning iteration 916/4000 [0m

                       Computation: 3274 steps/s (collection: 0.460s, learning 2.042s)
               Value function loss: 65037.3892
                    Surrogate loss: 0.0131
             Mean action noise std: 0.93
                       Mean reward: 2962.03
               Mean episode length: 225.93
                 Mean success rate: 32.50
                  Mean reward/step: 14.59
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7512064
                    Iteration time: 2.50s
                        Total time: 2353.99s
                               ETA: 7916.8s

################################################################################
                     [1m Learning iteration 917/4000 [0m

                       Computation: 3273 steps/s (collection: 0.477s, learning 2.025s)
               Value function loss: 48774.4103
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 2648.95
               Mean episode length: 206.91
                 Mean success rate: 30.50
                  Mean reward/step: 15.08
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 2.50s
                        Total time: 2356.49s
                               ETA: 7914.0s

################################################################################
                     [1m Learning iteration 918/4000 [0m

                       Computation: 3257 steps/s (collection: 0.488s, learning 2.027s)
               Value function loss: 79535.5525
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 2586.56
               Mean episode length: 203.31
                 Mean success rate: 29.50
                  Mean reward/step: 15.40
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7528448
                    Iteration time: 2.51s
                        Total time: 2359.01s
                               ETA: 7911.3s

################################################################################
                     [1m Learning iteration 919/4000 [0m

                       Computation: 3220 steps/s (collection: 0.459s, learning 2.085s)
               Value function loss: 65315.5024
                    Surrogate loss: 0.0125
             Mean action noise std: 0.93
                       Mean reward: 2557.24
               Mean episode length: 200.12
                 Mean success rate: 29.50
                  Mean reward/step: 15.19
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 2.54s
                        Total time: 2361.55s
                               ETA: 7908.6s

################################################################################
                     [1m Learning iteration 920/4000 [0m

                       Computation: 3197 steps/s (collection: 0.487s, learning 2.075s)
               Value function loss: 64545.8885
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 2689.31
               Mean episode length: 199.65
                 Mean success rate: 31.50
                  Mean reward/step: 15.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7544832
                    Iteration time: 2.56s
                        Total time: 2364.11s
                               ETA: 7906.0s

################################################################################
                     [1m Learning iteration 921/4000 [0m

                       Computation: 3174 steps/s (collection: 0.467s, learning 2.114s)
               Value function loss: 81778.4531
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 3183.78
               Mean episode length: 220.00
                 Mean success rate: 36.50
                  Mean reward/step: 15.29
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 2.58s
                        Total time: 2366.69s
                               ETA: 7903.5s

################################################################################
                     [1m Learning iteration 922/4000 [0m

                       Computation: 3163 steps/s (collection: 0.485s, learning 2.104s)
               Value function loss: 50635.2857
                    Surrogate loss: 0.0194
             Mean action noise std: 0.93
                       Mean reward: 3274.93
               Mean episode length: 224.65
                 Mean success rate: 37.50
                  Mean reward/step: 14.91
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7561216
                    Iteration time: 2.59s
                        Total time: 2369.28s
                               ETA: 7901.0s

################################################################################
                     [1m Learning iteration 923/4000 [0m

                       Computation: 3141 steps/s (collection: 0.507s, learning 2.101s)
               Value function loss: 68062.9212
                    Surrogate loss: 0.0161
             Mean action noise std: 0.93
                       Mean reward: 3792.59
               Mean episode length: 243.84
                 Mean success rate: 42.00
                  Mean reward/step: 14.79
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 2.61s
                        Total time: 2371.89s
                               ETA: 7898.6s

################################################################################
                     [1m Learning iteration 924/4000 [0m

                       Computation: 3160 steps/s (collection: 0.499s, learning 2.093s)
               Value function loss: 58418.8916
                    Surrogate loss: 0.0144
             Mean action noise std: 0.93
                       Mean reward: 4093.00
               Mean episode length: 253.91
                 Mean success rate: 45.00
                  Mean reward/step: 14.88
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7577600
                    Iteration time: 2.59s
                        Total time: 2374.48s
                               ETA: 7896.1s

################################################################################
                     [1m Learning iteration 925/4000 [0m

                       Computation: 3178 steps/s (collection: 0.493s, learning 2.084s)
               Value function loss: 72952.0492
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 4282.75
               Mean episode length: 260.50
                 Mean success rate: 46.50
                  Mean reward/step: 15.21
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 2.58s
                        Total time: 2377.06s
                               ETA: 7893.6s

################################################################################
                     [1m Learning iteration 926/4000 [0m

                       Computation: 3112 steps/s (collection: 0.500s, learning 2.132s)
               Value function loss: 59356.2547
                    Surrogate loss: 0.0169
             Mean action noise std: 0.93
                       Mean reward: 4146.06
               Mean episode length: 265.21
                 Mean success rate: 44.00
                  Mean reward/step: 14.54
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7593984
                    Iteration time: 2.63s
                        Total time: 2379.69s
                               ETA: 7891.2s

################################################################################
                     [1m Learning iteration 927/4000 [0m

                       Computation: 3045 steps/s (collection: 0.576s, learning 2.113s)
               Value function loss: 68858.0840
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 4259.10
               Mean episode length: 270.98
                 Mean success rate: 47.50
                  Mean reward/step: 14.36
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 2.69s
                        Total time: 2382.38s
                               ETA: 7889.1s

################################################################################
                     [1m Learning iteration 928/4000 [0m

                       Computation: 3134 steps/s (collection: 0.500s, learning 2.114s)
               Value function loss: 70051.2227
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 4190.98
               Mean episode length: 271.94
                 Mean success rate: 46.00
                  Mean reward/step: 14.73
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 7610368
                    Iteration time: 2.61s
                        Total time: 2384.99s
                               ETA: 7886.7s

################################################################################
                     [1m Learning iteration 929/4000 [0m

                       Computation: 3162 steps/s (collection: 0.480s, learning 2.111s)
               Value function loss: 44772.3860
                    Surrogate loss: 0.0169
             Mean action noise std: 0.93
                       Mean reward: 3263.39
               Mean episode length: 238.43
                 Mean success rate: 38.00
                  Mean reward/step: 14.66
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 2.59s
                        Total time: 2387.58s
                               ETA: 7884.2s

################################################################################
                     [1m Learning iteration 930/4000 [0m

                       Computation: 3019 steps/s (collection: 0.540s, learning 2.173s)
               Value function loss: 53307.7649
                    Surrogate loss: 0.0123
             Mean action noise std: 0.93
                       Mean reward: 3126.22
               Mean episode length: 236.09
                 Mean success rate: 37.50
                  Mean reward/step: 14.51
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7626752
                    Iteration time: 2.71s
                        Total time: 2390.30s
                               ETA: 7882.1s

################################################################################
                     [1m Learning iteration 931/4000 [0m

                       Computation: 3111 steps/s (collection: 0.464s, learning 2.169s)
               Value function loss: 71332.3272
                    Surrogate loss: 0.0131
             Mean action noise std: 0.93
                       Mean reward: 3023.60
               Mean episode length: 227.06
                 Mean success rate: 35.50
                  Mean reward/step: 14.47
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 2.63s
                        Total time: 2392.93s
                               ETA: 7879.7s

################################################################################
                     [1m Learning iteration 932/4000 [0m

                       Computation: 3138 steps/s (collection: 0.479s, learning 2.132s)
               Value function loss: 59468.2081
                    Surrogate loss: 0.0121
             Mean action noise std: 0.93
                       Mean reward: 2859.34
               Mean episode length: 208.39
                 Mean success rate: 32.50
                  Mean reward/step: 13.89
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7643136
                    Iteration time: 2.61s
                        Total time: 2395.54s
                               ETA: 7877.3s

################################################################################
                     [1m Learning iteration 933/4000 [0m

                       Computation: 3214 steps/s (collection: 0.485s, learning 2.063s)
               Value function loss: 49639.1893
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 2677.58
               Mean episode length: 196.71
                 Mean success rate: 30.50
                  Mean reward/step: 13.66
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 2.55s
                        Total time: 2398.09s
                               ETA: 7874.7s

################################################################################
                     [1m Learning iteration 934/4000 [0m

                       Computation: 3102 steps/s (collection: 0.512s, learning 2.128s)
               Value function loss: 62735.3856
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 3062.36
               Mean episode length: 216.29
                 Mean success rate: 34.50
                  Mean reward/step: 14.24
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7659520
                    Iteration time: 2.64s
                        Total time: 2400.73s
                               ETA: 7872.3s

################################################################################
                     [1m Learning iteration 935/4000 [0m

                       Computation: 3103 steps/s (collection: 0.521s, learning 2.119s)
               Value function loss: 65248.4328
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 3374.84
               Mean episode length: 228.15
                 Mean success rate: 37.50
                  Mean reward/step: 15.23
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 2.64s
                        Total time: 2403.37s
                               ETA: 7870.0s

################################################################################
                     [1m Learning iteration 936/4000 [0m

                       Computation: 3065 steps/s (collection: 0.514s, learning 2.158s)
               Value function loss: 60419.3194
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 3551.15
               Mean episode length: 228.69
                 Mean success rate: 38.50
                  Mean reward/step: 15.63
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7675904
                    Iteration time: 2.67s
                        Total time: 2406.04s
                               ETA: 7867.8s

################################################################################
                     [1m Learning iteration 937/4000 [0m

                       Computation: 3212 steps/s (collection: 0.456s, learning 2.094s)
               Value function loss: 73127.7003
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 3989.60
               Mean episode length: 254.44
                 Mean success rate: 44.00
                  Mean reward/step: 15.90
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 2.55s
                        Total time: 2408.59s
                               ETA: 7865.2s

################################################################################
                     [1m Learning iteration 938/4000 [0m

                       Computation: 3080 steps/s (collection: 0.499s, learning 2.160s)
               Value function loss: 54487.8986
                    Surrogate loss: 0.0182
             Mean action noise std: 0.93
                       Mean reward: 3978.66
               Mean episode length: 253.97
                 Mean success rate: 44.00
                  Mean reward/step: 16.37
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7692288
                    Iteration time: 2.66s
                        Total time: 2411.25s
                               ETA: 7862.9s

################################################################################
                     [1m Learning iteration 939/4000 [0m

                       Computation: 3100 steps/s (collection: 0.510s, learning 2.132s)
               Value function loss: 69158.5383
                    Surrogate loss: 0.0248
             Mean action noise std: 0.93
                       Mean reward: 4019.11
               Mean episode length: 258.83
                 Mean success rate: 44.50
                  Mean reward/step: 15.94
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 2.64s
                        Total time: 2413.89s
                               ETA: 7860.6s

################################################################################
                     [1m Learning iteration 940/4000 [0m

                       Computation: 3188 steps/s (collection: 0.486s, learning 2.084s)
               Value function loss: 72788.3655
                    Surrogate loss: 0.0289
             Mean action noise std: 0.93
                       Mean reward: 4516.18
               Mean episode length: 279.81
                 Mean success rate: 49.50
                  Mean reward/step: 15.15
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7708672
                    Iteration time: 2.57s
                        Total time: 2416.46s
                               ETA: 7858.0s

################################################################################
                     [1m Learning iteration 941/4000 [0m

                       Computation: 3183 steps/s (collection: 0.473s, learning 2.100s)
               Value function loss: 40774.3134
                    Surrogate loss: 0.0184
             Mean action noise std: 0.93
                       Mean reward: 4010.84
               Mean episode length: 259.78
                 Mean success rate: 43.50
                  Mean reward/step: 14.75
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 2.57s
                        Total time: 2419.03s
                               ETA: 7855.4s

################################################################################
                     [1m Learning iteration 942/4000 [0m

                       Computation: 3140 steps/s (collection: 0.509s, learning 2.099s)
               Value function loss: 55468.8806
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 3656.19
               Mean episode length: 252.99
                 Mean success rate: 42.50
                  Mean reward/step: 15.17
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7725056
                    Iteration time: 2.61s
                        Total time: 2421.64s
                               ETA: 7853.0s

################################################################################
                     [1m Learning iteration 943/4000 [0m

                       Computation: 3202 steps/s (collection: 0.476s, learning 2.082s)
               Value function loss: 52432.3625
                    Surrogate loss: 0.0118
             Mean action noise std: 0.93
                       Mean reward: 3806.93
               Mean episode length: 254.79
                 Mean success rate: 43.00
                  Mean reward/step: 15.80
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 2.56s
                        Total time: 2424.20s
                               ETA: 7850.4s

################################################################################
                     [1m Learning iteration 944/4000 [0m

                       Computation: 3185 steps/s (collection: 0.480s, learning 2.091s)
               Value function loss: 85532.6571
                    Surrogate loss: 0.0099
             Mean action noise std: 0.93
                       Mean reward: 3812.93
               Mean episode length: 256.51
                 Mean success rate: 43.00
                  Mean reward/step: 15.87
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7741440
                    Iteration time: 2.57s
                        Total time: 2426.77s
                               ETA: 7847.8s

################################################################################
                     [1m Learning iteration 945/4000 [0m

                       Computation: 3146 steps/s (collection: 0.508s, learning 2.096s)
               Value function loss: 71531.4568
                    Surrogate loss: 0.0121
             Mean action noise std: 0.93
                       Mean reward: 3978.28
               Mean episode length: 262.05
                 Mean success rate: 44.50
                  Mean reward/step: 15.02
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 2.60s
                        Total time: 2429.38s
                               ETA: 7845.4s

################################################################################
                     [1m Learning iteration 946/4000 [0m

                       Computation: 3198 steps/s (collection: 0.474s, learning 2.087s)
               Value function loss: 48813.3683
                    Surrogate loss: 0.0144
             Mean action noise std: 0.93
                       Mean reward: 3805.69
               Mean episode length: 259.93
                 Mean success rate: 42.00
                  Mean reward/step: 14.72
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7757824
                    Iteration time: 2.56s
                        Total time: 2431.94s
                               ETA: 7842.8s

################################################################################
                     [1m Learning iteration 947/4000 [0m

                       Computation: 3147 steps/s (collection: 0.491s, learning 2.112s)
               Value function loss: 74756.8800
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 3668.30
               Mean episode length: 251.14
                 Mean success rate: 40.00
                  Mean reward/step: 14.43
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 2.60s
                        Total time: 2434.54s
                               ETA: 7840.3s

################################################################################
                     [1m Learning iteration 948/4000 [0m

                       Computation: 3202 steps/s (collection: 0.476s, learning 2.082s)
               Value function loss: 71213.9463
                    Surrogate loss: 0.0127
             Mean action noise std: 0.93
                       Mean reward: 4042.90
               Mean episode length: 260.71
                 Mean success rate: 42.50
                  Mean reward/step: 13.85
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7774208
                    Iteration time: 2.56s
                        Total time: 2437.10s
                               ETA: 7837.7s

################################################################################
                     [1m Learning iteration 949/4000 [0m

                       Computation: 3223 steps/s (collection: 0.470s, learning 2.071s)
               Value function loss: 81407.4491
                    Surrogate loss: 0.0106
             Mean action noise std: 0.93
                       Mean reward: 4298.07
               Mean episode length: 274.45
                 Mean success rate: 43.50
                  Mean reward/step: 12.57
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 2.54s
                        Total time: 2439.64s
                               ETA: 7835.1s

################################################################################
                     [1m Learning iteration 950/4000 [0m

                       Computation: 3201 steps/s (collection: 0.489s, learning 2.069s)
               Value function loss: 47163.3981
                    Surrogate loss: 0.0162
             Mean action noise std: 0.93
                       Mean reward: 4114.34
               Mean episode length: 263.88
                 Mean success rate: 43.50
                  Mean reward/step: 11.78
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7790592
                    Iteration time: 2.56s
                        Total time: 2442.20s
                               ETA: 7832.5s

################################################################################
                     [1m Learning iteration 951/4000 [0m

                       Computation: 3098 steps/s (collection: 0.504s, learning 2.139s)
               Value function loss: 74307.8821
                    Surrogate loss: 0.0152
             Mean action noise std: 0.93
                       Mean reward: 4028.74
               Mean episode length: 269.93
                 Mean success rate: 43.00
                  Mean reward/step: 12.13
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 2.64s
                        Total time: 2444.84s
                               ETA: 7830.2s

################################################################################
                     [1m Learning iteration 952/4000 [0m

                       Computation: 3159 steps/s (collection: 0.500s, learning 2.093s)
               Value function loss: 53863.3354
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 4108.09
               Mean episode length: 271.19
                 Mean success rate: 44.50
                  Mean reward/step: 12.23
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7806976
                    Iteration time: 2.59s
                        Total time: 2447.43s
                               ETA: 7827.7s

################################################################################
                     [1m Learning iteration 953/4000 [0m

                       Computation: 3215 steps/s (collection: 0.492s, learning 2.056s)
               Value function loss: 59614.8389
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 3787.11
               Mean episode length: 264.89
                 Mean success rate: 41.00
                  Mean reward/step: 12.32
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 2.55s
                        Total time: 2449.98s
                               ETA: 7825.0s

################################################################################
                     [1m Learning iteration 954/4000 [0m

                       Computation: 3208 steps/s (collection: 0.473s, learning 2.080s)
               Value function loss: 57238.0161
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 3526.58
               Mean episode length: 250.90
                 Mean success rate: 40.00
                  Mean reward/step: 12.53
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7823360
                    Iteration time: 2.55s
                        Total time: 2452.54s
                               ETA: 7822.4s

################################################################################
                     [1m Learning iteration 955/4000 [0m

                       Computation: 3189 steps/s (collection: 0.454s, learning 2.114s)
               Value function loss: 43027.3428
                    Surrogate loss: 0.0196
             Mean action noise std: 0.93
                       Mean reward: 3251.00
               Mean episode length: 237.96
                 Mean success rate: 35.00
                  Mean reward/step: 12.84
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 2.57s
                        Total time: 2455.10s
                               ETA: 7819.9s

################################################################################
                     [1m Learning iteration 956/4000 [0m

                       Computation: 3178 steps/s (collection: 0.491s, learning 2.087s)
               Value function loss: 56007.6939
                    Surrogate loss: 0.0212
             Mean action noise std: 0.93
                       Mean reward: 3157.13
               Mean episode length: 237.54
                 Mean success rate: 34.00
                  Mean reward/step: 13.05
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7839744
                    Iteration time: 2.58s
                        Total time: 2457.68s
                               ETA: 7817.3s

################################################################################
                     [1m Learning iteration 957/4000 [0m

                       Computation: 3198 steps/s (collection: 0.487s, learning 2.074s)
               Value function loss: 31256.6205
                    Surrogate loss: 0.0222
             Mean action noise std: 0.93
                       Mean reward: 3024.18
               Mean episode length: 232.35
                 Mean success rate: 33.50
                  Mean reward/step: 13.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 2.56s
                        Total time: 2460.24s
                               ETA: 7814.7s

################################################################################
                     [1m Learning iteration 958/4000 [0m

                       Computation: 3197 steps/s (collection: 0.471s, learning 2.091s)
               Value function loss: 44472.1099
                    Surrogate loss: 0.0202
             Mean action noise std: 0.93
                       Mean reward: 2870.01
               Mean episode length: 216.59
                 Mean success rate: 32.50
                  Mean reward/step: 14.75
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7856128
                    Iteration time: 2.56s
                        Total time: 2462.80s
                               ETA: 7812.1s

################################################################################
                     [1m Learning iteration 959/4000 [0m

                       Computation: 3106 steps/s (collection: 0.513s, learning 2.123s)
               Value function loss: 52158.3344
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 2691.51
               Mean episode length: 210.46
                 Mean success rate: 32.00
                  Mean reward/step: 16.22
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 2.64s
                        Total time: 2465.44s
                               ETA: 7809.8s

################################################################################
                     [1m Learning iteration 960/4000 [0m

                       Computation: 3138 steps/s (collection: 0.536s, learning 2.074s)
               Value function loss: 72274.3962
                    Surrogate loss: 0.0135
             Mean action noise std: 0.93
                       Mean reward: 2928.97
               Mean episode length: 224.90
                 Mean success rate: 35.00
                  Mean reward/step: 16.17
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7872512
                    Iteration time: 2.61s
                        Total time: 2468.05s
                               ETA: 7807.4s

################################################################################
                     [1m Learning iteration 961/4000 [0m

                       Computation: 3161 steps/s (collection: 0.522s, learning 2.069s)
               Value function loss: 59798.2652
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 2859.42
               Mean episode length: 225.16
                 Mean success rate: 35.50
                  Mean reward/step: 16.12
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 2.59s
                        Total time: 2470.64s
                               ETA: 7804.9s

################################################################################
                     [1m Learning iteration 962/4000 [0m

                       Computation: 3243 steps/s (collection: 0.453s, learning 2.073s)
               Value function loss: 54235.4285
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 2986.77
               Mean episode length: 230.35
                 Mean success rate: 37.50
                  Mean reward/step: 16.51
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7888896
                    Iteration time: 2.53s
                        Total time: 2473.17s
                               ETA: 7802.2s

################################################################################
                     [1m Learning iteration 963/4000 [0m

                       Computation: 3109 steps/s (collection: 0.475s, learning 2.159s)
               Value function loss: 72018.2919
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 2984.29
               Mean episode length: 232.95
                 Mean success rate: 37.50
                  Mean reward/step: 16.96
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 2.63s
                        Total time: 2475.80s
                               ETA: 7799.8s

################################################################################
                     [1m Learning iteration 964/4000 [0m

                       Computation: 3136 steps/s (collection: 0.521s, learning 2.091s)
               Value function loss: 68351.8953
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 3277.47
               Mean episode length: 241.57
                 Mean success rate: 39.00
                  Mean reward/step: 16.85
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7905280
                    Iteration time: 2.61s
                        Total time: 2478.41s
                               ETA: 7797.4s

################################################################################
                     [1m Learning iteration 965/4000 [0m

                       Computation: 3202 steps/s (collection: 0.484s, learning 2.074s)
               Value function loss: 46163.4556
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 3100.63
               Mean episode length: 235.58
                 Mean success rate: 36.00
                  Mean reward/step: 16.31
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 2.56s
                        Total time: 2480.97s
                               ETA: 7794.8s

################################################################################
                     [1m Learning iteration 966/4000 [0m

                       Computation: 3172 steps/s (collection: 0.503s, learning 2.079s)
               Value function loss: 69569.7335
                    Surrogate loss: 0.0165
             Mean action noise std: 0.93
                       Mean reward: 2959.81
               Mean episode length: 223.38
                 Mean success rate: 33.50
                  Mean reward/step: 16.48
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7921664
                    Iteration time: 2.58s
                        Total time: 2483.55s
                               ETA: 7792.2s

################################################################################
                     [1m Learning iteration 967/4000 [0m

                       Computation: 3165 steps/s (collection: 0.518s, learning 2.070s)
               Value function loss: 81345.0177
                    Surrogate loss: 0.0133
             Mean action noise std: 0.93
                       Mean reward: 3122.88
               Mean episode length: 225.63
                 Mean success rate: 34.50
                  Mean reward/step: 15.66
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 2.59s
                        Total time: 2486.14s
                               ETA: 7789.7s

################################################################################
                     [1m Learning iteration 968/4000 [0m

                       Computation: 3148 steps/s (collection: 0.527s, learning 2.075s)
               Value function loss: 74676.7931
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 3270.22
               Mean episode length: 224.29
                 Mean success rate: 35.00
                  Mean reward/step: 15.18
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7938048
                    Iteration time: 2.60s
                        Total time: 2488.74s
                               ETA: 7787.3s

################################################################################
                     [1m Learning iteration 969/4000 [0m

                       Computation: 3188 steps/s (collection: 0.488s, learning 2.082s)
               Value function loss: 74642.9960
                    Surrogate loss: 0.0132
             Mean action noise std: 0.93
                       Mean reward: 3720.20
               Mean episode length: 238.74
                 Mean success rate: 39.00
                  Mean reward/step: 14.81
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 2.57s
                        Total time: 2491.31s
                               ETA: 7784.7s

################################################################################
                     [1m Learning iteration 970/4000 [0m

                       Computation: 3206 steps/s (collection: 0.472s, learning 2.083s)
               Value function loss: 74422.4937
                    Surrogate loss: 0.0122
             Mean action noise std: 0.93
                       Mean reward: 3894.00
               Mean episode length: 242.28
                 Mean success rate: 41.50
                  Mean reward/step: 14.67
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7954432
                    Iteration time: 2.55s
                        Total time: 2493.87s
                               ETA: 7782.1s

################################################################################
                     [1m Learning iteration 971/4000 [0m

                       Computation: 3186 steps/s (collection: 0.501s, learning 2.070s)
               Value function loss: 62730.4345
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 4181.10
               Mean episode length: 252.32
                 Mean success rate: 44.00
                  Mean reward/step: 14.30
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 2.57s
                        Total time: 2496.44s
                               ETA: 7779.5s

################################################################################
                     [1m Learning iteration 972/4000 [0m

                       Computation: 3207 steps/s (collection: 0.494s, learning 2.060s)
               Value function loss: 70418.6953
                    Surrogate loss: 0.0161
             Mean action noise std: 0.93
                       Mean reward: 4344.18
               Mean episode length: 253.66
                 Mean success rate: 44.50
                  Mean reward/step: 14.32
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7970816
                    Iteration time: 2.55s
                        Total time: 2498.99s
                               ETA: 7776.9s

################################################################################
                     [1m Learning iteration 973/4000 [0m

                       Computation: 3153 steps/s (collection: 0.525s, learning 2.073s)
               Value function loss: 39077.5056
                    Surrogate loss: 0.0176
             Mean action noise std: 0.93
                       Mean reward: 3969.30
               Mean episode length: 241.12
                 Mean success rate: 40.00
                  Mean reward/step: 14.55
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 2.60s
                        Total time: 2501.59s
                               ETA: 7774.5s

################################################################################
                     [1m Learning iteration 974/4000 [0m

                       Computation: 3208 steps/s (collection: 0.472s, learning 2.082s)
               Value function loss: 47647.6415
                    Surrogate loss: 0.0222
             Mean action noise std: 0.93
                       Mean reward: 3543.72
               Mean episode length: 224.33
                 Mean success rate: 36.00
                  Mean reward/step: 15.57
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7987200
                    Iteration time: 2.55s
                        Total time: 2504.14s
                               ETA: 7771.8s

################################################################################
                     [1m Learning iteration 975/4000 [0m

                       Computation: 3214 steps/s (collection: 0.484s, learning 2.065s)
               Value function loss: 61939.0116
                    Surrogate loss: 0.0185
             Mean action noise std: 0.93
                       Mean reward: 3329.34
               Mean episode length: 219.10
                 Mean success rate: 33.50
                  Mean reward/step: 15.96
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 2.55s
                        Total time: 2506.69s
                               ETA: 7769.2s

################################################################################
                     [1m Learning iteration 976/4000 [0m

                       Computation: 3170 steps/s (collection: 0.502s, learning 2.082s)
               Value function loss: 74316.1276
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 3402.04
               Mean episode length: 221.68
                 Mean success rate: 34.50
                  Mean reward/step: 15.77
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8003584
                    Iteration time: 2.58s
                        Total time: 2509.28s
                               ETA: 7766.7s

################################################################################
                     [1m Learning iteration 977/4000 [0m

                       Computation: 3104 steps/s (collection: 0.538s, learning 2.101s)
               Value function loss: 60699.1512
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 2980.11
               Mean episode length: 209.18
                 Mean success rate: 31.50
                  Mean reward/step: 15.46
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 2.64s
                        Total time: 2511.92s
                               ETA: 7764.3s

################################################################################
                     [1m Learning iteration 978/4000 [0m

                       Computation: 3198 steps/s (collection: 0.476s, learning 2.085s)
               Value function loss: 34568.6104
                    Surrogate loss: 0.0171
             Mean action noise std: 0.93
                       Mean reward: 2837.03
               Mean episode length: 201.65
                 Mean success rate: 30.00
                  Mean reward/step: 15.62
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8019968
                    Iteration time: 2.56s
                        Total time: 2514.48s
                               ETA: 7761.7s

################################################################################
                     [1m Learning iteration 979/4000 [0m

                       Computation: 3125 steps/s (collection: 0.499s, learning 2.122s)
               Value function loss: 72954.4244
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 3076.42
               Mean episode length: 215.41
                 Mean success rate: 32.50
                  Mean reward/step: 15.27
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 2.62s
                        Total time: 2517.10s
                               ETA: 7759.3s

################################################################################
                     [1m Learning iteration 980/4000 [0m

                       Computation: 3149 steps/s (collection: 0.501s, learning 2.100s)
               Value function loss: 80285.2080
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 3231.10
               Mean episode length: 220.44
                 Mean success rate: 34.50
                  Mean reward/step: 14.81
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 8036352
                    Iteration time: 2.60s
                        Total time: 2519.70s
                               ETA: 7756.9s

################################################################################
                     [1m Learning iteration 981/4000 [0m

                       Computation: 3198 steps/s (collection: 0.501s, learning 2.061s)
               Value function loss: 52649.8871
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 3485.90
               Mean episode length: 231.46
                 Mean success rate: 35.50
                  Mean reward/step: 14.95
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 2.56s
                        Total time: 2522.26s
                               ETA: 7754.3s

################################################################################
                     [1m Learning iteration 982/4000 [0m

                       Computation: 3224 steps/s (collection: 0.455s, learning 2.086s)
               Value function loss: 54053.1956
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 3389.84
               Mean episode length: 221.83
                 Mean success rate: 35.00
                  Mean reward/step: 14.88
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8052736
                    Iteration time: 2.54s
                        Total time: 2524.80s
                               ETA: 7751.6s

################################################################################
                     [1m Learning iteration 983/4000 [0m

                       Computation: 3264 steps/s (collection: 0.477s, learning 2.032s)
               Value function loss: 72490.0558
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 3521.39
               Mean episode length: 231.63
                 Mean success rate: 35.50
                  Mean reward/step: 14.00
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 2.51s
                        Total time: 2527.31s
                               ETA: 7748.9s

################################################################################
                     [1m Learning iteration 984/4000 [0m

                       Computation: 3244 steps/s (collection: 0.486s, learning 2.039s)
               Value function loss: 70913.1378
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 3835.52
               Mean episode length: 241.42
                 Mean success rate: 38.50
                  Mean reward/step: 14.14
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8069120
                    Iteration time: 2.52s
                        Total time: 2529.84s
                               ETA: 7746.2s

################################################################################
                     [1m Learning iteration 985/4000 [0m

                       Computation: 3178 steps/s (collection: 0.473s, learning 2.104s)
               Value function loss: 67925.7949
                    Surrogate loss: 0.0127
             Mean action noise std: 0.93
                       Mean reward: 3869.67
               Mean episode length: 244.06
                 Mean success rate: 38.00
                  Mean reward/step: 14.36
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 2.58s
                        Total time: 2532.41s
                               ETA: 7743.6s

################################################################################
                     [1m Learning iteration 986/4000 [0m

                       Computation: 3202 steps/s (collection: 0.473s, learning 2.085s)
               Value function loss: 51198.6373
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 3607.20
               Mean episode length: 237.00
                 Mean success rate: 35.00
                  Mean reward/step: 13.84
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8085504
                    Iteration time: 2.56s
                        Total time: 2534.97s
                               ETA: 7741.0s

################################################################################
                     [1m Learning iteration 987/4000 [0m

                       Computation: 3219 steps/s (collection: 0.457s, learning 2.088s)
               Value function loss: 68084.2920
                    Surrogate loss: 0.0125
             Mean action noise std: 0.93
                       Mean reward: 3754.94
               Mean episode length: 245.88
                 Mean success rate: 37.50
                  Mean reward/step: 14.25
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 2.54s
                        Total time: 2537.51s
                               ETA: 7738.4s

################################################################################
                     [1m Learning iteration 988/4000 [0m

                       Computation: 3186 steps/s (collection: 0.494s, learning 2.076s)
               Value function loss: 55962.1813
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 3937.80
               Mean episode length: 253.12
                 Mean success rate: 39.00
                  Mean reward/step: 14.07
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8101888
                    Iteration time: 2.57s
                        Total time: 2540.09s
                               ETA: 7735.8s

################################################################################
                     [1m Learning iteration 989/4000 [0m

                       Computation: 3115 steps/s (collection: 0.514s, learning 2.116s)
               Value function loss: 68536.9982
                    Surrogate loss: 0.0195
             Mean action noise std: 0.93
                       Mean reward: 4192.13
               Mean episode length: 261.11
                 Mean success rate: 42.00
                  Mean reward/step: 14.13
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 2.63s
                        Total time: 2542.72s
                               ETA: 7733.4s

################################################################################
                     [1m Learning iteration 990/4000 [0m

                       Computation: 3235 steps/s (collection: 0.477s, learning 2.055s)
               Value function loss: 69290.3273
                    Surrogate loss: 0.0166
             Mean action noise std: 0.93
                       Mean reward: 3980.91
               Mean episode length: 254.47
                 Mean success rate: 40.50
                  Mean reward/step: 14.93
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 8118272
                    Iteration time: 2.53s
                        Total time: 2545.25s
                               ETA: 7730.8s

################################################################################
                     [1m Learning iteration 991/4000 [0m

                       Computation: 3231 steps/s (collection: 0.446s, learning 2.089s)
               Value function loss: 46231.5929
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 3543.68
               Mean episode length: 242.81
                 Mean success rate: 36.50
                  Mean reward/step: 15.47
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 2.54s
                        Total time: 2547.78s
                               ETA: 7728.1s

################################################################################
                     [1m Learning iteration 992/4000 [0m

                       Computation: 3236 steps/s (collection: 0.477s, learning 2.054s)
               Value function loss: 58355.3631
                    Surrogate loss: 0.0212
             Mean action noise std: 0.93
                       Mean reward: 3548.50
               Mean episode length: 241.23
                 Mean success rate: 37.50
                  Mean reward/step: 15.48
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8134656
                    Iteration time: 2.53s
                        Total time: 2550.31s
                               ETA: 7725.4s

################################################################################
                     [1m Learning iteration 993/4000 [0m

                       Computation: 3268 steps/s (collection: 0.457s, learning 2.050s)
               Value function loss: 72803.1984
                    Surrogate loss: 0.0209
             Mean action noise std: 0.93
                       Mean reward: 3526.57
               Mean episode length: 234.08
                 Mean success rate: 37.00
                  Mean reward/step: 15.39
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 2.51s
                        Total time: 2552.82s
                               ETA: 7722.7s

################################################################################
                     [1m Learning iteration 994/4000 [0m

                       Computation: 3211 steps/s (collection: 0.465s, learning 2.086s)
               Value function loss: 64086.3212
                    Surrogate loss: 0.0166
             Mean action noise std: 0.93
                       Mean reward: 3464.29
               Mean episode length: 233.98
                 Mean success rate: 37.50
                  Mean reward/step: 15.51
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8151040
                    Iteration time: 2.55s
                        Total time: 2555.37s
                               ETA: 7720.0s

################################################################################
                     [1m Learning iteration 995/4000 [0m

                       Computation: 3246 steps/s (collection: 0.449s, learning 2.075s)
               Value function loss: 71379.8447
                    Surrogate loss: 0.0162
             Mean action noise std: 0.93
                       Mean reward: 3499.01
               Mean episode length: 238.40
                 Mean success rate: 39.50
                  Mean reward/step: 15.27
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 2.52s
                        Total time: 2557.89s
                               ETA: 7717.3s

################################################################################
                     [1m Learning iteration 996/4000 [0m

                       Computation: 3203 steps/s (collection: 0.477s, learning 2.081s)
               Value function loss: 79249.6548
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 3714.93
               Mean episode length: 250.71
                 Mean success rate: 41.00
                  Mean reward/step: 14.91
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 8167424
                    Iteration time: 2.56s
                        Total time: 2560.45s
                               ETA: 7714.7s

################################################################################
                     [1m Learning iteration 997/4000 [0m

                       Computation: 3282 steps/s (collection: 0.465s, learning 2.031s)
               Value function loss: 54793.4921
                    Surrogate loss: 0.0195
             Mean action noise std: 0.93
                       Mean reward: 3672.57
               Mean episode length: 252.85
                 Mean success rate: 43.00
                  Mean reward/step: 14.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 2.50s
                        Total time: 2562.95s
                               ETA: 7712.0s

################################################################################
                     [1m Learning iteration 998/4000 [0m

                       Computation: 3272 steps/s (collection: 0.457s, learning 2.046s)
               Value function loss: 81625.1358
                    Surrogate loss: 0.0124
             Mean action noise std: 0.93
                       Mean reward: 4046.61
               Mean episode length: 270.75
                 Mean success rate: 49.50
                  Mean reward/step: 13.75
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 8183808
                    Iteration time: 2.50s
                        Total time: 2565.45s
                               ETA: 7709.2s

################################################################################
                     [1m Learning iteration 999/4000 [0m

                       Computation: 3218 steps/s (collection: 0.497s, learning 2.048s)
               Value function loss: 51507.4541
                    Surrogate loss: 0.0135
             Mean action noise std: 0.93
                       Mean reward: 4160.78
               Mean episode length: 277.75
                 Mean success rate: 52.00
                  Mean reward/step: 13.45
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 2.55s
                        Total time: 2568.00s
                               ETA: 7706.6s

################################################################################
                     [1m Learning iteration 1000/4000 [0m

                       Computation: 3213 steps/s (collection: 0.478s, learning 2.071s)
               Value function loss: 45979.2720
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 4251.88
               Mean episode length: 281.99
                 Mean success rate: 52.50
                  Mean reward/step: 14.12
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8200192
                    Iteration time: 2.55s
                        Total time: 2570.54s
                               ETA: 7703.9s

################################################################################
                     [1m Learning iteration 1001/4000 [0m

                       Computation: 3225 steps/s (collection: 0.476s, learning 2.064s)
               Value function loss: 62706.3044
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 3740.77
               Mean episode length: 272.07
                 Mean success rate: 48.00
                  Mean reward/step: 13.99
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 2.54s
                        Total time: 2573.08s
                               ETA: 7701.3s

################################################################################
                     [1m Learning iteration 1002/4000 [0m

                       Computation: 3233 steps/s (collection: 0.471s, learning 2.062s)
               Value function loss: 62268.5494
                    Surrogate loss: 0.0132
             Mean action noise std: 0.93
                       Mean reward: 4134.12
               Mean episode length: 280.93
                 Mean success rate: 51.00
                  Mean reward/step: 13.82
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8216576
                    Iteration time: 2.53s
                        Total time: 2575.62s
                               ETA: 7698.6s

################################################################################
                     [1m Learning iteration 1003/4000 [0m

                       Computation: 3198 steps/s (collection: 0.495s, learning 2.066s)
               Value function loss: 43080.1696
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 3872.74
               Mean episode length: 267.17
                 Mean success rate: 48.00
                  Mean reward/step: 13.15
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 2.56s
                        Total time: 2578.18s
                               ETA: 7696.0s

################################################################################
                     [1m Learning iteration 1004/4000 [0m

                       Computation: 3251 steps/s (collection: 0.458s, learning 2.062s)
               Value function loss: 39383.0132
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 3900.11
               Mean episode length: 269.36
                 Mean success rate: 47.00
                  Mean reward/step: 13.49
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8232960
                    Iteration time: 2.52s
                        Total time: 2580.70s
                               ETA: 7693.3s

################################################################################
                     [1m Learning iteration 1005/4000 [0m

                       Computation: 3286 steps/s (collection: 0.452s, learning 2.040s)
               Value function loss: 64205.8309
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 3619.51
               Mean episode length: 256.70
                 Mean success rate: 44.00
                  Mean reward/step: 14.08
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 2.49s
                        Total time: 2583.19s
                               ETA: 7690.5s

################################################################################
                     [1m Learning iteration 1006/4000 [0m

                       Computation: 3170 steps/s (collection: 0.482s, learning 2.101s)
               Value function loss: 62690.6201
                    Surrogate loss: 0.0124
             Mean action noise std: 0.93
                       Mean reward: 3772.52
               Mean episode length: 260.06
                 Mean success rate: 44.50
                  Mean reward/step: 14.37
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8249344
                    Iteration time: 2.58s
                        Total time: 2585.77s
                               ETA: 7688.0s

################################################################################
                     [1m Learning iteration 1007/4000 [0m

                       Computation: 3213 steps/s (collection: 0.494s, learning 2.055s)
               Value function loss: 64296.2758
                    Surrogate loss: 0.0177
             Mean action noise std: 0.93
                       Mean reward: 3855.19
               Mean episode length: 259.75
                 Mean success rate: 44.00
                  Mean reward/step: 14.75
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 2.55s
                        Total time: 2588.32s
                               ETA: 7685.4s

################################################################################
                     [1m Learning iteration 1008/4000 [0m

                       Computation: 3218 steps/s (collection: 0.489s, learning 2.056s)
               Value function loss: 56931.1951
                    Surrogate loss: 0.0161
             Mean action noise std: 0.93
                       Mean reward: 3845.67
               Mean episode length: 247.45
                 Mean success rate: 43.50
                  Mean reward/step: 15.00
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8265728
                    Iteration time: 2.55s
                        Total time: 2590.87s
                               ETA: 7682.7s

################################################################################
                     [1m Learning iteration 1009/4000 [0m

                       Computation: 3294 steps/s (collection: 0.440s, learning 2.047s)
               Value function loss: 73111.0483
                    Surrogate loss: 0.0161
             Mean action noise std: 0.93
                       Mean reward: 3565.87
               Mean episode length: 242.97
                 Mean success rate: 41.50
                  Mean reward/step: 15.11
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 2.49s
                        Total time: 2593.36s
                               ETA: 7679.9s

################################################################################
                     [1m Learning iteration 1010/4000 [0m

                       Computation: 3246 steps/s (collection: 0.448s, learning 2.075s)
               Value function loss: 59199.7326
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 3644.36
               Mean episode length: 243.61
                 Mean success rate: 43.00
                  Mean reward/step: 15.09
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8282112
                    Iteration time: 2.52s
                        Total time: 2595.88s
                               ETA: 7677.2s

################################################################################
                     [1m Learning iteration 1011/4000 [0m

                       Computation: 3238 steps/s (collection: 0.492s, learning 2.037s)
               Value function loss: 63016.0173
                    Surrogate loss: 0.0175
             Mean action noise std: 0.93
                       Mean reward: 3481.85
               Mean episode length: 243.69
                 Mean success rate: 42.00
                  Mean reward/step: 14.40
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 2.53s
                        Total time: 2598.41s
                               ETA: 7674.5s

################################################################################
                     [1m Learning iteration 1012/4000 [0m

                       Computation: 3259 steps/s (collection: 0.465s, learning 2.048s)
               Value function loss: 70989.1679
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 3440.71
               Mean episode length: 244.05
                 Mean success rate: 41.50
                  Mean reward/step: 14.20
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 8298496
                    Iteration time: 2.51s
                        Total time: 2600.92s
                               ETA: 7671.8s

################################################################################
                     [1m Learning iteration 1013/4000 [0m

                       Computation: 3290 steps/s (collection: 0.459s, learning 2.030s)
               Value function loss: 72397.1317
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 3365.21
               Mean episode length: 248.01
                 Mean success rate: 42.00
                  Mean reward/step: 14.73
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 2.49s
                        Total time: 2603.41s
                               ETA: 7669.0s

################################################################################
                     [1m Learning iteration 1014/4000 [0m

                       Computation: 3324 steps/s (collection: 0.440s, learning 2.023s)
               Value function loss: 64007.9143
                    Surrogate loss: 0.0172
             Mean action noise std: 0.93
                       Mean reward: 3592.34
               Mean episode length: 259.85
                 Mean success rate: 45.00
                  Mean reward/step: 14.70
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8314880
                    Iteration time: 2.46s
                        Total time: 2605.87s
                               ETA: 7666.1s

################################################################################
                     [1m Learning iteration 1015/4000 [0m

                       Computation: 3319 steps/s (collection: 0.427s, learning 2.041s)
               Value function loss: 55214.4354
                    Surrogate loss: 0.0166
             Mean action noise std: 0.93
                       Mean reward: 3547.35
               Mean episode length: 255.51
                 Mean success rate: 43.50
                  Mean reward/step: 14.69
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 2.47s
                        Total time: 2608.34s
                               ETA: 7663.3s

################################################################################
                     [1m Learning iteration 1016/4000 [0m

                       Computation: 3262 steps/s (collection: 0.454s, learning 2.058s)
               Value function loss: 49131.1100
                    Surrogate loss: 0.0160
             Mean action noise std: 0.93
                       Mean reward: 3488.60
               Mean episode length: 252.22
                 Mean success rate: 42.50
                  Mean reward/step: 15.98
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8331264
                    Iteration time: 2.51s
                        Total time: 2610.85s
                               ETA: 7660.6s

################################################################################
                     [1m Learning iteration 1017/4000 [0m

                       Computation: 3163 steps/s (collection: 0.508s, learning 2.082s)
               Value function loss: 75541.1506
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 3687.56
               Mean episode length: 253.82
                 Mean success rate: 43.00
                  Mean reward/step: 15.87
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 2.59s
                        Total time: 2613.44s
                               ETA: 7658.1s

################################################################################
                     [1m Learning iteration 1018/4000 [0m

                       Computation: 3221 steps/s (collection: 0.499s, learning 2.044s)
               Value function loss: 72412.7883
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 3698.46
               Mean episode length: 255.16
                 Mean success rate: 44.50
                  Mean reward/step: 16.31
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8347648
                    Iteration time: 2.54s
                        Total time: 2615.99s
                               ETA: 7655.4s

################################################################################
                     [1m Learning iteration 1019/4000 [0m

                       Computation: 3269 steps/s (collection: 0.467s, learning 2.039s)
               Value function loss: 64249.4465
                    Surrogate loss: 0.0152
             Mean action noise std: 0.93
                       Mean reward: 3652.71
               Mean episode length: 256.98
                 Mean success rate: 44.50
                  Mean reward/step: 16.63
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 2.51s
                        Total time: 2618.49s
                               ETA: 7652.7s

################################################################################
                     [1m Learning iteration 1020/4000 [0m

                       Computation: 3243 steps/s (collection: 0.477s, learning 2.049s)
               Value function loss: 64918.1874
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 3644.75
               Mean episode length: 245.11
                 Mean success rate: 43.00
                  Mean reward/step: 16.75
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8364032
                    Iteration time: 2.53s
                        Total time: 2621.02s
                               ETA: 7650.0s

################################################################################
                     [1m Learning iteration 1021/4000 [0m

                       Computation: 3263 steps/s (collection: 0.459s, learning 2.051s)
               Value function loss: 40598.4149
                    Surrogate loss: 0.0176
             Mean action noise std: 0.93
                       Mean reward: 3487.32
               Mean episode length: 238.42
                 Mean success rate: 41.50
                  Mean reward/step: 16.86
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 2.51s
                        Total time: 2623.53s
                               ETA: 7647.2s

################################################################################
                     [1m Learning iteration 1022/4000 [0m

                       Computation: 3244 steps/s (collection: 0.461s, learning 2.064s)
               Value function loss: 74345.3908
                    Surrogate loss: 0.0135
             Mean action noise std: 0.93
                       Mean reward: 3882.79
               Mean episode length: 253.16
                 Mean success rate: 45.50
                  Mean reward/step: 17.40
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8380416
                    Iteration time: 2.53s
                        Total time: 2626.05s
                               ETA: 7644.6s

################################################################################
                     [1m Learning iteration 1023/4000 [0m

                       Computation: 3294 steps/s (collection: 0.440s, learning 2.047s)
               Value function loss: 93373.8295
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 4036.88
               Mean episode length: 255.90
                 Mean success rate: 46.00
                  Mean reward/step: 17.06
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 2.49s
                        Total time: 2628.54s
                               ETA: 7641.8s

################################################################################
                     [1m Learning iteration 1024/4000 [0m

                       Computation: 3236 steps/s (collection: 0.444s, learning 2.087s)
               Value function loss: 64045.1329
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 3924.15
               Mean episode length: 254.74
                 Mean success rate: 46.50
                  Mean reward/step: 15.97
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8396800
                    Iteration time: 2.53s
                        Total time: 2631.07s
                               ETA: 7639.1s

################################################################################
                     [1m Learning iteration 1025/4000 [0m

                       Computation: 3213 steps/s (collection: 0.453s, learning 2.096s)
               Value function loss: 68137.9833
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 4030.73
               Mean episode length: 255.41
                 Mean success rate: 47.00
                  Mean reward/step: 16.45
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 2.55s
                        Total time: 2633.62s
                               ETA: 7636.5s

################################################################################
                     [1m Learning iteration 1026/4000 [0m

                       Computation: 3258 steps/s (collection: 0.428s, learning 2.086s)
               Value function loss: 80382.3744
                    Surrogate loss: 0.0189
             Mean action noise std: 0.93
                       Mean reward: 4159.67
               Mean episode length: 257.25
                 Mean success rate: 48.00
                  Mean reward/step: 16.09
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8413184
                    Iteration time: 2.51s
                        Total time: 2636.13s
                               ETA: 7633.7s

################################################################################
                     [1m Learning iteration 1027/4000 [0m

                       Computation: 3208 steps/s (collection: 0.461s, learning 2.092s)
               Value function loss: 55734.0927
                    Surrogate loss: 0.0187
             Mean action noise std: 0.93
                       Mean reward: 4177.64
               Mean episode length: 258.81
                 Mean success rate: 50.00
                  Mean reward/step: 15.41
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 2.55s
                        Total time: 2638.69s
                               ETA: 7631.1s

################################################################################
                     [1m Learning iteration 1028/4000 [0m

                       Computation: 3168 steps/s (collection: 0.478s, learning 2.107s)
               Value function loss: 66762.3968
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 4050.89
               Mean episode length: 257.66
                 Mean success rate: 50.50
                  Mean reward/step: 15.64
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8429568
                    Iteration time: 2.59s
                        Total time: 2641.27s
                               ETA: 7628.6s

################################################################################
                     [1m Learning iteration 1029/4000 [0m

                       Computation: 3217 steps/s (collection: 0.458s, learning 2.088s)
               Value function loss: 83815.2476
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 4172.07
               Mean episode length: 255.77
                 Mean success rate: 51.00
                  Mean reward/step: 14.71
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 2.55s
                        Total time: 2643.82s
                               ETA: 7626.0s

################################################################################
                     [1m Learning iteration 1030/4000 [0m

                       Computation: 3111 steps/s (collection: 0.540s, learning 2.093s)
               Value function loss: 79551.6436
                    Surrogate loss: 0.0165
             Mean action noise std: 0.93
                       Mean reward: 4435.14
               Mean episode length: 265.56
                 Mean success rate: 53.00
                  Mean reward/step: 14.68
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8445952
                    Iteration time: 2.63s
                        Total time: 2646.45s
                               ETA: 7623.6s

################################################################################
                     [1m Learning iteration 1031/4000 [0m

                       Computation: 3263 steps/s (collection: 0.420s, learning 2.090s)
               Value function loss: 61731.2556
                    Surrogate loss: 0.0182
             Mean action noise std: 0.93
                       Mean reward: 4431.14
               Mean episode length: 265.89
                 Mean success rate: 52.50
                  Mean reward/step: 14.13
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 2.51s
                        Total time: 2648.96s
                               ETA: 7620.9s

################################################################################
                     [1m Learning iteration 1032/4000 [0m

                       Computation: 3163 steps/s (collection: 0.492s, learning 2.098s)
               Value function loss: 57161.9121
                    Surrogate loss: 0.0173
             Mean action noise std: 0.93
                       Mean reward: 4271.03
               Mean episode length: 261.19
                 Mean success rate: 50.50
                  Mean reward/step: 13.42
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 8462336
                    Iteration time: 2.59s
                        Total time: 2651.55s
                               ETA: 7618.4s

################################################################################
                     [1m Learning iteration 1033/4000 [0m

                       Computation: 3171 steps/s (collection: 0.474s, learning 2.109s)
               Value function loss: 72369.5769
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 4055.67
               Mean episode length: 250.35
                 Mean success rate: 46.00
                  Mean reward/step: 13.30
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 2.58s
                        Total time: 2654.13s
                               ETA: 7615.9s

################################################################################
                     [1m Learning iteration 1034/4000 [0m

                       Computation: 3185 steps/s (collection: 0.472s, learning 2.099s)
               Value function loss: 66851.8841
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 4386.00
               Mean episode length: 268.43
                 Mean success rate: 48.50
                  Mean reward/step: 12.53
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8478720
                    Iteration time: 2.57s
                        Total time: 2656.71s
                               ETA: 7613.3s

################################################################################
                     [1m Learning iteration 1035/4000 [0m

                       Computation: 3157 steps/s (collection: 0.488s, learning 2.107s)
               Value function loss: 70645.2985
                    Surrogate loss: 0.0188
             Mean action noise std: 0.93
                       Mean reward: 3868.11
               Mean episode length: 251.00
                 Mean success rate: 45.50
                  Mean reward/step: 12.20
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 2.59s
                        Total time: 2659.30s
                               ETA: 7610.8s

################################################################################
                     [1m Learning iteration 1036/4000 [0m

                       Computation: 3141 steps/s (collection: 0.509s, learning 2.098s)
               Value function loss: 62362.6719
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 3567.07
               Mean episode length: 236.16
                 Mean success rate: 42.50
                  Mean reward/step: 12.26
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8495104
                    Iteration time: 2.61s
                        Total time: 2661.91s
                               ETA: 7608.4s

################################################################################
                     [1m Learning iteration 1037/4000 [0m

                       Computation: 3164 steps/s (collection: 0.447s, learning 2.142s)
               Value function loss: 49515.1977
                    Surrogate loss: 0.0171
             Mean action noise std: 0.93
                       Mean reward: 3428.16
               Mean episode length: 236.19
                 Mean success rate: 41.50
                  Mean reward/step: 12.50
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 2.59s
                        Total time: 2664.50s
                               ETA: 7605.9s

################################################################################
                     [1m Learning iteration 1038/4000 [0m

                       Computation: 3170 steps/s (collection: 0.483s, learning 2.101s)
               Value function loss: 45706.5847
                    Surrogate loss: 0.0191
             Mean action noise std: 0.93
                       Mean reward: 3348.16
               Mean episode length: 235.21
                 Mean success rate: 41.50
                  Mean reward/step: 13.13
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8511488
                    Iteration time: 2.58s
                        Total time: 2667.08s
                               ETA: 7603.4s

################################################################################
                     [1m Learning iteration 1039/4000 [0m

                       Computation: 3112 steps/s (collection: 0.499s, learning 2.133s)
               Value function loss: 58188.1997
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 3232.70
               Mean episode length: 236.44
                 Mean success rate: 41.50
                  Mean reward/step: 12.77
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 2.63s
                        Total time: 2669.71s
                               ETA: 7601.0s

################################################################################
                     [1m Learning iteration 1040/4000 [0m

                       Computation: 3106 steps/s (collection: 0.498s, learning 2.139s)
               Value function loss: 59983.8857
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 2992.60
               Mean episode length: 224.74
                 Mean success rate: 38.50
                  Mean reward/step: 12.66
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 8527872
                    Iteration time: 2.64s
                        Total time: 2672.35s
                               ETA: 7598.6s

################################################################################
                     [1m Learning iteration 1041/4000 [0m

                       Computation: 3205 steps/s (collection: 0.492s, learning 2.064s)
               Value function loss: 39151.8708
                    Surrogate loss: 0.0196
             Mean action noise std: 0.93
                       Mean reward: 2946.15
               Mean episode length: 223.08
                 Mean success rate: 36.50
                  Mean reward/step: 13.46
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 2.56s
                        Total time: 2674.90s
                               ETA: 7596.0s

################################################################################
                     [1m Learning iteration 1042/4000 [0m

                       Computation: 3192 steps/s (collection: 0.477s, learning 2.089s)
               Value function loss: 56845.2632
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 2851.61
               Mean episode length: 224.62
                 Mean success rate: 36.00
                  Mean reward/step: 13.05
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8544256
                    Iteration time: 2.57s
                        Total time: 2677.47s
                               ETA: 7593.4s

################################################################################
                     [1m Learning iteration 1043/4000 [0m

                       Computation: 3112 steps/s (collection: 0.480s, learning 2.151s)
               Value function loss: 67668.5174
                    Surrogate loss: 0.0160
             Mean action noise std: 0.93
                       Mean reward: 2824.72
               Mean episode length: 223.00
                 Mean success rate: 36.00
                  Mean reward/step: 12.55
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 2.63s
                        Total time: 2680.10s
                               ETA: 7591.1s

################################################################################
                     [1m Learning iteration 1044/4000 [0m

                       Computation: 3039 steps/s (collection: 0.560s, learning 2.135s)
               Value function loss: 52161.4748
                    Surrogate loss: 0.0182
             Mean action noise std: 0.93
                       Mean reward: 3000.96
               Mean episode length: 229.75
                 Mean success rate: 38.50
                  Mean reward/step: 12.46
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 8560640
                    Iteration time: 2.69s
                        Total time: 2682.80s
                               ETA: 7588.9s

################################################################################
                     [1m Learning iteration 1045/4000 [0m

                       Computation: 3155 steps/s (collection: 0.496s, learning 2.100s)
               Value function loss: 35861.1312
                    Surrogate loss: 0.0202
             Mean action noise std: 0.93
                       Mean reward: 2758.88
               Mean episode length: 223.40
                 Mean success rate: 37.00
                  Mean reward/step: 13.03
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 2.60s
                        Total time: 2685.39s
                               ETA: 7586.4s

################################################################################
                     [1m Learning iteration 1046/4000 [0m

                       Computation: 3195 steps/s (collection: 0.441s, learning 2.122s)
               Value function loss: 65091.4171
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 2824.57
               Mean episode length: 230.07
                 Mean success rate: 39.50
                  Mean reward/step: 13.64
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 8577024
                    Iteration time: 2.56s
                        Total time: 2687.96s
                               ETA: 7583.8s

################################################################################
                     [1m Learning iteration 1047/4000 [0m

                       Computation: 3147 steps/s (collection: 0.473s, learning 2.129s)
               Value function loss: 37330.7634
                    Surrogate loss: 0.0188
             Mean action noise std: 0.93
                       Mean reward: 2725.69
               Mean episode length: 226.59
                 Mean success rate: 38.00
                  Mean reward/step: 13.87
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 2.60s
                        Total time: 2690.56s
                               ETA: 7581.3s

################################################################################
                     [1m Learning iteration 1048/4000 [0m

                       Computation: 3180 steps/s (collection: 0.479s, learning 2.096s)
               Value function loss: 65048.5533
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 2989.77
               Mean episode length: 235.57
                 Mean success rate: 41.00
                  Mean reward/step: 14.25
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8593408
                    Iteration time: 2.58s
                        Total time: 2693.14s
                               ETA: 7578.8s

################################################################################
                     [1m Learning iteration 1049/4000 [0m

                       Computation: 3248 steps/s (collection: 0.455s, learning 2.067s)
               Value function loss: 70130.3421
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 2999.93
               Mean episode length: 236.37
                 Mean success rate: 40.50
                  Mean reward/step: 13.99
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 2.52s
                        Total time: 2695.66s
                               ETA: 7576.1s

################################################################################
                     [1m Learning iteration 1050/4000 [0m

                       Computation: 3187 steps/s (collection: 0.491s, learning 2.079s)
               Value function loss: 55009.4159
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 2751.81
               Mean episode length: 219.57
                 Mean success rate: 37.00
                  Mean reward/step: 13.41
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 8609792
                    Iteration time: 2.57s
                        Total time: 2698.23s
                               ETA: 7573.5s

################################################################################
                     [1m Learning iteration 1051/4000 [0m

                       Computation: 3209 steps/s (collection: 0.460s, learning 2.092s)
               Value function loss: 58963.8800
                    Surrogate loss: 0.0129
             Mean action noise std: 0.93
                       Mean reward: 2767.19
               Mean episode length: 217.95
                 Mean success rate: 35.50
                  Mean reward/step: 13.35
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 2.55s
                        Total time: 2700.78s
                               ETA: 7570.9s

################################################################################
                     [1m Learning iteration 1052/4000 [0m

                       Computation: 3208 steps/s (collection: 0.465s, learning 2.088s)
               Value function loss: 64109.8646
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 2998.70
               Mean episode length: 223.70
                 Mean success rate: 36.50
                  Mean reward/step: 12.97
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 8626176
                    Iteration time: 2.55s
                        Total time: 2703.33s
                               ETA: 7568.3s

################################################################################
                     [1m Learning iteration 1053/4000 [0m

                       Computation: 3166 steps/s (collection: 0.487s, learning 2.100s)
               Value function loss: 43747.2674
                    Surrogate loss: 0.0175
             Mean action noise std: 0.93
                       Mean reward: 2851.64
               Mean episode length: 214.76
                 Mean success rate: 35.50
                  Mean reward/step: 13.19
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 2.59s
                        Total time: 2705.92s
                               ETA: 7565.8s

################################################################################
                     [1m Learning iteration 1054/4000 [0m

                       Computation: 3268 steps/s (collection: 0.466s, learning 2.041s)
               Value function loss: 57886.0621
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 2541.56
               Mean episode length: 196.03
                 Mean success rate: 32.00
                  Mean reward/step: 13.20
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 8642560
                    Iteration time: 2.51s
                        Total time: 2708.43s
                               ETA: 7563.1s

################################################################################
                     [1m Learning iteration 1055/4000 [0m

                       Computation: 3259 steps/s (collection: 0.461s, learning 2.053s)
               Value function loss: 65167.3751
                    Surrogate loss: 0.0184
             Mean action noise std: 0.93
                       Mean reward: 2502.68
               Mean episode length: 197.94
                 Mean success rate: 31.00
                  Mean reward/step: 13.16
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 2.51s
                        Total time: 2710.94s
                               ETA: 7560.3s

################################################################################
                     [1m Learning iteration 1056/4000 [0m

                       Computation: 3293 steps/s (collection: 0.445s, learning 2.042s)
               Value function loss: 65348.2814
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 2636.82
               Mean episode length: 201.60
                 Mean success rate: 33.00
                  Mean reward/step: 13.20
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8658944
                    Iteration time: 2.49s
                        Total time: 2713.43s
                               ETA: 7557.6s

################################################################################
                     [1m Learning iteration 1057/4000 [0m

                       Computation: 3267 steps/s (collection: 0.454s, learning 2.053s)
               Value function loss: 63771.0225
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 2572.62
               Mean episode length: 202.00
                 Mean success rate: 33.00
                  Mean reward/step: 13.81
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 2.51s
                        Total time: 2715.93s
                               ETA: 7554.8s

################################################################################
                     [1m Learning iteration 1058/4000 [0m

                       Computation: 3260 steps/s (collection: 0.474s, learning 2.039s)
               Value function loss: 62332.1918
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 2662.43
               Mean episode length: 202.76
                 Mean success rate: 33.00
                  Mean reward/step: 14.14
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8675328
                    Iteration time: 2.51s
                        Total time: 2718.45s
                               ETA: 7552.1s

################################################################################
                     [1m Learning iteration 1059/4000 [0m

                       Computation: 3204 steps/s (collection: 0.474s, learning 2.082s)
               Value function loss: 69083.2783
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 3001.37
               Mean episode length: 216.90
                 Mean success rate: 37.50
                  Mean reward/step: 14.30
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 2.56s
                        Total time: 2721.00s
                               ETA: 7549.5s

################################################################################
                     [1m Learning iteration 1060/4000 [0m

                       Computation: 3205 steps/s (collection: 0.465s, learning 2.090s)
               Value function loss: 56660.0919
                    Surrogate loss: 0.0198
             Mean action noise std: 0.93
                       Mean reward: 3196.20
               Mean episode length: 224.44
                 Mean success rate: 38.00
                  Mean reward/step: 14.25
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 8691712
                    Iteration time: 2.56s
                        Total time: 2723.56s
                               ETA: 7546.9s

################################################################################
                     [1m Learning iteration 1061/4000 [0m

                       Computation: 3128 steps/s (collection: 0.529s, learning 2.090s)
               Value function loss: 59361.3416
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 3059.33
               Mean episode length: 214.29
                 Mean success rate: 36.00
                  Mean reward/step: 14.09
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 2.62s
                        Total time: 2726.18s
                               ETA: 7544.5s

################################################################################
                     [1m Learning iteration 1062/4000 [0m

                       Computation: 3212 steps/s (collection: 0.460s, learning 2.091s)
               Value function loss: 66479.0719
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 3239.11
               Mean episode length: 222.94
                 Mean success rate: 38.00
                  Mean reward/step: 14.29
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8708096
                    Iteration time: 2.55s
                        Total time: 2728.73s
                               ETA: 7541.9s

################################################################################
                     [1m Learning iteration 1063/4000 [0m

                       Computation: 3170 steps/s (collection: 0.460s, learning 2.124s)
               Value function loss: 34635.1818
                    Surrogate loss: 0.0189
             Mean action noise std: 0.93
                       Mean reward: 3030.58
               Mean episode length: 213.41
                 Mean success rate: 35.50
                  Mean reward/step: 14.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 2.58s
                        Total time: 2731.31s
                               ETA: 7539.3s

################################################################################
                     [1m Learning iteration 1064/4000 [0m

                       Computation: 3074 steps/s (collection: 0.523s, learning 2.142s)
               Value function loss: 52297.0784
                    Surrogate loss: 0.0182
             Mean action noise std: 0.93
                       Mean reward: 3007.13
               Mean episode length: 218.34
                 Mean success rate: 34.50
                  Mean reward/step: 15.55
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8724480
                    Iteration time: 2.66s
                        Total time: 2733.98s
                               ETA: 7537.0s

################################################################################
                     [1m Learning iteration 1065/4000 [0m

                       Computation: 3134 steps/s (collection: 0.508s, learning 2.106s)
               Value function loss: 73376.9684
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 3101.81
               Mean episode length: 219.82
                 Mean success rate: 34.50
                  Mean reward/step: 15.47
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 2.61s
                        Total time: 2736.59s
                               ETA: 7534.6s

################################################################################
                     [1m Learning iteration 1066/4000 [0m

                       Computation: 3217 steps/s (collection: 0.458s, learning 2.088s)
               Value function loss: 58358.0439
                    Surrogate loss: 0.0171
             Mean action noise std: 0.93
                       Mean reward: 3221.08
               Mean episode length: 226.10
                 Mean success rate: 36.00
                  Mean reward/step: 15.10
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 8740864
                    Iteration time: 2.55s
                        Total time: 2739.14s
                               ETA: 7532.0s

################################################################################
                     [1m Learning iteration 1067/4000 [0m

                       Computation: 3146 steps/s (collection: 0.475s, learning 2.128s)
               Value function loss: 72314.0375
                    Surrogate loss: 0.0167
             Mean action noise std: 0.93
                       Mean reward: 3302.99
               Mean episode length: 229.97
                 Mean success rate: 37.50
                  Mean reward/step: 14.55
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 2.60s
                        Total time: 2741.74s
                               ETA: 7529.5s

################################################################################
                     [1m Learning iteration 1068/4000 [0m

                       Computation: 3093 steps/s (collection: 0.509s, learning 2.139s)
               Value function loss: 61964.4328
                    Surrogate loss: 0.0174
             Mean action noise std: 0.93
                       Mean reward: 3354.76
               Mean episode length: 235.00
                 Mean success rate: 38.00
                  Mean reward/step: 14.00
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8757248
                    Iteration time: 2.65s
                        Total time: 2744.39s
                               ETA: 7527.2s

################################################################################
                     [1m Learning iteration 1069/4000 [0m

                       Computation: 3189 steps/s (collection: 0.490s, learning 2.078s)
               Value function loss: 50327.1551
                    Surrogate loss: 0.0197
             Mean action noise std: 0.93
                       Mean reward: 3453.56
               Mean episode length: 234.35
                 Mean success rate: 39.50
                  Mean reward/step: 14.38
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 2.57s
                        Total time: 2746.96s
                               ETA: 7524.6s

################################################################################
                     [1m Learning iteration 1070/4000 [0m

                       Computation: 3190 steps/s (collection: 0.471s, learning 2.097s)
               Value function loss: 68526.0905
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 3626.74
               Mean episode length: 242.70
                 Mean success rate: 41.00
                  Mean reward/step: 15.61
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8773632
                    Iteration time: 2.57s
                        Total time: 2749.52s
                               ETA: 7522.0s

################################################################################
                     [1m Learning iteration 1071/4000 [0m

                       Computation: 3106 steps/s (collection: 0.502s, learning 2.135s)
               Value function loss: 68440.6819
                    Surrogate loss: 0.0204
             Mean action noise std: 0.93
                       Mean reward: 4168.83
               Mean episode length: 268.69
                 Mean success rate: 46.50
                  Mean reward/step: 15.36
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 2.64s
                        Total time: 2752.16s
                               ETA: 7519.7s

################################################################################
                     [1m Learning iteration 1072/4000 [0m

                       Computation: 3148 steps/s (collection: 0.450s, learning 2.151s)
               Value function loss: 43473.1136
                    Surrogate loss: 0.0193
             Mean action noise std: 0.93
                       Mean reward: 4139.99
               Mean episode length: 273.34
                 Mean success rate: 46.50
                  Mean reward/step: 15.11
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8790016
                    Iteration time: 2.60s
                        Total time: 2754.76s
                               ETA: 7517.2s

################################################################################
                     [1m Learning iteration 1073/4000 [0m

                       Computation: 3201 steps/s (collection: 0.495s, learning 2.063s)
               Value function loss: 43416.6426
                    Surrogate loss: 0.0179
             Mean action noise std: 0.93
                       Mean reward: 4014.76
               Mean episode length: 268.00
                 Mean success rate: 46.00
                  Mean reward/step: 16.12
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 2.56s
                        Total time: 2757.32s
                               ETA: 7514.6s

################################################################################
                     [1m Learning iteration 1074/4000 [0m

                       Computation: 3186 steps/s (collection: 0.456s, learning 2.115s)
               Value function loss: 52444.3208
                    Surrogate loss: 0.0170
             Mean action noise std: 0.93
                       Mean reward: 4171.07
               Mean episode length: 272.63
                 Mean success rate: 47.00
                  Mean reward/step: 16.48
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8806400
                    Iteration time: 2.57s
                        Total time: 2759.89s
                               ETA: 7512.0s

################################################################################
                     [1m Learning iteration 1075/4000 [0m

                       Computation: 3203 steps/s (collection: 0.483s, learning 2.074s)
               Value function loss: 63986.4996
                    Surrogate loss: 0.0173
             Mean action noise std: 0.93
                       Mean reward: 4325.57
               Mean episode length: 274.46
                 Mean success rate: 48.50
                  Mean reward/step: 16.63
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 2.56s
                        Total time: 2762.45s
                               ETA: 7509.4s

################################################################################
                     [1m Learning iteration 1076/4000 [0m

                       Computation: 3021 steps/s (collection: 0.552s, learning 2.159s)
               Value function loss: 72082.8905
                    Surrogate loss: 0.0171
             Mean action noise std: 0.93
                       Mean reward: 4426.04
               Mean episode length: 286.48
                 Mean success rate: 51.00
                  Mean reward/step: 16.14
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8822784
                    Iteration time: 2.71s
                        Total time: 2765.16s
                               ETA: 7507.3s

################################################################################
                     [1m Learning iteration 1077/4000 [0m

                       Computation: 3062 steps/s (collection: 0.533s, learning 2.142s)
               Value function loss: 101425.9134
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 4329.86
               Mean episode length: 286.61
                 Mean success rate: 52.00
                  Mean reward/step: 15.60
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 2.67s
                        Total time: 2767.84s
                               ETA: 7505.0s

################################################################################
                     [1m Learning iteration 1078/4000 [0m

                       Computation: 3170 steps/s (collection: 0.492s, learning 2.092s)
               Value function loss: 42938.8806
                    Surrogate loss: 0.0194
             Mean action noise std: 0.93
                       Mean reward: 4081.92
               Mean episode length: 275.73
                 Mean success rate: 50.00
                  Mean reward/step: 15.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8839168
                    Iteration time: 2.58s
                        Total time: 2770.42s
                               ETA: 7502.5s

################################################################################
                     [1m Learning iteration 1079/4000 [0m

                       Computation: 3181 steps/s (collection: 0.459s, learning 2.116s)
               Value function loss: 38473.4812
                    Surrogate loss: 0.0172
             Mean action noise std: 0.93
                       Mean reward: 4015.86
               Mean episode length: 266.38
                 Mean success rate: 50.00
                  Mean reward/step: 17.08
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 2.57s
                        Total time: 2772.99s
                               ETA: 7499.9s

################################################################################
                     [1m Learning iteration 1080/4000 [0m

                       Computation: 3132 steps/s (collection: 0.496s, learning 2.119s)
               Value function loss: 56295.8020
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 4209.78
               Mean episode length: 275.19
                 Mean success rate: 50.50
                  Mean reward/step: 17.57
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8855552
                    Iteration time: 2.61s
                        Total time: 2775.61s
                               ETA: 7497.5s

################################################################################
                     [1m Learning iteration 1081/4000 [0m

                       Computation: 3147 steps/s (collection: 0.494s, learning 2.109s)
               Value function loss: 94892.2634
                    Surrogate loss: 0.0162
             Mean action noise std: 0.93
                       Mean reward: 4628.08
               Mean episode length: 299.93
                 Mean success rate: 54.00
                  Mean reward/step: 17.11
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 2.60s
                        Total time: 2778.21s
                               ETA: 7495.0s

################################################################################
                     [1m Learning iteration 1082/4000 [0m

                       Computation: 3119 steps/s (collection: 0.546s, learning 2.080s)
               Value function loss: 76032.1029
                    Surrogate loss: 0.0114
             Mean action noise std: 0.93
                       Mean reward: 4342.27
               Mean episode length: 275.31
                 Mean success rate: 48.50
                  Mean reward/step: 15.61
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 8871936
                    Iteration time: 2.63s
                        Total time: 2780.84s
                               ETA: 7492.6s

################################################################################
                     [1m Learning iteration 1083/4000 [0m

                       Computation: 3154 steps/s (collection: 0.505s, learning 2.092s)
               Value function loss: 74191.0966
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 4389.92
               Mean episode length: 273.81
                 Mean success rate: 49.00
                  Mean reward/step: 15.50
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 2.60s
                        Total time: 2783.43s
                               ETA: 7490.1s

################################################################################
                     [1m Learning iteration 1084/4000 [0m

                       Computation: 3196 steps/s (collection: 0.458s, learning 2.105s)
               Value function loss: 58248.3446
                    Surrogate loss: 0.0174
             Mean action noise std: 0.93
                       Mean reward: 4308.83
               Mean episode length: 267.82
                 Mean success rate: 45.00
                  Mean reward/step: 15.52
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8888320
                    Iteration time: 2.56s
                        Total time: 2786.00s
                               ETA: 7487.5s

################################################################################
                     [1m Learning iteration 1085/4000 [0m

                       Computation: 3152 steps/s (collection: 0.487s, learning 2.112s)
               Value function loss: 61360.0232
                    Surrogate loss: 0.0152
             Mean action noise std: 0.93
                       Mean reward: 4342.38
               Mean episode length: 264.81
                 Mean success rate: 44.50
                  Mean reward/step: 15.97
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 2.60s
                        Total time: 2788.60s
                               ETA: 7485.0s

################################################################################
                     [1m Learning iteration 1086/4000 [0m

                       Computation: 3154 steps/s (collection: 0.487s, learning 2.110s)
               Value function loss: 74436.6881
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 4484.28
               Mean episode length: 263.56
                 Mean success rate: 47.50
                  Mean reward/step: 15.75
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8904704
                    Iteration time: 2.60s
                        Total time: 2791.19s
                               ETA: 7482.6s

################################################################################
                     [1m Learning iteration 1087/4000 [0m

                       Computation: 3204 steps/s (collection: 0.482s, learning 2.074s)
               Value function loss: 67806.6229
                    Surrogate loss: 0.0170
             Mean action noise std: 0.93
                       Mean reward: 4168.92
               Mean episode length: 249.24
                 Mean success rate: 45.00
                  Mean reward/step: 15.69
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 2.56s
                        Total time: 2793.75s
                               ETA: 7480.0s

################################################################################
                     [1m Learning iteration 1088/4000 [0m

                       Computation: 3216 steps/s (collection: 0.469s, learning 2.078s)
               Value function loss: 64977.3966
                    Surrogate loss: 0.0180
             Mean action noise std: 0.93
                       Mean reward: 4466.03
               Mean episode length: 263.26
                 Mean success rate: 48.00
                  Mean reward/step: 15.47
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8921088
                    Iteration time: 2.55s
                        Total time: 2796.30s
                               ETA: 7477.3s

################################################################################
                     [1m Learning iteration 1089/4000 [0m

                       Computation: 3184 steps/s (collection: 0.501s, learning 2.071s)
               Value function loss: 65902.0297
                    Surrogate loss: 0.0144
             Mean action noise std: 0.93
                       Mean reward: 4557.39
               Mean episode length: 263.63
                 Mean success rate: 48.50
                  Mean reward/step: 16.04
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 2.57s
                        Total time: 2798.87s
                               ETA: 7474.8s

################################################################################
                     [1m Learning iteration 1090/4000 [0m

                       Computation: 3090 steps/s (collection: 0.526s, learning 2.125s)
               Value function loss: 54104.2981
                    Surrogate loss: 0.0181
             Mean action noise std: 0.93
                       Mean reward: 4237.65
               Mean episode length: 255.08
                 Mean success rate: 46.50
                  Mean reward/step: 16.17
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8937472
                    Iteration time: 2.65s
                        Total time: 2801.52s
                               ETA: 7472.4s

################################################################################
                     [1m Learning iteration 1091/4000 [0m

                       Computation: 3122 steps/s (collection: 0.477s, learning 2.146s)
               Value function loss: 65038.9594
                    Surrogate loss: 0.0167
             Mean action noise std: 0.93
                       Mean reward: 4162.69
               Mean episode length: 258.38
                 Mean success rate: 46.50
                  Mean reward/step: 15.57
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 2.62s
                        Total time: 2804.14s
                               ETA: 7470.0s

################################################################################
                     [1m Learning iteration 1092/4000 [0m

                       Computation: 3145 steps/s (collection: 0.493s, learning 2.112s)
               Value function loss: 41594.0267
                    Surrogate loss: 0.0180
             Mean action noise std: 0.93
                       Mean reward: 3984.64
               Mean episode length: 261.05
                 Mean success rate: 46.00
                  Mean reward/step: 15.65
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8953856
                    Iteration time: 2.60s
                        Total time: 2806.75s
                               ETA: 7467.5s

################################################################################
                     [1m Learning iteration 1093/4000 [0m

                       Computation: 3179 steps/s (collection: 0.475s, learning 2.101s)
               Value function loss: 65536.0855
                    Surrogate loss: 0.0168
             Mean action noise std: 0.93
                       Mean reward: 3703.96
               Mean episode length: 250.53
                 Mean success rate: 42.00
                  Mean reward/step: 16.05
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 2.58s
                        Total time: 2809.32s
                               ETA: 7465.0s

################################################################################
                     [1m Learning iteration 1094/4000 [0m

                       Computation: 3145 steps/s (collection: 0.500s, learning 2.104s)
               Value function loss: 52901.2075
                    Surrogate loss: 0.0191
             Mean action noise std: 0.93
                       Mean reward: 3484.97
               Mean episode length: 244.38
                 Mean success rate: 41.00
                  Mean reward/step: 16.32
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8970240
                    Iteration time: 2.60s
                        Total time: 2811.93s
                               ETA: 7462.5s

################################################################################
                     [1m Learning iteration 1095/4000 [0m

                       Computation: 3190 steps/s (collection: 0.503s, learning 2.065s)
               Value function loss: 75689.0342
                    Surrogate loss: 0.0165
             Mean action noise std: 0.93
                       Mean reward: 3561.53
               Mean episode length: 250.80
                 Mean success rate: 42.00
                  Mean reward/step: 16.09
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 2.57s
                        Total time: 2814.49s
                               ETA: 7460.0s

################################################################################
                     [1m Learning iteration 1096/4000 [0m

                       Computation: 3246 steps/s (collection: 0.469s, learning 2.054s)
               Value function loss: 78183.1171
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 3764.75
               Mean episode length: 253.10
                 Mean success rate: 44.00
                  Mean reward/step: 15.59
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8986624
                    Iteration time: 2.52s
                        Total time: 2817.02s
                               ETA: 7457.3s

################################################################################
                     [1m Learning iteration 1097/4000 [0m

                       Computation: 3140 steps/s (collection: 0.474s, learning 2.134s)
               Value function loss: 57982.4569
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 3687.62
               Mean episode length: 244.51
                 Mean success rate: 44.50
                  Mean reward/step: 16.11
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 2.61s
                        Total time: 2819.63s
                               ETA: 7454.8s

################################################################################
                     [1m Learning iteration 1098/4000 [0m

                       Computation: 3200 steps/s (collection: 0.498s, learning 2.062s)
               Value function loss: 62686.3966
                    Surrogate loss: 0.0133
             Mean action noise std: 0.93
                       Mean reward: 4122.13
               Mean episode length: 254.31
                 Mean success rate: 47.00
                  Mean reward/step: 15.86
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 9003008
                    Iteration time: 2.56s
                        Total time: 2822.19s
                               ETA: 7452.2s

################################################################################
                     [1m Learning iteration 1099/4000 [0m

                       Computation: 3161 steps/s (collection: 0.491s, learning 2.101s)
               Value function loss: 64398.2856
                    Surrogate loss: 0.0184
             Mean action noise std: 0.93
                       Mean reward: 4293.96
               Mean episode length: 258.79
                 Mean success rate: 49.00
                  Mean reward/step: 16.36
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 2.59s
                        Total time: 2824.78s
                               ETA: 7449.7s

################################################################################
                     [1m Learning iteration 1100/4000 [0m

                       Computation: 3171 steps/s (collection: 0.467s, learning 2.117s)
               Value function loss: 50416.4872
                    Surrogate loss: 0.0159
             Mean action noise std: 0.93
                       Mean reward: 4298.71
               Mean episode length: 262.84
                 Mean success rate: 48.00
                  Mean reward/step: 16.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9019392
                    Iteration time: 2.58s
                        Total time: 2827.36s
                               ETA: 7447.2s

################################################################################
                     [1m Learning iteration 1101/4000 [0m

                       Computation: 3277 steps/s (collection: 0.474s, learning 2.025s)
               Value function loss: 97121.1736
                    Surrogate loss: 0.0185
             Mean action noise std: 0.93
                       Mean reward: 4828.04
               Mean episode length: 279.70
                 Mean success rate: 51.50
                  Mean reward/step: 16.25
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 2.50s
                        Total time: 2829.86s
                               ETA: 7444.4s

################################################################################
                     [1m Learning iteration 1102/4000 [0m

                       Computation: 3184 steps/s (collection: 0.498s, learning 2.074s)
               Value function loss: 58243.8388
                    Surrogate loss: 0.0128
             Mean action noise std: 0.93
                       Mean reward: 4643.04
               Mean episode length: 271.42
                 Mean success rate: 49.00
                  Mean reward/step: 15.72
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9035776
                    Iteration time: 2.57s
                        Total time: 2832.43s
                               ETA: 7441.9s

################################################################################
                     [1m Learning iteration 1103/4000 [0m

                       Computation: 3154 steps/s (collection: 0.502s, learning 2.095s)
               Value function loss: 55757.7237
                    Surrogate loss: 0.0152
             Mean action noise std: 0.93
                       Mean reward: 4235.22
               Mean episode length: 256.20
                 Mean success rate: 44.00
                  Mean reward/step: 16.60
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 2.60s
                        Total time: 2835.03s
                               ETA: 7439.4s

################################################################################
                     [1m Learning iteration 1104/4000 [0m

                       Computation: 3269 steps/s (collection: 0.439s, learning 2.067s)
               Value function loss: 71983.4039
                    Surrogate loss: 0.0144
             Mean action noise std: 0.93
                       Mean reward: 4310.12
               Mean episode length: 258.26
                 Mean success rate: 44.50
                  Mean reward/step: 18.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9052160
                    Iteration time: 2.51s
                        Total time: 2837.54s
                               ETA: 7436.7s

################################################################################
                     [1m Learning iteration 1105/4000 [0m

                       Computation: 3205 steps/s (collection: 0.476s, learning 2.080s)
               Value function loss: 72263.0474
                    Surrogate loss: 0.0180
             Mean action noise std: 0.93
                       Mean reward: 4155.78
               Mean episode length: 252.27
                 Mean success rate: 43.00
                  Mean reward/step: 17.98
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 2.56s
                        Total time: 2840.09s
                               ETA: 7434.1s

################################################################################
                     [1m Learning iteration 1106/4000 [0m

                       Computation: 3187 steps/s (collection: 0.477s, learning 2.093s)
               Value function loss: 70579.1873
                    Surrogate loss: 0.0172
             Mean action noise std: 0.93
                       Mean reward: 4310.29
               Mean episode length: 261.41
                 Mean success rate: 45.00
                  Mean reward/step: 17.37
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9068544
                    Iteration time: 2.57s
                        Total time: 2842.66s
                               ETA: 7431.5s

################################################################################
                     [1m Learning iteration 1107/4000 [0m

                       Computation: 3187 steps/s (collection: 0.485s, learning 2.085s)
               Value function loss: 72410.6633
                    Surrogate loss: 0.0159
             Mean action noise std: 0.93
                       Mean reward: 4398.65
               Mean episode length: 267.50
                 Mean success rate: 48.00
                  Mean reward/step: 16.98
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 2.57s
                        Total time: 2845.23s
                               ETA: 7428.9s

################################################################################
                     [1m Learning iteration 1108/4000 [0m

                       Computation: 3283 steps/s (collection: 0.439s, learning 2.057s)
               Value function loss: 52281.1939
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 4205.08
               Mean episode length: 264.50
                 Mean success rate: 46.50
                  Mean reward/step: 17.11
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9084928
                    Iteration time: 2.50s
                        Total time: 2847.73s
                               ETA: 7426.2s

################################################################################
                     [1m Learning iteration 1109/4000 [0m

                       Computation: 3168 steps/s (collection: 0.512s, learning 2.074s)
               Value function loss: 72491.9249
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 4570.60
               Mean episode length: 277.80
                 Mean success rate: 49.50
                  Mean reward/step: 16.59
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 2.59s
                        Total time: 2850.31s
                               ETA: 7423.6s

################################################################################
                     [1m Learning iteration 1110/4000 [0m

                       Computation: 3179 steps/s (collection: 0.480s, learning 2.097s)
               Value function loss: 64490.0414
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 4742.21
               Mean episode length: 284.07
                 Mean success rate: 51.00
                  Mean reward/step: 17.00
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9101312
                    Iteration time: 2.58s
                        Total time: 2852.89s
                               ETA: 7421.1s

################################################################################
                     [1m Learning iteration 1111/4000 [0m

                       Computation: 3163 steps/s (collection: 0.501s, learning 2.089s)
               Value function loss: 91297.1547
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 5032.17
               Mean episode length: 294.56
                 Mean success rate: 54.50
                  Mean reward/step: 16.92
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 2.59s
                        Total time: 2855.48s
                               ETA: 7418.6s

################################################################################
                     [1m Learning iteration 1112/4000 [0m

                       Computation: 3188 steps/s (collection: 0.490s, learning 2.079s)
               Value function loss: 87216.9673
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 4872.26
               Mean episode length: 287.35
                 Mean success rate: 53.00
                  Mean reward/step: 16.04
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 9117696
                    Iteration time: 2.57s
                        Total time: 2858.05s
                               ETA: 7416.0s

################################################################################
                     [1m Learning iteration 1113/4000 [0m

                       Computation: 3182 steps/s (collection: 0.459s, learning 2.115s)
               Value function loss: 71931.1817
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 4625.57
               Mean episode length: 274.97
                 Mean success rate: 51.00
                  Mean reward/step: 16.17
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 2.57s
                        Total time: 2860.62s
                               ETA: 7413.5s

################################################################################
                     [1m Learning iteration 1114/4000 [0m

                       Computation: 3215 steps/s (collection: 0.474s, learning 2.074s)
               Value function loss: 53980.6520
                    Surrogate loss: 0.0212
             Mean action noise std: 0.93
                       Mean reward: 4639.56
               Mean episode length: 274.20
                 Mean success rate: 51.50
                  Mean reward/step: 16.36
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9134080
                    Iteration time: 2.55s
                        Total time: 2863.17s
                               ETA: 7410.9s

################################################################################
                     [1m Learning iteration 1115/4000 [0m

                       Computation: 3155 steps/s (collection: 0.510s, learning 2.086s)
               Value function loss: 49985.0435
                    Surrogate loss: 0.0175
             Mean action noise std: 0.93
                       Mean reward: 4427.03
               Mean episode length: 264.93
                 Mean success rate: 49.50
                  Mean reward/step: 16.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 2.60s
                        Total time: 2865.76s
                               ETA: 7408.4s

################################################################################
                     [1m Learning iteration 1116/4000 [0m

                       Computation: 3203 steps/s (collection: 0.479s, learning 2.079s)
               Value function loss: 58356.4763
                    Surrogate loss: 0.0168
             Mean action noise std: 0.93
                       Mean reward: 4499.96
               Mean episode length: 267.18
                 Mean success rate: 51.50
                  Mean reward/step: 17.38
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9150464
                    Iteration time: 2.56s
                        Total time: 2868.32s
                               ETA: 7405.8s

################################################################################
                     [1m Learning iteration 1117/4000 [0m

                       Computation: 3231 steps/s (collection: 0.499s, learning 2.037s)
               Value function loss: 78447.0873
                    Surrogate loss: 0.0170
             Mean action noise std: 0.93
                       Mean reward: 4593.78
               Mean episode length: 272.77
                 Mean success rate: 52.00
                  Mean reward/step: 17.34
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 2.54s
                        Total time: 2870.86s
                               ETA: 7403.1s

################################################################################
                     [1m Learning iteration 1118/4000 [0m

                       Computation: 3229 steps/s (collection: 0.464s, learning 2.073s)
               Value function loss: 75082.8943
                    Surrogate loss: 0.0162
             Mean action noise std: 0.93
                       Mean reward: 4688.11
               Mean episode length: 278.98
                 Mean success rate: 51.00
                  Mean reward/step: 16.97
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9166848
                    Iteration time: 2.54s
                        Total time: 2873.39s
                               ETA: 7400.5s

################################################################################
                     [1m Learning iteration 1119/4000 [0m

                       Computation: 3181 steps/s (collection: 0.469s, learning 2.106s)
               Value function loss: 60356.7300
                    Surrogate loss: 0.0162
             Mean action noise std: 0.93
                       Mean reward: 4771.04
               Mean episode length: 280.49
                 Mean success rate: 53.00
                  Mean reward/step: 16.95
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 2.57s
                        Total time: 2875.97s
                               ETA: 7397.9s

################################################################################
                     [1m Learning iteration 1120/4000 [0m

                       Computation: 3190 steps/s (collection: 0.487s, learning 2.080s)
               Value function loss: 50552.3945
                    Surrogate loss: 0.0164
             Mean action noise std: 0.93
                       Mean reward: 4916.45
               Mean episode length: 287.21
                 Mean success rate: 55.00
                  Mean reward/step: 17.31
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9183232
                    Iteration time: 2.57s
                        Total time: 2878.54s
                               ETA: 7395.3s

################################################################################
                     [1m Learning iteration 1121/4000 [0m

                       Computation: 3173 steps/s (collection: 0.492s, learning 2.089s)
               Value function loss: 52641.1821
                    Surrogate loss: 0.0191
             Mean action noise std: 0.93
                       Mean reward: 4839.34
               Mean episode length: 281.86
                 Mean success rate: 53.00
                  Mean reward/step: 17.39
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 2.58s
                        Total time: 2881.12s
                               ETA: 7392.8s

################################################################################
                     [1m Learning iteration 1122/4000 [0m

                       Computation: 3216 steps/s (collection: 0.490s, learning 2.057s)
               Value function loss: 87766.2385
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 4888.45
               Mean episode length: 289.12
                 Mean success rate: 53.50
                  Mean reward/step: 17.58
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9199616
                    Iteration time: 2.55s
                        Total time: 2883.66s
                               ETA: 7390.2s

################################################################################
                     [1m Learning iteration 1123/4000 [0m

                       Computation: 3117 steps/s (collection: 0.508s, learning 2.120s)
               Value function loss: 75419.1157
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 4714.13
               Mean episode length: 278.21
                 Mean success rate: 50.50
                  Mean reward/step: 16.57
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 2.63s
                        Total time: 2886.29s
                               ETA: 7387.8s

################################################################################
                     [1m Learning iteration 1124/4000 [0m

                       Computation: 3233 steps/s (collection: 0.471s, learning 2.062s)
               Value function loss: 59067.9323
                    Surrogate loss: 0.0164
             Mean action noise std: 0.93
                       Mean reward: 4128.05
               Mean episode length: 255.88
                 Mean success rate: 47.00
                  Mean reward/step: 16.02
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 9216000
                    Iteration time: 2.53s
                        Total time: 2888.82s
                               ETA: 7385.1s

################################################################################
                     [1m Learning iteration 1125/4000 [0m

                       Computation: 3205 steps/s (collection: 0.458s, learning 2.098s)
               Value function loss: 66257.2690
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 3886.43
               Mean episode length: 248.32
                 Mean success rate: 45.00
                  Mean reward/step: 15.57
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 2.56s
                        Total time: 2891.38s
                               ETA: 7382.5s

################################################################################
                     [1m Learning iteration 1126/4000 [0m

                       Computation: 3179 steps/s (collection: 0.488s, learning 2.089s)
               Value function loss: 76303.2171
                    Surrogate loss: 0.0166
             Mean action noise std: 0.93
                       Mean reward: 4001.54
               Mean episode length: 247.60
                 Mean success rate: 45.00
                  Mean reward/step: 15.46
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 9232384
                    Iteration time: 2.58s
                        Total time: 2893.96s
                               ETA: 7380.0s

################################################################################
                     [1m Learning iteration 1127/4000 [0m

                       Computation: 3222 steps/s (collection: 0.482s, learning 2.061s)
               Value function loss: 77647.4284
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 4165.30
               Mean episode length: 251.53
                 Mean success rate: 46.00
                  Mean reward/step: 15.00
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 2.54s
                        Total time: 2896.50s
                               ETA: 7377.3s

################################################################################
                     [1m Learning iteration 1128/4000 [0m

                       Computation: 3220 steps/s (collection: 0.451s, learning 2.092s)
               Value function loss: 67475.9200
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 4306.39
               Mean episode length: 259.04
                 Mean success rate: 48.50
                  Mean reward/step: 13.95
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9248768
                    Iteration time: 2.54s
                        Total time: 2899.04s
                               ETA: 7374.7s

################################################################################
                     [1m Learning iteration 1129/4000 [0m

                       Computation: 3200 steps/s (collection: 0.498s, learning 2.062s)
               Value function loss: 76136.0677
                    Surrogate loss: 0.0152
             Mean action noise std: 0.93
                       Mean reward: 4535.18
               Mean episode length: 267.56
                 Mean success rate: 50.00
                  Mean reward/step: 15.02
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 2.56s
                        Total time: 2901.60s
                               ETA: 7372.1s

################################################################################
                     [1m Learning iteration 1130/4000 [0m

                       Computation: 3218 steps/s (collection: 0.497s, learning 2.048s)
               Value function loss: 68583.0091
                    Surrogate loss: 0.0131
             Mean action noise std: 0.93
                       Mean reward: 4725.84
               Mean episode length: 272.95
                 Mean success rate: 50.00
                  Mean reward/step: 15.75
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9265152
                    Iteration time: 2.55s
                        Total time: 2904.15s
                               ETA: 7369.5s

################################################################################
                     [1m Learning iteration 1131/4000 [0m

                       Computation: 3281 steps/s (collection: 0.454s, learning 2.042s)
               Value function loss: 56011.6592
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 4751.28
               Mean episode length: 267.69
                 Mean success rate: 49.00
                  Mean reward/step: 17.17
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 2.50s
                        Total time: 2906.64s
                               ETA: 7366.7s

################################################################################
                     [1m Learning iteration 1132/4000 [0m

                       Computation: 3203 steps/s (collection: 0.502s, learning 2.055s)
               Value function loss: 93066.5193
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 4952.03
               Mean episode length: 278.32
                 Mean success rate: 51.00
                  Mean reward/step: 17.77
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9281536
                    Iteration time: 2.56s
                        Total time: 2909.20s
                               ETA: 7364.2s

################################################################################
                     [1m Learning iteration 1133/4000 [0m

                       Computation: 3154 steps/s (collection: 0.488s, learning 2.109s)
               Value function loss: 55166.4154
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 4839.12
               Mean episode length: 277.10
                 Mean success rate: 49.50
                  Mean reward/step: 17.68
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 2.60s
                        Total time: 2911.80s
                               ETA: 7361.7s

################################################################################
                     [1m Learning iteration 1134/4000 [0m

                       Computation: 3139 steps/s (collection: 0.507s, learning 2.102s)
               Value function loss: 73349.2396
                    Surrogate loss: 0.0131
             Mean action noise std: 0.93
                       Mean reward: 4761.07
               Mean episode length: 280.80
                 Mean success rate: 49.50
                  Mean reward/step: 17.42
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9297920
                    Iteration time: 2.61s
                        Total time: 2914.41s
                               ETA: 7359.2s

################################################################################
                     [1m Learning iteration 1135/4000 [0m

                       Computation: 3185 steps/s (collection: 0.483s, learning 2.089s)
               Value function loss: 43470.2346
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 4460.20
               Mean episode length: 271.18
                 Mean success rate: 47.00
                  Mean reward/step: 17.79
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 2.57s
                        Total time: 2916.98s
                               ETA: 7356.6s

################################################################################
                     [1m Learning iteration 1136/4000 [0m

                       Computation: 3255 steps/s (collection: 0.430s, learning 2.087s)
               Value function loss: 53760.0011
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 4578.38
               Mean episode length: 274.06
                 Mean success rate: 48.00
                  Mean reward/step: 18.52
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9314304
                    Iteration time: 2.52s
                        Total time: 2919.50s
                               ETA: 7353.9s

################################################################################
                     [1m Learning iteration 1137/4000 [0m

                       Computation: 3220 steps/s (collection: 0.466s, learning 2.077s)
               Value function loss: 64885.6931
                    Surrogate loss: 0.0152
             Mean action noise std: 0.93
                       Mean reward: 4506.37
               Mean episode length: 275.05
                 Mean success rate: 48.50
                  Mean reward/step: 18.87
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 2.54s
                        Total time: 2922.04s
                               ETA: 7351.3s

################################################################################
                     [1m Learning iteration 1138/4000 [0m

                       Computation: 3162 steps/s (collection: 0.500s, learning 2.091s)
               Value function loss: 87352.6940
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 4684.61
               Mean episode length: 291.19
                 Mean success rate: 52.50
                  Mean reward/step: 18.49
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9330688
                    Iteration time: 2.59s
                        Total time: 2924.63s
                               ETA: 7348.8s

################################################################################
                     [1m Learning iteration 1139/4000 [0m

                       Computation: 3180 steps/s (collection: 0.489s, learning 2.087s)
               Value function loss: 67198.8301
                    Surrogate loss: 0.0186
             Mean action noise std: 0.93
                       Mean reward: 4540.29
               Mean episode length: 293.67
                 Mean success rate: 53.00
                  Mean reward/step: 18.20
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 2.58s
                        Total time: 2927.21s
                               ETA: 7346.3s

################################################################################
                     [1m Learning iteration 1140/4000 [0m

                       Computation: 3152 steps/s (collection: 0.512s, learning 2.087s)
               Value function loss: 77009.8019
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 4691.50
               Mean episode length: 300.71
                 Mean success rate: 53.50
                  Mean reward/step: 18.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9347072
                    Iteration time: 2.60s
                        Total time: 2929.80s
                               ETA: 7343.8s

################################################################################
                     [1m Learning iteration 1141/4000 [0m

                       Computation: 3276 steps/s (collection: 0.455s, learning 2.046s)
               Value function loss: 79392.4890
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 4914.88
               Mean episode length: 306.71
                 Mean success rate: 55.00
                  Mean reward/step: 17.98
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 2.50s
                        Total time: 2932.30s
                               ETA: 7341.0s

################################################################################
                     [1m Learning iteration 1142/4000 [0m

                       Computation: 3272 steps/s (collection: 0.446s, learning 2.057s)
               Value function loss: 91064.9355
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 5158.30
               Mean episode length: 315.02
                 Mean success rate: 57.00
                  Mean reward/step: 17.84
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9363456
                    Iteration time: 2.50s
                        Total time: 2934.81s
                               ETA: 7338.3s

################################################################################
                     [1m Learning iteration 1143/4000 [0m

                       Computation: 3250 steps/s (collection: 0.485s, learning 2.036s)
               Value function loss: 89741.6889
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 5623.69
               Mean episode length: 331.94
                 Mean success rate: 61.50
                  Mean reward/step: 17.80
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 2.52s
                        Total time: 2937.33s
                               ETA: 7335.6s

################################################################################
                     [1m Learning iteration 1144/4000 [0m

                       Computation: 3317 steps/s (collection: 0.449s, learning 2.020s)
               Value function loss: 80839.6517
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 5608.88
               Mean episode length: 324.71
                 Mean success rate: 59.00
                  Mean reward/step: 17.43
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9379840
                    Iteration time: 2.47s
                        Total time: 2939.80s
                               ETA: 7332.8s

################################################################################
                     [1m Learning iteration 1145/4000 [0m

                       Computation: 3251 steps/s (collection: 0.470s, learning 2.050s)
               Value function loss: 89607.0395
                    Surrogate loss: 0.0159
             Mean action noise std: 0.93
                       Mean reward: 5718.31
               Mean episode length: 323.81
                 Mean success rate: 59.00
                  Mean reward/step: 16.96
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 2.52s
                        Total time: 2942.32s
                               ETA: 7330.1s

################################################################################
                     [1m Learning iteration 1146/4000 [0m

                       Computation: 3200 steps/s (collection: 0.497s, learning 2.062s)
               Value function loss: 59867.4366
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 5968.12
               Mean episode length: 330.02
                 Mean success rate: 60.50
                  Mean reward/step: 16.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9396224
                    Iteration time: 2.56s
                        Total time: 2944.88s
                               ETA: 7327.5s

################################################################################
                     [1m Learning iteration 1147/4000 [0m

                       Computation: 3229 steps/s (collection: 0.483s, learning 2.053s)
               Value function loss: 56010.4550
                    Surrogate loss: 0.0167
             Mean action noise std: 0.93
                       Mean reward: 6082.11
               Mean episode length: 333.49
                 Mean success rate: 61.50
                  Mean reward/step: 16.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 2.54s
                        Total time: 2947.41s
                               ETA: 7324.9s

################################################################################
                     [1m Learning iteration 1148/4000 [0m

                       Computation: 3302 steps/s (collection: 0.437s, learning 2.044s)
               Value function loss: 88804.1263
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 6246.24
               Mean episode length: 331.43
                 Mean success rate: 62.00
                  Mean reward/step: 16.16
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9412608
                    Iteration time: 2.48s
                        Total time: 2949.89s
                               ETA: 7322.1s

################################################################################
                     [1m Learning iteration 1149/4000 [0m

                       Computation: 3227 steps/s (collection: 0.467s, learning 2.072s)
               Value function loss: 57872.6665
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 6203.42
               Mean episode length: 331.40
                 Mean success rate: 62.00
                  Mean reward/step: 15.81
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 2.54s
                        Total time: 2952.43s
                               ETA: 7319.5s

################################################################################
                     [1m Learning iteration 1150/4000 [0m

                       Computation: 3238 steps/s (collection: 0.466s, learning 2.064s)
               Value function loss: 59516.4559
                    Surrogate loss: 0.0190
             Mean action noise std: 0.93
                       Mean reward: 6115.24
               Mean episode length: 331.13
                 Mean success rate: 61.00
                  Mean reward/step: 15.96
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9428992
                    Iteration time: 2.53s
                        Total time: 2954.96s
                               ETA: 7316.8s

################################################################################
                     [1m Learning iteration 1151/4000 [0m

                       Computation: 3236 steps/s (collection: 0.458s, learning 2.073s)
               Value function loss: 59171.5062
                    Surrogate loss: 0.0162
             Mean action noise std: 0.93
                       Mean reward: 5336.29
               Mean episode length: 305.29
                 Mean success rate: 53.50
                  Mean reward/step: 16.50
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 2.53s
                        Total time: 2957.49s
                               ETA: 7314.1s

################################################################################
                     [1m Learning iteration 1152/4000 [0m

                       Computation: 3261 steps/s (collection: 0.470s, learning 2.042s)
               Value function loss: 49204.0124
                    Surrogate loss: 0.0131
             Mean action noise std: 0.93
                       Mean reward: 4823.10
               Mean episode length: 281.70
                 Mean success rate: 49.50
                  Mean reward/step: 17.04
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9445376
                    Iteration time: 2.51s
                        Total time: 2960.00s
                               ETA: 7311.4s

################################################################################
                     [1m Learning iteration 1153/4000 [0m

                       Computation: 3248 steps/s (collection: 0.450s, learning 2.073s)
               Value function loss: 52099.5729
                    Surrogate loss: 0.0129
             Mean action noise std: 0.93
                       Mean reward: 4783.37
               Mean episode length: 284.58
                 Mean success rate: 48.50
                  Mean reward/step: 16.92
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 2.52s
                        Total time: 2962.53s
                               ETA: 7308.8s

################################################################################
                     [1m Learning iteration 1154/4000 [0m

                       Computation: 3251 steps/s (collection: 0.474s, learning 2.045s)
               Value function loss: 70282.4917
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 4501.11
               Mean episode length: 276.71
                 Mean success rate: 46.00
                  Mean reward/step: 16.60
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9461760
                    Iteration time: 2.52s
                        Total time: 2965.05s
                               ETA: 7306.1s

################################################################################
                     [1m Learning iteration 1155/4000 [0m

                       Computation: 3189 steps/s (collection: 0.495s, learning 2.073s)
               Value function loss: 76788.9492
                    Surrogate loss: 0.0125
             Mean action noise std: 0.93
                       Mean reward: 4097.59
               Mean episode length: 264.61
                 Mean success rate: 42.50
                  Mean reward/step: 16.40
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 2.57s
                        Total time: 2967.61s
                               ETA: 7303.5s

################################################################################
                     [1m Learning iteration 1156/4000 [0m

                       Computation: 3144 steps/s (collection: 0.525s, learning 2.080s)
               Value function loss: 72769.0404
                    Surrogate loss: 0.0120
             Mean action noise std: 0.93
                       Mean reward: 3980.67
               Mean episode length: 252.12
                 Mean success rate: 41.50
                  Mean reward/step: 16.74
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9478144
                    Iteration time: 2.61s
                        Total time: 2970.22s
                               ETA: 7301.0s

################################################################################
                     [1m Learning iteration 1157/4000 [0m

                       Computation: 3245 steps/s (collection: 0.458s, learning 2.066s)
               Value function loss: 62741.7619
                    Surrogate loss: 0.0201
             Mean action noise std: 0.93
                       Mean reward: 4263.45
               Mean episode length: 260.71
                 Mean success rate: 44.50
                  Mean reward/step: 16.89
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 2.52s
                        Total time: 2972.74s
                               ETA: 7298.4s

################################################################################
                     [1m Learning iteration 1158/4000 [0m

                       Computation: 3260 steps/s (collection: 0.471s, learning 2.041s)
               Value function loss: 69284.0135
                    Surrogate loss: 0.0202
             Mean action noise std: 0.93
                       Mean reward: 4419.01
               Mean episode length: 269.37
                 Mean success rate: 45.50
                  Mean reward/step: 16.62
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 9494528
                    Iteration time: 2.51s
                        Total time: 2975.26s
                               ETA: 7295.7s

################################################################################
                     [1m Learning iteration 1159/4000 [0m

                       Computation: 3284 steps/s (collection: 0.455s, learning 2.039s)
               Value function loss: 66831.0889
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 4123.21
               Mean episode length: 255.97
                 Mean success rate: 42.50
                  Mean reward/step: 16.50
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 2.49s
                        Total time: 2977.75s
                               ETA: 7292.9s

################################################################################
                     [1m Learning iteration 1160/4000 [0m

                       Computation: 3261 steps/s (collection: 0.453s, learning 2.059s)
               Value function loss: 76676.6064
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 4225.17
               Mean episode length: 260.12
                 Mean success rate: 42.50
                  Mean reward/step: 16.94
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9510912
                    Iteration time: 2.51s
                        Total time: 2980.26s
                               ETA: 7290.2s

################################################################################
                     [1m Learning iteration 1161/4000 [0m

                       Computation: 3283 steps/s (collection: 0.448s, learning 2.048s)
               Value function loss: 70779.1239
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 4432.56
               Mean episode length: 264.58
                 Mean success rate: 44.50
                  Mean reward/step: 16.51
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 2.50s
                        Total time: 2982.76s
                               ETA: 7287.5s

################################################################################
                     [1m Learning iteration 1162/4000 [0m

                       Computation: 3086 steps/s (collection: 0.491s, learning 2.164s)
               Value function loss: 53130.0634
                    Surrogate loss: 0.0168
             Mean action noise std: 0.93
                       Mean reward: 4345.74
               Mean episode length: 260.30
                 Mean success rate: 44.00
                  Mean reward/step: 16.92
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9527296
                    Iteration time: 2.65s
                        Total time: 2985.41s
                               ETA: 7285.1s

################################################################################
                     [1m Learning iteration 1163/4000 [0m

                       Computation: 3266 steps/s (collection: 0.424s, learning 2.084s)
               Value function loss: 76752.8115
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 4092.82
               Mean episode length: 249.01
                 Mean success rate: 40.00
                  Mean reward/step: 17.51
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 2.51s
                        Total time: 2987.92s
                               ETA: 7282.4s

################################################################################
                     [1m Learning iteration 1164/4000 [0m

                       Computation: 3263 steps/s (collection: 0.458s, learning 2.052s)
               Value function loss: 64611.1823
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 4247.89
               Mean episode length: 252.07
                 Mean success rate: 41.50
                  Mean reward/step: 16.95
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9543680
                    Iteration time: 2.51s
                        Total time: 2990.43s
                               ETA: 7279.7s

################################################################################
                     [1m Learning iteration 1165/4000 [0m

                       Computation: 3143 steps/s (collection: 0.503s, learning 2.103s)
               Value function loss: 63968.2227
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 4234.77
               Mean episode length: 251.32
                 Mean success rate: 41.50
                  Mean reward/step: 17.55
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 2.61s
                        Total time: 2993.04s
                               ETA: 7277.2s

################################################################################
                     [1m Learning iteration 1166/4000 [0m

                       Computation: 3132 steps/s (collection: 0.500s, learning 2.115s)
               Value function loss: 84723.0623
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 4708.41
               Mean episode length: 267.85
                 Mean success rate: 46.00
                  Mean reward/step: 17.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9560064
                    Iteration time: 2.62s
                        Total time: 2995.65s
                               ETA: 7274.8s

################################################################################
                     [1m Learning iteration 1167/4000 [0m

                       Computation: 3117 steps/s (collection: 0.482s, learning 2.146s)
               Value function loss: 72445.0969
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 4913.50
               Mean episode length: 273.29
                 Mean success rate: 47.50
                  Mean reward/step: 17.60
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 2.63s
                        Total time: 2998.28s
                               ETA: 7272.4s

################################################################################
                     [1m Learning iteration 1168/4000 [0m

                       Computation: 3261 steps/s (collection: 0.446s, learning 2.066s)
               Value function loss: 70371.2296
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 4710.91
               Mean episode length: 269.13
                 Mean success rate: 45.50
                  Mean reward/step: 17.38
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9576448
                    Iteration time: 2.51s
                        Total time: 3000.79s
                               ETA: 7269.7s

################################################################################
                     [1m Learning iteration 1169/4000 [0m

                       Computation: 3251 steps/s (collection: 0.454s, learning 2.066s)
               Value function loss: 59061.0339
                    Surrogate loss: 0.0170
             Mean action noise std: 0.93
                       Mean reward: 4755.27
               Mean episode length: 272.05
                 Mean success rate: 46.00
                  Mean reward/step: 17.98
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 2.52s
                        Total time: 3003.31s
                               ETA: 7267.0s

################################################################################
                     [1m Learning iteration 1170/4000 [0m

                       Computation: 3250 steps/s (collection: 0.462s, learning 2.059s)
               Value function loss: 62590.4974
                    Surrogate loss: 0.0162
             Mean action noise std: 0.93
                       Mean reward: 5132.51
               Mean episode length: 288.04
                 Mean success rate: 49.50
                  Mean reward/step: 17.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9592832
                    Iteration time: 2.52s
                        Total time: 3005.83s
                               ETA: 7264.3s

################################################################################
                     [1m Learning iteration 1171/4000 [0m

                       Computation: 3141 steps/s (collection: 0.531s, learning 2.077s)
               Value function loss: 74020.0667
                    Surrogate loss: 0.0132
             Mean action noise std: 0.93
                       Mean reward: 5496.87
               Mean episode length: 308.50
                 Mean success rate: 54.00
                  Mean reward/step: 17.47
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 2.61s
                        Total time: 3008.44s
                               ETA: 7261.8s

################################################################################
                     [1m Learning iteration 1172/4000 [0m

                       Computation: 3157 steps/s (collection: 0.501s, learning 2.093s)
               Value function loss: 72029.5616
                    Surrogate loss: 0.0112
             Mean action noise std: 0.93
                       Mean reward: 5442.99
               Mean episode length: 304.36
                 Mean success rate: 53.50
                  Mean reward/step: 16.65
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9609216
                    Iteration time: 2.59s
                        Total time: 3011.03s
                               ETA: 7259.3s

################################################################################
                     [1m Learning iteration 1173/4000 [0m

                       Computation: 3154 steps/s (collection: 0.487s, learning 2.111s)
               Value function loss: 67186.1404
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 5644.64
               Mean episode length: 312.90
                 Mean success rate: 56.50
                  Mean reward/step: 16.28
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 2.60s
                        Total time: 3013.63s
                               ETA: 7256.8s

################################################################################
                     [1m Learning iteration 1174/4000 [0m

                       Computation: 3210 steps/s (collection: 0.468s, learning 2.084s)
               Value function loss: 74406.1030
                    Surrogate loss: 0.0125
             Mean action noise std: 0.93
                       Mean reward: 5250.74
               Mean episode length: 306.63
                 Mean success rate: 54.50
                  Mean reward/step: 15.91
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9625600
                    Iteration time: 2.55s
                        Total time: 3016.18s
                               ETA: 7254.2s

################################################################################
                     [1m Learning iteration 1175/4000 [0m

                       Computation: 3305 steps/s (collection: 0.450s, learning 2.028s)
               Value function loss: 59833.5868
                    Surrogate loss: 0.0159
             Mean action noise std: 0.93
                       Mean reward: 5418.18
               Mean episode length: 315.64
                 Mean success rate: 57.50
                  Mean reward/step: 15.48
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 2.48s
                        Total time: 3018.66s
                               ETA: 7251.5s

################################################################################
                     [1m Learning iteration 1176/4000 [0m

                       Computation: 3154 steps/s (collection: 0.492s, learning 2.104s)
               Value function loss: 62309.6743
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 5209.57
               Mean episode length: 306.25
                 Mean success rate: 55.50
                  Mean reward/step: 15.78
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9641984
                    Iteration time: 2.60s
                        Total time: 3021.26s
                               ETA: 7249.0s

################################################################################
                     [1m Learning iteration 1177/4000 [0m

                       Computation: 3228 steps/s (collection: 0.463s, learning 2.074s)
               Value function loss: 93130.5956
                    Surrogate loss: 0.0112
             Mean action noise std: 0.93
                       Mean reward: 5339.05
               Mean episode length: 307.24
                 Mean success rate: 56.50
                  Mean reward/step: 16.43
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 2.54s
                        Total time: 3023.79s
                               ETA: 7246.3s

################################################################################
                     [1m Learning iteration 1178/4000 [0m

                       Computation: 3109 steps/s (collection: 0.484s, learning 2.151s)
               Value function loss: 65564.8938
                    Surrogate loss: 0.0176
             Mean action noise std: 0.93
                       Mean reward: 5185.82
               Mean episode length: 305.94
                 Mean success rate: 55.00
                  Mean reward/step: 16.90
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9658368
                    Iteration time: 2.63s
                        Total time: 3026.43s
                               ETA: 7243.9s

################################################################################
                     [1m Learning iteration 1179/4000 [0m

                       Computation: 3147 steps/s (collection: 0.522s, learning 2.080s)
               Value function loss: 78595.9190
                    Surrogate loss: 0.0186
             Mean action noise std: 0.93
                       Mean reward: 5200.32
               Mean episode length: 309.14
                 Mean success rate: 55.50
                  Mean reward/step: 16.02
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 2.60s
                        Total time: 3029.03s
                               ETA: 7241.4s

################################################################################
                     [1m Learning iteration 1180/4000 [0m

                       Computation: 3154 steps/s (collection: 0.454s, learning 2.143s)
               Value function loss: 80527.8516
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 5306.17
               Mean episode length: 312.56
                 Mean success rate: 56.00
                  Mean reward/step: 15.93
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9674752
                    Iteration time: 2.60s
                        Total time: 3031.63s
                               ETA: 7238.9s

################################################################################
                     [1m Learning iteration 1181/4000 [0m

                       Computation: 3115 steps/s (collection: 0.512s, learning 2.117s)
               Value function loss: 63930.8604
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 5241.33
               Mean episode length: 311.06
                 Mean success rate: 56.00
                  Mean reward/step: 16.63
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 2.63s
                        Total time: 3034.26s
                               ETA: 7236.5s

################################################################################
                     [1m Learning iteration 1182/4000 [0m

                       Computation: 3096 steps/s (collection: 0.491s, learning 2.154s)
               Value function loss: 49028.8328
                    Surrogate loss: 0.0164
             Mean action noise std: 0.93
                       Mean reward: 4812.47
               Mean episode length: 286.33
                 Mean success rate: 51.00
                  Mean reward/step: 17.51
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9691136
                    Iteration time: 2.65s
                        Total time: 3036.90s
                               ETA: 7234.1s

################################################################################
                     [1m Learning iteration 1183/4000 [0m

                       Computation: 3073 steps/s (collection: 0.505s, learning 2.160s)
               Value function loss: 54842.3928
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 4706.46
               Mean episode length: 283.06
                 Mean success rate: 50.00
                  Mean reward/step: 17.89
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 2.67s
                        Total time: 3039.57s
                               ETA: 7231.8s

################################################################################
                     [1m Learning iteration 1184/4000 [0m

                       Computation: 3116 steps/s (collection: 0.517s, learning 2.112s)
               Value function loss: 64286.9209
                    Surrogate loss: 0.0178
             Mean action noise std: 0.93
                       Mean reward: 5026.76
               Mean episode length: 298.56
                 Mean success rate: 53.50
                  Mean reward/step: 18.04
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9707520
                    Iteration time: 2.63s
                        Total time: 3042.20s
                               ETA: 7229.4s

################################################################################
                     [1m Learning iteration 1185/4000 [0m

                       Computation: 3154 steps/s (collection: 0.484s, learning 2.113s)
               Value function loss: 48874.4958
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 4995.28
               Mean episode length: 304.39
                 Mean success rate: 53.50
                  Mean reward/step: 18.47
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 2.60s
                        Total time: 3044.79s
                               ETA: 7226.9s

################################################################################
                     [1m Learning iteration 1186/4000 [0m

                       Computation: 3116 steps/s (collection: 0.491s, learning 2.138s)
               Value function loss: 77599.7125
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 4808.85
               Mean episode length: 300.72
                 Mean success rate: 52.50
                  Mean reward/step: 18.48
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9723904
                    Iteration time: 2.63s
                        Total time: 3047.42s
                               ETA: 7224.5s

################################################################################
                     [1m Learning iteration 1187/4000 [0m

                       Computation: 3233 steps/s (collection: 0.504s, learning 2.029s)
               Value function loss: 86959.9148
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 4744.01
               Mean episode length: 298.28
                 Mean success rate: 53.00
                  Mean reward/step: 18.61
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 2.53s
                        Total time: 3049.96s
                               ETA: 7221.8s

################################################################################
                     [1m Learning iteration 1188/4000 [0m

                       Computation: 3218 steps/s (collection: 0.487s, learning 2.059s)
               Value function loss: 60975.2215
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 4679.02
               Mean episode length: 296.61
                 Mean success rate: 53.00
                  Mean reward/step: 17.81
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9740288
                    Iteration time: 2.55s
                        Total time: 3052.50s
                               ETA: 7219.2s

################################################################################
                     [1m Learning iteration 1189/4000 [0m

                       Computation: 3261 steps/s (collection: 0.464s, learning 2.048s)
               Value function loss: 79783.5004
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 4941.63
               Mean episode length: 305.42
                 Mean success rate: 54.50
                  Mean reward/step: 17.37
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 2.51s
                        Total time: 3055.01s
                               ETA: 7216.5s

################################################################################
                     [1m Learning iteration 1190/4000 [0m

                       Computation: 3237 steps/s (collection: 0.467s, learning 2.064s)
               Value function loss: 98682.8156
                    Surrogate loss: 0.0165
             Mean action noise std: 0.93
                       Mean reward: 5385.03
               Mean episode length: 324.23
                 Mean success rate: 58.00
                  Mean reward/step: 16.64
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9756672
                    Iteration time: 2.53s
                        Total time: 3057.54s
                               ETA: 7213.9s

################################################################################
                     [1m Learning iteration 1191/4000 [0m

                       Computation: 3232 steps/s (collection: 0.477s, learning 2.057s)
               Value function loss: 59427.7033
                    Surrogate loss: 0.0174
             Mean action noise std: 0.93
                       Mean reward: 5462.25
               Mean episode length: 329.32
                 Mean success rate: 59.00
                  Mean reward/step: 16.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 2.53s
                        Total time: 3060.08s
                               ETA: 7211.2s

################################################################################
                     [1m Learning iteration 1192/4000 [0m

                       Computation: 3253 steps/s (collection: 0.490s, learning 2.027s)
               Value function loss: 78809.8184
                    Surrogate loss: 0.0195
             Mean action noise std: 0.93
                       Mean reward: 5370.68
               Mean episode length: 316.52
                 Mean success rate: 58.50
                  Mean reward/step: 17.53
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9773056
                    Iteration time: 2.52s
                        Total time: 3062.60s
                               ETA: 7208.5s

################################################################################
                     [1m Learning iteration 1193/4000 [0m

                       Computation: 3263 steps/s (collection: 0.462s, learning 2.048s)
               Value function loss: 65218.6313
                    Surrogate loss: 0.0198
             Mean action noise std: 0.93
                       Mean reward: 5641.24
               Mean episode length: 321.23
                 Mean success rate: 59.50
                  Mean reward/step: 17.24
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 2.51s
                        Total time: 3065.11s
                               ETA: 7205.8s

################################################################################
                     [1m Learning iteration 1194/4000 [0m

                       Computation: 3177 steps/s (collection: 0.489s, learning 2.089s)
               Value function loss: 55838.2731
                    Surrogate loss: 0.0175
             Mean action noise std: 0.93
                       Mean reward: 5656.76
               Mean episode length: 317.53
                 Mean success rate: 60.00
                  Mean reward/step: 17.11
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9789440
                    Iteration time: 2.58s
                        Total time: 3067.68s
                               ETA: 7203.3s

################################################################################
                     [1m Learning iteration 1195/4000 [0m

                       Computation: 3228 steps/s (collection: 0.497s, learning 2.040s)
               Value function loss: 73772.6497
                    Surrogate loss: 0.0159
             Mean action noise std: 0.93
                       Mean reward: 5680.17
               Mean episode length: 320.43
                 Mean success rate: 59.50
                  Mean reward/step: 17.77
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 2.54s
                        Total time: 3070.22s
                               ETA: 7200.6s

################################################################################
                     [1m Learning iteration 1196/4000 [0m

                       Computation: 3221 steps/s (collection: 0.501s, learning 2.042s)
               Value function loss: 58739.0061
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 5322.48
               Mean episode length: 305.65
                 Mean success rate: 56.00
                  Mean reward/step: 17.81
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9805824
                    Iteration time: 2.54s
                        Total time: 3072.76s
                               ETA: 7198.0s

################################################################################
                     [1m Learning iteration 1197/4000 [0m

                       Computation: 3228 steps/s (collection: 0.475s, learning 2.063s)
               Value function loss: 57473.2815
                    Surrogate loss: 0.0168
             Mean action noise std: 0.93
                       Mean reward: 5175.39
               Mean episode length: 298.79
                 Mean success rate: 54.00
                  Mean reward/step: 18.23
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 2.54s
                        Total time: 3075.30s
                               ETA: 7195.4s

################################################################################
                     [1m Learning iteration 1198/4000 [0m

                       Computation: 3206 steps/s (collection: 0.510s, learning 2.045s)
               Value function loss: 72572.4440
                    Surrogate loss: 0.0172
             Mean action noise std: 0.93
                       Mean reward: 5237.25
               Mean episode length: 297.76
                 Mean success rate: 55.00
                  Mean reward/step: 18.84
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9822208
                    Iteration time: 2.55s
                        Total time: 3077.86s
                               ETA: 7192.8s

################################################################################
                     [1m Learning iteration 1199/4000 [0m

                       Computation: 3184 steps/s (collection: 0.455s, learning 2.117s)
               Value function loss: 45636.6399
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 5229.65
               Mean episode length: 299.77
                 Mean success rate: 54.50
                  Mean reward/step: 19.45
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 2.57s
                        Total time: 3080.43s
                               ETA: 7190.2s

################################################################################
                     [1m Learning iteration 1200/4000 [0m

                       Computation: 3142 steps/s (collection: 0.502s, learning 2.104s)
               Value function loss: 63836.3107
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 5173.78
               Mean episode length: 294.93
                 Mean success rate: 53.00
                  Mean reward/step: 19.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9838592
                    Iteration time: 2.61s
                        Total time: 3083.03s
                               ETA: 7187.8s

################################################################################
                     [1m Learning iteration 1201/4000 [0m

                       Computation: 3188 steps/s (collection: 0.470s, learning 2.099s)
               Value function loss: 68408.4464
                    Surrogate loss: 0.0208
             Mean action noise std: 0.93
                       Mean reward: 5213.05
               Mean episode length: 299.55
                 Mean success rate: 53.50
                  Mean reward/step: 19.49
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 2.57s
                        Total time: 3085.60s
                               ETA: 7185.2s

################################################################################
                     [1m Learning iteration 1202/4000 [0m

                       Computation: 3132 steps/s (collection: 0.492s, learning 2.124s)
               Value function loss: 67218.4214
                    Surrogate loss: 0.0244
             Mean action noise std: 0.93
                       Mean reward: 4824.15
               Mean episode length: 287.49
                 Mean success rate: 49.00
                  Mean reward/step: 19.00
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 9854976
                    Iteration time: 2.62s
                        Total time: 3088.22s
                               ETA: 7182.7s

################################################################################
                     [1m Learning iteration 1203/4000 [0m

                       Computation: 3132 steps/s (collection: 0.488s, learning 2.127s)
               Value function loss: 96992.0171
                    Surrogate loss: 0.0165
             Mean action noise std: 0.93
                       Mean reward: 5336.32
               Mean episode length: 303.51
                 Mean success rate: 53.50
                  Mean reward/step: 17.80
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 2.62s
                        Total time: 3090.83s
                               ETA: 7180.3s

################################################################################
                     [1m Learning iteration 1204/4000 [0m

                       Computation: 3031 steps/s (collection: 0.593s, learning 2.109s)
               Value function loss: 70296.1351
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 5329.77
               Mean episode length: 297.00
                 Mean success rate: 55.00
                  Mean reward/step: 16.71
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 9871360
                    Iteration time: 2.70s
                        Total time: 3093.54s
                               ETA: 7178.0s

################################################################################
                     [1m Learning iteration 1205/4000 [0m

                       Computation: 3148 steps/s (collection: 0.494s, learning 2.108s)
               Value function loss: 79681.8360
                    Surrogate loss: 0.0133
             Mean action noise std: 0.93
                       Mean reward: 5390.90
               Mean episode length: 295.12
                 Mean success rate: 54.50
                  Mean reward/step: 16.53
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 2.60s
                        Total time: 3096.14s
                               ETA: 7175.5s

################################################################################
                     [1m Learning iteration 1206/4000 [0m

                       Computation: 3154 steps/s (collection: 0.452s, learning 2.145s)
               Value function loss: 63275.1151
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 5206.35
               Mean episode length: 286.17
                 Mean success rate: 52.50
                  Mean reward/step: 16.30
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9887744
                    Iteration time: 2.60s
                        Total time: 3098.74s
                               ETA: 7173.0s

################################################################################
                     [1m Learning iteration 1207/4000 [0m

                       Computation: 3073 steps/s (collection: 0.548s, learning 2.117s)
               Value function loss: 74500.9672
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 5229.86
               Mean episode length: 286.63
                 Mean success rate: 54.00
                  Mean reward/step: 16.80
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 2.67s
                        Total time: 3101.40s
                               ETA: 7170.7s

################################################################################
                     [1m Learning iteration 1208/4000 [0m

                       Computation: 3128 steps/s (collection: 0.492s, learning 2.127s)
               Value function loss: 72943.5721
                    Surrogate loss: 0.0167
             Mean action noise std: 0.93
                       Mean reward: 5444.34
               Mean episode length: 292.94
                 Mean success rate: 55.00
                  Mean reward/step: 17.22
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9904128
                    Iteration time: 2.62s
                        Total time: 3104.02s
                               ETA: 7168.3s

################################################################################
                     [1m Learning iteration 1209/4000 [0m

                       Computation: 3116 steps/s (collection: 0.502s, learning 2.126s)
               Value function loss: 69881.4988
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 5618.87
               Mean episode length: 298.12
                 Mean success rate: 57.50
                  Mean reward/step: 17.10
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 2.63s
                        Total time: 3106.65s
                               ETA: 7165.8s

################################################################################
                     [1m Learning iteration 1210/4000 [0m

                       Computation: 3169 steps/s (collection: 0.481s, learning 2.104s)
               Value function loss: 71625.1059
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 5233.79
               Mean episode length: 288.32
                 Mean success rate: 55.00
                  Mean reward/step: 17.28
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9920512
                    Iteration time: 2.58s
                        Total time: 3109.23s
                               ETA: 7163.3s

################################################################################
                     [1m Learning iteration 1211/4000 [0m

                       Computation: 3053 steps/s (collection: 0.534s, learning 2.149s)
               Value function loss: 82536.6558
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 5256.58
               Mean episode length: 291.65
                 Mean success rate: 54.50
                  Mean reward/step: 17.04
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 2.68s
                        Total time: 3111.91s
                               ETA: 7161.0s

################################################################################
                     [1m Learning iteration 1212/4000 [0m

                       Computation: 3150 steps/s (collection: 0.503s, learning 2.097s)
               Value function loss: 68493.2231
                    Surrogate loss: 0.0140
             Mean action noise std: 0.93
                       Mean reward: 5285.22
               Mean episode length: 295.90
                 Mean success rate: 54.50
                  Mean reward/step: 17.33
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9936896
                    Iteration time: 2.60s
                        Total time: 3114.51s
                               ETA: 7158.5s

################################################################################
                     [1m Learning iteration 1213/4000 [0m

                       Computation: 3128 steps/s (collection: 0.485s, learning 2.133s)
               Value function loss: 82068.0879
                    Surrogate loss: 0.0168
             Mean action noise std: 0.93
                       Mean reward: 5386.45
               Mean episode length: 299.00
                 Mean success rate: 55.00
                  Mean reward/step: 17.71
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 2.62s
                        Total time: 3117.13s
                               ETA: 7156.1s

################################################################################
                     [1m Learning iteration 1214/4000 [0m

                       Computation: 3222 steps/s (collection: 0.448s, learning 2.094s)
               Value function loss: 57127.5146
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 5296.29
               Mean episode length: 297.94
                 Mean success rate: 54.50
                  Mean reward/step: 17.87
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9953280
                    Iteration time: 2.54s
                        Total time: 3119.68s
                               ETA: 7153.4s

################################################################################
                     [1m Learning iteration 1215/4000 [0m

                       Computation: 3178 steps/s (collection: 0.510s, learning 2.067s)
               Value function loss: 62336.0363
                    Surrogate loss: 0.0171
             Mean action noise std: 0.93
                       Mean reward: 4861.84
               Mean episode length: 280.42
                 Mean success rate: 50.00
                  Mean reward/step: 18.40
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 2.58s
                        Total time: 3122.25s
                               ETA: 7150.9s

################################################################################
                     [1m Learning iteration 1216/4000 [0m

                       Computation: 3132 steps/s (collection: 0.514s, learning 2.100s)
               Value function loss: 54392.4971
                    Surrogate loss: 0.0152
             Mean action noise std: 0.93
                       Mean reward: 4929.21
               Mean episode length: 285.33
                 Mean success rate: 51.50
                  Mean reward/step: 18.47
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9969664
                    Iteration time: 2.61s
                        Total time: 3124.87s
                               ETA: 7148.4s

################################################################################
                     [1m Learning iteration 1217/4000 [0m

                       Computation: 3220 steps/s (collection: 0.468s, learning 2.076s)
               Value function loss: 47526.8280
                    Surrogate loss: 0.0170
             Mean action noise std: 0.93
                       Mean reward: 5064.56
               Mean episode length: 289.00
                 Mean success rate: 52.50
                  Mean reward/step: 19.23
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 2.54s
                        Total time: 3127.41s
                               ETA: 7145.8s

################################################################################
                     [1m Learning iteration 1218/4000 [0m

                       Computation: 3157 steps/s (collection: 0.452s, learning 2.143s)
               Value function loss: 61745.8316
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 5239.73
               Mean episode length: 299.36
                 Mean success rate: 54.00
                  Mean reward/step: 19.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9986048
                    Iteration time: 2.59s
                        Total time: 3130.01s
                               ETA: 7143.3s

################################################################################
                     [1m Learning iteration 1219/4000 [0m

                       Computation: 3127 steps/s (collection: 0.479s, learning 2.140s)
               Value function loss: 81103.3276
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 5312.34
               Mean episode length: 305.29
                 Mean success rate: 55.50
                  Mean reward/step: 18.55
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 2.62s
                        Total time: 3132.62s
                               ETA: 7140.8s

################################################################################
                     [1m Learning iteration 1220/4000 [0m

                       Computation: 3056 steps/s (collection: 0.518s, learning 2.162s)
               Value function loss: 95224.7839
                    Surrogate loss: 0.0129
             Mean action noise std: 0.93
                       Mean reward: 5427.96
               Mean episode length: 308.68
                 Mean success rate: 55.00
                  Mean reward/step: 17.49
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 10002432
                    Iteration time: 2.68s
                        Total time: 3135.30s
                               ETA: 7138.5s

################################################################################
                     [1m Learning iteration 1221/4000 [0m

                       Computation: 3152 steps/s (collection: 0.507s, learning 2.092s)
               Value function loss: 65643.7157
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 4913.73
               Mean episode length: 290.09
                 Mean success rate: 50.00
                  Mean reward/step: 16.51
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 2.60s
                        Total time: 3137.90s
                               ETA: 7136.0s

################################################################################
                     [1m Learning iteration 1222/4000 [0m

                       Computation: 3212 steps/s (collection: 0.476s, learning 2.074s)
               Value function loss: 71875.2034
                    Surrogate loss: 0.0172
             Mean action noise std: 0.93
                       Mean reward: 5273.76
               Mean episode length: 301.18
                 Mean success rate: 53.00
                  Mean reward/step: 16.08
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10018816
                    Iteration time: 2.55s
                        Total time: 3140.45s
                               ETA: 7133.4s

################################################################################
                     [1m Learning iteration 1223/4000 [0m

                       Computation: 3173 steps/s (collection: 0.490s, learning 2.091s)
               Value function loss: 73966.7908
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 5021.79
               Mean episode length: 284.55
                 Mean success rate: 49.50
                  Mean reward/step: 15.87
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 2.58s
                        Total time: 3143.04s
                               ETA: 7130.9s

################################################################################
                     [1m Learning iteration 1224/4000 [0m

                       Computation: 3197 steps/s (collection: 0.479s, learning 2.083s)
               Value function loss: 71020.6607
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 4589.27
               Mean episode length: 260.20
                 Mean success rate: 43.50
                  Mean reward/step: 15.82
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 10035200
                    Iteration time: 2.56s
                        Total time: 3145.60s
                               ETA: 7128.3s

################################################################################
                     [1m Learning iteration 1225/4000 [0m

                       Computation: 3155 steps/s (collection: 0.479s, learning 2.117s)
               Value function loss: 79648.8703
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 4352.74
               Mean episode length: 249.53
                 Mean success rate: 43.00
                  Mean reward/step: 16.05
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 2.60s
                        Total time: 3148.19s
                               ETA: 7125.8s

################################################################################
                     [1m Learning iteration 1226/4000 [0m

                       Computation: 3165 steps/s (collection: 0.501s, learning 2.087s)
               Value function loss: 104035.5775
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 4691.36
               Mean episode length: 255.28
                 Mean success rate: 46.50
                  Mean reward/step: 15.62
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 10051584
                    Iteration time: 2.59s
                        Total time: 3150.78s
                               ETA: 7123.3s

################################################################################
                     [1m Learning iteration 1227/4000 [0m

                       Computation: 3148 steps/s (collection: 0.481s, learning 2.120s)
               Value function loss: 75015.9655
                    Surrogate loss: 0.0152
             Mean action noise std: 0.93
                       Mean reward: 4960.76
               Mean episode length: 268.33
                 Mean success rate: 49.00
                  Mean reward/step: 14.63
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 2.60s
                        Total time: 3153.38s
                               ETA: 7120.8s

################################################################################
                     [1m Learning iteration 1228/4000 [0m

                       Computation: 3146 steps/s (collection: 0.502s, learning 2.102s)
               Value function loss: 52408.1881
                    Surrogate loss: 0.0168
             Mean action noise std: 0.93
                       Mean reward: 4546.56
               Mean episode length: 261.39
                 Mean success rate: 45.00
                  Mean reward/step: 15.09
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10067968
                    Iteration time: 2.60s
                        Total time: 3155.99s
                               ETA: 7118.3s

################################################################################
                     [1m Learning iteration 1229/4000 [0m

                       Computation: 3166 steps/s (collection: 0.495s, learning 2.091s)
               Value function loss: 47801.6900
                    Surrogate loss: 0.0171
             Mean action noise std: 0.93
                       Mean reward: 4683.44
               Mean episode length: 273.00
                 Mean success rate: 48.00
                  Mean reward/step: 15.96
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 2.59s
                        Total time: 3158.57s
                               ETA: 7115.8s

################################################################################
                     [1m Learning iteration 1230/4000 [0m

                       Computation: 3191 steps/s (collection: 0.481s, learning 2.086s)
               Value function loss: 55343.0134
                    Surrogate loss: 0.0181
             Mean action noise std: 0.93
                       Mean reward: 4560.02
               Mean episode length: 267.86
                 Mean success rate: 47.00
                  Mean reward/step: 17.55
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10084352
                    Iteration time: 2.57s
                        Total time: 3161.14s
                               ETA: 7113.2s

################################################################################
                     [1m Learning iteration 1231/4000 [0m

                       Computation: 3186 steps/s (collection: 0.482s, learning 2.089s)
               Value function loss: 51176.1296
                    Surrogate loss: 0.0164
             Mean action noise std: 0.93
                       Mean reward: 4632.21
               Mean episode length: 274.98
                 Mean success rate: 48.00
                  Mean reward/step: 18.14
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 2.57s
                        Total time: 3163.71s
                               ETA: 7110.6s

################################################################################
                     [1m Learning iteration 1232/4000 [0m

                       Computation: 3148 steps/s (collection: 0.515s, learning 2.087s)
               Value function loss: 41960.3479
                    Surrogate loss: 0.0169
             Mean action noise std: 0.93
                       Mean reward: 4660.05
               Mean episode length: 275.31
                 Mean success rate: 47.00
                  Mean reward/step: 18.59
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10100736
                    Iteration time: 2.60s
                        Total time: 3166.31s
                               ETA: 7108.2s

################################################################################
                     [1m Learning iteration 1233/4000 [0m

                       Computation: 3196 steps/s (collection: 0.479s, learning 2.084s)
               Value function loss: 64771.5089
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 4725.50
               Mean episode length: 280.42
                 Mean success rate: 48.00
                  Mean reward/step: 19.33
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 2.56s
                        Total time: 3168.88s
                               ETA: 7105.6s

################################################################################
                     [1m Learning iteration 1234/4000 [0m

                       Computation: 3197 steps/s (collection: 0.500s, learning 2.062s)
               Value function loss: 67263.5961
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 4667.93
               Mean episode length: 284.07
                 Mean success rate: 48.50
                  Mean reward/step: 19.38
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10117120
                    Iteration time: 2.56s
                        Total time: 3171.44s
                               ETA: 7103.0s

################################################################################
                     [1m Learning iteration 1235/4000 [0m

                       Computation: 3257 steps/s (collection: 0.454s, learning 2.061s)
               Value function loss: 59527.0897
                    Surrogate loss: 0.0189
             Mean action noise std: 0.93
                       Mean reward: 4558.14
               Mean episode length: 281.17
                 Mean success rate: 49.50
                  Mean reward/step: 19.83
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 2.51s
                        Total time: 3173.95s
                               ETA: 7100.3s

################################################################################
                     [1m Learning iteration 1236/4000 [0m

                       Computation: 3217 steps/s (collection: 0.488s, learning 2.058s)
               Value function loss: 85226.7628
                    Surrogate loss: 0.0189
             Mean action noise std: 0.93
                       Mean reward: 4788.47
               Mean episode length: 285.55
                 Mean success rate: 51.50
                  Mean reward/step: 20.08
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10133504
                    Iteration time: 2.55s
                        Total time: 3176.50s
                               ETA: 7097.7s

################################################################################
                     [1m Learning iteration 1237/4000 [0m

                       Computation: 3175 steps/s (collection: 0.485s, learning 2.095s)
               Value function loss: 56028.7909
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 4841.30
               Mean episode length: 285.42
                 Mean success rate: 51.50
                  Mean reward/step: 20.20
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 2.58s
                        Total time: 3179.08s
                               ETA: 7095.1s

################################################################################
                     [1m Learning iteration 1238/4000 [0m

                       Computation: 3208 steps/s (collection: 0.474s, learning 2.079s)
               Value function loss: 88139.4625
                    Surrogate loss: 0.0115
             Mean action noise std: 0.93
                       Mean reward: 5301.98
               Mean episode length: 303.44
                 Mean success rate: 55.50
                  Mean reward/step: 20.35
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10149888
                    Iteration time: 2.55s
                        Total time: 3181.63s
                               ETA: 7092.5s

################################################################################
                     [1m Learning iteration 1239/4000 [0m

                       Computation: 3201 steps/s (collection: 0.464s, learning 2.095s)
               Value function loss: 67341.5794
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 5417.27
               Mean episode length: 308.90
                 Mean success rate: 57.50
                  Mean reward/step: 20.08
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 2.56s
                        Total time: 3184.19s
                               ETA: 7090.0s

################################################################################
                     [1m Learning iteration 1240/4000 [0m

                       Computation: 3188 steps/s (collection: 0.497s, learning 2.073s)
               Value function loss: 99475.9872
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 5892.01
               Mean episode length: 325.38
                 Mean success rate: 61.00
                  Mean reward/step: 20.60
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10166272
                    Iteration time: 2.57s
                        Total time: 3186.76s
                               ETA: 7087.4s

################################################################################
                     [1m Learning iteration 1241/4000 [0m

                       Computation: 3171 steps/s (collection: 0.480s, learning 2.103s)
               Value function loss: 63509.1677
                    Surrogate loss: 0.0159
             Mean action noise std: 0.93
                       Mean reward: 5867.84
               Mean episode length: 322.27
                 Mean success rate: 60.50
                  Mean reward/step: 20.31
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 2.58s
                        Total time: 3189.34s
                               ETA: 7084.9s

################################################################################
                     [1m Learning iteration 1242/4000 [0m

                       Computation: 3233 steps/s (collection: 0.496s, learning 2.038s)
               Value function loss: 102707.9900
                    Surrogate loss: 0.0184
             Mean action noise std: 0.93
                       Mean reward: 6464.47
               Mean episode length: 342.54
                 Mean success rate: 65.00
                  Mean reward/step: 19.42
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10182656
                    Iteration time: 2.53s
                        Total time: 3191.88s
                               ETA: 7082.2s

################################################################################
                     [1m Learning iteration 1243/4000 [0m

                       Computation: 3193 steps/s (collection: 0.478s, learning 2.088s)
               Value function loss: 61881.4438
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 6385.50
               Mean episode length: 335.04
                 Mean success rate: 63.00
                  Mean reward/step: 18.70
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 2.57s
                        Total time: 3194.44s
                               ETA: 7079.6s

################################################################################
                     [1m Learning iteration 1244/4000 [0m

                       Computation: 3127 steps/s (collection: 0.498s, learning 2.122s)
               Value function loss: 78881.9149
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 6621.24
               Mean episode length: 343.56
                 Mean success rate: 64.50
                  Mean reward/step: 18.98
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10199040
                    Iteration time: 2.62s
                        Total time: 3197.06s
                               ETA: 7077.2s

################################################################################
                     [1m Learning iteration 1245/4000 [0m

                       Computation: 3125 steps/s (collection: 0.500s, learning 2.121s)
               Value function loss: 77023.2456
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 6426.51
               Mean episode length: 338.42
                 Mean success rate: 62.50
                  Mean reward/step: 18.82
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 2.62s
                        Total time: 3199.68s
                               ETA: 7074.7s

################################################################################
                     [1m Learning iteration 1246/4000 [0m

                       Computation: 3259 steps/s (collection: 0.464s, learning 2.050s)
               Value function loss: 65239.9600
                    Surrogate loss: 0.0115
             Mean action noise std: 0.93
                       Mean reward: 6351.80
               Mean episode length: 326.78
                 Mean success rate: 59.00
                  Mean reward/step: 18.27
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10215424
                    Iteration time: 2.51s
                        Total time: 3202.20s
                               ETA: 7072.1s

################################################################################
                     [1m Learning iteration 1247/4000 [0m

                       Computation: 3138 steps/s (collection: 0.488s, learning 2.122s)
               Value function loss: 69298.2547
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 5996.55
               Mean episode length: 311.70
                 Mean success rate: 55.50
                  Mean reward/step: 18.13
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 2.61s
                        Total time: 3204.81s
                               ETA: 7069.6s

################################################################################
                     [1m Learning iteration 1248/4000 [0m

                       Computation: 3281 steps/s (collection: 0.446s, learning 2.050s)
               Value function loss: 64356.4661
                    Surrogate loss: 0.0165
             Mean action noise std: 0.93
                       Mean reward: 6099.82
               Mean episode length: 314.52
                 Mean success rate: 56.00
                  Mean reward/step: 18.49
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10231808
                    Iteration time: 2.50s
                        Total time: 3207.30s
                               ETA: 7066.9s

################################################################################
                     [1m Learning iteration 1249/4000 [0m

                       Computation: 3173 steps/s (collection: 0.520s, learning 2.061s)
               Value function loss: 72950.4271
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 5405.75
               Mean episode length: 292.16
                 Mean success rate: 50.00
                  Mean reward/step: 18.82
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 2.58s
                        Total time: 3209.88s
                               ETA: 7064.3s

################################################################################
                     [1m Learning iteration 1250/4000 [0m

                       Computation: 3216 steps/s (collection: 0.494s, learning 2.053s)
               Value function loss: 82536.5552
                    Surrogate loss: 0.0194
             Mean action noise std: 0.93
                       Mean reward: 5532.49
               Mean episode length: 292.37
                 Mean success rate: 50.50
                  Mean reward/step: 18.95
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10248192
                    Iteration time: 2.55s
                        Total time: 3212.43s
                               ETA: 7061.7s

################################################################################
                     [1m Learning iteration 1251/4000 [0m

                       Computation: 3176 steps/s (collection: 0.490s, learning 2.089s)
               Value function loss: 64913.0631
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 5472.24
               Mean episode length: 289.86
                 Mean success rate: 49.50
                  Mean reward/step: 18.46
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 2.58s
                        Total time: 3215.01s
                               ETA: 7059.2s

################################################################################
                     [1m Learning iteration 1252/4000 [0m

                       Computation: 3186 steps/s (collection: 0.481s, learning 2.090s)
               Value function loss: 66172.7925
                    Surrogate loss: 0.0176
             Mean action noise std: 0.93
                       Mean reward: 5382.50
               Mean episode length: 284.78
                 Mean success rate: 49.00
                  Mean reward/step: 18.48
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10264576
                    Iteration time: 2.57s
                        Total time: 3217.58s
                               ETA: 7056.6s

################################################################################
                     [1m Learning iteration 1253/4000 [0m

                       Computation: 3177 steps/s (collection: 0.485s, learning 2.094s)
               Value function loss: 86122.0484
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 5440.70
               Mean episode length: 290.10
                 Mean success rate: 51.00
                  Mean reward/step: 18.29
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 2.58s
                        Total time: 3220.16s
                               ETA: 7054.1s

################################################################################
                     [1m Learning iteration 1254/4000 [0m

                       Computation: 3176 steps/s (collection: 0.489s, learning 2.090s)
               Value function loss: 71548.6244
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 5476.52
               Mean episode length: 294.73
                 Mean success rate: 52.00
                  Mean reward/step: 17.75
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10280960
                    Iteration time: 2.58s
                        Total time: 3222.74s
                               ETA: 7051.5s

################################################################################
                     [1m Learning iteration 1255/4000 [0m

                       Computation: 3233 steps/s (collection: 0.460s, learning 2.074s)
               Value function loss: 67894.0242
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 5528.86
               Mean episode length: 296.27
                 Mean success rate: 52.50
                  Mean reward/step: 17.87
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 2.53s
                        Total time: 3225.27s
                               ETA: 7048.9s

################################################################################
                     [1m Learning iteration 1256/4000 [0m

                       Computation: 3171 steps/s (collection: 0.540s, learning 2.042s)
               Value function loss: 79370.0361
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 5658.32
               Mean episode length: 301.49
                 Mean success rate: 53.50
                  Mean reward/step: 17.94
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 10297344
                    Iteration time: 2.58s
                        Total time: 3227.86s
                               ETA: 7046.3s

################################################################################
                     [1m Learning iteration 1257/4000 [0m

                       Computation: 3222 steps/s (collection: 0.490s, learning 2.052s)
               Value function loss: 80061.8621
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 5459.25
               Mean episode length: 295.79
                 Mean success rate: 52.00
                  Mean reward/step: 17.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 2.54s
                        Total time: 3230.40s
                               ETA: 7043.7s

################################################################################
                     [1m Learning iteration 1258/4000 [0m

                       Computation: 3241 steps/s (collection: 0.437s, learning 2.090s)
               Value function loss: 57396.6316
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 5174.55
               Mean episode length: 290.31
                 Mean success rate: 50.00
                  Mean reward/step: 17.19
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10313728
                    Iteration time: 2.53s
                        Total time: 3232.92s
                               ETA: 7041.0s

################################################################################
                     [1m Learning iteration 1259/4000 [0m

                       Computation: 3264 steps/s (collection: 0.458s, learning 2.051s)
               Value function loss: 62427.6574
                    Surrogate loss: 0.0165
             Mean action noise std: 0.93
                       Mean reward: 5251.94
               Mean episode length: 289.93
                 Mean success rate: 49.50
                  Mean reward/step: 17.77
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 2.51s
                        Total time: 3235.43s
                               ETA: 7038.4s

################################################################################
                     [1m Learning iteration 1260/4000 [0m

                       Computation: 3177 steps/s (collection: 0.524s, learning 2.054s)
               Value function loss: 70791.8061
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 5301.35
               Mean episode length: 289.94
                 Mean success rate: 49.00
                  Mean reward/step: 18.24
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10330112
                    Iteration time: 2.58s
                        Total time: 3238.01s
                               ETA: 7035.8s

################################################################################
                     [1m Learning iteration 1261/4000 [0m

                       Computation: 3277 steps/s (collection: 0.477s, learning 2.023s)
               Value function loss: 79804.1048
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 5130.02
               Mean episode length: 282.88
                 Mean success rate: 46.50
                  Mean reward/step: 18.24
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 2.50s
                        Total time: 3240.51s
                               ETA: 7033.1s

################################################################################
                     [1m Learning iteration 1262/4000 [0m

                       Computation: 3285 steps/s (collection: 0.475s, learning 2.018s)
               Value function loss: 60966.8979
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 5169.96
               Mean episode length: 280.39
                 Mean success rate: 46.50
                  Mean reward/step: 17.72
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10346496
                    Iteration time: 2.49s
                        Total time: 3243.01s
                               ETA: 7030.4s

################################################################################
                     [1m Learning iteration 1263/4000 [0m

                       Computation: 3153 steps/s (collection: 0.534s, learning 2.064s)
               Value function loss: 68764.8101
                    Surrogate loss: 0.0176
             Mean action noise std: 0.93
                       Mean reward: 5292.66
               Mean episode length: 289.36
                 Mean success rate: 47.50
                  Mean reward/step: 18.43
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 2.60s
                        Total time: 3245.60s
                               ETA: 7027.9s

################################################################################
                     [1m Learning iteration 1264/4000 [0m

                       Computation: 3215 steps/s (collection: 0.483s, learning 2.065s)
               Value function loss: 62286.6242
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 4910.63
               Mean episode length: 276.75
                 Mean success rate: 43.50
                  Mean reward/step: 18.95
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10362880
                    Iteration time: 2.55s
                        Total time: 3248.15s
                               ETA: 7025.3s

################################################################################
                     [1m Learning iteration 1265/4000 [0m

                       Computation: 3214 steps/s (collection: 0.490s, learning 2.058s)
               Value function loss: 61703.6614
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 4656.43
               Mean episode length: 268.76
                 Mean success rate: 42.00
                  Mean reward/step: 19.23
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 2.55s
                        Total time: 3250.70s
                               ETA: 7022.6s

################################################################################
                     [1m Learning iteration 1266/4000 [0m

                       Computation: 3210 steps/s (collection: 0.494s, learning 2.058s)
               Value function loss: 82168.1311
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 5096.50
               Mean episode length: 289.72
                 Mean success rate: 47.00
                  Mean reward/step: 19.35
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10379264
                    Iteration time: 2.55s
                        Total time: 3253.25s
                               ETA: 7020.0s

################################################################################
                     [1m Learning iteration 1267/4000 [0m

                       Computation: 3219 steps/s (collection: 0.480s, learning 2.065s)
               Value function loss: 73094.6291
                    Surrogate loss: 0.0177
             Mean action noise std: 0.93
                       Mean reward: 5051.63
               Mean episode length: 289.38
                 Mean success rate: 47.50
                  Mean reward/step: 19.49
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 2.54s
                        Total time: 3255.80s
                               ETA: 7017.4s

################################################################################
                     [1m Learning iteration 1268/4000 [0m

                       Computation: 3237 steps/s (collection: 0.475s, learning 2.055s)
               Value function loss: 93502.4813
                    Surrogate loss: 0.0173
             Mean action noise std: 0.93
                       Mean reward: 5473.48
               Mean episode length: 308.21
                 Mean success rate: 51.00
                  Mean reward/step: 18.93
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10395648
                    Iteration time: 2.53s
                        Total time: 3258.33s
                               ETA: 7014.8s

################################################################################
                     [1m Learning iteration 1269/4000 [0m

                       Computation: 3191 steps/s (collection: 0.516s, learning 2.051s)
               Value function loss: 73284.7751
                    Surrogate loss: 0.0200
             Mean action noise std: 0.93
                       Mean reward: 5872.91
               Mean episode length: 315.69
                 Mean success rate: 53.50
                  Mean reward/step: 18.99
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 2.57s
                        Total time: 3260.89s
                               ETA: 7012.2s

################################################################################
                     [1m Learning iteration 1270/4000 [0m

                       Computation: 3243 steps/s (collection: 0.478s, learning 2.048s)
               Value function loss: 60840.4343
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 5919.15
               Mean episode length: 318.68
                 Mean success rate: 55.00
                  Mean reward/step: 18.93
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10412032
                    Iteration time: 2.53s
                        Total time: 3263.42s
                               ETA: 7009.5s

################################################################################
                     [1m Learning iteration 1271/4000 [0m

                       Computation: 3184 steps/s (collection: 0.480s, learning 2.093s)
               Value function loss: 77831.3739
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 5944.96
               Mean episode length: 316.40
                 Mean success rate: 56.00
                  Mean reward/step: 20.56
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 2.57s
                        Total time: 3265.99s
                               ETA: 7007.0s

################################################################################
                     [1m Learning iteration 1272/4000 [0m

                       Computation: 3229 steps/s (collection: 0.450s, learning 2.087s)
               Value function loss: 91310.0496
                    Surrogate loss: 0.0127
             Mean action noise std: 0.93
                       Mean reward: 6133.99
               Mean episode length: 320.07
                 Mean success rate: 57.50
                  Mean reward/step: 20.57
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 10428416
                    Iteration time: 2.54s
                        Total time: 3268.53s
                               ETA: 7004.4s

################################################################################
                     [1m Learning iteration 1273/4000 [0m

                       Computation: 3246 steps/s (collection: 0.465s, learning 2.058s)
               Value function loss: 70459.7227
                    Surrogate loss: 0.0122
             Mean action noise std: 0.93
                       Mean reward: 6352.30
               Mean episode length: 326.57
                 Mean success rate: 59.00
                  Mean reward/step: 19.89
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 2.52s
                        Total time: 3271.05s
                               ETA: 7001.7s

################################################################################
                     [1m Learning iteration 1274/4000 [0m

                       Computation: 3226 steps/s (collection: 0.477s, learning 2.062s)
               Value function loss: 69711.6085
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 6146.17
               Mean episode length: 307.04
                 Mean success rate: 55.50
                  Mean reward/step: 19.62
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10444800
                    Iteration time: 2.54s
                        Total time: 3273.59s
                               ETA: 6999.1s

################################################################################
                     [1m Learning iteration 1275/4000 [0m

                       Computation: 3129 steps/s (collection: 0.509s, learning 2.108s)
               Value function loss: 72054.7941
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 6317.63
               Mean episode length: 315.94
                 Mean success rate: 56.50
                  Mean reward/step: 20.12
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 2.62s
                        Total time: 3276.21s
                               ETA: 6996.6s

################################################################################
                     [1m Learning iteration 1276/4000 [0m

                       Computation: 3096 steps/s (collection: 0.527s, learning 2.119s)
               Value function loss: 91941.8025
                    Surrogate loss: 0.0152
             Mean action noise std: 0.93
                       Mean reward: 6314.26
               Mean episode length: 315.92
                 Mean success rate: 56.50
                  Mean reward/step: 19.95
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10461184
                    Iteration time: 2.65s
                        Total time: 3278.85s
                               ETA: 6994.2s

################################################################################
                     [1m Learning iteration 1277/4000 [0m

                       Computation: 3180 steps/s (collection: 0.480s, learning 2.095s)
               Value function loss: 85393.8130
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 6215.02
               Mean episode length: 313.58
                 Mean success rate: 56.00
                  Mean reward/step: 19.49
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 2.58s
                        Total time: 3281.43s
                               ETA: 6991.7s

################################################################################
                     [1m Learning iteration 1278/4000 [0m

                       Computation: 3201 steps/s (collection: 0.451s, learning 2.108s)
               Value function loss: 88953.8775
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 6245.48
               Mean episode length: 315.25
                 Mean success rate: 56.00
                  Mean reward/step: 19.11
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10477568
                    Iteration time: 2.56s
                        Total time: 3283.99s
                               ETA: 6989.1s

################################################################################
                     [1m Learning iteration 1279/4000 [0m

                       Computation: 3263 steps/s (collection: 0.437s, learning 2.073s)
               Value function loss: 67197.1196
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 6321.18
               Mean episode length: 316.89
                 Mean success rate: 56.00
                  Mean reward/step: 18.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 2.51s
                        Total time: 3286.50s
                               ETA: 6986.4s

################################################################################
                     [1m Learning iteration 1280/4000 [0m

                       Computation: 3253 steps/s (collection: 0.458s, learning 2.060s)
               Value function loss: 98106.8073
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 6464.85
               Mean episode length: 329.82
                 Mean success rate: 58.50
                  Mean reward/step: 18.48
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10493952
                    Iteration time: 2.52s
                        Total time: 3289.02s
                               ETA: 6983.7s

################################################################################
                     [1m Learning iteration 1281/4000 [0m

                       Computation: 3206 steps/s (collection: 0.503s, learning 2.052s)
               Value function loss: 81949.2536
                    Surrogate loss: 0.0123
             Mean action noise std: 0.93
                       Mean reward: 6559.49
               Mean episode length: 334.04
                 Mean success rate: 59.50
                  Mean reward/step: 18.51
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 2.55s
                        Total time: 3291.57s
                               ETA: 6981.1s

################################################################################
                     [1m Learning iteration 1282/4000 [0m

                       Computation: 3254 steps/s (collection: 0.442s, learning 2.075s)
               Value function loss: 70602.5046
                    Surrogate loss: 0.0140
             Mean action noise std: 0.93
                       Mean reward: 6321.22
               Mean episode length: 331.24
                 Mean success rate: 58.50
                  Mean reward/step: 18.32
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10510336
                    Iteration time: 2.52s
                        Total time: 3294.09s
                               ETA: 6978.4s

################################################################################
                     [1m Learning iteration 1283/4000 [0m

                       Computation: 3184 steps/s (collection: 0.507s, learning 2.065s)
               Value function loss: 67869.2152
                    Surrogate loss: 0.0132
             Mean action noise std: 0.93
                       Mean reward: 6257.31
               Mean episode length: 326.45
                 Mean success rate: 58.00
                  Mean reward/step: 18.93
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 2.57s
                        Total time: 3296.66s
                               ETA: 6975.9s

################################################################################
                     [1m Learning iteration 1284/4000 [0m

                       Computation: 3205 steps/s (collection: 0.484s, learning 2.072s)
               Value function loss: 87794.4082
                    Surrogate loss: 0.0179
             Mean action noise std: 0.93
                       Mean reward: 6458.63
               Mean episode length: 337.54
                 Mean success rate: 60.50
                  Mean reward/step: 18.47
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10526720
                    Iteration time: 2.56s
                        Total time: 3299.22s
                               ETA: 6973.3s

################################################################################
                     [1m Learning iteration 1285/4000 [0m

                       Computation: 3197 steps/s (collection: 0.505s, learning 2.057s)
               Value function loss: 71456.9640
                    Surrogate loss: 0.0186
             Mean action noise std: 0.93
                       Mean reward: 6503.21
               Mean episode length: 336.41
                 Mean success rate: 60.00
                  Mean reward/step: 18.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 2.56s
                        Total time: 3301.78s
                               ETA: 6970.7s

################################################################################
                     [1m Learning iteration 1286/4000 [0m

                       Computation: 3241 steps/s (collection: 0.479s, learning 2.048s)
               Value function loss: 47592.2965
                    Surrogate loss: 0.0124
             Mean action noise std: 0.93
                       Mean reward: 6134.31
               Mean episode length: 324.45
                 Mean success rate: 57.00
                  Mean reward/step: 18.93
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10543104
                    Iteration time: 2.53s
                        Total time: 3304.31s
                               ETA: 6968.1s

################################################################################
                     [1m Learning iteration 1287/4000 [0m

                       Computation: 3209 steps/s (collection: 0.490s, learning 2.062s)
               Value function loss: 83327.5803
                    Surrogate loss: 0.0133
             Mean action noise std: 0.93
                       Mean reward: 6198.08
               Mean episode length: 328.98
                 Mean success rate: 58.50
                  Mean reward/step: 19.40
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 2.55s
                        Total time: 3306.86s
                               ETA: 6965.5s

################################################################################
                     [1m Learning iteration 1288/4000 [0m

                       Computation: 3247 steps/s (collection: 0.490s, learning 2.033s)
               Value function loss: 78704.1068
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 6115.33
               Mean episode length: 323.31
                 Mean success rate: 57.00
                  Mean reward/step: 18.10
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 10559488
                    Iteration time: 2.52s
                        Total time: 3309.38s
                               ETA: 6962.8s

################################################################################
                     [1m Learning iteration 1289/4000 [0m

                       Computation: 3185 steps/s (collection: 0.491s, learning 2.080s)
               Value function loss: 71439.9259
                    Surrogate loss: 0.0121
             Mean action noise std: 0.93
                       Mean reward: 6104.92
               Mean episode length: 322.27
                 Mean success rate: 56.50
                  Mean reward/step: 18.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 2.57s
                        Total time: 3311.95s
                               ETA: 6960.2s

################################################################################
                     [1m Learning iteration 1290/4000 [0m

                       Computation: 3201 steps/s (collection: 0.503s, learning 2.056s)
               Value function loss: 51634.2623
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 6114.71
               Mean episode length: 319.23
                 Mean success rate: 57.00
                  Mean reward/step: 19.39
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10575872
                    Iteration time: 2.56s
                        Total time: 3314.51s
                               ETA: 6957.6s

################################################################################
                     [1m Learning iteration 1291/4000 [0m

                       Computation: 3249 steps/s (collection: 0.511s, learning 2.010s)
               Value function loss: 75459.2336
                    Surrogate loss: 0.0135
             Mean action noise std: 0.93
                       Mean reward: 5986.28
               Mean episode length: 317.13
                 Mean success rate: 55.50
                  Mean reward/step: 19.65
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 2.52s
                        Total time: 3317.03s
                               ETA: 6955.0s

################################################################################
                     [1m Learning iteration 1292/4000 [0m

                       Computation: 3249 steps/s (collection: 0.472s, learning 2.049s)
               Value function loss: 97673.8693
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 6091.13
               Mean episode length: 320.35
                 Mean success rate: 56.50
                  Mean reward/step: 19.10
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10592256
                    Iteration time: 2.52s
                        Total time: 3319.55s
                               ETA: 6952.3s

################################################################################
                     [1m Learning iteration 1293/4000 [0m

                       Computation: 3200 steps/s (collection: 0.483s, learning 2.077s)
               Value function loss: 78263.3303
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 5925.00
               Mean episode length: 318.07
                 Mean success rate: 56.00
                  Mean reward/step: 18.16
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 2.56s
                        Total time: 3322.11s
                               ETA: 6949.7s

################################################################################
                     [1m Learning iteration 1294/4000 [0m

                       Computation: 3166 steps/s (collection: 0.529s, learning 2.058s)
               Value function loss: 74608.5801
                    Surrogate loss: 0.0178
             Mean action noise std: 0.93
                       Mean reward: 6121.33
               Mean episode length: 328.02
                 Mean success rate: 57.50
                  Mean reward/step: 18.60
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10608640
                    Iteration time: 2.59s
                        Total time: 3324.70s
                               ETA: 6947.2s

################################################################################
                     [1m Learning iteration 1295/4000 [0m

                       Computation: 3241 steps/s (collection: 0.439s, learning 2.088s)
               Value function loss: 84586.4104
                    Surrogate loss: 0.0173
             Mean action noise std: 0.93
                       Mean reward: 6105.04
               Mean episode length: 325.25
                 Mean success rate: 56.00
                  Mean reward/step: 19.01
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 2.53s
                        Total time: 3327.23s
                               ETA: 6944.6s

################################################################################
                     [1m Learning iteration 1296/4000 [0m

                       Computation: 3233 steps/s (collection: 0.490s, learning 2.043s)
               Value function loss: 52101.9150
                    Surrogate loss: 0.0160
             Mean action noise std: 0.93
                       Mean reward: 5921.69
               Mean episode length: 318.75
                 Mean success rate: 54.50
                  Mean reward/step: 18.89
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10625024
                    Iteration time: 2.53s
                        Total time: 3329.76s
                               ETA: 6941.9s

################################################################################
                     [1m Learning iteration 1297/4000 [0m

                       Computation: 3233 steps/s (collection: 0.499s, learning 2.035s)
               Value function loss: 75180.8276
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 5855.65
               Mean episode length: 316.98
                 Mean success rate: 54.50
                  Mean reward/step: 19.00
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 2.53s
                        Total time: 3332.29s
                               ETA: 6939.3s

################################################################################
                     [1m Learning iteration 1298/4000 [0m

                       Computation: 3258 steps/s (collection: 0.482s, learning 2.032s)
               Value function loss: 80264.9683
                    Surrogate loss: 0.0175
             Mean action noise std: 0.93
                       Mean reward: 6037.15
               Mean episode length: 324.15
                 Mean success rate: 56.00
                  Mean reward/step: 18.98
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10641408
                    Iteration time: 2.51s
                        Total time: 3334.81s
                               ETA: 6936.6s

################################################################################
                     [1m Learning iteration 1299/4000 [0m

                       Computation: 3256 steps/s (collection: 0.483s, learning 2.032s)
               Value function loss: 74957.4725
                    Surrogate loss: 0.0179
             Mean action noise std: 0.93
                       Mean reward: 5790.41
               Mean episode length: 313.90
                 Mean success rate: 53.50
                  Mean reward/step: 19.26
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 2.52s
                        Total time: 3337.32s
                               ETA: 6933.9s

################################################################################
                     [1m Learning iteration 1300/4000 [0m

                       Computation: 3263 steps/s (collection: 0.465s, learning 2.046s)
               Value function loss: 65913.6135
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 5893.07
               Mean episode length: 315.35
                 Mean success rate: 52.50
                  Mean reward/step: 20.23
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10657792
                    Iteration time: 2.51s
                        Total time: 3339.83s
                               ETA: 6931.2s

################################################################################
                     [1m Learning iteration 1301/4000 [0m

                       Computation: 3260 steps/s (collection: 0.469s, learning 2.044s)
               Value function loss: 62111.4863
                    Surrogate loss: 0.0132
             Mean action noise std: 0.93
                       Mean reward: 5844.12
               Mean episode length: 312.11
                 Mean success rate: 53.00
                  Mean reward/step: 20.58
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 2.51s
                        Total time: 3342.34s
                               ETA: 6928.6s

################################################################################
                     [1m Learning iteration 1302/4000 [0m

                       Computation: 3256 steps/s (collection: 0.463s, learning 2.053s)
               Value function loss: 59020.8288
                    Surrogate loss: 0.0132
             Mean action noise std: 0.93
                       Mean reward: 6098.36
               Mean episode length: 316.19
                 Mean success rate: 54.50
                  Mean reward/step: 21.46
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10674176
                    Iteration time: 2.52s
                        Total time: 3344.86s
                               ETA: 6925.9s

################################################################################
                     [1m Learning iteration 1303/4000 [0m

                       Computation: 3226 steps/s (collection: 0.470s, learning 2.068s)
               Value function loss: 87011.0676
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 6137.56
               Mean episode length: 319.32
                 Mean success rate: 55.00
                  Mean reward/step: 21.23
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 2.54s
                        Total time: 3347.40s
                               ETA: 6923.3s

################################################################################
                     [1m Learning iteration 1304/4000 [0m

                       Computation: 3203 steps/s (collection: 0.468s, learning 2.089s)
               Value function loss: 71786.1883
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 6567.71
               Mean episode length: 335.00
                 Mean success rate: 59.00
                  Mean reward/step: 20.30
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10690560
                    Iteration time: 2.56s
                        Total time: 3349.96s
                               ETA: 6920.7s

################################################################################
                     [1m Learning iteration 1305/4000 [0m

                       Computation: 3255 steps/s (collection: 0.461s, learning 2.055s)
               Value function loss: 63338.0799
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 6529.40
               Mean episode length: 340.61
                 Mean success rate: 59.00
                  Mean reward/step: 20.27
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 2.52s
                        Total time: 3352.47s
                               ETA: 6918.0s

################################################################################
                     [1m Learning iteration 1306/4000 [0m

                       Computation: 3232 steps/s (collection: 0.470s, learning 2.064s)
               Value function loss: 61680.5870
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 6644.90
               Mean episode length: 340.18
                 Mean success rate: 59.00
                  Mean reward/step: 20.62
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10706944
                    Iteration time: 2.53s
                        Total time: 3355.01s
                               ETA: 6915.4s

################################################################################
                     [1m Learning iteration 1307/4000 [0m

                       Computation: 3264 steps/s (collection: 0.465s, learning 2.045s)
               Value function loss: 77767.7239
                    Surrogate loss: 0.0167
             Mean action noise std: 0.93
                       Mean reward: 6950.84
               Mean episode length: 348.29
                 Mean success rate: 60.50
                  Mean reward/step: 21.12
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 2.51s
                        Total time: 3357.52s
                               ETA: 6912.7s

################################################################################
                     [1m Learning iteration 1308/4000 [0m

                       Computation: 3163 steps/s (collection: 0.443s, learning 2.147s)
               Value function loss: 75979.4742
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 6713.46
               Mean episode length: 338.37
                 Mean success rate: 58.50
                  Mean reward/step: 21.05
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10723328
                    Iteration time: 2.59s
                        Total time: 3360.11s
                               ETA: 6910.2s

################################################################################
                     [1m Learning iteration 1309/4000 [0m

                       Computation: 3173 steps/s (collection: 0.498s, learning 2.084s)
               Value function loss: 101060.1626
                    Surrogate loss: 0.0088
             Mean action noise std: 0.93
                       Mean reward: 6873.29
               Mean episode length: 342.50
                 Mean success rate: 61.50
                  Mean reward/step: 20.61
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 2.58s
                        Total time: 3362.69s
                               ETA: 6907.6s

################################################################################
                     [1m Learning iteration 1310/4000 [0m

                       Computation: 3204 steps/s (collection: 0.471s, learning 2.085s)
               Value function loss: 108322.9190
                    Surrogate loss: 0.0113
             Mean action noise std: 0.93
                       Mean reward: 7266.36
               Mean episode length: 356.77
                 Mean success rate: 64.50
                  Mean reward/step: 19.57
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10739712
                    Iteration time: 2.56s
                        Total time: 3365.24s
                               ETA: 6905.0s

################################################################################
                     [1m Learning iteration 1311/4000 [0m

                       Computation: 3080 steps/s (collection: 0.515s, learning 2.145s)
               Value function loss: 111816.9266
                    Surrogate loss: 0.0125
             Mean action noise std: 0.93
                       Mean reward: 7360.28
               Mean episode length: 360.03
                 Mean success rate: 66.00
                  Mean reward/step: 19.14
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 2.66s
                        Total time: 3367.90s
                               ETA: 6902.7s

################################################################################
                     [1m Learning iteration 1312/4000 [0m

                       Computation: 3221 steps/s (collection: 0.452s, learning 2.091s)
               Value function loss: 75612.7015
                    Surrogate loss: 0.0195
             Mean action noise std: 0.93
                       Mean reward: 7119.91
               Mean episode length: 351.48
                 Mean success rate: 64.50
                  Mean reward/step: 19.07
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10756096
                    Iteration time: 2.54s
                        Total time: 3370.44s
                               ETA: 6900.0s

################################################################################
                     [1m Learning iteration 1313/4000 [0m

                       Computation: 3309 steps/s (collection: 0.431s, learning 2.044s)
               Value function loss: 98449.6029
                    Surrogate loss: 0.0126
             Mean action noise std: 0.93
                       Mean reward: 7305.51
               Mean episode length: 352.14
                 Mean success rate: 64.00
                  Mean reward/step: 19.46
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 2.47s
                        Total time: 3372.92s
                               ETA: 6897.3s

################################################################################
                     [1m Learning iteration 1314/4000 [0m

                       Computation: 3215 steps/s (collection: 0.471s, learning 2.077s)
               Value function loss: 97245.1583
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 7605.89
               Mean episode length: 361.50
                 Mean success rate: 67.00
                  Mean reward/step: 18.54
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10772480
                    Iteration time: 2.55s
                        Total time: 3375.47s
                               ETA: 6894.7s

################################################################################
                     [1m Learning iteration 1315/4000 [0m

                       Computation: 3137 steps/s (collection: 0.499s, learning 2.112s)
               Value function loss: 80117.2536
                    Surrogate loss: 0.0175
             Mean action noise std: 0.93
                       Mean reward: 7133.87
               Mean episode length: 348.63
                 Mean success rate: 63.00
                  Mean reward/step: 17.92
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 2.61s
                        Total time: 3378.08s
                               ETA: 6892.2s

################################################################################
                     [1m Learning iteration 1316/4000 [0m

                       Computation: 3171 steps/s (collection: 0.449s, learning 2.134s)
               Value function loss: 48350.1860
                    Surrogate loss: 0.0169
             Mean action noise std: 0.93
                       Mean reward: 6455.23
               Mean episode length: 325.50
                 Mean success rate: 56.50
                  Mean reward/step: 18.43
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10788864
                    Iteration time: 2.58s
                        Total time: 3380.66s
                               ETA: 6889.7s

################################################################################
                     [1m Learning iteration 1317/4000 [0m

                       Computation: 3178 steps/s (collection: 0.490s, learning 2.087s)
               Value function loss: 59934.7421
                    Surrogate loss: 0.0173
             Mean action noise std: 0.93
                       Mean reward: 6257.59
               Mean episode length: 323.93
                 Mean success rate: 55.50
                  Mean reward/step: 18.80
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 2.58s
                        Total time: 3383.24s
                               ETA: 6887.1s

################################################################################
                     [1m Learning iteration 1318/4000 [0m

                       Computation: 3171 steps/s (collection: 0.465s, learning 2.118s)
               Value function loss: 61119.6650
                    Surrogate loss: 0.0162
             Mean action noise std: 0.93
                       Mean reward: 5879.79
               Mean episode length: 310.17
                 Mean success rate: 52.00
                  Mean reward/step: 19.50
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10805248
                    Iteration time: 2.58s
                        Total time: 3385.82s
                               ETA: 6884.6s

################################################################################
                     [1m Learning iteration 1319/4000 [0m

                       Computation: 3184 steps/s (collection: 0.476s, learning 2.096s)
               Value function loss: 79056.9358
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 5639.64
               Mean episode length: 301.08
                 Mean success rate: 49.50
                  Mean reward/step: 19.91
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 2.57s
                        Total time: 3388.39s
                               ETA: 6882.0s

################################################################################
                     [1m Learning iteration 1320/4000 [0m

                       Computation: 3194 steps/s (collection: 0.453s, learning 2.112s)
               Value function loss: 71179.1318
                    Surrogate loss: 0.0166
             Mean action noise std: 0.93
                       Mean reward: 5596.95
               Mean episode length: 302.88
                 Mean success rate: 49.50
                  Mean reward/step: 19.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10821632
                    Iteration time: 2.56s
                        Total time: 3390.96s
                               ETA: 6879.5s

################################################################################
                     [1m Learning iteration 1321/4000 [0m

                       Computation: 3138 steps/s (collection: 0.504s, learning 2.106s)
               Value function loss: 90613.0809
                    Surrogate loss: 0.0166
             Mean action noise std: 0.93
                       Mean reward: 5086.67
               Mean episode length: 289.69
                 Mean success rate: 46.00
                  Mean reward/step: 18.89
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 2.61s
                        Total time: 3393.57s
                               ETA: 6877.0s

################################################################################
                     [1m Learning iteration 1322/4000 [0m

                       Computation: 3111 steps/s (collection: 0.480s, learning 2.153s)
               Value function loss: 55307.5011
                    Surrogate loss: 0.0171
             Mean action noise std: 0.93
                       Mean reward: 5080.22
               Mean episode length: 283.64
                 Mean success rate: 45.50
                  Mean reward/step: 18.84
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10838016
                    Iteration time: 2.63s
                        Total time: 3396.20s
                               ETA: 6874.5s

################################################################################
                     [1m Learning iteration 1323/4000 [0m

                       Computation: 3179 steps/s (collection: 0.466s, learning 2.111s)
               Value function loss: 71848.4078
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 5198.83
               Mean episode length: 283.82
                 Mean success rate: 47.00
                  Mean reward/step: 19.31
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 2.58s
                        Total time: 3398.78s
                               ETA: 6872.0s

################################################################################
                     [1m Learning iteration 1324/4000 [0m

                       Computation: 3105 steps/s (collection: 0.512s, learning 2.126s)
               Value function loss: 66384.2455
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 5692.85
               Mean episode length: 300.77
                 Mean success rate: 51.00
                  Mean reward/step: 18.86
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10854400
                    Iteration time: 2.64s
                        Total time: 3401.42s
                               ETA: 6869.6s

################################################################################
                     [1m Learning iteration 1325/4000 [0m

                       Computation: 3059 steps/s (collection: 0.538s, learning 2.140s)
               Value function loss: 78663.0833
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 5746.14
               Mean episode length: 297.26
                 Mean success rate: 50.50
                  Mean reward/step: 18.22
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 2.68s
                        Total time: 3404.09s
                               ETA: 6867.2s

################################################################################
                     [1m Learning iteration 1326/4000 [0m

                       Computation: 3204 steps/s (collection: 0.473s, learning 2.084s)
               Value function loss: 71069.0700
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 5921.43
               Mean episode length: 307.40
                 Mean success rate: 53.50
                  Mean reward/step: 17.42
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10870784
                    Iteration time: 2.56s
                        Total time: 3406.65s
                               ETA: 6864.6s

################################################################################
                     [1m Learning iteration 1327/4000 [0m

                       Computation: 3191 steps/s (collection: 0.459s, learning 2.107s)
               Value function loss: 68278.0756
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 5912.84
               Mean episode length: 303.55
                 Mean success rate: 53.50
                  Mean reward/step: 17.25
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 2.57s
                        Total time: 3409.22s
                               ETA: 6862.1s

################################################################################
                     [1m Learning iteration 1328/4000 [0m

                       Computation: 3253 steps/s (collection: 0.466s, learning 2.051s)
               Value function loss: 65430.8576
                    Surrogate loss: 0.0177
             Mean action noise std: 0.93
                       Mean reward: 5899.72
               Mean episode length: 303.69
                 Mean success rate: 53.50
                  Mean reward/step: 17.77
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10887168
                    Iteration time: 2.52s
                        Total time: 3411.73s
                               ETA: 6859.4s

################################################################################
                     [1m Learning iteration 1329/4000 [0m

                       Computation: 3297 steps/s (collection: 0.437s, learning 2.047s)
               Value function loss: 86015.9008
                    Surrogate loss: 0.0167
             Mean action noise std: 0.93
                       Mean reward: 6244.42
               Mean episode length: 315.88
                 Mean success rate: 57.00
                  Mean reward/step: 17.63
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 2.48s
                        Total time: 3414.22s
                               ETA: 6856.7s

################################################################################
                     [1m Learning iteration 1330/4000 [0m

                       Computation: 3288 steps/s (collection: 0.434s, learning 2.057s)
               Value function loss: 95825.6539
                    Surrogate loss: 0.0125
             Mean action noise std: 0.93
                       Mean reward: 6218.69
               Mean episode length: 323.89
                 Mean success rate: 57.50
                  Mean reward/step: 17.53
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10903552
                    Iteration time: 2.49s
                        Total time: 3416.71s
                               ETA: 6854.0s

################################################################################
                     [1m Learning iteration 1331/4000 [0m

                       Computation: 3350 steps/s (collection: 0.432s, learning 2.012s)
               Value function loss: 57034.1574
                    Surrogate loss: 0.0203
             Mean action noise std: 0.93
                       Mean reward: 6257.39
               Mean episode length: 326.58
                 Mean success rate: 57.00
                  Mean reward/step: 18.16
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 2.44s
                        Total time: 3419.15s
                               ETA: 6851.1s

################################################################################
                     [1m Learning iteration 1332/4000 [0m

                       Computation: 3288 steps/s (collection: 0.445s, learning 2.046s)
               Value function loss: 100372.9252
                    Surrogate loss: 0.0129
             Mean action noise std: 0.93
                       Mean reward: 6719.75
               Mean episode length: 340.23
                 Mean success rate: 61.50
                  Mean reward/step: 18.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10919936
                    Iteration time: 2.49s
                        Total time: 3421.64s
                               ETA: 6848.4s

################################################################################
                     [1m Learning iteration 1333/4000 [0m

                       Computation: 3284 steps/s (collection: 0.455s, learning 2.040s)
               Value function loss: 55890.2777
                    Surrogate loss: 0.0169
             Mean action noise std: 0.93
                       Mean reward: 6685.44
               Mean episode length: 342.25
                 Mean success rate: 62.00
                  Mean reward/step: 19.31
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 2.49s
                        Total time: 3424.14s
                               ETA: 6845.7s

################################################################################
                     [1m Learning iteration 1334/4000 [0m

                       Computation: 3315 steps/s (collection: 0.427s, learning 2.045s)
               Value function loss: 71828.5309
                    Surrogate loss: 0.0189
             Mean action noise std: 0.93
                       Mean reward: 6824.10
               Mean episode length: 345.92
                 Mean success rate: 62.50
                  Mean reward/step: 19.49
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10936320
                    Iteration time: 2.47s
                        Total time: 3426.61s
                               ETA: 6843.0s

################################################################################
                     [1m Learning iteration 1335/4000 [0m

                       Computation: 3272 steps/s (collection: 0.459s, learning 2.044s)
               Value function loss: 61755.0905
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 6360.38
               Mean episode length: 338.64
                 Mean success rate: 59.50
                  Mean reward/step: 20.10
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 2.50s
                        Total time: 3429.11s
                               ETA: 6840.3s

################################################################################
                     [1m Learning iteration 1336/4000 [0m

                       Computation: 3191 steps/s (collection: 0.480s, learning 2.087s)
               Value function loss: 87643.2934
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 6460.33
               Mean episode length: 344.56
                 Mean success rate: 60.00
                  Mean reward/step: 20.45
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10952704
                    Iteration time: 2.57s
                        Total time: 3431.68s
                               ETA: 6837.7s

################################################################################
                     [1m Learning iteration 1337/4000 [0m

                       Computation: 3220 steps/s (collection: 0.470s, learning 2.074s)
               Value function loss: 45942.8301
                    Surrogate loss: 0.0171
             Mean action noise std: 0.93
                       Mean reward: 6148.47
               Mean episode length: 332.08
                 Mean success rate: 57.50
                  Mean reward/step: 19.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 2.54s
                        Total time: 3434.22s
                               ETA: 6835.1s

################################################################################
                     [1m Learning iteration 1338/4000 [0m

                       Computation: 3297 steps/s (collection: 0.437s, learning 2.048s)
               Value function loss: 83294.9251
                    Surrogate loss: 0.0164
             Mean action noise std: 0.93
                       Mean reward: 6461.75
               Mean episode length: 346.36
                 Mean success rate: 60.00
                  Mean reward/step: 20.03
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10969088
                    Iteration time: 2.48s
                        Total time: 3436.71s
                               ETA: 6832.4s

################################################################################
                     [1m Learning iteration 1339/4000 [0m

                       Computation: 3328 steps/s (collection: 0.413s, learning 2.048s)
               Value function loss: 59245.8004
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 6405.41
               Mean episode length: 341.93
                 Mean success rate: 60.00
                  Mean reward/step: 20.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 2.46s
                        Total time: 3439.17s
                               ETA: 6829.6s

################################################################################
                     [1m Learning iteration 1340/4000 [0m

                       Computation: 3100 steps/s (collection: 0.536s, learning 2.106s)
               Value function loss: 93460.3412
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 6164.09
               Mean episode length: 333.39
                 Mean success rate: 58.00
                  Mean reward/step: 21.21
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10985472
                    Iteration time: 2.64s
                        Total time: 3441.81s
                               ETA: 6827.2s

################################################################################
                     [1m Learning iteration 1341/4000 [0m

                       Computation: 3106 steps/s (collection: 0.517s, learning 2.121s)
               Value function loss: 61720.8386
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 5936.85
               Mean episode length: 329.41
                 Mean success rate: 56.50
                  Mean reward/step: 20.57
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 2.64s
                        Total time: 3444.45s
                               ETA: 6824.7s

################################################################################
                     [1m Learning iteration 1342/4000 [0m

                       Computation: 3189 steps/s (collection: 0.473s, learning 2.096s)
               Value function loss: 66234.6275
                    Surrogate loss: 0.0175
             Mean action noise std: 0.93
                       Mean reward: 5890.39
               Mean episode length: 329.94
                 Mean success rate: 56.50
                  Mean reward/step: 21.08
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11001856
                    Iteration time: 2.57s
                        Total time: 3447.02s
                               ETA: 6822.2s

################################################################################
                     [1m Learning iteration 1343/4000 [0m

                       Computation: 3165 steps/s (collection: 0.480s, learning 2.107s)
               Value function loss: 78587.0625
                    Surrogate loss: 0.0135
             Mean action noise std: 0.93
                       Mean reward: 6115.09
               Mean episode length: 331.01
                 Mean success rate: 57.00
                  Mean reward/step: 21.01
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 2.59s
                        Total time: 3449.61s
                               ETA: 6819.6s

################################################################################
                     [1m Learning iteration 1344/4000 [0m

                       Computation: 3141 steps/s (collection: 0.512s, learning 2.096s)
               Value function loss: 117892.2077
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 6864.41
               Mean episode length: 352.80
                 Mean success rate: 63.00
                  Mean reward/step: 20.56
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11018240
                    Iteration time: 2.61s
                        Total time: 3452.21s
                               ETA: 6817.2s

################################################################################
                     [1m Learning iteration 1345/4000 [0m

                       Computation: 3112 steps/s (collection: 0.491s, learning 2.141s)
               Value function loss: 103302.5896
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 6948.36
               Mean episode length: 347.01
                 Mean success rate: 64.00
                  Mean reward/step: 19.99
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 2.63s
                        Total time: 3454.85s
                               ETA: 6814.7s

################################################################################
                     [1m Learning iteration 1346/4000 [0m

                       Computation: 3127 steps/s (collection: 0.488s, learning 2.131s)
               Value function loss: 68598.3852
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 6886.20
               Mean episode length: 340.11
                 Mean success rate: 62.50
                  Mean reward/step: 19.69
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11034624
                    Iteration time: 2.62s
                        Total time: 3457.46s
                               ETA: 6812.3s

################################################################################
                     [1m Learning iteration 1347/4000 [0m

                       Computation: 3187 steps/s (collection: 0.465s, learning 2.105s)
               Value function loss: 66050.1397
                    Surrogate loss: 0.0161
             Mean action noise std: 0.93
                       Mean reward: 7043.44
               Mean episode length: 349.96
                 Mean success rate: 63.50
                  Mean reward/step: 20.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 2.57s
                        Total time: 3460.03s
                               ETA: 6809.7s

################################################################################
                     [1m Learning iteration 1348/4000 [0m

                       Computation: 3146 steps/s (collection: 0.479s, learning 2.125s)
               Value function loss: 79145.3392
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 7171.00
               Mean episode length: 353.40
                 Mean success rate: 64.50
                  Mean reward/step: 20.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11051008
                    Iteration time: 2.60s
                        Total time: 3462.64s
                               ETA: 6807.2s

################################################################################
                     [1m Learning iteration 1349/4000 [0m

                       Computation: 3151 steps/s (collection: 0.487s, learning 2.112s)
               Value function loss: 83409.8294
                    Surrogate loss: 0.0178
             Mean action noise std: 0.93
                       Mean reward: 6928.04
               Mean episode length: 345.60
                 Mean success rate: 63.00
                  Mean reward/step: 20.50
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 2.60s
                        Total time: 3465.24s
                               ETA: 6804.7s

################################################################################
                     [1m Learning iteration 1350/4000 [0m

                       Computation: 3219 steps/s (collection: 0.461s, learning 2.083s)
               Value function loss: 77384.9487
                    Surrogate loss: 0.0166
             Mean action noise std: 0.93
                       Mean reward: 7214.07
               Mean episode length: 352.84
                 Mean success rate: 65.50
                  Mean reward/step: 19.90
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11067392
                    Iteration time: 2.54s
                        Total time: 3467.78s
                               ETA: 6802.1s

################################################################################
                     [1m Learning iteration 1351/4000 [0m

                       Computation: 3127 steps/s (collection: 0.489s, learning 2.130s)
               Value function loss: 98033.0786
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 7126.99
               Mean episode length: 345.18
                 Mean success rate: 65.00
                  Mean reward/step: 19.77
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 2.62s
                        Total time: 3470.40s
                               ETA: 6799.6s

################################################################################
                     [1m Learning iteration 1352/4000 [0m

                       Computation: 3250 steps/s (collection: 0.475s, learning 2.045s)
               Value function loss: 91185.0145
                    Surrogate loss: 0.0160
             Mean action noise std: 0.93
                       Mean reward: 6635.77
               Mean episode length: 327.98
                 Mean success rate: 60.50
                  Mean reward/step: 19.16
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 11083776
                    Iteration time: 2.52s
                        Total time: 3472.92s
                               ETA: 6797.0s

################################################################################
                     [1m Learning iteration 1353/4000 [0m

                       Computation: 3234 steps/s (collection: 0.444s, learning 2.089s)
               Value function loss: 69258.5797
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 6319.56
               Mean episode length: 319.73
                 Mean success rate: 57.50
                  Mean reward/step: 19.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 2.53s
                        Total time: 3475.45s
                               ETA: 6794.3s

################################################################################
                     [1m Learning iteration 1354/4000 [0m

                       Computation: 3073 steps/s (collection: 0.525s, learning 2.141s)
               Value function loss: 69531.8310
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 6217.59
               Mean episode length: 313.43
                 Mean success rate: 56.00
                  Mean reward/step: 20.40
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11100160
                    Iteration time: 2.67s
                        Total time: 3478.12s
                               ETA: 6792.0s

################################################################################
                     [1m Learning iteration 1355/4000 [0m

                       Computation: 3147 steps/s (collection: 0.484s, learning 2.118s)
               Value function loss: 78276.5456
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 6449.68
               Mean episode length: 319.26
                 Mean success rate: 58.50
                  Mean reward/step: 20.88
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 2.60s
                        Total time: 3480.72s
                               ETA: 6789.5s

################################################################################
                     [1m Learning iteration 1356/4000 [0m

                       Computation: 3216 steps/s (collection: 0.449s, learning 2.097s)
               Value function loss: 109889.4989
                    Surrogate loss: 0.0152
             Mean action noise std: 0.93
                       Mean reward: 6576.40
               Mean episode length: 321.44
                 Mean success rate: 59.00
                  Mean reward/step: 20.67
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11116544
                    Iteration time: 2.55s
                        Total time: 3483.27s
                               ETA: 6786.9s

################################################################################
                     [1m Learning iteration 1357/4000 [0m

                       Computation: 3148 steps/s (collection: 0.494s, learning 2.108s)
               Value function loss: 63644.1252
                    Surrogate loss: 0.0189
             Mean action noise std: 0.93
                       Mean reward: 6766.08
               Mean episode length: 329.26
                 Mean success rate: 60.50
                  Mean reward/step: 20.60
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 2.60s
                        Total time: 3485.87s
                               ETA: 6784.4s

################################################################################
                     [1m Learning iteration 1358/4000 [0m

                       Computation: 2973 steps/s (collection: 0.567s, learning 2.188s)
               Value function loss: 93285.6355
                    Surrogate loss: 0.0167
             Mean action noise std: 0.93
                       Mean reward: 6765.05
               Mean episode length: 325.95
                 Mean success rate: 59.00
                  Mean reward/step: 20.72
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11132928
                    Iteration time: 2.75s
                        Total time: 3488.62s
                               ETA: 6782.2s

################################################################################
                     [1m Learning iteration 1359/4000 [0m

                       Computation: 3106 steps/s (collection: 0.535s, learning 2.102s)
               Value function loss: 98997.4492
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 6605.64
               Mean episode length: 321.56
                 Mean success rate: 58.50
                  Mean reward/step: 20.73
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 2.64s
                        Total time: 3491.26s
                               ETA: 6779.7s

################################################################################
                     [1m Learning iteration 1360/4000 [0m

                       Computation: 3134 steps/s (collection: 0.483s, learning 2.131s)
               Value function loss: 87110.1776
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 6491.73
               Mean episode length: 322.00
                 Mean success rate: 58.00
                  Mean reward/step: 20.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11149312
                    Iteration time: 2.61s
                        Total time: 3493.88s
                               ETA: 6777.2s

################################################################################
                     [1m Learning iteration 1361/4000 [0m

                       Computation: 3203 steps/s (collection: 0.468s, learning 2.089s)
               Value function loss: 110054.1990
                    Surrogate loss: 0.0166
             Mean action noise std: 0.93
                       Mean reward: 7274.87
               Mean episode length: 344.86
                 Mean success rate: 64.50
                  Mean reward/step: 20.44
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 2.56s
                        Total time: 3496.43s
                               ETA: 6774.7s

################################################################################
                     [1m Learning iteration 1362/4000 [0m

                       Computation: 3039 steps/s (collection: 0.552s, learning 2.144s)
               Value function loss: 66805.2803
                    Surrogate loss: 0.0234
             Mean action noise std: 0.93
                       Mean reward: 7433.22
               Mean episode length: 351.74
                 Mean success rate: 67.50
                  Mean reward/step: 20.08
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11165696
                    Iteration time: 2.70s
                        Total time: 3499.13s
                               ETA: 6772.3s

################################################################################
                     [1m Learning iteration 1363/4000 [0m

                       Computation: 3165 steps/s (collection: 0.469s, learning 2.119s)
               Value function loss: 92269.1497
                    Surrogate loss: 0.0173
             Mean action noise std: 0.93
                       Mean reward: 7396.03
               Mean episode length: 351.99
                 Mean success rate: 67.50
                  Mean reward/step: 19.88
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 2.59s
                        Total time: 3501.72s
                               ETA: 6769.8s

################################################################################
                     [1m Learning iteration 1364/4000 [0m

                       Computation: 3176 steps/s (collection: 0.483s, learning 2.096s)
               Value function loss: 79327.7650
                    Surrogate loss: 0.0196
             Mean action noise std: 0.93
                       Mean reward: 6973.76
               Mean episode length: 339.10
                 Mean success rate: 64.00
                  Mean reward/step: 20.02
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11182080
                    Iteration time: 2.58s
                        Total time: 3504.29s
                               ETA: 6767.3s

################################################################################
                     [1m Learning iteration 1365/4000 [0m

                       Computation: 3193 steps/s (collection: 0.485s, learning 2.080s)
               Value function loss: 83921.1444
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 7043.30
               Mean episode length: 340.70
                 Mean success rate: 64.50
                  Mean reward/step: 19.91
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 2.57s
                        Total time: 3506.86s
                               ETA: 6764.7s

################################################################################
                     [1m Learning iteration 1366/4000 [0m

                       Computation: 3158 steps/s (collection: 0.492s, learning 2.102s)
               Value function loss: 67212.7993
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 6742.99
               Mean episode length: 333.76
                 Mean success rate: 63.50
                  Mean reward/step: 20.19
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11198464
                    Iteration time: 2.59s
                        Total time: 3509.45s
                               ETA: 6762.2s

################################################################################
                     [1m Learning iteration 1367/4000 [0m

                       Computation: 3139 steps/s (collection: 0.483s, learning 2.126s)
               Value function loss: 80855.3673
                    Surrogate loss: 0.0180
             Mean action noise std: 0.93
                       Mean reward: 7184.78
               Mean episode length: 350.30
                 Mean success rate: 67.00
                  Mean reward/step: 20.14
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 2.61s
                        Total time: 3512.06s
                               ETA: 6759.7s

################################################################################
                     [1m Learning iteration 1368/4000 [0m

                       Computation: 3137 steps/s (collection: 0.487s, learning 2.124s)
               Value function loss: 68234.7620
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 7214.78
               Mean episode length: 352.90
                 Mean success rate: 66.00
                  Mean reward/step: 19.82
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11214848
                    Iteration time: 2.61s
                        Total time: 3514.67s
                               ETA: 6757.2s

################################################################################
                     [1m Learning iteration 1369/4000 [0m

                       Computation: 3140 steps/s (collection: 0.503s, learning 2.106s)
               Value function loss: 84717.9867
                    Surrogate loss: 0.0201
             Mean action noise std: 0.93
                       Mean reward: 6099.12
               Mean episode length: 316.71
                 Mean success rate: 57.00
                  Mean reward/step: 19.66
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 2.61s
                        Total time: 3517.28s
                               ETA: 6754.7s

################################################################################
                     [1m Learning iteration 1370/4000 [0m

                       Computation: 3172 steps/s (collection: 0.492s, learning 2.090s)
               Value function loss: 64175.0479
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 5997.50
               Mean episode length: 310.43
                 Mean success rate: 55.00
                  Mean reward/step: 20.01
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11231232
                    Iteration time: 2.58s
                        Total time: 3519.86s
                               ETA: 6752.2s

################################################################################
                     [1m Learning iteration 1371/4000 [0m

                       Computation: 3163 steps/s (collection: 0.477s, learning 2.112s)
               Value function loss: 65450.8151
                    Surrogate loss: 0.0207
             Mean action noise std: 0.93
                       Mean reward: 5904.25
               Mean episode length: 307.06
                 Mean success rate: 54.00
                  Mean reward/step: 20.38
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 2.59s
                        Total time: 3522.45s
                               ETA: 6749.7s

################################################################################
                     [1m Learning iteration 1372/4000 [0m

                       Computation: 3184 steps/s (collection: 0.467s, learning 2.105s)
               Value function loss: 96838.7052
                    Surrogate loss: 0.0173
             Mean action noise std: 0.93
                       Mean reward: 5866.09
               Mean episode length: 303.82
                 Mean success rate: 56.00
                  Mean reward/step: 19.91
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 11247616
                    Iteration time: 2.57s
                        Total time: 3525.02s
                               ETA: 6747.1s

################################################################################
                     [1m Learning iteration 1373/4000 [0m

                       Computation: 3171 steps/s (collection: 0.471s, learning 2.112s)
               Value function loss: 56467.8595
                    Surrogate loss: 0.0164
             Mean action noise std: 0.93
                       Mean reward: 5850.05
               Mean episode length: 304.00
                 Mean success rate: 54.50
                  Mean reward/step: 19.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 2.58s
                        Total time: 3527.61s
                               ETA: 6744.6s

################################################################################
                     [1m Learning iteration 1374/4000 [0m

                       Computation: 3169 steps/s (collection: 0.513s, learning 2.072s)
               Value function loss: 104537.8223
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 5671.99
               Mean episode length: 296.46
                 Mean success rate: 52.50
                  Mean reward/step: 20.50
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 11264000
                    Iteration time: 2.58s
                        Total time: 3530.19s
                               ETA: 6742.0s

################################################################################
                     [1m Learning iteration 1375/4000 [0m

                       Computation: 3153 steps/s (collection: 0.480s, learning 2.118s)
               Value function loss: 88269.7902
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 5629.60
               Mean episode length: 292.29
                 Mean success rate: 52.00
                  Mean reward/step: 19.50
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 2.60s
                        Total time: 3532.79s
                               ETA: 6739.5s

################################################################################
                     [1m Learning iteration 1376/4000 [0m

                       Computation: 3172 steps/s (collection: 0.489s, learning 2.093s)
               Value function loss: 74945.2780
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 6024.22
               Mean episode length: 304.44
                 Mean success rate: 55.00
                  Mean reward/step: 19.25
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11280384
                    Iteration time: 2.58s
                        Total time: 3535.37s
                               ETA: 6737.0s

################################################################################
                     [1m Learning iteration 1377/4000 [0m

                       Computation: 3206 steps/s (collection: 0.493s, learning 2.062s)
               Value function loss: 65063.1104
                    Surrogate loss: 0.0181
             Mean action noise std: 0.93
                       Mean reward: 6151.77
               Mean episode length: 308.86
                 Mean success rate: 55.50
                  Mean reward/step: 19.80
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 2.55s
                        Total time: 3537.93s
                               ETA: 6734.4s

################################################################################
                     [1m Learning iteration 1378/4000 [0m

                       Computation: 3211 steps/s (collection: 0.488s, learning 2.063s)
               Value function loss: 80793.7976
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 6353.14
               Mean episode length: 319.08
                 Mean success rate: 57.50
                  Mean reward/step: 20.08
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11296768
                    Iteration time: 2.55s
                        Total time: 3540.48s
                               ETA: 6731.8s

################################################################################
                     [1m Learning iteration 1379/4000 [0m

                       Computation: 3169 steps/s (collection: 0.497s, learning 2.088s)
               Value function loss: 78683.9954
                    Surrogate loss: 0.0178
             Mean action noise std: 0.93
                       Mean reward: 6527.78
               Mean episode length: 323.35
                 Mean success rate: 59.50
                  Mean reward/step: 19.98
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 2.58s
                        Total time: 3543.06s
                               ETA: 6729.3s

################################################################################
                     [1m Learning iteration 1380/4000 [0m

                       Computation: 3214 steps/s (collection: 0.462s, learning 2.087s)
               Value function loss: 88667.2841
                    Surrogate loss: 0.0182
             Mean action noise std: 0.93
                       Mean reward: 6521.60
               Mean episode length: 320.49
                 Mean success rate: 58.50
                  Mean reward/step: 20.32
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11313152
                    Iteration time: 2.55s
                        Total time: 3545.61s
                               ETA: 6726.6s

################################################################################
                     [1m Learning iteration 1381/4000 [0m

                       Computation: 3216 steps/s (collection: 0.481s, learning 2.066s)
               Value function loss: 98081.0799
                    Surrogate loss: 0.0117
             Mean action noise std: 0.93
                       Mean reward: 6960.75
               Mean episode length: 333.88
                 Mean success rate: 61.50
                  Mean reward/step: 19.62
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 2.55s
                        Total time: 3548.16s
                               ETA: 6724.0s

################################################################################
                     [1m Learning iteration 1382/4000 [0m

                       Computation: 3212 steps/s (collection: 0.453s, learning 2.097s)
               Value function loss: 85210.4632
                    Surrogate loss: 0.0126
             Mean action noise std: 0.93
                       Mean reward: 7101.07
               Mean episode length: 341.35
                 Mean success rate: 64.00
                  Mean reward/step: 18.96
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11329536
                    Iteration time: 2.55s
                        Total time: 3550.71s
                               ETA: 6721.4s

################################################################################
                     [1m Learning iteration 1383/4000 [0m

                       Computation: 3180 steps/s (collection: 0.497s, learning 2.080s)
               Value function loss: 79291.0639
                    Surrogate loss: 0.0174
             Mean action noise std: 0.93
                       Mean reward: 6888.37
               Mean episode length: 335.44
                 Mean success rate: 62.50
                  Mean reward/step: 19.03
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 2.58s
                        Total time: 3553.28s
                               ETA: 6718.9s

################################################################################
                     [1m Learning iteration 1384/4000 [0m

                       Computation: 3239 steps/s (collection: 0.454s, learning 2.075s)
               Value function loss: 64813.9395
                    Surrogate loss: 0.0167
             Mean action noise std: 0.93
                       Mean reward: 6687.23
               Mean episode length: 329.25
                 Mean success rate: 61.00
                  Mean reward/step: 19.15
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11345920
                    Iteration time: 2.53s
                        Total time: 3555.81s
                               ETA: 6716.2s

################################################################################
                     [1m Learning iteration 1385/4000 [0m

                       Computation: 3246 steps/s (collection: 0.441s, learning 2.082s)
               Value function loss: 100939.4957
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 7010.23
               Mean episode length: 341.75
                 Mean success rate: 64.50
                  Mean reward/step: 19.48
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 2.52s
                        Total time: 3558.34s
                               ETA: 6713.6s

################################################################################
                     [1m Learning iteration 1386/4000 [0m

                       Computation: 3247 steps/s (collection: 0.450s, learning 2.073s)
               Value function loss: 70507.4174
                    Surrogate loss: 0.0164
             Mean action noise std: 0.93
                       Mean reward: 7086.73
               Mean episode length: 347.77
                 Mean success rate: 65.00
                  Mean reward/step: 18.99
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11362304
                    Iteration time: 2.52s
                        Total time: 3560.86s
                               ETA: 6710.9s

################################################################################
                     [1m Learning iteration 1387/4000 [0m

                       Computation: 3170 steps/s (collection: 0.491s, learning 2.093s)
               Value function loss: 99384.0135
                    Surrogate loss: 0.0179
             Mean action noise std: 0.93
                       Mean reward: 6838.09
               Mean episode length: 347.85
                 Mean success rate: 64.00
                  Mean reward/step: 19.37
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 2.58s
                        Total time: 3563.44s
                               ETA: 6708.4s

################################################################################
                     [1m Learning iteration 1388/4000 [0m

                       Computation: 3200 steps/s (collection: 0.499s, learning 2.061s)
               Value function loss: 56618.8310
                    Surrogate loss: 0.0174
             Mean action noise std: 0.93
                       Mean reward: 6873.44
               Mean episode length: 352.00
                 Mean success rate: 64.50
                  Mean reward/step: 19.46
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11378688
                    Iteration time: 2.56s
                        Total time: 3566.00s
                               ETA: 6705.8s

################################################################################
                     [1m Learning iteration 1389/4000 [0m

                       Computation: 3264 steps/s (collection: 0.496s, learning 2.014s)
               Value function loss: 81898.6667
                    Surrogate loss: 0.0167
             Mean action noise std: 0.93
                       Mean reward: 6701.00
               Mean episode length: 345.24
                 Mean success rate: 63.00
                  Mean reward/step: 19.74
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 2.51s
                        Total time: 3568.51s
                               ETA: 6703.2s

################################################################################
                     [1m Learning iteration 1390/4000 [0m

                       Computation: 3224 steps/s (collection: 0.477s, learning 2.063s)
               Value function loss: 83993.2706
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 6554.46
               Mean episode length: 343.36
                 Mean success rate: 61.50
                  Mean reward/step: 19.37
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11395072
                    Iteration time: 2.54s
                        Total time: 3571.05s
                               ETA: 6700.5s

################################################################################
                     [1m Learning iteration 1391/4000 [0m

                       Computation: 3202 steps/s (collection: 0.506s, learning 2.052s)
               Value function loss: 79011.5699
                    Surrogate loss: 0.0182
             Mean action noise std: 0.93
                       Mean reward: 6797.97
               Mean episode length: 350.11
                 Mean success rate: 63.50
                  Mean reward/step: 19.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 2.56s
                        Total time: 3573.61s
                               ETA: 6697.9s

################################################################################
                     [1m Learning iteration 1392/4000 [0m

                       Computation: 3208 steps/s (collection: 0.490s, learning 2.063s)
               Value function loss: 94067.5396
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 7007.60
               Mean episode length: 357.01
                 Mean success rate: 66.00
                  Mean reward/step: 19.24
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11411456
                    Iteration time: 2.55s
                        Total time: 3576.16s
                               ETA: 6695.4s

################################################################################
                     [1m Learning iteration 1393/4000 [0m

                       Computation: 3255 steps/s (collection: 0.467s, learning 2.049s)
               Value function loss: 64455.9748
                    Surrogate loss: 0.0175
             Mean action noise std: 0.93
                       Mean reward: 6620.03
               Mean episode length: 343.89
                 Mean success rate: 62.50
                  Mean reward/step: 19.03
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 2.52s
                        Total time: 3578.68s
                               ETA: 6692.7s

################################################################################
                     [1m Learning iteration 1394/4000 [0m

                       Computation: 3195 steps/s (collection: 0.495s, learning 2.069s)
               Value function loss: 53846.0966
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 6391.38
               Mean episode length: 335.29
                 Mean success rate: 60.50
                  Mean reward/step: 19.58
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11427840
                    Iteration time: 2.56s
                        Total time: 3581.24s
                               ETA: 6690.1s

################################################################################
                     [1m Learning iteration 1395/4000 [0m

                       Computation: 3287 steps/s (collection: 0.466s, learning 2.025s)
               Value function loss: 58369.5407
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 6596.96
               Mean episode length: 342.13
                 Mean success rate: 63.00
                  Mean reward/step: 20.51
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 2.49s
                        Total time: 3583.73s
                               ETA: 6687.4s

################################################################################
                     [1m Learning iteration 1396/4000 [0m

                       Computation: 3206 steps/s (collection: 0.471s, learning 2.084s)
               Value function loss: 111092.0484
                    Surrogate loss: 0.0131
             Mean action noise std: 0.93
                       Mean reward: 7167.09
               Mean episode length: 352.88
                 Mean success rate: 66.00
                  Mean reward/step: 20.76
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11444224
                    Iteration time: 2.55s
                        Total time: 3586.29s
                               ETA: 6684.8s

################################################################################
                     [1m Learning iteration 1397/4000 [0m

                       Computation: 3252 steps/s (collection: 0.454s, learning 2.065s)
               Value function loss: 63615.2587
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 6961.86
               Mean episode length: 347.53
                 Mean success rate: 64.50
                  Mean reward/step: 20.24
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 2.52s
                        Total time: 3588.81s
                               ETA: 6682.2s

################################################################################
                     [1m Learning iteration 1398/4000 [0m

                       Computation: 3265 steps/s (collection: 0.430s, learning 2.079s)
               Value function loss: 100119.3285
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 6956.13
               Mean episode length: 350.83
                 Mean success rate: 65.00
                  Mean reward/step: 20.61
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11460608
                    Iteration time: 2.51s
                        Total time: 3591.32s
                               ETA: 6679.5s

################################################################################
                     [1m Learning iteration 1399/4000 [0m

                       Computation: 3175 steps/s (collection: 0.477s, learning 2.103s)
               Value function loss: 65144.9556
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 6840.84
               Mean episode length: 349.15
                 Mean success rate: 64.00
                  Mean reward/step: 21.32
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 2.58s
                        Total time: 3593.90s
                               ETA: 6676.9s

################################################################################
                     [1m Learning iteration 1400/4000 [0m

                       Computation: 3158 steps/s (collection: 0.494s, learning 2.099s)
               Value function loss: 96738.8449
                    Surrogate loss: 0.0135
             Mean action noise std: 0.93
                       Mean reward: 6870.33
               Mean episode length: 354.35
                 Mean success rate: 64.50
                  Mean reward/step: 21.90
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11476992
                    Iteration time: 2.59s
                        Total time: 3596.49s
                               ETA: 6674.4s

################################################################################
                     [1m Learning iteration 1401/4000 [0m

                       Computation: 3170 steps/s (collection: 0.506s, learning 2.078s)
               Value function loss: 61977.0460
                    Surrogate loss: 0.0167
             Mean action noise std: 0.93
                       Mean reward: 6970.00
               Mean episode length: 357.61
                 Mean success rate: 64.50
                  Mean reward/step: 21.30
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 2.58s
                        Total time: 3599.07s
                               ETA: 6671.9s

################################################################################
                     [1m Learning iteration 1402/4000 [0m

                       Computation: 3244 steps/s (collection: 0.462s, learning 2.063s)
               Value function loss: 109338.1032
                    Surrogate loss: 0.0167
             Mean action noise std: 0.93
                       Mean reward: 7255.13
               Mean episode length: 364.33
                 Mean success rate: 66.50
                  Mean reward/step: 21.43
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11493376
                    Iteration time: 2.52s
                        Total time: 3601.60s
                               ETA: 6669.2s

################################################################################
                     [1m Learning iteration 1403/4000 [0m

                       Computation: 3177 steps/s (collection: 0.508s, learning 2.071s)
               Value function loss: 91669.4144
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 7624.06
               Mean episode length: 375.65
                 Mean success rate: 69.50
                  Mean reward/step: 20.95
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 2.58s
                        Total time: 3604.18s
                               ETA: 6666.7s

################################################################################
                     [1m Learning iteration 1404/4000 [0m

                       Computation: 3218 steps/s (collection: 0.489s, learning 2.057s)
               Value function loss: 67106.3152
                    Surrogate loss: 0.0177
             Mean action noise std: 0.93
                       Mean reward: 7702.46
               Mean episode length: 376.94
                 Mean success rate: 69.50
                  Mean reward/step: 21.25
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11509760
                    Iteration time: 2.55s
                        Total time: 3606.72s
                               ETA: 6664.1s

################################################################################
                     [1m Learning iteration 1405/4000 [0m

                       Computation: 3190 steps/s (collection: 0.480s, learning 2.088s)
               Value function loss: 98365.4790
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 7745.37
               Mean episode length: 377.52
                 Mean success rate: 70.00
                  Mean reward/step: 21.20
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 2.57s
                        Total time: 3609.29s
                               ETA: 6661.5s

################################################################################
                     [1m Learning iteration 1406/4000 [0m

                       Computation: 3172 steps/s (collection: 0.490s, learning 2.093s)
               Value function loss: 114232.5437
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 7631.12
               Mean episode length: 375.85
                 Mean success rate: 69.00
                  Mean reward/step: 20.49
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11526144
                    Iteration time: 2.58s
                        Total time: 3611.87s
                               ETA: 6659.0s

################################################################################
                     [1m Learning iteration 1407/4000 [0m

                       Computation: 3161 steps/s (collection: 0.512s, learning 2.079s)
               Value function loss: 94209.4617
                    Surrogate loss: 0.0160
             Mean action noise std: 0.93
                       Mean reward: 7875.82
               Mean episode length: 383.50
                 Mean success rate: 71.50
                  Mean reward/step: 19.00
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 2.59s
                        Total time: 3614.46s
                               ETA: 6656.5s

################################################################################
                     [1m Learning iteration 1408/4000 [0m

                       Computation: 3172 steps/s (collection: 0.513s, learning 2.069s)
               Value function loss: 71991.8800
                    Surrogate loss: 0.0212
             Mean action noise std: 0.93
                       Mean reward: 8155.13
               Mean episode length: 389.69
                 Mean success rate: 73.00
                  Mean reward/step: 18.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11542528
                    Iteration time: 2.58s
                        Total time: 3617.05s
                               ETA: 6653.9s

################################################################################
                     [1m Learning iteration 1409/4000 [0m

                       Computation: 3203 steps/s (collection: 0.471s, learning 2.086s)
               Value function loss: 69906.0203
                    Surrogate loss: 0.0189
             Mean action noise std: 0.93
                       Mean reward: 8217.49
               Mean episode length: 392.58
                 Mean success rate: 74.00
                  Mean reward/step: 19.43
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 2.56s
                        Total time: 3619.60s
                               ETA: 6651.3s

################################################################################
                     [1m Learning iteration 1410/4000 [0m

                       Computation: 3197 steps/s (collection: 0.495s, learning 2.067s)
               Value function loss: 47861.0591
                    Surrogate loss: 0.0160
             Mean action noise std: 0.93
                       Mean reward: 8160.85
               Mean episode length: 388.54
                 Mean success rate: 74.00
                  Mean reward/step: 19.23
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11558912
                    Iteration time: 2.56s
                        Total time: 3622.16s
                               ETA: 6648.8s

################################################################################
                     [1m Learning iteration 1411/4000 [0m

                       Computation: 3278 steps/s (collection: 0.424s, learning 2.075s)
               Value function loss: 85110.3498
                    Surrogate loss: 0.0161
             Mean action noise std: 0.93
                       Mean reward: 8046.46
               Mean episode length: 384.85
                 Mean success rate: 73.00
                  Mean reward/step: 19.76
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 2.50s
                        Total time: 3624.66s
                               ETA: 6646.1s

################################################################################
                     [1m Learning iteration 1412/4000 [0m

                       Computation: 3312 steps/s (collection: 0.428s, learning 2.045s)
               Value function loss: 75945.1844
                    Surrogate loss: 0.0189
             Mean action noise std: 0.92
                       Mean reward: 8042.97
               Mean episode length: 389.24
                 Mean success rate: 73.00
                  Mean reward/step: 19.46
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11575296
                    Iteration time: 2.47s
                        Total time: 3627.14s
                               ETA: 6643.3s

################################################################################
                     [1m Learning iteration 1413/4000 [0m

                       Computation: 3278 steps/s (collection: 0.453s, learning 2.045s)
               Value function loss: 68741.9017
                    Surrogate loss: 0.0196
             Mean action noise std: 0.92
                       Mean reward: 8067.88
               Mean episode length: 390.10
                 Mean success rate: 73.50
                  Mean reward/step: 19.68
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 2.50s
                        Total time: 3629.63s
                               ETA: 6640.6s

################################################################################
                     [1m Learning iteration 1414/4000 [0m

                       Computation: 3089 steps/s (collection: 0.553s, learning 2.098s)
               Value function loss: 79491.1555
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 8254.98
               Mean episode length: 399.95
                 Mean success rate: 75.00
                  Mean reward/step: 19.92
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11591680
                    Iteration time: 2.65s
                        Total time: 3632.29s
                               ETA: 6638.2s

################################################################################
                     [1m Learning iteration 1415/4000 [0m

                       Computation: 3210 steps/s (collection: 0.503s, learning 2.049s)
               Value function loss: 52456.0740
                    Surrogate loss: 0.0162
             Mean action noise std: 0.92
                       Mean reward: 8012.03
               Mean episode length: 393.39
                 Mean success rate: 73.00
                  Mean reward/step: 19.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 2.55s
                        Total time: 3634.84s
                               ETA: 6635.6s

################################################################################
                     [1m Learning iteration 1416/4000 [0m

                       Computation: 3225 steps/s (collection: 0.465s, learning 2.074s)
               Value function loss: 102613.7179
                    Surrogate loss: 0.0167
             Mean action noise std: 0.92
                       Mean reward: 8269.10
               Mean episode length: 399.95
                 Mean success rate: 75.00
                  Mean reward/step: 20.03
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11608064
                    Iteration time: 2.54s
                        Total time: 3637.38s
                               ETA: 6633.0s

################################################################################
                     [1m Learning iteration 1417/4000 [0m

                       Computation: 3212 steps/s (collection: 0.445s, learning 2.105s)
               Value function loss: 77679.6398
                    Surrogate loss: 0.0169
             Mean action noise std: 0.92
                       Mean reward: 8133.13
               Mean episode length: 393.72
                 Mean success rate: 74.50
                  Mean reward/step: 19.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 2.55s
                        Total time: 3639.93s
                               ETA: 6630.4s

################################################################################
                     [1m Learning iteration 1418/4000 [0m

                       Computation: 3142 steps/s (collection: 0.463s, learning 2.144s)
               Value function loss: 92001.6604
                    Surrogate loss: 0.0152
             Mean action noise std: 0.92
                       Mean reward: 8303.14
               Mean episode length: 403.66
                 Mean success rate: 77.00
                  Mean reward/step: 19.41
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11624448
                    Iteration time: 2.61s
                        Total time: 3642.54s
                               ETA: 6627.9s

################################################################################
                     [1m Learning iteration 1419/4000 [0m

                       Computation: 3224 steps/s (collection: 0.488s, learning 2.053s)
               Value function loss: 82767.2266
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 8124.94
               Mean episode length: 395.92
                 Mean success rate: 75.00
                  Mean reward/step: 19.28
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 2.54s
                        Total time: 3645.08s
                               ETA: 6625.3s

################################################################################
                     [1m Learning iteration 1420/4000 [0m

                       Computation: 3253 steps/s (collection: 0.447s, learning 2.070s)
               Value function loss: 64537.3433
                    Surrogate loss: 0.0185
             Mean action noise std: 0.92
                       Mean reward: 8009.68
               Mean episode length: 394.60
                 Mean success rate: 73.50
                  Mean reward/step: 20.08
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11640832
                    Iteration time: 2.52s
                        Total time: 3647.59s
                               ETA: 6622.7s

################################################################################
                     [1m Learning iteration 1421/4000 [0m

                       Computation: 3202 steps/s (collection: 0.487s, learning 2.071s)
               Value function loss: 88227.9902
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 8098.81
               Mean episode length: 404.25
                 Mean success rate: 76.00
                  Mean reward/step: 19.30
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 2.56s
                        Total time: 3650.15s
                               ETA: 6620.1s

################################################################################
                     [1m Learning iteration 1422/4000 [0m

                       Computation: 3219 steps/s (collection: 0.485s, learning 2.059s)
               Value function loss: 82659.5979
                    Surrogate loss: 0.0168
             Mean action noise std: 0.92
                       Mean reward: 7631.69
               Mean episode length: 389.56
                 Mean success rate: 72.50
                  Mean reward/step: 19.02
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11657216
                    Iteration time: 2.54s
                        Total time: 3652.70s
                               ETA: 6617.5s

################################################################################
                     [1m Learning iteration 1423/4000 [0m

                       Computation: 3215 steps/s (collection: 0.509s, learning 2.039s)
               Value function loss: 85579.0727
                    Surrogate loss: 0.0215
             Mean action noise std: 0.92
                       Mean reward: 7648.11
               Mean episode length: 393.57
                 Mean success rate: 73.00
                  Mean reward/step: 19.31
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 2.55s
                        Total time: 3655.24s
                               ETA: 6614.9s

################################################################################
                     [1m Learning iteration 1424/4000 [0m

                       Computation: 3264 steps/s (collection: 0.481s, learning 2.029s)
               Value function loss: 72036.9128
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 7640.01
               Mean episode length: 391.02
                 Mean success rate: 73.00
                  Mean reward/step: 19.31
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11673600
                    Iteration time: 2.51s
                        Total time: 3657.75s
                               ETA: 6612.2s

################################################################################
                     [1m Learning iteration 1425/4000 [0m

                       Computation: 3298 steps/s (collection: 0.419s, learning 2.065s)
               Value function loss: 65927.5894
                    Surrogate loss: 0.0157
             Mean action noise std: 0.92
                       Mean reward: 7620.78
               Mean episode length: 398.94
                 Mean success rate: 74.00
                  Mean reward/step: 20.38
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 2.48s
                        Total time: 3660.24s
                               ETA: 6609.5s

################################################################################
                     [1m Learning iteration 1426/4000 [0m

                       Computation: 3216 steps/s (collection: 0.493s, learning 2.054s)
               Value function loss: 74983.9153
                    Surrogate loss: 0.0155
             Mean action noise std: 0.92
                       Mean reward: 7246.40
               Mean episode length: 385.67
                 Mean success rate: 70.50
                  Mean reward/step: 21.55
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11689984
                    Iteration time: 2.55s
                        Total time: 3662.79s
                               ETA: 6606.9s

################################################################################
                     [1m Learning iteration 1427/4000 [0m

                       Computation: 3262 steps/s (collection: 0.490s, learning 2.021s)
               Value function loss: 88021.0795
                    Surrogate loss: 0.0177
             Mean action noise std: 0.92
                       Mean reward: 7371.34
               Mean episode length: 385.77
                 Mean success rate: 70.50
                  Mean reward/step: 21.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 2.51s
                        Total time: 3665.30s
                               ETA: 6604.2s

################################################################################
                     [1m Learning iteration 1428/4000 [0m

                       Computation: 3197 steps/s (collection: 0.482s, learning 2.080s)
               Value function loss: 66463.4360
                    Surrogate loss: 0.0179
             Mean action noise std: 0.92
                       Mean reward: 7678.75
               Mean episode length: 397.93
                 Mean success rate: 73.50
                  Mean reward/step: 21.35
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11706368
                    Iteration time: 2.56s
                        Total time: 3667.86s
                               ETA: 6601.6s

################################################################################
                     [1m Learning iteration 1429/4000 [0m

                       Computation: 3245 steps/s (collection: 0.478s, learning 2.047s)
               Value function loss: 80156.5277
                    Surrogate loss: 0.0202
             Mean action noise std: 0.92
                       Mean reward: 7628.65
               Mean episode length: 390.02
                 Mean success rate: 73.00
                  Mean reward/step: 21.41
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 2.52s
                        Total time: 3670.38s
                               ETA: 6599.0s

################################################################################
                     [1m Learning iteration 1430/4000 [0m

                       Computation: 3201 steps/s (collection: 0.490s, learning 2.069s)
               Value function loss: 85739.3589
                    Surrogate loss: 0.0148
             Mean action noise std: 0.92
                       Mean reward: 7809.28
               Mean episode length: 393.71
                 Mean success rate: 73.50
                  Mean reward/step: 21.16
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11722752
                    Iteration time: 2.56s
                        Total time: 3672.94s
                               ETA: 6596.4s

################################################################################
                     [1m Learning iteration 1431/4000 [0m

                       Computation: 3256 steps/s (collection: 0.460s, learning 2.055s)
               Value function loss: 66290.9508
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 7930.58
               Mean episode length: 397.98
                 Mean success rate: 74.50
                  Mean reward/step: 20.38
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 2.52s
                        Total time: 3675.46s
                               ETA: 6593.7s

################################################################################
                     [1m Learning iteration 1432/4000 [0m

                       Computation: 3197 steps/s (collection: 0.481s, learning 2.081s)
               Value function loss: 73154.6232
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 7970.68
               Mean episode length: 400.94
                 Mean success rate: 74.00
                  Mean reward/step: 20.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11739136
                    Iteration time: 2.56s
                        Total time: 3678.02s
                               ETA: 6591.2s

################################################################################
                     [1m Learning iteration 1433/4000 [0m

                       Computation: 3223 steps/s (collection: 0.468s, learning 2.073s)
               Value function loss: 94872.4908
                    Surrogate loss: 0.0173
             Mean action noise std: 0.92
                       Mean reward: 8159.69
               Mean episode length: 402.41
                 Mean success rate: 75.00
                  Mean reward/step: 19.79
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 2.54s
                        Total time: 3680.56s
                               ETA: 6588.6s

################################################################################
                     [1m Learning iteration 1434/4000 [0m

                       Computation: 3224 steps/s (collection: 0.491s, learning 2.049s)
               Value function loss: 99873.6256
                    Surrogate loss: 0.0155
             Mean action noise std: 0.92
                       Mean reward: 8093.84
               Mean episode length: 393.20
                 Mean success rate: 74.50
                  Mean reward/step: 19.22
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11755520
                    Iteration time: 2.54s
                        Total time: 3683.10s
                               ETA: 6585.9s

################################################################################
                     [1m Learning iteration 1435/4000 [0m

                       Computation: 3236 steps/s (collection: 0.480s, learning 2.051s)
               Value function loss: 71077.2571
                    Surrogate loss: 0.0175
             Mean action noise std: 0.92
                       Mean reward: 8259.31
               Mean episode length: 391.45
                 Mean success rate: 74.50
                  Mean reward/step: 18.38
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 2.53s
                        Total time: 3685.63s
                               ETA: 6583.3s

################################################################################
                     [1m Learning iteration 1436/4000 [0m

                       Computation: 3219 steps/s (collection: 0.484s, learning 2.060s)
               Value function loss: 75198.1225
                    Surrogate loss: 0.0189
             Mean action noise std: 0.92
                       Mean reward: 8059.72
               Mean episode length: 394.99
                 Mean success rate: 73.00
                  Mean reward/step: 19.01
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11771904
                    Iteration time: 2.54s
                        Total time: 3688.18s
                               ETA: 6580.7s

################################################################################
                     [1m Learning iteration 1437/4000 [0m

                       Computation: 3189 steps/s (collection: 0.496s, learning 2.072s)
               Value function loss: 80622.7605
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 7843.70
               Mean episode length: 390.63
                 Mean success rate: 72.00
                  Mean reward/step: 19.27
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 2.57s
                        Total time: 3690.75s
                               ETA: 6578.2s

################################################################################
                     [1m Learning iteration 1438/4000 [0m

                       Computation: 3187 steps/s (collection: 0.526s, learning 2.044s)
               Value function loss: 70332.7041
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 8069.62
               Mean episode length: 396.59
                 Mean success rate: 72.50
                  Mean reward/step: 19.42
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11788288
                    Iteration time: 2.57s
                        Total time: 3693.32s
                               ETA: 6575.6s

################################################################################
                     [1m Learning iteration 1439/4000 [0m

                       Computation: 3170 steps/s (collection: 0.545s, learning 2.039s)
               Value function loss: 84033.5354
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 7718.83
               Mean episode length: 385.87
                 Mean success rate: 69.50
                  Mean reward/step: 19.41
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 2.58s
                        Total time: 3695.90s
                               ETA: 6573.1s

################################################################################
                     [1m Learning iteration 1440/4000 [0m

                       Computation: 3272 steps/s (collection: 0.456s, learning 2.047s)
               Value function loss: 68905.5684
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 7411.93
               Mean episode length: 378.61
                 Mean success rate: 68.00
                  Mean reward/step: 19.79
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11804672
                    Iteration time: 2.50s
                        Total time: 3698.40s
                               ETA: 6570.4s

################################################################################
                     [1m Learning iteration 1441/4000 [0m

                       Computation: 3189 steps/s (collection: 0.447s, learning 2.121s)
               Value function loss: 68581.7676
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 7294.33
               Mean episode length: 376.43
                 Mean success rate: 68.50
                  Mean reward/step: 19.92
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 2.57s
                        Total time: 3700.97s
                               ETA: 6567.8s

################################################################################
                     [1m Learning iteration 1442/4000 [0m

                       Computation: 3183 steps/s (collection: 0.525s, learning 2.048s)
               Value function loss: 90755.4788
                    Surrogate loss: 0.0171
             Mean action noise std: 0.92
                       Mean reward: 6965.76
               Mean episode length: 367.09
                 Mean success rate: 65.50
                  Mean reward/step: 20.24
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11821056
                    Iteration time: 2.57s
                        Total time: 3703.54s
                               ETA: 6565.3s

################################################################################
                     [1m Learning iteration 1443/4000 [0m

                       Computation: 3210 steps/s (collection: 0.514s, learning 2.038s)
               Value function loss: 87340.6034
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 6901.65
               Mean episode length: 364.87
                 Mean success rate: 64.50
                  Mean reward/step: 19.50
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 2.55s
                        Total time: 3706.10s
                               ETA: 6562.7s

################################################################################
                     [1m Learning iteration 1444/4000 [0m

                       Computation: 3268 steps/s (collection: 0.450s, learning 2.056s)
               Value function loss: 43321.1700
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 6676.96
               Mean episode length: 355.51
                 Mean success rate: 62.00
                  Mean reward/step: 19.45
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11837440
                    Iteration time: 2.51s
                        Total time: 3708.60s
                               ETA: 6560.0s

################################################################################
                     [1m Learning iteration 1445/4000 [0m

                       Computation: 3195 steps/s (collection: 0.455s, learning 2.109s)
               Value function loss: 103732.0678
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 6889.40
               Mean episode length: 355.32
                 Mean success rate: 64.50
                  Mean reward/step: 19.50
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 2.56s
                        Total time: 3711.17s
                               ETA: 6557.4s

################################################################################
                     [1m Learning iteration 1446/4000 [0m

                       Computation: 3188 steps/s (collection: 0.444s, learning 2.125s)
               Value function loss: 72965.1611
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 6444.60
               Mean episode length: 341.18
                 Mean success rate: 60.00
                  Mean reward/step: 19.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11853824
                    Iteration time: 2.57s
                        Total time: 3713.74s
                               ETA: 6554.9s

################################################################################
                     [1m Learning iteration 1447/4000 [0m

                       Computation: 3233 steps/s (collection: 0.430s, learning 2.103s)
               Value function loss: 88105.6266
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 6247.54
               Mean episode length: 333.92
                 Mean success rate: 58.00
                  Mean reward/step: 19.75
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 2.53s
                        Total time: 3716.27s
                               ETA: 6552.2s

################################################################################
                     [1m Learning iteration 1448/4000 [0m

                       Computation: 3207 steps/s (collection: 0.474s, learning 2.080s)
               Value function loss: 78969.6080
                    Surrogate loss: 0.0148
             Mean action noise std: 0.92
                       Mean reward: 6237.32
               Mean episode length: 334.07
                 Mean success rate: 58.50
                  Mean reward/step: 20.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11870208
                    Iteration time: 2.55s
                        Total time: 3718.82s
                               ETA: 6549.6s

################################################################################
                     [1m Learning iteration 1449/4000 [0m

                       Computation: 3235 steps/s (collection: 0.452s, learning 2.080s)
               Value function loss: 97955.1566
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 6787.51
               Mean episode length: 345.61
                 Mean success rate: 62.00
                  Mean reward/step: 20.50
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 2.53s
                        Total time: 3721.35s
                               ETA: 6547.0s

################################################################################
                     [1m Learning iteration 1450/4000 [0m

                       Computation: 3222 steps/s (collection: 0.451s, learning 2.091s)
               Value function loss: 93276.4031
                    Surrogate loss: 0.0186
             Mean action noise std: 0.92
                       Mean reward: 6904.37
               Mean episode length: 348.36
                 Mean success rate: 61.50
                  Mean reward/step: 20.37
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11886592
                    Iteration time: 2.54s
                        Total time: 3723.90s
                               ETA: 6544.4s

################################################################################
                     [1m Learning iteration 1451/4000 [0m

                       Computation: 3181 steps/s (collection: 0.463s, learning 2.112s)
               Value function loss: 97339.6195
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 7111.59
               Mean episode length: 347.38
                 Mean success rate: 63.00
                  Mean reward/step: 19.90
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 2.58s
                        Total time: 3726.47s
                               ETA: 6541.9s

################################################################################
                     [1m Learning iteration 1452/4000 [0m

                       Computation: 3182 steps/s (collection: 0.449s, learning 2.125s)
               Value function loss: 85358.5811
                    Surrogate loss: 0.0119
             Mean action noise std: 0.92
                       Mean reward: 7253.43
               Mean episode length: 355.00
                 Mean success rate: 65.00
                  Mean reward/step: 19.85
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11902976
                    Iteration time: 2.57s
                        Total time: 3729.05s
                               ETA: 6539.3s

################################################################################
                     [1m Learning iteration 1453/4000 [0m

                       Computation: 3213 steps/s (collection: 0.463s, learning 2.086s)
               Value function loss: 110621.3861
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 7383.57
               Mean episode length: 359.44
                 Mean success rate: 65.00
                  Mean reward/step: 20.32
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 2.55s
                        Total time: 3731.60s
                               ETA: 6536.7s

################################################################################
                     [1m Learning iteration 1454/4000 [0m

                       Computation: 3216 steps/s (collection: 0.439s, learning 2.108s)
               Value function loss: 90906.4872
                    Surrogate loss: 0.0132
             Mean action noise std: 0.92
                       Mean reward: 7334.59
               Mean episode length: 362.79
                 Mean success rate: 65.50
                  Mean reward/step: 20.16
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11919360
                    Iteration time: 2.55s
                        Total time: 3734.14s
                               ETA: 6534.1s

################################################################################
                     [1m Learning iteration 1455/4000 [0m

                       Computation: 3225 steps/s (collection: 0.477s, learning 2.063s)
               Value function loss: 70665.0820
                    Surrogate loss: 0.0197
             Mean action noise std: 0.92
                       Mean reward: 7251.02
               Mean episode length: 357.94
                 Mean success rate: 65.00
                  Mean reward/step: 19.65
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 2.54s
                        Total time: 3736.68s
                               ETA: 6531.5s

################################################################################
                     [1m Learning iteration 1456/4000 [0m

                       Computation: 3187 steps/s (collection: 0.478s, learning 2.092s)
               Value function loss: 66076.1946
                    Surrogate loss: 0.0167
             Mean action noise std: 0.92
                       Mean reward: 7329.63
               Mean episode length: 357.94
                 Mean success rate: 65.50
                  Mean reward/step: 19.82
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 11935744
                    Iteration time: 2.57s
                        Total time: 3739.25s
                               ETA: 6528.9s

################################################################################
                     [1m Learning iteration 1457/4000 [0m

                       Computation: 3105 steps/s (collection: 0.505s, learning 2.133s)
               Value function loss: 77367.8293
                    Surrogate loss: 0.0173
             Mean action noise std: 0.92
                       Mean reward: 7070.42
               Mean episode length: 353.45
                 Mean success rate: 64.00
                  Mean reward/step: 19.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 2.64s
                        Total time: 3741.89s
                               ETA: 6526.5s

################################################################################
                     [1m Learning iteration 1458/4000 [0m

                       Computation: 3149 steps/s (collection: 0.511s, learning 2.091s)
               Value function loss: 103992.3356
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 6865.63
               Mean episode length: 350.69
                 Mean success rate: 64.00
                  Mean reward/step: 19.43
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11952128
                    Iteration time: 2.60s
                        Total time: 3744.49s
                               ETA: 6524.0s

################################################################################
                     [1m Learning iteration 1459/4000 [0m

                       Computation: 3178 steps/s (collection: 0.466s, learning 2.111s)
               Value function loss: 67816.0437
                    Surrogate loss: 0.0157
             Mean action noise std: 0.92
                       Mean reward: 6808.44
               Mean episode length: 348.29
                 Mean success rate: 64.00
                  Mean reward/step: 19.33
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 2.58s
                        Total time: 3747.07s
                               ETA: 6521.4s

################################################################################
                     [1m Learning iteration 1460/4000 [0m

                       Computation: 3144 steps/s (collection: 0.443s, learning 2.161s)
               Value function loss: 75589.2373
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 6463.35
               Mean episode length: 333.90
                 Mean success rate: 61.00
                  Mean reward/step: 19.54
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11968512
                    Iteration time: 2.60s
                        Total time: 3749.67s
                               ETA: 6518.9s

################################################################################
                     [1m Learning iteration 1461/4000 [0m

                       Computation: 3131 steps/s (collection: 0.520s, learning 2.096s)
               Value function loss: 73614.8552
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 6780.65
               Mean episode length: 343.62
                 Mean success rate: 63.50
                  Mean reward/step: 19.18
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 2.62s
                        Total time: 3752.29s
                               ETA: 6516.5s

################################################################################
                     [1m Learning iteration 1462/4000 [0m

                       Computation: 3131 steps/s (collection: 0.514s, learning 2.102s)
               Value function loss: 84502.8844
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 6825.84
               Mean episode length: 343.60
                 Mean success rate: 64.00
                  Mean reward/step: 18.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11984896
                    Iteration time: 2.62s
                        Total time: 3754.91s
                               ETA: 6514.0s

################################################################################
                     [1m Learning iteration 1463/4000 [0m

                       Computation: 3158 steps/s (collection: 0.485s, learning 2.109s)
               Value function loss: 75264.1787
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 6826.26
               Mean episode length: 336.87
                 Mean success rate: 63.50
                  Mean reward/step: 18.53
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 2.59s
                        Total time: 3757.50s
                               ETA: 6511.5s

################################################################################
                     [1m Learning iteration 1464/4000 [0m

                       Computation: 3134 steps/s (collection: 0.495s, learning 2.119s)
               Value function loss: 80634.1511
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 6948.43
               Mean episode length: 348.86
                 Mean success rate: 65.50
                  Mean reward/step: 18.07
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12001280
                    Iteration time: 2.61s
                        Total time: 3760.11s
                               ETA: 6509.0s

################################################################################
                     [1m Learning iteration 1465/4000 [0m

                       Computation: 3146 steps/s (collection: 0.505s, learning 2.099s)
               Value function loss: 78470.8773
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 6697.29
               Mean episode length: 336.08
                 Mean success rate: 62.50
                  Mean reward/step: 18.40
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 2.60s
                        Total time: 3762.72s
                               ETA: 6506.5s

################################################################################
                     [1m Learning iteration 1466/4000 [0m

                       Computation: 3183 steps/s (collection: 0.474s, learning 2.099s)
               Value function loss: 68534.7824
                    Surrogate loss: 0.0148
             Mean action noise std: 0.92
                       Mean reward: 6550.60
               Mean episode length: 329.59
                 Mean success rate: 61.00
                  Mean reward/step: 18.58
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12017664
                    Iteration time: 2.57s
                        Total time: 3765.29s
                               ETA: 6503.9s

################################################################################
                     [1m Learning iteration 1467/4000 [0m

                       Computation: 3141 steps/s (collection: 0.489s, learning 2.118s)
               Value function loss: 98838.4751
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 6514.10
               Mean episode length: 329.23
                 Mean success rate: 60.00
                  Mean reward/step: 18.80
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 2.61s
                        Total time: 3767.90s
                               ETA: 6501.4s

################################################################################
                     [1m Learning iteration 1468/4000 [0m

                       Computation: 3235 steps/s (collection: 0.474s, learning 2.058s)
               Value function loss: 76708.7023
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 6296.20
               Mean episode length: 324.67
                 Mean success rate: 57.50
                  Mean reward/step: 18.89
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12034048
                    Iteration time: 2.53s
                        Total time: 3770.43s
                               ETA: 6498.8s

################################################################################
                     [1m Learning iteration 1469/4000 [0m

                       Computation: 3227 steps/s (collection: 0.475s, learning 2.063s)
               Value function loss: 92717.3985
                    Surrogate loss: 0.0162
             Mean action noise std: 0.92
                       Mean reward: 5994.47
               Mean episode length: 317.93
                 Mean success rate: 55.50
                  Mean reward/step: 18.36
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 2.54s
                        Total time: 3772.97s
                               ETA: 6496.2s

################################################################################
                     [1m Learning iteration 1470/4000 [0m

                       Computation: 3140 steps/s (collection: 0.488s, learning 2.120s)
               Value function loss: 83989.1923
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 5999.96
               Mean episode length: 324.29
                 Mean success rate: 56.00
                  Mean reward/step: 18.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12050432
                    Iteration time: 2.61s
                        Total time: 3775.58s
                               ETA: 6493.7s

################################################################################
                     [1m Learning iteration 1471/4000 [0m

                       Computation: 3210 steps/s (collection: 0.467s, learning 2.085s)
               Value function loss: 65783.8876
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 5838.46
               Mean episode length: 315.27
                 Mean success rate: 54.50
                  Mean reward/step: 19.31
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 2.55s
                        Total time: 3778.13s
                               ETA: 6491.1s

################################################################################
                     [1m Learning iteration 1472/4000 [0m

                       Computation: 3244 steps/s (collection: 0.457s, learning 2.068s)
               Value function loss: 72589.3110
                    Surrogate loss: 0.0169
             Mean action noise std: 0.92
                       Mean reward: 6003.36
               Mean episode length: 318.18
                 Mean success rate: 56.00
                  Mean reward/step: 19.58
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12066816
                    Iteration time: 2.53s
                        Total time: 3780.65s
                               ETA: 6488.5s

################################################################################
                     [1m Learning iteration 1473/4000 [0m

                       Computation: 3232 steps/s (collection: 0.477s, learning 2.057s)
               Value function loss: 74749.3252
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 5727.63
               Mean episode length: 318.07
                 Mean success rate: 55.00
                  Mean reward/step: 20.14
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 2.53s
                        Total time: 3783.19s
                               ETA: 6485.8s

################################################################################
                     [1m Learning iteration 1474/4000 [0m

                       Computation: 3150 steps/s (collection: 0.509s, learning 2.091s)
               Value function loss: 73561.1935
                    Surrogate loss: 0.0156
             Mean action noise std: 0.92
                       Mean reward: 5733.55
               Mean episode length: 316.95
                 Mean success rate: 55.00
                  Mean reward/step: 20.34
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12083200
                    Iteration time: 2.60s
                        Total time: 3785.79s
                               ETA: 6483.3s

################################################################################
                     [1m Learning iteration 1475/4000 [0m

                       Computation: 3232 steps/s (collection: 0.478s, learning 2.055s)
               Value function loss: 66850.0878
                    Surrogate loss: 0.0166
             Mean action noise std: 0.92
                       Mean reward: 5796.85
               Mean episode length: 318.94
                 Mean success rate: 55.50
                  Mean reward/step: 20.70
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 2.53s
                        Total time: 3788.32s
                               ETA: 6480.7s

################################################################################
                     [1m Learning iteration 1476/4000 [0m

                       Computation: 3257 steps/s (collection: 0.460s, learning 2.054s)
               Value function loss: 73495.6727
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 5871.66
               Mean episode length: 316.81
                 Mean success rate: 56.00
                  Mean reward/step: 20.87
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12099584
                    Iteration time: 2.51s
                        Total time: 3790.83s
                               ETA: 6478.0s

################################################################################
                     [1m Learning iteration 1477/4000 [0m

                       Computation: 3295 steps/s (collection: 0.426s, learning 2.060s)
               Value function loss: 63868.3009
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 5947.66
               Mean episode length: 318.98
                 Mean success rate: 56.50
                  Mean reward/step: 21.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 2.49s
                        Total time: 3793.32s
                               ETA: 6475.3s

################################################################################
                     [1m Learning iteration 1478/4000 [0m

                       Computation: 3201 steps/s (collection: 0.501s, learning 2.058s)
               Value function loss: 64198.6405
                    Surrogate loss: 0.0112
             Mean action noise std: 0.92
                       Mean reward: 5644.17
               Mean episode length: 302.68
                 Mean success rate: 53.50
                  Mean reward/step: 21.67
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 12115968
                    Iteration time: 2.56s
                        Total time: 3795.88s
                               ETA: 6472.8s

################################################################################
                     [1m Learning iteration 1479/4000 [0m

                       Computation: 3291 steps/s (collection: 0.444s, learning 2.044s)
               Value function loss: 82975.2216
                    Surrogate loss: 0.0181
             Mean action noise std: 0.92
                       Mean reward: 5737.90
               Mean episode length: 302.04
                 Mean success rate: 53.50
                  Mean reward/step: 21.33
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 2.49s
                        Total time: 3798.37s
                               ETA: 6470.1s

################################################################################
                     [1m Learning iteration 1480/4000 [0m

                       Computation: 3165 steps/s (collection: 0.493s, learning 2.094s)
               Value function loss: 84368.6988
                    Surrogate loss: 0.0197
             Mean action noise std: 0.92
                       Mean reward: 6014.98
               Mean episode length: 310.97
                 Mean success rate: 56.00
                  Mean reward/step: 20.80
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12132352
                    Iteration time: 2.59s
                        Total time: 3800.96s
                               ETA: 6467.5s

################################################################################
                     [1m Learning iteration 1481/4000 [0m

                       Computation: 3118 steps/s (collection: 0.507s, learning 2.119s)
               Value function loss: 77679.1245
                    Surrogate loss: 0.0197
             Mean action noise std: 0.92
                       Mean reward: 6347.63
               Mean episode length: 320.12
                 Mean success rate: 58.00
                  Mean reward/step: 20.68
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 2.63s
                        Total time: 3803.58s
                               ETA: 6465.1s

################################################################################
                     [1m Learning iteration 1482/4000 [0m

                       Computation: 3108 steps/s (collection: 0.535s, learning 2.100s)
               Value function loss: 100604.8662
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 6577.15
               Mean episode length: 326.90
                 Mean success rate: 59.00
                  Mean reward/step: 20.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12148736
                    Iteration time: 2.64s
                        Total time: 3806.22s
                               ETA: 6462.6s

################################################################################
                     [1m Learning iteration 1483/4000 [0m

                       Computation: 3106 steps/s (collection: 0.494s, learning 2.143s)
               Value function loss: 101856.3712
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 6685.57
               Mean episode length: 330.82
                 Mean success rate: 59.00
                  Mean reward/step: 20.68
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 2.64s
                        Total time: 3808.85s
                               ETA: 6460.2s

################################################################################
                     [1m Learning iteration 1484/4000 [0m

                       Computation: 3101 steps/s (collection: 0.521s, learning 2.121s)
               Value function loss: 112179.4662
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 7165.28
               Mean episode length: 345.37
                 Mean success rate: 62.00
                  Mean reward/step: 20.19
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12165120
                    Iteration time: 2.64s
                        Total time: 3811.50s
                               ETA: 6457.7s

################################################################################
                     [1m Learning iteration 1485/4000 [0m

                       Computation: 3182 steps/s (collection: 0.493s, learning 2.081s)
               Value function loss: 78731.4058
                    Surrogate loss: 0.0128
             Mean action noise std: 0.92
                       Mean reward: 6949.94
               Mean episode length: 338.33
                 Mean success rate: 61.00
                  Mean reward/step: 19.01
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 2.57s
                        Total time: 3814.07s
                               ETA: 6455.2s

################################################################################
                     [1m Learning iteration 1486/4000 [0m

                       Computation: 3197 steps/s (collection: 0.492s, learning 2.070s)
               Value function loss: 74447.5329
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 7077.80
               Mean episode length: 342.09
                 Mean success rate: 62.50
                  Mean reward/step: 19.15
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12181504
                    Iteration time: 2.56s
                        Total time: 3816.63s
                               ETA: 6452.6s

################################################################################
                     [1m Learning iteration 1487/4000 [0m

                       Computation: 3095 steps/s (collection: 0.510s, learning 2.136s)
               Value function loss: 89745.0608
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 7046.06
               Mean episode length: 343.92
                 Mean success rate: 63.50
                  Mean reward/step: 19.26
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 2.65s
                        Total time: 3819.28s
                               ETA: 6450.2s

################################################################################
                     [1m Learning iteration 1488/4000 [0m

                       Computation: 3045 steps/s (collection: 0.542s, learning 2.148s)
               Value function loss: 59635.5677
                    Surrogate loss: 0.0123
             Mean action noise std: 0.92
                       Mean reward: 6848.31
               Mean episode length: 332.21
                 Mean success rate: 60.50
                  Mean reward/step: 20.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12197888
                    Iteration time: 2.69s
                        Total time: 3821.97s
                               ETA: 6447.8s

################################################################################
                     [1m Learning iteration 1489/4000 [0m

                       Computation: 3155 steps/s (collection: 0.477s, learning 2.119s)
               Value function loss: 99975.7307
                    Surrogate loss: 0.0171
             Mean action noise std: 0.92
                       Mean reward: 7074.33
               Mean episode length: 338.63
                 Mean success rate: 62.00
                  Mean reward/step: 20.89
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 2.60s
                        Total time: 3824.56s
                               ETA: 6445.3s

################################################################################
                     [1m Learning iteration 1490/4000 [0m

                       Computation: 3094 steps/s (collection: 0.537s, learning 2.111s)
               Value function loss: 80561.1998
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 6791.96
               Mean episode length: 323.92
                 Mean success rate: 59.50
                  Mean reward/step: 21.14
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12214272
                    Iteration time: 2.65s
                        Total time: 3827.21s
                               ETA: 6442.9s

################################################################################
                     [1m Learning iteration 1491/4000 [0m

                       Computation: 3146 steps/s (collection: 0.474s, learning 2.130s)
               Value function loss: 63283.0293
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 6341.03
               Mean episode length: 307.12
                 Mean success rate: 56.50
                  Mean reward/step: 21.06
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 2.60s
                        Total time: 3829.82s
                               ETA: 6440.4s

################################################################################
                     [1m Learning iteration 1492/4000 [0m

                       Computation: 3167 steps/s (collection: 0.473s, learning 2.113s)
               Value function loss: 74481.7487
                    Surrogate loss: 0.0156
             Mean action noise std: 0.92
                       Mean reward: 6224.97
               Mean episode length: 307.92
                 Mean success rate: 57.00
                  Mean reward/step: 21.54
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12230656
                    Iteration time: 2.59s
                        Total time: 3832.40s
                               ETA: 6437.8s

################################################################################
                     [1m Learning iteration 1493/4000 [0m

                       Computation: 3151 steps/s (collection: 0.511s, learning 2.088s)
               Value function loss: 89468.2610
                    Surrogate loss: 0.0122
             Mean action noise std: 0.92
                       Mean reward: 6422.50
               Mean episode length: 313.29
                 Mean success rate: 58.00
                  Mean reward/step: 22.07
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 2.60s
                        Total time: 3835.00s
                               ETA: 6435.3s

################################################################################
                     [1m Learning iteration 1494/4000 [0m

                       Computation: 3146 steps/s (collection: 0.488s, learning 2.115s)
               Value function loss: 104480.8434
                    Surrogate loss: 0.0137
             Mean action noise std: 0.92
                       Mean reward: 6753.11
               Mean episode length: 322.87
                 Mean success rate: 59.50
                  Mean reward/step: 21.48
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12247040
                    Iteration time: 2.60s
                        Total time: 3837.60s
                               ETA: 6432.8s

################################################################################
                     [1m Learning iteration 1495/4000 [0m

                       Computation: 3138 steps/s (collection: 0.494s, learning 2.116s)
               Value function loss: 76245.7458
                    Surrogate loss: 0.0170
             Mean action noise std: 0.92
                       Mean reward: 7077.97
               Mean episode length: 335.28
                 Mean success rate: 62.50
                  Mean reward/step: 21.36
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 2.61s
                        Total time: 3840.21s
                               ETA: 6430.3s

################################################################################
                     [1m Learning iteration 1496/4000 [0m

                       Computation: 3127 steps/s (collection: 0.494s, learning 2.125s)
               Value function loss: 77964.3591
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 7355.04
               Mean episode length: 344.25
                 Mean success rate: 64.00
                  Mean reward/step: 21.44
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12263424
                    Iteration time: 2.62s
                        Total time: 3842.83s
                               ETA: 6427.8s

################################################################################
                     [1m Learning iteration 1497/4000 [0m

                       Computation: 3110 steps/s (collection: 0.548s, learning 2.086s)
               Value function loss: 99018.3855
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 7506.96
               Mean episode length: 355.65
                 Mean success rate: 66.50
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 2.63s
                        Total time: 3845.47s
                               ETA: 6425.4s

################################################################################
                     [1m Learning iteration 1498/4000 [0m

                       Computation: 3182 steps/s (collection: 0.469s, learning 2.105s)
               Value function loss: 81768.1994
                    Surrogate loss: 0.0228
             Mean action noise std: 0.92
                       Mean reward: 7175.42
               Mean episode length: 346.50
                 Mean success rate: 64.00
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12279808
                    Iteration time: 2.57s
                        Total time: 3848.04s
                               ETA: 6422.8s

################################################################################
                     [1m Learning iteration 1499/4000 [0m

                       Computation: 3156 steps/s (collection: 0.471s, learning 2.125s)
               Value function loss: 92014.9185
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 7601.32
               Mean episode length: 364.85
                 Mean success rate: 68.50
                  Mean reward/step: 21.56
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 2.60s
                        Total time: 3850.64s
                               ETA: 6420.3s

################################################################################
                     [1m Learning iteration 1500/4000 [0m

                       Computation: 3118 steps/s (collection: 0.511s, learning 2.116s)
               Value function loss: 95962.5118
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 7745.97
               Mean episode length: 372.69
                 Mean success rate: 70.50
                  Mean reward/step: 21.81
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12296192
                    Iteration time: 2.63s
                        Total time: 3853.26s
                               ETA: 6417.8s

################################################################################
                     [1m Learning iteration 1501/4000 [0m

                       Computation: 3196 steps/s (collection: 0.480s, learning 2.083s)
               Value function loss: 105110.1558
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 7705.46
               Mean episode length: 369.61
                 Mean success rate: 70.00
                  Mean reward/step: 21.05
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 2.56s
                        Total time: 3855.83s
                               ETA: 6415.3s

################################################################################
                     [1m Learning iteration 1502/4000 [0m

                       Computation: 3184 steps/s (collection: 0.475s, learning 2.098s)
               Value function loss: 99140.3823
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 7447.12
               Mean episode length: 356.46
                 Mean success rate: 68.50
                  Mean reward/step: 21.01
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 12312576
                    Iteration time: 2.57s
                        Total time: 3858.40s
                               ETA: 6412.7s

################################################################################
                     [1m Learning iteration 1503/4000 [0m

                       Computation: 3207 steps/s (collection: 0.466s, learning 2.088s)
               Value function loss: 88632.5174
                    Surrogate loss: 0.0177
             Mean action noise std: 0.92
                       Mean reward: 7597.58
               Mean episode length: 360.01
                 Mean success rate: 69.50
                  Mean reward/step: 20.27
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 2.55s
                        Total time: 3860.95s
                               ETA: 6410.1s

################################################################################
                     [1m Learning iteration 1504/4000 [0m

                       Computation: 3160 steps/s (collection: 0.492s, learning 2.100s)
               Value function loss: 73197.8448
                    Surrogate loss: 0.0177
             Mean action noise std: 0.92
                       Mean reward: 7321.51
               Mean episode length: 354.55
                 Mean success rate: 67.50
                  Mean reward/step: 20.84
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12328960
                    Iteration time: 2.59s
                        Total time: 3863.54s
                               ETA: 6407.6s

################################################################################
                     [1m Learning iteration 1505/4000 [0m

                       Computation: 3189 steps/s (collection: 0.468s, learning 2.100s)
               Value function loss: 77765.5520
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 7301.13
               Mean episode length: 349.64
                 Mean success rate: 66.00
                  Mean reward/step: 21.78
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 2.57s
                        Total time: 3866.11s
                               ETA: 6405.0s

################################################################################
                     [1m Learning iteration 1506/4000 [0m

                       Computation: 3260 steps/s (collection: 0.462s, learning 2.051s)
               Value function loss: 66261.8421
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 7632.24
               Mean episode length: 357.18
                 Mean success rate: 68.50
                  Mean reward/step: 22.18
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12345344
                    Iteration time: 2.51s
                        Total time: 3868.63s
                               ETA: 6402.4s

################################################################################
                     [1m Learning iteration 1507/4000 [0m

                       Computation: 3181 steps/s (collection: 0.490s, learning 2.085s)
               Value function loss: 84287.9235
                    Surrogate loss: 0.0106
             Mean action noise std: 0.92
                       Mean reward: 7526.58
               Mean episode length: 355.77
                 Mean success rate: 67.50
                  Mean reward/step: 22.52
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 2.57s
                        Total time: 3871.20s
                               ETA: 6399.8s

################################################################################
                     [1m Learning iteration 1508/4000 [0m

                       Computation: 3216 steps/s (collection: 0.472s, learning 2.075s)
               Value function loss: 82612.3162
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 7525.73
               Mean episode length: 352.95
                 Mean success rate: 67.50
                  Mean reward/step: 22.39
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12361728
                    Iteration time: 2.55s
                        Total time: 3873.75s
                               ETA: 6397.2s

################################################################################
                     [1m Learning iteration 1509/4000 [0m

                       Computation: 3147 steps/s (collection: 0.464s, learning 2.138s)
               Value function loss: 93417.0520
                    Surrogate loss: 0.0175
             Mean action noise std: 0.92
                       Mean reward: 7851.37
               Mean episode length: 363.09
                 Mean success rate: 69.50
                  Mean reward/step: 21.85
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 2.60s
                        Total time: 3876.35s
                               ETA: 6394.7s

################################################################################
                     [1m Learning iteration 1510/4000 [0m

                       Computation: 3181 steps/s (collection: 0.499s, learning 2.076s)
               Value function loss: 102907.6553
                    Surrogate loss: 0.0121
             Mean action noise std: 0.92
                       Mean reward: 8061.42
               Mean episode length: 368.99
                 Mean success rate: 70.00
                  Mean reward/step: 21.45
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12378112
                    Iteration time: 2.58s
                        Total time: 3878.93s
                               ETA: 6392.1s

################################################################################
                     [1m Learning iteration 1511/4000 [0m

                       Computation: 3176 steps/s (collection: 0.489s, learning 2.090s)
               Value function loss: 60474.5772
                    Surrogate loss: 0.0171
             Mean action noise std: 0.92
                       Mean reward: 8208.31
               Mean episode length: 374.85
                 Mean success rate: 71.50
                  Mean reward/step: 20.99
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 2.58s
                        Total time: 3881.50s
                               ETA: 6389.6s

################################################################################
                     [1m Learning iteration 1512/4000 [0m

                       Computation: 3171 steps/s (collection: 0.467s, learning 2.115s)
               Value function loss: 85024.0854
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 8231.45
               Mean episode length: 376.17
                 Mean success rate: 71.00
                  Mean reward/step: 21.17
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12394496
                    Iteration time: 2.58s
                        Total time: 3884.09s
                               ETA: 6387.1s

################################################################################
                     [1m Learning iteration 1513/4000 [0m

                       Computation: 3171 steps/s (collection: 0.462s, learning 2.121s)
               Value function loss: 80540.5724
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 8104.63
               Mean episode length: 369.07
                 Mean success rate: 71.00
                  Mean reward/step: 21.25
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 2.58s
                        Total time: 3886.67s
                               ETA: 6384.5s

################################################################################
                     [1m Learning iteration 1514/4000 [0m

                       Computation: 3222 steps/s (collection: 0.464s, learning 2.078s)
               Value function loss: 108665.9539
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 8447.42
               Mean episode length: 377.33
                 Mean success rate: 74.00
                  Mean reward/step: 21.07
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12410880
                    Iteration time: 2.54s
                        Total time: 3889.21s
                               ETA: 6381.9s

################################################################################
                     [1m Learning iteration 1515/4000 [0m

                       Computation: 3175 steps/s (collection: 0.484s, learning 2.096s)
               Value function loss: 78459.6542
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 8289.42
               Mean episode length: 377.58
                 Mean success rate: 73.00
                  Mean reward/step: 20.90
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 2.58s
                        Total time: 3891.79s
                               ETA: 6379.4s

################################################################################
                     [1m Learning iteration 1516/4000 [0m

                       Computation: 3152 steps/s (collection: 0.518s, learning 2.081s)
               Value function loss: 91586.9913
                    Surrogate loss: 0.0181
             Mean action noise std: 0.92
                       Mean reward: 8314.60
               Mean episode length: 377.15
                 Mean success rate: 73.00
                  Mean reward/step: 22.08
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12427264
                    Iteration time: 2.60s
                        Total time: 3894.39s
                               ETA: 6376.8s

################################################################################
                     [1m Learning iteration 1517/4000 [0m

                       Computation: 3166 steps/s (collection: 0.517s, learning 2.070s)
               Value function loss: 82556.5280
                    Surrogate loss: 0.0178
             Mean action noise std: 0.92
                       Mean reward: 8484.77
               Mean episode length: 385.12
                 Mean success rate: 74.00
                  Mean reward/step: 21.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 2.59s
                        Total time: 3896.98s
                               ETA: 6374.3s

################################################################################
                     [1m Learning iteration 1518/4000 [0m

                       Computation: 3196 steps/s (collection: 0.473s, learning 2.090s)
               Value function loss: 118688.0369
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 8392.34
               Mean episode length: 385.00
                 Mean success rate: 73.00
                  Mean reward/step: 21.09
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12443648
                    Iteration time: 2.56s
                        Total time: 3899.54s
                               ETA: 6371.7s

################################################################################
                     [1m Learning iteration 1519/4000 [0m

                       Computation: 3226 steps/s (collection: 0.461s, learning 2.077s)
               Value function loss: 67372.9483
                    Surrogate loss: 0.0182
             Mean action noise std: 0.92
                       Mean reward: 8165.46
               Mean episode length: 376.29
                 Mean success rate: 72.50
                  Mean reward/step: 20.60
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 2.54s
                        Total time: 3902.08s
                               ETA: 6369.1s

################################################################################
                     [1m Learning iteration 1520/4000 [0m

                       Computation: 3231 steps/s (collection: 0.481s, learning 2.054s)
               Value function loss: 91212.4232
                    Surrogate loss: 0.0190
             Mean action noise std: 0.92
                       Mean reward: 8288.50
               Mean episode length: 386.73
                 Mean success rate: 73.00
                  Mean reward/step: 21.32
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12460032
                    Iteration time: 2.54s
                        Total time: 3904.62s
                               ETA: 6366.5s

################################################################################
                     [1m Learning iteration 1521/4000 [0m

                       Computation: 3100 steps/s (collection: 0.509s, learning 2.133s)
               Value function loss: 79719.9272
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 7999.32
               Mean episode length: 376.94
                 Mean success rate: 71.50
                  Mean reward/step: 21.28
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 2.64s
                        Total time: 3907.26s
                               ETA: 6364.1s

################################################################################
                     [1m Learning iteration 1522/4000 [0m

                       Computation: 3188 steps/s (collection: 0.474s, learning 2.095s)
               Value function loss: 45018.5281
                    Surrogate loss: 0.0132
             Mean action noise std: 0.92
                       Mean reward: 7813.48
               Mean episode length: 369.31
                 Mean success rate: 69.50
                  Mean reward/step: 21.84
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12476416
                    Iteration time: 2.57s
                        Total time: 3909.83s
                               ETA: 6361.5s

################################################################################
                     [1m Learning iteration 1523/4000 [0m

                       Computation: 3198 steps/s (collection: 0.486s, learning 2.076s)
               Value function loss: 91270.2772
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 7897.59
               Mean episode length: 373.14
                 Mean success rate: 71.00
                  Mean reward/step: 21.67
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 2.56s
                        Total time: 3912.39s
                               ETA: 6358.9s

################################################################################
                     [1m Learning iteration 1524/4000 [0m

                       Computation: 3151 steps/s (collection: 0.485s, learning 2.114s)
               Value function loss: 102617.7533
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 7760.73
               Mean episode length: 365.70
                 Mean success rate: 70.00
                  Mean reward/step: 21.26
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12492800
                    Iteration time: 2.60s
                        Total time: 3914.99s
                               ETA: 6356.4s

################################################################################
                     [1m Learning iteration 1525/4000 [0m

                       Computation: 3164 steps/s (collection: 0.484s, learning 2.105s)
               Value function loss: 73019.8120
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 7517.85
               Mean episode length: 359.85
                 Mean success rate: 69.00
                  Mean reward/step: 20.74
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 2.59s
                        Total time: 3917.58s
                               ETA: 6353.9s

################################################################################
                     [1m Learning iteration 1526/4000 [0m

                       Computation: 3198 steps/s (collection: 0.476s, learning 2.086s)
               Value function loss: 60076.4616
                    Surrogate loss: 0.0157
             Mean action noise std: 0.92
                       Mean reward: 7098.33
               Mean episode length: 345.71
                 Mean success rate: 65.50
                  Mean reward/step: 21.08
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12509184
                    Iteration time: 2.56s
                        Total time: 3920.14s
                               ETA: 6351.3s

################################################################################
                     [1m Learning iteration 1527/4000 [0m

                       Computation: 3162 steps/s (collection: 0.497s, learning 2.094s)
               Value function loss: 62894.4121
                    Surrogate loss: 0.0169
             Mean action noise std: 0.92
                       Mean reward: 7011.28
               Mean episode length: 338.75
                 Mean success rate: 64.50
                  Mean reward/step: 21.86
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 2.59s
                        Total time: 3922.73s
                               ETA: 6348.8s

################################################################################
                     [1m Learning iteration 1528/4000 [0m

                       Computation: 3208 steps/s (collection: 0.499s, learning 2.054s)
               Value function loss: 99516.5946
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 7221.54
               Mean episode length: 350.07
                 Mean success rate: 65.50
                  Mean reward/step: 21.97
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12525568
                    Iteration time: 2.55s
                        Total time: 3925.28s
                               ETA: 6346.2s

################################################################################
                     [1m Learning iteration 1529/4000 [0m

                       Computation: 3170 steps/s (collection: 0.496s, learning 2.088s)
               Value function loss: 65272.7319
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 7424.63
               Mean episode length: 352.11
                 Mean success rate: 67.00
                  Mean reward/step: 21.47
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 2.58s
                        Total time: 3927.86s
                               ETA: 6343.6s

################################################################################
                     [1m Learning iteration 1530/4000 [0m

                       Computation: 3205 steps/s (collection: 0.487s, learning 2.069s)
               Value function loss: 128235.9434
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 7496.08
               Mean episode length: 354.78
                 Mean success rate: 67.50
                  Mean reward/step: 20.76
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 12541952
                    Iteration time: 2.56s
                        Total time: 3930.42s
                               ETA: 6341.0s

################################################################################
                     [1m Learning iteration 1531/4000 [0m

                       Computation: 3187 steps/s (collection: 0.487s, learning 2.084s)
               Value function loss: 74253.2807
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 7454.11
               Mean episode length: 353.89
                 Mean success rate: 66.50
                  Mean reward/step: 20.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 2.57s
                        Total time: 3932.99s
                               ETA: 6338.5s

################################################################################
                     [1m Learning iteration 1532/4000 [0m

                       Computation: 3176 steps/s (collection: 0.518s, learning 2.061s)
               Value function loss: 79889.6442
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 7344.21
               Mean episode length: 351.69
                 Mean success rate: 66.00
                  Mean reward/step: 20.94
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12558336
                    Iteration time: 2.58s
                        Total time: 3935.57s
                               ETA: 6335.9s

################################################################################
                     [1m Learning iteration 1533/4000 [0m

                       Computation: 3186 steps/s (collection: 0.493s, learning 2.078s)
               Value function loss: 98612.4787
                    Surrogate loss: 0.0166
             Mean action noise std: 0.92
                       Mean reward: 7735.18
               Mean episode length: 360.08
                 Mean success rate: 68.00
                  Mean reward/step: 21.32
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 2.57s
                        Total time: 3938.14s
                               ETA: 6333.4s

################################################################################
                     [1m Learning iteration 1534/4000 [0m

                       Computation: 3250 steps/s (collection: 0.483s, learning 2.038s)
               Value function loss: 98672.1510
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 7792.57
               Mean episode length: 359.67
                 Mean success rate: 68.50
                  Mean reward/step: 20.60
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12574720
                    Iteration time: 2.52s
                        Total time: 3940.66s
                               ETA: 6330.7s

################################################################################
                     [1m Learning iteration 1535/4000 [0m

                       Computation: 3222 steps/s (collection: 0.476s, learning 2.066s)
               Value function loss: 73019.8637
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 8016.98
               Mean episode length: 365.25
                 Mean success rate: 69.50
                  Mean reward/step: 20.43
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 2.54s
                        Total time: 3943.20s
                               ETA: 6328.1s

################################################################################
                     [1m Learning iteration 1536/4000 [0m

                       Computation: 3240 steps/s (collection: 0.486s, learning 2.042s)
               Value function loss: 93730.7912
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 7980.01
               Mean episode length: 365.94
                 Mean success rate: 70.00
                  Mean reward/step: 21.49
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12591104
                    Iteration time: 2.53s
                        Total time: 3945.73s
                               ETA: 6325.5s

################################################################################
                     [1m Learning iteration 1537/4000 [0m

                       Computation: 3235 steps/s (collection: 0.474s, learning 2.058s)
               Value function loss: 66675.9853
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 8121.78
               Mean episode length: 372.94
                 Mean success rate: 71.50
                  Mean reward/step: 21.16
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 2.53s
                        Total time: 3948.26s
                               ETA: 6322.9s

################################################################################
                     [1m Learning iteration 1538/4000 [0m

                       Computation: 3223 steps/s (collection: 0.466s, learning 2.075s)
               Value function loss: 67605.8234
                    Surrogate loss: 0.0199
             Mean action noise std: 0.92
                       Mean reward: 8110.89
               Mean episode length: 369.70
                 Mean success rate: 71.00
                  Mean reward/step: 22.00
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12607488
                    Iteration time: 2.54s
                        Total time: 3950.80s
                               ETA: 6320.3s

################################################################################
                     [1m Learning iteration 1539/4000 [0m

                       Computation: 3225 steps/s (collection: 0.471s, learning 2.069s)
               Value function loss: 82712.2508
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 7689.73
               Mean episode length: 362.26
                 Mean success rate: 67.50
                  Mean reward/step: 22.16
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 2.54s
                        Total time: 3953.34s
                               ETA: 6317.6s

################################################################################
                     [1m Learning iteration 1540/4000 [0m

                       Computation: 3203 steps/s (collection: 0.466s, learning 2.091s)
               Value function loss: 95537.9895
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 7714.31
               Mean episode length: 359.60
                 Mean success rate: 67.00
                  Mean reward/step: 21.52
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 12623872
                    Iteration time: 2.56s
                        Total time: 3955.90s
                               ETA: 6315.1s

################################################################################
                     [1m Learning iteration 1541/4000 [0m

                       Computation: 3170 steps/s (collection: 0.506s, learning 2.077s)
               Value function loss: 70371.8459
                    Surrogate loss: 0.0132
             Mean action noise std: 0.92
                       Mean reward: 7532.10
               Mean episode length: 352.11
                 Mean success rate: 65.00
                  Mean reward/step: 20.88
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 2.58s
                        Total time: 3958.49s
                               ETA: 6312.5s

################################################################################
                     [1m Learning iteration 1542/4000 [0m

                       Computation: 3279 steps/s (collection: 0.419s, learning 2.079s)
               Value function loss: 68153.9560
                    Surrogate loss: 0.0169
             Mean action noise std: 0.92
                       Mean reward: 7337.05
               Mean episode length: 347.13
                 Mean success rate: 63.50
                  Mean reward/step: 21.15
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12640256
                    Iteration time: 2.50s
                        Total time: 3960.98s
                               ETA: 6309.8s

################################################################################
                     [1m Learning iteration 1543/4000 [0m

                       Computation: 3153 steps/s (collection: 0.477s, learning 2.121s)
               Value function loss: 82669.0926
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 7079.33
               Mean episode length: 343.42
                 Mean success rate: 62.00
                  Mean reward/step: 21.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 2.60s
                        Total time: 3963.58s
                               ETA: 6307.3s

################################################################################
                     [1m Learning iteration 1544/4000 [0m

                       Computation: 3179 steps/s (collection: 0.483s, learning 2.093s)
               Value function loss: 100200.5988
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 7698.71
               Mean episode length: 363.69
                 Mean success rate: 67.00
                  Mean reward/step: 21.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12656640
                    Iteration time: 2.58s
                        Total time: 3966.16s
                               ETA: 6304.8s

################################################################################
                     [1m Learning iteration 1545/4000 [0m

                       Computation: 3210 steps/s (collection: 0.485s, learning 2.066s)
               Value function loss: 94618.6674
                    Surrogate loss: 0.0126
             Mean action noise std: 0.92
                       Mean reward: 7481.95
               Mean episode length: 355.04
                 Mean success rate: 64.50
                  Mean reward/step: 21.78
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 2.55s
                        Total time: 3968.71s
                               ETA: 6302.2s

################################################################################
                     [1m Learning iteration 1546/4000 [0m

                       Computation: 3203 steps/s (collection: 0.494s, learning 2.063s)
               Value function loss: 108411.4682
                    Surrogate loss: 0.0127
             Mean action noise std: 0.92
                       Mean reward: 7393.84
               Mean episode length: 350.86
                 Mean success rate: 64.50
                  Mean reward/step: 21.15
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12673024
                    Iteration time: 2.56s
                        Total time: 3971.27s
                               ETA: 6299.6s

################################################################################
                     [1m Learning iteration 1547/4000 [0m

                       Computation: 3209 steps/s (collection: 0.488s, learning 2.065s)
               Value function loss: 46729.9105
                    Surrogate loss: 0.0157
             Mean action noise std: 0.92
                       Mean reward: 7383.04
               Mean episode length: 351.95
                 Mean success rate: 65.00
                  Mean reward/step: 21.57
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 2.55s
                        Total time: 3973.82s
                               ETA: 6297.0s

################################################################################
                     [1m Learning iteration 1548/4000 [0m

                       Computation: 3194 steps/s (collection: 0.475s, learning 2.090s)
               Value function loss: 106454.7350
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 7336.41
               Mean episode length: 352.21
                 Mean success rate: 64.50
                  Mean reward/step: 21.25
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 12689408
                    Iteration time: 2.56s
                        Total time: 3976.38s
                               ETA: 6294.4s

################################################################################
                     [1m Learning iteration 1549/4000 [0m

                       Computation: 3210 steps/s (collection: 0.488s, learning 2.063s)
               Value function loss: 113701.9115
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 7674.23
               Mean episode length: 362.70
                 Mean success rate: 67.00
                  Mean reward/step: 20.58
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 2.55s
                        Total time: 3978.93s
                               ETA: 6291.9s

################################################################################
                     [1m Learning iteration 1550/4000 [0m

                       Computation: 3226 steps/s (collection: 0.470s, learning 2.069s)
               Value function loss: 82440.1455
                    Surrogate loss: 0.0168
             Mean action noise std: 0.92
                       Mean reward: 7621.66
               Mean episode length: 358.75
                 Mean success rate: 66.50
                  Mean reward/step: 19.89
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12705792
                    Iteration time: 2.54s
                        Total time: 3981.47s
                               ETA: 6289.2s

################################################################################
                     [1m Learning iteration 1551/4000 [0m

                       Computation: 3270 steps/s (collection: 0.455s, learning 2.050s)
               Value function loss: 90365.1156
                    Surrogate loss: 0.0169
             Mean action noise std: 0.92
                       Mean reward: 7768.21
               Mean episode length: 357.44
                 Mean success rate: 66.50
                  Mean reward/step: 19.78
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 2.50s
                        Total time: 3983.98s
                               ETA: 6286.6s

################################################################################
                     [1m Learning iteration 1552/4000 [0m

                       Computation: 3213 steps/s (collection: 0.473s, learning 2.076s)
               Value function loss: 80438.5122
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 7356.02
               Mean episode length: 341.51
                 Mean success rate: 63.50
                  Mean reward/step: 19.76
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12722176
                    Iteration time: 2.55s
                        Total time: 3986.53s
                               ETA: 6284.0s

################################################################################
                     [1m Learning iteration 1553/4000 [0m

                       Computation: 3240 steps/s (collection: 0.438s, learning 2.090s)
               Value function loss: 43531.9455
                    Surrogate loss: 0.0182
             Mean action noise std: 0.92
                       Mean reward: 7238.04
               Mean episode length: 335.39
                 Mean success rate: 62.50
                  Mean reward/step: 20.24
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 2.53s
                        Total time: 3989.06s
                               ETA: 6281.4s

################################################################################
                     [1m Learning iteration 1554/4000 [0m

                       Computation: 3180 steps/s (collection: 0.516s, learning 2.060s)
               Value function loss: 99941.3328
                    Surrogate loss: 0.0173
             Mean action noise std: 0.92
                       Mean reward: 6897.17
               Mean episode length: 327.93
                 Mean success rate: 60.50
                  Mean reward/step: 21.00
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12738560
                    Iteration time: 2.58s
                        Total time: 3991.63s
                               ETA: 6278.8s

################################################################################
                     [1m Learning iteration 1555/4000 [0m

                       Computation: 3235 steps/s (collection: 0.445s, learning 2.087s)
               Value function loss: 107553.0275
                    Surrogate loss: 0.0148
             Mean action noise std: 0.92
                       Mean reward: 7116.98
               Mean episode length: 333.38
                 Mean success rate: 61.50
                  Mean reward/step: 20.14
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 2.53s
                        Total time: 3994.16s
                               ETA: 6276.2s

################################################################################
                     [1m Learning iteration 1556/4000 [0m

                       Computation: 3125 steps/s (collection: 0.502s, learning 2.120s)
               Value function loss: 90332.9696
                    Surrogate loss: 0.0127
             Mean action noise std: 0.92
                       Mean reward: 7155.03
               Mean episode length: 339.42
                 Mean success rate: 63.00
                  Mean reward/step: 19.88
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12754944
                    Iteration time: 2.62s
                        Total time: 3996.78s
                               ETA: 6273.7s

################################################################################
                     [1m Learning iteration 1557/4000 [0m

                       Computation: 3231 steps/s (collection: 0.467s, learning 2.068s)
               Value function loss: 66161.5665
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 7157.62
               Mean episode length: 340.50
                 Mean success rate: 63.00
                  Mean reward/step: 19.98
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 2.53s
                        Total time: 3999.32s
                               ETA: 6271.1s

################################################################################
                     [1m Learning iteration 1558/4000 [0m

                       Computation: 3267 steps/s (collection: 0.469s, learning 2.039s)
               Value function loss: 64690.3960
                    Surrogate loss: 0.0141
             Mean action noise std: 0.92
                       Mean reward: 7123.02
               Mean episode length: 341.44
                 Mean success rate: 63.50
                  Mean reward/step: 20.69
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12771328
                    Iteration time: 2.51s
                        Total time: 4001.83s
                               ETA: 6268.4s

################################################################################
                     [1m Learning iteration 1559/4000 [0m

                       Computation: 3134 steps/s (collection: 0.521s, learning 2.093s)
               Value function loss: 89247.9571
                    Surrogate loss: 0.0128
             Mean action noise std: 0.92
                       Mean reward: 7002.85
               Mean episode length: 343.87
                 Mean success rate: 63.00
                  Mean reward/step: 21.16
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 2.61s
                        Total time: 4004.44s
                               ETA: 6265.9s

################################################################################
                     [1m Learning iteration 1560/4000 [0m

                       Computation: 3168 steps/s (collection: 0.495s, learning 2.090s)
               Value function loss: 75215.2760
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 7113.74
               Mean episode length: 350.01
                 Mean success rate: 63.50
                  Mean reward/step: 21.26
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12787712
                    Iteration time: 2.59s
                        Total time: 4007.03s
                               ETA: 6263.4s

################################################################################
                     [1m Learning iteration 1561/4000 [0m

                       Computation: 3197 steps/s (collection: 0.475s, learning 2.087s)
               Value function loss: 86735.1618
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 7028.36
               Mean episode length: 349.86
                 Mean success rate: 63.50
                  Mean reward/step: 21.16
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 2.56s
                        Total time: 4009.59s
                               ETA: 6260.8s

################################################################################
                     [1m Learning iteration 1562/4000 [0m

                       Computation: 3247 steps/s (collection: 0.454s, learning 2.069s)
               Value function loss: 87120.6631
                    Surrogate loss: 0.0175
             Mean action noise std: 0.91
                       Mean reward: 7304.04
               Mean episode length: 362.08
                 Mean success rate: 65.00
                  Mean reward/step: 21.01
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12804096
                    Iteration time: 2.52s
                        Total time: 4012.11s
                               ETA: 6258.2s

################################################################################
                     [1m Learning iteration 1563/4000 [0m

                       Computation: 3267 steps/s (collection: 0.439s, learning 2.068s)
               Value function loss: 82144.1441
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 7240.04
               Mean episode length: 356.51
                 Mean success rate: 63.50
                  Mean reward/step: 21.13
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 2.51s
                        Total time: 4014.62s
                               ETA: 6255.5s

################################################################################
                     [1m Learning iteration 1564/4000 [0m

                       Computation: 3263 steps/s (collection: 0.461s, learning 2.049s)
               Value function loss: 102283.6760
                    Surrogate loss: 0.0121
             Mean action noise std: 0.92
                       Mean reward: 7130.00
               Mean episode length: 356.32
                 Mean success rate: 64.00
                  Mean reward/step: 20.85
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 12820480
                    Iteration time: 2.51s
                        Total time: 4017.13s
                               ETA: 6252.9s

################################################################################
                     [1m Learning iteration 1565/4000 [0m

                       Computation: 3254 steps/s (collection: 0.469s, learning 2.048s)
               Value function loss: 90519.5106
                    Surrogate loss: 0.0152
             Mean action noise std: 0.92
                       Mean reward: 7204.59
               Mean episode length: 356.54
                 Mean success rate: 64.50
                  Mean reward/step: 20.68
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 2.52s
                        Total time: 4019.65s
                               ETA: 6250.2s

################################################################################
                     [1m Learning iteration 1566/4000 [0m

                       Computation: 3323 steps/s (collection: 0.448s, learning 2.017s)
               Value function loss: 88230.0933
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 7154.00
               Mean episode length: 353.90
                 Mean success rate: 64.50
                  Mean reward/step: 20.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12836864
                    Iteration time: 2.46s
                        Total time: 4022.11s
                               ETA: 6247.5s

################################################################################
                     [1m Learning iteration 1567/4000 [0m

                       Computation: 3264 steps/s (collection: 0.456s, learning 2.053s)
               Value function loss: 89532.4631
                    Surrogate loss: 0.0159
             Mean action noise std: 0.91
                       Mean reward: 7380.42
               Mean episode length: 363.18
                 Mean success rate: 67.00
                  Mean reward/step: 20.80
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 2.51s
                        Total time: 4024.62s
                               ETA: 6244.8s

################################################################################
                     [1m Learning iteration 1568/4000 [0m

                       Computation: 3206 steps/s (collection: 0.507s, learning 2.048s)
               Value function loss: 68627.4343
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 7156.17
               Mean episode length: 352.68
                 Mean success rate: 65.00
                  Mean reward/step: 20.23
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12853248
                    Iteration time: 2.55s
                        Total time: 4027.17s
                               ETA: 6242.2s

################################################################################
                     [1m Learning iteration 1569/4000 [0m

                       Computation: 3204 steps/s (collection: 0.456s, learning 2.101s)
               Value function loss: 63655.8274
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 6904.61
               Mean episode length: 343.27
                 Mean success rate: 64.00
                  Mean reward/step: 21.45
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 2.56s
                        Total time: 4029.73s
                               ETA: 6239.7s

################################################################################
                     [1m Learning iteration 1570/4000 [0m

                       Computation: 3245 steps/s (collection: 0.457s, learning 2.067s)
               Value function loss: 103775.3897
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 6945.38
               Mean episode length: 337.63
                 Mean success rate: 64.50
                  Mean reward/step: 21.30
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 12869632
                    Iteration time: 2.52s
                        Total time: 4032.26s
                               ETA: 6237.0s

################################################################################
                     [1m Learning iteration 1571/4000 [0m

                       Computation: 3289 steps/s (collection: 0.437s, learning 2.054s)
               Value function loss: 87318.9515
                    Surrogate loss: 0.0134
             Mean action noise std: 0.92
                       Mean reward: 7064.29
               Mean episode length: 345.50
                 Mean success rate: 65.50
                  Mean reward/step: 20.61
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 2.49s
                        Total time: 4034.75s
                               ETA: 6234.3s

################################################################################
                     [1m Learning iteration 1572/4000 [0m

                       Computation: 3264 steps/s (collection: 0.468s, learning 2.041s)
               Value function loss: 88805.6144
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 7443.59
               Mean episode length: 353.58
                 Mean success rate: 67.50
                  Mean reward/step: 20.29
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12886016
                    Iteration time: 2.51s
                        Total time: 4037.25s
                               ETA: 6231.7s

################################################################################
                     [1m Learning iteration 1573/4000 [0m

                       Computation: 3277 steps/s (collection: 0.448s, learning 2.051s)
               Value function loss: 46998.2388
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 7333.35
               Mean episode length: 350.50
                 Mean success rate: 65.50
                  Mean reward/step: 21.15
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 2.50s
                        Total time: 4039.75s
                               ETA: 6229.0s

################################################################################
                     [1m Learning iteration 1574/4000 [0m

                       Computation: 3320 steps/s (collection: 0.441s, learning 2.026s)
               Value function loss: 88106.8159
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 7348.84
               Mean episode length: 353.59
                 Mean success rate: 66.00
                  Mean reward/step: 22.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12902400
                    Iteration time: 2.47s
                        Total time: 4042.22s
                               ETA: 6226.3s

################################################################################
                     [1m Learning iteration 1575/4000 [0m

                       Computation: 3272 steps/s (collection: 0.439s, learning 2.064s)
               Value function loss: 86698.5575
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 7285.74
               Mean episode length: 347.25
                 Mean success rate: 65.50
                  Mean reward/step: 21.89
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 2.50s
                        Total time: 4044.72s
                               ETA: 6223.6s

################################################################################
                     [1m Learning iteration 1576/4000 [0m

                       Computation: 3193 steps/s (collection: 0.479s, learning 2.086s)
               Value function loss: 87412.7298
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 7046.71
               Mean episode length: 335.25
                 Mean success rate: 63.00
                  Mean reward/step: 21.18
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12918784
                    Iteration time: 2.57s
                        Total time: 4047.29s
                               ETA: 6221.1s

################################################################################
                     [1m Learning iteration 1577/4000 [0m

                       Computation: 3226 steps/s (collection: 0.476s, learning 2.062s)
               Value function loss: 84547.1171
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 7590.23
               Mean episode length: 353.98
                 Mean success rate: 67.00
                  Mean reward/step: 21.37
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 2.54s
                        Total time: 4049.83s
                               ETA: 6218.5s

################################################################################
                     [1m Learning iteration 1578/4000 [0m

                       Computation: 3232 steps/s (collection: 0.425s, learning 2.108s)
               Value function loss: 69578.2611
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 7598.38
               Mean episode length: 355.55
                 Mean success rate: 67.50
                  Mean reward/step: 21.33
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12935168
                    Iteration time: 2.53s
                        Total time: 4052.36s
                               ETA: 6215.8s

################################################################################
                     [1m Learning iteration 1579/4000 [0m

                       Computation: 3246 steps/s (collection: 0.450s, learning 2.072s)
               Value function loss: 92062.1545
                    Surrogate loss: 0.0167
             Mean action noise std: 0.92
                       Mean reward: 7387.29
               Mean episode length: 354.07
                 Mean success rate: 66.50
                  Mean reward/step: 21.76
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 2.52s
                        Total time: 4054.89s
                               ETA: 6213.2s

################################################################################
                     [1m Learning iteration 1580/4000 [0m

                       Computation: 3221 steps/s (collection: 0.498s, learning 2.045s)
               Value function loss: 86040.4594
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 7482.42
               Mean episode length: 352.88
                 Mean success rate: 66.00
                  Mean reward/step: 21.99
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12951552
                    Iteration time: 2.54s
                        Total time: 4057.43s
                               ETA: 6210.6s

################################################################################
                     [1m Learning iteration 1581/4000 [0m

                       Computation: 3286 steps/s (collection: 0.479s, learning 2.014s)
               Value function loss: 83792.3818
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 7621.07
               Mean episode length: 359.12
                 Mean success rate: 66.50
                  Mean reward/step: 21.40
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 2.49s
                        Total time: 4059.92s
                               ETA: 6207.9s

################################################################################
                     [1m Learning iteration 1582/4000 [0m

                       Computation: 3259 steps/s (collection: 0.476s, learning 2.037s)
               Value function loss: 79083.8918
                    Surrogate loss: 0.0127
             Mean action noise std: 0.92
                       Mean reward: 7520.06
               Mean episode length: 352.88
                 Mean success rate: 66.50
                  Mean reward/step: 21.09
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12967936
                    Iteration time: 2.51s
                        Total time: 4062.43s
                               ETA: 6205.3s

################################################################################
                     [1m Learning iteration 1583/4000 [0m

                       Computation: 3293 steps/s (collection: 0.442s, learning 2.046s)
               Value function loss: 102461.1878
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 7374.46
               Mean episode length: 347.07
                 Mean success rate: 64.50
                  Mean reward/step: 21.11
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 2.49s
                        Total time: 4064.92s
                               ETA: 6202.6s

################################################################################
                     [1m Learning iteration 1584/4000 [0m

                       Computation: 3301 steps/s (collection: 0.436s, learning 2.046s)
               Value function loss: 52954.8753
                    Surrogate loss: 0.0179
             Mean action noise std: 0.92
                       Mean reward: 7541.74
               Mean episode length: 352.30
                 Mean success rate: 65.00
                  Mean reward/step: 21.16
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12984320
                    Iteration time: 2.48s
                        Total time: 4067.40s
                               ETA: 6199.9s

################################################################################
                     [1m Learning iteration 1585/4000 [0m

                       Computation: 3283 steps/s (collection: 0.449s, learning 2.046s)
               Value function loss: 88926.1601
                    Surrogate loss: 0.0163
             Mean action noise std: 0.92
                       Mean reward: 7811.26
               Mean episode length: 366.38
                 Mean success rate: 67.50
                  Mean reward/step: 21.30
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 2.49s
                        Total time: 4069.90s
                               ETA: 6197.2s

################################################################################
                     [1m Learning iteration 1586/4000 [0m

                       Computation: 3199 steps/s (collection: 0.488s, learning 2.072s)
               Value function loss: 112600.1964
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 7756.69
               Mean episode length: 365.15
                 Mean success rate: 66.50
                  Mean reward/step: 20.37
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 13000704
                    Iteration time: 2.56s
                        Total time: 4072.46s
                               ETA: 6194.7s

################################################################################
                     [1m Learning iteration 1587/4000 [0m

                       Computation: 3191 steps/s (collection: 0.491s, learning 2.076s)
               Value function loss: 107681.8740
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 8160.69
               Mean episode length: 383.50
                 Mean success rate: 69.50
                  Mean reward/step: 19.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 2.57s
                        Total time: 4075.03s
                               ETA: 6192.1s

################################################################################
                     [1m Learning iteration 1588/4000 [0m

                       Computation: 3097 steps/s (collection: 0.493s, learning 2.152s)
               Value function loss: 59748.0881
                    Surrogate loss: 0.0152
             Mean action noise std: 0.92
                       Mean reward: 8031.57
               Mean episode length: 377.53
                 Mean success rate: 68.50
                  Mean reward/step: 20.09
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13017088
                    Iteration time: 2.64s
                        Total time: 4077.67s
                               ETA: 6189.6s

################################################################################
                     [1m Learning iteration 1589/4000 [0m

                       Computation: 3197 steps/s (collection: 0.471s, learning 2.091s)
               Value function loss: 60282.0264
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 7932.31
               Mean episode length: 372.47
                 Mean success rate: 68.00
                  Mean reward/step: 21.25
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 2.56s
                        Total time: 4080.23s
                               ETA: 6187.1s

################################################################################
                     [1m Learning iteration 1590/4000 [0m

                       Computation: 3178 steps/s (collection: 0.484s, learning 2.093s)
               Value function loss: 97542.2497
                    Surrogate loss: 0.0169
             Mean action noise std: 0.92
                       Mean reward: 7863.63
               Mean episode length: 370.19
                 Mean success rate: 68.50
                  Mean reward/step: 21.16
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13033472
                    Iteration time: 2.58s
                        Total time: 4082.81s
                               ETA: 6184.5s

################################################################################
                     [1m Learning iteration 1591/4000 [0m

                       Computation: 3240 steps/s (collection: 0.457s, learning 2.071s)
               Value function loss: 72635.0859
                    Surrogate loss: 0.0117
             Mean action noise std: 0.92
                       Mean reward: 7613.46
               Mean episode length: 365.50
                 Mean success rate: 67.00
                  Mean reward/step: 20.62
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 2.53s
                        Total time: 4085.34s
                               ETA: 6181.9s

################################################################################
                     [1m Learning iteration 1592/4000 [0m

                       Computation: 3186 steps/s (collection: 0.464s, learning 2.107s)
               Value function loss: 73636.8647
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 7482.29
               Mean episode length: 361.42
                 Mean success rate: 67.00
                  Mean reward/step: 20.45
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13049856
                    Iteration time: 2.57s
                        Total time: 4087.91s
                               ETA: 6179.3s

################################################################################
                     [1m Learning iteration 1593/4000 [0m

                       Computation: 3186 steps/s (collection: 0.449s, learning 2.122s)
               Value function loss: 89951.3806
                    Surrogate loss: 0.0128
             Mean action noise std: 0.92
                       Mean reward: 7509.02
               Mean episode length: 363.30
                 Mean success rate: 68.00
                  Mean reward/step: 20.74
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 2.57s
                        Total time: 4090.48s
                               ETA: 6176.8s

################################################################################
                     [1m Learning iteration 1594/4000 [0m

                       Computation: 3292 steps/s (collection: 0.441s, learning 2.047s)
               Value function loss: 86896.0802
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 7013.49
               Mean episode length: 345.55
                 Mean success rate: 64.50
                  Mean reward/step: 21.02
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13066240
                    Iteration time: 2.49s
                        Total time: 4092.97s
                               ETA: 6174.1s

################################################################################
                     [1m Learning iteration 1595/4000 [0m

                       Computation: 3213 steps/s (collection: 0.483s, learning 2.066s)
               Value function loss: 70482.1496
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 7333.88
               Mean episode length: 354.69
                 Mean success rate: 66.50
                  Mean reward/step: 21.28
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 2.55s
                        Total time: 4095.52s
                               ETA: 6171.5s

################################################################################
                     [1m Learning iteration 1596/4000 [0m

                       Computation: 3258 steps/s (collection: 0.439s, learning 2.075s)
               Value function loss: 77051.5531
                    Surrogate loss: 0.0131
             Mean action noise std: 0.92
                       Mean reward: 7158.63
               Mean episode length: 342.58
                 Mean success rate: 65.00
                  Mean reward/step: 21.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13082624
                    Iteration time: 2.51s
                        Total time: 4098.03s
                               ETA: 6168.9s

################################################################################
                     [1m Learning iteration 1597/4000 [0m

                       Computation: 3215 steps/s (collection: 0.457s, learning 2.091s)
               Value function loss: 94806.7216
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 7188.95
               Mean episode length: 340.61
                 Mean success rate: 64.50
                  Mean reward/step: 22.25
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 2.55s
                        Total time: 4100.58s
                               ETA: 6166.3s

################################################################################
                     [1m Learning iteration 1598/4000 [0m

                       Computation: 3122 steps/s (collection: 0.475s, learning 2.149s)
               Value function loss: 89324.3659
                    Surrogate loss: 0.0152
             Mean action noise std: 0.92
                       Mean reward: 6970.89
               Mean episode length: 336.14
                 Mean success rate: 62.50
                  Mean reward/step: 22.66
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13099008
                    Iteration time: 2.62s
                        Total time: 4103.20s
                               ETA: 6163.8s

################################################################################
                     [1m Learning iteration 1599/4000 [0m

                       Computation: 3118 steps/s (collection: 0.508s, learning 2.118s)
               Value function loss: 79137.0216
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 7477.65
               Mean episode length: 352.88
                 Mean success rate: 66.00
                  Mean reward/step: 22.00
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 2.63s
                        Total time: 4105.83s
                               ETA: 6161.3s

################################################################################
                     [1m Learning iteration 1600/4000 [0m

                       Computation: 3199 steps/s (collection: 0.475s, learning 2.085s)
               Value function loss: 72505.6430
                    Surrogate loss: 0.0197
             Mean action noise std: 0.92
                       Mean reward: 7634.50
               Mean episode length: 355.18
                 Mean success rate: 67.00
                  Mean reward/step: 22.33
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13115392
                    Iteration time: 2.56s
                        Total time: 4108.39s
                               ETA: 6158.7s

################################################################################
                     [1m Learning iteration 1601/4000 [0m

                       Computation: 3166 steps/s (collection: 0.464s, learning 2.123s)
               Value function loss: 82715.0441
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 7598.32
               Mean episode length: 352.94
                 Mean success rate: 66.50
                  Mean reward/step: 22.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 2.59s
                        Total time: 4110.98s
                               ETA: 6156.2s

################################################################################
                     [1m Learning iteration 1602/4000 [0m

                       Computation: 3156 steps/s (collection: 0.470s, learning 2.126s)
               Value function loss: 126198.6064
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 7871.09
               Mean episode length: 362.05
                 Mean success rate: 68.00
                  Mean reward/step: 21.89
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13131776
                    Iteration time: 2.60s
                        Total time: 4113.57s
                               ETA: 6153.7s

################################################################################
                     [1m Learning iteration 1603/4000 [0m

                       Computation: 3090 steps/s (collection: 0.505s, learning 2.146s)
               Value function loss: 82699.3264
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 8134.98
               Mean episode length: 372.76
                 Mean success rate: 70.00
                  Mean reward/step: 21.44
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 2.65s
                        Total time: 4116.22s
                               ETA: 6151.2s

################################################################################
                     [1m Learning iteration 1604/4000 [0m

                       Computation: 3152 steps/s (collection: 0.473s, learning 2.126s)
               Value function loss: 53933.0963
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 7854.42
               Mean episode length: 365.19
                 Mean success rate: 68.50
                  Mean reward/step: 21.84
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13148160
                    Iteration time: 2.60s
                        Total time: 4118.82s
                               ETA: 6148.7s

################################################################################
                     [1m Learning iteration 1605/4000 [0m

                       Computation: 3149 steps/s (collection: 0.485s, learning 2.116s)
               Value function loss: 80466.9664
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 7939.65
               Mean episode length: 366.90
                 Mean success rate: 70.00
                  Mean reward/step: 22.59
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 2.60s
                        Total time: 4121.42s
                               ETA: 6146.2s

################################################################################
                     [1m Learning iteration 1606/4000 [0m

                       Computation: 3215 steps/s (collection: 0.452s, learning 2.095s)
               Value function loss: 112905.9590
                    Surrogate loss: 0.0137
             Mean action noise std: 0.92
                       Mean reward: 7909.69
               Mean episode length: 367.94
                 Mean success rate: 70.00
                  Mean reward/step: 21.80
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 13164544
                    Iteration time: 2.55s
                        Total time: 4123.97s
                               ETA: 6143.6s

################################################################################
                     [1m Learning iteration 1607/4000 [0m

                       Computation: 3134 steps/s (collection: 0.464s, learning 2.150s)
               Value function loss: 81019.3862
                    Surrogate loss: 0.0157
             Mean action noise std: 0.92
                       Mean reward: 8067.89
               Mean episode length: 373.54
                 Mean success rate: 72.50
                  Mean reward/step: 21.34
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 2.61s
                        Total time: 4126.58s
                               ETA: 6141.1s

################################################################################
                     [1m Learning iteration 1608/4000 [0m

                       Computation: 3163 steps/s (collection: 0.483s, learning 2.106s)
               Value function loss: 117076.3668
                    Surrogate loss: 0.0155
             Mean action noise std: 0.92
                       Mean reward: 8288.90
               Mean episode length: 378.54
                 Mean success rate: 74.00
                  Mean reward/step: 21.81
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13180928
                    Iteration time: 2.59s
                        Total time: 4129.17s
                               ETA: 6138.6s

################################################################################
                     [1m Learning iteration 1609/4000 [0m

                       Computation: 3232 steps/s (collection: 0.465s, learning 2.069s)
               Value function loss: 68822.4310
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 8150.64
               Mean episode length: 371.00
                 Mean success rate: 73.00
                  Mean reward/step: 21.86
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 2.53s
                        Total time: 4131.71s
                               ETA: 6136.0s

################################################################################
                     [1m Learning iteration 1610/4000 [0m

                       Computation: 3211 steps/s (collection: 0.481s, learning 2.070s)
               Value function loss: 91822.5329
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 8294.45
               Mean episode length: 373.50
                 Mean success rate: 74.00
                  Mean reward/step: 21.90
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13197312
                    Iteration time: 2.55s
                        Total time: 4134.26s
                               ETA: 6133.4s

################################################################################
                     [1m Learning iteration 1611/4000 [0m

                       Computation: 3183 steps/s (collection: 0.490s, learning 2.083s)
               Value function loss: 82520.2669
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 8224.45
               Mean episode length: 372.60
                 Mean success rate: 73.50
                  Mean reward/step: 21.53
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 2.57s
                        Total time: 4136.83s
                               ETA: 6130.8s

################################################################################
                     [1m Learning iteration 1612/4000 [0m

                       Computation: 3253 steps/s (collection: 0.464s, learning 2.054s)
               Value function loss: 58939.9917
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 8454.72
               Mean episode length: 380.62
                 Mean success rate: 75.00
                  Mean reward/step: 21.94
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 13213696
                    Iteration time: 2.52s
                        Total time: 4139.35s
                               ETA: 6128.2s

################################################################################
                     [1m Learning iteration 1613/4000 [0m

                       Computation: 3250 steps/s (collection: 0.464s, learning 2.057s)
               Value function loss: 101530.4666
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 8453.19
               Mean episode length: 379.06
                 Mean success rate: 75.00
                  Mean reward/step: 21.80
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 2.52s
                        Total time: 4141.87s
                               ETA: 6125.6s

################################################################################
                     [1m Learning iteration 1614/4000 [0m

                       Computation: 3194 steps/s (collection: 0.500s, learning 2.064s)
               Value function loss: 96326.8336
                    Surrogate loss: 0.0137
             Mean action noise std: 0.92
                       Mean reward: 8417.00
               Mean episode length: 382.10
                 Mean success rate: 74.00
                  Mean reward/step: 21.49
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13230080
                    Iteration time: 2.56s
                        Total time: 4144.43s
                               ETA: 6123.0s

################################################################################
                     [1m Learning iteration 1615/4000 [0m

                       Computation: 3234 steps/s (collection: 0.487s, learning 2.046s)
               Value function loss: 37822.9976
                    Surrogate loss: 0.0182
             Mean action noise std: 0.92
                       Mean reward: 8130.03
               Mean episode length: 372.07
                 Mean success rate: 71.50
                  Mean reward/step: 22.10
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 2.53s
                        Total time: 4146.97s
                               ETA: 6120.4s

################################################################################
                     [1m Learning iteration 1616/4000 [0m

                       Computation: 3249 steps/s (collection: 0.456s, learning 2.065s)
               Value function loss: 96425.2592
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 8498.88
               Mean episode length: 383.56
                 Mean success rate: 74.00
                  Mean reward/step: 22.87
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13246464
                    Iteration time: 2.52s
                        Total time: 4149.49s
                               ETA: 6117.7s

################################################################################
                     [1m Learning iteration 1617/4000 [0m

                       Computation: 3185 steps/s (collection: 0.475s, learning 2.096s)
               Value function loss: 92867.0667
                    Surrogate loss: 0.0166
             Mean action noise std: 0.92
                       Mean reward: 8526.24
               Mean episode length: 385.41
                 Mean success rate: 74.00
                  Mean reward/step: 22.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 2.57s
                        Total time: 4152.06s
                               ETA: 6115.2s

################################################################################
                     [1m Learning iteration 1618/4000 [0m

                       Computation: 3273 steps/s (collection: 0.448s, learning 2.055s)
               Value function loss: 84134.6120
                    Surrogate loss: 0.0170
             Mean action noise std: 0.92
                       Mean reward: 8237.19
               Mean episode length: 374.82
                 Mean success rate: 71.50
                  Mean reward/step: 21.76
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13262848
                    Iteration time: 2.50s
                        Total time: 4154.56s
                               ETA: 6112.5s

################################################################################
                     [1m Learning iteration 1619/4000 [0m

                       Computation: 3155 steps/s (collection: 0.508s, learning 2.088s)
               Value function loss: 88456.7883
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 8204.36
               Mean episode length: 375.95
                 Mean success rate: 72.50
                  Mean reward/step: 21.75
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 2.60s
                        Total time: 4157.16s
                               ETA: 6110.0s

################################################################################
                     [1m Learning iteration 1620/4000 [0m

                       Computation: 3205 steps/s (collection: 0.480s, learning 2.075s)
               Value function loss: 44144.9422
                    Surrogate loss: 0.0169
             Mean action noise std: 0.92
                       Mean reward: 8127.05
               Mean episode length: 374.42
                 Mean success rate: 72.00
                  Mean reward/step: 22.26
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 13279232
                    Iteration time: 2.56s
                        Total time: 4159.71s
                               ETA: 6107.4s

################################################################################
                     [1m Learning iteration 1621/4000 [0m

                       Computation: 3171 steps/s (collection: 0.492s, learning 2.091s)
               Value function loss: 123878.5510
                    Surrogate loss: 0.0134
             Mean action noise std: 0.92
                       Mean reward: 7927.43
               Mean episode length: 365.71
                 Mean success rate: 70.50
                  Mean reward/step: 21.98
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 2.58s
                        Total time: 4162.30s
                               ETA: 6104.9s

################################################################################
                     [1m Learning iteration 1622/4000 [0m

                       Computation: 3115 steps/s (collection: 0.498s, learning 2.132s)
               Value function loss: 104422.2437
                    Surrogate loss: 0.0141
             Mean action noise std: 0.92
                       Mean reward: 8070.54
               Mean episode length: 369.42
                 Mean success rate: 71.50
                  Mean reward/step: 21.46
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13295616
                    Iteration time: 2.63s
                        Total time: 4164.93s
                               ETA: 6102.4s

################################################################################
                     [1m Learning iteration 1623/4000 [0m

                       Computation: 3170 steps/s (collection: 0.470s, learning 2.114s)
               Value function loss: 76394.8219
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 8132.16
               Mean episode length: 370.02
                 Mean success rate: 71.50
                  Mean reward/step: 21.06
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 2.58s
                        Total time: 4167.51s
                               ETA: 6099.9s

################################################################################
                     [1m Learning iteration 1624/4000 [0m

                       Computation: 3121 steps/s (collection: 0.532s, learning 2.092s)
               Value function loss: 87092.7959
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 8297.53
               Mean episode length: 375.54
                 Mean success rate: 73.00
                  Mean reward/step: 20.96
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13312000
                    Iteration time: 2.62s
                        Total time: 4170.13s
                               ETA: 6097.4s

################################################################################
                     [1m Learning iteration 1625/4000 [0m

                       Computation: 3133 steps/s (collection: 0.510s, learning 2.104s)
               Value function loss: 99405.8348
                    Surrogate loss: 0.0148
             Mean action noise std: 0.92
                       Mean reward: 7570.68
               Mean episode length: 354.05
                 Mean success rate: 68.00
                  Mean reward/step: 20.48
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 2.61s
                        Total time: 4172.75s
                               ETA: 6094.9s

################################################################################
                     [1m Learning iteration 1626/4000 [0m

                       Computation: 3158 steps/s (collection: 0.508s, learning 2.086s)
               Value function loss: 65109.8386
                    Surrogate loss: 0.0176
             Mean action noise std: 0.92
                       Mean reward: 7644.38
               Mean episode length: 358.79
                 Mean success rate: 69.00
                  Mean reward/step: 20.46
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13328384
                    Iteration time: 2.59s
                        Total time: 4175.34s
                               ETA: 6092.4s

################################################################################
                     [1m Learning iteration 1627/4000 [0m

                       Computation: 3075 steps/s (collection: 0.519s, learning 2.145s)
               Value function loss: 63948.7020
                    Surrogate loss: 0.0166
             Mean action noise std: 0.92
                       Mean reward: 7527.89
               Mean episode length: 353.07
                 Mean success rate: 67.50
                  Mean reward/step: 20.94
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 2.66s
                        Total time: 4178.00s
                               ETA: 6089.9s

################################################################################
                     [1m Learning iteration 1628/4000 [0m

                       Computation: 3118 steps/s (collection: 0.495s, learning 2.132s)
               Value function loss: 74144.9968
                    Surrogate loss: 0.0116
             Mean action noise std: 0.92
                       Mean reward: 7501.50
               Mean episode length: 353.01
                 Mean success rate: 67.50
                  Mean reward/step: 21.38
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13344768
                    Iteration time: 2.63s
                        Total time: 4180.63s
                               ETA: 6087.5s

################################################################################
                     [1m Learning iteration 1629/4000 [0m

                       Computation: 3129 steps/s (collection: 0.519s, learning 2.099s)
               Value function loss: 91676.5998
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 7555.32
               Mean episode length: 352.30
                 Mean success rate: 67.00
                  Mean reward/step: 21.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 2.62s
                        Total time: 4183.25s
                               ETA: 6085.0s

################################################################################
                     [1m Learning iteration 1630/4000 [0m

                       Computation: 3195 steps/s (collection: 0.466s, learning 2.098s)
               Value function loss: 77812.3172
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 7643.06
               Mean episode length: 357.21
                 Mean success rate: 67.50
                  Mean reward/step: 20.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13361152
                    Iteration time: 2.56s
                        Total time: 4185.81s
                               ETA: 6082.4s

################################################################################
                     [1m Learning iteration 1631/4000 [0m

                       Computation: 3178 steps/s (collection: 0.464s, learning 2.113s)
               Value function loss: 66947.0869
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 7154.12
               Mean episode length: 339.94
                 Mean success rate: 64.00
                  Mean reward/step: 21.70
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 2.58s
                        Total time: 4188.39s
                               ETA: 6079.8s

################################################################################
                     [1m Learning iteration 1632/4000 [0m

                       Computation: 3134 steps/s (collection: 0.523s, learning 2.091s)
               Value function loss: 68795.8137
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 6913.48
               Mean episode length: 330.06
                 Mean success rate: 61.00
                  Mean reward/step: 22.03
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13377536
                    Iteration time: 2.61s
                        Total time: 4191.00s
                               ETA: 6077.3s

################################################################################
                     [1m Learning iteration 1633/4000 [0m

                       Computation: 3164 steps/s (collection: 0.540s, learning 2.049s)
               Value function loss: 78110.9949
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 6956.18
               Mean episode length: 332.30
                 Mean success rate: 61.00
                  Mean reward/step: 22.06
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 2.59s
                        Total time: 4193.59s
                               ETA: 6074.8s

################################################################################
                     [1m Learning iteration 1634/4000 [0m

                       Computation: 3156 steps/s (collection: 0.534s, learning 2.062s)
               Value function loss: 100519.7342
                    Surrogate loss: 0.0126
             Mean action noise std: 0.92
                       Mean reward: 7219.22
               Mean episode length: 336.26
                 Mean success rate: 63.00
                  Mean reward/step: 21.53
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13393920
                    Iteration time: 2.60s
                        Total time: 4196.19s
                               ETA: 6072.3s

################################################################################
                     [1m Learning iteration 1635/4000 [0m

                       Computation: 3153 steps/s (collection: 0.514s, learning 2.083s)
               Value function loss: 65052.1021
                    Surrogate loss: 0.0157
             Mean action noise std: 0.92
                       Mean reward: 7284.36
               Mean episode length: 338.60
                 Mean success rate: 63.00
                  Mean reward/step: 21.12
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 2.60s
                        Total time: 4198.79s
                               ETA: 6069.8s

################################################################################
                     [1m Learning iteration 1636/4000 [0m

                       Computation: 3164 steps/s (collection: 0.545s, learning 2.044s)
               Value function loss: 74576.0934
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 7112.69
               Mean episode length: 335.23
                 Mean success rate: 62.50
                  Mean reward/step: 21.34
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13410304
                    Iteration time: 2.59s
                        Total time: 4201.37s
                               ETA: 6067.2s

################################################################################
                     [1m Learning iteration 1637/4000 [0m

                       Computation: 3195 steps/s (collection: 0.496s, learning 2.068s)
               Value function loss: 108514.6752
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 7062.32
               Mean episode length: 335.80
                 Mean success rate: 62.50
                  Mean reward/step: 21.19
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 2.56s
                        Total time: 4203.94s
                               ETA: 6064.7s

################################################################################
                     [1m Learning iteration 1638/4000 [0m

                       Computation: 3159 steps/s (collection: 0.519s, learning 2.074s)
               Value function loss: 106186.3814
                    Surrogate loss: 0.0100
             Mean action noise std: 0.92
                       Mean reward: 6788.51
               Mean episode length: 324.33
                 Mean success rate: 60.50
                  Mean reward/step: 20.51
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 13426688
                    Iteration time: 2.59s
                        Total time: 4206.53s
                               ETA: 6062.1s

################################################################################
                     [1m Learning iteration 1639/4000 [0m

                       Computation: 3143 steps/s (collection: 0.525s, learning 2.081s)
               Value function loss: 90403.5748
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 7110.26
               Mean episode length: 329.82
                 Mean success rate: 62.50
                  Mean reward/step: 20.62
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 2.61s
                        Total time: 4209.14s
                               ETA: 6059.6s

################################################################################
                     [1m Learning iteration 1640/4000 [0m

                       Computation: 3106 steps/s (collection: 0.551s, learning 2.086s)
               Value function loss: 98297.7344
                    Surrogate loss: 0.0134
             Mean action noise std: 0.92
                       Mean reward: 7689.78
               Mean episode length: 350.81
                 Mean success rate: 67.00
                  Mean reward/step: 19.03
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13443072
                    Iteration time: 2.64s
                        Total time: 4211.77s
                               ETA: 6057.2s

################################################################################
                     [1m Learning iteration 1641/4000 [0m

                       Computation: 3172 steps/s (collection: 0.493s, learning 2.089s)
               Value function loss: 110803.4809
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 7211.54
               Mean episode length: 337.70
                 Mean success rate: 63.50
                  Mean reward/step: 18.83
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 2.58s
                        Total time: 4214.36s
                               ETA: 6054.6s

################################################################################
                     [1m Learning iteration 1642/4000 [0m

                       Computation: 3195 steps/s (collection: 0.506s, learning 2.058s)
               Value function loss: 59465.9980
                    Surrogate loss: 0.0128
             Mean action noise std: 0.92
                       Mean reward: 7235.81
               Mean episode length: 341.54
                 Mean success rate: 64.00
                  Mean reward/step: 19.07
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13459456
                    Iteration time: 2.56s
                        Total time: 4216.92s
                               ETA: 6052.0s

################################################################################
                     [1m Learning iteration 1643/4000 [0m

                       Computation: 3149 steps/s (collection: 0.520s, learning 2.081s)
               Value function loss: 65075.8861
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 6920.93
               Mean episode length: 332.83
                 Mean success rate: 61.00
                  Mean reward/step: 19.86
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 2.60s
                        Total time: 4219.52s
                               ETA: 6049.5s

################################################################################
                     [1m Learning iteration 1644/4000 [0m

                       Computation: 3136 steps/s (collection: 0.496s, learning 2.116s)
               Value function loss: 93677.8612
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 6820.01
               Mean episode length: 326.25
                 Mean success rate: 59.50
                  Mean reward/step: 20.55
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 13475840
                    Iteration time: 2.61s
                        Total time: 4222.13s
                               ETA: 6047.0s

################################################################################
                     [1m Learning iteration 1645/4000 [0m

                       Computation: 3204 steps/s (collection: 0.491s, learning 2.065s)
               Value function loss: 83967.5094
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 6935.19
               Mean episode length: 329.94
                 Mean success rate: 59.50
                  Mean reward/step: 20.89
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 2.56s
                        Total time: 4224.69s
                               ETA: 6044.4s

################################################################################
                     [1m Learning iteration 1646/4000 [0m

                       Computation: 3130 steps/s (collection: 0.489s, learning 2.128s)
               Value function loss: 57653.6347
                    Surrogate loss: 0.0156
             Mean action noise std: 0.92
                       Mean reward: 6763.64
               Mean episode length: 322.90
                 Mean success rate: 57.50
                  Mean reward/step: 21.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13492224
                    Iteration time: 2.62s
                        Total time: 4227.31s
                               ETA: 6041.9s

################################################################################
                     [1m Learning iteration 1647/4000 [0m

                       Computation: 3107 steps/s (collection: 0.501s, learning 2.135s)
               Value function loss: 73977.4475
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 6519.88
               Mean episode length: 321.12
                 Mean success rate: 56.50
                  Mean reward/step: 21.47
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 2.64s
                        Total time: 4229.94s
                               ETA: 6039.5s

################################################################################
                     [1m Learning iteration 1648/4000 [0m

                       Computation: 3124 steps/s (collection: 0.518s, learning 2.103s)
               Value function loss: 81437.5368
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 6044.18
               Mean episode length: 303.12
                 Mean success rate: 52.50
                  Mean reward/step: 21.80
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13508608
                    Iteration time: 2.62s
                        Total time: 4232.56s
                               ETA: 6037.0s

################################################################################
                     [1m Learning iteration 1649/4000 [0m

                       Computation: 3160 steps/s (collection: 0.522s, learning 2.070s)
               Value function loss: 69127.0838
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 6394.96
               Mean episode length: 313.54
                 Mean success rate: 55.00
                  Mean reward/step: 21.82
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 2.59s
                        Total time: 4235.16s
                               ETA: 6034.5s

################################################################################
                     [1m Learning iteration 1650/4000 [0m

                       Computation: 3141 steps/s (collection: 0.507s, learning 2.101s)
               Value function loss: 102664.6268
                    Surrogate loss: 0.0101
             Mean action noise std: 0.92
                       Mean reward: 6782.88
               Mean episode length: 320.83
                 Mean success rate: 57.00
                  Mean reward/step: 21.85
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13524992
                    Iteration time: 2.61s
                        Total time: 4237.76s
                               ETA: 6031.9s

################################################################################
                     [1m Learning iteration 1651/4000 [0m

                       Computation: 3196 steps/s (collection: 0.466s, learning 2.097s)
               Value function loss: 38524.9432
                    Surrogate loss: 0.0124
             Mean action noise std: 0.92
                       Mean reward: 6720.19
               Mean episode length: 321.53
                 Mean success rate: 57.00
                  Mean reward/step: 22.31
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 2.56s
                        Total time: 4240.33s
                               ETA: 6029.4s

################################################################################
                     [1m Learning iteration 1652/4000 [0m

                       Computation: 3213 steps/s (collection: 0.456s, learning 2.093s)
               Value function loss: 80328.8246
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 6710.16
               Mean episode length: 321.94
                 Mean success rate: 57.00
                  Mean reward/step: 23.53
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13541376
                    Iteration time: 2.55s
                        Total time: 4242.88s
                               ETA: 6026.8s

################################################################################
                     [1m Learning iteration 1653/4000 [0m

                       Computation: 3128 steps/s (collection: 0.527s, learning 2.092s)
               Value function loss: 125273.3271
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 7090.55
               Mean episode length: 339.59
                 Mean success rate: 61.50
                  Mean reward/step: 22.74
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 2.62s
                        Total time: 4245.49s
                               ETA: 6024.3s

################################################################################
                     [1m Learning iteration 1654/4000 [0m

                       Computation: 3186 steps/s (collection: 0.499s, learning 2.072s)
               Value function loss: 60644.9485
                    Surrogate loss: 0.0126
             Mean action noise std: 0.92
                       Mean reward: 6823.10
               Mean episode length: 330.99
                 Mean success rate: 60.00
                  Mean reward/step: 21.70
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13557760
                    Iteration time: 2.57s
                        Total time: 4248.07s
                               ETA: 6021.7s

################################################################################
                     [1m Learning iteration 1655/4000 [0m

                       Computation: 3190 steps/s (collection: 0.466s, learning 2.102s)
               Value function loss: 83481.1914
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 7291.40
               Mean episode length: 348.49
                 Mean success rate: 63.50
                  Mean reward/step: 21.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 2.57s
                        Total time: 4250.63s
                               ETA: 6019.2s

################################################################################
                     [1m Learning iteration 1656/4000 [0m

                       Computation: 3145 steps/s (collection: 0.523s, learning 2.081s)
               Value function loss: 112834.7275
                    Surrogate loss: 0.0174
             Mean action noise std: 0.92
                       Mean reward: 7647.01
               Mean episode length: 358.58
                 Mean success rate: 66.50
                  Mean reward/step: 21.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13574144
                    Iteration time: 2.60s
                        Total time: 4253.24s
                               ETA: 6016.7s

################################################################################
                     [1m Learning iteration 1657/4000 [0m

                       Computation: 3183 steps/s (collection: 0.507s, learning 2.066s)
               Value function loss: 106707.2212
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 7587.67
               Mean episode length: 361.25
                 Mean success rate: 68.50
                  Mean reward/step: 20.47
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 2.57s
                        Total time: 4255.81s
                               ETA: 6014.1s

################################################################################
                     [1m Learning iteration 1658/4000 [0m

                       Computation: 3135 steps/s (collection: 0.511s, learning 2.102s)
               Value function loss: 81887.4004
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 7210.23
               Mean episode length: 348.44
                 Mean success rate: 65.50
                  Mean reward/step: 19.99
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13590528
                    Iteration time: 2.61s
                        Total time: 4258.42s
                               ETA: 6011.6s

################################################################################
                     [1m Learning iteration 1659/4000 [0m

                       Computation: 3159 steps/s (collection: 0.512s, learning 2.081s)
               Value function loss: 76268.4125
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 7468.02
               Mean episode length: 351.49
                 Mean success rate: 67.00
                  Mean reward/step: 20.10
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 2.59s
                        Total time: 4261.02s
                               ETA: 6009.1s

################################################################################
                     [1m Learning iteration 1660/4000 [0m

                       Computation: 3129 steps/s (collection: 0.540s, learning 2.078s)
               Value function loss: 106277.6171
                    Surrogate loss: 0.0134
             Mean action noise std: 0.92
                       Mean reward: 7323.46
               Mean episode length: 340.40
                 Mean success rate: 65.00
                  Mean reward/step: 20.47
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13606912
                    Iteration time: 2.62s
                        Total time: 4263.63s
                               ETA: 6006.6s

################################################################################
                     [1m Learning iteration 1661/4000 [0m

                       Computation: 3209 steps/s (collection: 0.444s, learning 2.109s)
               Value function loss: 87844.3654
                    Surrogate loss: 0.0132
             Mean action noise std: 0.92
                       Mean reward: 7241.19
               Mean episode length: 333.71
                 Mean success rate: 63.00
                  Mean reward/step: 19.61
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 2.55s
                        Total time: 4266.19s
                               ETA: 6004.0s

################################################################################
                     [1m Learning iteration 1662/4000 [0m

                       Computation: 3220 steps/s (collection: 0.474s, learning 2.069s)
               Value function loss: 64476.6950
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 7430.40
               Mean episode length: 341.61
                 Mean success rate: 64.50
                  Mean reward/step: 19.83
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13623296
                    Iteration time: 2.54s
                        Total time: 4268.73s
                               ETA: 6001.4s

################################################################################
                     [1m Learning iteration 1663/4000 [0m

                       Computation: 3054 steps/s (collection: 0.531s, learning 2.151s)
               Value function loss: 77874.7021
                    Surrogate loss: 0.0184
             Mean action noise std: 0.92
                       Mean reward: 6932.93
               Mean episode length: 326.42
                 Mean success rate: 61.00
                  Mean reward/step: 20.81
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 2.68s
                        Total time: 4271.41s
                               ETA: 5999.0s

################################################################################
                     [1m Learning iteration 1664/4000 [0m

                       Computation: 3180 steps/s (collection: 0.488s, learning 2.088s)
               Value function loss: 91115.4443
                    Surrogate loss: 0.0196
             Mean action noise std: 0.92
                       Mean reward: 6634.92
               Mean episode length: 312.06
                 Mean success rate: 56.00
                  Mean reward/step: 21.15
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 13639680
                    Iteration time: 2.58s
                        Total time: 4273.99s
                               ETA: 5996.4s

################################################################################
                     [1m Learning iteration 1665/4000 [0m

                       Computation: 3147 steps/s (collection: 0.503s, learning 2.100s)
               Value function loss: 90406.2000
                    Surrogate loss: 0.0177
             Mean action noise std: 0.91
                       Mean reward: 6371.91
               Mean episode length: 304.93
                 Mean success rate: 54.00
                  Mean reward/step: 20.53
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 2.60s
                        Total time: 4276.59s
                               ETA: 5993.9s

################################################################################
                     [1m Learning iteration 1666/4000 [0m

                       Computation: 3227 steps/s (collection: 0.452s, learning 2.086s)
               Value function loss: 66844.2727
                    Surrogate loss: 0.0187
             Mean action noise std: 0.91
                       Mean reward: 6626.43
               Mean episode length: 310.30
                 Mean success rate: 56.00
                  Mean reward/step: 20.49
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13656064
                    Iteration time: 2.54s
                        Total time: 4279.13s
                               ETA: 5991.3s

################################################################################
                     [1m Learning iteration 1667/4000 [0m

                       Computation: 3179 steps/s (collection: 0.490s, learning 2.087s)
               Value function loss: 64015.8528
                    Surrogate loss: 0.0193
             Mean action noise std: 0.91
                       Mean reward: 6303.46
               Mean episode length: 300.95
                 Mean success rate: 54.00
                  Mean reward/step: 21.25
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 2.58s
                        Total time: 4281.70s
                               ETA: 5988.7s

################################################################################
                     [1m Learning iteration 1668/4000 [0m

                       Computation: 3207 steps/s (collection: 0.472s, learning 2.082s)
               Value function loss: 89253.3043
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 6450.50
               Mean episode length: 308.92
                 Mean success rate: 54.50
                  Mean reward/step: 21.68
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13672448
                    Iteration time: 2.55s
                        Total time: 4284.26s
                               ETA: 5986.2s

################################################################################
                     [1m Learning iteration 1669/4000 [0m

                       Computation: 3174 steps/s (collection: 0.501s, learning 2.080s)
               Value function loss: 96566.8664
                    Surrogate loss: 0.0148
             Mean action noise std: 0.91
                       Mean reward: 6593.37
               Mean episode length: 312.60
                 Mean success rate: 56.00
                  Mean reward/step: 21.32
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 2.58s
                        Total time: 4286.84s
                               ETA: 5983.6s

################################################################################
                     [1m Learning iteration 1670/4000 [0m

                       Computation: 3228 steps/s (collection: 0.462s, learning 2.075s)
               Value function loss: 85639.4479
                    Surrogate loss: 0.0170
             Mean action noise std: 0.91
                       Mean reward: 6587.22
               Mean episode length: 311.30
                 Mean success rate: 55.50
                  Mean reward/step: 21.22
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13688832
                    Iteration time: 2.54s
                        Total time: 4289.38s
                               ETA: 5981.0s

################################################################################
                     [1m Learning iteration 1671/4000 [0m

                       Computation: 3240 steps/s (collection: 0.464s, learning 2.064s)
               Value function loss: 69616.4429
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 6737.62
               Mean episode length: 319.35
                 Mean success rate: 57.50
                  Mean reward/step: 20.92
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 2.53s
                        Total time: 4291.91s
                               ETA: 5978.4s

################################################################################
                     [1m Learning iteration 1672/4000 [0m

                       Computation: 3166 steps/s (collection: 0.501s, learning 2.086s)
               Value function loss: 108672.0514
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 6870.07
               Mean episode length: 328.27
                 Mean success rate: 60.50
                  Mean reward/step: 20.74
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 13705216
                    Iteration time: 2.59s
                        Total time: 4294.49s
                               ETA: 5975.8s

################################################################################
                     [1m Learning iteration 1673/4000 [0m

                       Computation: 3129 steps/s (collection: 0.500s, learning 2.117s)
               Value function loss: 92899.0609
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 6926.83
               Mean episode length: 330.05
                 Mean success rate: 61.00
                  Mean reward/step: 19.63
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 2.62s
                        Total time: 4297.11s
                               ETA: 5973.3s

################################################################################
                     [1m Learning iteration 1674/4000 [0m

                       Computation: 3203 steps/s (collection: 0.481s, learning 2.076s)
               Value function loss: 62016.9138
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 6887.65
               Mean episode length: 329.98
                 Mean success rate: 61.00
                  Mean reward/step: 19.86
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13721600
                    Iteration time: 2.56s
                        Total time: 4299.67s
                               ETA: 5970.8s

################################################################################
                     [1m Learning iteration 1675/4000 [0m

                       Computation: 3229 steps/s (collection: 0.478s, learning 2.058s)
               Value function loss: 86214.0018
                    Surrogate loss: 0.0196
             Mean action noise std: 0.91
                       Mean reward: 6949.49
               Mean episode length: 338.16
                 Mean success rate: 62.00
                  Mean reward/step: 20.99
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 2.54s
                        Total time: 4302.20s
                               ETA: 5968.2s

################################################################################
                     [1m Learning iteration 1676/4000 [0m

                       Computation: 3231 steps/s (collection: 0.468s, learning 2.068s)
               Value function loss: 83846.1111
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 6404.79
               Mean episode length: 316.69
                 Mean success rate: 58.00
                  Mean reward/step: 21.37
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13737984
                    Iteration time: 2.54s
                        Total time: 4304.74s
                               ETA: 5965.5s

################################################################################
                     [1m Learning iteration 1677/4000 [0m

                       Computation: 3242 steps/s (collection: 0.479s, learning 2.047s)
               Value function loss: 75198.4865
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 6181.89
               Mean episode length: 309.18
                 Mean success rate: 56.50
                  Mean reward/step: 21.36
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 2.53s
                        Total time: 4307.27s
                               ETA: 5962.9s

################################################################################
                     [1m Learning iteration 1678/4000 [0m

                       Computation: 3112 steps/s (collection: 0.556s, learning 2.076s)
               Value function loss: 70097.8794
                    Surrogate loss: 0.0203
             Mean action noise std: 0.91
                       Mean reward: 5961.46
               Mean episode length: 299.76
                 Mean success rate: 54.50
                  Mean reward/step: 20.96
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13754368
                    Iteration time: 2.63s
                        Total time: 4309.90s
                               ETA: 5960.4s

################################################################################
                     [1m Learning iteration 1679/4000 [0m

                       Computation: 3244 steps/s (collection: 0.442s, learning 2.082s)
               Value function loss: 107285.9016
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 6029.15
               Mean episode length: 299.75
                 Mean success rate: 54.50
                  Mean reward/step: 20.77
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 2.52s
                        Total time: 4312.42s
                               ETA: 5957.8s

################################################################################
                     [1m Learning iteration 1680/4000 [0m

                       Computation: 3218 steps/s (collection: 0.462s, learning 2.083s)
               Value function loss: 118478.4611
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 6358.98
               Mean episode length: 308.78
                 Mean success rate: 56.50
                  Mean reward/step: 19.78
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 13770752
                    Iteration time: 2.55s
                        Total time: 4314.97s
                               ETA: 5955.2s

################################################################################
                     [1m Learning iteration 1681/4000 [0m

                       Computation: 3280 steps/s (collection: 0.424s, learning 2.073s)
               Value function loss: 107338.1026
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 6251.97
               Mean episode length: 302.11
                 Mean success rate: 56.50
                  Mean reward/step: 19.25
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 2.50s
                        Total time: 4317.46s
                               ETA: 5952.6s

################################################################################
                     [1m Learning iteration 1682/4000 [0m

                       Computation: 3193 steps/s (collection: 0.469s, learning 2.096s)
               Value function loss: 66469.7412
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 5858.96
               Mean episode length: 283.32
                 Mean success rate: 53.00
                  Mean reward/step: 19.96
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13787136
                    Iteration time: 2.57s
                        Total time: 4320.03s
                               ETA: 5950.0s

################################################################################
                     [1m Learning iteration 1683/4000 [0m

                       Computation: 3250 steps/s (collection: 0.474s, learning 2.046s)
               Value function loss: 71693.3929
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 6025.27
               Mean episode length: 291.63
                 Mean success rate: 54.00
                  Mean reward/step: 21.20
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 2.52s
                        Total time: 4322.55s
                               ETA: 5947.4s

################################################################################
                     [1m Learning iteration 1684/4000 [0m

                       Computation: 3150 steps/s (collection: 0.513s, learning 2.087s)
               Value function loss: 82304.9232
                    Surrogate loss: 0.0116
             Mean action noise std: 0.91
                       Mean reward: 6072.46
               Mean episode length: 293.36
                 Mean success rate: 54.50
                  Mean reward/step: 21.62
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13803520
                    Iteration time: 2.60s
                        Total time: 4325.15s
                               ETA: 5944.8s

################################################################################
                     [1m Learning iteration 1685/4000 [0m

                       Computation: 3289 steps/s (collection: 0.460s, learning 2.030s)
               Value function loss: 101649.2972
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 6180.66
               Mean episode length: 293.94
                 Mean success rate: 57.00
                  Mean reward/step: 21.51
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 2.49s
                        Total time: 4327.64s
                               ETA: 5942.2s

################################################################################
                     [1m Learning iteration 1686/4000 [0m

                       Computation: 3154 steps/s (collection: 0.481s, learning 2.116s)
               Value function loss: 63301.9872
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 6044.46
               Mean episode length: 288.69
                 Mean success rate: 56.00
                  Mean reward/step: 21.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13819904
                    Iteration time: 2.60s
                        Total time: 4330.24s
                               ETA: 5939.6s

################################################################################
                     [1m Learning iteration 1687/4000 [0m

                       Computation: 3256 steps/s (collection: 0.439s, learning 2.077s)
               Value function loss: 83139.2776
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 6000.15
               Mean episode length: 284.50
                 Mean success rate: 55.50
                  Mean reward/step: 22.20
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 2.52s
                        Total time: 4332.75s
                               ETA: 5937.0s

################################################################################
                     [1m Learning iteration 1688/4000 [0m

                       Computation: 3222 steps/s (collection: 0.469s, learning 2.073s)
               Value function loss: 113541.9478
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 6371.32
               Mean episode length: 303.72
                 Mean success rate: 58.50
                  Mean reward/step: 22.23
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13836288
                    Iteration time: 2.54s
                        Total time: 4335.29s
                               ETA: 5934.4s

################################################################################
                     [1m Learning iteration 1689/4000 [0m

                       Computation: 3166 steps/s (collection: 0.460s, learning 2.127s)
               Value function loss: 80224.1176
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 6306.56
               Mean episode length: 303.10
                 Mean success rate: 57.50
                  Mean reward/step: 21.61
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 2.59s
                        Total time: 4337.88s
                               ETA: 5931.9s

################################################################################
                     [1m Learning iteration 1690/4000 [0m

                       Computation: 3176 steps/s (collection: 0.474s, learning 2.105s)
               Value function loss: 82693.2934
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 6544.83
               Mean episode length: 310.08
                 Mean success rate: 60.00
                  Mean reward/step: 22.21
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13852672
                    Iteration time: 2.58s
                        Total time: 4340.46s
                               ETA: 5929.3s

################################################################################
                     [1m Learning iteration 1691/4000 [0m

                       Computation: 3153 steps/s (collection: 0.498s, learning 2.099s)
               Value function loss: 72848.2407
                    Surrogate loss: 0.0159
             Mean action noise std: 0.91
                       Mean reward: 6225.55
               Mean episode length: 299.16
                 Mean success rate: 58.00
                  Mean reward/step: 22.69
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 2.60s
                        Total time: 4343.06s
                               ETA: 5926.8s

################################################################################
                     [1m Learning iteration 1692/4000 [0m

                       Computation: 3126 steps/s (collection: 0.520s, learning 2.101s)
               Value function loss: 79677.4936
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 6324.09
               Mean episode length: 296.30
                 Mean success rate: 58.00
                  Mean reward/step: 22.82
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13869056
                    Iteration time: 2.62s
                        Total time: 4345.68s
                               ETA: 5924.3s

################################################################################
                     [1m Learning iteration 1693/4000 [0m

                       Computation: 3244 steps/s (collection: 0.434s, learning 2.090s)
               Value function loss: 107179.3248
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 6613.54
               Mean episode length: 309.34
                 Mean success rate: 59.00
                  Mean reward/step: 22.36
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 2.52s
                        Total time: 4348.20s
                               ETA: 5921.7s

################################################################################
                     [1m Learning iteration 1694/4000 [0m

                       Computation: 3253 steps/s (collection: 0.460s, learning 2.058s)
               Value function loss: 90057.9292
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 6926.23
               Mean episode length: 319.46
                 Mean success rate: 62.00
                  Mean reward/step: 22.34
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13885440
                    Iteration time: 2.52s
                        Total time: 4350.72s
                               ETA: 5919.0s

################################################################################
                     [1m Learning iteration 1695/4000 [0m

                       Computation: 3153 steps/s (collection: 0.492s, learning 2.106s)
               Value function loss: 133649.8564
                    Surrogate loss: 0.0127
             Mean action noise std: 0.92
                       Mean reward: 7242.10
               Mean episode length: 329.37
                 Mean success rate: 64.00
                  Mean reward/step: 22.02
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 2.60s
                        Total time: 4353.32s
                               ETA: 5916.5s

################################################################################
                     [1m Learning iteration 1696/4000 [0m

                       Computation: 3185 steps/s (collection: 0.540s, learning 2.031s)
               Value function loss: 91821.8494
                    Surrogate loss: 0.0160
             Mean action noise std: 0.91
                       Mean reward: 6950.25
               Mean episode length: 316.38
                 Mean success rate: 61.50
                  Mean reward/step: 20.74
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13901824
                    Iteration time: 2.57s
                        Total time: 4355.89s
                               ETA: 5914.0s

################################################################################
                     [1m Learning iteration 1697/4000 [0m

                       Computation: 3241 steps/s (collection: 0.463s, learning 2.065s)
               Value function loss: 92658.1703
                    Surrogate loss: 0.0118
             Mean action noise std: 0.92
                       Mean reward: 7222.69
               Mean episode length: 324.58
                 Mean success rate: 63.50
                  Mean reward/step: 19.56
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 2.53s
                        Total time: 4358.42s
                               ETA: 5911.3s

################################################################################
                     [1m Learning iteration 1698/4000 [0m

                       Computation: 3193 steps/s (collection: 0.477s, learning 2.088s)
               Value function loss: 63622.8201
                    Surrogate loss: 0.0173
             Mean action noise std: 0.91
                       Mean reward: 7215.33
               Mean episode length: 325.65
                 Mean success rate: 64.50
                  Mean reward/step: 19.42
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13918208
                    Iteration time: 2.57s
                        Total time: 4360.98s
                               ETA: 5908.8s

################################################################################
                     [1m Learning iteration 1699/4000 [0m

                       Computation: 3185 steps/s (collection: 0.466s, learning 2.106s)
               Value function loss: 77857.3025
                    Surrogate loss: 0.0170
             Mean action noise std: 0.91
                       Mean reward: 7575.43
               Mean episode length: 340.19
                 Mean success rate: 67.50
                  Mean reward/step: 19.72
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 2.57s
                        Total time: 4363.56s
                               ETA: 5906.2s

################################################################################
                     [1m Learning iteration 1700/4000 [0m

                       Computation: 3114 steps/s (collection: 0.509s, learning 2.122s)
               Value function loss: 79997.4195
                    Surrogate loss: 0.0170
             Mean action noise std: 0.92
                       Mean reward: 7547.83
               Mean episode length: 341.37
                 Mean success rate: 68.00
                  Mean reward/step: 20.12
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13934592
                    Iteration time: 2.63s
                        Total time: 4366.19s
                               ETA: 5903.7s

################################################################################
                     [1m Learning iteration 1701/4000 [0m

                       Computation: 3168 steps/s (collection: 0.471s, learning 2.114s)
               Value function loss: 88877.6452
                    Surrogate loss: 0.0132
             Mean action noise std: 0.92
                       Mean reward: 7512.00
               Mean episode length: 339.69
                 Mean success rate: 67.00
                  Mean reward/step: 19.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 2.59s
                        Total time: 4368.77s
                               ETA: 5901.2s

################################################################################
                     [1m Learning iteration 1702/4000 [0m

                       Computation: 3160 steps/s (collection: 0.515s, learning 2.077s)
               Value function loss: 51119.3255
                    Surrogate loss: 0.0132
             Mean action noise std: 0.92
                       Mean reward: 6693.64
               Mean episode length: 311.74
                 Mean success rate: 60.50
                  Mean reward/step: 19.92
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13950976
                    Iteration time: 2.59s
                        Total time: 4371.36s
                               ETA: 5898.6s

################################################################################
                     [1m Learning iteration 1703/4000 [0m

                       Computation: 3199 steps/s (collection: 0.469s, learning 2.092s)
               Value function loss: 63449.1428
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 6171.00
               Mean episode length: 295.59
                 Mean success rate: 58.00
                  Mean reward/step: 20.92
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 2.56s
                        Total time: 4373.92s
                               ETA: 5896.1s

################################################################################
                     [1m Learning iteration 1704/4000 [0m

                       Computation: 3234 steps/s (collection: 0.488s, learning 2.045s)
               Value function loss: 90893.6009
                    Surrogate loss: 0.0141
             Mean action noise std: 0.92
                       Mean reward: 6278.27
               Mean episode length: 299.43
                 Mean success rate: 58.50
                  Mean reward/step: 20.72
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13967360
                    Iteration time: 2.53s
                        Total time: 4376.46s
                               ETA: 5893.5s

################################################################################
                     [1m Learning iteration 1705/4000 [0m

                       Computation: 3226 steps/s (collection: 0.468s, learning 2.072s)
               Value function loss: 49724.4918
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 6274.87
               Mean episode length: 301.42
                 Mean success rate: 59.00
                  Mean reward/step: 20.37
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 2.54s
                        Total time: 4379.00s
                               ETA: 5890.9s

################################################################################
                     [1m Learning iteration 1706/4000 [0m

                       Computation: 3244 steps/s (collection: 0.474s, learning 2.051s)
               Value function loss: 71582.4977
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 6168.08
               Mean episode length: 300.40
                 Mean success rate: 57.00
                  Mean reward/step: 20.60
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13983744
                    Iteration time: 2.52s
                        Total time: 4381.52s
                               ETA: 5888.2s

################################################################################
                     [1m Learning iteration 1707/4000 [0m

                       Computation: 3205 steps/s (collection: 0.482s, learning 2.074s)
               Value function loss: 112132.9107
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 6383.08
               Mean episode length: 307.31
                 Mean success rate: 58.00
                  Mean reward/step: 20.66
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 2.56s
                        Total time: 4384.08s
                               ETA: 5885.6s

################################################################################
                     [1m Learning iteration 1708/4000 [0m

                       Computation: 3190 steps/s (collection: 0.474s, learning 2.094s)
               Value function loss: 80539.5546
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 6357.93
               Mean episode length: 308.43
                 Mean success rate: 59.50
                  Mean reward/step: 20.47
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 14000128
                    Iteration time: 2.57s
                        Total time: 4386.64s
                               ETA: 5883.1s

################################################################################
                     [1m Learning iteration 1709/4000 [0m

                       Computation: 3185 steps/s (collection: 0.486s, learning 2.086s)
               Value function loss: 80676.2336
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 6159.67
               Mean episode length: 304.01
                 Mean success rate: 59.00
                  Mean reward/step: 20.61
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 2.57s
                        Total time: 4389.22s
                               ETA: 5880.5s

################################################################################
                     [1m Learning iteration 1710/4000 [0m

                       Computation: 3237 steps/s (collection: 0.488s, learning 2.042s)
               Value function loss: 74810.9056
                    Surrogate loss: 0.0170
             Mean action noise std: 0.92
                       Mean reward: 6295.95
               Mean episode length: 312.56
                 Mean success rate: 60.50
                  Mean reward/step: 21.81
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14016512
                    Iteration time: 2.53s
                        Total time: 4391.75s
                               ETA: 5877.9s

################################################################################
                     [1m Learning iteration 1711/4000 [0m

                       Computation: 3218 steps/s (collection: 0.479s, learning 2.067s)
               Value function loss: 74660.2243
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 6226.66
               Mean episode length: 308.38
                 Mean success rate: 60.00
                  Mean reward/step: 21.78
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 2.55s
                        Total time: 4394.29s
                               ETA: 5875.3s

################################################################################
                     [1m Learning iteration 1712/4000 [0m

                       Computation: 3231 steps/s (collection: 0.464s, learning 2.070s)
               Value function loss: 97211.6583
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 6638.92
               Mean episode length: 326.93
                 Mean success rate: 64.00
                  Mean reward/step: 21.90
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14032896
                    Iteration time: 2.53s
                        Total time: 4396.83s
                               ETA: 5872.7s

################################################################################
                     [1m Learning iteration 1713/4000 [0m

                       Computation: 3272 steps/s (collection: 0.466s, learning 2.037s)
               Value function loss: 68461.9548
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 6360.59
               Mean episode length: 315.48
                 Mean success rate: 62.00
                  Mean reward/step: 21.33
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 2.50s
                        Total time: 4399.33s
                               ETA: 5870.1s

################################################################################
                     [1m Learning iteration 1714/4000 [0m

                       Computation: 3279 steps/s (collection: 0.429s, learning 2.068s)
               Value function loss: 85982.6505
                    Surrogate loss: 0.0171
             Mean action noise std: 0.91
                       Mean reward: 6652.33
               Mean episode length: 321.92
                 Mean success rate: 64.00
                  Mean reward/step: 21.64
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14049280
                    Iteration time: 2.50s
                        Total time: 4401.83s
                               ETA: 5867.4s

################################################################################
                     [1m Learning iteration 1715/4000 [0m

                       Computation: 3265 steps/s (collection: 0.469s, learning 2.039s)
               Value function loss: 83400.7483
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 6697.83
               Mean episode length: 321.50
                 Mean success rate: 64.00
                  Mean reward/step: 21.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 2.51s
                        Total time: 4404.34s
                               ETA: 5864.7s

################################################################################
                     [1m Learning iteration 1716/4000 [0m

                       Computation: 3255 steps/s (collection: 0.453s, learning 2.063s)
               Value function loss: 88266.2018
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 6487.47
               Mean episode length: 319.68
                 Mean success rate: 63.00
                  Mean reward/step: 20.76
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 14065664
                    Iteration time: 2.52s
                        Total time: 4406.85s
                               ETA: 5862.1s

################################################################################
                     [1m Learning iteration 1717/4000 [0m

                       Computation: 3240 steps/s (collection: 0.464s, learning 2.064s)
               Value function loss: 79660.0368
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 6533.41
               Mean episode length: 315.29
                 Mean success rate: 61.50
                  Mean reward/step: 20.00
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 2.53s
                        Total time: 4409.38s
                               ETA: 5859.5s

################################################################################
                     [1m Learning iteration 1718/4000 [0m

                       Computation: 3249 steps/s (collection: 0.469s, learning 2.052s)
               Value function loss: 72550.4261
                    Surrogate loss: 0.0175
             Mean action noise std: 0.92
                       Mean reward: 6499.03
               Mean episode length: 309.70
                 Mean success rate: 61.00
                  Mean reward/step: 20.56
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14082048
                    Iteration time: 2.52s
                        Total time: 4411.90s
                               ETA: 5856.9s

################################################################################
                     [1m Learning iteration 1719/4000 [0m

                       Computation: 3218 steps/s (collection: 0.481s, learning 2.064s)
               Value function loss: 81771.5010
                    Surrogate loss: 0.0122
             Mean action noise std: 0.92
                       Mean reward: 6702.34
               Mean episode length: 319.08
                 Mean success rate: 61.50
                  Mean reward/step: 20.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 2.55s
                        Total time: 4414.45s
                               ETA: 5854.3s

################################################################################
                     [1m Learning iteration 1720/4000 [0m

                       Computation: 3280 steps/s (collection: 0.464s, learning 2.033s)
               Value function loss: 104732.9193
                    Surrogate loss: 0.0132
             Mean action noise std: 0.92
                       Mean reward: 6636.68
               Mean episode length: 315.69
                 Mean success rate: 60.50
                  Mean reward/step: 20.61
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14098432
                    Iteration time: 2.50s
                        Total time: 4416.94s
                               ETA: 5851.6s

################################################################################
                     [1m Learning iteration 1721/4000 [0m

                       Computation: 3255 steps/s (collection: 0.460s, learning 2.056s)
               Value function loss: 83802.8320
                    Surrogate loss: 0.0162
             Mean action noise std: 0.92
                       Mean reward: 6982.16
               Mean episode length: 326.87
                 Mean success rate: 62.00
                  Mean reward/step: 21.32
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 2.52s
                        Total time: 4419.46s
                               ETA: 5849.0s

################################################################################
                     [1m Learning iteration 1722/4000 [0m

                       Computation: 3212 steps/s (collection: 0.482s, learning 2.068s)
               Value function loss: 92415.2186
                    Surrogate loss: 0.0107
             Mean action noise std: 0.92
                       Mean reward: 6980.61
               Mean episode length: 329.92
                 Mean success rate: 62.50
                  Mean reward/step: 21.29
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14114816
                    Iteration time: 2.55s
                        Total time: 4422.01s
                               ETA: 5846.4s

################################################################################
                     [1m Learning iteration 1723/4000 [0m

                       Computation: 3285 steps/s (collection: 0.461s, learning 2.033s)
               Value function loss: 104368.4752
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 6740.70
               Mean episode length: 321.50
                 Mean success rate: 61.00
                  Mean reward/step: 21.22
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 2.49s
                        Total time: 4424.50s
                               ETA: 5843.7s

################################################################################
                     [1m Learning iteration 1724/4000 [0m

                       Computation: 3291 steps/s (collection: 0.445s, learning 2.044s)
               Value function loss: 85708.0146
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 7113.94
               Mean episode length: 330.21
                 Mean success rate: 63.50
                  Mean reward/step: 20.82
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14131200
                    Iteration time: 2.49s
                        Total time: 4426.99s
                               ETA: 5841.1s

################################################################################
                     [1m Learning iteration 1725/4000 [0m

                       Computation: 3252 steps/s (collection: 0.445s, learning 2.074s)
               Value function loss: 70214.6844
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 7023.32
               Mean episode length: 332.99
                 Mean success rate: 64.00
                  Mean reward/step: 21.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 2.52s
                        Total time: 4429.51s
                               ETA: 5838.4s

################################################################################
                     [1m Learning iteration 1726/4000 [0m

                       Computation: 3195 steps/s (collection: 0.415s, learning 2.149s)
               Value function loss: 73838.6233
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 7337.85
               Mean episode length: 345.94
                 Mean success rate: 67.50
                  Mean reward/step: 21.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14147584
                    Iteration time: 2.56s
                        Total time: 4432.07s
                               ETA: 5835.9s

################################################################################
                     [1m Learning iteration 1727/4000 [0m

                       Computation: 3151 steps/s (collection: 0.502s, learning 2.097s)
               Value function loss: 116936.6359
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 7748.71
               Mean episode length: 361.98
                 Mean success rate: 70.00
                  Mean reward/step: 21.12
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 2.60s
                        Total time: 4434.67s
                               ETA: 5833.3s

################################################################################
                     [1m Learning iteration 1728/4000 [0m

                       Computation: 3200 steps/s (collection: 0.471s, learning 2.089s)
               Value function loss: 86285.0455
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 7322.36
               Mean episode length: 342.43
                 Mean success rate: 66.50
                  Mean reward/step: 20.42
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14163968
                    Iteration time: 2.56s
                        Total time: 4437.23s
                               ETA: 5830.8s

################################################################################
                     [1m Learning iteration 1729/4000 [0m

                       Computation: 3155 steps/s (collection: 0.495s, learning 2.101s)
               Value function loss: 81279.9608
                    Surrogate loss: 0.0169
             Mean action noise std: 0.91
                       Mean reward: 7173.16
               Mean episode length: 339.24
                 Mean success rate: 66.00
                  Mean reward/step: 20.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 2.60s
                        Total time: 4439.83s
                               ETA: 5828.2s

################################################################################
                     [1m Learning iteration 1730/4000 [0m

                       Computation: 3197 steps/s (collection: 0.450s, learning 2.112s)
               Value function loss: 61983.0949
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 7166.24
               Mean episode length: 339.98
                 Mean success rate: 66.00
                  Mean reward/step: 21.30
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14180352
                    Iteration time: 2.56s
                        Total time: 4442.39s
                               ETA: 5825.7s

################################################################################
                     [1m Learning iteration 1731/4000 [0m

                       Computation: 3202 steps/s (collection: 0.458s, learning 2.100s)
               Value function loss: 91777.3925
                    Surrogate loss: 0.0111
             Mean action noise std: 0.91
                       Mean reward: 7186.84
               Mean episode length: 341.07
                 Mean success rate: 67.00
                  Mean reward/step: 22.15
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 2.56s
                        Total time: 4444.95s
                               ETA: 5823.1s

################################################################################
                     [1m Learning iteration 1732/4000 [0m

                       Computation: 3209 steps/s (collection: 0.447s, learning 2.105s)
               Value function loss: 75102.1588
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 7149.51
               Mean episode length: 342.25
                 Mean success rate: 66.00
                  Mean reward/step: 21.92
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14196736
                    Iteration time: 2.55s
                        Total time: 4447.50s
                               ETA: 5820.5s

################################################################################
                     [1m Learning iteration 1733/4000 [0m

                       Computation: 3228 steps/s (collection: 0.453s, learning 2.084s)
               Value function loss: 73250.0844
                    Surrogate loss: 0.0172
             Mean action noise std: 0.91
                       Mean reward: 7198.08
               Mean episode length: 337.58
                 Mean success rate: 65.50
                  Mean reward/step: 22.30
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 2.54s
                        Total time: 4450.04s
                               ETA: 5817.9s

################################################################################
                     [1m Learning iteration 1734/4000 [0m

                       Computation: 3264 steps/s (collection: 0.441s, learning 2.068s)
               Value function loss: 95991.0636
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 7064.85
               Mean episode length: 334.23
                 Mean success rate: 64.00
                  Mean reward/step: 22.08
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 14213120
                    Iteration time: 2.51s
                        Total time: 4452.55s
                               ETA: 5815.3s

################################################################################
                     [1m Learning iteration 1735/4000 [0m

                       Computation: 3204 steps/s (collection: 0.465s, learning 2.091s)
               Value function loss: 92746.3172
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 6777.30
               Mean episode length: 325.69
                 Mean success rate: 62.00
                  Mean reward/step: 21.90
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 2.56s
                        Total time: 4455.10s
                               ETA: 5812.7s

################################################################################
                     [1m Learning iteration 1736/4000 [0m

                       Computation: 3188 steps/s (collection: 0.488s, learning 2.081s)
               Value function loss: 86891.8808
                    Surrogate loss: 0.0162
             Mean action noise std: 0.92
                       Mean reward: 6965.40
               Mean episode length: 333.62
                 Mean success rate: 63.00
                  Mean reward/step: 21.84
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14229504
                    Iteration time: 2.57s
                        Total time: 4457.67s
                               ETA: 5810.1s

################################################################################
                     [1m Learning iteration 1737/4000 [0m

                       Computation: 3090 steps/s (collection: 0.489s, learning 2.162s)
               Value function loss: 79535.6496
                    Surrogate loss: 0.0131
             Mean action noise std: 0.92
                       Mean reward: 6744.32
               Mean episode length: 324.98
                 Mean success rate: 62.50
                  Mean reward/step: 21.59
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 2.65s
                        Total time: 4460.32s
                               ETA: 5807.7s

################################################################################
                     [1m Learning iteration 1738/4000 [0m

                       Computation: 3029 steps/s (collection: 0.529s, learning 2.175s)
               Value function loss: 123128.1783
                    Surrogate loss: 0.0141
             Mean action noise std: 0.92
                       Mean reward: 7107.94
               Mean episode length: 335.13
                 Mean success rate: 65.00
                  Mean reward/step: 21.95
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14245888
                    Iteration time: 2.70s
                        Total time: 4463.03s
                               ETA: 5805.3s

################################################################################
                     [1m Learning iteration 1739/4000 [0m

                       Computation: 3136 steps/s (collection: 0.478s, learning 2.134s)
               Value function loss: 81987.7368
                    Surrogate loss: 0.0152
             Mean action noise std: 0.92
                       Mean reward: 7127.89
               Mean episode length: 335.57
                 Mean success rate: 64.00
                  Mean reward/step: 21.59
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 2.61s
                        Total time: 4465.64s
                               ETA: 5802.8s

################################################################################
                     [1m Learning iteration 1740/4000 [0m

                       Computation: 3227 steps/s (collection: 0.433s, learning 2.105s)
               Value function loss: 84458.5229
                    Surrogate loss: 0.0155
             Mean action noise std: 0.92
                       Mean reward: 7259.34
               Mean episode length: 337.34
                 Mean success rate: 65.00
                  Mean reward/step: 22.20
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14262272
                    Iteration time: 2.54s
                        Total time: 4468.18s
                               ETA: 5800.2s

################################################################################
                     [1m Learning iteration 1741/4000 [0m

                       Computation: 3143 steps/s (collection: 0.485s, learning 2.121s)
               Value function loss: 61460.9585
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 7001.83
               Mean episode length: 332.04
                 Mean success rate: 64.00
                  Mean reward/step: 23.42
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 2.61s
                        Total time: 4470.78s
                               ETA: 5797.6s

################################################################################
                     [1m Learning iteration 1742/4000 [0m

                       Computation: 3189 steps/s (collection: 0.477s, learning 2.092s)
               Value function loss: 105192.9309
                    Surrogate loss: 0.0181
             Mean action noise std: 0.92
                       Mean reward: 7174.76
               Mean episode length: 335.83
                 Mean success rate: 65.00
                  Mean reward/step: 23.70
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14278656
                    Iteration time: 2.57s
                        Total time: 4473.35s
                               ETA: 5795.1s

################################################################################
                     [1m Learning iteration 1743/4000 [0m

                       Computation: 3094 steps/s (collection: 0.511s, learning 2.136s)
               Value function loss: 93482.3522
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 7463.68
               Mean episode length: 340.12
                 Mean success rate: 67.00
                  Mean reward/step: 22.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 2.65s
                        Total time: 4476.00s
                               ETA: 5792.6s

################################################################################
                     [1m Learning iteration 1744/4000 [0m

                       Computation: 3193 steps/s (collection: 0.453s, learning 2.112s)
               Value function loss: 90179.2534
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 7755.30
               Mean episode length: 344.87
                 Mean success rate: 68.50
                  Mean reward/step: 22.13
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14295040
                    Iteration time: 2.56s
                        Total time: 4478.56s
                               ETA: 5790.1s

################################################################################
                     [1m Learning iteration 1745/4000 [0m

                       Computation: 3169 steps/s (collection: 0.507s, learning 2.077s)
               Value function loss: 92792.5791
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 8020.17
               Mean episode length: 353.82
                 Mean success rate: 70.00
                  Mean reward/step: 22.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 2.58s
                        Total time: 4481.15s
                               ETA: 5787.5s

################################################################################
                     [1m Learning iteration 1746/4000 [0m

                       Computation: 3134 steps/s (collection: 0.466s, learning 2.147s)
               Value function loss: 80719.6874
                    Surrogate loss: 0.0156
             Mean action noise std: 0.92
                       Mean reward: 8155.00
               Mean episode length: 358.61
                 Mean success rate: 70.50
                  Mean reward/step: 22.15
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14311424
                    Iteration time: 2.61s
                        Total time: 4483.76s
                               ETA: 5785.0s

################################################################################
                     [1m Learning iteration 1747/4000 [0m

                       Computation: 3137 steps/s (collection: 0.509s, learning 2.102s)
               Value function loss: 104166.7373
                    Surrogate loss: 0.0152
             Mean action noise std: 0.92
                       Mean reward: 8305.78
               Mean episode length: 362.29
                 Mean success rate: 70.50
                  Mean reward/step: 22.04
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 2.61s
                        Total time: 4486.37s
                               ETA: 5782.5s

################################################################################
                     [1m Learning iteration 1748/4000 [0m

                       Computation: 3221 steps/s (collection: 0.457s, learning 2.086s)
               Value function loss: 67820.7980
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 8354.74
               Mean episode length: 362.69
                 Mean success rate: 71.50
                  Mean reward/step: 22.13
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14327808
                    Iteration time: 2.54s
                        Total time: 4488.92s
                               ETA: 5779.9s

################################################################################
                     [1m Learning iteration 1749/4000 [0m

                       Computation: 3188 steps/s (collection: 0.512s, learning 2.057s)
               Value function loss: 82775.0261
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 7811.18
               Mean episode length: 344.30
                 Mean success rate: 68.50
                  Mean reward/step: 23.05
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 2.57s
                        Total time: 4491.49s
                               ETA: 5777.3s

################################################################################
                     [1m Learning iteration 1750/4000 [0m

                       Computation: 3161 steps/s (collection: 0.493s, learning 2.098s)
               Value function loss: 113763.9479
                    Surrogate loss: 0.0178
             Mean action noise std: 0.92
                       Mean reward: 8203.39
               Mean episode length: 358.19
                 Mean success rate: 71.00
                  Mean reward/step: 22.11
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14344192
                    Iteration time: 2.59s
                        Total time: 4494.08s
                               ETA: 5774.8s

################################################################################
                     [1m Learning iteration 1751/4000 [0m

                       Computation: 3240 steps/s (collection: 0.511s, learning 2.017s)
               Value function loss: 103790.2754
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 7942.33
               Mean episode length: 353.13
                 Mean success rate: 70.00
                  Mean reward/step: 21.30
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 2.53s
                        Total time: 4496.61s
                               ETA: 5772.2s

################################################################################
                     [1m Learning iteration 1752/4000 [0m

                       Computation: 3220 steps/s (collection: 0.458s, learning 2.086s)
               Value function loss: 78265.6299
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 7969.76
               Mean episode length: 354.59
                 Mean success rate: 70.50
                  Mean reward/step: 21.04
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14360576
                    Iteration time: 2.54s
                        Total time: 4499.15s
                               ETA: 5769.6s

################################################################################
                     [1m Learning iteration 1753/4000 [0m

                       Computation: 3208 steps/s (collection: 0.500s, learning 2.053s)
               Value function loss: 105191.8279
                    Surrogate loss: 0.0132
             Mean action noise std: 0.92
                       Mean reward: 7665.66
               Mean episode length: 343.79
                 Mean success rate: 69.00
                  Mean reward/step: 20.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 2.55s
                        Total time: 4501.70s
                               ETA: 5767.0s

################################################################################
                     [1m Learning iteration 1754/4000 [0m

                       Computation: 3199 steps/s (collection: 0.493s, learning 2.067s)
               Value function loss: 80252.8060
                    Surrogate loss: 0.0170
             Mean action noise std: 0.92
                       Mean reward: 8019.23
               Mean episode length: 354.04
                 Mean success rate: 71.00
                  Mean reward/step: 20.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14376960
                    Iteration time: 2.56s
                        Total time: 4504.26s
                               ETA: 5764.4s

################################################################################
                     [1m Learning iteration 1755/4000 [0m

                       Computation: 3195 steps/s (collection: 0.505s, learning 2.059s)
               Value function loss: 105395.8088
                    Surrogate loss: 0.0163
             Mean action noise std: 0.92
                       Mean reward: 7941.46
               Mean episode length: 353.21
                 Mean success rate: 71.50
                  Mean reward/step: 20.61
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 2.56s
                        Total time: 4506.83s
                               ETA: 5761.9s

################################################################################
                     [1m Learning iteration 1756/4000 [0m

                       Computation: 3263 steps/s (collection: 0.446s, learning 2.064s)
               Value function loss: 76519.1764
                    Surrogate loss: 0.0162
             Mean action noise std: 0.92
                       Mean reward: 7507.13
               Mean episode length: 342.83
                 Mean success rate: 67.50
                  Mean reward/step: 20.34
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 14393344
                    Iteration time: 2.51s
                        Total time: 4509.34s
                               ETA: 5759.2s

################################################################################
                     [1m Learning iteration 1757/4000 [0m

                       Computation: 3231 steps/s (collection: 0.441s, learning 2.094s)
               Value function loss: 87141.6077
                    Surrogate loss: 0.0171
             Mean action noise std: 0.92
                       Mean reward: 7877.39
               Mean episode length: 353.14
                 Mean success rate: 69.00
                  Mean reward/step: 20.46
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 2.54s
                        Total time: 4511.87s
                               ETA: 5756.6s

################################################################################
                     [1m Learning iteration 1758/4000 [0m

                       Computation: 3263 steps/s (collection: 0.448s, learning 2.062s)
               Value function loss: 90314.4094
                    Surrogate loss: 0.0182
             Mean action noise std: 0.92
                       Mean reward: 7878.23
               Mean episode length: 354.73
                 Mean success rate: 68.50
                  Mean reward/step: 20.37
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14409728
                    Iteration time: 2.51s
                        Total time: 4514.38s
                               ETA: 5754.0s

################################################################################
                     [1m Learning iteration 1759/4000 [0m

                       Computation: 3265 steps/s (collection: 0.462s, learning 2.047s)
               Value function loss: 106469.9580
                    Surrogate loss: 0.0157
             Mean action noise std: 0.92
                       Mean reward: 7727.84
               Mean episode length: 348.77
                 Mean success rate: 68.00
                  Mean reward/step: 20.62
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 2.51s
                        Total time: 4516.89s
                               ETA: 5751.3s

################################################################################
                     [1m Learning iteration 1760/4000 [0m

                       Computation: 3170 steps/s (collection: 0.454s, learning 2.130s)
               Value function loss: 64126.9195
                    Surrogate loss: 0.0173
             Mean action noise std: 0.92
                       Mean reward: 7301.88
               Mean episode length: 337.58
                 Mean success rate: 65.50
                  Mean reward/step: 20.84
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14426112
                    Iteration time: 2.58s
                        Total time: 4519.47s
                               ETA: 5748.8s

################################################################################
                     [1m Learning iteration 1761/4000 [0m

                       Computation: 3163 steps/s (collection: 0.494s, learning 2.096s)
               Value function loss: 84538.9220
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 6987.94
               Mean episode length: 325.25
                 Mean success rate: 62.00
                  Mean reward/step: 20.64
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 2.59s
                        Total time: 4522.06s
                               ETA: 5746.3s

################################################################################
                     [1m Learning iteration 1762/4000 [0m

                       Computation: 3176 steps/s (collection: 0.490s, learning 2.089s)
               Value function loss: 74347.1118
                    Surrogate loss: 0.0186
             Mean action noise std: 0.92
                       Mean reward: 7007.52
               Mean episode length: 330.02
                 Mean success rate: 63.00
                  Mean reward/step: 21.06
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14442496
                    Iteration time: 2.58s
                        Total time: 4524.64s
                               ETA: 5743.7s

################################################################################
                     [1m Learning iteration 1763/4000 [0m

                       Computation: 3080 steps/s (collection: 0.550s, learning 2.110s)
               Value function loss: 56085.4814
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 6650.62
               Mean episode length: 313.86
                 Mean success rate: 59.00
                  Mean reward/step: 21.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 2.66s
                        Total time: 4527.30s
                               ETA: 5741.3s

################################################################################
                     [1m Learning iteration 1764/4000 [0m

                       Computation: 3122 steps/s (collection: 0.492s, learning 2.132s)
               Value function loss: 81737.1141
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 6536.59
               Mean episode length: 315.41
                 Mean success rate: 59.00
                  Mean reward/step: 22.94
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14458880
                    Iteration time: 2.62s
                        Total time: 4529.93s
                               ETA: 5738.8s

################################################################################
                     [1m Learning iteration 1765/4000 [0m

                       Computation: 3099 steps/s (collection: 0.534s, learning 2.109s)
               Value function loss: 96835.0239
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 6404.13
               Mean episode length: 317.39
                 Mean success rate: 59.50
                  Mean reward/step: 22.48
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 2.64s
                        Total time: 4532.57s
                               ETA: 5736.3s

################################################################################
                     [1m Learning iteration 1766/4000 [0m

                       Computation: 3114 steps/s (collection: 0.504s, learning 2.126s)
               Value function loss: 95227.2396
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 6392.26
               Mean episode length: 311.15
                 Mean success rate: 58.00
                  Mean reward/step: 22.06
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14475264
                    Iteration time: 2.63s
                        Total time: 4535.20s
                               ETA: 5733.8s

################################################################################
                     [1m Learning iteration 1767/4000 [0m

                       Computation: 3127 steps/s (collection: 0.511s, learning 2.109s)
               Value function loss: 88000.6170
                    Surrogate loss: 0.0131
             Mean action noise std: 0.92
                       Mean reward: 6394.63
               Mean episode length: 312.17
                 Mean success rate: 57.00
                  Mean reward/step: 21.96
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 2.62s
                        Total time: 4537.82s
                               ETA: 5731.3s

################################################################################
                     [1m Learning iteration 1768/4000 [0m

                       Computation: 3210 steps/s (collection: 0.481s, learning 2.070s)
               Value function loss: 98552.0180
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 6778.68
               Mean episode length: 327.44
                 Mean success rate: 59.00
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14491648
                    Iteration time: 2.55s
                        Total time: 4540.37s
                               ETA: 5728.7s

################################################################################
                     [1m Learning iteration 1769/4000 [0m

                       Computation: 3178 steps/s (collection: 0.478s, learning 2.099s)
               Value function loss: 81750.0123
                    Surrogate loss: 0.0117
             Mean action noise std: 0.92
                       Mean reward: 7030.19
               Mean episode length: 336.96
                 Mean success rate: 61.00
                  Mean reward/step: 22.17
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 2.58s
                        Total time: 4542.95s
                               ETA: 5726.2s

################################################################################
                     [1m Learning iteration 1770/4000 [0m

                       Computation: 3158 steps/s (collection: 0.491s, learning 2.103s)
               Value function loss: 76720.5106
                    Surrogate loss: 0.0122
             Mean action noise std: 0.92
                       Mean reward: 7187.52
               Mean episode length: 343.52
                 Mean success rate: 62.00
                  Mean reward/step: 22.78
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14508032
                    Iteration time: 2.59s
                        Total time: 4545.54s
                               ETA: 5723.6s

################################################################################
                     [1m Learning iteration 1771/4000 [0m

                       Computation: 3111 steps/s (collection: 0.482s, learning 2.150s)
               Value function loss: 96234.6451
                    Surrogate loss: 0.0152
             Mean action noise std: 0.92
                       Mean reward: 7585.71
               Mean episode length: 356.34
                 Mean success rate: 65.00
                  Mean reward/step: 22.90
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 2.63s
                        Total time: 4548.17s
                               ETA: 5721.2s

################################################################################
                     [1m Learning iteration 1772/4000 [0m

                       Computation: 3218 steps/s (collection: 0.456s, learning 2.089s)
               Value function loss: 97765.0629
                    Surrogate loss: 0.0177
             Mean action noise std: 0.92
                       Mean reward: 7885.09
               Mean episode length: 364.59
                 Mean success rate: 68.00
                  Mean reward/step: 22.47
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14524416
                    Iteration time: 2.55s
                        Total time: 4550.72s
                               ETA: 5718.6s

################################################################################
                     [1m Learning iteration 1773/4000 [0m

                       Computation: 3224 steps/s (collection: 0.482s, learning 2.059s)
               Value function loss: 81160.9555
                    Surrogate loss: 0.0141
             Mean action noise std: 0.92
                       Mean reward: 7924.37
               Mean episode length: 360.91
                 Mean success rate: 68.00
                  Mean reward/step: 22.78
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 2.54s
                        Total time: 4553.26s
                               ETA: 5716.0s

################################################################################
                     [1m Learning iteration 1774/4000 [0m

                       Computation: 3102 steps/s (collection: 0.523s, learning 2.117s)
               Value function loss: 70406.6323
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 8170.19
               Mean episode length: 369.87
                 Mean success rate: 69.00
                  Mean reward/step: 22.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14540800
                    Iteration time: 2.64s
                        Total time: 4555.90s
                               ETA: 5713.5s

################################################################################
                     [1m Learning iteration 1775/4000 [0m

                       Computation: 3059 steps/s (collection: 0.537s, learning 2.141s)
               Value function loss: 88009.1262
                    Surrogate loss: 0.0120
             Mean action noise std: 0.92
                       Mean reward: 7948.84
               Mean episode length: 364.92
                 Mean success rate: 67.50
                  Mean reward/step: 22.80
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 2.68s
                        Total time: 4558.58s
                               ETA: 5711.1s

################################################################################
                     [1m Learning iteration 1776/4000 [0m

                       Computation: 3191 steps/s (collection: 0.472s, learning 2.095s)
               Value function loss: 88732.4740
                    Surrogate loss: 0.0121
             Mean action noise std: 0.92
                       Mean reward: 8113.77
               Mean episode length: 367.51
                 Mean success rate: 68.00
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14557184
                    Iteration time: 2.57s
                        Total time: 4561.14s
                               ETA: 5708.5s

################################################################################
                     [1m Learning iteration 1777/4000 [0m

                       Computation: 3160 steps/s (collection: 0.485s, learning 2.107s)
               Value function loss: 88758.3343
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 8032.19
               Mean episode length: 362.62
                 Mean success rate: 68.50
                  Mean reward/step: 22.94
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 2.59s
                        Total time: 4563.74s
                               ETA: 5706.0s

################################################################################
                     [1m Learning iteration 1778/4000 [0m

                       Computation: 3119 steps/s (collection: 0.503s, learning 2.123s)
               Value function loss: 78958.4984
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 7550.69
               Mean episode length: 348.08
                 Mean success rate: 65.00
                  Mean reward/step: 22.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14573568
                    Iteration time: 2.63s
                        Total time: 4566.36s
                               ETA: 5703.5s

################################################################################
                     [1m Learning iteration 1779/4000 [0m

                       Computation: 3093 steps/s (collection: 0.504s, learning 2.144s)
               Value function loss: 50394.9303
                    Surrogate loss: 0.0111
             Mean action noise std: 0.92
                       Mean reward: 7345.93
               Mean episode length: 338.36
                 Mean success rate: 63.00
                  Mean reward/step: 22.92
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 2.65s
                        Total time: 4569.01s
                               ETA: 5701.0s

################################################################################
                     [1m Learning iteration 1780/4000 [0m

                       Computation: 3132 steps/s (collection: 0.518s, learning 2.097s)
               Value function loss: 118816.1146
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 7468.75
               Mean episode length: 343.89
                 Mean success rate: 64.50
                  Mean reward/step: 23.29
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14589952
                    Iteration time: 2.62s
                        Total time: 4571.63s
                               ETA: 5698.5s

################################################################################
                     [1m Learning iteration 1781/4000 [0m

                       Computation: 3144 steps/s (collection: 0.496s, learning 2.109s)
               Value function loss: 97761.6241
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 7518.37
               Mean episode length: 344.42
                 Mean success rate: 65.00
                  Mean reward/step: 22.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 2.61s
                        Total time: 4574.23s
                               ETA: 5696.0s

################################################################################
                     [1m Learning iteration 1782/4000 [0m

                       Computation: 3089 steps/s (collection: 0.518s, learning 2.134s)
               Value function loss: 121259.7531
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 7591.50
               Mean episode length: 348.86
                 Mean success rate: 65.00
                  Mean reward/step: 22.40
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14606336
                    Iteration time: 2.65s
                        Total time: 4576.88s
                               ETA: 5693.5s

################################################################################
                     [1m Learning iteration 1783/4000 [0m

                       Computation: 3175 steps/s (collection: 0.490s, learning 2.089s)
               Value function loss: 95486.9767
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 7976.30
               Mean episode length: 356.21
                 Mean success rate: 68.00
                  Mean reward/step: 21.85
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 2.58s
                        Total time: 4579.46s
                               ETA: 5691.0s

################################################################################
                     [1m Learning iteration 1784/4000 [0m

                       Computation: 3172 steps/s (collection: 0.481s, learning 2.101s)
               Value function loss: 95568.4595
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 8308.21
               Mean episode length: 363.44
                 Mean success rate: 70.00
                  Mean reward/step: 21.95
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14622720
                    Iteration time: 2.58s
                        Total time: 4582.04s
                               ETA: 5688.4s

################################################################################
                     [1m Learning iteration 1785/4000 [0m

                       Computation: 3185 steps/s (collection: 0.465s, learning 2.107s)
               Value function loss: 86648.1357
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 8468.72
               Mean episode length: 367.90
                 Mean success rate: 71.00
                  Mean reward/step: 22.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 2.57s
                        Total time: 4584.62s
                               ETA: 5685.8s

################################################################################
                     [1m Learning iteration 1786/4000 [0m

                       Computation: 3160 steps/s (collection: 0.489s, learning 2.102s)
               Value function loss: 85587.1821
                    Surrogate loss: 0.0170
             Mean action noise std: 0.92
                       Mean reward: 8723.80
               Mean episode length: 372.93
                 Mean success rate: 71.50
                  Mean reward/step: 22.48
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14639104
                    Iteration time: 2.59s
                        Total time: 4587.21s
                               ETA: 5683.3s

################################################################################
                     [1m Learning iteration 1787/4000 [0m

                       Computation: 3133 steps/s (collection: 0.498s, learning 2.117s)
               Value function loss: 89735.6765
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 8901.02
               Mean episode length: 379.33
                 Mean success rate: 73.00
                  Mean reward/step: 23.22
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 2.61s
                        Total time: 4589.82s
                               ETA: 5680.8s

################################################################################
                     [1m Learning iteration 1788/4000 [0m

                       Computation: 3136 steps/s (collection: 0.514s, learning 2.098s)
               Value function loss: 72815.5431
                    Surrogate loss: 0.0174
             Mean action noise std: 0.92
                       Mean reward: 8941.43
               Mean episode length: 382.43
                 Mean success rate: 74.00
                  Mean reward/step: 22.15
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14655488
                    Iteration time: 2.61s
                        Total time: 4592.43s
                               ETA: 5678.3s

################################################################################
                     [1m Learning iteration 1789/4000 [0m

                       Computation: 3252 steps/s (collection: 0.454s, learning 2.066s)
               Value function loss: 87003.0800
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 8735.20
               Mean episode length: 371.69
                 Mean success rate: 72.00
                  Mean reward/step: 22.02
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 2.52s
                        Total time: 4594.95s
                               ETA: 5675.7s

################################################################################
                     [1m Learning iteration 1790/4000 [0m

                       Computation: 3193 steps/s (collection: 0.471s, learning 2.094s)
               Value function loss: 85398.7159
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 8703.08
               Mean episode length: 370.94
                 Mean success rate: 70.50
                  Mean reward/step: 21.92
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14671872
                    Iteration time: 2.56s
                        Total time: 4597.52s
                               ETA: 5673.1s

################################################################################
                     [1m Learning iteration 1791/4000 [0m

                       Computation: 3173 steps/s (collection: 0.483s, learning 2.099s)
               Value function loss: 77220.2446
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 8409.21
               Mean episode length: 362.46
                 Mean success rate: 68.50
                  Mean reward/step: 21.86
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 2.58s
                        Total time: 4600.10s
                               ETA: 5670.5s

################################################################################
                     [1m Learning iteration 1792/4000 [0m

                       Computation: 3158 steps/s (collection: 0.498s, learning 2.096s)
               Value function loss: 74101.6709
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 8116.51
               Mean episode length: 355.36
                 Mean success rate: 66.00
                  Mean reward/step: 21.96
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14688256
                    Iteration time: 2.59s
                        Total time: 4602.69s
                               ETA: 5668.0s

################################################################################
                     [1m Learning iteration 1793/4000 [0m

                       Computation: 3142 steps/s (collection: 0.495s, learning 2.112s)
               Value function loss: 97753.8063
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 7966.30
               Mean episode length: 352.77
                 Mean success rate: 66.00
                  Mean reward/step: 21.43
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 2.61s
                        Total time: 4605.30s
                               ETA: 5665.5s

################################################################################
                     [1m Learning iteration 1794/4000 [0m

                       Computation: 3219 steps/s (collection: 0.469s, learning 2.076s)
               Value function loss: 88644.6564
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 7789.29
               Mean episode length: 347.46
                 Mean success rate: 64.50
                  Mean reward/step: 21.12
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14704640
                    Iteration time: 2.54s
                        Total time: 4607.84s
                               ETA: 5662.9s

################################################################################
                     [1m Learning iteration 1795/4000 [0m

                       Computation: 3164 steps/s (collection: 0.487s, learning 2.101s)
               Value function loss: 71231.8437
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 7198.19
               Mean episode length: 330.79
                 Mean success rate: 61.00
                  Mean reward/step: 21.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 2.59s
                        Total time: 4610.43s
                               ETA: 5660.4s

################################################################################
                     [1m Learning iteration 1796/4000 [0m

                       Computation: 3195 steps/s (collection: 0.462s, learning 2.102s)
               Value function loss: 113200.8850
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 7287.45
               Mean episode length: 336.46
                 Mean success rate: 63.00
                  Mean reward/step: 21.74
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14721024
                    Iteration time: 2.56s
                        Total time: 4613.00s
                               ETA: 5657.8s

################################################################################
                     [1m Learning iteration 1797/4000 [0m

                       Computation: 3194 steps/s (collection: 0.479s, learning 2.086s)
               Value function loss: 97647.3359
                    Surrogate loss: 0.0162
             Mean action noise std: 0.92
                       Mean reward: 7114.65
               Mean episode length: 331.60
                 Mean success rate: 61.50
                  Mean reward/step: 21.24
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 2.56s
                        Total time: 4615.56s
                               ETA: 5655.2s

################################################################################
                     [1m Learning iteration 1798/4000 [0m

                       Computation: 3186 steps/s (collection: 0.486s, learning 2.085s)
               Value function loss: 125771.0385
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 7619.93
               Mean episode length: 349.29
                 Mean success rate: 66.00
                  Mean reward/step: 20.48
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14737408
                    Iteration time: 2.57s
                        Total time: 4618.13s
                               ETA: 5652.7s

################################################################################
                     [1m Learning iteration 1799/4000 [0m

                       Computation: 3130 steps/s (collection: 0.448s, learning 2.169s)
               Value function loss: 94476.0726
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 7614.07
               Mean episode length: 344.74
                 Mean success rate: 65.50
                  Mean reward/step: 20.72
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 2.62s
                        Total time: 4620.75s
                               ETA: 5650.1s

################################################################################
                     [1m Learning iteration 1800/4000 [0m

                       Computation: 3131 steps/s (collection: 0.490s, learning 2.126s)
               Value function loss: 93090.1212
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 7186.92
               Mean episode length: 331.61
                 Mean success rate: 63.00
                  Mean reward/step: 20.86
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 14753792
                    Iteration time: 2.62s
                        Total time: 4623.36s
                               ETA: 5647.6s

################################################################################
                     [1m Learning iteration 1801/4000 [0m

                       Computation: 3215 steps/s (collection: 0.469s, learning 2.079s)
               Value function loss: 79260.3399
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 6957.66
               Mean episode length: 322.25
                 Mean success rate: 60.50
                  Mean reward/step: 20.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 2.55s
                        Total time: 4625.91s
                               ETA: 5645.1s

################################################################################
                     [1m Learning iteration 1802/4000 [0m

                       Computation: 3215 steps/s (collection: 0.470s, learning 2.078s)
               Value function loss: 70328.9121
                    Surrogate loss: 0.0168
             Mean action noise std: 0.92
                       Mean reward: 6740.08
               Mean episode length: 314.37
                 Mean success rate: 59.50
                  Mean reward/step: 20.89
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14770176
                    Iteration time: 2.55s
                        Total time: 4628.46s
                               ETA: 5642.5s

################################################################################
                     [1m Learning iteration 1803/4000 [0m

                       Computation: 3171 steps/s (collection: 0.481s, learning 2.102s)
               Value function loss: 107678.7743
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 6645.10
               Mean episode length: 308.12
                 Mean success rate: 57.00
                  Mean reward/step: 20.70
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 2.58s
                        Total time: 4631.04s
                               ETA: 5639.9s

################################################################################
                     [1m Learning iteration 1804/4000 [0m

                       Computation: 3226 steps/s (collection: 0.447s, learning 2.092s)
               Value function loss: 77935.6807
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 6664.86
               Mean episode length: 308.02
                 Mean success rate: 56.50
                  Mean reward/step: 19.99
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14786560
                    Iteration time: 2.54s
                        Total time: 4633.58s
                               ETA: 5637.3s

################################################################################
                     [1m Learning iteration 1805/4000 [0m

                       Computation: 3216 steps/s (collection: 0.487s, learning 2.059s)
               Value function loss: 79034.5357
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 6623.69
               Mean episode length: 311.21
                 Mean success rate: 56.00
                  Mean reward/step: 19.95
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 2.55s
                        Total time: 4636.13s
                               ETA: 5634.7s

################################################################################
                     [1m Learning iteration 1806/4000 [0m

                       Computation: 3189 steps/s (collection: 0.473s, learning 2.096s)
               Value function loss: 95325.0838
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 6274.73
               Mean episode length: 300.32
                 Mean success rate: 53.00
                  Mean reward/step: 20.50
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14802944
                    Iteration time: 2.57s
                        Total time: 4638.70s
                               ETA: 5632.2s

################################################################################
                     [1m Learning iteration 1807/4000 [0m

                       Computation: 3260 steps/s (collection: 0.459s, learning 2.053s)
               Value function loss: 104726.3369
                    Surrogate loss: 0.0174
             Mean action noise std: 0.92
                       Mean reward: 6754.83
               Mean episode length: 315.44
                 Mean success rate: 56.50
                  Mean reward/step: 20.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 2.51s
                        Total time: 4641.21s
                               ETA: 5629.5s

################################################################################
                     [1m Learning iteration 1808/4000 [0m

                       Computation: 3214 steps/s (collection: 0.457s, learning 2.091s)
               Value function loss: 97473.4538
                    Surrogate loss: 0.0155
             Mean action noise std: 0.92
                       Mean reward: 6759.39
               Mean episode length: 319.12
                 Mean success rate: 58.00
                  Mean reward/step: 20.65
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 14819328
                    Iteration time: 2.55s
                        Total time: 4643.76s
                               ETA: 5626.9s

################################################################################
                     [1m Learning iteration 1809/4000 [0m

                       Computation: 3169 steps/s (collection: 0.486s, learning 2.098s)
               Value function loss: 79348.4950
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 6746.17
               Mean episode length: 324.80
                 Mean success rate: 59.00
                  Mean reward/step: 20.49
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 2.58s
                        Total time: 4646.34s
                               ETA: 5624.4s

################################################################################
                     [1m Learning iteration 1810/4000 [0m

                       Computation: 3238 steps/s (collection: 0.443s, learning 2.086s)
               Value function loss: 62758.7920
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 6892.51
               Mean episode length: 327.18
                 Mean success rate: 59.00
                  Mean reward/step: 20.94
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14835712
                    Iteration time: 2.53s
                        Total time: 4648.87s
                               ETA: 5621.8s

################################################################################
                     [1m Learning iteration 1811/4000 [0m

                       Computation: 3217 steps/s (collection: 0.484s, learning 2.062s)
               Value function loss: 73166.2139
                    Surrogate loss: 0.0128
             Mean action noise std: 0.92
                       Mean reward: 6275.96
               Mean episode length: 308.28
                 Mean success rate: 55.50
                  Mean reward/step: 21.26
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 2.55s
                        Total time: 4651.42s
                               ETA: 5619.2s

################################################################################
                     [1m Learning iteration 1812/4000 [0m

                       Computation: 3180 steps/s (collection: 0.494s, learning 2.082s)
               Value function loss: 95343.1838
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 6177.90
               Mean episode length: 301.04
                 Mean success rate: 55.00
                  Mean reward/step: 21.58
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14852096
                    Iteration time: 2.58s
                        Total time: 4653.99s
                               ETA: 5616.6s

################################################################################
                     [1m Learning iteration 1813/4000 [0m

                       Computation: 3234 steps/s (collection: 0.463s, learning 2.070s)
               Value function loss: 83769.4698
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 6133.26
               Mean episode length: 301.32
                 Mean success rate: 55.00
                  Mean reward/step: 21.34
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 2.53s
                        Total time: 4656.53s
                               ETA: 5614.0s

################################################################################
                     [1m Learning iteration 1814/4000 [0m

                       Computation: 3233 steps/s (collection: 0.457s, learning 2.077s)
               Value function loss: 77207.8536
                    Surrogate loss: 0.0119
             Mean action noise std: 0.92
                       Mean reward: 6066.26
               Mean episode length: 304.72
                 Mean success rate: 54.50
                  Mean reward/step: 21.08
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14868480
                    Iteration time: 2.53s
                        Total time: 4659.06s
                               ETA: 5611.4s

################################################################################
                     [1m Learning iteration 1815/4000 [0m

                       Computation: 3104 steps/s (collection: 0.524s, learning 2.115s)
               Value function loss: 105623.0859
                    Surrogate loss: 0.0148
             Mean action noise std: 0.92
                       Mean reward: 6103.50
               Mean episode length: 307.29
                 Mean success rate: 55.00
                  Mean reward/step: 21.54
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 2.64s
                        Total time: 4661.70s
                               ETA: 5608.9s

################################################################################
                     [1m Learning iteration 1816/4000 [0m

                       Computation: 3159 steps/s (collection: 0.471s, learning 2.122s)
               Value function loss: 83058.2712
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 5971.87
               Mean episode length: 301.31
                 Mean success rate: 53.50
                  Mean reward/step: 21.35
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14884864
                    Iteration time: 2.59s
                        Total time: 4664.29s
                               ETA: 5606.4s

################################################################################
                     [1m Learning iteration 1817/4000 [0m

                       Computation: 3205 steps/s (collection: 0.504s, learning 2.051s)
               Value function loss: 81306.7894
                    Surrogate loss: 0.0152
             Mean action noise std: 0.92
                       Mean reward: 6277.21
               Mean episode length: 307.03
                 Mean success rate: 56.00
                  Mean reward/step: 22.19
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 2.56s
                        Total time: 4666.85s
                               ETA: 5603.8s

################################################################################
                     [1m Learning iteration 1818/4000 [0m

                       Computation: 3172 steps/s (collection: 0.517s, learning 2.065s)
               Value function loss: 119410.8445
                    Surrogate loss: 0.0171
             Mean action noise std: 0.92
                       Mean reward: 6409.83
               Mean episode length: 310.21
                 Mean success rate: 56.50
                  Mean reward/step: 22.70
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 14901248
                    Iteration time: 2.58s
                        Total time: 4669.43s
                               ETA: 5601.3s

################################################################################
                     [1m Learning iteration 1819/4000 [0m

                       Computation: 3202 steps/s (collection: 0.491s, learning 2.067s)
               Value function loss: 91490.6353
                    Surrogate loss: 0.0116
             Mean action noise std: 0.92
                       Mean reward: 6905.98
               Mean episode length: 324.46
                 Mean success rate: 60.00
                  Mean reward/step: 22.31
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 2.56s
                        Total time: 4671.99s
                               ETA: 5598.7s

################################################################################
                     [1m Learning iteration 1820/4000 [0m

                       Computation: 3200 steps/s (collection: 0.485s, learning 2.075s)
               Value function loss: 84012.6563
                    Surrogate loss: 0.0157
             Mean action noise std: 0.92
                       Mean reward: 6992.55
               Mean episode length: 327.77
                 Mean success rate: 61.50
                  Mean reward/step: 22.62
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14917632
                    Iteration time: 2.56s
                        Total time: 4674.55s
                               ETA: 5596.1s

################################################################################
                     [1m Learning iteration 1821/4000 [0m

                       Computation: 3100 steps/s (collection: 0.535s, learning 2.107s)
               Value function loss: 92381.5075
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 7156.89
               Mean episode length: 332.94
                 Mean success rate: 62.00
                  Mean reward/step: 22.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 2.64s
                        Total time: 4677.19s
                               ETA: 5593.6s

################################################################################
                     [1m Learning iteration 1822/4000 [0m

                       Computation: 3168 steps/s (collection: 0.492s, learning 2.093s)
               Value function loss: 73883.9257
                    Surrogate loss: 0.0177
             Mean action noise std: 0.92
                       Mean reward: 7504.06
               Mean episode length: 342.41
                 Mean success rate: 65.00
                  Mean reward/step: 23.16
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14934016
                    Iteration time: 2.59s
                        Total time: 4679.77s
                               ETA: 5591.1s

################################################################################
                     [1m Learning iteration 1823/4000 [0m

                       Computation: 3178 steps/s (collection: 0.503s, learning 2.073s)
               Value function loss: 73782.4963
                    Surrogate loss: 0.0132
             Mean action noise std: 0.92
                       Mean reward: 6931.79
               Mean episode length: 317.21
                 Mean success rate: 60.50
                  Mean reward/step: 23.30
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.58s
                        Total time: 4682.35s
                               ETA: 5588.5s

################################################################################
                     [1m Learning iteration 1824/4000 [0m

                       Computation: 3129 steps/s (collection: 0.539s, learning 2.079s)
               Value function loss: 97853.5804
                    Surrogate loss: 0.0156
             Mean action noise std: 0.92
                       Mean reward: 6982.31
               Mean episode length: 314.88
                 Mean success rate: 60.50
                  Mean reward/step: 22.91
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14950400
                    Iteration time: 2.62s
                        Total time: 4684.97s
                               ETA: 5586.0s

################################################################################
                     [1m Learning iteration 1825/4000 [0m

                       Computation: 3207 steps/s (collection: 0.485s, learning 2.069s)
               Value function loss: 114852.6152
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 7162.84
               Mean episode length: 324.49
                 Mean success rate: 62.00
                  Mean reward/step: 21.62
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 2.55s
                        Total time: 4687.52s
                               ETA: 5583.4s

################################################################################
                     [1m Learning iteration 1826/4000 [0m

                       Computation: 3175 steps/s (collection: 0.494s, learning 2.086s)
               Value function loss: 102550.6588
                    Surrogate loss: 0.0134
             Mean action noise std: 0.92
                       Mean reward: 7434.78
               Mean episode length: 335.27
                 Mean success rate: 64.50
                  Mean reward/step: 21.54
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14966784
                    Iteration time: 2.58s
                        Total time: 4690.10s
                               ETA: 5580.9s

################################################################################
                     [1m Learning iteration 1827/4000 [0m

                       Computation: 3208 steps/s (collection: 0.477s, learning 2.076s)
               Value function loss: 114754.6099
                    Surrogate loss: 0.0141
             Mean action noise std: 0.92
                       Mean reward: 7642.99
               Mean episode length: 346.30
                 Mean success rate: 67.50
                  Mean reward/step: 21.11
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 2.55s
                        Total time: 4692.66s
                               ETA: 5578.3s

################################################################################
                     [1m Learning iteration 1828/4000 [0m

                       Computation: 3232 steps/s (collection: 0.457s, learning 2.077s)
               Value function loss: 100709.6494
                    Surrogate loss: 0.0106
             Mean action noise std: 0.92
                       Mean reward: 7573.73
               Mean episode length: 342.94
                 Mean success rate: 67.00
                  Mean reward/step: 20.81
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14983168
                    Iteration time: 2.53s
                        Total time: 4695.19s
                               ETA: 5575.7s

################################################################################
                     [1m Learning iteration 1829/4000 [0m

                       Computation: 3245 steps/s (collection: 0.461s, learning 2.063s)
               Value function loss: 109945.8947
                    Surrogate loss: 0.0102
             Mean action noise std: 0.92
                       Mean reward: 7592.87
               Mean episode length: 344.04
                 Mean success rate: 67.50
                  Mean reward/step: 21.12
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 2.52s
                        Total time: 4697.71s
                               ETA: 5573.1s

################################################################################
                     [1m Learning iteration 1830/4000 [0m

                       Computation: 3234 steps/s (collection: 0.469s, learning 2.064s)
               Value function loss: 60264.0231
                    Surrogate loss: 0.0126
             Mean action noise std: 0.92
                       Mean reward: 7396.07
               Mean episode length: 335.58
                 Mean success rate: 66.00
                  Mean reward/step: 21.45
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14999552
                    Iteration time: 2.53s
                        Total time: 4700.25s
                               ETA: 5570.5s

################################################################################
                     [1m Learning iteration 1831/4000 [0m

                       Computation: 3136 steps/s (collection: 0.522s, learning 2.090s)
               Value function loss: 73083.7920
                    Surrogate loss: 0.0121
             Mean action noise std: 0.92
                       Mean reward: 7891.77
               Mean episode length: 350.01
                 Mean success rate: 68.50
                  Mean reward/step: 22.07
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 2.61s
                        Total time: 4702.86s
                               ETA: 5568.0s

################################################################################
                     [1m Learning iteration 1832/4000 [0m

                       Computation: 3257 steps/s (collection: 0.489s, learning 2.026s)
               Value function loss: 99467.7490
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 8148.25
               Mean episode length: 357.38
                 Mean success rate: 70.50
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15015936
                    Iteration time: 2.52s
                        Total time: 4705.37s
                               ETA: 5565.3s

################################################################################
                     [1m Learning iteration 1833/4000 [0m

                       Computation: 3245 steps/s (collection: 0.455s, learning 2.069s)
               Value function loss: 78839.6561
                    Surrogate loss: 0.0155
             Mean action noise std: 0.92
                       Mean reward: 8198.38
               Mean episode length: 358.64
                 Mean success rate: 71.00
                  Mean reward/step: 23.39
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 2.52s
                        Total time: 4707.90s
                               ETA: 5562.7s

################################################################################
                     [1m Learning iteration 1834/4000 [0m

                       Computation: 3188 steps/s (collection: 0.516s, learning 2.054s)
               Value function loss: 130455.3354
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 8045.75
               Mean episode length: 355.40
                 Mean success rate: 69.50
                  Mean reward/step: 23.07
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 15032320
                    Iteration time: 2.57s
                        Total time: 4710.47s
                               ETA: 5560.1s

################################################################################
                     [1m Learning iteration 1835/4000 [0m

                       Computation: 3183 steps/s (collection: 0.495s, learning 2.079s)
               Value function loss: 76727.8387
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 8067.18
               Mean episode length: 354.49
                 Mean success rate: 68.50
                  Mean reward/step: 22.03
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 2.57s
                        Total time: 4713.04s
                               ETA: 5557.6s

################################################################################
                     [1m Learning iteration 1836/4000 [0m

                       Computation: 3217 steps/s (collection: 0.492s, learning 2.054s)
               Value function loss: 96013.2646
                    Surrogate loss: 0.0168
             Mean action noise std: 0.92
                       Mean reward: 7670.16
               Mean episode length: 342.61
                 Mean success rate: 65.00
                  Mean reward/step: 22.03
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15048704
                    Iteration time: 2.55s
                        Total time: 4715.59s
                               ETA: 5555.0s

################################################################################
                     [1m Learning iteration 1837/4000 [0m

                       Computation: 3213 steps/s (collection: 0.498s, learning 2.051s)
               Value function loss: 59340.7010
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 7280.80
               Mean episode length: 331.53
                 Mean success rate: 63.50
                  Mean reward/step: 22.25
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 2.55s
                        Total time: 4718.13s
                               ETA: 5552.4s

################################################################################
                     [1m Learning iteration 1838/4000 [0m

                       Computation: 3243 steps/s (collection: 0.466s, learning 2.060s)
               Value function loss: 99265.3763
                    Surrogate loss: 0.0162
             Mean action noise std: 0.92
                       Mean reward: 7467.35
               Mean episode length: 339.44
                 Mean success rate: 65.50
                  Mean reward/step: 22.10
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15065088
                    Iteration time: 2.53s
                        Total time: 4720.66s
                               ETA: 5549.8s

################################################################################
                     [1m Learning iteration 1839/4000 [0m

                       Computation: 3217 steps/s (collection: 0.476s, learning 2.069s)
               Value function loss: 93735.2199
                    Surrogate loss: 0.0109
             Mean action noise std: 0.92
                       Mean reward: 7667.09
               Mean episode length: 347.59
                 Mean success rate: 67.00
                  Mean reward/step: 22.13
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 2.55s
                        Total time: 4723.21s
                               ETA: 5547.2s

################################################################################
                     [1m Learning iteration 1840/4000 [0m

                       Computation: 3158 steps/s (collection: 0.494s, learning 2.099s)
               Value function loss: 101210.3594
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 7200.52
               Mean episode length: 337.72
                 Mean success rate: 64.50
                  Mean reward/step: 22.44
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 15081472
                    Iteration time: 2.59s
                        Total time: 4725.80s
                               ETA: 5544.7s

################################################################################
                     [1m Learning iteration 1841/4000 [0m

                       Computation: 3260 steps/s (collection: 0.441s, learning 2.071s)
               Value function loss: 78057.8796
                    Surrogate loss: 0.0141
             Mean action noise std: 0.92
                       Mean reward: 7053.16
               Mean episode length: 330.63
                 Mean success rate: 62.50
                  Mean reward/step: 22.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 2.51s
                        Total time: 4728.31s
                               ETA: 5542.0s

################################################################################
                     [1m Learning iteration 1842/4000 [0m

                       Computation: 3266 steps/s (collection: 0.454s, learning 2.053s)
               Value function loss: 76042.1829
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 7269.53
               Mean episode length: 338.22
                 Mean success rate: 63.50
                  Mean reward/step: 22.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15097856
                    Iteration time: 2.51s
                        Total time: 4730.82s
                               ETA: 5539.4s

################################################################################
                     [1m Learning iteration 1843/4000 [0m

                       Computation: 3241 steps/s (collection: 0.472s, learning 2.055s)
               Value function loss: 94587.6125
                    Surrogate loss: 0.0172
             Mean action noise std: 0.92
                       Mean reward: 7427.70
               Mean episode length: 344.19
                 Mean success rate: 65.00
                  Mean reward/step: 23.33
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 2.53s
                        Total time: 4733.35s
                               ETA: 5536.8s

################################################################################
                     [1m Learning iteration 1844/4000 [0m

                       Computation: 3203 steps/s (collection: 0.497s, learning 2.060s)
               Value function loss: 84445.0730
                    Surrogate loss: 0.0134
             Mean action noise std: 0.92
                       Mean reward: 7686.11
               Mean episode length: 351.99
                 Mean success rate: 66.00
                  Mean reward/step: 22.52
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15114240
                    Iteration time: 2.56s
                        Total time: 4735.90s
                               ETA: 5534.2s

################################################################################
                     [1m Learning iteration 1845/4000 [0m

                       Computation: 3198 steps/s (collection: 0.493s, learning 2.068s)
               Value function loss: 87630.5869
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 8106.69
               Mean episode length: 364.95
                 Mean success rate: 68.50
                  Mean reward/step: 22.79
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 2.56s
                        Total time: 4738.47s
                               ETA: 5531.6s

################################################################################
                     [1m Learning iteration 1846/4000 [0m

                       Computation: 3258 steps/s (collection: 0.465s, learning 2.048s)
               Value function loss: 105001.6882
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 7629.08
               Mean episode length: 347.61
                 Mean success rate: 63.50
                  Mean reward/step: 22.42
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 15130624
                    Iteration time: 2.51s
                        Total time: 4740.98s
                               ETA: 5529.0s

################################################################################
                     [1m Learning iteration 1847/4000 [0m

                       Computation: 3215 steps/s (collection: 0.481s, learning 2.067s)
               Value function loss: 57823.5131
                    Surrogate loss: 0.0123
             Mean action noise std: 0.92
                       Mean reward: 7609.46
               Mean episode length: 348.68
                 Mean success rate: 64.50
                  Mean reward/step: 22.27
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 2.55s
                        Total time: 4743.53s
                               ETA: 5526.4s

################################################################################
                     [1m Learning iteration 1848/4000 [0m

                       Computation: 3251 steps/s (collection: 0.472s, learning 2.047s)
               Value function loss: 84935.0250
                    Surrogate loss: 0.0128
             Mean action noise std: 0.92
                       Mean reward: 7774.29
               Mean episode length: 351.69
                 Mean success rate: 65.50
                  Mean reward/step: 22.80
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15147008
                    Iteration time: 2.52s
                        Total time: 4746.05s
                               ETA: 5523.8s

################################################################################
                     [1m Learning iteration 1849/4000 [0m

                       Computation: 3251 steps/s (collection: 0.446s, learning 2.074s)
               Value function loss: 93556.1521
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 7904.56
               Mean episode length: 356.09
                 Mean success rate: 66.50
                  Mean reward/step: 22.84
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 2.52s
                        Total time: 4748.57s
                               ETA: 5521.2s

################################################################################
                     [1m Learning iteration 1850/4000 [0m

                       Computation: 3251 steps/s (collection: 0.470s, learning 2.050s)
               Value function loss: 109553.4959
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 7820.25
               Mean episode length: 350.46
                 Mean success rate: 66.00
                  Mean reward/step: 21.69
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 15163392
                    Iteration time: 2.52s
                        Total time: 4751.09s
                               ETA: 5518.5s

################################################################################
                     [1m Learning iteration 1851/4000 [0m

                       Computation: 3260 steps/s (collection: 0.436s, learning 2.076s)
               Value function loss: 58357.2975
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 7638.11
               Mean episode length: 342.01
                 Mean success rate: 64.50
                  Mean reward/step: 21.64
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 2.51s
                        Total time: 4753.60s
                               ETA: 5515.9s

################################################################################
                     [1m Learning iteration 1852/4000 [0m

                       Computation: 3273 steps/s (collection: 0.450s, learning 2.053s)
               Value function loss: 73027.6120
                    Surrogate loss: 0.0162
             Mean action noise std: 0.92
                       Mean reward: 7648.10
               Mean episode length: 339.32
                 Mean success rate: 64.50
                  Mean reward/step: 22.63
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15179776
                    Iteration time: 2.50s
                        Total time: 4756.10s
                               ETA: 5513.3s

################################################################################
                     [1m Learning iteration 1853/4000 [0m

                       Computation: 3278 steps/s (collection: 0.457s, learning 2.041s)
               Value function loss: 85600.2527
                    Surrogate loss: 0.0155
             Mean action noise std: 0.92
                       Mean reward: 7536.75
               Mean episode length: 336.08
                 Mean success rate: 65.50
                  Mean reward/step: 22.98
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 2.50s
                        Total time: 4758.60s
                               ETA: 5510.6s

################################################################################
                     [1m Learning iteration 1854/4000 [0m

                       Computation: 3284 steps/s (collection: 0.466s, learning 2.028s)
               Value function loss: 84196.0369
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 7256.64
               Mean episode length: 327.35
                 Mean success rate: 62.50
                  Mean reward/step: 22.78
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 15196160
                    Iteration time: 2.49s
                        Total time: 4761.09s
                               ETA: 5508.0s

################################################################################
                     [1m Learning iteration 1855/4000 [0m

                       Computation: 3253 steps/s (collection: 0.449s, learning 2.069s)
               Value function loss: 97824.3428
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 7565.61
               Mean episode length: 336.55
                 Mean success rate: 66.00
                  Mean reward/step: 22.50
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 2.52s
                        Total time: 4763.61s
                               ETA: 5505.4s

################################################################################
                     [1m Learning iteration 1856/4000 [0m

                       Computation: 3222 steps/s (collection: 0.460s, learning 2.082s)
               Value function loss: 121211.0593
                    Surrogate loss: 0.0134
             Mean action noise std: 0.92
                       Mean reward: 7583.07
               Mean episode length: 336.19
                 Mean success rate: 65.50
                  Mean reward/step: 21.78
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 15212544
                    Iteration time: 2.54s
                        Total time: 4766.15s
                               ETA: 5502.8s

################################################################################
                     [1m Learning iteration 1857/4000 [0m

                       Computation: 3202 steps/s (collection: 0.536s, learning 2.022s)
               Value function loss: 69107.1292
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 7305.34
               Mean episode length: 325.25
                 Mean success rate: 64.00
                  Mean reward/step: 21.17
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 2.56s
                        Total time: 4768.71s
                               ETA: 5500.2s

################################################################################
                     [1m Learning iteration 1858/4000 [0m

                       Computation: 3230 steps/s (collection: 0.460s, learning 2.077s)
               Value function loss: 74351.3759
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 7273.79
               Mean episode length: 327.33
                 Mean success rate: 64.50
                  Mean reward/step: 21.82
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15228928
                    Iteration time: 2.54s
                        Total time: 4771.25s
                               ETA: 5497.6s

################################################################################
                     [1m Learning iteration 1859/4000 [0m

                       Computation: 3262 steps/s (collection: 0.457s, learning 2.055s)
               Value function loss: 103286.3606
                    Surrogate loss: 0.0137
             Mean action noise std: 0.92
                       Mean reward: 7028.70
               Mean episode length: 322.07
                 Mean success rate: 63.00
                  Mean reward/step: 21.89
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 2.51s
                        Total time: 4773.76s
                               ETA: 5495.0s

################################################################################
                     [1m Learning iteration 1860/4000 [0m

                       Computation: 3195 steps/s (collection: 0.479s, learning 2.085s)
               Value function loss: 103030.9111
                    Surrogate loss: 0.0124
             Mean action noise std: 0.92
                       Mean reward: 6705.33
               Mean episode length: 309.34
                 Mean success rate: 60.00
                  Mean reward/step: 21.67
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 15245312
                    Iteration time: 2.56s
                        Total time: 4776.32s
                               ETA: 5492.4s

################################################################################
                     [1m Learning iteration 1861/4000 [0m

                       Computation: 3247 steps/s (collection: 0.457s, learning 2.066s)
               Value function loss: 100259.7957
                    Surrogate loss: 0.0118
             Mean action noise std: 0.92
                       Mean reward: 6965.79
               Mean episode length: 316.99
                 Mean success rate: 62.50
                  Mean reward/step: 21.16
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 2.52s
                        Total time: 4778.85s
                               ETA: 5489.8s

################################################################################
                     [1m Learning iteration 1862/4000 [0m

                       Computation: 3260 steps/s (collection: 0.435s, learning 2.077s)
               Value function loss: 94709.1543
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 6981.73
               Mean episode length: 315.12
                 Mean success rate: 62.00
                  Mean reward/step: 20.88
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15261696
                    Iteration time: 2.51s
                        Total time: 4781.36s
                               ETA: 5487.1s

################################################################################
                     [1m Learning iteration 1863/4000 [0m

                       Computation: 3261 steps/s (collection: 0.424s, learning 2.087s)
               Value function loss: 49580.0617
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 6831.16
               Mean episode length: 306.72
                 Mean success rate: 61.00
                  Mean reward/step: 21.88
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 2.51s
                        Total time: 4783.87s
                               ETA: 5484.5s

################################################################################
                     [1m Learning iteration 1864/4000 [0m

                       Computation: 3273 steps/s (collection: 0.439s, learning 2.063s)
               Value function loss: 81197.2142
                    Surrogate loss: 0.0152
             Mean action noise std: 0.92
                       Mean reward: 6677.84
               Mean episode length: 302.56
                 Mean success rate: 59.00
                  Mean reward/step: 22.71
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15278080
                    Iteration time: 2.50s
                        Total time: 4786.37s
                               ETA: 5481.9s

################################################################################
                     [1m Learning iteration 1865/4000 [0m

                       Computation: 3254 steps/s (collection: 0.445s, learning 2.072s)
               Value function loss: 118407.6187
                    Surrogate loss: 0.0115
             Mean action noise std: 0.92
                       Mean reward: 7031.73
               Mean episode length: 313.03
                 Mean success rate: 60.50
                  Mean reward/step: 22.84
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 2.52s
                        Total time: 4788.89s
                               ETA: 5479.2s

################################################################################
                     [1m Learning iteration 1866/4000 [0m

                       Computation: 3145 steps/s (collection: 0.503s, learning 2.101s)
               Value function loss: 78095.9439
                    Surrogate loss: 0.0121
             Mean action noise std: 0.92
                       Mean reward: 7131.63
               Mean episode length: 313.44
                 Mean success rate: 61.50
                  Mean reward/step: 21.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15294464
                    Iteration time: 2.60s
                        Total time: 4791.49s
                               ETA: 5476.7s

################################################################################
                     [1m Learning iteration 1867/4000 [0m

                       Computation: 3177 steps/s (collection: 0.508s, learning 2.070s)
               Value function loss: 64513.6359
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 6951.04
               Mean episode length: 305.14
                 Mean success rate: 59.50
                  Mean reward/step: 21.60
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 2.58s
                        Total time: 4794.07s
                               ETA: 5474.2s

################################################################################
                     [1m Learning iteration 1868/4000 [0m

                       Computation: 3070 steps/s (collection: 0.562s, learning 2.106s)
               Value function loss: 68734.2732
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 6922.22
               Mean episode length: 302.10
                 Mean success rate: 58.50
                  Mean reward/step: 22.10
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15310848
                    Iteration time: 2.67s
                        Total time: 4796.74s
                               ETA: 5471.7s

################################################################################
                     [1m Learning iteration 1869/4000 [0m

                       Computation: 3163 steps/s (collection: 0.496s, learning 2.093s)
               Value function loss: 86545.1048
                    Surrogate loss: 0.0104
             Mean action noise std: 0.92
                       Mean reward: 6939.28
               Mean episode length: 305.93
                 Mean success rate: 59.00
                  Mean reward/step: 22.35
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 2.59s
                        Total time: 4799.33s
                               ETA: 5469.2s

################################################################################
                     [1m Learning iteration 1870/4000 [0m

                       Computation: 3224 steps/s (collection: 0.493s, learning 2.048s)
               Value function loss: 93844.9380
                    Surrogate loss: 0.0117
             Mean action noise std: 0.92
                       Mean reward: 7165.41
               Mean episode length: 320.73
                 Mean success rate: 61.00
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15327232
                    Iteration time: 2.54s
                        Total time: 4801.87s
                               ETA: 5466.6s

################################################################################
                     [1m Learning iteration 1871/4000 [0m

                       Computation: 3138 steps/s (collection: 0.519s, learning 2.091s)
               Value function loss: 97792.4274
                    Surrogate loss: 0.0114
             Mean action noise std: 0.92
                       Mean reward: 7061.34
               Mean episode length: 319.81
                 Mean success rate: 60.50
                  Mean reward/step: 21.85
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 2.61s
                        Total time: 4804.48s
                               ETA: 5464.1s

################################################################################
                     [1m Learning iteration 1872/4000 [0m

                       Computation: 3191 steps/s (collection: 0.478s, learning 2.088s)
               Value function loss: 115680.2289
                    Surrogate loss: 0.0111
             Mean action noise std: 0.92
                       Mean reward: 7369.79
               Mean episode length: 335.62
                 Mean success rate: 63.00
                  Mean reward/step: 20.99
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 15343616
                    Iteration time: 2.57s
                        Total time: 4807.04s
                               ETA: 5461.5s

################################################################################
                     [1m Learning iteration 1873/4000 [0m

                       Computation: 3166 steps/s (collection: 0.516s, learning 2.071s)
               Value function loss: 68225.7421
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 6767.48
               Mean episode length: 316.08
                 Mean success rate: 59.00
                  Mean reward/step: 21.16
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 2.59s
                        Total time: 4809.63s
                               ETA: 5459.0s

################################################################################
                     [1m Learning iteration 1874/4000 [0m

                       Computation: 3208 steps/s (collection: 0.456s, learning 2.097s)
               Value function loss: 93797.3049
                    Surrogate loss: 0.0119
             Mean action noise std: 0.92
                       Mean reward: 6813.46
               Mean episode length: 321.58
                 Mean success rate: 60.00
                  Mean reward/step: 21.33
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15360000
                    Iteration time: 2.55s
                        Total time: 4812.18s
                               ETA: 5456.4s

################################################################################
                     [1m Learning iteration 1875/4000 [0m

                       Computation: 3274 steps/s (collection: 0.431s, learning 2.070s)
               Value function loss: 102695.9068
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 7380.99
               Mean episode length: 345.39
                 Mean success rate: 64.50
                  Mean reward/step: 21.76
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 2.50s
                        Total time: 4814.69s
                               ETA: 5453.7s

################################################################################
                     [1m Learning iteration 1876/4000 [0m

                       Computation: 3197 steps/s (collection: 0.479s, learning 2.083s)
               Value function loss: 112730.3623
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 7632.70
               Mean episode length: 351.61
                 Mean success rate: 66.50
                  Mean reward/step: 20.91
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15376384
                    Iteration time: 2.56s
                        Total time: 4817.25s
                               ETA: 5451.2s

################################################################################
                     [1m Learning iteration 1877/4000 [0m

                       Computation: 3081 steps/s (collection: 0.527s, learning 2.131s)
               Value function loss: 104181.2729
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 7427.00
               Mean episode length: 335.93
                 Mean success rate: 64.50
                  Mean reward/step: 20.45
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 2.66s
                        Total time: 4819.91s
                               ETA: 5448.7s

################################################################################
                     [1m Learning iteration 1878/4000 [0m

                       Computation: 3120 steps/s (collection: 0.487s, learning 2.138s)
               Value function loss: 62410.1892
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 7378.44
               Mean episode length: 333.87
                 Mean success rate: 63.50
                  Mean reward/step: 21.55
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15392768
                    Iteration time: 2.62s
                        Total time: 4822.53s
                               ETA: 5446.2s

################################################################################
                     [1m Learning iteration 1879/4000 [0m

                       Computation: 3198 steps/s (collection: 0.473s, learning 2.088s)
               Value function loss: 65939.2980
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 7413.89
               Mean episode length: 336.92
                 Mean success rate: 64.00
                  Mean reward/step: 22.82
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 2.56s
                        Total time: 4825.09s
                               ETA: 5443.6s

################################################################################
                     [1m Learning iteration 1880/4000 [0m

                       Computation: 3219 steps/s (collection: 0.451s, learning 2.093s)
               Value function loss: 91788.3631
                    Surrogate loss: 0.0162
             Mean action noise std: 0.92
                       Mean reward: 7504.36
               Mean episode length: 333.97
                 Mean success rate: 64.00
                  Mean reward/step: 23.15
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15409152
                    Iteration time: 2.54s
                        Total time: 4827.64s
                               ETA: 5441.0s

################################################################################
                     [1m Learning iteration 1881/4000 [0m

                       Computation: 3139 steps/s (collection: 0.481s, learning 2.128s)
               Value function loss: 96702.4256
                    Surrogate loss: 0.0122
             Mean action noise std: 0.92
                       Mean reward: 7781.26
               Mean episode length: 347.77
                 Mean success rate: 67.00
                  Mean reward/step: 23.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 2.61s
                        Total time: 4830.25s
                               ETA: 5438.5s

################################################################################
                     [1m Learning iteration 1882/4000 [0m

                       Computation: 3151 steps/s (collection: 0.478s, learning 2.121s)
               Value function loss: 77368.2873
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 7816.70
               Mean episode length: 351.65
                 Mean success rate: 67.00
                  Mean reward/step: 22.79
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15425536
                    Iteration time: 2.60s
                        Total time: 4832.85s
                               ETA: 5436.0s

################################################################################
                     [1m Learning iteration 1883/4000 [0m

                       Computation: 3094 steps/s (collection: 0.504s, learning 2.143s)
               Value function loss: 69519.0377
                    Surrogate loss: 0.0131
             Mean action noise std: 0.92
                       Mean reward: 7826.17
               Mean episode length: 350.93
                 Mean success rate: 66.00
                  Mean reward/step: 23.37
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 2.65s
                        Total time: 4835.49s
                               ETA: 5433.5s

################################################################################
                     [1m Learning iteration 1884/4000 [0m

                       Computation: 3192 steps/s (collection: 0.484s, learning 2.082s)
               Value function loss: 77631.4257
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 7846.92
               Mean episode length: 351.56
                 Mean success rate: 66.50
                  Mean reward/step: 23.98
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15441920
                    Iteration time: 2.57s
                        Total time: 4838.06s
                               ETA: 5430.9s

################################################################################
                     [1m Learning iteration 1885/4000 [0m

                       Computation: 3234 steps/s (collection: 0.440s, learning 2.093s)
               Value function loss: 78645.2150
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 7838.03
               Mean episode length: 353.82
                 Mean success rate: 66.50
                  Mean reward/step: 23.92
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 2.53s
                        Total time: 4840.59s
                               ETA: 5428.3s

################################################################################
                     [1m Learning iteration 1886/4000 [0m

                       Computation: 3182 steps/s (collection: 0.467s, learning 2.107s)
               Value function loss: 74319.6630
                    Surrogate loss: 0.0118
             Mean action noise std: 0.92
                       Mean reward: 7963.41
               Mean episode length: 361.21
                 Mean success rate: 67.50
                  Mean reward/step: 23.99
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15458304
                    Iteration time: 2.57s
                        Total time: 4843.16s
                               ETA: 5425.8s

################################################################################
                     [1m Learning iteration 1887/4000 [0m

                       Computation: 3170 steps/s (collection: 0.456s, learning 2.128s)
               Value function loss: 150367.8785
                    Surrogate loss: 0.0113
             Mean action noise std: 0.92
                       Mean reward: 8725.77
               Mean episode length: 389.25
                 Mean success rate: 73.00
                  Mean reward/step: 23.97
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 2.58s
                        Total time: 4845.75s
                               ETA: 5423.2s

################################################################################
                     [1m Learning iteration 1888/4000 [0m

                       Computation: 3214 steps/s (collection: 0.440s, learning 2.109s)
               Value function loss: 109024.5262
                    Surrogate loss: 0.0124
             Mean action noise std: 0.92
                       Mean reward: 8968.46
               Mean episode length: 396.46
                 Mean success rate: 75.00
                  Mean reward/step: 22.43
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 15474688
                    Iteration time: 2.55s
                        Total time: 4848.30s
                               ETA: 5420.6s

################################################################################
                     [1m Learning iteration 1889/4000 [0m

                       Computation: 3252 steps/s (collection: 0.461s, learning 2.058s)
               Value function loss: 73407.3161
                    Surrogate loss: 0.0148
             Mean action noise std: 0.92
                       Mean reward: 8573.40
               Mean episode length: 384.11
                 Mean success rate: 72.50
                  Mean reward/step: 22.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 2.52s
                        Total time: 4850.82s
                               ETA: 5418.0s

################################################################################
                     [1m Learning iteration 1890/4000 [0m

                       Computation: 3192 steps/s (collection: 0.484s, learning 2.081s)
               Value function loss: 89799.7786
                    Surrogate loss: 0.0155
             Mean action noise std: 0.92
                       Mean reward: 8695.62
               Mean episode length: 387.54
                 Mean success rate: 73.00
                  Mean reward/step: 22.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15491072
                    Iteration time: 2.57s
                        Total time: 4853.38s
                               ETA: 5415.5s

################################################################################
                     [1m Learning iteration 1891/4000 [0m

                       Computation: 3193 steps/s (collection: 0.495s, learning 2.069s)
               Value function loss: 106057.6581
                    Surrogate loss: 0.0183
             Mean action noise std: 0.92
                       Mean reward: 8714.11
               Mean episode length: 385.39
                 Mean success rate: 73.50
                  Mean reward/step: 23.14
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 2.56s
                        Total time: 4855.95s
                               ETA: 5412.9s

################################################################################
                     [1m Learning iteration 1892/4000 [0m

                       Computation: 3218 steps/s (collection: 0.476s, learning 2.069s)
               Value function loss: 114618.9573
                    Surrogate loss: 0.0128
             Mean action noise std: 0.92
                       Mean reward: 8726.93
               Mean episode length: 382.55
                 Mean success rate: 74.00
                  Mean reward/step: 22.40
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 15507456
                    Iteration time: 2.55s
                        Total time: 4858.49s
                               ETA: 5410.3s

################################################################################
                     [1m Learning iteration 1893/4000 [0m

                       Computation: 3245 steps/s (collection: 0.454s, learning 2.070s)
               Value function loss: 78563.7287
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 8395.49
               Mean episode length: 370.14
                 Mean success rate: 72.50
                  Mean reward/step: 21.39
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 2.52s
                        Total time: 4861.02s
                               ETA: 5407.7s

################################################################################
                     [1m Learning iteration 1894/4000 [0m

                       Computation: 3231 steps/s (collection: 0.461s, learning 2.073s)
               Value function loss: 61217.2597
                    Surrogate loss: 0.0163
             Mean action noise std: 0.92
                       Mean reward: 8069.66
               Mean episode length: 356.33
                 Mean success rate: 70.50
                  Mean reward/step: 21.93
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15523840
                    Iteration time: 2.53s
                        Total time: 4863.55s
                               ETA: 5405.1s

################################################################################
                     [1m Learning iteration 1895/4000 [0m

                       Computation: 3279 steps/s (collection: 0.458s, learning 2.040s)
               Value function loss: 72689.9268
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 7515.76
               Mean episode length: 336.06
                 Mean success rate: 66.00
                  Mean reward/step: 22.71
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 2.50s
                        Total time: 4866.05s
                               ETA: 5402.4s

################################################################################
                     [1m Learning iteration 1896/4000 [0m

                       Computation: 3307 steps/s (collection: 0.432s, learning 2.044s)
               Value function loss: 76563.1810
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 7219.72
               Mean episode length: 330.01
                 Mean success rate: 63.50
                  Mean reward/step: 22.63
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15540224
                    Iteration time: 2.48s
                        Total time: 4868.52s
                               ETA: 5399.8s

################################################################################
                     [1m Learning iteration 1897/4000 [0m

                       Computation: 3264 steps/s (collection: 0.451s, learning 2.058s)
               Value function loss: 82662.4595
                    Surrogate loss: 0.0163
             Mean action noise std: 0.92
                       Mean reward: 7069.99
               Mean episode length: 319.89
                 Mean success rate: 61.50
                  Mean reward/step: 22.88
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 2.51s
                        Total time: 4871.03s
                               ETA: 5397.1s

################################################################################
                     [1m Learning iteration 1898/4000 [0m

                       Computation: 3271 steps/s (collection: 0.463s, learning 2.041s)
               Value function loss: 94909.8250
                    Surrogate loss: 0.0167
             Mean action noise std: 0.92
                       Mean reward: 6775.53
               Mean episode length: 311.90
                 Mean success rate: 60.00
                  Mean reward/step: 23.30
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15556608
                    Iteration time: 2.50s
                        Total time: 4873.54s
                               ETA: 5394.5s

################################################################################
                     [1m Learning iteration 1899/4000 [0m

                       Computation: 3222 steps/s (collection: 0.482s, learning 2.060s)
               Value function loss: 91708.2472
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 6599.06
               Mean episode length: 307.52
                 Mean success rate: 58.50
                  Mean reward/step: 23.06
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 2.54s
                        Total time: 4876.08s
                               ETA: 5391.9s

################################################################################
                     [1m Learning iteration 1900/4000 [0m

                       Computation: 3181 steps/s (collection: 0.446s, learning 2.129s)
               Value function loss: 68941.9014
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 6718.48
               Mean episode length: 309.68
                 Mean success rate: 59.00
                  Mean reward/step: 23.13
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15572992
                    Iteration time: 2.57s
                        Total time: 4878.65s
                               ETA: 5389.4s

################################################################################
                     [1m Learning iteration 1901/4000 [0m

                       Computation: 3203 steps/s (collection: 0.426s, learning 2.131s)
               Value function loss: 75804.5606
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 6996.07
               Mean episode length: 314.67
                 Mean success rate: 60.00
                  Mean reward/step: 23.53
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 2.56s
                        Total time: 4881.21s
                               ETA: 5386.8s

################################################################################
                     [1m Learning iteration 1902/4000 [0m

                       Computation: 3119 steps/s (collection: 0.508s, learning 2.118s)
               Value function loss: 75852.2786
                    Surrogate loss: 0.0156
             Mean action noise std: 0.92
                       Mean reward: 6580.97
               Mean episode length: 300.93
                 Mean success rate: 56.00
                  Mean reward/step: 23.98
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15589376
                    Iteration time: 2.63s
                        Total time: 4883.84s
                               ETA: 5384.3s

################################################################################
                     [1m Learning iteration 1903/4000 [0m

                       Computation: 3108 steps/s (collection: 0.530s, learning 2.105s)
               Value function loss: 102311.1037
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 6947.56
               Mean episode length: 315.21
                 Mean success rate: 59.50
                  Mean reward/step: 23.69
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 2.64s
                        Total time: 4886.47s
                               ETA: 5381.8s

################################################################################
                     [1m Learning iteration 1904/4000 [0m

                       Computation: 3206 steps/s (collection: 0.470s, learning 2.084s)
               Value function loss: 107665.9166
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 7003.84
               Mean episode length: 313.31
                 Mean success rate: 60.50
                  Mean reward/step: 22.56
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 15605760
                    Iteration time: 2.55s
                        Total time: 4889.03s
                               ETA: 5379.2s

################################################################################
                     [1m Learning iteration 1905/4000 [0m

                       Computation: 3198 steps/s (collection: 0.477s, learning 2.085s)
               Value function loss: 79038.6722
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 7419.51
               Mean episode length: 326.05
                 Mean success rate: 63.50
                  Mean reward/step: 22.92
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 2.56s
                        Total time: 4891.59s
                               ETA: 5376.6s

################################################################################
                     [1m Learning iteration 1906/4000 [0m

                       Computation: 3155 steps/s (collection: 0.511s, learning 2.085s)
               Value function loss: 88372.7641
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 7572.95
               Mean episode length: 330.31
                 Mean success rate: 64.50
                  Mean reward/step: 23.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15622144
                    Iteration time: 2.60s
                        Total time: 4894.19s
                               ETA: 5374.1s

################################################################################
                     [1m Learning iteration 1907/4000 [0m

                       Computation: 3093 steps/s (collection: 0.521s, learning 2.127s)
               Value function loss: 103376.5008
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 7648.94
               Mean episode length: 325.53
                 Mean success rate: 64.50
                  Mean reward/step: 24.14
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 2.65s
                        Total time: 4896.83s
                               ETA: 5371.6s

################################################################################
                     [1m Learning iteration 1908/4000 [0m

                       Computation: 3176 steps/s (collection: 0.480s, learning 2.098s)
               Value function loss: 120721.1705
                    Surrogate loss: 0.0179
             Mean action noise std: 0.92
                       Mean reward: 7819.49
               Mean episode length: 330.49
                 Mean success rate: 66.00
                  Mean reward/step: 23.57
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15638528
                    Iteration time: 2.58s
                        Total time: 4899.41s
                               ETA: 5369.1s

################################################################################
                     [1m Learning iteration 1909/4000 [0m

                       Computation: 3211 steps/s (collection: 0.476s, learning 2.075s)
               Value function loss: 67423.8662
                    Surrogate loss: 0.0175
             Mean action noise std: 0.92
                       Mean reward: 7892.16
               Mean episode length: 334.41
                 Mean success rate: 67.00
                  Mean reward/step: 22.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 2.55s
                        Total time: 4901.96s
                               ETA: 5366.5s

################################################################################
                     [1m Learning iteration 1910/4000 [0m

                       Computation: 3111 steps/s (collection: 0.486s, learning 2.147s)
               Value function loss: 80407.8141
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 8280.05
               Mean episode length: 347.15
                 Mean success rate: 69.00
                  Mean reward/step: 23.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15654912
                    Iteration time: 2.63s
                        Total time: 4904.60s
                               ETA: 5364.0s

################################################################################
                     [1m Learning iteration 1911/4000 [0m

                       Computation: 3152 steps/s (collection: 0.495s, learning 2.103s)
               Value function loss: 87722.9675
                    Surrogate loss: 0.0181
             Mean action noise std: 0.92
                       Mean reward: 8117.74
               Mean episode length: 342.34
                 Mean success rate: 68.50
                  Mean reward/step: 24.27
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 2.60s
                        Total time: 4907.19s
                               ETA: 5361.5s

################################################################################
                     [1m Learning iteration 1912/4000 [0m

                       Computation: 3102 steps/s (collection: 0.545s, learning 2.095s)
               Value function loss: 105032.6762
                    Surrogate loss: 0.0157
             Mean action noise std: 0.92
                       Mean reward: 8532.69
               Mean episode length: 353.69
                 Mean success rate: 70.00
                  Mean reward/step: 23.85
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15671296
                    Iteration time: 2.64s
                        Total time: 4909.83s
                               ETA: 5359.0s

################################################################################
                     [1m Learning iteration 1913/4000 [0m

                       Computation: 3215 steps/s (collection: 0.488s, learning 2.059s)
               Value function loss: 97327.1021
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 8841.79
               Mean episode length: 364.08
                 Mean success rate: 72.50
                  Mean reward/step: 23.62
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 2.55s
                        Total time: 4912.38s
                               ETA: 5356.4s

################################################################################
                     [1m Learning iteration 1914/4000 [0m

                       Computation: 3101 steps/s (collection: 0.499s, learning 2.141s)
               Value function loss: 84248.9999
                    Surrogate loss: 0.0141
             Mean action noise std: 0.92
                       Mean reward: 8487.70
               Mean episode length: 352.46
                 Mean success rate: 70.50
                  Mean reward/step: 23.16
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15687680
                    Iteration time: 2.64s
                        Total time: 4915.02s
                               ETA: 5353.9s

################################################################################
                     [1m Learning iteration 1915/4000 [0m

                       Computation: 3134 steps/s (collection: 0.502s, learning 2.111s)
               Value function loss: 98692.6474
                    Surrogate loss: 0.0155
             Mean action noise std: 0.92
                       Mean reward: 8833.87
               Mean episode length: 363.31
                 Mean success rate: 73.00
                  Mean reward/step: 23.45
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 2.61s
                        Total time: 4917.64s
                               ETA: 5351.4s

################################################################################
                     [1m Learning iteration 1916/4000 [0m

                       Computation: 3155 steps/s (collection: 0.535s, learning 2.062s)
               Value function loss: 61938.6732
                    Surrogate loss: 0.0137
             Mean action noise std: 0.92
                       Mean reward: 8650.26
               Mean episode length: 359.80
                 Mean success rate: 71.50
                  Mean reward/step: 23.83
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15704064
                    Iteration time: 2.60s
                        Total time: 4920.23s
                               ETA: 5348.9s

################################################################################
                     [1m Learning iteration 1917/4000 [0m

                       Computation: 3207 steps/s (collection: 0.478s, learning 2.076s)
               Value function loss: 75287.7696
                    Surrogate loss: 0.0122
             Mean action noise std: 0.92
                       Mean reward: 8415.75
               Mean episode length: 353.28
                 Mean success rate: 70.00
                  Mean reward/step: 24.07
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 2.55s
                        Total time: 4922.79s
                               ETA: 5346.3s

################################################################################
                     [1m Learning iteration 1918/4000 [0m

                       Computation: 3104 steps/s (collection: 0.513s, learning 2.125s)
               Value function loss: 100862.4146
                    Surrogate loss: 0.0162
             Mean action noise std: 0.92
                       Mean reward: 8383.35
               Mean episode length: 351.27
                 Mean success rate: 69.50
                  Mean reward/step: 23.28
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 15720448
                    Iteration time: 2.64s
                        Total time: 4925.43s
                               ETA: 5343.8s

################################################################################
                     [1m Learning iteration 1919/4000 [0m

                       Computation: 3123 steps/s (collection: 0.497s, learning 2.126s)
               Value function loss: 138205.3799
                    Surrogate loss: 0.0169
             Mean action noise std: 0.92
                       Mean reward: 8474.21
               Mean episode length: 354.56
                 Mean success rate: 69.50
                  Mean reward/step: 22.20
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 2.62s
                        Total time: 4928.05s
                               ETA: 5341.3s

################################################################################
                     [1m Learning iteration 1920/4000 [0m

                       Computation: 3082 steps/s (collection: 0.549s, learning 2.108s)
               Value function loss: 111951.0938
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 8392.67
               Mean episode length: 354.82
                 Mean success rate: 69.00
                  Mean reward/step: 21.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15736832
                    Iteration time: 2.66s
                        Total time: 4930.71s
                               ETA: 5338.8s

################################################################################
                     [1m Learning iteration 1921/4000 [0m

                       Computation: 3216 steps/s (collection: 0.466s, learning 2.081s)
               Value function loss: 79304.8354
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 7919.37
               Mean episode length: 340.17
                 Mean success rate: 66.50
                  Mean reward/step: 22.86
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 2.55s
                        Total time: 4933.25s
                               ETA: 5336.2s

################################################################################
                     [1m Learning iteration 1922/4000 [0m

                       Computation: 3159 steps/s (collection: 0.487s, learning 2.106s)
               Value function loss: 87625.9327
                    Surrogate loss: 0.0187
             Mean action noise std: 0.92
                       Mean reward: 8077.20
               Mean episode length: 345.65
                 Mean success rate: 67.50
                  Mean reward/step: 23.30
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15753216
                    Iteration time: 2.59s
                        Total time: 4935.85s
                               ETA: 5333.7s

################################################################################
                     [1m Learning iteration 1923/4000 [0m

                       Computation: 3045 steps/s (collection: 0.548s, learning 2.141s)
               Value function loss: 107846.5302
                    Surrogate loss: 0.0107
             Mean action noise std: 0.92
                       Mean reward: 7736.00
               Mean episode length: 339.66
                 Mean success rate: 64.50
                  Mean reward/step: 23.45
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 2.69s
                        Total time: 4938.54s
                               ETA: 5331.3s

################################################################################
                     [1m Learning iteration 1924/4000 [0m

                       Computation: 3119 steps/s (collection: 0.514s, learning 2.113s)
               Value function loss: 108507.1013
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 8187.35
               Mean episode length: 358.12
                 Mean success rate: 69.00
                  Mean reward/step: 22.52
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15769600
                    Iteration time: 2.63s
                        Total time: 4941.16s
                               ETA: 5328.8s

################################################################################
                     [1m Learning iteration 1925/4000 [0m

                       Computation: 3140 steps/s (collection: 0.490s, learning 2.118s)
               Value function loss: 74808.4350
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 8261.35
               Mean episode length: 357.15
                 Mean success rate: 70.00
                  Mean reward/step: 22.20
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 2.61s
                        Total time: 4943.77s
                               ETA: 5326.2s

################################################################################
                     [1m Learning iteration 1926/4000 [0m

                       Computation: 3175 steps/s (collection: 0.473s, learning 2.107s)
               Value function loss: 54512.8445
                    Surrogate loss: 0.0152
             Mean action noise std: 0.92
                       Mean reward: 8127.22
               Mean episode length: 352.67
                 Mean success rate: 69.50
                  Mean reward/step: 22.69
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15785984
                    Iteration time: 2.58s
                        Total time: 4946.35s
                               ETA: 5323.7s

################################################################################
                     [1m Learning iteration 1927/4000 [0m

                       Computation: 3175 steps/s (collection: 0.477s, learning 2.103s)
               Value function loss: 103736.2375
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 7891.19
               Mean episode length: 344.86
                 Mean success rate: 68.00
                  Mean reward/step: 22.82
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 2.58s
                        Total time: 4948.93s
                               ETA: 5321.1s

################################################################################
                     [1m Learning iteration 1928/4000 [0m

                       Computation: 3145 steps/s (collection: 0.500s, learning 2.105s)
               Value function loss: 76676.3371
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 7248.68
               Mean episode length: 323.09
                 Mean success rate: 64.50
                  Mean reward/step: 22.98
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15802368
                    Iteration time: 2.60s
                        Total time: 4951.53s
                               ETA: 5318.6s

################################################################################
                     [1m Learning iteration 1929/4000 [0m

                       Computation: 3113 steps/s (collection: 0.521s, learning 2.109s)
               Value function loss: 104503.1701
                    Surrogate loss: 0.0141
             Mean action noise std: 0.92
                       Mean reward: 7637.93
               Mean episode length: 334.56
                 Mean success rate: 66.50
                  Mean reward/step: 22.63
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 2.63s
                        Total time: 4954.16s
                               ETA: 5316.1s

################################################################################
                     [1m Learning iteration 1930/4000 [0m

                       Computation: 3169 steps/s (collection: 0.483s, learning 2.102s)
               Value function loss: 66071.5051
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 7343.58
               Mean episode length: 324.25
                 Mean success rate: 64.00
                  Mean reward/step: 22.64
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15818752
                    Iteration time: 2.58s
                        Total time: 4956.75s
                               ETA: 5313.6s

################################################################################
                     [1m Learning iteration 1931/4000 [0m

                       Computation: 3152 steps/s (collection: 0.516s, learning 2.082s)
               Value function loss: 59704.2104
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 7283.30
               Mean episode length: 320.85
                 Mean success rate: 64.50
                  Mean reward/step: 22.84
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 2.60s
                        Total time: 4959.35s
                               ETA: 5311.0s

################################################################################
                     [1m Learning iteration 1932/4000 [0m

                       Computation: 3057 steps/s (collection: 0.542s, learning 2.137s)
               Value function loss: 104510.6070
                    Surrogate loss: 0.0123
             Mean action noise std: 0.92
                       Mean reward: 7547.46
               Mean episode length: 328.44
                 Mean success rate: 66.00
                  Mean reward/step: 22.86
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15835136
                    Iteration time: 2.68s
                        Total time: 4962.03s
                               ETA: 5308.6s

################################################################################
                     [1m Learning iteration 1933/4000 [0m

                       Computation: 3105 steps/s (collection: 0.535s, learning 2.102s)
               Value function loss: 82716.3748
                    Surrogate loss: 0.0148
             Mean action noise std: 0.92
                       Mean reward: 7529.18
               Mean episode length: 325.75
                 Mean success rate: 65.00
                  Mean reward/step: 22.99
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 2.64s
                        Total time: 4964.66s
                               ETA: 5306.1s

################################################################################
                     [1m Learning iteration 1934/4000 [0m

                       Computation: 3195 steps/s (collection: 0.458s, learning 2.106s)
               Value function loss: 106194.8816
                    Surrogate loss: 0.0126
             Mean action noise std: 0.92
                       Mean reward: 7807.27
               Mean episode length: 337.82
                 Mean success rate: 67.50
                  Mean reward/step: 22.80
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15851520
                    Iteration time: 2.56s
                        Total time: 4967.23s
                               ETA: 5303.5s

################################################################################
                     [1m Learning iteration 1935/4000 [0m

                       Computation: 3126 steps/s (collection: 0.555s, learning 2.065s)
               Value function loss: 118743.9668
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 8003.08
               Mean episode length: 344.71
                 Mean success rate: 67.50
                  Mean reward/step: 22.15
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 2.62s
                        Total time: 4969.85s
                               ETA: 5301.0s

################################################################################
                     [1m Learning iteration 1936/4000 [0m

                       Computation: 3119 steps/s (collection: 0.539s, learning 2.087s)
               Value function loss: 81206.2870
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 8030.23
               Mean episode length: 346.39
                 Mean success rate: 66.00
                  Mean reward/step: 21.79
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15867904
                    Iteration time: 2.63s
                        Total time: 4972.47s
                               ETA: 5298.5s

################################################################################
                     [1m Learning iteration 1937/4000 [0m

                       Computation: 3107 steps/s (collection: 0.522s, learning 2.114s)
               Value function loss: 96200.0566
                    Surrogate loss: 0.0170
             Mean action noise std: 0.92
                       Mean reward: 7606.82
               Mean episode length: 331.97
                 Mean success rate: 64.00
                  Mean reward/step: 22.33
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 2.64s
                        Total time: 4975.11s
                               ETA: 5296.0s

################################################################################
                     [1m Learning iteration 1938/4000 [0m

                       Computation: 3133 steps/s (collection: 0.525s, learning 2.090s)
               Value function loss: 65302.0566
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 7506.74
               Mean episode length: 331.38
                 Mean success rate: 64.50
                  Mean reward/step: 22.38
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15884288
                    Iteration time: 2.61s
                        Total time: 4977.72s
                               ETA: 5293.5s

################################################################################
                     [1m Learning iteration 1939/4000 [0m

                       Computation: 3119 steps/s (collection: 0.524s, learning 2.102s)
               Value function loss: 117572.9689
                    Surrogate loss: 0.0190
             Mean action noise std: 0.92
                       Mean reward: 7503.69
               Mean episode length: 336.05
                 Mean success rate: 65.00
                  Mean reward/step: 22.09
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 2.63s
                        Total time: 4980.35s
                               ETA: 5291.0s

################################################################################
                     [1m Learning iteration 1940/4000 [0m

                       Computation: 3204 steps/s (collection: 0.497s, learning 2.059s)
               Value function loss: 64049.9771
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 7502.24
               Mean episode length: 336.91
                 Mean success rate: 65.50
                  Mean reward/step: 22.21
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15900672
                    Iteration time: 2.56s
                        Total time: 4982.91s
                               ETA: 5288.4s

################################################################################
                     [1m Learning iteration 1941/4000 [0m

                       Computation: 3163 steps/s (collection: 0.512s, learning 2.077s)
               Value function loss: 97835.6333
                    Surrogate loss: 0.0184
             Mean action noise std: 0.92
                       Mean reward: 7351.47
               Mean episode length: 330.58
                 Mean success rate: 64.00
                  Mean reward/step: 22.48
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 2.59s
                        Total time: 4985.50s
                               ETA: 5285.9s

################################################################################
                     [1m Learning iteration 1942/4000 [0m

                       Computation: 3148 steps/s (collection: 0.491s, learning 2.110s)
               Value function loss: 46720.1813
                    Surrogate loss: 0.0184
             Mean action noise std: 0.92
                       Mean reward: 7366.27
               Mean episode length: 332.77
                 Mean success rate: 63.50
                  Mean reward/step: 23.25
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15917056
                    Iteration time: 2.60s
                        Total time: 4988.10s
                               ETA: 5283.3s

################################################################################
                     [1m Learning iteration 1943/4000 [0m

                       Computation: 3180 steps/s (collection: 0.514s, learning 2.062s)
               Value function loss: 91217.6804
                    Surrogate loss: 0.0122
             Mean action noise std: 0.92
                       Mean reward: 7397.32
               Mean episode length: 334.85
                 Mean success rate: 64.50
                  Mean reward/step: 23.95
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 2.58s
                        Total time: 4990.67s
                               ETA: 5280.8s

################################################################################
                     [1m Learning iteration 1944/4000 [0m

                       Computation: 3134 steps/s (collection: 0.534s, learning 2.080s)
               Value function loss: 80220.2502
                    Surrogate loss: 0.0180
             Mean action noise std: 0.92
                       Mean reward: 7277.44
               Mean episode length: 331.93
                 Mean success rate: 65.00
                  Mean reward/step: 23.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15933440
                    Iteration time: 2.61s
                        Total time: 4993.29s
                               ETA: 5278.3s

################################################################################
                     [1m Learning iteration 1945/4000 [0m

                       Computation: 3222 steps/s (collection: 0.485s, learning 2.057s)
               Value function loss: 84623.3668
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 7632.40
               Mean episode length: 344.92
                 Mean success rate: 68.00
                  Mean reward/step: 23.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 2.54s
                        Total time: 4995.83s
                               ETA: 5275.7s

################################################################################
                     [1m Learning iteration 1946/4000 [0m

                       Computation: 3169 steps/s (collection: 0.536s, learning 2.049s)
               Value function loss: 88929.6454
                    Surrogate loss: 0.0123
             Mean action noise std: 0.92
                       Mean reward: 7915.68
               Mean episode length: 350.95
                 Mean success rate: 70.00
                  Mean reward/step: 23.11
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15949824
                    Iteration time: 2.59s
                        Total time: 4998.41s
                               ETA: 5273.1s

################################################################################
                     [1m Learning iteration 1947/4000 [0m

                       Computation: 3183 steps/s (collection: 0.513s, learning 2.061s)
               Value function loss: 69706.9290
                    Surrogate loss: 0.0155
             Mean action noise std: 0.92
                       Mean reward: 8229.10
               Mean episode length: 359.00
                 Mean success rate: 71.50
                  Mean reward/step: 23.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 2.57s
                        Total time: 5000.99s
                               ETA: 5270.5s

################################################################################
                     [1m Learning iteration 1948/4000 [0m

                       Computation: 3185 steps/s (collection: 0.508s, learning 2.063s)
               Value function loss: 77005.9289
                    Surrogate loss: 0.0162
             Mean action noise std: 0.92
                       Mean reward: 8061.38
               Mean episode length: 353.36
                 Mean success rate: 69.00
                  Mean reward/step: 23.60
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15966208
                    Iteration time: 2.57s
                        Total time: 5003.56s
                               ETA: 5268.0s

################################################################################
                     [1m Learning iteration 1949/4000 [0m

                       Computation: 3226 steps/s (collection: 0.470s, learning 2.069s)
               Value function loss: 93732.6109
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 7991.59
               Mean episode length: 344.52
                 Mean success rate: 68.50
                  Mean reward/step: 24.00
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 2.54s
                        Total time: 5006.10s
                               ETA: 5265.4s

################################################################################
                     [1m Learning iteration 1950/4000 [0m

                       Computation: 3132 steps/s (collection: 0.540s, learning 2.075s)
               Value function loss: 122342.3723
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 7803.71
               Mean episode length: 340.04
                 Mean success rate: 68.00
                  Mean reward/step: 23.39
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 15982592
                    Iteration time: 2.61s
                        Total time: 5008.71s
                               ETA: 5262.9s

################################################################################
                     [1m Learning iteration 1951/4000 [0m

                       Computation: 3163 steps/s (collection: 0.486s, learning 2.103s)
               Value function loss: 98982.4317
                    Surrogate loss: 0.0131
             Mean action noise std: 0.92
                       Mean reward: 7735.08
               Mean episode length: 337.38
                 Mean success rate: 68.50
                  Mean reward/step: 22.00
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 2.59s
                        Total time: 5011.30s
                               ETA: 5260.3s

################################################################################
                     [1m Learning iteration 1952/4000 [0m

                       Computation: 3189 steps/s (collection: 0.470s, learning 2.099s)
               Value function loss: 91642.9078
                    Surrogate loss: 0.0134
             Mean action noise std: 0.92
                       Mean reward: 8055.68
               Mean episode length: 348.46
                 Mean success rate: 71.00
                  Mean reward/step: 21.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15998976
                    Iteration time: 2.57s
                        Total time: 5013.87s
                               ETA: 5257.8s

################################################################################
                     [1m Learning iteration 1953/4000 [0m

                       Computation: 3139 steps/s (collection: 0.515s, learning 2.094s)
               Value function loss: 98193.8065
                    Surrogate loss: 0.0172
             Mean action noise std: 0.92
                       Mean reward: 8246.68
               Mean episode length: 352.50
                 Mean success rate: 72.50
                  Mean reward/step: 21.80
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 2.61s
                        Total time: 5016.48s
                               ETA: 5255.2s

################################################################################
                     [1m Learning iteration 1954/4000 [0m

                       Computation: 3183 steps/s (collection: 0.495s, learning 2.078s)
               Value function loss: 103518.8457
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 7944.26
               Mean episode length: 343.11
                 Mean success rate: 70.50
                  Mean reward/step: 22.02
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 16015360
                    Iteration time: 2.57s
                        Total time: 5019.05s
                               ETA: 5252.7s

################################################################################
                     [1m Learning iteration 1955/4000 [0m

                       Computation: 3230 steps/s (collection: 0.493s, learning 2.042s)
               Value function loss: 82043.7143
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 7556.91
               Mean episode length: 332.48
                 Mean success rate: 67.50
                  Mean reward/step: 21.11
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 2.54s
                        Total time: 5021.59s
                               ETA: 5250.1s

################################################################################
                     [1m Learning iteration 1956/4000 [0m

                       Computation: 3166 steps/s (collection: 0.505s, learning 2.082s)
               Value function loss: 81433.6036
                    Surrogate loss: 0.0184
             Mean action noise std: 0.92
                       Mean reward: 7797.29
               Mean episode length: 339.21
                 Mean success rate: 69.00
                  Mean reward/step: 21.37
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16031744
                    Iteration time: 2.59s
                        Total time: 5024.18s
                               ETA: 5247.5s

################################################################################
                     [1m Learning iteration 1957/4000 [0m

                       Computation: 3204 steps/s (collection: 0.492s, learning 2.064s)
               Value function loss: 78853.0639
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 7975.55
               Mean episode length: 346.85
                 Mean success rate: 69.50
                  Mean reward/step: 22.02
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 2.56s
                        Total time: 5026.73s
                               ETA: 5245.0s

################################################################################
                     [1m Learning iteration 1958/4000 [0m

                       Computation: 3109 steps/s (collection: 0.531s, learning 2.103s)
               Value function loss: 81580.0813
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 7858.36
               Mean episode length: 342.82
                 Mean success rate: 69.00
                  Mean reward/step: 23.00
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16048128
                    Iteration time: 2.63s
                        Total time: 5029.37s
                               ETA: 5242.5s

################################################################################
                     [1m Learning iteration 1959/4000 [0m

                       Computation: 3133 steps/s (collection: 0.546s, learning 2.068s)
               Value function loss: 113187.7390
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 7700.65
               Mean episode length: 339.21
                 Mean success rate: 67.50
                  Mean reward/step: 23.53
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 2.61s
                        Total time: 5031.98s
                               ETA: 5239.9s

################################################################################
                     [1m Learning iteration 1960/4000 [0m

                       Computation: 3252 steps/s (collection: 0.449s, learning 2.070s)
               Value function loss: 49909.6793
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 7620.27
               Mean episode length: 334.31
                 Mean success rate: 66.00
                  Mean reward/step: 23.69
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 16064512
                    Iteration time: 2.52s
                        Total time: 5034.50s
                               ETA: 5237.3s

################################################################################
                     [1m Learning iteration 1961/4000 [0m

                       Computation: 3303 steps/s (collection: 0.428s, learning 2.052s)
               Value function loss: 92640.3710
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 7725.86
               Mean episode length: 338.55
                 Mean success rate: 67.00
                  Mean reward/step: 23.96
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 2.48s
                        Total time: 5036.98s
                               ETA: 5234.7s

################################################################################
                     [1m Learning iteration 1962/4000 [0m

                       Computation: 3153 steps/s (collection: 0.489s, learning 2.109s)
               Value function loss: 57637.7973
                    Surrogate loss: 0.0168
             Mean action noise std: 0.92
                       Mean reward: 7591.08
               Mean episode length: 334.33
                 Mean success rate: 66.00
                  Mean reward/step: 24.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16080896
                    Iteration time: 2.60s
                        Total time: 5039.58s
                               ETA: 5232.1s

################################################################################
                     [1m Learning iteration 1963/4000 [0m

                       Computation: 3158 steps/s (collection: 0.482s, learning 2.112s)
               Value function loss: 79391.8831
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 7696.73
               Mean episode length: 338.94
                 Mean success rate: 66.50
                  Mean reward/step: 24.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 2.59s
                        Total time: 5042.17s
                               ETA: 5229.6s

################################################################################
                     [1m Learning iteration 1964/4000 [0m

                       Computation: 3231 steps/s (collection: 0.457s, learning 2.078s)
               Value function loss: 63463.8571
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 7660.24
               Mean episode length: 336.33
                 Mean success rate: 66.50
                  Mean reward/step: 24.60
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16097280
                    Iteration time: 2.54s
                        Total time: 5044.71s
                               ETA: 5227.0s

################################################################################
                     [1m Learning iteration 1965/4000 [0m

                       Computation: 3161 steps/s (collection: 0.525s, learning 2.066s)
               Value function loss: 96631.9508
                    Surrogate loss: 0.0114
             Mean action noise std: 0.92
                       Mean reward: 7971.60
               Mean episode length: 346.75
                 Mean success rate: 69.50
                  Mean reward/step: 24.26
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 2.59s
                        Total time: 5047.30s
                               ETA: 5224.4s

################################################################################
                     [1m Learning iteration 1966/4000 [0m

                       Computation: 3171 steps/s (collection: 0.499s, learning 2.085s)
               Value function loss: 121564.9362
                    Surrogate loss: 0.0120
             Mean action noise std: 0.92
                       Mean reward: 8181.22
               Mean episode length: 356.89
                 Mean success rate: 71.50
                  Mean reward/step: 22.88
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 16113664
                    Iteration time: 2.58s
                        Total time: 5049.88s
                               ETA: 5221.9s

################################################################################
                     [1m Learning iteration 1967/4000 [0m

                       Computation: 3181 steps/s (collection: 0.495s, learning 2.080s)
               Value function loss: 102137.8398
                    Surrogate loss: 0.0166
             Mean action noise std: 0.92
                       Mean reward: 8328.56
               Mean episode length: 361.27
                 Mean success rate: 72.00
                  Mean reward/step: 22.05
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 2.57s
                        Total time: 5052.46s
                               ETA: 5219.3s

################################################################################
                     [1m Learning iteration 1968/4000 [0m

                       Computation: 3289 steps/s (collection: 0.443s, learning 2.047s)
               Value function loss: 85865.5309
                    Surrogate loss: 0.0183
             Mean action noise std: 0.92
                       Mean reward: 8683.05
               Mean episode length: 376.61
                 Mean success rate: 74.50
                  Mean reward/step: 22.33
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16130048
                    Iteration time: 2.49s
                        Total time: 5054.95s
                               ETA: 5216.7s

################################################################################
                     [1m Learning iteration 1969/4000 [0m

                       Computation: 3248 steps/s (collection: 0.449s, learning 2.073s)
               Value function loss: 95824.3266
                    Surrogate loss: 0.0186
             Mean action noise std: 0.92
                       Mean reward: 8463.24
               Mean episode length: 369.56
                 Mean success rate: 72.50
                  Mean reward/step: 22.86
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 2.52s
                        Total time: 5057.47s
                               ETA: 5214.1s

################################################################################
                     [1m Learning iteration 1970/4000 [0m

                       Computation: 3212 steps/s (collection: 0.482s, learning 2.068s)
               Value function loss: 133816.6063
                    Surrogate loss: 0.0156
             Mean action noise std: 0.92
                       Mean reward: 8639.61
               Mean episode length: 376.76
                 Mean success rate: 73.50
                  Mean reward/step: 22.83
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16146432
                    Iteration time: 2.55s
                        Total time: 5060.02s
                               ETA: 5211.5s

################################################################################
                     [1m Learning iteration 1971/4000 [0m

                       Computation: 3220 steps/s (collection: 0.482s, learning 2.061s)
               Value function loss: 70161.5200
                    Surrogate loss: 0.0189
             Mean action noise std: 0.92
                       Mean reward: 8787.43
               Mean episode length: 379.92
                 Mean success rate: 74.50
                  Mean reward/step: 23.01
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 2.54s
                        Total time: 5062.56s
                               ETA: 5208.9s

################################################################################
                     [1m Learning iteration 1972/4000 [0m

                       Computation: 3314 steps/s (collection: 0.442s, learning 2.030s)
               Value function loss: 97524.8077
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 8883.01
               Mean episode length: 382.08
                 Mean success rate: 74.00
                  Mean reward/step: 23.91
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16162816
                    Iteration time: 2.47s
                        Total time: 5065.03s
                               ETA: 5206.2s

################################################################################
                     [1m Learning iteration 1973/4000 [0m

                       Computation: 3207 steps/s (collection: 0.498s, learning 2.057s)
               Value function loss: 57548.8435
                    Surrogate loss: 0.0131
             Mean action noise std: 0.92
                       Mean reward: 8870.44
               Mean episode length: 382.74
                 Mean success rate: 74.50
                  Mean reward/step: 24.14
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 2.55s
                        Total time: 5067.59s
                               ETA: 5203.6s

################################################################################
                     [1m Learning iteration 1974/4000 [0m

                       Computation: 3178 steps/s (collection: 0.520s, learning 2.057s)
               Value function loss: 122059.9646
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 8920.79
               Mean episode length: 386.21
                 Mean success rate: 74.50
                  Mean reward/step: 23.83
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 16179200
                    Iteration time: 2.58s
                        Total time: 5070.16s
                               ETA: 5201.1s

################################################################################
                     [1m Learning iteration 1975/4000 [0m

                       Computation: 3272 steps/s (collection: 0.475s, learning 2.029s)
               Value function loss: 80519.8646
                    Surrogate loss: 0.0115
             Mean action noise std: 0.92
                       Mean reward: 8854.80
               Mean episode length: 379.23
                 Mean success rate: 73.50
                  Mean reward/step: 22.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 2.50s
                        Total time: 5072.67s
                               ETA: 5198.5s

################################################################################
                     [1m Learning iteration 1976/4000 [0m

                       Computation: 3217 steps/s (collection: 0.467s, learning 2.079s)
               Value function loss: 65218.5716
                    Surrogate loss: 0.0134
             Mean action noise std: 0.92
                       Mean reward: 8951.47
               Mean episode length: 383.58
                 Mean success rate: 74.50
                  Mean reward/step: 23.50
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 16195584
                    Iteration time: 2.55s
                        Total time: 5075.21s
                               ETA: 5195.9s

################################################################################
                     [1m Learning iteration 1977/4000 [0m

                       Computation: 3227 steps/s (collection: 0.501s, learning 2.037s)
               Value function loss: 88324.8817
                    Surrogate loss: 0.0116
             Mean action noise std: 0.92
                       Mean reward: 8835.00
               Mean episode length: 379.10
                 Mean success rate: 73.50
                  Mean reward/step: 24.04
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 2.54s
                        Total time: 5077.75s
                               ETA: 5193.3s

################################################################################
                     [1m Learning iteration 1978/4000 [0m

                       Computation: 3223 steps/s (collection: 0.470s, learning 2.071s)
               Value function loss: 73518.5479
                    Surrogate loss: 0.0137
             Mean action noise std: 0.92
                       Mean reward: 8777.37
               Mean episode length: 377.17
                 Mean success rate: 73.50
                  Mean reward/step: 24.00
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16211968
                    Iteration time: 2.54s
                        Total time: 5080.29s
                               ETA: 5190.7s

################################################################################
                     [1m Learning iteration 1979/4000 [0m

                       Computation: 3160 steps/s (collection: 0.511s, learning 2.080s)
               Value function loss: 57035.5962
                    Surrogate loss: 0.0114
             Mean action noise std: 0.92
                       Mean reward: 8426.10
               Mean episode length: 364.51
                 Mean success rate: 70.50
                  Mean reward/step: 23.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 2.59s
                        Total time: 5082.89s
                               ETA: 5188.1s

################################################################################
                     [1m Learning iteration 1980/4000 [0m

                       Computation: 3222 steps/s (collection: 0.472s, learning 2.070s)
               Value function loss: 82727.9708
                    Surrogate loss: 0.0121
             Mean action noise std: 0.92
                       Mean reward: 7979.36
               Mean episode length: 349.26
                 Mean success rate: 67.50
                  Mean reward/step: 23.75
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16228352
                    Iteration time: 2.54s
                        Total time: 5085.43s
                               ETA: 5185.5s

################################################################################
                     [1m Learning iteration 1981/4000 [0m

                       Computation: 3188 steps/s (collection: 0.503s, learning 2.067s)
               Value function loss: 148923.0936
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 8275.02
               Mean episode length: 359.97
                 Mean success rate: 69.50
                  Mean reward/step: 24.16
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 2.57s
                        Total time: 5088.00s
                               ETA: 5183.0s

################################################################################
                     [1m Learning iteration 1982/4000 [0m

                       Computation: 3203 steps/s (collection: 0.489s, learning 2.068s)
               Value function loss: 117790.0566
                    Surrogate loss: 0.0134
             Mean action noise std: 0.92
                       Mean reward: 8610.56
               Mean episode length: 368.30
                 Mean success rate: 71.50
                  Mean reward/step: 22.68
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16244736
                    Iteration time: 2.56s
                        Total time: 5090.55s
                               ETA: 5180.4s

################################################################################
                     [1m Learning iteration 1983/4000 [0m

                       Computation: 3213 steps/s (collection: 0.505s, learning 2.044s)
               Value function loss: 83251.0271
                    Surrogate loss: 0.0183
             Mean action noise std: 0.92
                       Mean reward: 8906.06
               Mean episode length: 376.09
                 Mean success rate: 73.50
                  Mean reward/step: 21.34
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 2.55s
                        Total time: 5093.10s
                               ETA: 5177.8s

################################################################################
                     [1m Learning iteration 1984/4000 [0m

                       Computation: 3274 steps/s (collection: 0.474s, learning 2.027s)
               Value function loss: 116024.9913
                    Surrogate loss: 0.0137
             Mean action noise std: 0.92
                       Mean reward: 9059.96
               Mean episode length: 379.67
                 Mean success rate: 73.50
                  Mean reward/step: 21.83
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16261120
                    Iteration time: 2.50s
                        Total time: 5095.60s
                               ETA: 5175.2s

################################################################################
                     [1m Learning iteration 1985/4000 [0m

                       Computation: 3227 steps/s (collection: 0.484s, learning 2.054s)
               Value function loss: 92904.5771
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 8813.20
               Mean episode length: 371.50
                 Mean success rate: 73.00
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 2.54s
                        Total time: 5098.14s
                               ETA: 5172.6s

################################################################################
                     [1m Learning iteration 1986/4000 [0m

                       Computation: 3207 steps/s (collection: 0.505s, learning 2.049s)
               Value function loss: 116031.4299
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 9060.66
               Mean episode length: 378.40
                 Mean success rate: 73.00
                  Mean reward/step: 20.85
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 16277504
                    Iteration time: 2.55s
                        Total time: 5100.70s
                               ETA: 5170.0s

################################################################################
                     [1m Learning iteration 1987/4000 [0m

                       Computation: 3248 steps/s (collection: 0.466s, learning 2.056s)
               Value function loss: 71072.5785
                    Surrogate loss: 0.0175
             Mean action noise std: 0.92
                       Mean reward: 9211.26
               Mean episode length: 383.38
                 Mean success rate: 75.00
                  Mean reward/step: 20.40
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 2.52s
                        Total time: 5103.22s
                               ETA: 5167.4s

################################################################################
                     [1m Learning iteration 1988/4000 [0m

                       Computation: 3303 steps/s (collection: 0.457s, learning 2.022s)
               Value function loss: 78251.7213
                    Surrogate loss: 0.0170
             Mean action noise std: 0.92
                       Mean reward: 8581.74
               Mean episode length: 365.37
                 Mean success rate: 70.00
                  Mean reward/step: 20.96
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 16293888
                    Iteration time: 2.48s
                        Total time: 5105.70s
                               ETA: 5164.7s

################################################################################
                     [1m Learning iteration 1989/4000 [0m

                       Computation: 3198 steps/s (collection: 0.463s, learning 2.098s)
               Value function loss: 96182.9678
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 7523.97
               Mean episode length: 334.14
                 Mean success rate: 62.50
                  Mean reward/step: 21.95
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 2.56s
                        Total time: 5108.26s
                               ETA: 5162.2s

################################################################################
                     [1m Learning iteration 1990/4000 [0m

                       Computation: 3242 steps/s (collection: 0.474s, learning 2.052s)
               Value function loss: 124491.8707
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 7447.38
               Mean episode length: 332.08
                 Mean success rate: 61.50
                  Mean reward/step: 21.71
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16310272
                    Iteration time: 2.53s
                        Total time: 5110.79s
                               ETA: 5159.6s

################################################################################
                     [1m Learning iteration 1991/4000 [0m

                       Computation: 3295 steps/s (collection: 0.445s, learning 2.041s)
               Value function loss: 28675.3114
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 7046.31
               Mean episode length: 318.34
                 Mean success rate: 59.50
                  Mean reward/step: 22.02
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 2.49s
                        Total time: 5113.27s
                               ETA: 5156.9s

################################################################################
                     [1m Learning iteration 1992/4000 [0m

                       Computation: 3329 steps/s (collection: 0.415s, learning 2.045s)
               Value function loss: 73744.1399
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 7118.90
               Mean episode length: 321.18
                 Mean success rate: 60.00
                  Mean reward/step: 24.06
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16326656
                    Iteration time: 2.46s
                        Total time: 5115.73s
                               ETA: 5154.2s

################################################################################
                     [1m Learning iteration 1993/4000 [0m

                       Computation: 3278 steps/s (collection: 0.440s, learning 2.058s)
               Value function loss: 68321.0399
                    Surrogate loss: 0.0118
             Mean action noise std: 0.92
                       Mean reward: 6788.97
               Mean episode length: 314.46
                 Mean success rate: 57.50
                  Mean reward/step: 24.47
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 2.50s
                        Total time: 5118.23s
                               ETA: 5151.6s

################################################################################
                     [1m Learning iteration 1994/4000 [0m

                       Computation: 3200 steps/s (collection: 0.478s, learning 2.081s)
               Value function loss: 75004.1189
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 7011.19
               Mean episode length: 318.26
                 Mean success rate: 59.50
                  Mean reward/step: 24.28
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16343040
                    Iteration time: 2.56s
                        Total time: 5120.79s
                               ETA: 5149.0s

################################################################################
                     [1m Learning iteration 1995/4000 [0m

                       Computation: 3251 steps/s (collection: 0.476s, learning 2.044s)
               Value function loss: 67646.2552
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 6889.35
               Mean episode length: 315.25
                 Mean success rate: 58.50
                  Mean reward/step: 23.83
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 2.52s
                        Total time: 5123.31s
                               ETA: 5146.4s

################################################################################
                     [1m Learning iteration 1996/4000 [0m

                       Computation: 3228 steps/s (collection: 0.485s, learning 2.052s)
               Value function loss: 95258.5893
                    Surrogate loss: 0.0156
             Mean action noise std: 0.92
                       Mean reward: 6885.56
               Mean episode length: 320.88
                 Mean success rate: 58.50
                  Mean reward/step: 23.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16359424
                    Iteration time: 2.54s
                        Total time: 5125.85s
                               ETA: 5143.8s

################################################################################
                     [1m Learning iteration 1997/4000 [0m

                       Computation: 3216 steps/s (collection: 0.494s, learning 2.053s)
               Value function loss: 140552.6346
                    Surrogate loss: 0.0170
             Mean action noise std: 0.92
                       Mean reward: 7728.94
               Mean episode length: 344.18
                 Mean success rate: 64.00
                  Mean reward/step: 22.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 2.55s
                        Total time: 5128.39s
                               ETA: 5141.2s

################################################################################
                     [1m Learning iteration 1998/4000 [0m

                       Computation: 3193 steps/s (collection: 0.476s, learning 2.090s)
               Value function loss: 93317.0140
                    Surrogate loss: 0.0108
             Mean action noise std: 0.92
                       Mean reward: 7955.90
               Mean episode length: 352.52
                 Mean success rate: 66.50
                  Mean reward/step: 21.23
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 16375808
                    Iteration time: 2.57s
                        Total time: 5130.96s
                               ETA: 5138.7s

################################################################################
                     [1m Learning iteration 1999/4000 [0m

                       Computation: 3215 steps/s (collection: 0.503s, learning 2.045s)
               Value function loss: 76141.2461
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 7688.46
               Mean episode length: 341.67
                 Mean success rate: 64.00
                  Mean reward/step: 21.14
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 2.55s
                        Total time: 5133.51s
                               ETA: 5136.1s

################################################################################
                     [1m Learning iteration 2000/4000 [0m

                       Computation: 3207 steps/s (collection: 0.490s, learning 2.064s)
               Value function loss: 107986.5855
                    Surrogate loss: 0.0169
             Mean action noise std: 0.92
                       Mean reward: 8020.57
               Mean episode length: 353.75
                 Mean success rate: 66.50
                  Mean reward/step: 22.06
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16392192
                    Iteration time: 2.55s
                        Total time: 5136.06s
                               ETA: 5133.5s

################################################################################
                     [1m Learning iteration 2001/4000 [0m

                       Computation: 3221 steps/s (collection: 0.472s, learning 2.071s)
               Value function loss: 100987.6504
                    Surrogate loss: 0.0156
             Mean action noise std: 0.92
                       Mean reward: 7952.24
               Mean episode length: 348.94
                 Mean success rate: 65.00
                  Mean reward/step: 22.99
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 2.54s
                        Total time: 5138.60s
                               ETA: 5130.9s

################################################################################
                     [1m Learning iteration 2002/4000 [0m

                       Computation: 3197 steps/s (collection: 0.492s, learning 2.070s)
               Value function loss: 79179.0795
                    Surrogate loss: 0.0167
             Mean action noise std: 0.92
                       Mean reward: 8025.67
               Mean episode length: 351.24
                 Mean success rate: 65.50
                  Mean reward/step: 22.16
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16408576
                    Iteration time: 2.56s
                        Total time: 5141.17s
                               ETA: 5128.3s

################################################################################
                     [1m Learning iteration 2003/4000 [0m

                       Computation: 3238 steps/s (collection: 0.472s, learning 2.058s)
               Value function loss: 81477.4633
                    Surrogate loss: 0.0128
             Mean action noise std: 0.92
                       Mean reward: 8165.27
               Mean episode length: 355.20
                 Mean success rate: 66.50
                  Mean reward/step: 22.95
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 2.53s
                        Total time: 5143.69s
                               ETA: 5125.7s

################################################################################
                     [1m Learning iteration 2004/4000 [0m

                       Computation: 3236 steps/s (collection: 0.476s, learning 2.055s)
               Value function loss: 100336.1072
                    Surrogate loss: 0.0123
             Mean action noise std: 0.92
                       Mean reward: 7937.08
               Mean episode length: 348.53
                 Mean success rate: 64.50
                  Mean reward/step: 22.93
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16424960
                    Iteration time: 2.53s
                        Total time: 5146.23s
                               ETA: 5123.1s

################################################################################
                     [1m Learning iteration 2005/4000 [0m

                       Computation: 3187 steps/s (collection: 0.505s, learning 2.065s)
               Value function loss: 94793.6303
                    Surrogate loss: 0.0152
             Mean action noise std: 0.92
                       Mean reward: 8049.32
               Mean episode length: 350.21
                 Mean success rate: 65.50
                  Mean reward/step: 23.07
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 2.57s
                        Total time: 5148.80s
                               ETA: 5120.6s

################################################################################
                     [1m Learning iteration 2006/4000 [0m

                       Computation: 3205 steps/s (collection: 0.466s, learning 2.089s)
               Value function loss: 104819.8487
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 8370.96
               Mean episode length: 361.34
                 Mean success rate: 67.50
                  Mean reward/step: 23.10
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16441344
                    Iteration time: 2.56s
                        Total time: 5151.35s
                               ETA: 5118.0s

################################################################################
                     [1m Learning iteration 2007/4000 [0m

                       Computation: 3244 steps/s (collection: 0.440s, learning 2.085s)
               Value function loss: 50368.8244
                    Surrogate loss: 0.0131
             Mean action noise std: 0.92
                       Mean reward: 8510.54
               Mean episode length: 365.63
                 Mean success rate: 68.50
                  Mean reward/step: 24.26
       Mean episode length/episode: 31.51
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 2.52s
                        Total time: 5153.88s
                               ETA: 5115.4s

################################################################################
                     [1m Learning iteration 2008/4000 [0m

                       Computation: 3230 steps/s (collection: 0.460s, learning 2.076s)
               Value function loss: 58499.9662
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 8508.49
               Mean episode length: 368.00
                 Mean success rate: 69.00
                  Mean reward/step: 24.97
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16457728
                    Iteration time: 2.54s
                        Total time: 5156.41s
                               ETA: 5112.8s

################################################################################
                     [1m Learning iteration 2009/4000 [0m

                       Computation: 3170 steps/s (collection: 0.479s, learning 2.105s)
               Value function loss: 80492.9187
                    Surrogate loss: 0.0110
             Mean action noise std: 0.92
                       Mean reward: 8511.76
               Mean episode length: 367.94
                 Mean success rate: 69.00
                  Mean reward/step: 25.14
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 2.58s
                        Total time: 5159.00s
                               ETA: 5110.2s

################################################################################
                     [1m Learning iteration 2010/4000 [0m

                       Computation: 3218 steps/s (collection: 0.442s, learning 2.103s)
               Value function loss: 61129.6731
                    Surrogate loss: 0.0106
             Mean action noise std: 0.92
                       Mean reward: 8410.90
               Mean episode length: 368.05
                 Mean success rate: 70.00
                  Mean reward/step: 24.88
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16474112
                    Iteration time: 2.54s
                        Total time: 5161.54s
                               ETA: 5107.6s

################################################################################
                     [1m Learning iteration 2011/4000 [0m

                       Computation: 3199 steps/s (collection: 0.476s, learning 2.085s)
               Value function loss: 62318.3261
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 8376.75
               Mean episode length: 366.82
                 Mean success rate: 69.00
                  Mean reward/step: 25.00
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 2.56s
                        Total time: 5164.10s
                               ETA: 5105.1s

################################################################################
                     [1m Learning iteration 2012/4000 [0m

                       Computation: 3165 steps/s (collection: 0.477s, learning 2.111s)
               Value function loss: 107495.2596
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 8469.38
               Mean episode length: 368.30
                 Mean success rate: 69.00
                  Mean reward/step: 25.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16490496
                    Iteration time: 2.59s
                        Total time: 5166.69s
                               ETA: 5102.5s

################################################################################
                     [1m Learning iteration 2013/4000 [0m

                       Computation: 3186 steps/s (collection: 0.488s, learning 2.082s)
               Value function loss: 135223.2948
                    Surrogate loss: 0.0131
             Mean action noise std: 0.92
                       Mean reward: 9107.73
               Mean episode length: 388.22
                 Mean success rate: 73.50
                  Mean reward/step: 23.92
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 2.57s
                        Total time: 5169.26s
                               ETA: 5100.0s

################################################################################
                     [1m Learning iteration 2014/4000 [0m

                       Computation: 3222 steps/s (collection: 0.456s, learning 2.085s)
               Value function loss: 94637.3134
                    Surrogate loss: 0.0126
             Mean action noise std: 0.92
                       Mean reward: 9160.74
               Mean episode length: 388.32
                 Mean success rate: 74.50
                  Mean reward/step: 22.64
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16506880
                    Iteration time: 2.54s
                        Total time: 5171.80s
                               ETA: 5097.4s

################################################################################
                     [1m Learning iteration 2015/4000 [0m

                       Computation: 3128 steps/s (collection: 0.484s, learning 2.134s)
               Value function loss: 110762.3865
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 9118.69
               Mean episode length: 389.94
                 Mean success rate: 75.00
                  Mean reward/step: 22.76
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 2.62s
                        Total time: 5174.42s
                               ETA: 5094.9s

################################################################################
                     [1m Learning iteration 2016/4000 [0m

                       Computation: 3284 steps/s (collection: 0.469s, learning 2.025s)
               Value function loss: 81024.6541
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 8988.17
               Mean episode length: 386.21
                 Mean success rate: 73.50
                  Mean reward/step: 22.67
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16523264
                    Iteration time: 2.49s
                        Total time: 5176.92s
                               ETA: 5092.2s

################################################################################
                     [1m Learning iteration 2017/4000 [0m

                       Computation: 3121 steps/s (collection: 0.501s, learning 2.124s)
               Value function loss: 142848.3234
                    Surrogate loss: 0.0127
             Mean action noise std: 0.92
                       Mean reward: 8985.38
               Mean episode length: 386.89
                 Mean success rate: 73.50
                  Mean reward/step: 22.12
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 2.62s
                        Total time: 5179.54s
                               ETA: 5089.7s

################################################################################
                     [1m Learning iteration 2018/4000 [0m

                       Computation: 3123 steps/s (collection: 0.490s, learning 2.132s)
               Value function loss: 83237.4692
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 8918.21
               Mean episode length: 382.05
                 Mean success rate: 72.50
                  Mean reward/step: 21.09
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16539648
                    Iteration time: 2.62s
                        Total time: 5182.16s
                               ETA: 5087.2s

################################################################################
                     [1m Learning iteration 2019/4000 [0m

                       Computation: 3078 steps/s (collection: 0.511s, learning 2.149s)
               Value function loss: 124451.0037
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 9037.23
               Mean episode length: 383.27
                 Mean success rate: 73.00
                  Mean reward/step: 21.64
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 2.66s
                        Total time: 5184.82s
                               ETA: 5084.7s

################################################################################
                     [1m Learning iteration 2020/4000 [0m

                       Computation: 3116 steps/s (collection: 0.503s, learning 2.125s)
               Value function loss: 116325.7246
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 8925.37
               Mean episode length: 379.65
                 Mean success rate: 73.50
                  Mean reward/step: 21.51
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16556032
                    Iteration time: 2.63s
                        Total time: 5187.45s
                               ETA: 5082.2s

################################################################################
                     [1m Learning iteration 2021/4000 [0m

                       Computation: 3179 steps/s (collection: 0.449s, learning 2.127s)
               Value function loss: 94340.9758
                    Surrogate loss: 0.0179
             Mean action noise std: 0.92
                       Mean reward: 8743.14
               Mean episode length: 373.08
                 Mean success rate: 72.00
                  Mean reward/step: 21.58
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 2.58s
                        Total time: 5190.03s
                               ETA: 5079.7s

################################################################################
                     [1m Learning iteration 2022/4000 [0m

                       Computation: 3095 steps/s (collection: 0.484s, learning 2.162s)
               Value function loss: 74007.7015
                    Surrogate loss: 0.0172
             Mean action noise std: 0.92
                       Mean reward: 8640.30
               Mean episode length: 368.53
                 Mean success rate: 70.00
                  Mean reward/step: 21.31
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16572416
                    Iteration time: 2.65s
                        Total time: 5192.67s
                               ETA: 5077.2s

################################################################################
                     [1m Learning iteration 2023/4000 [0m

                       Computation: 3162 steps/s (collection: 0.465s, learning 2.126s)
               Value function loss: 61475.1134
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 8470.36
               Mean episode length: 362.31
                 Mean success rate: 69.00
                  Mean reward/step: 22.44
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 2.59s
                        Total time: 5195.27s
                               ETA: 5074.6s

################################################################################
                     [1m Learning iteration 2024/4000 [0m

                       Computation: 3173 steps/s (collection: 0.462s, learning 2.119s)
               Value function loss: 63135.5921
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 8326.69
               Mean episode length: 357.51
                 Mean success rate: 68.00
                  Mean reward/step: 23.93
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16588800
                    Iteration time: 2.58s
                        Total time: 5197.85s
                               ETA: 5072.1s

################################################################################
                     [1m Learning iteration 2025/4000 [0m

                       Computation: 3163 steps/s (collection: 0.435s, learning 2.155s)
               Value function loss: 73384.5971
                    Surrogate loss: 0.0171
             Mean action noise std: 0.92
                       Mean reward: 8381.12
               Mean episode length: 350.99
                 Mean success rate: 67.00
                  Mean reward/step: 24.59
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 2.59s
                        Total time: 5200.44s
                               ETA: 5069.5s

################################################################################
                     [1m Learning iteration 2026/4000 [0m

                       Computation: 3194 steps/s (collection: 0.477s, learning 2.088s)
               Value function loss: 43035.4394
                    Surrogate loss: 0.0117
             Mean action noise std: 0.92
                       Mean reward: 8290.79
               Mean episode length: 350.95
                 Mean success rate: 66.50
                  Mean reward/step: 24.66
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 16605184
                    Iteration time: 2.56s
                        Total time: 5203.00s
                               ETA: 5067.0s

################################################################################
                     [1m Learning iteration 2027/4000 [0m

                       Computation: 3092 steps/s (collection: 0.524s, learning 2.125s)
               Value function loss: 74505.9611
                    Surrogate loss: 0.0176
             Mean action noise std: 0.92
                       Mean reward: 7994.98
               Mean episode length: 345.64
                 Mean success rate: 65.00
                  Mean reward/step: 24.16
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 2.65s
                        Total time: 5205.65s
                               ETA: 5064.5s

################################################################################
                     [1m Learning iteration 2028/4000 [0m

                       Computation: 3155 steps/s (collection: 0.520s, learning 2.077s)
               Value function loss: 108081.1911
                    Surrogate loss: 0.0221
             Mean action noise std: 0.92
                       Mean reward: 8032.44
               Mean episode length: 350.81
                 Mean success rate: 66.00
                  Mean reward/step: 23.15
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16621568
                    Iteration time: 2.60s
                        Total time: 5208.25s
                               ETA: 5061.9s

################################################################################
                     [1m Learning iteration 2029/4000 [0m

                       Computation: 3265 steps/s (collection: 0.465s, learning 2.044s)
               Value function loss: 105651.8838
                    Surrogate loss: 0.0116
             Mean action noise std: 0.92
                       Mean reward: 8193.46
               Mean episode length: 358.60
                 Mean success rate: 67.00
                  Mean reward/step: 22.03
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 2.51s
                        Total time: 5210.75s
                               ETA: 5059.3s

################################################################################
                     [1m Learning iteration 2030/4000 [0m

                       Computation: 3254 steps/s (collection: 0.456s, learning 2.061s)
               Value function loss: 96660.0992
                    Surrogate loss: 0.0183
             Mean action noise std: 0.92
                       Mean reward: 8211.24
               Mean episode length: 358.15
                 Mean success rate: 67.00
                  Mean reward/step: 22.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16637952
                    Iteration time: 2.52s
                        Total time: 5213.27s
                               ETA: 5056.7s

################################################################################
                     [1m Learning iteration 2031/4000 [0m

                       Computation: 3219 steps/s (collection: 0.472s, learning 2.073s)
               Value function loss: 102074.8405
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 8281.78
               Mean episode length: 360.13
                 Mean success rate: 68.00
                  Mean reward/step: 22.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 2.54s
                        Total time: 5215.82s
                               ETA: 5054.1s

################################################################################
                     [1m Learning iteration 2032/4000 [0m

                       Computation: 3260 steps/s (collection: 0.467s, learning 2.045s)
               Value function loss: 102354.8668
                    Surrogate loss: 0.0171
             Mean action noise std: 0.92
                       Mean reward: 8390.49
               Mean episode length: 367.94
                 Mean success rate: 70.00
                  Mean reward/step: 22.83
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16654336
                    Iteration time: 2.51s
                        Total time: 5218.33s
                               ETA: 5051.5s

################################################################################
                     [1m Learning iteration 2033/4000 [0m

                       Computation: 3268 steps/s (collection: 0.476s, learning 2.030s)
               Value function loss: 109283.7177
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 8873.38
               Mean episode length: 385.26
                 Mean success rate: 74.50
                  Mean reward/step: 21.89
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 2.51s
                        Total time: 5220.84s
                               ETA: 5048.9s

################################################################################
                     [1m Learning iteration 2034/4000 [0m

                       Computation: 3193 steps/s (collection: 0.494s, learning 2.071s)
               Value function loss: 63846.6948
                    Surrogate loss: 0.0190
             Mean action noise std: 0.92
                       Mean reward: 8511.21
               Mean episode length: 375.65
                 Mean success rate: 72.00
                  Mean reward/step: 22.55
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16670720
                    Iteration time: 2.57s
                        Total time: 5223.40s
                               ETA: 5046.3s

################################################################################
                     [1m Learning iteration 2035/4000 [0m

                       Computation: 3229 steps/s (collection: 0.478s, learning 2.058s)
               Value function loss: 126655.4550
                    Surrogate loss: 0.0118
             Mean action noise std: 0.92
                       Mean reward: 8859.70
               Mean episode length: 385.79
                 Mean success rate: 74.50
                  Mean reward/step: 22.58
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 2.54s
                        Total time: 5225.94s
                               ETA: 5043.7s

################################################################################
                     [1m Learning iteration 2036/4000 [0m

                       Computation: 3257 steps/s (collection: 0.474s, learning 2.041s)
               Value function loss: 113880.3551
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 8800.52
               Mean episode length: 384.25
                 Mean success rate: 73.50
                  Mean reward/step: 21.54
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16687104
                    Iteration time: 2.52s
                        Total time: 5228.45s
                               ETA: 5041.1s

################################################################################
                     [1m Learning iteration 2037/4000 [0m

                       Computation: 3232 steps/s (collection: 0.437s, learning 2.097s)
               Value function loss: 105684.1667
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 8780.26
               Mean episode length: 383.19
                 Mean success rate: 74.00
                  Mean reward/step: 21.35
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 2.53s
                        Total time: 5230.99s
                               ETA: 5038.5s

################################################################################
                     [1m Learning iteration 2038/4000 [0m

                       Computation: 3255 steps/s (collection: 0.442s, learning 2.074s)
               Value function loss: 48534.4335
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 8215.83
               Mean episode length: 364.44
                 Mean success rate: 69.50
                  Mean reward/step: 21.66
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16703488
                    Iteration time: 2.52s
                        Total time: 5233.50s
                               ETA: 5035.9s

################################################################################
                     [1m Learning iteration 2039/4000 [0m

                       Computation: 3216 steps/s (collection: 0.445s, learning 2.102s)
               Value function loss: 70122.1203
                    Surrogate loss: 0.0121
             Mean action noise std: 0.92
                       Mean reward: 7936.05
               Mean episode length: 355.15
                 Mean success rate: 67.00
                  Mean reward/step: 22.54
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 2.55s
                        Total time: 5236.05s
                               ETA: 5033.3s

################################################################################
                     [1m Learning iteration 2040/4000 [0m

                       Computation: 3194 steps/s (collection: 0.513s, learning 2.052s)
               Value function loss: 50237.0206
                    Surrogate loss: 0.0117
             Mean action noise std: 0.92
                       Mean reward: 7779.09
               Mean episode length: 347.17
                 Mean success rate: 66.50
                  Mean reward/step: 22.72
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16719872
                    Iteration time: 2.56s
                        Total time: 5238.61s
                               ETA: 5030.7s

################################################################################
                     [1m Learning iteration 2041/4000 [0m

                       Computation: 3384 steps/s (collection: 0.439s, learning 1.981s)
               Value function loss: 63360.9328
                    Surrogate loss: 0.0124
             Mean action noise std: 0.92
                       Mean reward: 7430.79
               Mean episode length: 336.08
                 Mean success rate: 63.00
                  Mean reward/step: 23.38
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 2.42s
                        Total time: 5241.03s
                               ETA: 5028.0s

################################################################################
                     [1m Learning iteration 2042/4000 [0m

                       Computation: 3270 steps/s (collection: 0.443s, learning 2.062s)
               Value function loss: 63419.0491
                    Surrogate loss: 0.0137
             Mean action noise std: 0.92
                       Mean reward: 7568.47
               Mean episode length: 340.35
                 Mean success rate: 63.50
                  Mean reward/step: 23.90
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16736256
                    Iteration time: 2.51s
                        Total time: 5243.54s
                               ETA: 5025.4s

################################################################################
                     [1m Learning iteration 2043/4000 [0m

                       Computation: 3178 steps/s (collection: 0.490s, learning 2.087s)
               Value function loss: 123956.3604
                    Surrogate loss: 0.0152
             Mean action noise std: 0.92
                       Mean reward: 8046.55
               Mean episode length: 361.73
                 Mean success rate: 68.00
                  Mean reward/step: 24.24
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 2.58s
                        Total time: 5246.12s
                               ETA: 5022.8s

################################################################################
                     [1m Learning iteration 2044/4000 [0m

                       Computation: 3232 steps/s (collection: 0.476s, learning 2.058s)
               Value function loss: 124978.5871
                    Surrogate loss: 0.0107
             Mean action noise std: 0.92
                       Mean reward: 8057.49
               Mean episode length: 360.69
                 Mean success rate: 68.00
                  Mean reward/step: 22.99
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16752640
                    Iteration time: 2.53s
                        Total time: 5248.65s
                               ETA: 5020.2s

################################################################################
                     [1m Learning iteration 2045/4000 [0m

                       Computation: 3107 steps/s (collection: 0.513s, learning 2.123s)
               Value function loss: 84438.6245
                    Surrogate loss: 0.0120
             Mean action noise std: 0.92
                       Mean reward: 7962.24
               Mean episode length: 353.69
                 Mean success rate: 68.00
                  Mean reward/step: 22.12
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 2.64s
                        Total time: 5251.29s
                               ETA: 5017.7s

################################################################################
                     [1m Learning iteration 2046/4000 [0m

                       Computation: 3150 steps/s (collection: 0.525s, learning 2.075s)
               Value function loss: 82758.2557
                    Surrogate loss: 0.0122
             Mean action noise std: 0.92
                       Mean reward: 7323.58
               Mean episode length: 335.94
                 Mean success rate: 63.50
                  Mean reward/step: 22.02
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16769024
                    Iteration time: 2.60s
                        Total time: 5253.89s
                               ETA: 5015.2s

################################################################################
                     [1m Learning iteration 2047/4000 [0m

                       Computation: 3026 steps/s (collection: 0.544s, learning 2.163s)
               Value function loss: 81329.7707
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 7622.85
               Mean episode length: 343.37
                 Mean success rate: 65.00
                  Mean reward/step: 22.90
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 2.71s
                        Total time: 5256.59s
                               ETA: 5012.8s

################################################################################
                     [1m Learning iteration 2048/4000 [0m

                       Computation: 3246 steps/s (collection: 0.465s, learning 2.058s)
               Value function loss: 133053.1215
                    Surrogate loss: 0.0126
             Mean action noise std: 0.92
                       Mean reward: 7955.44
               Mean episode length: 352.28
                 Mean success rate: 68.00
                  Mean reward/step: 22.74
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 16785408
                    Iteration time: 2.52s
                        Total time: 5259.12s
                               ETA: 5010.1s

################################################################################
                     [1m Learning iteration 2049/4000 [0m

                       Computation: 3166 steps/s (collection: 0.503s, learning 2.084s)
               Value function loss: 101482.4723
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 8244.61
               Mean episode length: 359.00
                 Mean success rate: 70.00
                  Mean reward/step: 21.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 2.59s
                        Total time: 5261.70s
                               ETA: 5007.6s

################################################################################
                     [1m Learning iteration 2050/4000 [0m

                       Computation: 3243 steps/s (collection: 0.473s, learning 2.053s)
               Value function loss: 88336.7395
                    Surrogate loss: 0.0120
             Mean action noise std: 0.92
                       Mean reward: 8134.85
               Mean episode length: 354.45
                 Mean success rate: 68.50
                  Mean reward/step: 21.71
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16801792
                    Iteration time: 2.53s
                        Total time: 5264.23s
                               ETA: 5005.0s

################################################################################
                     [1m Learning iteration 2051/4000 [0m

                       Computation: 3061 steps/s (collection: 0.496s, learning 2.180s)
               Value function loss: 108465.9834
                    Surrogate loss: 0.0127
             Mean action noise std: 0.92
                       Mean reward: 7743.56
               Mean episode length: 338.14
                 Mean success rate: 64.50
                  Mean reward/step: 20.96
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 2.68s
                        Total time: 5266.90s
                               ETA: 5002.5s

################################################################################
                     [1m Learning iteration 2052/4000 [0m

                       Computation: 3066 steps/s (collection: 0.515s, learning 2.156s)
               Value function loss: 100345.5015
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 7530.49
               Mean episode length: 338.61
                 Mean success rate: 63.00
                  Mean reward/step: 20.94
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16818176
                    Iteration time: 2.67s
                        Total time: 5269.58s
                               ETA: 5000.1s

################################################################################
                     [1m Learning iteration 2053/4000 [0m

                       Computation: 3138 steps/s (collection: 0.524s, learning 2.086s)
               Value function loss: 106877.7340
                    Surrogate loss: 0.0121
             Mean action noise std: 0.92
                       Mean reward: 7801.19
               Mean episode length: 347.32
                 Mean success rate: 64.50
                  Mean reward/step: 20.93
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 2.61s
                        Total time: 5272.19s
                               ETA: 4997.5s

################################################################################
                     [1m Learning iteration 2054/4000 [0m

                       Computation: 3206 steps/s (collection: 0.468s, learning 2.087s)
               Value function loss: 85162.1912
                    Surrogate loss: 0.0110
             Mean action noise std: 0.92
                       Mean reward: 7415.45
               Mean episode length: 334.14
                 Mean success rate: 62.00
                  Mean reward/step: 21.27
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 16834560
                    Iteration time: 2.56s
                        Total time: 5274.74s
                               ETA: 4995.0s

################################################################################
                     [1m Learning iteration 2055/4000 [0m

                       Computation: 3195 steps/s (collection: 0.513s, learning 2.050s)
               Value function loss: 70382.9733
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 7290.66
               Mean episode length: 331.05
                 Mean success rate: 61.00
                  Mean reward/step: 22.07
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 2.56s
                        Total time: 5277.30s
                               ETA: 4992.4s

################################################################################
                     [1m Learning iteration 2056/4000 [0m

                       Computation: 3148 steps/s (collection: 0.566s, learning 2.037s)
               Value function loss: 45166.8793
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 6548.89
               Mean episode length: 308.26
                 Mean success rate: 56.50
                  Mean reward/step: 22.98
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16850944
                    Iteration time: 2.60s
                        Total time: 5279.91s
                               ETA: 4989.9s

################################################################################
                     [1m Learning iteration 2057/4000 [0m

                       Computation: 3241 steps/s (collection: 0.464s, learning 2.063s)
               Value function loss: 59084.0463
                    Surrogate loss: 0.0100
             Mean action noise std: 0.92
                       Mean reward: 6366.50
               Mean episode length: 303.65
                 Mean success rate: 55.00
                  Mean reward/step: 24.30
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 2.53s
                        Total time: 5282.43s
                               ETA: 4987.3s

################################################################################
                     [1m Learning iteration 2058/4000 [0m

                       Computation: 3248 steps/s (collection: 0.470s, learning 2.051s)
               Value function loss: 53568.6579
                    Surrogate loss: 0.0112
             Mean action noise std: 0.92
                       Mean reward: 6321.91
               Mean episode length: 300.78
                 Mean success rate: 54.50
                  Mean reward/step: 24.85
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16867328
                    Iteration time: 2.52s
                        Total time: 5284.96s
                               ETA: 4984.6s

################################################################################
                     [1m Learning iteration 2059/4000 [0m

                       Computation: 3172 steps/s (collection: 0.492s, learning 2.090s)
               Value function loss: 140274.2834
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 6182.43
               Mean episode length: 294.67
                 Mean success rate: 53.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 2.58s
                        Total time: 5287.54s
                               ETA: 4982.1s

################################################################################
                     [1m Learning iteration 2060/4000 [0m

                       Computation: 3202 steps/s (collection: 0.498s, learning 2.060s)
               Value function loss: 88068.8994
                    Surrogate loss: 0.0137
             Mean action noise std: 0.92
                       Mean reward: 6623.07
               Mean episode length: 303.13
                 Mean success rate: 56.50
                  Mean reward/step: 22.85
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16883712
                    Iteration time: 2.56s
                        Total time: 5290.10s
                               ETA: 4979.5s

################################################################################
                     [1m Learning iteration 2061/4000 [0m

                       Computation: 3244 steps/s (collection: 0.482s, learning 2.043s)
               Value function loss: 79745.1129
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 6530.04
               Mean episode length: 297.12
                 Mean success rate: 56.00
                  Mean reward/step: 22.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 2.52s
                        Total time: 5292.62s
                               ETA: 4976.9s

################################################################################
                     [1m Learning iteration 2062/4000 [0m

                       Computation: 3208 steps/s (collection: 0.464s, learning 2.089s)
               Value function loss: 89586.7316
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 6733.34
               Mean episode length: 304.53
                 Mean success rate: 56.50
                  Mean reward/step: 22.85
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16900096
                    Iteration time: 2.55s
                        Total time: 5295.17s
                               ETA: 4974.3s

################################################################################
                     [1m Learning iteration 2063/4000 [0m

                       Computation: 3233 steps/s (collection: 0.482s, learning 2.051s)
               Value function loss: 57251.0276
                    Surrogate loss: 0.0132
             Mean action noise std: 0.92
                       Mean reward: 6678.60
               Mean episode length: 303.98
                 Mean success rate: 56.50
                  Mean reward/step: 23.17
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 2.53s
                        Total time: 5297.71s
                               ETA: 4971.7s

################################################################################
                     [1m Learning iteration 2064/4000 [0m

                       Computation: 3223 steps/s (collection: 0.507s, learning 2.034s)
               Value function loss: 114067.7955
                    Surrogate loss: 0.0157
             Mean action noise std: 0.92
                       Mean reward: 7108.94
               Mean episode length: 321.26
                 Mean success rate: 58.50
                  Mean reward/step: 23.23
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 16916480
                    Iteration time: 2.54s
                        Total time: 5300.25s
                               ETA: 4969.1s

################################################################################
                     [1m Learning iteration 2065/4000 [0m

                       Computation: 3166 steps/s (collection: 0.494s, learning 2.093s)
               Value function loss: 84566.4135
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 7305.70
               Mean episode length: 327.06
                 Mean success rate: 59.50
                  Mean reward/step: 22.88
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 2.59s
                        Total time: 5302.84s
                               ETA: 4966.6s

################################################################################
                     [1m Learning iteration 2066/4000 [0m

                       Computation: 3177 steps/s (collection: 0.539s, learning 2.039s)
               Value function loss: 159217.2621
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 7711.21
               Mean episode length: 339.17
                 Mean success rate: 62.50
                  Mean reward/step: 23.21
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 16932864
                    Iteration time: 2.58s
                        Total time: 5305.41s
                               ETA: 4964.0s

################################################################################
                     [1m Learning iteration 2067/4000 [0m

                       Computation: 3187 steps/s (collection: 0.512s, learning 2.058s)
               Value function loss: 117592.9930
                    Surrogate loss: 0.0114
             Mean action noise std: 0.92
                       Mean reward: 8104.63
               Mean episode length: 348.61
                 Mean success rate: 64.00
                  Mean reward/step: 22.32
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 2.57s
                        Total time: 5307.98s
                               ETA: 4961.5s

################################################################################
                     [1m Learning iteration 2068/4000 [0m

                       Computation: 3298 steps/s (collection: 0.440s, learning 2.044s)
               Value function loss: 109692.3885
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 8127.67
               Mean episode length: 350.00
                 Mean success rate: 64.00
                  Mean reward/step: 21.91
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16949248
                    Iteration time: 2.48s
                        Total time: 5310.47s
                               ETA: 4958.8s

################################################################################
                     [1m Learning iteration 2069/4000 [0m

                       Computation: 3204 steps/s (collection: 0.472s, learning 2.084s)
               Value function loss: 73892.3212
                    Surrogate loss: 0.0122
             Mean action noise std: 0.92
                       Mean reward: 7773.29
               Mean episode length: 342.01
                 Mean success rate: 61.50
                  Mean reward/step: 22.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 2.56s
                        Total time: 5313.02s
                               ETA: 4956.3s

################################################################################
                     [1m Learning iteration 2070/4000 [0m

                       Computation: 3214 steps/s (collection: 0.501s, learning 2.047s)
               Value function loss: 89637.4572
                    Surrogate loss: 0.0183
             Mean action noise std: 0.92
                       Mean reward: 8140.23
               Mean episode length: 350.00
                 Mean success rate: 64.00
                  Mean reward/step: 22.79
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16965632
                    Iteration time: 2.55s
                        Total time: 5315.57s
                               ETA: 4953.7s

################################################################################
                     [1m Learning iteration 2071/4000 [0m

                       Computation: 3218 steps/s (collection: 0.467s, learning 2.079s)
               Value function loss: 82769.0635
                    Surrogate loss: 0.0174
             Mean action noise std: 0.92
                       Mean reward: 8344.40
               Mean episode length: 352.58
                 Mean success rate: 65.00
                  Mean reward/step: 22.75
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 2.55s
                        Total time: 5318.12s
                               ETA: 4951.1s

################################################################################
                     [1m Learning iteration 2072/4000 [0m

                       Computation: 3219 steps/s (collection: 0.468s, learning 2.076s)
               Value function loss: 75406.1127
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 8220.10
               Mean episode length: 349.02
                 Mean success rate: 64.00
                  Mean reward/step: 22.86
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16982016
                    Iteration time: 2.54s
                        Total time: 5320.66s
                               ETA: 4948.5s

################################################################################
                     [1m Learning iteration 2073/4000 [0m

                       Computation: 3206 steps/s (collection: 0.472s, learning 2.083s)
               Value function loss: 63620.5491
                    Surrogate loss: 0.0131
             Mean action noise std: 0.92
                       Mean reward: 7740.14
               Mean episode length: 333.69
                 Mean success rate: 61.00
                  Mean reward/step: 23.33
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 2.56s
                        Total time: 5323.22s
                               ETA: 4945.9s

################################################################################
                     [1m Learning iteration 2074/4000 [0m

                       Computation: 3262 steps/s (collection: 0.480s, learning 2.032s)
               Value function loss: 72170.2495
                    Surrogate loss: 0.0131
             Mean action noise std: 0.92
                       Mean reward: 7355.07
               Mean episode length: 323.13
                 Mean success rate: 58.00
                  Mean reward/step: 23.70
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16998400
                    Iteration time: 2.51s
                        Total time: 5325.73s
                               ETA: 4943.3s

################################################################################
                     [1m Learning iteration 2075/4000 [0m

                       Computation: 3250 steps/s (collection: 0.468s, learning 2.052s)
               Value function loss: 152788.8383
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 7522.09
               Mean episode length: 326.72
                 Mean success rate: 59.50
                  Mean reward/step: 22.84
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.52s
                        Total time: 5328.25s
                               ETA: 4940.7s

################################################################################
                     [1m Learning iteration 2076/4000 [0m

                       Computation: 3265 steps/s (collection: 0.473s, learning 2.035s)
               Value function loss: 60432.1991
                    Surrogate loss: 0.0123
             Mean action noise std: 0.92
                       Mean reward: 6884.59
               Mean episode length: 305.56
                 Mean success rate: 55.50
                  Mean reward/step: 22.07
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17014784
                    Iteration time: 2.51s
                        Total time: 5330.76s
                               ETA: 4938.1s
Traceback (most recent call last):
  File "tools/train_ppo.py", line 51, in <module>
    train()
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "tools/train_ppo.py", line 47, in train
    ppo.run(num_learning_iterations=max_iterations, log_interval=cfg.train.learn.save_interval)
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/mvp/ppo/ppo.py", line 267, in run
    mean_value_loss, mean_surrogate_loss = self.update(it, num_learning_iterations)
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/mvp/ppo/ppo.py", line 415, in update
    mean_value_loss += value_loss.item()
KeyboardInterrupt
