PopSpikeActor(
  (encoder): PopSpikeEncoderRegularSpike()
  (snn): SpikeMLP(
    (hidden_layers): ModuleList(
      (0): Linear(in_features=340, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=64, bias=True)
    )
    (out_pop_layer): Linear(in_features=64, out_features=90, bias=True)
  )
  (decoder): PopSpikeDecoder(
    (decoder): Conv1d(9, 9, kernel_size=(10,), stride=(1,), groups=9)
    (output_activation): ELU(alpha=1.0)
  )
)
Sequential(
  (0): Linear(in_features=34, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=1, bias=True)
)
################################################################################
                      [1m Learning iteration 0/4000 [0m

                       Computation: 2866 steps/s (collection: 0.603s, learning 2.255s)
               Value function loss: 3.1283
                    Surrogate loss: 0.0103
             Mean action noise std: 1.00
                       Mean reward: 4.62
               Mean episode length: 14.82
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 22.08
--------------------------------------------------------------------------------
                   Total timesteps: 8192
                    Iteration time: 2.86s
                        Total time: 2.86s
                               ETA: 11431.2s

################################################################################
                      [1m Learning iteration 1/4000 [0m

                       Computation: 3120 steps/s (collection: 0.440s, learning 2.185s)
               Value function loss: 3.8134
                    Surrogate loss: 0.0085
             Mean action noise std: 1.00
                       Mean reward: 6.68
               Mean episode length: 23.05
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 13.86
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 2.63s
                        Total time: 5.48s
                               ETA: 10963.1s

################################################################################
                      [1m Learning iteration 2/4000 [0m

                       Computation: 3163 steps/s (collection: 0.468s, learning 2.122s)
               Value function loss: 3.8568
                    Surrogate loss: 0.0087
             Mean action noise std: 1.00
                       Mean reward: 6.69
               Mean episode length: 22.59
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 14.37
--------------------------------------------------------------------------------
                   Total timesteps: 24576
                    Iteration time: 2.59s
                        Total time: 8.07s
                               ETA: 10758.2s

################################################################################
                      [1m Learning iteration 3/4000 [0m

                       Computation: 3216 steps/s (collection: 0.443s, learning 2.104s)
               Value function loss: 3.5950
                    Surrogate loss: 0.0095
             Mean action noise std: 1.00
                       Mean reward: 7.21
               Mean episode length: 25.16
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 16.38
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 2.55s
                        Total time: 10.62s
                               ETA: 10611.8s

################################################################################
                      [1m Learning iteration 4/4000 [0m

                       Computation: 3197 steps/s (collection: 0.484s, learning 2.078s)
               Value function loss: 4.0636
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 7.83
               Mean episode length: 28.48
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 15.43
--------------------------------------------------------------------------------
                   Total timesteps: 40960
                    Iteration time: 2.56s
                        Total time: 13.18s
                               ETA: 10535.1s

################################################################################
                      [1m Learning iteration 5/4000 [0m

                       Computation: 3141 steps/s (collection: 0.479s, learning 2.129s)
               Value function loss: 3.2136
                    Surrogate loss: 0.0080
             Mean action noise std: 1.00
                       Mean reward: 8.66
               Mean episode length: 31.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 18.88
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 2.61s
                        Total time: 15.79s
                               ETA: 10513.4s

################################################################################
                      [1m Learning iteration 6/4000 [0m

                       Computation: 3241 steps/s (collection: 0.441s, learning 2.086s)
               Value function loss: 3.4731
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 8.58
               Mean episode length: 31.14
                 Mean success rate: 0.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 18.45
--------------------------------------------------------------------------------
                   Total timesteps: 57344
                    Iteration time: 2.53s
                        Total time: 18.32s
                               ETA: 10451.2s

################################################################################
                      [1m Learning iteration 7/4000 [0m

                       Computation: 3240 steps/s (collection: 0.443s, learning 2.085s)
               Value function loss: 2.7483
                    Surrogate loss: 0.0063
             Mean action noise std: 1.00
                       Mean reward: 8.63
               Mean episode length: 32.23
                 Mean success rate: 0.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 21.85
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 2.53s
                        Total time: 20.85s
                               ETA: 10404.4s

################################################################################
                      [1m Learning iteration 8/4000 [0m

                       Computation: 3221 steps/s (collection: 0.428s, learning 2.115s)
               Value function loss: 2.5918
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 10.03
               Mean episode length: 36.16
                 Mean success rate: 0.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 22.88
--------------------------------------------------------------------------------
                   Total timesteps: 73728
                    Iteration time: 2.54s
                        Total time: 23.39s
                               ETA: 10374.1s

################################################################################
                      [1m Learning iteration 9/4000 [0m

                       Computation: 3200 steps/s (collection: 0.469s, learning 2.091s)
               Value function loss: 3.1224
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 10.42
               Mean episode length: 37.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 24.24
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 2.56s
                        Total time: 25.95s
                               ETA: 10355.9s

################################################################################
                      [1m Learning iteration 10/4000 [0m

                       Computation: 3277 steps/s (collection: 0.435s, learning 2.064s)
               Value function loss: 2.8596
                    Surrogate loss: 0.0041
             Mean action noise std: 1.00
                       Mean reward: 12.12
               Mean episode length: 48.09
                 Mean success rate: 0.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 90112
                    Iteration time: 2.50s
                        Total time: 28.45s
                               ETA: 10318.8s

################################################################################
                      [1m Learning iteration 11/4000 [0m

                       Computation: 3152 steps/s (collection: 0.470s, learning 2.129s)
               Value function loss: 3.2226
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 13.18
               Mean episode length: 57.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 2.60s
                        Total time: 31.05s
                               ETA: 10320.3s

################################################################################
                      [1m Learning iteration 12/4000 [0m

                       Computation: 3157 steps/s (collection: 0.443s, learning 2.151s)
               Value function loss: 2.2252
                    Surrogate loss: 0.0039
             Mean action noise std: 1.00
                       Mean reward: 14.89
               Mean episode length: 68.00
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 106496
                    Iteration time: 2.59s
                        Total time: 33.64s
                               ETA: 10319.9s

################################################################################
                      [1m Learning iteration 13/4000 [0m

                       Computation: 3183 steps/s (collection: 0.445s, learning 2.129s)
               Value function loss: 3.3070
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 16.29
               Mean episode length: 76.03
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 2.57s
                        Total time: 36.21s
                               ETA: 10313.2s

################################################################################
                      [1m Learning iteration 14/4000 [0m

                       Computation: 3137 steps/s (collection: 0.481s, learning 2.130s)
               Value function loss: 4.6267
                    Surrogate loss: 0.0064
             Mean action noise std: 1.00
                       Mean reward: 19.43
               Mean episode length: 94.08
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 25.44
--------------------------------------------------------------------------------
                   Total timesteps: 122880
                    Iteration time: 2.61s
                        Total time: 38.82s
                               ETA: 10317.0s

################################################################################
                      [1m Learning iteration 15/4000 [0m

                       Computation: 3100 steps/s (collection: 0.516s, learning 2.126s)
               Value function loss: 2.8597
                    Surrogate loss: 0.0035
             Mean action noise std: 1.00
                       Mean reward: 22.45
               Mean episode length: 108.05
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 25.84
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 2.64s
                        Total time: 41.47s
                               ETA: 10327.8s

################################################################################
                      [1m Learning iteration 16/4000 [0m

                       Computation: 3209 steps/s (collection: 0.440s, learning 2.112s)
               Value function loss: 2.2277
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 22.92
               Mean episode length: 112.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 139264
                    Iteration time: 2.55s
                        Total time: 44.02s
                               ETA: 10316.0s

################################################################################
                      [1m Learning iteration 17/4000 [0m

                       Computation: 3225 steps/s (collection: 0.443s, learning 2.097s)
               Value function loss: 2.0657
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 25.13
               Mean episode length: 124.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 2.54s
                        Total time: 46.56s
                               ETA: 10302.4s

################################################################################
                      [1m Learning iteration 18/4000 [0m

                       Computation: 3080 steps/s (collection: 0.458s, learning 2.201s)
               Value function loss: 2.3741
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 26.61
               Mean episode length: 133.50
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 155648
                    Iteration time: 2.66s
                        Total time: 49.22s
                               ETA: 10315.0s

################################################################################
                      [1m Learning iteration 19/4000 [0m

                       Computation: 3085 steps/s (collection: 0.496s, learning 2.158s)
               Value function loss: 2.5915
                    Surrogate loss: 0.0036
             Mean action noise std: 1.00
                       Mean reward: 28.41
               Mean episode length: 143.26
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 2.65s
                        Total time: 51.87s
                               ETA: 10325.2s

################################################################################
                      [1m Learning iteration 20/4000 [0m

                       Computation: 3157 steps/s (collection: 0.469s, learning 2.125s)
               Value function loss: 3.0292
                    Surrogate loss: 0.0032
             Mean action noise std: 1.00
                       Mean reward: 32.70
               Mean episode length: 166.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 172032
                    Iteration time: 2.59s
                        Total time: 54.47s
                               ETA: 10322.7s

################################################################################
                      [1m Learning iteration 21/4000 [0m

                       Computation: 3194 steps/s (collection: 0.444s, learning 2.121s)
               Value function loss: 5.7467
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 37.21
               Mean episode length: 192.68
                 Mean success rate: 0.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 2.56s
                        Total time: 57.03s
                               ETA: 10314.9s

################################################################################
                      [1m Learning iteration 22/4000 [0m

                       Computation: 3167 steps/s (collection: 0.447s, learning 2.139s)
               Value function loss: 5.5011
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 39.63
               Mean episode length: 204.99
                 Mean success rate: 0.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 188416
                    Iteration time: 2.59s
                        Total time: 59.62s
                               ETA: 10311.2s

################################################################################
                      [1m Learning iteration 23/4000 [0m

                       Computation: 3204 steps/s (collection: 0.467s, learning 2.089s)
               Value function loss: 6.1925
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 39.85
               Mean episode length: 201.23
                 Mean success rate: 0.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 2.56s
                        Total time: 62.17s
                               ETA: 10302.7s

################################################################################
                      [1m Learning iteration 24/4000 [0m

                       Computation: 3216 steps/s (collection: 0.470s, learning 2.077s)
               Value function loss: 7.7792
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 36.47
               Mean episode length: 177.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 204800
                    Iteration time: 2.55s
                        Total time: 64.72s
                               ETA: 10293.1s

################################################################################
                      [1m Learning iteration 25/4000 [0m

                       Computation: 3230 steps/s (collection: 0.474s, learning 2.062s)
               Value function loss: 7.1043
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 36.77
               Mean episode length: 175.19
                 Mean success rate: 0.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 2.54s
                        Total time: 67.26s
                               ETA: 10282.4s

################################################################################
                      [1m Learning iteration 26/4000 [0m

                       Computation: 3231 steps/s (collection: 0.478s, learning 2.057s)
               Value function loss: 10.8822
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 35.38
               Mean episode length: 162.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 25.44
--------------------------------------------------------------------------------
                   Total timesteps: 221184
                    Iteration time: 2.53s
                        Total time: 69.79s
                               ETA: 10272.2s

################################################################################
                      [1m Learning iteration 27/4000 [0m

                       Computation: 3211 steps/s (collection: 0.485s, learning 2.065s)
               Value function loss: 6.4432
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 36.67
               Mean episode length: 167.35
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 2.55s
                        Total time: 72.34s
                               ETA: 10264.8s

################################################################################
                      [1m Learning iteration 28/4000 [0m

                       Computation: 3158 steps/s (collection: 0.527s, learning 2.067s)
               Value function loss: 9.6782
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 39.70
               Mean episode length: 178.57
                 Mean success rate: 0.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 237568
                    Iteration time: 2.59s
                        Total time: 74.94s
                               ETA: 10263.6s

################################################################################
                      [1m Learning iteration 29/4000 [0m

                       Computation: 3281 steps/s (collection: 0.453s, learning 2.043s)
               Value function loss: 9.4927
                    Surrogate loss: 0.0018
             Mean action noise std: 1.00
                       Mean reward: 40.83
               Mean episode length: 176.27
                 Mean success rate: 0.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 2.50s
                        Total time: 77.43s
                               ETA: 10249.4s

################################################################################
                      [1m Learning iteration 30/4000 [0m

                       Computation: 3214 steps/s (collection: 0.507s, learning 2.042s)
               Value function loss: 9.1109
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 46.31
               Mean episode length: 198.49
                 Mean success rate: 0.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 253952
                    Iteration time: 2.55s
                        Total time: 79.98s
                               ETA: 10242.7s

################################################################################
                      [1m Learning iteration 31/4000 [0m

                       Computation: 3226 steps/s (collection: 0.450s, learning 2.089s)
               Value function loss: 5.6722
                    Surrogate loss: 0.0036
             Mean action noise std: 1.00
                       Mean reward: 49.30
               Mean episode length: 209.64
                 Mean success rate: 0.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 2.54s
                        Total time: 82.52s
                               ETA: 10235.0s

################################################################################
                      [1m Learning iteration 32/4000 [0m

                       Computation: 3262 steps/s (collection: 0.441s, learning 2.070s)
               Value function loss: 6.1134
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 52.14
               Mean episode length: 221.65
                 Mean success rate: 0.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 270336
                    Iteration time: 2.51s
                        Total time: 85.03s
                               ETA: 10224.3s

################################################################################
                      [1m Learning iteration 33/4000 [0m

                       Computation: 3227 steps/s (collection: 0.465s, learning 2.073s)
               Value function loss: 5.7729
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 55.80
               Mean episode length: 233.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 2.54s
                        Total time: 87.57s
                               ETA: 10217.3s

################################################################################
                      [1m Learning iteration 34/4000 [0m

                       Computation: 3141 steps/s (collection: 0.496s, learning 2.112s)
               Value function loss: 12.9635
                    Surrogate loss: 0.0029
             Mean action noise std: 1.00
                       Mean reward: 63.10
               Mean episode length: 254.89
                 Mean success rate: 0.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 286720
                    Iteration time: 2.61s
                        Total time: 90.18s
                               ETA: 10218.3s

################################################################################
                      [1m Learning iteration 35/4000 [0m

                       Computation: 3053 steps/s (collection: 0.548s, learning 2.135s)
               Value function loss: 11.0533
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 64.74
               Mean episode length: 254.58
                 Mean success rate: 0.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 2.68s
                        Total time: 92.86s
                               ETA: 10227.5s

################################################################################
                      [1m Learning iteration 36/4000 [0m

                       Computation: 3252 steps/s (collection: 0.437s, learning 2.081s)
               Value function loss: 3.5824
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 66.93
               Mean episode length: 262.63
                 Mean success rate: 0.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 31.27
--------------------------------------------------------------------------------
                   Total timesteps: 303104
                    Iteration time: 2.52s
                        Total time: 95.38s
                               ETA: 10218.4s

################################################################################
                      [1m Learning iteration 37/4000 [0m

                       Computation: 3196 steps/s (collection: 0.451s, learning 2.112s)
               Value function loss: 9.0855
                    Surrogate loss: 0.0033
             Mean action noise std: 1.00
                       Mean reward: 71.67
               Mean episode length: 275.45
                 Mean success rate: 0.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 2.56s
                        Total time: 97.94s
                               ETA: 10214.2s

################################################################################
                      [1m Learning iteration 38/4000 [0m

                       Computation: 3211 steps/s (collection: 0.441s, learning 2.110s)
               Value function loss: 8.4355
                    Surrogate loss: 0.0036
             Mean action noise std: 1.00
                       Mean reward: 75.24
               Mean episode length: 282.52
                 Mean success rate: 0.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 319488
                    Iteration time: 2.55s
                        Total time: 100.49s
                               ETA: 10208.9s

################################################################################
                      [1m Learning iteration 39/4000 [0m

                       Computation: 3086 steps/s (collection: 0.499s, learning 2.154s)
               Value function loss: 8.7662
                    Surrogate loss: 0.0038
             Mean action noise std: 1.00
                       Mean reward: 79.57
               Mean episode length: 295.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 2.65s
                        Total time: 103.15s
                               ETA: 10214.0s

################################################################################
                      [1m Learning iteration 40/4000 [0m

                       Computation: 3174 steps/s (collection: 0.472s, learning 2.108s)
               Value function loss: 11.5762
                    Surrogate loss: 0.0039
             Mean action noise std: 1.00
                       Mean reward: 86.27
               Mean episode length: 308.54
                 Mean success rate: 0.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 335872
                    Iteration time: 2.58s
                        Total time: 105.73s
                               ETA: 10211.6s

################################################################################
                      [1m Learning iteration 41/4000 [0m

                       Computation: 3281 steps/s (collection: 0.430s, learning 2.066s)
               Value function loss: 14.4842
                    Surrogate loss: 0.0034
             Mean action noise std: 1.00
                       Mean reward: 93.02
               Mean episode length: 321.09
                 Mean success rate: 0.00
                  Mean reward/step: 0.37
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 2.50s
                        Total time: 108.22s
                               ETA: 10201.2s

################################################################################
                      [1m Learning iteration 42/4000 [0m

                       Computation: 3193 steps/s (collection: 0.427s, learning 2.137s)
               Value function loss: 22.1281
                    Surrogate loss: 0.0032
             Mean action noise std: 1.00
                       Mean reward: 101.16
               Mean episode length: 332.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.38
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 352256
                    Iteration time: 2.56s
                        Total time: 110.79s
                               ETA: 10197.6s

################################################################################
                      [1m Learning iteration 43/4000 [0m

                       Computation: 3147 steps/s (collection: 0.454s, learning 2.149s)
               Value function loss: 24.4975
                    Surrogate loss: 0.0041
             Mean action noise std: 1.00
                       Mean reward: 105.28
               Mean episode length: 338.48
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 2.60s
                        Total time: 113.39s
                               ETA: 10197.4s

################################################################################
                      [1m Learning iteration 44/4000 [0m

                       Computation: 3103 steps/s (collection: 0.515s, learning 2.124s)
               Value function loss: 32.2286
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 112.07
               Mean episode length: 344.86
                 Mean success rate: 0.00
                  Mean reward/step: 0.40
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 368640
                    Iteration time: 2.64s
                        Total time: 116.03s
                               ETA: 10200.3s

################################################################################
                      [1m Learning iteration 45/4000 [0m

                       Computation: 3175 steps/s (collection: 0.475s, learning 2.104s)
               Value function loss: 25.4297
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 112.04
               Mean episode length: 338.38
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 2.58s
                        Total time: 118.61s
                               ETA: 10197.8s

################################################################################
                      [1m Learning iteration 46/4000 [0m

                       Computation: 3251 steps/s (collection: 0.448s, learning 2.071s)
               Value function loss: 26.4374
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 111.56
               Mean episode length: 324.31
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 385024
                    Iteration time: 2.52s
                        Total time: 121.13s
                               ETA: 10190.2s

################################################################################
                      [1m Learning iteration 47/4000 [0m

                       Computation: 3197 steps/s (collection: 0.482s, learning 2.079s)
               Value function loss: 16.9964
                    Surrogate loss: 0.0036
             Mean action noise std: 1.00
                       Mean reward: 104.64
               Mean episode length: 303.60
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 2.56s
                        Total time: 123.69s
                               ETA: 10186.4s

################################################################################
                      [1m Learning iteration 48/4000 [0m

                       Computation: 3181 steps/s (collection: 0.496s, learning 2.079s)
               Value function loss: 19.8705
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 102.16
               Mean episode length: 287.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 401408
                    Iteration time: 2.57s
                        Total time: 126.26s
                               ETA: 10183.6s

################################################################################
                      [1m Learning iteration 49/4000 [0m

                       Computation: 3282 steps/s (collection: 0.439s, learning 2.057s)
               Value function loss: 23.9072
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 95.30
               Mean episode length: 264.50
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 2.50s
                        Total time: 128.76s
                               ETA: 10174.7s

################################################################################
                      [1m Learning iteration 50/4000 [0m

                       Computation: 3188 steps/s (collection: 0.461s, learning 2.108s)
               Value function loss: 20.5074
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 100.75
               Mean episode length: 273.92
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 417792
                    Iteration time: 2.57s
                        Total time: 131.33s
                               ETA: 10171.6s

################################################################################
                      [1m Learning iteration 51/4000 [0m

                       Computation: 3203 steps/s (collection: 0.462s, learning 2.096s)
               Value function loss: 15.8841
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 99.09
               Mean episode length: 264.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.44
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 2.56s
                        Total time: 133.89s
                               ETA: 10167.7s

################################################################################
                      [1m Learning iteration 52/4000 [0m

                       Computation: 3239 steps/s (collection: 0.443s, learning 2.086s)
               Value function loss: 16.1889
                    Surrogate loss: 0.0040
             Mean action noise std: 1.00
                       Mean reward: 95.92
               Mean episode length: 250.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 434176
                    Iteration time: 2.53s
                        Total time: 136.42s
                               ETA: 10161.7s

################################################################################
                      [1m Learning iteration 53/4000 [0m

                       Computation: 3216 steps/s (collection: 0.467s, learning 2.080s)
               Value function loss: 9.0023
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 99.00
               Mean episode length: 255.99
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 2.55s
                        Total time: 138.96s
                               ETA: 10157.2s

################################################################################
                      [1m Learning iteration 54/4000 [0m

                       Computation: 3249 steps/s (collection: 0.452s, learning 2.070s)
               Value function loss: 9.8996
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 101.10
               Mean episode length: 257.30
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 450560
                    Iteration time: 2.52s
                        Total time: 141.48s
                               ETA: 10150.9s

################################################################################
                      [1m Learning iteration 55/4000 [0m

                       Computation: 3280 steps/s (collection: 0.416s, learning 2.081s)
               Value function loss: 12.9161
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 105.24
               Mean episode length: 266.01
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 2.50s
                        Total time: 143.98s
                               ETA: 10143.0s

################################################################################
                      [1m Learning iteration 56/4000 [0m

                       Computation: 3209 steps/s (collection: 0.461s, learning 2.091s)
               Value function loss: 15.3861
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 107.39
               Mean episode length: 268.93
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 466944
                    Iteration time: 2.55s
                        Total time: 146.53s
                               ETA: 10139.1s

################################################################################
                      [1m Learning iteration 57/4000 [0m

                       Computation: 3265 steps/s (collection: 0.450s, learning 2.058s)
               Value function loss: 23.6978
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 111.81
               Mean episode length: 276.33
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 2.51s
                        Total time: 149.04s
                               ETA: 10132.3s

################################################################################
                      [1m Learning iteration 58/4000 [0m

                       Computation: 3219 steps/s (collection: 0.415s, learning 2.129s)
               Value function loss: 19.7106
                    Surrogate loss: 0.0042
             Mean action noise std: 1.00
                       Mean reward: 117.32
               Mean episode length: 283.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 483328
                    Iteration time: 2.54s
                        Total time: 151.59s
                               ETA: 10128.0s

################################################################################
                      [1m Learning iteration 59/4000 [0m

                       Computation: 3195 steps/s (collection: 0.454s, learning 2.110s)
               Value function loss: 31.0698
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 128.72
               Mean episode length: 300.71
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 2.56s
                        Total time: 154.15s
                               ETA: 10125.1s

################################################################################
                      [1m Learning iteration 60/4000 [0m

                       Computation: 3139 steps/s (collection: 0.475s, learning 2.134s)
               Value function loss: 36.0209
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 139.14
               Mean episode length: 318.85
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 499712
                    Iteration time: 2.61s
                        Total time: 156.76s
                               ETA: 10125.1s

################################################################################
                      [1m Learning iteration 61/4000 [0m

                       Computation: 3239 steps/s (collection: 0.470s, learning 2.059s)
               Value function loss: 42.3936
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 139.34
               Mean episode length: 315.93
                 Mean success rate: 0.00
                  Mean reward/step: 0.46
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 2.53s
                        Total time: 159.29s
                               ETA: 10119.9s

################################################################################
                      [1m Learning iteration 62/4000 [0m

                       Computation: 3287 steps/s (collection: 0.427s, learning 2.065s)
               Value function loss: 30.3265
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 140.99
               Mean episode length: 317.82
                 Mean success rate: 0.00
                  Mean reward/step: 0.46
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 516096
                    Iteration time: 2.49s
                        Total time: 161.78s
                               ETA: 10112.5s

################################################################################
                      [1m Learning iteration 63/4000 [0m

                       Computation: 3185 steps/s (collection: 0.464s, learning 2.107s)
               Value function loss: 24.7763
                    Surrogate loss: 0.0035
             Mean action noise std: 1.00
                       Mean reward: 133.79
               Mean episode length: 300.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 2.57s
                        Total time: 164.35s
                               ETA: 10110.2s

################################################################################
                      [1m Learning iteration 64/4000 [0m

                       Computation: 3215 steps/s (collection: 0.437s, learning 2.111s)
               Value function loss: 23.9777
                    Surrogate loss: 0.0034
             Mean action noise std: 1.00
                       Mean reward: 129.14
               Mean episode length: 291.09
                 Mean success rate: 0.00
                  Mean reward/step: 0.44
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 532480
                    Iteration time: 2.55s
                        Total time: 166.90s
                               ETA: 10106.4s

################################################################################
                      [1m Learning iteration 65/4000 [0m

                       Computation: 3269 steps/s (collection: 0.442s, learning 2.063s)
               Value function loss: 24.5467
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 136.94
               Mean episode length: 309.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 2.51s
                        Total time: 169.40s
                               ETA: 10100.1s

################################################################################
                      [1m Learning iteration 66/4000 [0m

                       Computation: 3233 steps/s (collection: 0.445s, learning 2.089s)
               Value function loss: 21.8619
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 129.36
               Mean episode length: 294.04
                 Mean success rate: 0.00
                  Mean reward/step: 0.46
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 548864
                    Iteration time: 2.53s
                        Total time: 171.94s
                               ETA: 10095.6s

################################################################################
                      [1m Learning iteration 67/4000 [0m

                       Computation: 3176 steps/s (collection: 0.461s, learning 2.117s)
               Value function loss: 26.5264
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 124.98
               Mean episode length: 284.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.46
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 2.58s
                        Total time: 174.52s
                               ETA: 10093.8s

################################################################################
                      [1m Learning iteration 68/4000 [0m

                       Computation: 3159 steps/s (collection: 0.467s, learning 2.126s)
               Value function loss: 24.4709
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 118.37
               Mean episode length: 267.95
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 565248
                    Iteration time: 2.59s
                        Total time: 177.11s
                               ETA: 10092.7s

################################################################################
                      [1m Learning iteration 69/4000 [0m

                       Computation: 3226 steps/s (collection: 0.435s, learning 2.104s)
               Value function loss: 17.6898
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 115.42
               Mean episode length: 261.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 2.54s
                        Total time: 179.65s
                               ETA: 10088.6s

################################################################################
                      [1m Learning iteration 70/4000 [0m

                       Computation: 3176 steps/s (collection: 0.471s, learning 2.109s)
               Value function loss: 17.2208
                    Surrogate loss: 0.0039
             Mean action noise std: 1.00
                       Mean reward: 110.19
               Mean episode length: 250.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 581632
                    Iteration time: 2.58s
                        Total time: 182.23s
                               ETA: 10086.7s

################################################################################
                      [1m Learning iteration 71/4000 [0m

                       Computation: 3215 steps/s (collection: 0.444s, learning 2.104s)
               Value function loss: 26.0518
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 106.09
               Mean episode length: 240.19
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 2.55s
                        Total time: 184.78s
                               ETA: 10083.1s

################################################################################
                      [1m Learning iteration 72/4000 [0m

                       Computation: 3218 steps/s (collection: 0.436s, learning 2.109s)
               Value function loss: 24.0797
                    Surrogate loss: 0.0042
             Mean action noise std: 1.00
                       Mean reward: 97.80
               Mean episode length: 221.05
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 598016
                    Iteration time: 2.55s
                        Total time: 187.32s
                               ETA: 10079.4s

################################################################################
                      [1m Learning iteration 73/4000 [0m

                       Computation: 3169 steps/s (collection: 0.499s, learning 2.085s)
               Value function loss: 44.6417
                    Surrogate loss: 0.0066
             Mean action noise std: 1.00
                       Mean reward: 99.66
               Mean episode length: 222.53
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 2.58s
                        Total time: 189.91s
                               ETA: 10077.8s

################################################################################
                      [1m Learning iteration 74/4000 [0m

                       Computation: 3170 steps/s (collection: 0.468s, learning 2.116s)
               Value function loss: 31.7702
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 103.43
               Mean episode length: 226.38
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 614400
                    Iteration time: 2.58s
                        Total time: 192.49s
                               ETA: 10076.2s

################################################################################
                      [1m Learning iteration 75/4000 [0m

                       Computation: 3167 steps/s (collection: 0.436s, learning 2.150s)
               Value function loss: 41.9884
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 109.62
               Mean episode length: 232.68
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 2.59s
                        Total time: 195.08s
                               ETA: 10074.7s

################################################################################
                      [1m Learning iteration 76/4000 [0m

                       Computation: 3181 steps/s (collection: 0.485s, learning 2.090s)
               Value function loss: 35.2574
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 122.23
               Mean episode length: 257.58
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 630784
                    Iteration time: 2.58s
                        Total time: 197.65s
                               ETA: 10072.5s

################################################################################
                      [1m Learning iteration 77/4000 [0m

                       Computation: 3202 steps/s (collection: 0.462s, learning 2.095s)
               Value function loss: 34.7643
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 126.97
               Mean episode length: 262.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 2.56s
                        Total time: 200.21s
                               ETA: 10069.5s

################################################################################
                      [1m Learning iteration 78/4000 [0m

                       Computation: 3267 steps/s (collection: 0.419s, learning 2.088s)
               Value function loss: 29.6807
                    Surrogate loss: 0.0064
             Mean action noise std: 1.00
                       Mean reward: 125.26
               Mean episode length: 253.67
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 647168
                    Iteration time: 2.51s
                        Total time: 202.72s
                               ETA: 10064.0s

################################################################################
                      [1m Learning iteration 79/4000 [0m

                       Computation: 3153 steps/s (collection: 0.429s, learning 2.168s)
               Value function loss: 29.6905
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 134.54
               Mean episode length: 269.25
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 2.60s
                        Total time: 205.31s
                               ETA: 10062.9s

################################################################################
                      [1m Learning iteration 80/4000 [0m

                       Computation: 3135 steps/s (collection: 0.491s, learning 2.121s)
               Value function loss: 31.7222
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 133.08
               Mean episode length: 267.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 663552
                    Iteration time: 2.61s
                        Total time: 207.93s
                               ETA: 10062.6s

################################################################################
                      [1m Learning iteration 81/4000 [0m

                       Computation: 3258 steps/s (collection: 0.445s, learning 2.068s)
               Value function loss: 27.7732
                    Surrogate loss: 0.0061
             Mean action noise std: 1.00
                       Mean reward: 136.20
               Mean episode length: 269.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 2.51s
                        Total time: 210.44s
                               ETA: 10057.5s

################################################################################
                      [1m Learning iteration 82/4000 [0m

                       Computation: 3248 steps/s (collection: 0.447s, learning 2.075s)
               Value function loss: 30.7312
                    Surrogate loss: 0.0041
             Mean action noise std: 1.00
                       Mean reward: 132.53
               Mean episode length: 259.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 679936
                    Iteration time: 2.52s
                        Total time: 212.96s
                               ETA: 10052.8s

################################################################################
                      [1m Learning iteration 83/4000 [0m

                       Computation: 3175 steps/s (collection: 0.452s, learning 2.128s)
               Value function loss: 27.1442
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 134.15
               Mean episode length: 259.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 2.58s
                        Total time: 215.54s
                               ETA: 10050.9s

################################################################################
                      [1m Learning iteration 84/4000 [0m

                       Computation: 3205 steps/s (collection: 0.449s, learning 2.106s)
               Value function loss: 26.6053
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 136.70
               Mean episode length: 264.04
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 696320
                    Iteration time: 2.56s
                        Total time: 218.10s
                               ETA: 10047.9s

################################################################################
                      [1m Learning iteration 85/4000 [0m

                       Computation: 3221 steps/s (collection: 0.446s, learning 2.097s)
               Value function loss: 25.3604
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 135.75
               Mean episode length: 261.64
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 2.54s
                        Total time: 220.64s
                               ETA: 10044.3s

################################################################################
                      [1m Learning iteration 86/4000 [0m

                       Computation: 3173 steps/s (collection: 0.477s, learning 2.105s)
               Value function loss: 34.4341
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 133.45
               Mean episode length: 257.25
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 712704
                    Iteration time: 2.58s
                        Total time: 223.22s
                               ETA: 10042.4s

################################################################################
                      [1m Learning iteration 87/4000 [0m

                       Computation: 3178 steps/s (collection: 0.500s, learning 2.077s)
               Value function loss: 32.1831
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 131.79
               Mean episode length: 254.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 2.58s
                        Total time: 225.80s
                               ETA: 10040.3s

################################################################################
                      [1m Learning iteration 88/4000 [0m

                       Computation: 3143 steps/s (collection: 0.529s, learning 2.077s)
               Value function loss: 37.2450
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 126.47
               Mean episode length: 244.41
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 729088
                    Iteration time: 2.61s
                        Total time: 228.40s
                               ETA: 10039.5s

################################################################################
                      [1m Learning iteration 89/4000 [0m

                       Computation: 3189 steps/s (collection: 0.493s, learning 2.075s)
               Value function loss: 39.5968
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 124.42
               Mean episode length: 242.92
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 2.57s
                        Total time: 230.97s
                               ETA: 10037.1s

################################################################################
                      [1m Learning iteration 90/4000 [0m

                       Computation: 3219 steps/s (collection: 0.454s, learning 2.090s)
               Value function loss: 32.8439
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 119.32
               Mean episode length: 235.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 745472
                    Iteration time: 2.54s
                        Total time: 233.52s
                               ETA: 10033.5s

################################################################################
                      [1m Learning iteration 91/4000 [0m

                       Computation: 3211 steps/s (collection: 0.495s, learning 2.056s)
               Value function loss: 37.9471
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 124.57
               Mean episode length: 247.24
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 2.55s
                        Total time: 236.07s
                               ETA: 10030.3s

################################################################################
                      [1m Learning iteration 92/4000 [0m

                       Computation: 3182 steps/s (collection: 0.502s, learning 2.073s)
               Value function loss: 40.7917
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 135.83
               Mean episode length: 267.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 761856
                    Iteration time: 2.57s
                        Total time: 238.64s
                               ETA: 10028.1s

################################################################################
                      [1m Learning iteration 93/4000 [0m

                       Computation: 3268 steps/s (collection: 0.471s, learning 2.035s)
               Value function loss: 32.4753
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 135.19
               Mean episode length: 268.40
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 2.51s
                        Total time: 241.15s
                               ETA: 10023.1s

################################################################################
                      [1m Learning iteration 94/4000 [0m

                       Computation: 3303 steps/s (collection: 0.407s, learning 2.073s)
               Value function loss: 18.4680
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 141.56
               Mean episode length: 280.09
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 778240
                    Iteration time: 2.48s
                        Total time: 243.63s
                               ETA: 10017.0s

################################################################################
                      [1m Learning iteration 95/4000 [0m

                       Computation: 3177 steps/s (collection: 0.484s, learning 2.093s)
               Value function loss: 33.6609
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 141.30
               Mean episode length: 282.29
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 2.58s
                        Total time: 246.21s
                               ETA: 10014.9s

################################################################################
                      [1m Learning iteration 96/4000 [0m

                       Computation: 3228 steps/s (collection: 0.459s, learning 2.078s)
               Value function loss: 38.5378
                    Surrogate loss: 0.0042
             Mean action noise std: 1.00
                       Mean reward: 140.17
               Mean episode length: 282.96
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 794624
                    Iteration time: 2.54s
                        Total time: 248.74s
                               ETA: 10011.3s

################################################################################
                      [1m Learning iteration 97/4000 [0m

                       Computation: 3192 steps/s (collection: 0.465s, learning 2.101s)
               Value function loss: 34.0907
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 139.40
               Mean episode length: 274.39
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 2.57s
                        Total time: 251.31s
                               ETA: 10008.8s

################################################################################
                      [1m Learning iteration 98/4000 [0m

                       Computation: 3251 steps/s (collection: 0.453s, learning 2.067s)
               Value function loss: 37.1248
                    Surrogate loss: 0.0068
             Mean action noise std: 1.00
                       Mean reward: 121.86
               Mean episode length: 245.19
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 811008
                    Iteration time: 2.52s
                        Total time: 253.83s
                               ETA: 10004.4s

################################################################################
                      [1m Learning iteration 99/4000 [0m

                       Computation: 3258 steps/s (collection: 0.431s, learning 2.083s)
               Value function loss: 28.0205
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 119.99
               Mean episode length: 239.60
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 2.51s
                        Total time: 256.34s
                               ETA: 9999.9s

################################################################################
                     [1m Learning iteration 100/4000 [0m

                       Computation: 3366 steps/s (collection: 0.414s, learning 2.020s)
               Value function loss: 25.8569
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 118.13
               Mean episode length: 233.82
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 827392
                    Iteration time: 2.43s
                        Total time: 258.78s
                               ETA: 9992.3s

################################################################################
                     [1m Learning iteration 101/4000 [0m

                       Computation: 3321 steps/s (collection: 0.410s, learning 2.056s)
               Value function loss: 26.2587
                    Surrogate loss: 0.0064
             Mean action noise std: 1.00
                       Mean reward: 119.72
               Mean episode length: 234.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 2.47s
                        Total time: 261.24s
                               ETA: 9986.1s

################################################################################
                     [1m Learning iteration 102/4000 [0m

                       Computation: 3282 steps/s (collection: 0.445s, learning 2.050s)
               Value function loss: 43.1468
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 124.16
               Mean episode length: 240.84
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 843776
                    Iteration time: 2.50s
                        Total time: 263.74s
                               ETA: 9981.1s

################################################################################
                     [1m Learning iteration 103/4000 [0m

                       Computation: 3237 steps/s (collection: 0.476s, learning 2.055s)
               Value function loss: 15.2156
                    Surrogate loss: 0.0080
             Mean action noise std: 1.00
                       Mean reward: 128.26
               Mean episode length: 251.25
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 2.53s
                        Total time: 266.27s
                               ETA: 9977.4s

################################################################################
                     [1m Learning iteration 104/4000 [0m

                       Computation: 3219 steps/s (collection: 0.460s, learning 2.084s)
               Value function loss: 43.0457
                    Surrogate loss: 0.0072
             Mean action noise std: 1.00
                       Mean reward: 131.81
               Mean episode length: 255.72
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 860160
                    Iteration time: 2.54s
                        Total time: 268.81s
                               ETA: 9974.2s

################################################################################
                     [1m Learning iteration 105/4000 [0m

                       Computation: 3168 steps/s (collection: 0.511s, learning 2.075s)
               Value function loss: 49.2865
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 135.94
               Mean episode length: 260.05
                 Mean success rate: 0.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 2.59s
                        Total time: 271.40s
                               ETA: 9972.6s

################################################################################
                     [1m Learning iteration 106/4000 [0m

                       Computation: 3146 steps/s (collection: 0.517s, learning 2.087s)
               Value function loss: 26.9617
                    Surrogate loss: 0.0068
             Mean action noise std: 1.00
                       Mean reward: 140.92
               Mean episode length: 262.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 876544
                    Iteration time: 2.60s
                        Total time: 274.00s
                               ETA: 9971.6s

################################################################################
                     [1m Learning iteration 107/4000 [0m

                       Computation: 3238 steps/s (collection: 0.465s, learning 2.065s)
               Value function loss: 52.9709
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 140.46
               Mean episode length: 254.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 2.53s
                        Total time: 276.53s
                               ETA: 9968.0s

################################################################################
                     [1m Learning iteration 108/4000 [0m

                       Computation: 3269 steps/s (collection: 0.474s, learning 2.032s)
               Value function loss: 42.1914
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 144.17
               Mean episode length: 259.42
                 Mean success rate: 0.00
                  Mean reward/step: 0.58
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 892928
                    Iteration time: 2.51s
                        Total time: 279.04s
                               ETA: 9963.4s

################################################################################
                     [1m Learning iteration 109/4000 [0m

                       Computation: 3288 steps/s (collection: 0.445s, learning 2.046s)
               Value function loss: 28.8739
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 144.40
               Mean episode length: 258.71
                 Mean success rate: 0.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 2.49s
                        Total time: 281.53s
                               ETA: 9958.4s

################################################################################
                     [1m Learning iteration 110/4000 [0m

                       Computation: 3216 steps/s (collection: 0.449s, learning 2.098s)
               Value function loss: 46.9436
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 130.21
               Mean episode length: 235.82
                 Mean success rate: 0.00
                  Mean reward/step: 0.60
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 909312
                    Iteration time: 2.55s
                        Total time: 284.08s
                               ETA: 9955.5s

################################################################################
                     [1m Learning iteration 111/4000 [0m

                       Computation: 3319 steps/s (collection: 0.416s, learning 2.052s)
               Value function loss: 41.7105
                    Surrogate loss: 0.0079
             Mean action noise std: 1.00
                       Mean reward: 138.63
               Mean episode length: 248.23
                 Mean success rate: 0.00
                  Mean reward/step: 0.60
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 2.47s
                        Total time: 286.54s
                               ETA: 9949.7s

################################################################################
                     [1m Learning iteration 112/4000 [0m

                       Computation: 3217 steps/s (collection: 0.470s, learning 2.076s)
               Value function loss: 39.3818
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 134.15
               Mean episode length: 243.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 925696
                    Iteration time: 2.55s
                        Total time: 289.09s
                               ETA: 9946.7s

################################################################################
                     [1m Learning iteration 113/4000 [0m

                       Computation: 3225 steps/s (collection: 0.460s, learning 2.080s)
               Value function loss: 38.5511
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 132.58
               Mean episode length: 238.49
                 Mean success rate: 0.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 2.54s
                        Total time: 291.63s
                               ETA: 9943.6s

################################################################################
                     [1m Learning iteration 114/4000 [0m

                       Computation: 3232 steps/s (collection: 0.475s, learning 2.059s)
               Value function loss: 34.7986
                    Surrogate loss: 0.0063
             Mean action noise std: 1.00
                       Mean reward: 133.23
               Mean episode length: 236.09
                 Mean success rate: 0.00
                  Mean reward/step: 0.60
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 942080
                    Iteration time: 2.53s
                        Total time: 294.16s
                               ETA: 9940.2s

################################################################################
                     [1m Learning iteration 115/4000 [0m

                       Computation: 3192 steps/s (collection: 0.459s, learning 2.107s)
               Value function loss: 41.0368
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 139.35
               Mean episode length: 243.09
                 Mean success rate: 0.00
                  Mean reward/step: 0.61
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 2.57s
                        Total time: 296.73s
                               ETA: 9937.9s

################################################################################
                     [1m Learning iteration 116/4000 [0m

                       Computation: 3199 steps/s (collection: 0.473s, learning 2.087s)
               Value function loss: 26.7107
                    Surrogate loss: 0.0071
             Mean action noise std: 1.00
                       Mean reward: 136.52
               Mean episode length: 237.87
                 Mean success rate: 0.00
                  Mean reward/step: 0.61
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 958464
                    Iteration time: 2.56s
                        Total time: 299.29s
                               ETA: 9935.4s

################################################################################
                     [1m Learning iteration 117/4000 [0m

                       Computation: 3227 steps/s (collection: 0.452s, learning 2.086s)
               Value function loss: 42.3745
                    Surrogate loss: 0.0061
             Mean action noise std: 1.00
                       Mean reward: 135.91
               Mean episode length: 233.29
                 Mean success rate: 0.00
                  Mean reward/step: 0.62
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 2.54s
                        Total time: 301.83s
                               ETA: 9932.2s

################################################################################
                     [1m Learning iteration 118/4000 [0m

                       Computation: 3201 steps/s (collection: 0.485s, learning 2.074s)
               Value function loss: 47.6728
                    Surrogate loss: 0.0081
             Mean action noise std: 1.00
                       Mean reward: 134.29
               Mean episode length: 233.00
                 Mean success rate: 0.00
                  Mean reward/step: 0.64
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 974848
                    Iteration time: 2.56s
                        Total time: 304.39s
                               ETA: 9929.7s

################################################################################
                     [1m Learning iteration 119/4000 [0m

                       Computation: 3206 steps/s (collection: 0.490s, learning 2.064s)
               Value function loss: 42.5769
                    Surrogate loss: 0.0069
             Mean action noise std: 1.00
                       Mean reward: 134.82
               Mean episode length: 232.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 2.55s
                        Total time: 306.94s
                               ETA: 9927.0s

################################################################################
                     [1m Learning iteration 120/4000 [0m

                       Computation: 3208 steps/s (collection: 0.486s, learning 2.067s)
               Value function loss: 50.4354
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 141.47
               Mean episode length: 242.85
                 Mean success rate: 0.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 991232
                    Iteration time: 2.55s
                        Total time: 309.50s
                               ETA: 9924.3s

################################################################################
                     [1m Learning iteration 121/4000 [0m

                       Computation: 3262 steps/s (collection: 0.453s, learning 2.058s)
               Value function loss: 63.1436
                    Surrogate loss: 0.0096
             Mean action noise std: 1.00
                       Mean reward: 145.06
               Mean episode length: 239.66
                 Mean success rate: 0.00
                  Mean reward/step: 0.68
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 2.51s
                        Total time: 312.01s
                               ETA: 9920.3s

################################################################################
                     [1m Learning iteration 122/4000 [0m

                       Computation: 3292 steps/s (collection: 0.452s, learning 2.036s)
               Value function loss: 56.5434
                    Surrogate loss: 0.0079
             Mean action noise std: 1.00
                       Mean reward: 147.86
               Mean episode length: 237.81
                 Mean success rate: 0.00
                  Mean reward/step: 0.69
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1007616
                    Iteration time: 2.49s
                        Total time: 314.49s
                               ETA: 9915.5s

################################################################################
                     [1m Learning iteration 123/4000 [0m

                       Computation: 3238 steps/s (collection: 0.454s, learning 2.076s)
               Value function loss: 74.4658
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 149.03
               Mean episode length: 236.84
                 Mean success rate: 0.00
                  Mean reward/step: 0.71
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 2.53s
                        Total time: 317.02s
                               ETA: 9912.1s

################################################################################
                     [1m Learning iteration 124/4000 [0m

                       Computation: 3215 steps/s (collection: 0.497s, learning 2.050s)
               Value function loss: 89.1640
                    Surrogate loss: 0.0086
             Mean action noise std: 1.00
                       Mean reward: 144.97
               Mean episode length: 223.06
                 Mean success rate: 0.00
                  Mean reward/step: 0.71
       Mean episode length/episode: 25.52
--------------------------------------------------------------------------------
                   Total timesteps: 1024000
                    Iteration time: 2.55s
                        Total time: 319.57s
                               ETA: 9909.3s

################################################################################
                     [1m Learning iteration 125/4000 [0m

                       Computation: 3222 steps/s (collection: 0.456s, learning 2.086s)
               Value function loss: 44.5617
                    Surrogate loss: 0.0072
             Mean action noise std: 1.00
                       Mean reward: 133.21
               Mean episode length: 201.63
                 Mean success rate: 0.00
                  Mean reward/step: 0.72
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 2.54s
                        Total time: 322.11s
                               ETA: 9906.3s

################################################################################
                     [1m Learning iteration 126/4000 [0m

                       Computation: 3228 steps/s (collection: 0.449s, learning 2.088s)
               Value function loss: 82.2475
                    Surrogate loss: 0.0086
             Mean action noise std: 1.00
                       Mean reward: 128.57
               Mean episode length: 187.05
                 Mean success rate: 0.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 25.13
--------------------------------------------------------------------------------
                   Total timesteps: 1040384
                    Iteration time: 2.54s
                        Total time: 324.65s
                               ETA: 9903.1s

################################################################################
                     [1m Learning iteration 127/4000 [0m

                       Computation: 3258 steps/s (collection: 0.466s, learning 2.048s)
               Value function loss: 68.4725
                    Surrogate loss: 0.0089
             Mean action noise std: 1.00
                       Mean reward: 118.76
               Mean episode length: 173.00
                 Mean success rate: 0.00
                  Mean reward/step: 0.71
       Mean episode length/episode: 24.98
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 2.51s
                        Total time: 327.16s
                               ETA: 9899.3s

################################################################################
                     [1m Learning iteration 128/4000 [0m

                       Computation: 3220 steps/s (collection: 0.476s, learning 2.068s)
               Value function loss: 45.0033
                    Surrogate loss: 0.0074
             Mean action noise std: 1.00
                       Mean reward: 111.90
               Mean episode length: 165.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1056768
                    Iteration time: 2.54s
                        Total time: 329.71s
                               ETA: 9896.4s

################################################################################
                     [1m Learning iteration 129/4000 [0m

                       Computation: 3275 steps/s (collection: 0.476s, learning 2.025s)
               Value function loss: 80.6657
                    Surrogate loss: 0.0078
             Mean action noise std: 1.00
                       Mean reward: 108.60
               Mean episode length: 159.87
                 Mean success rate: 0.00
                  Mean reward/step: 0.78
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 2.50s
                        Total time: 332.21s
                               ETA: 9892.2s

################################################################################
                     [1m Learning iteration 130/4000 [0m

                       Computation: 3198 steps/s (collection: 0.470s, learning 2.092s)
               Value function loss: 62.6777
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 114.68
               Mean episode length: 162.59
                 Mean success rate: 0.00
                  Mean reward/step: 0.76
       Mean episode length/episode: 25.21
--------------------------------------------------------------------------------
                   Total timesteps: 1073152
                    Iteration time: 2.56s
                        Total time: 334.77s
                               ETA: 9889.8s

################################################################################
                     [1m Learning iteration 131/4000 [0m

                       Computation: 3239 steps/s (collection: 0.441s, learning 2.088s)
               Value function loss: 48.7324
                    Surrogate loss: 0.0063
             Mean action noise std: 1.00
                       Mean reward: 105.05
               Mean episode length: 144.96
                 Mean success rate: 0.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 2.53s
                        Total time: 337.30s
                               ETA: 9886.5s

################################################################################
                     [1m Learning iteration 132/4000 [0m

                       Computation: 3266 steps/s (collection: 0.464s, learning 2.044s)
               Value function loss: 86.5847
                    Surrogate loss: 0.0074
             Mean action noise std: 1.00
                       Mean reward: 95.55
               Mean episode length: 130.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.77
       Mean episode length/episode: 25.44
--------------------------------------------------------------------------------
                   Total timesteps: 1089536
                    Iteration time: 2.51s
                        Total time: 339.81s
                               ETA: 9882.5s

################################################################################
                     [1m Learning iteration 133/4000 [0m

                       Computation: 3206 steps/s (collection: 0.491s, learning 2.063s)
               Value function loss: 61.1689
                    Surrogate loss: 0.0095
             Mean action noise std: 1.00
                       Mean reward: 85.66
               Mean episode length: 116.67
                 Mean success rate: 0.00
                  Mean reward/step: 0.74
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 2.55s
                        Total time: 342.36s
                               ETA: 9880.0s

################################################################################
                     [1m Learning iteration 134/4000 [0m

                       Computation: 3293 steps/s (collection: 0.449s, learning 2.038s)
               Value function loss: 55.0219
                    Surrogate loss: 0.0074
             Mean action noise std: 1.00
                       Mean reward: 92.77
               Mean episode length: 123.38
                 Mean success rate: 0.00
                  Mean reward/step: 0.74
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1105920
                    Iteration time: 2.49s
                        Total time: 344.85s
                               ETA: 9875.5s

################################################################################
                     [1m Learning iteration 135/4000 [0m

                       Computation: 3212 steps/s (collection: 0.480s, learning 2.069s)
               Value function loss: 50.9187
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 96.64
               Mean episode length: 128.16
                 Mean success rate: 0.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 2.55s
                        Total time: 347.40s
                               ETA: 9872.8s

################################################################################
                     [1m Learning iteration 136/4000 [0m

                       Computation: 3275 steps/s (collection: 0.455s, learning 2.046s)
               Value function loss: 69.2348
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 106.11
               Mean episode length: 139.31
                 Mean success rate: 0.00
                  Mean reward/step: 0.70
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1122304
                    Iteration time: 2.50s
                        Total time: 349.90s
                               ETA: 9868.8s

################################################################################
                     [1m Learning iteration 137/4000 [0m

                       Computation: 3238 steps/s (collection: 0.467s, learning 2.062s)
               Value function loss: 83.2344
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 111.50
               Mean episode length: 144.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.77
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 2.53s
                        Total time: 352.43s
                               ETA: 9865.5s

################################################################################
                     [1m Learning iteration 138/4000 [0m

                       Computation: 3228 steps/s (collection: 0.469s, learning 2.068s)
               Value function loss: 74.2863
                    Surrogate loss: 0.0061
             Mean action noise std: 1.00
                       Mean reward: 117.27
               Mean episode length: 153.81
                 Mean success rate: 0.00
                  Mean reward/step: 0.76
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 1138688
                    Iteration time: 2.54s
                        Total time: 354.97s
                               ETA: 9862.5s

################################################################################
                     [1m Learning iteration 139/4000 [0m

                       Computation: 3323 steps/s (collection: 0.430s, learning 2.034s)
               Value function loss: 51.2307
                    Surrogate loss: 0.0080
             Mean action noise std: 1.00
                       Mean reward: 121.84
               Mean episode length: 160.95
                 Mean success rate: 0.00
                  Mean reward/step: 0.77
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 2.46s
                        Total time: 357.43s
                               ETA: 9857.5s

################################################################################
                     [1m Learning iteration 140/4000 [0m

                       Computation: 3186 steps/s (collection: 0.482s, learning 2.089s)
               Value function loss: 72.9740
                    Surrogate loss: 0.0082
             Mean action noise std: 1.00
                       Mean reward: 126.49
               Mean episode length: 165.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1155072
                    Iteration time: 2.57s
                        Total time: 360.00s
                               ETA: 9855.4s

################################################################################
                     [1m Learning iteration 141/4000 [0m

                       Computation: 3194 steps/s (collection: 0.495s, learning 2.069s)
               Value function loss: 75.9194
                    Surrogate loss: 0.0087
             Mean action noise std: 1.00
                       Mean reward: 125.82
               Mean episode length: 168.96
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 2.56s
                        Total time: 362.57s
                               ETA: 9853.2s

################################################################################
                     [1m Learning iteration 142/4000 [0m

                       Computation: 3212 steps/s (collection: 0.449s, learning 2.101s)
               Value function loss: 84.5570
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 137.18
               Mean episode length: 186.16
                 Mean success rate: 0.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1171456
                    Iteration time: 2.55s
                        Total time: 365.12s
                               ETA: 9850.5s

################################################################################
                     [1m Learning iteration 143/4000 [0m

                       Computation: 3165 steps/s (collection: 0.481s, learning 2.107s)
               Value function loss: 89.5909
                    Surrogate loss: 0.0076
             Mean action noise std: 1.00
                       Mean reward: 150.86
               Mean episode length: 203.01
                 Mean success rate: 0.00
                  Mean reward/step: 0.81
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 2.59s
                        Total time: 367.71s
                               ETA: 9848.9s

################################################################################
                     [1m Learning iteration 144/4000 [0m

                       Computation: 3203 steps/s (collection: 0.483s, learning 2.074s)
               Value function loss: 83.6582
                    Surrogate loss: 0.0083
             Mean action noise std: 1.00
                       Mean reward: 141.25
               Mean episode length: 191.85
                 Mean success rate: 0.00
                  Mean reward/step: 0.83
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1187840
                    Iteration time: 2.56s
                        Total time: 370.26s
                               ETA: 9846.5s

################################################################################
                     [1m Learning iteration 145/4000 [0m

                       Computation: 3137 steps/s (collection: 0.514s, learning 2.097s)
               Value function loss: 102.2159
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 147.55
               Mean episode length: 198.64
                 Mean success rate: 0.00
                  Mean reward/step: 0.87
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 2.61s
                        Total time: 372.88s
                               ETA: 9845.4s

################################################################################
                     [1m Learning iteration 146/4000 [0m

                       Computation: 3172 steps/s (collection: 0.476s, learning 2.106s)
               Value function loss: 125.0849
                    Surrogate loss: 0.0090
             Mean action noise std: 1.00
                       Mean reward: 147.10
               Mean episode length: 194.68
                 Mean success rate: 0.00
                  Mean reward/step: 0.87
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 1204224
                    Iteration time: 2.58s
                        Total time: 375.46s
                               ETA: 9843.6s

################################################################################
                     [1m Learning iteration 147/4000 [0m

                       Computation: 3239 steps/s (collection: 0.445s, learning 2.083s)
               Value function loss: 78.8622
                    Surrogate loss: 0.0076
             Mean action noise std: 1.00
                       Mean reward: 133.77
               Mean episode length: 174.78
                 Mean success rate: 0.00
                  Mean reward/step: 0.82
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 2.53s
                        Total time: 377.99s
                               ETA: 9840.4s

################################################################################
                     [1m Learning iteration 148/4000 [0m

                       Computation: 3211 steps/s (collection: 0.480s, learning 2.071s)
               Value function loss: 88.6118
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 141.32
               Mean episode length: 185.29
                 Mean success rate: 0.00
                  Mean reward/step: 0.84
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1220608
                    Iteration time: 2.55s
                        Total time: 380.54s
                               ETA: 9837.8s

################################################################################
                     [1m Learning iteration 149/4000 [0m

                       Computation: 3128 steps/s (collection: 0.444s, learning 2.174s)
               Value function loss: 96.8380
                    Surrogate loss: 0.0064
             Mean action noise std: 1.00
                       Mean reward: 139.38
               Mean episode length: 176.81
                 Mean success rate: 0.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 2.62s
                        Total time: 383.16s
                               ETA: 9836.9s

################################################################################
                     [1m Learning iteration 150/4000 [0m

                       Computation: 3274 steps/s (collection: 0.434s, learning 2.067s)
               Value function loss: 83.1551
                    Surrogate loss: 0.0094
             Mean action noise std: 1.00
                       Mean reward: 133.36
               Mean episode length: 166.54
                 Mean success rate: 0.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1236992
                    Iteration time: 2.50s
                        Total time: 385.66s
                               ETA: 9833.0s

################################################################################
                     [1m Learning iteration 151/4000 [0m

                       Computation: 3179 steps/s (collection: 0.481s, learning 2.096s)
               Value function loss: 77.3634
                    Surrogate loss: 0.0090
             Mean action noise std: 1.00
                       Mean reward: 137.65
               Mean episode length: 170.04
                 Mean success rate: 0.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 2.58s
                        Total time: 388.23s
                               ETA: 9831.0s

################################################################################
                     [1m Learning iteration 152/4000 [0m

                       Computation: 3165 steps/s (collection: 0.479s, learning 2.109s)
               Value function loss: 110.4915
                    Surrogate loss: 0.0107
             Mean action noise std: 1.00
                       Mean reward: 142.56
               Mean episode length: 173.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 1253376
                    Iteration time: 2.59s
                        Total time: 390.82s
                               ETA: 9829.3s

################################################################################
                     [1m Learning iteration 153/4000 [0m

                       Computation: 3126 steps/s (collection: 0.526s, learning 2.094s)
               Value function loss: 95.6215
                    Surrogate loss: 0.0091
             Mean action noise std: 1.00
                       Mean reward: 138.81
               Mean episode length: 168.44
                 Mean success rate: 0.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 2.62s
                        Total time: 393.44s
                               ETA: 9828.4s

################################################################################
                     [1m Learning iteration 154/4000 [0m

                       Computation: 3156 steps/s (collection: 0.496s, learning 2.099s)
               Value function loss: 103.7097
                    Surrogate loss: 0.0072
             Mean action noise std: 1.00
                       Mean reward: 146.40
               Mean episode length: 176.99
                 Mean success rate: 0.00
                  Mean reward/step: 0.87
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 1269760
                    Iteration time: 2.60s
                        Total time: 396.04s
                               ETA: 9826.8s

################################################################################
                     [1m Learning iteration 155/4000 [0m

                       Computation: 3135 steps/s (collection: 0.488s, learning 2.125s)
               Value function loss: 100.4470
                    Surrogate loss: 0.0088
             Mean action noise std: 1.00
                       Mean reward: 145.24
               Mean episode length: 172.22
                 Mean success rate: 0.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 2.61s
                        Total time: 398.65s
                               ETA: 9825.7s

################################################################################
                     [1m Learning iteration 156/4000 [0m

                       Computation: 3070 steps/s (collection: 0.528s, learning 2.140s)
               Value function loss: 117.5515
                    Surrogate loss: 0.0087
             Mean action noise std: 1.00
                       Mean reward: 150.09
               Mean episode length: 176.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1286144
                    Iteration time: 2.67s
                        Total time: 401.32s
                               ETA: 9825.9s

################################################################################
                     [1m Learning iteration 157/4000 [0m

                       Computation: 3058 steps/s (collection: 0.535s, learning 2.143s)
               Value function loss: 106.9417
                    Surrogate loss: 0.0081
             Mean action noise std: 1.00
                       Mean reward: 163.57
               Mean episode length: 188.76
                 Mean success rate: 0.00
                  Mean reward/step: 0.89
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 2.68s
                        Total time: 404.00s
                               ETA: 9826.3s

################################################################################
                     [1m Learning iteration 158/4000 [0m

                       Computation: 3192 steps/s (collection: 0.470s, learning 2.096s)
               Value function loss: 86.5738
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 175.17
               Mean episode length: 199.22
                 Mean success rate: 0.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1302528
                    Iteration time: 2.57s
                        Total time: 406.56s
                               ETA: 9824.0s

################################################################################
                     [1m Learning iteration 159/4000 [0m

                       Computation: 3090 steps/s (collection: 0.496s, learning 2.155s)
               Value function loss: 94.4800
                    Surrogate loss: 0.0097
             Mean action noise std: 1.00
                       Mean reward: 180.61
               Mean episode length: 204.85
                 Mean success rate: 0.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 2.65s
                        Total time: 409.21s
                               ETA: 9823.6s

################################################################################
                     [1m Learning iteration 160/4000 [0m

                       Computation: 2969 steps/s (collection: 0.540s, learning 2.218s)
               Value function loss: 101.9745
                    Surrogate loss: 0.0086
             Mean action noise std: 1.00
                       Mean reward: 180.84
               Mean episode length: 204.71
                 Mean success rate: 0.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1318912
                    Iteration time: 2.76s
                        Total time: 411.97s
                               ETA: 9825.9s

################################################################################
                     [1m Learning iteration 161/4000 [0m

                       Computation: 3047 steps/s (collection: 0.548s, learning 2.140s)
               Value function loss: 137.9992
                    Surrogate loss: 0.0094
             Mean action noise std: 1.00
                       Mean reward: 187.80
               Mean episode length: 211.35
                 Mean success rate: 0.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 2.69s
                        Total time: 414.66s
                               ETA: 9826.4s

################################################################################
                     [1m Learning iteration 162/4000 [0m

                       Computation: 3093 steps/s (collection: 0.483s, learning 2.165s)
               Value function loss: 129.7058
                    Surrogate loss: 0.0093
             Mean action noise std: 1.00
                       Mean reward: 194.63
               Mean episode length: 218.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1335296
                    Iteration time: 2.65s
                        Total time: 417.31s
                               ETA: 9825.9s

################################################################################
                     [1m Learning iteration 163/4000 [0m

                       Computation: 3159 steps/s (collection: 0.489s, learning 2.104s)
               Value function loss: 98.3090
                    Surrogate loss: 0.0090
             Mean action noise std: 1.00
                       Mean reward: 184.29
               Mean episode length: 208.47
                 Mean success rate: 0.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 2.59s
                        Total time: 419.90s
                               ETA: 9824.1s

################################################################################
                     [1m Learning iteration 164/4000 [0m

                       Computation: 3197 steps/s (collection: 0.486s, learning 2.076s)
               Value function loss: 141.8225
                    Surrogate loss: 0.0090
             Mean action noise std: 1.00
                       Mean reward: 179.68
               Mean episode length: 198.74
                 Mean success rate: 0.00
                  Mean reward/step: 0.87
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 1351680
                    Iteration time: 2.56s
                        Total time: 422.46s
                               ETA: 9821.6s

################################################################################
                     [1m Learning iteration 165/4000 [0m

                       Computation: 3163 steps/s (collection: 0.507s, learning 2.082s)
               Value function loss: 116.7080
                    Surrogate loss: 0.0086
             Mean action noise std: 1.00
                       Mean reward: 177.34
               Mean episode length: 196.54
                 Mean success rate: 0.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 2.59s
                        Total time: 425.05s
                               ETA: 9819.7s

################################################################################
                     [1m Learning iteration 166/4000 [0m

                       Computation: 3121 steps/s (collection: 0.494s, learning 2.130s)
               Value function loss: 117.6921
                    Surrogate loss: 0.0103
             Mean action noise std: 1.00
                       Mean reward: 181.72
               Mean episode length: 199.12
                 Mean success rate: 0.00
                  Mean reward/step: 0.96
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1368064
                    Iteration time: 2.62s
                        Total time: 427.68s
                               ETA: 9818.6s

################################################################################
                     [1m Learning iteration 167/4000 [0m

                       Computation: 3226 steps/s (collection: 0.480s, learning 2.059s)
               Value function loss: 103.2204
                    Surrogate loss: 0.0087
             Mean action noise std: 1.00
                       Mean reward: 188.87
               Mean episode length: 204.76
                 Mean success rate: 0.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 2.54s
                        Total time: 430.21s
                               ETA: 9815.5s

################################################################################
                     [1m Learning iteration 168/4000 [0m

                       Computation: 3218 steps/s (collection: 0.447s, learning 2.098s)
               Value function loss: 134.6427
                    Surrogate loss: 0.0092
             Mean action noise std: 1.00
                       Mean reward: 206.49
               Mean episode length: 223.30
                 Mean success rate: 0.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1384448
                    Iteration time: 2.55s
                        Total time: 432.76s
                               ETA: 9812.6s

################################################################################
                     [1m Learning iteration 169/4000 [0m

                       Computation: 3215 steps/s (collection: 0.493s, learning 2.055s)
               Value function loss: 84.2138
                    Surrogate loss: 0.0105
             Mean action noise std: 1.00
                       Mean reward: 210.11
               Mean episode length: 225.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 2.55s
                        Total time: 435.31s
                               ETA: 9809.8s

################################################################################
                     [1m Learning iteration 170/4000 [0m

                       Computation: 3226 steps/s (collection: 0.479s, learning 2.060s)
               Value function loss: 83.7027
                    Surrogate loss: 0.0096
             Mean action noise std: 1.00
                       Mean reward: 215.83
               Mean episode length: 232.09
                 Mean success rate: 0.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1400832
                    Iteration time: 2.54s
                        Total time: 437.85s
                               ETA: 9806.7s

################################################################################
                     [1m Learning iteration 171/4000 [0m

                       Computation: 3235 steps/s (collection: 0.456s, learning 2.076s)
               Value function loss: 105.0537
                    Surrogate loss: 0.0095
             Mean action noise std: 1.00
                       Mean reward: 225.49
               Mean episode length: 243.67
                 Mean success rate: 0.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 2.53s
                        Total time: 440.38s
                               ETA: 9803.5s

################################################################################
                     [1m Learning iteration 172/4000 [0m

                       Computation: 3245 steps/s (collection: 0.472s, learning 2.052s)
               Value function loss: 79.5936
                    Surrogate loss: 0.0110
             Mean action noise std: 1.00
                       Mean reward: 220.85
               Mean episode length: 239.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1417216
                    Iteration time: 2.52s
                        Total time: 442.90s
                               ETA: 9800.2s

################################################################################
                     [1m Learning iteration 173/4000 [0m

                       Computation: 3221 steps/s (collection: 0.491s, learning 2.052s)
               Value function loss: 127.2679
                    Surrogate loss: 0.0111
             Mean action noise std: 1.00
                       Mean reward: 210.21
               Mean episode length: 231.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 2.54s
                        Total time: 445.45s
                               ETA: 9797.2s

################################################################################
                     [1m Learning iteration 174/4000 [0m

                       Computation: 3208 steps/s (collection: 0.489s, learning 2.065s)
               Value function loss: 101.9490
                    Surrogate loss: 0.0100
             Mean action noise std: 1.00
                       Mean reward: 206.57
               Mean episode length: 224.53
                 Mean success rate: 0.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1433600
                    Iteration time: 2.55s
                        Total time: 448.00s
                               ETA: 9794.5s

################################################################################
                     [1m Learning iteration 175/4000 [0m

                       Computation: 3134 steps/s (collection: 0.485s, learning 2.129s)
               Value function loss: 111.6536
                    Surrogate loss: 0.0106
             Mean action noise std: 1.00
                       Mean reward: 203.61
               Mean episode length: 221.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 2.61s
                        Total time: 450.61s
                               ETA: 9793.2s

################################################################################
                     [1m Learning iteration 176/4000 [0m

                       Computation: 3024 steps/s (collection: 0.522s, learning 2.187s)
               Value function loss: 125.7333
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 201.01
               Mean episode length: 219.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1449984
                    Iteration time: 2.71s
                        Total time: 453.32s
                               ETA: 9793.8s

################################################################################
                     [1m Learning iteration 177/4000 [0m

                       Computation: 3096 steps/s (collection: 0.538s, learning 2.108s)
               Value function loss: 127.0730
                    Surrogate loss: 0.0101
             Mean action noise std: 1.00
                       Mean reward: 199.22
               Mean episode length: 213.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.98
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 2.65s
                        Total time: 455.97s
                               ETA: 9793.0s

################################################################################
                     [1m Learning iteration 178/4000 [0m

                       Computation: 3119 steps/s (collection: 0.494s, learning 2.132s)
               Value function loss: 158.1019
                    Surrogate loss: 0.0104
             Mean action noise std: 1.00
                       Mean reward: 205.72
               Mean episode length: 219.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1466368
                    Iteration time: 2.63s
                        Total time: 458.59s
                               ETA: 9791.9s

################################################################################
                     [1m Learning iteration 179/4000 [0m

                       Computation: 3182 steps/s (collection: 0.479s, learning 2.095s)
               Value function loss: 146.4845
                    Surrogate loss: 0.0091
             Mean action noise std: 1.00
                       Mean reward: 219.07
               Mean episode length: 229.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 2.57s
                        Total time: 461.17s
                               ETA: 9789.6s

################################################################################
                     [1m Learning iteration 180/4000 [0m

                       Computation: 3158 steps/s (collection: 0.478s, learning 2.116s)
               Value function loss: 154.6074
                    Surrogate loss: 0.0081
             Mean action noise std: 1.00
                       Mean reward: 224.18
               Mean episode length: 232.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1482752
                    Iteration time: 2.59s
                        Total time: 463.76s
                               ETA: 9787.7s

################################################################################
                     [1m Learning iteration 181/4000 [0m

                       Computation: 3185 steps/s (collection: 0.494s, learning 2.078s)
               Value function loss: 118.7467
                    Surrogate loss: 0.0085
             Mean action noise std: 1.00
                       Mean reward: 228.01
               Mean episode length: 235.51
                 Mean success rate: 0.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 2.57s
                        Total time: 466.33s
                               ETA: 9785.3s

################################################################################
                     [1m Learning iteration 182/4000 [0m

                       Computation: 3086 steps/s (collection: 0.519s, learning 2.135s)
               Value function loss: 164.2603
                    Surrogate loss: 0.0108
             Mean action noise std: 1.00
                       Mean reward: 245.69
               Mean episode length: 250.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1499136
                    Iteration time: 2.65s
                        Total time: 468.99s
                               ETA: 9784.7s

################################################################################
                     [1m Learning iteration 183/4000 [0m

                       Computation: 3123 steps/s (collection: 0.499s, learning 2.123s)
               Value function loss: 97.9334
                    Surrogate loss: 0.0100
             Mean action noise std: 1.00
                       Mean reward: 250.74
               Mean episode length: 250.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 2.62s
                        Total time: 471.61s
                               ETA: 9783.3s

################################################################################
                     [1m Learning iteration 184/4000 [0m

                       Computation: 3061 steps/s (collection: 0.568s, learning 2.108s)
               Value function loss: 147.0822
                    Surrogate loss: 0.0098
             Mean action noise std: 1.00
                       Mean reward: 263.00
               Mean episode length: 256.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1515520
                    Iteration time: 2.68s
                        Total time: 474.29s
                               ETA: 9783.1s

################################################################################
                     [1m Learning iteration 185/4000 [0m

                       Computation: 3162 steps/s (collection: 0.509s, learning 2.082s)
               Value function loss: 111.8393
                    Surrogate loss: 0.0098
             Mean action noise std: 1.00
                       Mean reward: 266.21
               Mean episode length: 256.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 2.59s
                        Total time: 476.88s
                               ETA: 9781.1s

################################################################################
                     [1m Learning iteration 186/4000 [0m

                       Computation: 3082 steps/s (collection: 0.523s, learning 2.134s)
               Value function loss: 94.9042
                    Surrogate loss: 0.0091
             Mean action noise std: 1.00
                       Mean reward: 262.37
               Mean episode length: 249.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1531904
                    Iteration time: 2.66s
                        Total time: 479.53s
                               ETA: 9780.4s

################################################################################
                     [1m Learning iteration 187/4000 [0m

                       Computation: 3050 steps/s (collection: 0.552s, learning 2.133s)
               Value function loss: 149.6863
                    Surrogate loss: 0.0090
             Mean action noise std: 1.00
                       Mean reward: 255.03
               Mean episode length: 242.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 2.69s
                        Total time: 482.22s
                               ETA: 9780.3s

################################################################################
                     [1m Learning iteration 188/4000 [0m

                       Computation: 3157 steps/s (collection: 0.489s, learning 2.105s)
               Value function loss: 151.5809
                    Surrogate loss: 0.0104
             Mean action noise std: 1.00
                       Mean reward: 238.43
               Mean episode length: 225.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1548288
                    Iteration time: 2.59s
                        Total time: 484.81s
                               ETA: 9778.3s

################################################################################
                     [1m Learning iteration 189/4000 [0m

                       Computation: 3131 steps/s (collection: 0.514s, learning 2.102s)
               Value function loss: 172.6672
                    Surrogate loss: 0.0102
             Mean action noise std: 1.00
                       Mean reward: 240.69
               Mean episode length: 228.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 2.62s
                        Total time: 487.43s
                               ETA: 9776.8s

################################################################################
                     [1m Learning iteration 190/4000 [0m

                       Computation: 2983 steps/s (collection: 0.553s, learning 2.193s)
               Value function loss: 134.0828
                    Surrogate loss: 0.0106
             Mean action noise std: 1.00
                       Mean reward: 235.63
               Mean episode length: 225.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1564672
                    Iteration time: 2.75s
                        Total time: 490.17s
                               ETA: 9777.8s

################################################################################
                     [1m Learning iteration 191/4000 [0m

                       Computation: 3100 steps/s (collection: 0.505s, learning 2.137s)
               Value function loss: 116.4166
                    Surrogate loss: 0.0091
             Mean action noise std: 1.00
                       Mean reward: 234.68
               Mean episode length: 224.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 2.64s
                        Total time: 492.82s
                               ETA: 9776.8s

################################################################################
                     [1m Learning iteration 192/4000 [0m

                       Computation: 3088 steps/s (collection: 0.509s, learning 2.143s)
               Value function loss: 130.4872
                    Surrogate loss: 0.0099
             Mean action noise std: 1.00
                       Mean reward: 245.73
               Mean episode length: 232.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1581056
                    Iteration time: 2.65s
                        Total time: 495.47s
                               ETA: 9775.9s

################################################################################
                     [1m Learning iteration 193/4000 [0m

                       Computation: 3109 steps/s (collection: 0.509s, learning 2.126s)
               Value function loss: 110.5072
                    Surrogate loss: 0.0101
             Mean action noise std: 1.00
                       Mean reward: 252.78
               Mean episode length: 238.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 2.63s
                        Total time: 498.10s
                               ETA: 9774.6s

################################################################################
                     [1m Learning iteration 194/4000 [0m

                       Computation: 3139 steps/s (collection: 0.503s, learning 2.106s)
               Value function loss: 151.6195
                    Surrogate loss: 0.0112
             Mean action noise std: 1.00
                       Mean reward: 256.25
               Mean episode length: 240.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1597440
                    Iteration time: 2.61s
                        Total time: 500.71s
                               ETA: 9772.9s

################################################################################
                     [1m Learning iteration 195/4000 [0m

                       Computation: 3139 steps/s (collection: 0.488s, learning 2.121s)
               Value function loss: 160.0600
                    Surrogate loss: 0.0110
             Mean action noise std: 1.00
                       Mean reward: 261.61
               Mean episode length: 244.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 2.61s
                        Total time: 503.32s
                               ETA: 9771.1s

################################################################################
                     [1m Learning iteration 196/4000 [0m

                       Computation: 3113 steps/s (collection: 0.477s, learning 2.154s)
               Value function loss: 163.9488
                    Surrogate loss: 0.0105
             Mean action noise std: 1.00
                       Mean reward: 257.86
               Mean episode length: 239.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1613824
                    Iteration time: 2.63s
                        Total time: 505.95s
                               ETA: 9769.8s

################################################################################
                     [1m Learning iteration 197/4000 [0m

                       Computation: 3076 steps/s (collection: 0.542s, learning 2.120s)
               Value function loss: 154.7246
                    Surrogate loss: 0.0105
             Mean action noise std: 1.00
                       Mean reward: 260.82
               Mean episode length: 238.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 2.66s
                        Total time: 508.61s
                               ETA: 9769.0s

################################################################################
                     [1m Learning iteration 198/4000 [0m

                       Computation: 3197 steps/s (collection: 0.486s, learning 2.076s)
               Value function loss: 139.6342
                    Surrogate loss: 0.0094
             Mean action noise std: 1.00
                       Mean reward: 258.35
               Mean episode length: 233.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1630208
                    Iteration time: 2.56s
                        Total time: 511.18s
                               ETA: 9766.3s

################################################################################
                     [1m Learning iteration 199/4000 [0m

                       Computation: 3080 steps/s (collection: 0.516s, learning 2.143s)
               Value function loss: 186.3533
                    Surrogate loss: 0.0071
             Mean action noise std: 1.00
                       Mean reward: 238.57
               Mean episode length: 212.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 2.66s
                        Total time: 513.84s
                               ETA: 9765.5s

################################################################################
                     [1m Learning iteration 200/4000 [0m

                       Computation: 3190 steps/s (collection: 0.481s, learning 2.086s)
               Value function loss: 144.4941
                    Surrogate loss: 0.0113
             Mean action noise std: 1.00
                       Mean reward: 233.57
               Mean episode length: 206.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1646592
                    Iteration time: 2.57s
                        Total time: 516.40s
                               ETA: 9762.8s

################################################################################
                     [1m Learning iteration 201/4000 [0m

                       Computation: 3155 steps/s (collection: 0.480s, learning 2.116s)
               Value function loss: 96.0841
                    Surrogate loss: 0.0099
             Mean action noise std: 1.00
                       Mean reward: 231.05
               Mean episode length: 203.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 2.60s
                        Total time: 519.00s
                               ETA: 9760.8s

################################################################################
                     [1m Learning iteration 202/4000 [0m

                       Computation: 3179 steps/s (collection: 0.478s, learning 2.099s)
               Value function loss: 139.1151
                    Surrogate loss: 0.0117
             Mean action noise std: 1.00
                       Mean reward: 238.28
               Mean episode length: 210.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1662976
                    Iteration time: 2.58s
                        Total time: 521.58s
                               ETA: 9758.3s

################################################################################
                     [1m Learning iteration 203/4000 [0m

                       Computation: 3137 steps/s (collection: 0.482s, learning 2.129s)
               Value function loss: 99.1356
                    Surrogate loss: 0.0106
             Mean action noise std: 1.00
                       Mean reward: 228.83
               Mean episode length: 202.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 2.61s
                        Total time: 524.19s
                               ETA: 9756.6s

################################################################################
                     [1m Learning iteration 204/4000 [0m

                       Computation: 3226 steps/s (collection: 0.451s, learning 2.088s)
               Value function loss: 141.6112
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 242.25
               Mean episode length: 213.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1679360
                    Iteration time: 2.54s
                        Total time: 526.73s
                               ETA: 9753.4s

################################################################################
                     [1m Learning iteration 205/4000 [0m

                       Computation: 3162 steps/s (collection: 0.475s, learning 2.115s)
               Value function loss: 161.9034
                    Surrogate loss: 0.0130
             Mean action noise std: 0.99
                       Mean reward: 254.83
               Mean episode length: 226.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 2.59s
                        Total time: 529.32s
                               ETA: 9751.2s

################################################################################
                     [1m Learning iteration 206/4000 [0m

                       Computation: 3099 steps/s (collection: 0.471s, learning 2.172s)
               Value function loss: 124.5393
                    Surrogate loss: 0.0109
             Mean action noise std: 0.99
                       Mean reward: 257.90
               Mean episode length: 226.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1695744
                    Iteration time: 2.64s
                        Total time: 531.96s
                               ETA: 9750.0s

################################################################################
                     [1m Learning iteration 207/4000 [0m

                       Computation: 3138 steps/s (collection: 0.514s, learning 2.096s)
               Value function loss: 209.1845
                    Surrogate loss: 0.0120
             Mean action noise std: 0.99
                       Mean reward: 260.30
               Mean episode length: 227.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 2.61s
                        Total time: 534.57s
                               ETA: 9748.2s

################################################################################
                     [1m Learning iteration 208/4000 [0m

                       Computation: 3174 steps/s (collection: 0.482s, learning 2.099s)
               Value function loss: 106.7144
                    Surrogate loss: 0.0123
             Mean action noise std: 1.00
                       Mean reward: 254.97
               Mean episode length: 221.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1712128
                    Iteration time: 2.58s
                        Total time: 537.15s
                               ETA: 9745.8s

################################################################################
                     [1m Learning iteration 209/4000 [0m

                       Computation: 3148 steps/s (collection: 0.474s, learning 2.127s)
               Value function loss: 108.7396
                    Surrogate loss: 0.0149
             Mean action noise std: 1.00
                       Mean reward: 244.51
               Mean episode length: 210.32
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 2.60s
                        Total time: 539.75s
                               ETA: 9743.8s

################################################################################
                     [1m Learning iteration 210/4000 [0m

                       Computation: 3158 steps/s (collection: 0.490s, learning 2.104s)
               Value function loss: 207.4792
                    Surrogate loss: 0.0127
             Mean action noise std: 1.00
                       Mean reward: 246.76
               Mean episode length: 209.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 1728512
                    Iteration time: 2.59s
                        Total time: 542.34s
                               ETA: 9741.6s

################################################################################
                     [1m Learning iteration 211/4000 [0m

                       Computation: 3164 steps/s (collection: 0.521s, learning 2.067s)
               Value function loss: 231.5603
                    Surrogate loss: 0.0103
             Mean action noise std: 1.00
                       Mean reward: 235.34
               Mean episode length: 200.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 2.59s
                        Total time: 544.93s
                               ETA: 9739.4s

################################################################################
                     [1m Learning iteration 212/4000 [0m

                       Computation: 3223 steps/s (collection: 0.464s, learning 2.078s)
               Value function loss: 191.8161
                    Surrogate loss: 0.0126
             Mean action noise std: 1.00
                       Mean reward: 252.69
               Mean episode length: 212.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1744896
                    Iteration time: 2.54s
                        Total time: 547.47s
                               ETA: 9736.3s

################################################################################
                     [1m Learning iteration 213/4000 [0m

                       Computation: 3168 steps/s (collection: 0.497s, learning 2.089s)
               Value function loss: 194.3133
                    Surrogate loss: 0.0100
             Mean action noise std: 1.00
                       Mean reward: 259.95
               Mean episode length: 217.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 2.59s
                        Total time: 550.06s
                               ETA: 9734.0s

################################################################################
                     [1m Learning iteration 214/4000 [0m

                       Computation: 3092 steps/s (collection: 0.539s, learning 2.110s)
               Value function loss: 137.2428
                    Surrogate loss: 0.0119
             Mean action noise std: 1.00
                       Mean reward: 268.87
               Mean episode length: 225.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1761280
                    Iteration time: 2.65s
                        Total time: 552.71s
                               ETA: 9732.8s

################################################################################
                     [1m Learning iteration 215/4000 [0m

                       Computation: 3108 steps/s (collection: 0.542s, learning 2.093s)
               Value function loss: 152.7671
                    Surrogate loss: 0.0122
             Mean action noise std: 1.00
                       Mean reward: 267.98
               Mean episode length: 223.87
                 Mean success rate: 0.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 2.63s
                        Total time: 555.34s
                               ETA: 9731.4s

################################################################################
                     [1m Learning iteration 216/4000 [0m

                       Computation: 3149 steps/s (collection: 0.498s, learning 2.103s)
               Value function loss: 152.7341
                    Surrogate loss: 0.0125
             Mean action noise std: 1.00
                       Mean reward: 272.26
               Mean episode length: 227.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1777664
                    Iteration time: 2.60s
                        Total time: 557.95s
                               ETA: 9729.3s

################################################################################
                     [1m Learning iteration 217/4000 [0m

                       Computation: 3106 steps/s (collection: 0.502s, learning 2.135s)
               Value function loss: 126.2953
                    Surrogate loss: 0.0120
             Mean action noise std: 1.00
                       Mean reward: 263.64
               Mean episode length: 221.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 2.64s
                        Total time: 560.58s
                               ETA: 9727.9s

################################################################################
                     [1m Learning iteration 218/4000 [0m

                       Computation: 3144 steps/s (collection: 0.514s, learning 2.092s)
               Value function loss: 155.3682
                    Surrogate loss: 0.0117
             Mean action noise std: 1.00
                       Mean reward: 261.96
               Mean episode length: 218.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1794048
                    Iteration time: 2.61s
                        Total time: 563.19s
                               ETA: 9725.9s

################################################################################
                     [1m Learning iteration 219/4000 [0m

                       Computation: 3188 steps/s (collection: 0.478s, learning 2.091s)
               Value function loss: 101.8096
                    Surrogate loss: 0.0115
             Mean action noise std: 1.00
                       Mean reward: 266.79
               Mean episode length: 224.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 2.57s
                        Total time: 565.76s
                               ETA: 9723.3s

################################################################################
                     [1m Learning iteration 220/4000 [0m

                       Computation: 3117 steps/s (collection: 0.527s, learning 2.101s)
               Value function loss: 161.7858
                    Surrogate loss: 0.0113
             Mean action noise std: 1.00
                       Mean reward: 263.81
               Mean episode length: 223.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1810432
                    Iteration time: 2.63s
                        Total time: 568.39s
                               ETA: 9721.7s

################################################################################
                     [1m Learning iteration 221/4000 [0m

                       Computation: 3191 steps/s (collection: 0.494s, learning 2.073s)
               Value function loss: 129.2847
                    Surrogate loss: 0.0133
             Mean action noise std: 1.00
                       Mean reward: 267.35
               Mean episode length: 225.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 2.57s
                        Total time: 570.95s
                               ETA: 9719.0s

################################################################################
                     [1m Learning iteration 222/4000 [0m

                       Computation: 3190 steps/s (collection: 0.492s, learning 2.075s)
               Value function loss: 132.6386
                    Surrogate loss: 0.0125
             Mean action noise std: 1.00
                       Mean reward: 275.28
               Mean episode length: 233.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1826816
                    Iteration time: 2.57s
                        Total time: 573.52s
                               ETA: 9716.4s

################################################################################
                     [1m Learning iteration 223/4000 [0m

                       Computation: 3156 steps/s (collection: 0.538s, learning 2.057s)
               Value function loss: 161.2509
                    Surrogate loss: 0.0133
             Mean action noise std: 1.00
                       Mean reward: 296.32
               Mean episode length: 248.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 2.60s
                        Total time: 576.11s
                               ETA: 9714.2s

################################################################################
                     [1m Learning iteration 224/4000 [0m

                       Computation: 3130 steps/s (collection: 0.507s, learning 2.110s)
               Value function loss: 140.7678
                    Surrogate loss: 0.0135
             Mean action noise std: 1.00
                       Mean reward: 321.85
               Mean episode length: 267.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1843200
                    Iteration time: 2.62s
                        Total time: 578.73s
                               ETA: 9712.4s

################################################################################
                     [1m Learning iteration 225/4000 [0m

                       Computation: 3214 steps/s (collection: 0.483s, learning 2.065s)
               Value function loss: 154.7917
                    Surrogate loss: 0.0109
             Mean action noise std: 0.99
                       Mean reward: 338.54
               Mean episode length: 281.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 2.55s
                        Total time: 581.28s
                               ETA: 9709.4s

################################################################################
                     [1m Learning iteration 226/4000 [0m

                       Computation: 3158 steps/s (collection: 0.515s, learning 2.079s)
               Value function loss: 202.3345
                    Surrogate loss: 0.0107
             Mean action noise std: 0.99
                       Mean reward: 352.89
               Mean episode length: 293.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1859584
                    Iteration time: 2.59s
                        Total time: 583.87s
                               ETA: 9707.2s

################################################################################
                     [1m Learning iteration 227/4000 [0m

                       Computation: 3195 steps/s (collection: 0.476s, learning 2.088s)
               Value function loss: 144.1165
                    Surrogate loss: 0.0130
             Mean action noise std: 0.99
                       Mean reward: 360.55
               Mean episode length: 299.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 2.56s
                        Total time: 586.44s
                               ETA: 9704.5s

################################################################################
                     [1m Learning iteration 228/4000 [0m

                       Computation: 3141 steps/s (collection: 0.484s, learning 2.124s)
               Value function loss: 195.9928
                    Surrogate loss: 0.0148
             Mean action noise std: 0.99
                       Mean reward: 349.10
               Mean episode length: 292.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1875968
                    Iteration time: 2.61s
                        Total time: 589.05s
                               ETA: 9702.5s

################################################################################
                     [1m Learning iteration 229/4000 [0m

                       Computation: 3181 steps/s (collection: 0.481s, learning 2.093s)
               Value function loss: 200.5659
                    Surrogate loss: 0.0111
             Mean action noise std: 0.99
                       Mean reward: 338.18
               Mean episode length: 283.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 2.57s
                        Total time: 591.62s
                               ETA: 9700.0s

################################################################################
                     [1m Learning iteration 230/4000 [0m

                       Computation: 3174 steps/s (collection: 0.485s, learning 2.095s)
               Value function loss: 138.5102
                    Surrogate loss: 0.0162
             Mean action noise std: 0.99
                       Mean reward: 326.71
               Mean episode length: 274.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1892352
                    Iteration time: 2.58s
                        Total time: 594.20s
                               ETA: 9697.6s

################################################################################
                     [1m Learning iteration 231/4000 [0m

                       Computation: 3249 steps/s (collection: 0.468s, learning 2.054s)
               Value function loss: 155.5884
                    Surrogate loss: 0.0093
             Mean action noise std: 0.99
                       Mean reward: 334.37
               Mean episode length: 278.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 2.52s
                        Total time: 596.72s
                               ETA: 9694.2s

################################################################################
                     [1m Learning iteration 232/4000 [0m

                       Computation: 3212 steps/s (collection: 0.504s, learning 2.045s)
               Value function loss: 151.7483
                    Surrogate loss: 0.0119
             Mean action noise std: 0.99
                       Mean reward: 336.36
               Mean episode length: 279.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1908736
                    Iteration time: 2.55s
                        Total time: 599.27s
                               ETA: 9691.2s

################################################################################
                     [1m Learning iteration 233/4000 [0m

                       Computation: 3250 steps/s (collection: 0.456s, learning 2.064s)
               Value function loss: 134.5358
                    Surrogate loss: 0.0119
             Mean action noise std: 0.99
                       Mean reward: 342.58
               Mean episode length: 284.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 2.52s
                        Total time: 601.79s
                               ETA: 9687.8s

################################################################################
                     [1m Learning iteration 234/4000 [0m

                       Computation: 3121 steps/s (collection: 0.545s, learning 2.080s)
               Value function loss: 145.7987
                    Surrogate loss: 0.0118
             Mean action noise std: 0.99
                       Mean reward: 344.84
               Mean episode length: 285.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1925120
                    Iteration time: 2.62s
                        Total time: 604.42s
                               ETA: 9686.1s

################################################################################
                     [1m Learning iteration 235/4000 [0m

                       Computation: 3205 steps/s (collection: 0.499s, learning 2.056s)
               Value function loss: 102.9389
                    Surrogate loss: 0.0132
             Mean action noise std: 0.99
                       Mean reward: 340.29
               Mean episode length: 278.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 2.56s
                        Total time: 606.97s
                               ETA: 9683.3s

################################################################################
                     [1m Learning iteration 236/4000 [0m

                       Computation: 3278 steps/s (collection: 0.454s, learning 2.044s)
               Value function loss: 172.5470
                    Surrogate loss: 0.0134
             Mean action noise std: 0.99
                       Mean reward: 333.44
               Mean episode length: 276.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1941504
                    Iteration time: 2.50s
                        Total time: 609.47s
                               ETA: 9679.5s

################################################################################
                     [1m Learning iteration 237/4000 [0m

                       Computation: 3263 steps/s (collection: 0.450s, learning 2.060s)
               Value function loss: 120.0800
                    Surrogate loss: 0.0151
             Mean action noise std: 0.99
                       Mean reward: 328.73
               Mean episode length: 272.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 2.51s
                        Total time: 611.98s
                               ETA: 9676.0s

################################################################################
                     [1m Learning iteration 238/4000 [0m

                       Computation: 3288 steps/s (collection: 0.482s, learning 2.009s)
               Value function loss: 176.0414
                    Surrogate loss: 0.0119
             Mean action noise std: 0.99
                       Mean reward: 324.46
               Mean episode length: 269.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1957888
                    Iteration time: 2.49s
                        Total time: 614.47s
                               ETA: 9672.1s

################################################################################
                     [1m Learning iteration 239/4000 [0m

                       Computation: 3301 steps/s (collection: 0.454s, learning 2.027s)
               Value function loss: 194.8969
                    Surrogate loss: 0.0121
             Mean action noise std: 0.99
                       Mean reward: 318.37
               Mean episode length: 261.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 2.48s
                        Total time: 616.95s
                               ETA: 9668.2s

################################################################################
                     [1m Learning iteration 240/4000 [0m

                       Computation: 3253 steps/s (collection: 0.476s, learning 2.042s)
               Value function loss: 222.8612
                    Surrogate loss: 0.0130
             Mean action noise std: 0.99
                       Mean reward: 314.70
               Mean episode length: 259.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1974272
                    Iteration time: 2.52s
                        Total time: 619.47s
                               ETA: 9664.8s

################################################################################
                     [1m Learning iteration 241/4000 [0m

                       Computation: 3218 steps/s (collection: 0.512s, learning 2.033s)
               Value function loss: 235.4257
                    Surrogate loss: 0.0148
             Mean action noise std: 0.99
                       Mean reward: 306.93
               Mean episode length: 252.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 2.55s
                        Total time: 622.02s
                               ETA: 9661.8s

################################################################################
                     [1m Learning iteration 242/4000 [0m

                       Computation: 3224 steps/s (collection: 0.490s, learning 2.051s)
               Value function loss: 215.5987
                    Surrogate loss: 0.0131
             Mean action noise std: 0.99
                       Mean reward: 312.11
               Mean episode length: 252.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1990656
                    Iteration time: 2.54s
                        Total time: 624.56s
                               ETA: 9658.8s

################################################################################
                     [1m Learning iteration 243/4000 [0m

                       Computation: 3279 steps/s (collection: 0.472s, learning 2.027s)
               Value function loss: 180.5007
                    Surrogate loss: 0.0130
             Mean action noise std: 0.99
                       Mean reward: 307.58
               Mean episode length: 248.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 2.50s
                        Total time: 627.05s
                               ETA: 9655.1s

################################################################################
                     [1m Learning iteration 244/4000 [0m

                       Computation: 3102 steps/s (collection: 0.566s, learning 2.074s)
               Value function loss: 136.1496
                    Surrogate loss: 0.0115
             Mean action noise std: 0.99
                       Mean reward: 305.81
               Mean episode length: 246.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2007040
                    Iteration time: 2.64s
                        Total time: 629.69s
                               ETA: 9653.6s

################################################################################
                     [1m Learning iteration 245/4000 [0m

                       Computation: 3152 steps/s (collection: 0.508s, learning 2.091s)
               Value function loss: 176.2436
                    Surrogate loss: 0.0144
             Mean action noise std: 0.99
                       Mean reward: 304.43
               Mean episode length: 243.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 2.60s
                        Total time: 632.29s
                               ETA: 9651.5s

################################################################################
                     [1m Learning iteration 246/4000 [0m

                       Computation: 3071 steps/s (collection: 0.573s, learning 2.094s)
               Value function loss: 149.8855
                    Surrogate loss: 0.0121
             Mean action noise std: 0.99
                       Mean reward: 288.21
               Mean episode length: 229.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 2023424
                    Iteration time: 2.67s
                        Total time: 634.96s
                               ETA: 9650.4s

################################################################################
                     [1m Learning iteration 247/4000 [0m

                       Computation: 3188 steps/s (collection: 0.511s, learning 2.058s)
               Value function loss: 146.1251
                    Surrogate loss: 0.0120
             Mean action noise std: 0.99
                       Mean reward: 292.34
               Mean episode length: 232.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 2.57s
                        Total time: 637.53s
                               ETA: 9647.8s

################################################################################
                     [1m Learning iteration 248/4000 [0m

                       Computation: 3169 steps/s (collection: 0.524s, learning 2.061s)
               Value function loss: 155.0465
                    Surrogate loss: 0.0147
             Mean action noise std: 0.99
                       Mean reward: 270.22
               Mean episode length: 214.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2039808
                    Iteration time: 2.58s
                        Total time: 640.11s
                               ETA: 9645.4s

################################################################################
                     [1m Learning iteration 249/4000 [0m

                       Computation: 3166 steps/s (collection: 0.502s, learning 2.085s)
               Value function loss: 194.8277
                    Surrogate loss: 0.0185
             Mean action noise std: 0.99
                       Mean reward: 250.27
               Mean episode length: 195.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 2.59s
                        Total time: 642.70s
                               ETA: 9643.1s

################################################################################
                     [1m Learning iteration 250/4000 [0m

                       Computation: 3058 steps/s (collection: 0.521s, learning 2.157s)
               Value function loss: 172.2797
                    Surrogate loss: 0.0113
             Mean action noise std: 0.99
                       Mean reward: 251.45
               Mean episode length: 194.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2056192
                    Iteration time: 2.68s
                        Total time: 645.38s
                               ETA: 9642.1s

################################################################################
                     [1m Learning iteration 251/4000 [0m

                       Computation: 3053 steps/s (collection: 0.517s, learning 2.166s)
               Value function loss: 147.4292
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 247.11
               Mean episode length: 192.45
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 2.68s
                        Total time: 648.06s
                               ETA: 9641.2s

################################################################################
                     [1m Learning iteration 252/4000 [0m

                       Computation: 3192 steps/s (collection: 0.480s, learning 2.086s)
               Value function loss: 154.3100
                    Surrogate loss: 0.0103
             Mean action noise std: 0.99
                       Mean reward: 249.75
               Mean episode length: 194.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2072576
                    Iteration time: 2.57s
                        Total time: 650.63s
                               ETA: 9638.6s

################################################################################
                     [1m Learning iteration 253/4000 [0m

                       Computation: 3240 steps/s (collection: 0.472s, learning 2.056s)
               Value function loss: 124.5113
                    Surrogate loss: 0.0125
             Mean action noise std: 0.99
                       Mean reward: 251.99
               Mean episode length: 195.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 2.53s
                        Total time: 653.16s
                               ETA: 9635.3s

################################################################################
                     [1m Learning iteration 254/4000 [0m

                       Computation: 3188 steps/s (collection: 0.494s, learning 2.075s)
               Value function loss: 117.3424
                    Surrogate loss: 0.0120
             Mean action noise std: 0.99
                       Mean reward: 260.29
               Mean episode length: 204.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2088960
                    Iteration time: 2.57s
                        Total time: 655.73s
                               ETA: 9632.7s

################################################################################
                     [1m Learning iteration 255/4000 [0m

                       Computation: 3178 steps/s (collection: 0.514s, learning 2.064s)
               Value function loss: 132.3911
                    Surrogate loss: 0.0138
             Mean action noise std: 0.99
                       Mean reward: 263.94
               Mean episode length: 210.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 2.58s
                        Total time: 658.30s
                               ETA: 9630.3s

################################################################################
                     [1m Learning iteration 256/4000 [0m

                       Computation: 3198 steps/s (collection: 0.490s, learning 2.071s)
               Value function loss: 113.0445
                    Surrogate loss: 0.0155
             Mean action noise std: 0.99
                       Mean reward: 279.49
               Mean episode length: 224.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2105344
                    Iteration time: 2.56s
                        Total time: 660.86s
                               ETA: 9627.5s

################################################################################
                     [1m Learning iteration 257/4000 [0m

                       Computation: 3166 steps/s (collection: 0.506s, learning 2.081s)
               Value function loss: 183.3840
                    Surrogate loss: 0.0167
             Mean action noise std: 0.99
                       Mean reward: 298.04
               Mean episode length: 238.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 2.59s
                        Total time: 663.45s
                               ETA: 9625.2s

################################################################################
                     [1m Learning iteration 258/4000 [0m

                       Computation: 3208 steps/s (collection: 0.501s, learning 2.053s)
               Value function loss: 127.5703
                    Surrogate loss: 0.0113
             Mean action noise std: 0.99
                       Mean reward: 295.04
               Mean episode length: 237.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2121728
                    Iteration time: 2.55s
                        Total time: 666.00s
                               ETA: 9622.3s

################################################################################
                     [1m Learning iteration 259/4000 [0m

                       Computation: 3263 steps/s (collection: 0.445s, learning 2.065s)
               Value function loss: 157.2796
                    Surrogate loss: 0.0134
             Mean action noise std: 0.99
                       Mean reward: 311.90
               Mean episode length: 249.36
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 2.51s
                        Total time: 668.51s
                               ETA: 9618.9s

################################################################################
                     [1m Learning iteration 260/4000 [0m

                       Computation: 3252 steps/s (collection: 0.447s, learning 2.072s)
               Value function loss: 195.0599
                    Surrogate loss: 0.0129
             Mean action noise std: 0.99
                       Mean reward: 301.69
               Mean episode length: 240.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 2138112
                    Iteration time: 2.52s
                        Total time: 671.03s
                               ETA: 9615.6s

################################################################################
                     [1m Learning iteration 261/4000 [0m

                       Computation: 3175 steps/s (collection: 0.463s, learning 2.116s)
               Value function loss: 174.3081
                    Surrogate loss: 0.0138
             Mean action noise std: 0.99
                       Mean reward: 317.58
               Mean episode length: 250.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 2.58s
                        Total time: 673.61s
                               ETA: 9613.1s

################################################################################
                     [1m Learning iteration 262/4000 [0m

                       Computation: 3266 steps/s (collection: 0.458s, learning 2.050s)
               Value function loss: 185.8898
                    Surrogate loss: 0.0126
             Mean action noise std: 0.99
                       Mean reward: 294.62
               Mean episode length: 233.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 2154496
                    Iteration time: 2.51s
                        Total time: 676.12s
                               ETA: 9609.7s

################################################################################
                     [1m Learning iteration 263/4000 [0m

                       Computation: 3237 steps/s (collection: 0.470s, learning 2.060s)
               Value function loss: 145.3442
                    Surrogate loss: 0.0132
             Mean action noise std: 0.99
                       Mean reward: 294.37
               Mean episode length: 232.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 2.53s
                        Total time: 678.65s
                               ETA: 9606.5s

################################################################################
                     [1m Learning iteration 264/4000 [0m

                       Computation: 3283 steps/s (collection: 0.449s, learning 2.047s)
               Value function loss: 166.7716
                    Surrogate loss: 0.0127
             Mean action noise std: 0.99
                       Mean reward: 278.23
               Mean episode length: 219.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2170880
                    Iteration time: 2.50s
                        Total time: 681.15s
                               ETA: 9602.9s

################################################################################
                     [1m Learning iteration 265/4000 [0m

                       Computation: 3271 steps/s (collection: 0.455s, learning 2.050s)
               Value function loss: 181.4095
                    Surrogate loss: 0.0149
             Mean action noise std: 1.00
                       Mean reward: 297.92
               Mean episode length: 232.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 2.50s
                        Total time: 683.65s
                               ETA: 9599.4s

################################################################################
                     [1m Learning iteration 266/4000 [0m

                       Computation: 3310 steps/s (collection: 0.454s, learning 2.020s)
               Value function loss: 204.6809
                    Surrogate loss: 0.0151
             Mean action noise std: 1.00
                       Mean reward: 298.68
               Mean episode length: 231.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2187264
                    Iteration time: 2.47s
                        Total time: 686.12s
                               ETA: 9595.5s

################################################################################
                     [1m Learning iteration 267/4000 [0m

                       Computation: 3299 steps/s (collection: 0.436s, learning 2.047s)
               Value function loss: 223.2235
                    Surrogate loss: 0.0149
             Mean action noise std: 1.00
                       Mean reward: 283.16
               Mean episode length: 216.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 2.48s
                        Total time: 688.61s
                               ETA: 9591.7s

################################################################################
                     [1m Learning iteration 268/4000 [0m

                       Computation: 3358 steps/s (collection: 0.439s, learning 2.000s)
               Value function loss: 206.6283
                    Surrogate loss: 0.0105
             Mean action noise std: 1.00
                       Mean reward: 284.01
               Mean episode length: 214.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2203648
                    Iteration time: 2.44s
                        Total time: 691.05s
                               ETA: 9587.3s

################################################################################
                     [1m Learning iteration 269/4000 [0m

                       Computation: 3368 steps/s (collection: 0.434s, learning 1.998s)
               Value function loss: 313.6257
                    Surrogate loss: 0.0073
             Mean action noise std: 0.99
                       Mean reward: 283.68
               Mean episode length: 210.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 2.43s
                        Total time: 693.48s
                               ETA: 9582.8s

################################################################################
                     [1m Learning iteration 270/4000 [0m

                       Computation: 3394 steps/s (collection: 0.422s, learning 1.991s)
               Value function loss: 139.6900
                    Surrogate loss: 0.0136
             Mean action noise std: 0.99
                       Mean reward: 279.76
               Mean episode length: 209.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2220032
                    Iteration time: 2.41s
                        Total time: 695.89s
                               ETA: 9578.1s

################################################################################
                     [1m Learning iteration 271/4000 [0m

                       Computation: 3329 steps/s (collection: 0.442s, learning 2.019s)
               Value function loss: 152.6002
                    Surrogate loss: 0.0153
             Mean action noise std: 0.99
                       Mean reward: 270.66
               Mean episode length: 202.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 2.46s
                        Total time: 698.35s
                               ETA: 9574.1s

################################################################################
                     [1m Learning iteration 272/4000 [0m

                       Computation: 3393 steps/s (collection: 0.424s, learning 1.991s)
               Value function loss: 145.7987
                    Surrogate loss: 0.0124
             Mean action noise std: 0.99
                       Mean reward: 277.07
               Mean episode length: 204.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2236416
                    Iteration time: 2.41s
                        Total time: 700.77s
                               ETA: 9569.4s

################################################################################
                     [1m Learning iteration 273/4000 [0m

                       Computation: 3192 steps/s (collection: 0.527s, learning 2.038s)
               Value function loss: 146.8988
                    Surrogate loss: 0.0143
             Mean action noise std: 0.99
                       Mean reward: 277.75
               Mean episode length: 206.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 2.57s
                        Total time: 703.33s
                               ETA: 9566.9s

################################################################################
                     [1m Learning iteration 274/4000 [0m

                       Computation: 3232 steps/s (collection: 0.502s, learning 2.032s)
               Value function loss: 134.7778
                    Surrogate loss: 0.0173
             Mean action noise std: 0.99
                       Mean reward: 291.09
               Mean episode length: 219.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2252800
                    Iteration time: 2.53s
                        Total time: 705.87s
                               ETA: 9563.9s

################################################################################
                     [1m Learning iteration 275/4000 [0m

                       Computation: 3264 steps/s (collection: 0.490s, learning 2.020s)
               Value function loss: 161.8390
                    Surrogate loss: 0.0191
             Mean action noise std: 0.99
                       Mean reward: 286.96
               Mean episode length: 218.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 2.51s
                        Total time: 708.38s
                               ETA: 9560.5s

################################################################################
                     [1m Learning iteration 276/4000 [0m

                       Computation: 3214 steps/s (collection: 0.512s, learning 2.036s)
               Value function loss: 225.1839
                    Surrogate loss: 0.0147
             Mean action noise std: 0.99
                       Mean reward: 292.26
               Mean episode length: 223.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2269184
                    Iteration time: 2.55s
                        Total time: 710.92s
                               ETA: 9557.7s

################################################################################
                     [1m Learning iteration 277/4000 [0m

                       Computation: 3219 steps/s (collection: 0.487s, learning 2.057s)
               Value function loss: 258.8311
                    Surrogate loss: 0.0170
             Mean action noise std: 0.99
                       Mean reward: 285.23
               Mean episode length: 221.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 2.54s
                        Total time: 713.47s
                               ETA: 9554.8s

################################################################################
                     [1m Learning iteration 278/4000 [0m

                       Computation: 3178 steps/s (collection: 0.545s, learning 2.033s)
               Value function loss: 130.6305
                    Surrogate loss: 0.0132
             Mean action noise std: 0.99
                       Mean reward: 300.73
               Mean episode length: 230.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2285568
                    Iteration time: 2.58s
                        Total time: 716.05s
                               ETA: 9552.4s

################################################################################
                     [1m Learning iteration 279/4000 [0m

                       Computation: 3172 steps/s (collection: 0.520s, learning 2.062s)
               Value function loss: 162.3286
                    Surrogate loss: 0.0147
             Mean action noise std: 0.99
                       Mean reward: 291.44
               Mean episode length: 219.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 2.58s
                        Total time: 718.63s
                               ETA: 9550.1s

################################################################################
                     [1m Learning iteration 280/4000 [0m

                       Computation: 3295 steps/s (collection: 0.434s, learning 2.052s)
               Value function loss: 100.3883
                    Surrogate loss: 0.0164
             Mean action noise std: 0.99
                       Mean reward: 287.84
               Mean episode length: 218.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2301952
                    Iteration time: 2.49s
                        Total time: 721.11s
                               ETA: 9546.4s

################################################################################
                     [1m Learning iteration 281/4000 [0m

                       Computation: 3261 steps/s (collection: 0.462s, learning 2.050s)
               Value function loss: 174.6075
                    Surrogate loss: 0.0170
             Mean action noise std: 0.99
                       Mean reward: 293.30
               Mean episode length: 221.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 2.51s
                        Total time: 723.63s
                               ETA: 9543.1s

################################################################################
                     [1m Learning iteration 282/4000 [0m

                       Computation: 3210 steps/s (collection: 0.493s, learning 2.059s)
               Value function loss: 120.6157
                    Surrogate loss: 0.0133
             Mean action noise std: 0.99
                       Mean reward: 295.58
               Mean episode length: 222.27
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2318336
                    Iteration time: 2.55s
                        Total time: 726.18s
                               ETA: 9540.4s

################################################################################
                     [1m Learning iteration 283/4000 [0m

                       Computation: 3223 steps/s (collection: 0.482s, learning 2.059s)
               Value function loss: 159.7877
                    Surrogate loss: 0.0149
             Mean action noise std: 0.99
                       Mean reward: 309.04
               Mean episode length: 230.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 2.54s
                        Total time: 728.72s
                               ETA: 9537.5s

################################################################################
                     [1m Learning iteration 284/4000 [0m

                       Computation: 3278 steps/s (collection: 0.446s, learning 2.053s)
               Value function loss: 155.8453
                    Surrogate loss: 0.0139
             Mean action noise std: 0.99
                       Mean reward: 318.34
               Mean episode length: 234.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2334720
                    Iteration time: 2.50s
                        Total time: 731.22s
                               ETA: 9534.1s

################################################################################
                     [1m Learning iteration 285/4000 [0m

                       Computation: 3305 steps/s (collection: 0.441s, learning 2.037s)
               Value function loss: 203.8552
                    Surrogate loss: 0.0174
             Mean action noise std: 0.99
                       Mean reward: 316.98
               Mean episode length: 234.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 2.48s
                        Total time: 733.70s
                               ETA: 9530.4s

################################################################################
                     [1m Learning iteration 286/4000 [0m

                       Computation: 3306 steps/s (collection: 0.454s, learning 2.023s)
               Value function loss: 123.1591
                    Surrogate loss: 0.0139
             Mean action noise std: 0.99
                       Mean reward: 336.11
               Mean episode length: 249.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2351104
                    Iteration time: 2.48s
                        Total time: 736.17s
                               ETA: 9526.6s

################################################################################
                     [1m Learning iteration 287/4000 [0m

                       Computation: 3259 steps/s (collection: 0.469s, learning 2.044s)
               Value function loss: 125.5454
                    Surrogate loss: 0.0170
             Mean action noise std: 0.99
                       Mean reward: 331.23
               Mean episode length: 245.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 2.51s
                        Total time: 738.69s
                               ETA: 9523.4s

################################################################################
                     [1m Learning iteration 288/4000 [0m

                       Computation: 3313 steps/s (collection: 0.427s, learning 2.045s)
               Value function loss: 152.7668
                    Surrogate loss: 0.0165
             Mean action noise std: 0.99
                       Mean reward: 332.50
               Mean episode length: 247.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2367488
                    Iteration time: 2.47s
                        Total time: 741.16s
                               ETA: 9519.7s

################################################################################
                     [1m Learning iteration 289/4000 [0m

                       Computation: 3312 steps/s (collection: 0.443s, learning 2.029s)
               Value function loss: 172.7354
                    Surrogate loss: 0.0121
             Mean action noise std: 0.99
                       Mean reward: 319.88
               Mean episode length: 236.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 2.47s
                        Total time: 743.63s
                               ETA: 9515.9s

################################################################################
                     [1m Learning iteration 290/4000 [0m

                       Computation: 3227 steps/s (collection: 0.478s, learning 2.061s)
               Value function loss: 110.9864
                    Surrogate loss: 0.0177
             Mean action noise std: 0.99
                       Mean reward: 319.02
               Mean episode length: 237.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2383872
                    Iteration time: 2.54s
                        Total time: 746.17s
                               ETA: 9513.0s

################################################################################
                     [1m Learning iteration 291/4000 [0m

                       Computation: 3275 steps/s (collection: 0.447s, learning 2.053s)
               Value function loss: 165.5681
                    Surrogate loss: 0.0134
             Mean action noise std: 0.99
                       Mean reward: 311.32
               Mean episode length: 233.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 2.50s
                        Total time: 748.67s
                               ETA: 9509.7s

################################################################################
                     [1m Learning iteration 292/4000 [0m

                       Computation: 3079 steps/s (collection: 0.474s, learning 2.186s)
               Value function loss: 193.7494
                    Surrogate loss: 0.0135
             Mean action noise std: 0.99
                       Mean reward: 319.90
               Mean episode length: 238.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2400256
                    Iteration time: 2.66s
                        Total time: 751.33s
                               ETA: 9508.3s

################################################################################
                     [1m Learning iteration 293/4000 [0m

                       Computation: 3160 steps/s (collection: 0.484s, learning 2.107s)
               Value function loss: 133.1154
                    Surrogate loss: 0.0135
             Mean action noise std: 0.99
                       Mean reward: 317.88
               Mean episode length: 233.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 2.59s
                        Total time: 753.92s
                               ETA: 9506.1s

################################################################################
                     [1m Learning iteration 294/4000 [0m

                       Computation: 3205 steps/s (collection: 0.449s, learning 2.107s)
               Value function loss: 155.1290
                    Surrogate loss: 0.0132
             Mean action noise std: 0.99
                       Mean reward: 312.74
               Mean episode length: 231.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2416640
                    Iteration time: 2.56s
                        Total time: 756.48s
                               ETA: 9503.4s

################################################################################
                     [1m Learning iteration 295/4000 [0m

                       Computation: 3110 steps/s (collection: 0.475s, learning 2.159s)
               Value function loss: 112.5517
                    Surrogate loss: 0.0164
             Mean action noise std: 0.99
                       Mean reward: 316.17
               Mean episode length: 234.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 2.63s
                        Total time: 759.11s
                               ETA: 9501.7s

################################################################################
                     [1m Learning iteration 296/4000 [0m

                       Computation: 3099 steps/s (collection: 0.507s, learning 2.137s)
               Value function loss: 165.8967
                    Surrogate loss: 0.0160
             Mean action noise std: 0.99
                       Mean reward: 310.83
               Mean episode length: 231.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2433024
                    Iteration time: 2.64s
                        Total time: 761.76s
                               ETA: 9500.1s

################################################################################
                     [1m Learning iteration 297/4000 [0m

                       Computation: 3052 steps/s (collection: 0.508s, learning 2.175s)
               Value function loss: 131.4663
                    Surrogate loss: 0.0148
             Mean action noise std: 0.99
                       Mean reward: 323.54
               Mean episode length: 240.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 2.68s
                        Total time: 764.44s
                               ETA: 9499.0s

################################################################################
                     [1m Learning iteration 298/4000 [0m

                       Computation: 3102 steps/s (collection: 0.506s, learning 2.134s)
               Value function loss: 126.3374
                    Surrogate loss: 0.0145
             Mean action noise std: 0.99
                       Mean reward: 322.80
               Mean episode length: 241.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2449408
                    Iteration time: 2.64s
                        Total time: 767.08s
                               ETA: 9497.4s

################################################################################
                     [1m Learning iteration 299/4000 [0m

                       Computation: 3107 steps/s (collection: 0.504s, learning 2.133s)
               Value function loss: 150.2801
                    Surrogate loss: 0.0157
             Mean action noise std: 0.99
                       Mean reward: 321.87
               Mean episode length: 242.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 2.64s
                        Total time: 769.72s
                               ETA: 9495.7s

################################################################################
                     [1m Learning iteration 300/4000 [0m

                       Computation: 3047 steps/s (collection: 0.537s, learning 2.151s)
               Value function loss: 166.1365
                    Surrogate loss: 0.0139
             Mean action noise std: 0.99
                       Mean reward: 341.09
               Mean episode length: 254.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2465792
                    Iteration time: 2.69s
                        Total time: 772.40s
                               ETA: 9494.7s

################################################################################
                     [1m Learning iteration 301/4000 [0m

                       Computation: 3121 steps/s (collection: 0.522s, learning 2.102s)
               Value function loss: 212.1615
                    Surrogate loss: 0.0140
             Mean action noise std: 0.99
                       Mean reward: 371.26
               Mean episode length: 274.68
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 2.62s
                        Total time: 775.03s
                               ETA: 9492.8s

################################################################################
                     [1m Learning iteration 302/4000 [0m

                       Computation: 3219 steps/s (collection: 0.475s, learning 2.070s)
               Value function loss: 103.7322
                    Surrogate loss: 0.0164
             Mean action noise std: 0.99
                       Mean reward: 367.21
               Mean episode length: 272.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2482176
                    Iteration time: 2.54s
                        Total time: 777.57s
                               ETA: 9490.0s

################################################################################
                     [1m Learning iteration 303/4000 [0m

                       Computation: 3210 steps/s (collection: 0.484s, learning 2.068s)
               Value function loss: 140.4857
                    Surrogate loss: 0.0153
             Mean action noise std: 0.99
                       Mean reward: 371.25
               Mean episode length: 275.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 2.55s
                        Total time: 780.12s
                               ETA: 9487.2s

################################################################################
                     [1m Learning iteration 304/4000 [0m

                       Computation: 3175 steps/s (collection: 0.537s, learning 2.043s)
               Value function loss: 161.2084
                    Surrogate loss: 0.0139
             Mean action noise std: 0.98
                       Mean reward: 366.29
               Mean episode length: 270.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2498560
                    Iteration time: 2.58s
                        Total time: 782.70s
                               ETA: 9484.8s

################################################################################
                     [1m Learning iteration 305/4000 [0m

                       Computation: 3121 steps/s (collection: 0.520s, learning 2.104s)
               Value function loss: 136.9455
                    Surrogate loss: 0.0144
             Mean action noise std: 0.98
                       Mean reward: 366.35
               Mean episode length: 270.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 2.62s
                        Total time: 785.33s
                               ETA: 9483.0s

################################################################################
                     [1m Learning iteration 306/4000 [0m

                       Computation: 3233 steps/s (collection: 0.466s, learning 2.067s)
               Value function loss: 146.7619
                    Surrogate loss: 0.0178
             Mean action noise std: 0.99
                       Mean reward: 365.48
               Mean episode length: 270.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2514944
                    Iteration time: 2.53s
                        Total time: 787.86s
                               ETA: 9480.0s

################################################################################
                     [1m Learning iteration 307/4000 [0m

                       Computation: 3233 steps/s (collection: 0.489s, learning 2.044s)
               Value function loss: 263.9252
                    Surrogate loss: 0.0144
             Mean action noise std: 0.99
                       Mean reward: 362.00
               Mean episode length: 268.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 2.53s
                        Total time: 790.39s
                               ETA: 9477.0s

################################################################################
                     [1m Learning iteration 308/4000 [0m

                       Computation: 3109 steps/s (collection: 0.528s, learning 2.107s)
               Value function loss: 163.6968
                    Surrogate loss: 0.0136
             Mean action noise std: 0.99
                       Mean reward: 368.41
               Mean episode length: 272.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2531328
                    Iteration time: 2.63s
                        Total time: 793.03s
                               ETA: 9475.3s

################################################################################
                     [1m Learning iteration 309/4000 [0m

                       Computation: 3179 steps/s (collection: 0.507s, learning 2.069s)
               Value function loss: 105.3270
                    Surrogate loss: 0.0180
             Mean action noise std: 0.99
                       Mean reward: 359.81
               Mean episode length: 264.63
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 2.58s
                        Total time: 795.61s
                               ETA: 9472.8s

################################################################################
                     [1m Learning iteration 310/4000 [0m

                       Computation: 3156 steps/s (collection: 0.483s, learning 2.112s)
               Value function loss: 194.5755
                    Surrogate loss: 0.0154
             Mean action noise std: 0.99
                       Mean reward: 363.81
               Mean episode length: 267.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2547712
                    Iteration time: 2.59s
                        Total time: 798.20s
                               ETA: 9470.6s

################################################################################
                     [1m Learning iteration 311/4000 [0m

                       Computation: 3202 steps/s (collection: 0.503s, learning 2.055s)
               Value function loss: 101.7224
                    Surrogate loss: 0.0151
             Mean action noise std: 0.99
                       Mean reward: 368.36
               Mean episode length: 269.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 2.56s
                        Total time: 800.76s
                               ETA: 9467.9s

################################################################################
                     [1m Learning iteration 312/4000 [0m

                       Computation: 3194 steps/s (collection: 0.492s, learning 2.072s)
               Value function loss: 145.9540
                    Surrogate loss: 0.0177
             Mean action noise std: 0.99
                       Mean reward: 357.36
               Mean episode length: 261.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2564096
                    Iteration time: 2.56s
                        Total time: 803.32s
                               ETA: 9465.4s

################################################################################
                     [1m Learning iteration 313/4000 [0m

                       Computation: 3198 steps/s (collection: 0.504s, learning 2.057s)
               Value function loss: 119.4041
                    Surrogate loss: 0.0191
             Mean action noise std: 0.99
                       Mean reward: 350.48
               Mean episode length: 255.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 2.56s
                        Total time: 805.88s
                               ETA: 9462.7s

################################################################################
                     [1m Learning iteration 314/4000 [0m

                       Computation: 3206 steps/s (collection: 0.520s, learning 2.035s)
               Value function loss: 112.1515
                    Surrogate loss: 0.0176
             Mean action noise std: 0.99
                       Mean reward: 348.97
               Mean episode length: 254.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2580480
                    Iteration time: 2.55s
                        Total time: 808.44s
                               ETA: 9460.0s

################################################################################
                     [1m Learning iteration 315/4000 [0m

                       Computation: 3127 steps/s (collection: 0.508s, learning 2.111s)
               Value function loss: 179.6339
                    Surrogate loss: 0.0133
             Mean action noise std: 0.99
                       Mean reward: 349.20
               Mean episode length: 254.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 2.62s
                        Total time: 811.06s
                               ETA: 9458.1s

################################################################################
                     [1m Learning iteration 316/4000 [0m

                       Computation: 3042 steps/s (collection: 0.537s, learning 2.155s)
               Value function loss: 128.8405
                    Surrogate loss: 0.0150
             Mean action noise std: 0.99
                       Mean reward: 360.46
               Mean episode length: 262.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 2596864
                    Iteration time: 2.69s
                        Total time: 813.75s
                               ETA: 9457.0s

################################################################################
                     [1m Learning iteration 317/4000 [0m

                       Computation: 3142 steps/s (collection: 0.503s, learning 2.104s)
               Value function loss: 192.7254
                    Surrogate loss: 0.0141
             Mean action noise std: 0.99
                       Mean reward: 347.95
               Mean episode length: 253.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 2.61s
                        Total time: 816.36s
                               ETA: 9454.9s

################################################################################
                     [1m Learning iteration 318/4000 [0m

                       Computation: 3188 steps/s (collection: 0.475s, learning 2.095s)
               Value function loss: 127.9806
                    Surrogate loss: 0.0150
             Mean action noise std: 0.98
                       Mean reward: 338.10
               Mean episode length: 248.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2613248
                    Iteration time: 2.57s
                        Total time: 818.93s
                               ETA: 9452.3s

################################################################################
                     [1m Learning iteration 319/4000 [0m

                       Computation: 3163 steps/s (collection: 0.497s, learning 2.092s)
               Value function loss: 131.0846
                    Surrogate loss: 0.0131
             Mean action noise std: 0.99
                       Mean reward: 340.41
               Mean episode length: 250.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 2.59s
                        Total time: 821.52s
                               ETA: 9450.0s

################################################################################
                     [1m Learning iteration 320/4000 [0m

                       Computation: 3064 steps/s (collection: 0.523s, learning 2.150s)
               Value function loss: 167.5838
                    Surrogate loss: 0.0146
             Mean action noise std: 0.99
                       Mean reward: 341.37
               Mean episode length: 251.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2629632
                    Iteration time: 2.67s
                        Total time: 824.19s
                               ETA: 9448.7s

################################################################################
                     [1m Learning iteration 321/4000 [0m

                       Computation: 3097 steps/s (collection: 0.490s, learning 2.155s)
               Value function loss: 183.3547
                    Surrogate loss: 0.0127
             Mean action noise std: 0.99
                       Mean reward: 340.03
               Mean episode length: 248.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 2.64s
                        Total time: 826.83s
                               ETA: 9447.0s

################################################################################
                     [1m Learning iteration 322/4000 [0m

                       Computation: 3186 steps/s (collection: 0.466s, learning 2.104s)
               Value function loss: 157.5686
                    Surrogate loss: 0.0121
             Mean action noise std: 0.98
                       Mean reward: 354.46
               Mean episode length: 259.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2646016
                    Iteration time: 2.57s
                        Total time: 829.40s
                               ETA: 9444.4s

################################################################################
                     [1m Learning iteration 323/4000 [0m

                       Computation: 3109 steps/s (collection: 0.503s, learning 2.132s)
               Value function loss: 189.8503
                    Surrogate loss: 0.0152
             Mean action noise std: 0.98
                       Mean reward: 368.71
               Mean episode length: 265.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 2.63s
                        Total time: 832.04s
                               ETA: 9442.6s

################################################################################
                     [1m Learning iteration 324/4000 [0m

                       Computation: 3018 steps/s (collection: 0.557s, learning 2.157s)
               Value function loss: 183.8065
                    Surrogate loss: 0.0126
             Mean action noise std: 0.99
                       Mean reward: 397.46
               Mean episode length: 282.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2662400
                    Iteration time: 2.71s
                        Total time: 834.75s
                               ETA: 9441.7s

################################################################################
                     [1m Learning iteration 325/4000 [0m

                       Computation: 3099 steps/s (collection: 0.541s, learning 2.102s)
               Value function loss: 199.4186
                    Surrogate loss: 0.0144
             Mean action noise std: 0.98
                       Mean reward: 387.85
               Mean episode length: 273.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 2.64s
                        Total time: 837.40s
                               ETA: 9440.0s

################################################################################
                     [1m Learning iteration 326/4000 [0m

                       Computation: 3113 steps/s (collection: 0.524s, learning 2.107s)
               Value function loss: 150.5563
                    Surrogate loss: 0.0143
             Mean action noise std: 0.99
                       Mean reward: 370.70
               Mean episode length: 261.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2678784
                    Iteration time: 2.63s
                        Total time: 840.03s
                               ETA: 9438.1s

################################################################################
                     [1m Learning iteration 327/4000 [0m

                       Computation: 3266 steps/s (collection: 0.431s, learning 2.077s)
               Value function loss: 177.7082
                    Surrogate loss: 0.0158
             Mean action noise std: 0.99
                       Mean reward: 362.42
               Mean episode length: 254.70
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 2.51s
                        Total time: 842.53s
                               ETA: 9434.8s

################################################################################
                     [1m Learning iteration 328/4000 [0m

                       Computation: 3135 steps/s (collection: 0.503s, learning 2.110s)
               Value function loss: 218.2535
                    Surrogate loss: 0.0140
             Mean action noise std: 0.99
                       Mean reward: 340.21
               Mean episode length: 239.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 2695168
                    Iteration time: 2.61s
                        Total time: 845.15s
                               ETA: 9432.8s

################################################################################
                     [1m Learning iteration 329/4000 [0m

                       Computation: 3195 steps/s (collection: 0.497s, learning 2.067s)
               Value function loss: 242.0653
                    Surrogate loss: 0.0127
             Mean action noise std: 0.99
                       Mean reward: 316.66
               Mean episode length: 224.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 2.56s
                        Total time: 847.71s
                               ETA: 9430.1s

################################################################################
                     [1m Learning iteration 330/4000 [0m

                       Computation: 3222 steps/s (collection: 0.502s, learning 2.040s)
               Value function loss: 183.7247
                    Surrogate loss: 0.0155
             Mean action noise std: 0.99
                       Mean reward: 315.97
               Mean episode length: 223.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2711552
                    Iteration time: 2.54s
                        Total time: 850.25s
                               ETA: 9427.3s

################################################################################
                     [1m Learning iteration 331/4000 [0m

                       Computation: 3241 steps/s (collection: 0.479s, learning 2.048s)
               Value function loss: 191.4886
                    Surrogate loss: 0.0153
             Mean action noise std: 0.99
                       Mean reward: 301.62
               Mean episode length: 212.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 2.53s
                        Total time: 852.78s
                               ETA: 9424.3s

################################################################################
                     [1m Learning iteration 332/4000 [0m

                       Computation: 3251 steps/s (collection: 0.451s, learning 2.069s)
               Value function loss: 152.9084
                    Surrogate loss: 0.0134
             Mean action noise std: 0.99
                       Mean reward: 309.87
               Mean episode length: 218.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2727936
                    Iteration time: 2.52s
                        Total time: 855.30s
                               ETA: 9421.1s

################################################################################
                     [1m Learning iteration 333/4000 [0m

                       Computation: 3065 steps/s (collection: 0.519s, learning 2.153s)
               Value function loss: 185.0409
                    Surrogate loss: 0.0154
             Mean action noise std: 0.99
                       Mean reward: 309.32
               Mean episode length: 217.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 2.67s
                        Total time: 857.97s
                               ETA: 9419.7s

################################################################################
                     [1m Learning iteration 334/4000 [0m

                       Computation: 3241 steps/s (collection: 0.495s, learning 2.032s)
               Value function loss: 183.7540
                    Surrogate loss: 0.0143
             Mean action noise std: 0.99
                       Mean reward: 310.31
               Mean episode length: 216.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2744320
                    Iteration time: 2.53s
                        Total time: 860.50s
                               ETA: 9416.7s

################################################################################
                     [1m Learning iteration 335/4000 [0m

                       Computation: 3208 steps/s (collection: 0.471s, learning 2.083s)
               Value function loss: 194.2557
                    Surrogate loss: 0.0118
             Mean action noise std: 0.99
                       Mean reward: 300.62
               Mean episode length: 211.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 2.55s
                        Total time: 863.05s
                               ETA: 9414.0s

################################################################################
                     [1m Learning iteration 336/4000 [0m

                       Computation: 3114 steps/s (collection: 0.543s, learning 2.087s)
               Value function loss: 180.5848
                    Surrogate loss: 0.0139
             Mean action noise std: 0.99
                       Mean reward: 296.92
               Mean episode length: 207.37
                 Mean success rate: 0.50
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2760704
                    Iteration time: 2.63s
                        Total time: 865.68s
                               ETA: 9412.1s

################################################################################
                     [1m Learning iteration 337/4000 [0m

                       Computation: 3219 steps/s (collection: 0.486s, learning 2.059s)
               Value function loss: 209.1080
                    Surrogate loss: 0.0160
             Mean action noise std: 0.99
                       Mean reward: 312.96
               Mean episode length: 217.32
                 Mean success rate: 0.50
                  Mean reward/step: 1.41
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 2.54s
                        Total time: 868.23s
                               ETA: 9409.2s

################################################################################
                     [1m Learning iteration 338/4000 [0m

                       Computation: 3165 steps/s (collection: 0.476s, learning 2.111s)
               Value function loss: 168.2449
                    Surrogate loss: 0.0185
             Mean action noise std: 0.99
                       Mean reward: 299.35
               Mean episode length: 208.82
                 Mean success rate: 0.50
                  Mean reward/step: 1.47
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2777088
                    Iteration time: 2.59s
                        Total time: 870.82s
                               ETA: 9406.9s

################################################################################
                     [1m Learning iteration 339/4000 [0m

                       Computation: 3113 steps/s (collection: 0.498s, learning 2.133s)
               Value function loss: 148.4332
                    Surrogate loss: 0.0150
             Mean action noise std: 0.99
                       Mean reward: 314.53
               Mean episode length: 220.59
                 Mean success rate: 0.50
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 2.63s
                        Total time: 873.45s
                               ETA: 9405.0s

################################################################################
                     [1m Learning iteration 340/4000 [0m

                       Computation: 3174 steps/s (collection: 0.488s, learning 2.092s)
               Value function loss: 177.7260
                    Surrogate loss: 0.0154
             Mean action noise std: 0.99
                       Mean reward: 318.17
               Mean episode length: 223.53
                 Mean success rate: 0.50
                  Mean reward/step: 1.52
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2793472
                    Iteration time: 2.58s
                        Total time: 876.03s
                               ETA: 9402.5s

################################################################################
                     [1m Learning iteration 341/4000 [0m

                       Computation: 3127 steps/s (collection: 0.485s, learning 2.135s)
               Value function loss: 186.1689
                    Surrogate loss: 0.0155
             Mean action noise std: 0.99
                       Mean reward: 326.01
               Mean episode length: 229.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 2.62s
                        Total time: 878.65s
                               ETA: 9400.5s

################################################################################
                     [1m Learning iteration 342/4000 [0m

                       Computation: 3200 steps/s (collection: 0.480s, learning 2.079s)
               Value function loss: 132.7574
                    Surrogate loss: 0.0157
             Mean action noise std: 0.99
                       Mean reward: 315.25
               Mean episode length: 223.16
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2809856
                    Iteration time: 2.56s
                        Total time: 881.21s
                               ETA: 9397.8s

################################################################################
                     [1m Learning iteration 343/4000 [0m

                       Computation: 3151 steps/s (collection: 0.485s, learning 2.114s)
               Value function loss: 187.4145
                    Surrogate loss: 0.0156
             Mean action noise std: 0.99
                       Mean reward: 318.12
               Mean episode length: 226.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 2.60s
                        Total time: 883.81s
                               ETA: 9395.6s

################################################################################
                     [1m Learning iteration 344/4000 [0m

                       Computation: 3164 steps/s (collection: 0.487s, learning 2.102s)
               Value function loss: 168.7168
                    Surrogate loss: 0.0163
             Mean action noise std: 0.99
                       Mean reward: 316.98
               Mean episode length: 224.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2826240
                    Iteration time: 2.59s
                        Total time: 886.40s
                               ETA: 9393.2s

################################################################################
                     [1m Learning iteration 345/4000 [0m

                       Computation: 3129 steps/s (collection: 0.509s, learning 2.109s)
               Value function loss: 185.1525
                    Surrogate loss: 0.0154
             Mean action noise std: 0.99
                       Mean reward: 318.73
               Mean episode length: 225.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 2.62s
                        Total time: 889.01s
                               ETA: 9391.2s

################################################################################
                     [1m Learning iteration 346/4000 [0m

                       Computation: 3194 steps/s (collection: 0.477s, learning 2.087s)
               Value function loss: 203.4031
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 317.67
               Mean episode length: 224.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2842624
                    Iteration time: 2.56s
                        Total time: 891.58s
                               ETA: 9388.5s

################################################################################
                     [1m Learning iteration 347/4000 [0m

                       Computation: 3157 steps/s (collection: 0.500s, learning 2.094s)
               Value function loss: 140.5943
                    Surrogate loss: 0.0149
             Mean action noise std: 0.98
                       Mean reward: 323.05
               Mean episode length: 226.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 2.59s
                        Total time: 894.17s
                               ETA: 9386.2s

################################################################################
                     [1m Learning iteration 348/4000 [0m

                       Computation: 3175 steps/s (collection: 0.455s, learning 2.125s)
               Value function loss: 136.4154
                    Surrogate loss: 0.0199
             Mean action noise std: 0.98
                       Mean reward: 324.61
               Mean episode length: 227.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2859008
                    Iteration time: 2.58s
                        Total time: 896.75s
                               ETA: 9383.8s

################################################################################
                     [1m Learning iteration 349/4000 [0m

                       Computation: 3139 steps/s (collection: 0.517s, learning 2.093s)
               Value function loss: 115.0015
                    Surrogate loss: 0.0298
             Mean action noise std: 0.98
                       Mean reward: 327.70
               Mean episode length: 228.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 2.61s
                        Total time: 899.36s
                               ETA: 9381.6s

################################################################################
                     [1m Learning iteration 350/4000 [0m

                       Computation: 3117 steps/s (collection: 0.494s, learning 2.134s)
               Value function loss: 228.5967
                    Surrogate loss: 0.0302
             Mean action noise std: 0.98
                       Mean reward: 348.88
               Mean episode length: 243.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2875392
                    Iteration time: 2.63s
                        Total time: 901.99s
                               ETA: 9379.7s

################################################################################
                     [1m Learning iteration 351/4000 [0m

                       Computation: 3142 steps/s (collection: 0.497s, learning 2.110s)
               Value function loss: 164.8360
                    Surrogate loss: 0.0160
             Mean action noise std: 0.98
                       Mean reward: 334.93
               Mean episode length: 232.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 2.61s
                        Total time: 904.60s
                               ETA: 9377.5s

################################################################################
                     [1m Learning iteration 352/4000 [0m

                       Computation: 3092 steps/s (collection: 0.511s, learning 2.139s)
               Value function loss: 201.9044
                    Surrogate loss: 0.0127
             Mean action noise std: 0.98
                       Mean reward: 332.13
               Mean episode length: 229.80
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2891776
                    Iteration time: 2.65s
                        Total time: 907.24s
                               ETA: 9375.7s

################################################################################
                     [1m Learning iteration 353/4000 [0m

                       Computation: 3165 steps/s (collection: 0.501s, learning 2.087s)
               Value function loss: 218.5514
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 342.67
               Mean episode length: 235.78
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 2.59s
                        Total time: 909.83s
                               ETA: 9373.3s

################################################################################
                     [1m Learning iteration 354/4000 [0m

                       Computation: 3195 steps/s (collection: 0.472s, learning 2.091s)
               Value function loss: 174.7613
                    Surrogate loss: 0.0160
             Mean action noise std: 0.98
                       Mean reward: 346.13
               Mean episode length: 238.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2908160
                    Iteration time: 2.56s
                        Total time: 912.40s
                               ETA: 9370.7s

################################################################################
                     [1m Learning iteration 355/4000 [0m

                       Computation: 3216 steps/s (collection: 0.487s, learning 2.060s)
               Value function loss: 274.7385
                    Surrogate loss: 0.0138
             Mean action noise std: 0.98
                       Mean reward: 345.30
               Mean episode length: 238.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 2.55s
                        Total time: 914.94s
                               ETA: 9367.9s

################################################################################
                     [1m Learning iteration 356/4000 [0m

                       Computation: 3140 steps/s (collection: 0.486s, learning 2.123s)
               Value function loss: 226.0993
                    Surrogate loss: 0.0139
             Mean action noise std: 0.98
                       Mean reward: 340.22
               Mean episode length: 230.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2924544
                    Iteration time: 2.61s
                        Total time: 917.55s
                               ETA: 9365.7s

################################################################################
                     [1m Learning iteration 357/4000 [0m

                       Computation: 3156 steps/s (collection: 0.493s, learning 2.103s)
               Value function loss: 176.1132
                    Surrogate loss: 0.0147
             Mean action noise std: 0.98
                       Mean reward: 345.51
               Mean episode length: 233.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 2.60s
                        Total time: 920.15s
                               ETA: 9363.4s

################################################################################
                     [1m Learning iteration 358/4000 [0m

                       Computation: 3157 steps/s (collection: 0.485s, learning 2.110s)
               Value function loss: 173.5816
                    Surrogate loss: 0.0128
             Mean action noise std: 0.98
                       Mean reward: 355.29
               Mean episode length: 240.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2940928
                    Iteration time: 2.59s
                        Total time: 922.74s
                               ETA: 9361.1s

################################################################################
                     [1m Learning iteration 359/4000 [0m

                       Computation: 3150 steps/s (collection: 0.498s, learning 2.103s)
               Value function loss: 283.2635
                    Surrogate loss: 0.0131
             Mean action noise std: 0.98
                       Mean reward: 352.19
               Mean episode length: 236.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 2.60s
                        Total time: 925.34s
                               ETA: 9358.8s

################################################################################
                     [1m Learning iteration 360/4000 [0m

                       Computation: 3122 steps/s (collection: 0.492s, learning 2.131s)
               Value function loss: 128.7600
                    Surrogate loss: 0.0190
             Mean action noise std: 0.99
                       Mean reward: 353.41
               Mean episode length: 237.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 2957312
                    Iteration time: 2.62s
                        Total time: 927.97s
                               ETA: 9356.8s

################################################################################
                     [1m Learning iteration 361/4000 [0m

                       Computation: 3207 steps/s (collection: 0.514s, learning 2.040s)
               Value function loss: 175.1328
                    Surrogate loss: 0.0195
             Mean action noise std: 0.99
                       Mean reward: 364.48
               Mean episode length: 243.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 2.55s
                        Total time: 930.52s
                               ETA: 9354.0s

################################################################################
                     [1m Learning iteration 362/4000 [0m

                       Computation: 3168 steps/s (collection: 0.498s, learning 2.088s)
               Value function loss: 153.6745
                    Surrogate loss: 0.0164
             Mean action noise std: 0.99
                       Mean reward: 372.50
               Mean episode length: 251.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2973696
                    Iteration time: 2.59s
                        Total time: 933.11s
                               ETA: 9351.6s

################################################################################
                     [1m Learning iteration 363/4000 [0m

                       Computation: 3199 steps/s (collection: 0.473s, learning 2.087s)
               Value function loss: 190.4559
                    Surrogate loss: 0.0166
             Mean action noise std: 0.99
                       Mean reward: 383.27
               Mean episode length: 260.14
                 Mean success rate: 0.50
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 2.56s
                        Total time: 935.67s
                               ETA: 9348.9s

################################################################################
                     [1m Learning iteration 364/4000 [0m

                       Computation: 3181 steps/s (collection: 0.503s, learning 2.072s)
               Value function loss: 145.7685
                    Surrogate loss: 0.0161
             Mean action noise std: 0.99
                       Mean reward: 374.98
               Mean episode length: 256.43
                 Mean success rate: 0.50
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2990080
                    Iteration time: 2.57s
                        Total time: 938.24s
                               ETA: 9346.4s

################################################################################
                     [1m Learning iteration 365/4000 [0m

                       Computation: 3214 steps/s (collection: 0.471s, learning 2.077s)
               Value function loss: 142.9282
                    Surrogate loss: 0.0172
             Mean action noise std: 0.99
                       Mean reward: 375.20
               Mean episode length: 255.25
                 Mean success rate: 0.50
                  Mean reward/step: 1.49
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 2.55s
                        Total time: 940.79s
                               ETA: 9343.6s

################################################################################
                     [1m Learning iteration 366/4000 [0m

                       Computation: 3214 steps/s (collection: 0.467s, learning 2.082s)
               Value function loss: 196.3861
                    Surrogate loss: 0.0166
             Mean action noise std: 0.99
                       Mean reward: 374.99
               Mean episode length: 258.57
                 Mean success rate: 0.50
                  Mean reward/step: 1.51
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3006464
                    Iteration time: 2.55s
                        Total time: 943.34s
                               ETA: 9340.8s

################################################################################
                     [1m Learning iteration 367/4000 [0m

                       Computation: 3155 steps/s (collection: 0.507s, learning 2.089s)
               Value function loss: 225.6412
                    Surrogate loss: 0.0159
             Mean action noise std: 0.99
                       Mean reward: 376.57
               Mean episode length: 259.69
                 Mean success rate: 0.50
                  Mean reward/step: 1.51
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 2.60s
                        Total time: 945.93s
                               ETA: 9338.5s

################################################################################
                     [1m Learning iteration 368/4000 [0m

                       Computation: 3162 steps/s (collection: 0.487s, learning 2.103s)
               Value function loss: 229.5510
                    Surrogate loss: 0.0180
             Mean action noise std: 0.99
                       Mean reward: 366.78
               Mean episode length: 252.72
                 Mean success rate: 0.50
                  Mean reward/step: 1.51
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3022848
                    Iteration time: 2.59s
                        Total time: 948.52s
                               ETA: 9336.1s

################################################################################
                     [1m Learning iteration 369/4000 [0m

                       Computation: 3205 steps/s (collection: 0.484s, learning 2.071s)
               Value function loss: 183.3160
                    Surrogate loss: 0.0162
             Mean action noise std: 0.99
                       Mean reward: 373.09
               Mean episode length: 260.42
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 2.56s
                        Total time: 951.08s
                               ETA: 9333.4s

################################################################################
                     [1m Learning iteration 370/4000 [0m

                       Computation: 3165 steps/s (collection: 0.505s, learning 2.082s)
               Value function loss: 178.6824
                    Surrogate loss: 0.0162
             Mean action noise std: 0.99
                       Mean reward: 393.70
               Mean episode length: 272.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3039232
                    Iteration time: 2.59s
                        Total time: 953.67s
                               ETA: 9331.0s

################################################################################
                     [1m Learning iteration 371/4000 [0m

                       Computation: 3251 steps/s (collection: 0.446s, learning 2.074s)
               Value function loss: 168.1727
                    Surrogate loss: 0.0165
             Mean action noise std: 0.99
                       Mean reward: 410.18
               Mean episode length: 284.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 2.52s
                        Total time: 956.19s
                               ETA: 9328.0s

################################################################################
                     [1m Learning iteration 372/4000 [0m

                       Computation: 3180 steps/s (collection: 0.496s, learning 2.080s)
               Value function loss: 170.5976
                    Surrogate loss: 0.0150
             Mean action noise std: 0.99
                       Mean reward: 405.20
               Mean episode length: 278.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3055616
                    Iteration time: 2.58s
                        Total time: 958.76s
                               ETA: 9325.4s

################################################################################
                     [1m Learning iteration 373/4000 [0m

                       Computation: 3157 steps/s (collection: 0.514s, learning 2.081s)
               Value function loss: 167.5668
                    Surrogate loss: 0.0154
             Mean action noise std: 0.99
                       Mean reward: 412.56
               Mean episode length: 280.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 2.59s
                        Total time: 961.36s
                               ETA: 9323.1s

################################################################################
                     [1m Learning iteration 374/4000 [0m

                       Computation: 3183 steps/s (collection: 0.474s, learning 2.100s)
               Value function loss: 246.8655
                    Surrogate loss: 0.0166
             Mean action noise std: 0.99
                       Mean reward: 409.88
               Mean episode length: 276.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 3072000
                    Iteration time: 2.57s
                        Total time: 963.93s
                               ETA: 9320.6s

################################################################################
                     [1m Learning iteration 375/4000 [0m

                       Computation: 3237 steps/s (collection: 0.452s, learning 2.079s)
               Value function loss: 167.8938
                    Surrogate loss: 0.0167
             Mean action noise std: 0.99
                       Mean reward: 409.39
               Mean episode length: 274.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 2.53s
                        Total time: 966.46s
                               ETA: 9317.6s

################################################################################
                     [1m Learning iteration 376/4000 [0m

                       Computation: 3189 steps/s (collection: 0.496s, learning 2.072s)
               Value function loss: 166.4698
                    Surrogate loss: 0.0157
             Mean action noise std: 0.99
                       Mean reward: 388.95
               Mean episode length: 259.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3088384
                    Iteration time: 2.57s
                        Total time: 969.03s
                               ETA: 9315.0s

################################################################################
                     [1m Learning iteration 377/4000 [0m

                       Computation: 3158 steps/s (collection: 0.494s, learning 2.100s)
               Value function loss: 211.7240
                    Surrogate loss: 0.0118
             Mean action noise std: 0.99
                       Mean reward: 374.20
               Mean episode length: 250.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 2.59s
                        Total time: 971.62s
                               ETA: 9312.7s

################################################################################
                     [1m Learning iteration 378/4000 [0m

                       Computation: 3187 steps/s (collection: 0.532s, learning 2.038s)
               Value function loss: 229.6157
                    Surrogate loss: 0.0125
             Mean action noise std: 0.99
                       Mean reward: 378.97
               Mean episode length: 253.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3104768
                    Iteration time: 2.57s
                        Total time: 974.19s
                               ETA: 9310.1s

################################################################################
                     [1m Learning iteration 379/4000 [0m

                       Computation: 3238 steps/s (collection: 0.479s, learning 2.051s)
               Value function loss: 141.9133
                    Surrogate loss: 0.0159
             Mean action noise std: 0.99
                       Mean reward: 375.53
               Mean episode length: 253.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 2.53s
                        Total time: 976.72s
                               ETA: 9307.1s

################################################################################
                     [1m Learning iteration 380/4000 [0m

                       Computation: 3120 steps/s (collection: 0.486s, learning 2.139s)
               Value function loss: 164.7270
                    Surrogate loss: 0.0169
             Mean action noise std: 0.99
                       Mean reward: 377.58
               Mean episode length: 255.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3121152
                    Iteration time: 2.63s
                        Total time: 979.35s
                               ETA: 9305.1s

################################################################################
                     [1m Learning iteration 381/4000 [0m

                       Computation: 3174 steps/s (collection: 0.532s, learning 2.048s)
               Value function loss: 166.5633
                    Surrogate loss: 0.0135
             Mean action noise std: 0.99
                       Mean reward: 368.41
               Mean episode length: 250.21
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 2.58s
                        Total time: 981.93s
                               ETA: 9302.6s

################################################################################
                     [1m Learning iteration 382/4000 [0m

                       Computation: 3181 steps/s (collection: 0.504s, learning 2.071s)
               Value function loss: 190.6071
                    Surrogate loss: 0.0152
             Mean action noise std: 0.99
                       Mean reward: 366.65
               Mean episode length: 249.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3137536
                    Iteration time: 2.57s
                        Total time: 984.50s
                               ETA: 9300.1s

################################################################################
                     [1m Learning iteration 383/4000 [0m

                       Computation: 3177 steps/s (collection: 0.469s, learning 2.109s)
               Value function loss: 163.2800
                    Surrogate loss: 0.0159
             Mean action noise std: 0.99
                       Mean reward: 369.40
               Mean episode length: 252.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 2.58s
                        Total time: 987.08s
                               ETA: 9297.6s

################################################################################
                     [1m Learning iteration 384/4000 [0m

                       Computation: 3209 steps/s (collection: 0.515s, learning 2.037s)
               Value function loss: 215.5439
                    Surrogate loss: 0.0156
             Mean action noise std: 0.99
                       Mean reward: 353.75
               Mean episode length: 245.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 3153920
                    Iteration time: 2.55s
                        Total time: 989.63s
                               ETA: 9294.8s

################################################################################
                     [1m Learning iteration 385/4000 [0m

                       Computation: 3260 steps/s (collection: 0.450s, learning 2.063s)
               Value function loss: 105.6896
                    Surrogate loss: 0.0187
             Mean action noise std: 0.99
                       Mean reward: 355.66
               Mean episode length: 246.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 2.51s
                        Total time: 992.14s
                               ETA: 9291.7s

################################################################################
                     [1m Learning iteration 386/4000 [0m

                       Computation: 3234 steps/s (collection: 0.491s, learning 2.042s)
               Value function loss: 154.3828
                    Surrogate loss: 0.0175
             Mean action noise std: 0.99
                       Mean reward: 358.55
               Mean episode length: 247.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3170304
                    Iteration time: 2.53s
                        Total time: 994.68s
                               ETA: 9288.8s

################################################################################
                     [1m Learning iteration 387/4000 [0m

                       Computation: 3125 steps/s (collection: 0.493s, learning 2.128s)
               Value function loss: 175.0332
                    Surrogate loss: 0.0153
             Mean action noise std: 0.99
                       Mean reward: 355.79
               Mean episode length: 246.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 2.62s
                        Total time: 997.30s
                               ETA: 9286.7s

################################################################################
                     [1m Learning iteration 388/4000 [0m

                       Computation: 3129 steps/s (collection: 0.533s, learning 2.084s)
               Value function loss: 184.7399
                    Surrogate loss: 0.0170
             Mean action noise std: 0.99
                       Mean reward: 350.26
               Mean episode length: 243.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3186688
                    Iteration time: 2.62s
                        Total time: 999.92s
                               ETA: 9284.6s

################################################################################
                     [1m Learning iteration 389/4000 [0m

                       Computation: 3158 steps/s (collection: 0.531s, learning 2.062s)
               Value function loss: 285.8107
                    Surrogate loss: 0.0149
             Mean action noise std: 0.99
                       Mean reward: 346.87
               Mean episode length: 237.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 2.59s
                        Total time: 1002.51s
                               ETA: 9282.2s

################################################################################
                     [1m Learning iteration 390/4000 [0m

                       Computation: 3248 steps/s (collection: 0.455s, learning 2.067s)
               Value function loss: 235.6148
                    Surrogate loss: 0.0118
             Mean action noise std: 0.99
                       Mean reward: 359.81
               Mean episode length: 243.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3203072
                    Iteration time: 2.52s
                        Total time: 1005.03s
                               ETA: 9279.2s

################################################################################
                     [1m Learning iteration 391/4000 [0m

                       Computation: 3195 steps/s (collection: 0.489s, learning 2.074s)
               Value function loss: 179.4308
                    Surrogate loss: 0.0170
             Mean action noise std: 0.99
                       Mean reward: 346.30
               Mean episode length: 236.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 2.56s
                        Total time: 1007.59s
                               ETA: 9276.5s

################################################################################
                     [1m Learning iteration 392/4000 [0m

                       Computation: 3152 steps/s (collection: 0.559s, learning 2.039s)
               Value function loss: 168.0104
                    Surrogate loss: 0.0160
             Mean action noise std: 0.99
                       Mean reward: 332.69
               Mean episode length: 224.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3219456
                    Iteration time: 2.60s
                        Total time: 1010.19s
                               ETA: 9274.2s

################################################################################
                     [1m Learning iteration 393/4000 [0m

                       Computation: 3131 steps/s (collection: 0.545s, learning 2.071s)
               Value function loss: 248.9945
                    Surrogate loss: 0.0135
             Mean action noise std: 0.99
                       Mean reward: 339.13
               Mean episode length: 231.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 2.62s
                        Total time: 1012.81s
                               ETA: 9272.1s

################################################################################
                     [1m Learning iteration 394/4000 [0m

                       Computation: 3167 steps/s (collection: 0.508s, learning 2.078s)
               Value function loss: 145.9181
                    Surrogate loss: 0.0154
             Mean action noise std: 0.99
                       Mean reward: 342.33
               Mean episode length: 234.34
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3235840
                    Iteration time: 2.59s
                        Total time: 1015.39s
                               ETA: 9269.7s

################################################################################
                     [1m Learning iteration 395/4000 [0m

                       Computation: 3233 steps/s (collection: 0.469s, learning 2.064s)
               Value function loss: 149.8448
                    Surrogate loss: 0.0167
             Mean action noise std: 0.99
                       Mean reward: 329.10
               Mean episode length: 225.33
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 2.53s
                        Total time: 1017.93s
                               ETA: 9266.7s

################################################################################
                     [1m Learning iteration 396/4000 [0m

                       Computation: 3183 steps/s (collection: 0.469s, learning 2.104s)
               Value function loss: 189.7705
                    Surrogate loss: 0.0141
             Mean action noise std: 0.99
                       Mean reward: 334.91
               Mean episode length: 228.03
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3252224
                    Iteration time: 2.57s
                        Total time: 1020.50s
                               ETA: 9264.2s

################################################################################
                     [1m Learning iteration 397/4000 [0m

                       Computation: 3249 steps/s (collection: 0.468s, learning 2.053s)
               Value function loss: 136.3036
                    Surrogate loss: 0.0193
             Mean action noise std: 0.99
                       Mean reward: 323.93
               Mean episode length: 219.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 2.52s
                        Total time: 1023.02s
                               ETA: 9261.2s

################################################################################
                     [1m Learning iteration 398/4000 [0m

                       Computation: 3250 steps/s (collection: 0.458s, learning 2.063s)
               Value function loss: 171.9598
                    Surrogate loss: 0.0188
             Mean action noise std: 0.99
                       Mean reward: 335.09
               Mean episode length: 225.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3268608
                    Iteration time: 2.52s
                        Total time: 1025.54s
                               ETA: 9258.2s

################################################################################
                     [1m Learning iteration 399/4000 [0m

                       Computation: 3224 steps/s (collection: 0.470s, learning 2.070s)
               Value function loss: 182.1110
                    Surrogate loss: 0.0150
             Mean action noise std: 0.99
                       Mean reward: 332.92
               Mean episode length: 225.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 2.54s
                        Total time: 1028.08s
                               ETA: 9255.3s

################################################################################
                     [1m Learning iteration 400/4000 [0m

                       Computation: 3285 steps/s (collection: 0.438s, learning 2.055s)
               Value function loss: 182.5228
                    Surrogate loss: 0.0128
             Mean action noise std: 0.99
                       Mean reward: 329.48
               Mean episode length: 223.14
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3284992
                    Iteration time: 2.49s
                        Total time: 1030.58s
                               ETA: 9252.1s

################################################################################
                     [1m Learning iteration 401/4000 [0m

                       Computation: 3243 steps/s (collection: 0.444s, learning 2.082s)
               Value function loss: 133.9369
                    Surrogate loss: 0.0149
             Mean action noise std: 0.99
                       Mean reward: 331.36
               Mean episode length: 224.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 2.53s
                        Total time: 1033.10s
                               ETA: 9249.1s

################################################################################
                     [1m Learning iteration 402/4000 [0m

                       Computation: 3245 steps/s (collection: 0.458s, learning 2.066s)
               Value function loss: 200.3917
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 340.01
               Mean episode length: 230.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3301376
                    Iteration time: 2.52s
                        Total time: 1035.63s
                               ETA: 9246.1s

################################################################################
                     [1m Learning iteration 403/4000 [0m

                       Computation: 3278 steps/s (collection: 0.445s, learning 2.054s)
               Value function loss: 158.2567
                    Surrogate loss: 0.0156
             Mean action noise std: 0.99
                       Mean reward: 331.12
               Mean episode length: 224.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 2.50s
                        Total time: 1038.13s
                               ETA: 9242.9s

################################################################################
                     [1m Learning iteration 404/4000 [0m

                       Computation: 3240 steps/s (collection: 0.460s, learning 2.068s)
               Value function loss: 338.9676
                    Surrogate loss: 0.0123
             Mean action noise std: 0.99
                       Mean reward: 364.61
               Mean episode length: 241.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3317760
                    Iteration time: 2.53s
                        Total time: 1040.65s
                               ETA: 9240.0s

################################################################################
                     [1m Learning iteration 405/4000 [0m

                       Computation: 3219 steps/s (collection: 0.466s, learning 2.078s)
               Value function loss: 264.2636
                    Surrogate loss: 0.0125
             Mean action noise std: 0.99
                       Mean reward: 372.90
               Mean episode length: 247.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 2.54s
                        Total time: 1043.20s
                               ETA: 9237.2s

################################################################################
                     [1m Learning iteration 406/4000 [0m

                       Computation: 3209 steps/s (collection: 0.470s, learning 2.082s)
               Value function loss: 215.4283
                    Surrogate loss: 0.0136
             Mean action noise std: 0.99
                       Mean reward: 373.27
               Mean episode length: 243.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3334144
                    Iteration time: 2.55s
                        Total time: 1045.75s
                               ETA: 9234.5s

################################################################################
                     [1m Learning iteration 407/4000 [0m

                       Computation: 3230 steps/s (collection: 0.487s, learning 2.049s)
               Value function loss: 239.2202
                    Surrogate loss: 0.0125
             Mean action noise std: 0.99
                       Mean reward: 392.61
               Mean episode length: 251.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 2.54s
                        Total time: 1048.29s
                               ETA: 9231.6s

################################################################################
                     [1m Learning iteration 408/4000 [0m

                       Computation: 3219 steps/s (collection: 0.469s, learning 2.076s)
               Value function loss: 216.0094
                    Surrogate loss: 0.0141
             Mean action noise std: 0.99
                       Mean reward: 400.30
               Mean episode length: 258.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3350528
                    Iteration time: 2.54s
                        Total time: 1050.83s
                               ETA: 9228.8s

################################################################################
                     [1m Learning iteration 409/4000 [0m

                       Computation: 3248 steps/s (collection: 0.457s, learning 2.064s)
               Value function loss: 215.5379
                    Surrogate loss: 0.0184
             Mean action noise std: 0.99
                       Mean reward: 418.93
               Mean episode length: 274.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 2.52s
                        Total time: 1053.35s
                               ETA: 9225.8s

################################################################################
                     [1m Learning iteration 410/4000 [0m

                       Computation: 3201 steps/s (collection: 0.510s, learning 2.049s)
               Value function loss: 144.1694
                    Surrogate loss: 0.0143
             Mean action noise std: 0.99
                       Mean reward: 418.66
               Mean episode length: 274.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3366912
                    Iteration time: 2.56s
                        Total time: 1055.91s
                               ETA: 9223.2s

################################################################################
                     [1m Learning iteration 411/4000 [0m

                       Computation: 3259 steps/s (collection: 0.476s, learning 2.037s)
               Value function loss: 191.9038
                    Surrogate loss: 0.0149
             Mean action noise std: 0.99
                       Mean reward: 411.45
               Mean episode length: 270.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 2.51s
                        Total time: 1058.42s
                               ETA: 9220.1s

################################################################################
                     [1m Learning iteration 412/4000 [0m

                       Computation: 3256 steps/s (collection: 0.443s, learning 2.073s)
               Value function loss: 219.4877
                    Surrogate loss: 0.0136
             Mean action noise std: 0.99
                       Mean reward: 415.04
               Mean episode length: 277.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3383296
                    Iteration time: 2.52s
                        Total time: 1060.94s
                               ETA: 9217.1s

################################################################################
                     [1m Learning iteration 413/4000 [0m

                       Computation: 3194 steps/s (collection: 0.478s, learning 2.087s)
               Value function loss: 187.9690
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 414.19
               Mean episode length: 278.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 2.56s
                        Total time: 1063.51s
                               ETA: 9214.5s

################################################################################
                     [1m Learning iteration 414/4000 [0m

                       Computation: 3203 steps/s (collection: 0.500s, learning 2.058s)
               Value function loss: 140.8292
                    Surrogate loss: 0.0172
             Mean action noise std: 0.99
                       Mean reward: 412.72
               Mean episode length: 277.56
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 3399680
                    Iteration time: 2.56s
                        Total time: 1066.06s
                               ETA: 9211.8s

################################################################################
                     [1m Learning iteration 415/4000 [0m

                       Computation: 3223 steps/s (collection: 0.493s, learning 2.049s)
               Value function loss: 287.5783
                    Surrogate loss: 0.0152
             Mean action noise std: 0.99
                       Mean reward: 395.90
               Mean episode length: 264.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 2.54s
                        Total time: 1068.60s
                               ETA: 9209.0s

################################################################################
                     [1m Learning iteration 416/4000 [0m

                       Computation: 3183 steps/s (collection: 0.514s, learning 2.059s)
               Value function loss: 208.3526
                    Surrogate loss: 0.0144
             Mean action noise std: 0.99
                       Mean reward: 383.84
               Mean episode length: 257.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3416064
                    Iteration time: 2.57s
                        Total time: 1071.18s
                               ETA: 9206.5s

################################################################################
                     [1m Learning iteration 417/4000 [0m

                       Computation: 3170 steps/s (collection: 0.527s, learning 2.057s)
               Value function loss: 202.7029
                    Surrogate loss: 0.0157
             Mean action noise std: 0.99
                       Mean reward: 372.92
               Mean episode length: 251.37
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 2.58s
                        Total time: 1073.76s
                               ETA: 9204.0s

################################################################################
                     [1m Learning iteration 418/4000 [0m

                       Computation: 3163 steps/s (collection: 0.523s, learning 2.067s)
               Value function loss: 162.8933
                    Surrogate loss: 0.0190
             Mean action noise std: 0.99
                       Mean reward: 380.76
               Mean episode length: 254.20
                 Mean success rate: 0.50
                  Mean reward/step: 1.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3432448
                    Iteration time: 2.59s
                        Total time: 1076.35s
                               ETA: 9201.6s

################################################################################
                     [1m Learning iteration 419/4000 [0m

                       Computation: 3212 steps/s (collection: 0.495s, learning 2.055s)
               Value function loss: 239.3733
                    Surrogate loss: 0.0167
             Mean action noise std: 0.99
                       Mean reward: 370.57
               Mean episode length: 247.88
                 Mean success rate: 0.50
                  Mean reward/step: 1.50
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 2.55s
                        Total time: 1078.90s
                               ETA: 9198.9s

################################################################################
                     [1m Learning iteration 420/4000 [0m

                       Computation: 3208 steps/s (collection: 0.461s, learning 2.092s)
               Value function loss: 206.9985
                    Surrogate loss: 0.0159
             Mean action noise std: 0.99
                       Mean reward: 384.67
               Mean episode length: 255.95
                 Mean success rate: 0.50
                  Mean reward/step: 1.50
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3448832
                    Iteration time: 2.55s
                        Total time: 1081.45s
                               ETA: 9196.2s

################################################################################
                     [1m Learning iteration 421/4000 [0m

                       Computation: 3196 steps/s (collection: 0.453s, learning 2.110s)
               Value function loss: 280.0133
                    Surrogate loss: 0.0140
             Mean action noise std: 0.99
                       Mean reward: 389.79
               Mean episode length: 257.04
                 Mean success rate: 0.50
                  Mean reward/step: 1.53
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 2.56s
                        Total time: 1084.02s
                               ETA: 9193.6s

################################################################################
                     [1m Learning iteration 422/4000 [0m

                       Computation: 3229 steps/s (collection: 0.460s, learning 2.077s)
               Value function loss: 200.6290
                    Surrogate loss: 0.0149
             Mean action noise std: 0.98
                       Mean reward: 391.12
               Mean episode length: 257.56
                 Mean success rate: 0.50
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3465216
                    Iteration time: 2.54s
                        Total time: 1086.55s
                               ETA: 9190.8s

################################################################################
                     [1m Learning iteration 423/4000 [0m

                       Computation: 3206 steps/s (collection: 0.451s, learning 2.103s)
               Value function loss: 237.4336
                    Surrogate loss: 0.0118
             Mean action noise std: 0.98
                       Mean reward: 378.15
               Mean episode length: 250.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 2.55s
                        Total time: 1089.11s
                               ETA: 9188.1s

################################################################################
                     [1m Learning iteration 424/4000 [0m

                       Computation: 3272 steps/s (collection: 0.440s, learning 2.064s)
               Value function loss: 318.3063
                    Surrogate loss: 0.0161
             Mean action noise std: 0.99
                       Mean reward: 395.47
               Mean episode length: 259.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3481600
                    Iteration time: 2.50s
                        Total time: 1091.61s
                               ETA: 9184.9s

################################################################################
                     [1m Learning iteration 425/4000 [0m

                       Computation: 3168 steps/s (collection: 0.461s, learning 2.124s)
               Value function loss: 281.6076
                    Surrogate loss: 0.0159
             Mean action noise std: 0.99
                       Mean reward: 382.92
               Mean episode length: 250.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 2.59s
                        Total time: 1094.20s
                               ETA: 9182.5s

################################################################################
                     [1m Learning iteration 426/4000 [0m

                       Computation: 3111 steps/s (collection: 0.486s, learning 2.148s)
               Value function loss: 230.1633
                    Surrogate loss: 0.0148
             Mean action noise std: 0.99
                       Mean reward: 373.81
               Mean episode length: 244.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3497984
                    Iteration time: 2.63s
                        Total time: 1096.83s
                               ETA: 9180.5s

################################################################################
                     [1m Learning iteration 427/4000 [0m

                       Computation: 3262 steps/s (collection: 0.446s, learning 2.065s)
               Value function loss: 229.8406
                    Surrogate loss: 0.0185
             Mean action noise std: 0.99
                       Mean reward: 399.02
               Mean episode length: 260.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 2.51s
                        Total time: 1099.34s
                               ETA: 9177.4s

################################################################################
                     [1m Learning iteration 428/4000 [0m

                       Computation: 3114 steps/s (collection: 0.493s, learning 2.137s)
               Value function loss: 208.2512
                    Surrogate loss: 0.0163
             Mean action noise std: 0.99
                       Mean reward: 372.07
               Mean episode length: 243.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3514368
                    Iteration time: 2.63s
                        Total time: 1101.97s
                               ETA: 9175.4s

################################################################################
                     [1m Learning iteration 429/4000 [0m

                       Computation: 3262 steps/s (collection: 0.446s, learning 2.065s)
               Value function loss: 169.0739
                    Surrogate loss: 0.0180
             Mean action noise std: 0.98
                       Mean reward: 377.41
               Mean episode length: 247.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 2.51s
                        Total time: 1104.48s
                               ETA: 9172.3s

################################################################################
                     [1m Learning iteration 430/4000 [0m

                       Computation: 3086 steps/s (collection: 0.507s, learning 2.147s)
               Value function loss: 310.2524
                    Surrogate loss: 0.0153
             Mean action noise std: 0.98
                       Mean reward: 364.38
               Mean episode length: 238.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3530752
                    Iteration time: 2.65s
                        Total time: 1107.14s
                               ETA: 9170.5s

################################################################################
                     [1m Learning iteration 431/4000 [0m

                       Computation: 3190 steps/s (collection: 0.479s, learning 2.088s)
               Value function loss: 261.2105
                    Surrogate loss: 0.0150
             Mean action noise std: 0.98
                       Mean reward: 348.79
               Mean episode length: 226.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 2.57s
                        Total time: 1109.70s
                               ETA: 9167.9s

################################################################################
                     [1m Learning iteration 432/4000 [0m

                       Computation: 3134 steps/s (collection: 0.513s, learning 2.101s)
               Value function loss: 254.6129
                    Surrogate loss: 0.0142
             Mean action noise std: 0.98
                       Mean reward: 331.37
               Mean episode length: 216.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3547136
                    Iteration time: 2.61s
                        Total time: 1112.32s
                               ETA: 9165.7s

################################################################################
                     [1m Learning iteration 433/4000 [0m

                       Computation: 3174 steps/s (collection: 0.481s, learning 2.099s)
               Value function loss: 296.5489
                    Surrogate loss: 0.0142
             Mean action noise std: 0.99
                       Mean reward: 336.07
               Mean episode length: 217.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 2.58s
                        Total time: 1114.90s
                               ETA: 9163.2s

################################################################################
                     [1m Learning iteration 434/4000 [0m

                       Computation: 3001 steps/s (collection: 0.583s, learning 2.146s)
               Value function loss: 302.3445
                    Surrogate loss: 0.0165
             Mean action noise std: 0.98
                       Mean reward: 355.19
               Mean episode length: 225.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3563520
                    Iteration time: 2.73s
                        Total time: 1117.63s
                               ETA: 9162.0s

################################################################################
                     [1m Learning iteration 435/4000 [0m

                       Computation: 3080 steps/s (collection: 0.534s, learning 2.125s)
               Value function loss: 337.4700
                    Surrogate loss: 0.0104
             Mean action noise std: 0.98
                       Mean reward: 372.11
               Mean episode length: 233.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 2.66s
                        Total time: 1120.29s
                               ETA: 9160.1s

################################################################################
                     [1m Learning iteration 436/4000 [0m

                       Computation: 3171 steps/s (collection: 0.500s, learning 2.083s)
               Value function loss: 372.5050
                    Surrogate loss: 0.0143
             Mean action noise std: 0.99
                       Mean reward: 375.40
               Mean episode length: 234.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3579904
                    Iteration time: 2.58s
                        Total time: 1122.87s
                               ETA: 9157.7s

################################################################################
                     [1m Learning iteration 437/4000 [0m

                       Computation: 3084 steps/s (collection: 0.491s, learning 2.166s)
               Value function loss: 312.7769
                    Surrogate loss: 0.0155
             Mean action noise std: 0.99
                       Mean reward: 396.64
               Mean episode length: 247.54
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 2.66s
                        Total time: 1125.53s
                               ETA: 9155.8s

################################################################################
                     [1m Learning iteration 438/4000 [0m

                       Computation: 3032 steps/s (collection: 0.523s, learning 2.178s)
               Value function loss: 329.1273
                    Surrogate loss: 0.0138
             Mean action noise std: 0.98
                       Mean reward: 432.83
               Mean episode length: 264.69
                 Mean success rate: 0.50
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3596288
                    Iteration time: 2.70s
                        Total time: 1128.23s
                               ETA: 9154.3s

################################################################################
                     [1m Learning iteration 439/4000 [0m

                       Computation: 3036 steps/s (collection: 0.550s, learning 2.148s)
               Value function loss: 280.2444
                    Surrogate loss: 0.0148
             Mean action noise std: 0.99
                       Mean reward: 448.04
               Mean episode length: 272.92
                 Mean success rate: 1.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 2.70s
                        Total time: 1130.93s
                               ETA: 9152.8s

################################################################################
                     [1m Learning iteration 440/4000 [0m

                       Computation: 3078 steps/s (collection: 0.507s, learning 2.153s)
               Value function loss: 300.4815
                    Surrogate loss: 0.0132
             Mean action noise std: 0.99
                       Mean reward: 434.83
               Mean episode length: 266.26
                 Mean success rate: 1.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3612672
                    Iteration time: 2.66s
                        Total time: 1133.59s
                               ETA: 9150.9s

################################################################################
                     [1m Learning iteration 441/4000 [0m

                       Computation: 3083 steps/s (collection: 0.474s, learning 2.183s)
               Value function loss: 294.8595
                    Surrogate loss: 0.0153
             Mean action noise std: 0.99
                       Mean reward: 438.49
               Mean episode length: 269.42
                 Mean success rate: 1.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 2.66s
                        Total time: 1136.24s
                               ETA: 9149.1s

################################################################################
                     [1m Learning iteration 442/4000 [0m

                       Computation: 3087 steps/s (collection: 0.530s, learning 2.124s)
               Value function loss: 279.0587
                    Surrogate loss: 0.0150
             Mean action noise std: 0.99
                       Mean reward: 430.64
               Mean episode length: 266.35
                 Mean success rate: 1.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3629056
                    Iteration time: 2.65s
                        Total time: 1138.90s
                               ETA: 9147.2s

################################################################################
                     [1m Learning iteration 443/4000 [0m

                       Computation: 3237 steps/s (collection: 0.477s, learning 2.054s)
               Value function loss: 305.0133
                    Surrogate loss: 0.0115
             Mean action noise std: 0.99
                       Mean reward: 423.82
               Mean episode length: 260.57
                 Mean success rate: 1.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 2.53s
                        Total time: 1141.43s
                               ETA: 9144.3s

################################################################################
                     [1m Learning iteration 444/4000 [0m

                       Computation: 3207 steps/s (collection: 0.484s, learning 2.070s)
               Value function loss: 325.8960
                    Surrogate loss: 0.0118
             Mean action noise std: 0.99
                       Mean reward: 436.60
               Mean episode length: 263.00
                 Mean success rate: 1.50
                  Mean reward/step: 1.59
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3645440
                    Iteration time: 2.55s
                        Total time: 1143.98s
                               ETA: 9141.6s

################################################################################
                     [1m Learning iteration 445/4000 [0m

                       Computation: 3251 steps/s (collection: 0.472s, learning 2.048s)
               Value function loss: 322.7938
                    Surrogate loss: 0.0127
             Mean action noise std: 0.99
                       Mean reward: 423.10
               Mean episode length: 256.31
                 Mean success rate: 1.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 2.52s
                        Total time: 1146.50s
                               ETA: 9138.6s

################################################################################
                     [1m Learning iteration 446/4000 [0m

                       Computation: 3180 steps/s (collection: 0.496s, learning 2.080s)
               Value function loss: 368.1579
                    Surrogate loss: 0.0143
             Mean action noise std: 0.99
                       Mean reward: 456.65
               Mean episode length: 275.21
                 Mean success rate: 1.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3661824
                    Iteration time: 2.58s
                        Total time: 1149.07s
                               ETA: 9136.0s

################################################################################
                     [1m Learning iteration 447/4000 [0m

                       Computation: 3162 steps/s (collection: 0.510s, learning 2.080s)
               Value function loss: 259.7762
                    Surrogate loss: 0.0164
             Mean action noise std: 0.99
                       Mean reward: 466.88
               Mean episode length: 283.69
                 Mean success rate: 1.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 2.59s
                        Total time: 1151.67s
                               ETA: 9133.6s

################################################################################
                     [1m Learning iteration 448/4000 [0m

                       Computation: 3143 steps/s (collection: 0.494s, learning 2.113s)
               Value function loss: 381.5570
                    Surrogate loss: 0.0106
             Mean action noise std: 0.99
                       Mean reward: 459.06
               Mean episode length: 274.41
                 Mean success rate: 1.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3678208
                    Iteration time: 2.61s
                        Total time: 1154.27s
                               ETA: 9131.3s

################################################################################
                     [1m Learning iteration 449/4000 [0m

                       Computation: 3170 steps/s (collection: 0.492s, learning 2.092s)
               Value function loss: 181.8228
                    Surrogate loss: 0.0167
             Mean action noise std: 0.98
                       Mean reward: 460.64
               Mean episode length: 275.32
                 Mean success rate: 1.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 2.58s
                        Total time: 1156.86s
                               ETA: 9128.9s

################################################################################
                     [1m Learning iteration 450/4000 [0m

                       Computation: 3216 steps/s (collection: 0.504s, learning 2.043s)
               Value function loss: 288.7123
                    Surrogate loss: 0.0155
             Mean action noise std: 0.98
                       Mean reward: 471.93
               Mean episode length: 284.46
                 Mean success rate: 1.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3694592
                    Iteration time: 2.55s
                        Total time: 1159.40s
                               ETA: 9126.1s

################################################################################
                     [1m Learning iteration 451/4000 [0m

                       Computation: 3186 steps/s (collection: 0.507s, learning 2.064s)
               Value function loss: 379.3747
                    Surrogate loss: 0.0137
             Mean action noise std: 0.98
                       Mean reward: 457.62
               Mean episode length: 278.49
                 Mean success rate: 1.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 2.57s
                        Total time: 1161.97s
                               ETA: 9123.5s

################################################################################
                     [1m Learning iteration 452/4000 [0m

                       Computation: 3194 steps/s (collection: 0.502s, learning 2.062s)
               Value function loss: 449.9739
                    Surrogate loss: 0.0132
             Mean action noise std: 0.98
                       Mean reward: 437.39
               Mean episode length: 263.42
                 Mean success rate: 1.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 3710976
                    Iteration time: 2.56s
                        Total time: 1164.54s
                               ETA: 9120.9s

################################################################################
                     [1m Learning iteration 453/4000 [0m

                       Computation: 3171 steps/s (collection: 0.533s, learning 2.049s)
               Value function loss: 460.9582
                    Surrogate loss: 0.0165
             Mean action noise std: 0.98
                       Mean reward: 420.39
               Mean episode length: 253.09
                 Mean success rate: 1.50
                  Mean reward/step: 1.75
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 2.58s
                        Total time: 1167.12s
                               ETA: 9118.4s

################################################################################
                     [1m Learning iteration 454/4000 [0m

                       Computation: 3161 steps/s (collection: 0.486s, learning 2.105s)
               Value function loss: 649.7948
                    Surrogate loss: 0.0118
             Mean action noise std: 0.98
                       Mean reward: 421.59
               Mean episode length: 256.12
                 Mean success rate: 1.50
                  Mean reward/step: 1.80
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3727360
                    Iteration time: 2.59s
                        Total time: 1169.71s
                               ETA: 9116.0s

################################################################################
                     [1m Learning iteration 455/4000 [0m

                       Computation: 3163 steps/s (collection: 0.505s, learning 2.085s)
               Value function loss: 419.0722
                    Surrogate loss: 0.0128
             Mean action noise std: 0.98
                       Mean reward: 428.46
               Mean episode length: 259.44
                 Mean success rate: 1.50
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 2.59s
                        Total time: 1172.30s
                               ETA: 9113.6s

################################################################################
                     [1m Learning iteration 456/4000 [0m

                       Computation: 3127 steps/s (collection: 0.520s, learning 2.099s)
               Value function loss: 264.4525
                    Surrogate loss: 0.0138
             Mean action noise std: 0.98
                       Mean reward: 448.64
               Mean episode length: 272.91
                 Mean success rate: 1.50
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3743744
                    Iteration time: 2.62s
                        Total time: 1174.92s
                               ETA: 9111.4s

################################################################################
                     [1m Learning iteration 457/4000 [0m

                       Computation: 3144 steps/s (collection: 0.486s, learning 2.119s)
               Value function loss: 437.2548
                    Surrogate loss: 0.0162
             Mean action noise std: 0.98
                       Mean reward: 450.97
               Mean episode length: 277.92
                 Mean success rate: 1.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 2.60s
                        Total time: 1177.52s
                               ETA: 9109.1s

################################################################################
                     [1m Learning iteration 458/4000 [0m

                       Computation: 3117 steps/s (collection: 0.525s, learning 2.103s)
               Value function loss: 388.7656
                    Surrogate loss: 0.0132
             Mean action noise std: 0.98
                       Mean reward: 437.92
               Mean episode length: 274.52
                 Mean success rate: 0.50
                  Mean reward/step: 1.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 3760128
                    Iteration time: 2.63s
                        Total time: 1180.15s
                               ETA: 9107.0s

################################################################################
                     [1m Learning iteration 459/4000 [0m

                       Computation: 3135 steps/s (collection: 0.493s, learning 2.120s)
               Value function loss: 503.1345
                    Surrogate loss: 0.0122
             Mean action noise std: 0.98
                       Mean reward: 442.10
               Mean episode length: 275.12
                 Mean success rate: 0.50
                  Mean reward/step: 1.70
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 2.61s
                        Total time: 1182.77s
                               ETA: 9104.7s

################################################################################
                     [1m Learning iteration 460/4000 [0m

                       Computation: 3088 steps/s (collection: 0.557s, learning 2.096s)
               Value function loss: 383.8588
                    Surrogate loss: 0.0129
             Mean action noise std: 0.98
                       Mean reward: 433.36
               Mean episode length: 272.25
                 Mean success rate: 0.50
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3776512
                    Iteration time: 2.65s
                        Total time: 1185.42s
                               ETA: 9102.8s

################################################################################
                     [1m Learning iteration 461/4000 [0m

                       Computation: 3134 steps/s (collection: 0.480s, learning 2.133s)
               Value function loss: 455.4128
                    Surrogate loss: 0.0118
             Mean action noise std: 0.98
                       Mean reward: 440.73
               Mean episode length: 276.90
                 Mean success rate: 0.50
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 2.61s
                        Total time: 1188.03s
                               ETA: 9100.5s

################################################################################
                     [1m Learning iteration 462/4000 [0m

                       Computation: 3094 steps/s (collection: 0.528s, learning 2.120s)
               Value function loss: 590.8715
                    Surrogate loss: 0.0128
             Mean action noise std: 0.98
                       Mean reward: 431.32
               Mean episode length: 270.37
                 Mean success rate: 0.50
                  Mean reward/step: 1.79
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3792896
                    Iteration time: 2.65s
                        Total time: 1190.68s
                               ETA: 9098.5s

################################################################################
                     [1m Learning iteration 463/4000 [0m

                       Computation: 3096 steps/s (collection: 0.510s, learning 2.136s)
               Value function loss: 449.4334
                    Surrogate loss: 0.0155
             Mean action noise std: 0.98
                       Mean reward: 433.03
               Mean episode length: 267.01
                 Mean success rate: 1.50
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 2.65s
                        Total time: 1193.32s
                               ETA: 9096.5s

################################################################################
                     [1m Learning iteration 464/4000 [0m

                       Computation: 3067 steps/s (collection: 0.501s, learning 2.170s)
               Value function loss: 410.3624
                    Surrogate loss: 0.0140
             Mean action noise std: 0.98
                       Mean reward: 440.14
               Mean episode length: 270.98
                 Mean success rate: 1.50
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3809280
                    Iteration time: 2.67s
                        Total time: 1196.00s
                               ETA: 9094.7s

################################################################################
                     [1m Learning iteration 465/4000 [0m

                       Computation: 3123 steps/s (collection: 0.508s, learning 2.115s)
               Value function loss: 392.0692
                    Surrogate loss: 0.0127
             Mean action noise std: 0.98
                       Mean reward: 431.86
               Mean episode length: 266.16
                 Mean success rate: 1.50
                  Mean reward/step: 1.75
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 2.62s
                        Total time: 1198.62s
                               ETA: 9092.5s

################################################################################
                     [1m Learning iteration 466/4000 [0m

                       Computation: 3112 steps/s (collection: 0.510s, learning 2.122s)
               Value function loss: 359.7367
                    Surrogate loss: 0.0130
             Mean action noise std: 0.98
                       Mean reward: 428.70
               Mean episode length: 265.70
                 Mean success rate: 1.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3825664
                    Iteration time: 2.63s
                        Total time: 1201.25s
                               ETA: 9090.4s

################################################################################
                     [1m Learning iteration 467/4000 [0m

                       Computation: 2851 steps/s (collection: 0.758s, learning 2.116s)
               Value function loss: 374.4627
                    Surrogate loss: 0.0164
             Mean action noise std: 0.98
                       Mean reward: 439.53
               Mean episode length: 268.12
                 Mean success rate: 1.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 2.87s
                        Total time: 1204.12s
                               ETA: 9090.1s

################################################################################
                     [1m Learning iteration 468/4000 [0m

                       Computation: 3040 steps/s (collection: 0.570s, learning 2.125s)
               Value function loss: 392.6871
                    Surrogate loss: 0.0160
             Mean action noise std: 0.98
                       Mean reward: 445.53
               Mean episode length: 264.19
                 Mean success rate: 1.50
                  Mean reward/step: 1.60
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3842048
                    Iteration time: 2.69s
                        Total time: 1206.82s
                               ETA: 9088.4s

################################################################################
                     [1m Learning iteration 469/4000 [0m

                       Computation: 3252 steps/s (collection: 0.461s, learning 2.057s)
               Value function loss: 406.5825
                    Surrogate loss: 0.0148
             Mean action noise std: 0.98
                       Mean reward: 458.25
               Mean episode length: 267.61
                 Mean success rate: 2.50
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 2.52s
                        Total time: 1209.34s
                               ETA: 9085.5s

################################################################################
                     [1m Learning iteration 470/4000 [0m

                       Computation: 3164 steps/s (collection: 0.480s, learning 2.108s)
               Value function loss: 400.6384
                    Surrogate loss: 0.0153
             Mean action noise std: 0.98
                       Mean reward: 453.59
               Mean episode length: 264.21
                 Mean success rate: 1.50
                  Mean reward/step: 1.66
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3858432
                    Iteration time: 2.59s
                        Total time: 1211.92s
                               ETA: 9083.0s

################################################################################
                     [1m Learning iteration 471/4000 [0m

                       Computation: 3136 steps/s (collection: 0.499s, learning 2.113s)
               Value function loss: 272.0734
                    Surrogate loss: 0.0147
             Mean action noise std: 0.98
                       Mean reward: 472.36
               Mean episode length: 267.38
                 Mean success rate: 2.50
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 2.61s
                        Total time: 1214.54s
                               ETA: 9080.7s

################################################################################
                     [1m Learning iteration 472/4000 [0m

                       Computation: 2968 steps/s (collection: 0.545s, learning 2.214s)
               Value function loss: 339.6411
                    Surrogate loss: 0.0150
             Mean action noise std: 0.98
                       Mean reward: 489.55
               Mean episode length: 274.19
                 Mean success rate: 3.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3874816
                    Iteration time: 2.76s
                        Total time: 1217.30s
                               ETA: 9079.5s

################################################################################
                     [1m Learning iteration 473/4000 [0m

                       Computation: 3155 steps/s (collection: 0.503s, learning 2.093s)
               Value function loss: 277.7614
                    Surrogate loss: 0.0151
             Mean action noise std: 0.98
                       Mean reward: 487.88
               Mean episode length: 273.65
                 Mean success rate: 3.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 2.60s
                        Total time: 1219.89s
                               ETA: 9077.1s

################################################################################
                     [1m Learning iteration 474/4000 [0m

                       Computation: 3126 steps/s (collection: 0.490s, learning 2.131s)
               Value function loss: 238.1142
                    Surrogate loss: 0.0151
             Mean action noise std: 0.98
                       Mean reward: 469.49
               Mean episode length: 272.78
                 Mean success rate: 2.50
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3891200
                    Iteration time: 2.62s
                        Total time: 1222.51s
                               ETA: 9074.9s

################################################################################
                     [1m Learning iteration 475/4000 [0m

                       Computation: 3025 steps/s (collection: 0.514s, learning 2.195s)
               Value function loss: 443.9995
                    Surrogate loss: 0.0137
             Mean action noise std: 0.98
                       Mean reward: 474.12
               Mean episode length: 277.35
                 Mean success rate: 2.50
                  Mean reward/step: 1.73
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 2.71s
                        Total time: 1225.22s
                               ETA: 9073.3s

################################################################################
                     [1m Learning iteration 476/4000 [0m

                       Computation: 3067 steps/s (collection: 0.535s, learning 2.136s)
               Value function loss: 367.9460
                    Surrogate loss: 0.0151
             Mean action noise std: 0.98
                       Mean reward: 496.06
               Mean episode length: 290.32
                 Mean success rate: 1.50
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3907584
                    Iteration time: 2.67s
                        Total time: 1227.89s
                               ETA: 9071.5s

################################################################################
                     [1m Learning iteration 477/4000 [0m

                       Computation: 3086 steps/s (collection: 0.539s, learning 2.115s)
               Value function loss: 336.2176
                    Surrogate loss: 0.0164
             Mean action noise std: 0.98
                       Mean reward: 504.54
               Mean episode length: 298.86
                 Mean success rate: 1.50
                  Mean reward/step: 1.74
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 2.65s
                        Total time: 1230.54s
                               ETA: 9069.5s

################################################################################
                     [1m Learning iteration 478/4000 [0m

                       Computation: 3136 steps/s (collection: 0.517s, learning 2.096s)
               Value function loss: 356.6424
                    Surrogate loss: 0.0156
             Mean action noise std: 0.98
                       Mean reward: 460.33
               Mean episode length: 282.14
                 Mean success rate: 0.50
                  Mean reward/step: 1.62
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 3923968
                    Iteration time: 2.61s
                        Total time: 1233.16s
                               ETA: 9067.2s

################################################################################
                     [1m Learning iteration 479/4000 [0m

                       Computation: 3194 steps/s (collection: 0.474s, learning 2.091s)
               Value function loss: 294.0887
                    Surrogate loss: 0.0158
             Mean action noise std: 0.98
                       Mean reward: 436.94
               Mean episode length: 271.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 2.56s
                        Total time: 1235.72s
                               ETA: 9064.5s

################################################################################
                     [1m Learning iteration 480/4000 [0m

                       Computation: 3131 steps/s (collection: 0.520s, learning 2.097s)
               Value function loss: 301.3564
                    Surrogate loss: 0.0154
             Mean action noise std: 0.98
                       Mean reward: 438.77
               Mean episode length: 273.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3940352
                    Iteration time: 2.62s
                        Total time: 1238.34s
                               ETA: 9062.3s

################################################################################
                     [1m Learning iteration 481/4000 [0m

                       Computation: 3128 steps/s (collection: 0.521s, learning 2.098s)
               Value function loss: 247.9165
                    Surrogate loss: 0.0174
             Mean action noise std: 0.98
                       Mean reward: 428.50
               Mean episode length: 266.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 2.62s
                        Total time: 1240.96s
                               ETA: 9060.0s

################################################################################
                     [1m Learning iteration 482/4000 [0m

                       Computation: 3201 steps/s (collection: 0.475s, learning 2.084s)
               Value function loss: 337.3080
                    Surrogate loss: 0.0147
             Mean action noise std: 0.98
                       Mean reward: 411.66
               Mean episode length: 259.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3956736
                    Iteration time: 2.56s
                        Total time: 1243.52s
                               ETA: 9057.3s

################################################################################
                     [1m Learning iteration 483/4000 [0m

                       Computation: 3159 steps/s (collection: 0.495s, learning 2.098s)
               Value function loss: 534.8591
                    Surrogate loss: 0.0173
             Mean action noise std: 0.98
                       Mean reward: 398.08
               Mean episode length: 253.01
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 2.59s
                        Total time: 1246.11s
                               ETA: 9054.9s

################################################################################
                     [1m Learning iteration 484/4000 [0m

                       Computation: 3184 steps/s (collection: 0.515s, learning 2.057s)
               Value function loss: 425.2783
                    Surrogate loss: 0.0156
             Mean action noise std: 0.98
                       Mean reward: 426.91
               Mean episode length: 268.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3973120
                    Iteration time: 2.57s
                        Total time: 1248.68s
                               ETA: 9052.3s

################################################################################
                     [1m Learning iteration 485/4000 [0m

                       Computation: 3143 steps/s (collection: 0.492s, learning 2.113s)
               Value function loss: 429.4136
                    Surrogate loss: 0.0164
             Mean action noise std: 0.98
                       Mean reward: 428.89
               Mean episode length: 266.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 2.61s
                        Total time: 1251.29s
                               ETA: 9049.9s

################################################################################
                     [1m Learning iteration 486/4000 [0m

                       Computation: 3138 steps/s (collection: 0.521s, learning 2.089s)
               Value function loss: 286.0846
                    Surrogate loss: 0.0166
             Mean action noise std: 0.98
                       Mean reward: 416.01
               Mean episode length: 253.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3989504
                    Iteration time: 2.61s
                        Total time: 1253.90s
                               ETA: 9047.6s

################################################################################
                     [1m Learning iteration 487/4000 [0m

                       Computation: 3179 steps/s (collection: 0.490s, learning 2.087s)
               Value function loss: 465.5965
                    Surrogate loss: 0.0163
             Mean action noise std: 0.98
                       Mean reward: 434.57
               Mean episode length: 259.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 2.58s
                        Total time: 1256.47s
                               ETA: 9045.1s

################################################################################
                     [1m Learning iteration 488/4000 [0m

                       Computation: 3167 steps/s (collection: 0.482s, learning 2.105s)
               Value function loss: 314.4575
                    Surrogate loss: 0.0148
             Mean action noise std: 0.98
                       Mean reward: 432.50
               Mean episode length: 256.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 4005888
                    Iteration time: 2.59s
                        Total time: 1259.06s
                               ETA: 9042.6s

################################################################################
                     [1m Learning iteration 489/4000 [0m

                       Computation: 3077 steps/s (collection: 0.506s, learning 2.156s)
               Value function loss: 427.3620
                    Surrogate loss: 0.0135
             Mean action noise std: 0.98
                       Mean reward: 420.34
               Mean episode length: 244.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 2.66s
                        Total time: 1261.72s
                               ETA: 9040.6s

################################################################################
                     [1m Learning iteration 490/4000 [0m

                       Computation: 3110 steps/s (collection: 0.491s, learning 2.143s)
               Value function loss: 418.5270
                    Surrogate loss: 0.0129
             Mean action noise std: 0.98
                       Mean reward: 429.99
               Mean episode length: 247.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4022272
                    Iteration time: 2.63s
                        Total time: 1264.36s
                               ETA: 9038.5s

################################################################################
                     [1m Learning iteration 491/4000 [0m

                       Computation: 3206 steps/s (collection: 0.477s, learning 2.078s)
               Value function loss: 414.8901
                    Surrogate loss: 0.0145
             Mean action noise std: 0.98
                       Mean reward: 429.68
               Mean episode length: 250.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 2.55s
                        Total time: 1266.91s
                               ETA: 9035.7s

################################################################################
                     [1m Learning iteration 492/4000 [0m

                       Computation: 3129 steps/s (collection: 0.503s, learning 2.114s)
               Value function loss: 381.4631
                    Surrogate loss: 0.0145
             Mean action noise std: 0.98
                       Mean reward: 440.92
               Mean episode length: 251.69
                 Mean success rate: 0.50
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4038656
                    Iteration time: 2.62s
                        Total time: 1269.53s
                               ETA: 9033.5s

################################################################################
                     [1m Learning iteration 493/4000 [0m

                       Computation: 3139 steps/s (collection: 0.509s, learning 2.100s)
               Value function loss: 324.5947
                    Surrogate loss: 0.0127
             Mean action noise std: 0.98
                       Mean reward: 447.46
               Mean episode length: 258.61
                 Mean success rate: 0.50
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 2.61s
                        Total time: 1272.14s
                               ETA: 9031.1s

################################################################################
                     [1m Learning iteration 494/4000 [0m

                       Computation: 3245 steps/s (collection: 0.505s, learning 2.018s)
               Value function loss: 623.6911
                    Surrogate loss: 0.0130
             Mean action noise std: 0.98
                       Mean reward: 465.23
               Mean episode length: 267.20
                 Mean success rate: 0.50
                  Mean reward/step: 1.71
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4055040
                    Iteration time: 2.52s
                        Total time: 1274.66s
                               ETA: 9028.2s

################################################################################
                     [1m Learning iteration 495/4000 [0m

                       Computation: 3160 steps/s (collection: 0.489s, learning 2.103s)
               Value function loss: 530.6764
                    Surrogate loss: 0.0126
             Mean action noise std: 0.98
                       Mean reward: 466.97
               Mean episode length: 272.88
                 Mean success rate: 0.50
                  Mean reward/step: 1.63
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 2.59s
                        Total time: 1277.25s
                               ETA: 9025.7s

################################################################################
                     [1m Learning iteration 496/4000 [0m

                       Computation: 3142 steps/s (collection: 0.493s, learning 2.114s)
               Value function loss: 368.9673
                    Surrogate loss: 0.0158
             Mean action noise std: 0.98
                       Mean reward: 443.07
               Mean episode length: 261.91
                 Mean success rate: 0.50
                  Mean reward/step: 1.66
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4071424
                    Iteration time: 2.61s
                        Total time: 1279.86s
                               ETA: 9023.4s

################################################################################
                     [1m Learning iteration 497/4000 [0m

                       Computation: 3023 steps/s (collection: 0.581s, learning 2.129s)
               Value function loss: 315.3031
                    Surrogate loss: 0.0153
             Mean action noise std: 0.98
                       Mean reward: 447.34
               Mean episode length: 253.28
                 Mean success rate: 1.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 2.71s
                        Total time: 1282.57s
                               ETA: 9021.8s

################################################################################
                     [1m Learning iteration 498/4000 [0m

                       Computation: 3155 steps/s (collection: 0.528s, learning 2.068s)
               Value function loss: 562.2634
                    Surrogate loss: 0.0139
             Mean action noise std: 0.98
                       Mean reward: 438.57
               Mean episode length: 253.91
                 Mean success rate: 0.50
                  Mean reward/step: 1.77
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4087808
                    Iteration time: 2.60s
                        Total time: 1285.17s
                               ETA: 9019.3s

################################################################################
                     [1m Learning iteration 499/4000 [0m

                       Computation: 3206 steps/s (collection: 0.488s, learning 2.067s)
               Value function loss: 715.9205
                    Surrogate loss: 0.0121
             Mean action noise std: 0.98
                       Mean reward: 456.24
               Mean episode length: 260.21
                 Mean success rate: 0.50
                  Mean reward/step: 1.89
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 2.56s
                        Total time: 1287.72s
                               ETA: 9016.6s

################################################################################
                     [1m Learning iteration 500/4000 [0m

                       Computation: 3226 steps/s (collection: 0.483s, learning 2.057s)
               Value function loss: 479.7489
                    Surrogate loss: 0.0136
             Mean action noise std: 0.98
                       Mean reward: 447.92
               Mean episode length: 261.85
                 Mean success rate: 0.50
                  Mean reward/step: 1.95
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4104192
                    Iteration time: 2.54s
                        Total time: 1290.26s
                               ETA: 9013.8s

################################################################################
                     [1m Learning iteration 501/4000 [0m

                       Computation: 3184 steps/s (collection: 0.489s, learning 2.083s)
               Value function loss: 755.0702
                    Surrogate loss: 0.0125
             Mean action noise std: 0.98
                       Mean reward: 445.70
               Mean episode length: 262.82
                 Mean success rate: 0.50
                  Mean reward/step: 1.80
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 2.57s
                        Total time: 1292.83s
                               ETA: 9011.2s

################################################################################
                     [1m Learning iteration 502/4000 [0m

                       Computation: 3231 steps/s (collection: 0.485s, learning 2.051s)
               Value function loss: 574.8113
                    Surrogate loss: 0.0111
             Mean action noise std: 0.98
                       Mean reward: 473.45
               Mean episode length: 267.85
                 Mean success rate: 1.00
                  Mean reward/step: 1.68
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4120576
                    Iteration time: 2.54s
                        Total time: 1295.37s
                               ETA: 9008.3s

################################################################################
                     [1m Learning iteration 503/4000 [0m

                       Computation: 3187 steps/s (collection: 0.490s, learning 2.080s)
               Value function loss: 665.2865
                    Surrogate loss: 0.0129
             Mean action noise std: 0.98
                       Mean reward: 468.17
               Mean episode length: 271.36
                 Mean success rate: 0.50
                  Mean reward/step: 1.83
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 2.57s
                        Total time: 1297.94s
                               ETA: 9005.7s

################################################################################
                     [1m Learning iteration 504/4000 [0m

                       Computation: 3181 steps/s (collection: 0.497s, learning 2.078s)
               Value function loss: 651.4087
                    Surrogate loss: 0.0146
             Mean action noise std: 0.98
                       Mean reward: 465.56
               Mean episode length: 266.52
                 Mean success rate: 1.00
                  Mean reward/step: 1.80
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4136960
                    Iteration time: 2.58s
                        Total time: 1300.51s
                               ETA: 9003.2s

################################################################################
                     [1m Learning iteration 505/4000 [0m

                       Computation: 3149 steps/s (collection: 0.519s, learning 2.082s)
               Value function loss: 835.2438
                    Surrogate loss: 0.0127
             Mean action noise std: 0.98
                       Mean reward: 463.78
               Mean episode length: 264.95
                 Mean success rate: 1.50
                  Mean reward/step: 1.79
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 2.60s
                        Total time: 1303.11s
                               ETA: 9000.8s

################################################################################
                     [1m Learning iteration 506/4000 [0m

                       Computation: 3179 steps/s (collection: 0.461s, learning 2.115s)
               Value function loss: 976.8824
                    Surrogate loss: 0.0129
             Mean action noise std: 0.98
                       Mean reward: 451.07
               Mean episode length: 258.79
                 Mean success rate: 1.50
                  Mean reward/step: 1.99
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4153344
                    Iteration time: 2.58s
                        Total time: 1305.69s
                               ETA: 8998.2s

################################################################################
                     [1m Learning iteration 507/4000 [0m

                       Computation: 3236 steps/s (collection: 0.460s, learning 2.071s)
               Value function loss: 732.9444
                    Surrogate loss: 0.0122
             Mean action noise std: 0.98
                       Mean reward: 441.81
               Mean episode length: 245.16
                 Mean success rate: 2.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 2.53s
                        Total time: 1308.22s
                               ETA: 8995.3s

################################################################################
                     [1m Learning iteration 508/4000 [0m

                       Computation: 3253 steps/s (collection: 0.448s, learning 2.069s)
               Value function loss: 824.9008
                    Surrogate loss: 0.0126
             Mean action noise std: 0.98
                       Mean reward: 445.53
               Mean episode length: 249.84
                 Mean success rate: 2.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4169728
                    Iteration time: 2.52s
                        Total time: 1310.74s
                               ETA: 8992.3s

################################################################################
                     [1m Learning iteration 509/4000 [0m

                       Computation: 3284 steps/s (collection: 0.450s, learning 2.044s)
               Value function loss: 703.7721
                    Surrogate loss: 0.0132
             Mean action noise std: 0.98
                       Mean reward: 445.41
               Mean episode length: 252.49
                 Mean success rate: 2.50
                  Mean reward/step: 1.89
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 2.49s
                        Total time: 1313.23s
                               ETA: 8989.2s

################################################################################
                     [1m Learning iteration 510/4000 [0m

                       Computation: 3147 steps/s (collection: 0.490s, learning 2.113s)
               Value function loss: 976.6554
                    Surrogate loss: 0.0120
             Mean action noise std: 0.98
                       Mean reward: 458.62
               Mean episode length: 256.10
                 Mean success rate: 3.00
                  Mean reward/step: 1.91
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 4186112
                    Iteration time: 2.60s
                        Total time: 1315.84s
                               ETA: 8986.8s

################################################################################
                     [1m Learning iteration 511/4000 [0m

                       Computation: 3248 steps/s (collection: 0.483s, learning 2.039s)
               Value function loss: 917.8232
                    Surrogate loss: 0.0113
             Mean action noise std: 0.98
                       Mean reward: 507.17
               Mean episode length: 282.41
                 Mean success rate: 3.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 2.52s
                        Total time: 1318.36s
                               ETA: 8983.9s

################################################################################
                     [1m Learning iteration 512/4000 [0m

                       Computation: 3159 steps/s (collection: 0.478s, learning 2.115s)
               Value function loss: 708.3183
                    Surrogate loss: 0.0151
             Mean action noise std: 0.98
                       Mean reward: 518.48
               Mean episode length: 286.82
                 Mean success rate: 2.50
                  Mean reward/step: 1.82
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4202496
                    Iteration time: 2.59s
                        Total time: 1320.95s
                               ETA: 8981.4s

################################################################################
                     [1m Learning iteration 513/4000 [0m

                       Computation: 3212 steps/s (collection: 0.442s, learning 2.108s)
               Value function loss: 1194.9851
                    Surrogate loss: 0.0096
             Mean action noise std: 0.98
                       Mean reward: 532.67
               Mean episode length: 296.56
                 Mean success rate: 2.50
                  Mean reward/step: 1.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 2.55s
                        Total time: 1323.50s
                               ETA: 8978.7s

################################################################################
                     [1m Learning iteration 514/4000 [0m

                       Computation: 3192 steps/s (collection: 0.487s, learning 2.079s)
               Value function loss: 856.0832
                    Surrogate loss: 0.0125
             Mean action noise std: 0.98
                       Mean reward: 564.86
               Mean episode length: 308.37
                 Mean success rate: 3.00
                  Mean reward/step: 1.93
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4218880
                    Iteration time: 2.57s
                        Total time: 1326.07s
                               ETA: 8976.1s

################################################################################
                     [1m Learning iteration 515/4000 [0m

                       Computation: 3201 steps/s (collection: 0.498s, learning 2.061s)
               Value function loss: 825.1440
                    Surrogate loss: 0.0131
             Mean action noise std: 0.98
                       Mean reward: 549.86
               Mean episode length: 297.14
                 Mean success rate: 4.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 2.56s
                        Total time: 1328.63s
                               ETA: 8973.4s

################################################################################
                     [1m Learning iteration 516/4000 [0m

                       Computation: 3123 steps/s (collection: 0.499s, learning 2.124s)
               Value function loss: 615.9918
                    Surrogate loss: 0.0148
             Mean action noise std: 0.98
                       Mean reward: 551.34
               Mean episode length: 297.16
                 Mean success rate: 3.50
                  Mean reward/step: 1.82
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4235264
                    Iteration time: 2.62s
                        Total time: 1331.25s
                               ETA: 8971.1s

################################################################################
                     [1m Learning iteration 517/4000 [0m

                       Computation: 3270 steps/s (collection: 0.442s, learning 2.063s)
               Value function loss: 983.8370
                    Surrogate loss: 0.0134
             Mean action noise std: 0.98
                       Mean reward: 568.40
               Mean episode length: 296.62
                 Mean success rate: 4.00
                  Mean reward/step: 1.85
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 2.50s
                        Total time: 1333.75s
                               ETA: 8968.1s

################################################################################
                     [1m Learning iteration 518/4000 [0m

                       Computation: 3234 steps/s (collection: 0.456s, learning 2.077s)
               Value function loss: 896.2290
                    Surrogate loss: 0.0133
             Mean action noise std: 0.98
                       Mean reward: 545.58
               Mean episode length: 282.87
                 Mean success rate: 4.50
                  Mean reward/step: 1.98
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4251648
                    Iteration time: 2.53s
                        Total time: 1336.29s
                               ETA: 8965.2s

################################################################################
                     [1m Learning iteration 519/4000 [0m

                       Computation: 3246 steps/s (collection: 0.469s, learning 2.055s)
               Value function loss: 1316.9717
                    Surrogate loss: 0.0112
             Mean action noise std: 0.98
                       Mean reward: 538.19
               Mean episode length: 277.68
                 Mean success rate: 5.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 2.52s
                        Total time: 1338.81s
                               ETA: 8962.3s

################################################################################
                     [1m Learning iteration 520/4000 [0m

                       Computation: 3236 steps/s (collection: 0.464s, learning 2.067s)
               Value function loss: 1213.2463
                    Surrogate loss: 0.0133
             Mean action noise std: 0.98
                       Mean reward: 525.83
               Mean episode length: 277.99
                 Mean success rate: 4.50
                  Mean reward/step: 2.01
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4268032
                    Iteration time: 2.53s
                        Total time: 1341.34s
                               ETA: 8959.4s

################################################################################
                     [1m Learning iteration 521/4000 [0m

                       Computation: 3213 steps/s (collection: 0.480s, learning 2.069s)
               Value function loss: 1586.3360
                    Surrogate loss: 0.0106
             Mean action noise std: 0.98
                       Mean reward: 539.29
               Mean episode length: 281.18
                 Mean success rate: 5.50
                  Mean reward/step: 1.99
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 2.55s
                        Total time: 1343.89s
                               ETA: 8956.7s

################################################################################
                     [1m Learning iteration 522/4000 [0m

                       Computation: 3157 steps/s (collection: 0.475s, learning 2.120s)
               Value function loss: 1299.9774
                    Surrogate loss: 0.0132
             Mean action noise std: 0.98
                       Mean reward: 532.34
               Mean episode length: 286.43
                 Mean success rate: 4.00
                  Mean reward/step: 1.94
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4284416
                    Iteration time: 2.59s
                        Total time: 1346.49s
                               ETA: 8954.3s

################################################################################
                     [1m Learning iteration 523/4000 [0m

                       Computation: 3204 steps/s (collection: 0.484s, learning 2.073s)
               Value function loss: 1772.3566
                    Surrogate loss: 0.0127
             Mean action noise std: 0.98
                       Mean reward: 519.72
               Mean episode length: 287.48
                 Mean success rate: 3.50
                  Mean reward/step: 1.96
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 2.56s
                        Total time: 1349.04s
                               ETA: 8951.6s

################################################################################
                     [1m Learning iteration 524/4000 [0m

                       Computation: 3148 steps/s (collection: 0.532s, learning 2.069s)
               Value function loss: 1413.2575
                    Surrogate loss: 0.0140
             Mean action noise std: 0.98
                       Mean reward: 477.04
               Mean episode length: 266.15
                 Mean success rate: 3.50
                  Mean reward/step: 1.84
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4300800
                    Iteration time: 2.60s
                        Total time: 1351.64s
                               ETA: 8949.2s

################################################################################
                     [1m Learning iteration 525/4000 [0m

                       Computation: 3161 steps/s (collection: 0.468s, learning 2.123s)
               Value function loss: 1592.0382
                    Surrogate loss: 0.0125
             Mean action noise std: 0.98
                       Mean reward: 479.28
               Mean episode length: 266.56
                 Mean success rate: 4.00
                  Mean reward/step: 2.16
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 2.59s
                        Total time: 1354.23s
                               ETA: 8946.7s

################################################################################
                     [1m Learning iteration 526/4000 [0m

                       Computation: 3166 steps/s (collection: 0.489s, learning 2.098s)
               Value function loss: 3049.8928
                    Surrogate loss: 0.0085
             Mean action noise std: 0.98
                       Mean reward: 496.00
               Mean episode length: 254.72
                 Mean success rate: 5.50
                  Mean reward/step: 2.19
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4317184
                    Iteration time: 2.59s
                        Total time: 1356.82s
                               ETA: 8944.2s

################################################################################
                     [1m Learning iteration 527/4000 [0m

                       Computation: 3112 steps/s (collection: 0.499s, learning 2.132s)
               Value function loss: 1693.9934
                    Surrogate loss: 0.0134
             Mean action noise std: 0.98
                       Mean reward: 503.16
               Mean episode length: 253.75
                 Mean success rate: 5.50
                  Mean reward/step: 2.05
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 2.63s
                        Total time: 1359.45s
                               ETA: 8942.0s

################################################################################
                     [1m Learning iteration 528/4000 [0m

                       Computation: 3188 steps/s (collection: 0.498s, learning 2.071s)
               Value function loss: 1621.3105
                    Surrogate loss: 0.0121
             Mean action noise std: 0.98
                       Mean reward: 510.11
               Mean episode length: 250.70
                 Mean success rate: 6.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4333568
                    Iteration time: 2.57s
                        Total time: 1362.02s
                               ETA: 8939.4s

################################################################################
                     [1m Learning iteration 529/4000 [0m

                       Computation: 3186 steps/s (collection: 0.481s, learning 2.091s)
               Value function loss: 1462.7513
                    Surrogate loss: 0.0104
             Mean action noise std: 0.98
                       Mean reward: 529.11
               Mean episode length: 257.94
                 Mean success rate: 6.00
                  Mean reward/step: 1.82
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 2.57s
                        Total time: 1364.59s
                               ETA: 8936.8s

################################################################################
                     [1m Learning iteration 530/4000 [0m

                       Computation: 3197 steps/s (collection: 0.519s, learning 2.043s)
               Value function loss: 2143.9244
                    Surrogate loss: 0.0106
             Mean action noise std: 0.98
                       Mean reward: 535.68
               Mean episode length: 263.65
                 Mean success rate: 5.50
                  Mean reward/step: 2.12
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4349952
                    Iteration time: 2.56s
                        Total time: 1367.16s
                               ETA: 8934.1s

################################################################################
                     [1m Learning iteration 531/4000 [0m

                       Computation: 3127 steps/s (collection: 0.522s, learning 2.098s)
               Value function loss: 2812.1074
                    Surrogate loss: 0.0104
             Mean action noise std: 0.98
                       Mean reward: 571.73
               Mean episode length: 281.21
                 Mean success rate: 6.50
                  Mean reward/step: 2.24
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 2.62s
                        Total time: 1369.78s
                               ETA: 8931.9s

################################################################################
                     [1m Learning iteration 532/4000 [0m

                       Computation: 3103 steps/s (collection: 0.532s, learning 2.108s)
               Value function loss: 2320.8058
                    Surrogate loss: 0.0115
             Mean action noise std: 0.98
                       Mean reward: 583.18
               Mean episode length: 286.33
                 Mean success rate: 7.50
                  Mean reward/step: 2.08
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4366336
                    Iteration time: 2.64s
                        Total time: 1372.42s
                               ETA: 8929.7s

################################################################################
                     [1m Learning iteration 533/4000 [0m

                       Computation: 3231 steps/s (collection: 0.469s, learning 2.066s)
               Value function loss: 2233.9926
                    Surrogate loss: 0.0100
             Mean action noise std: 0.98
                       Mean reward: 560.15
               Mean episode length: 280.39
                 Mean success rate: 8.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 2.54s
                        Total time: 1374.95s
                               ETA: 8926.9s

################################################################################
                     [1m Learning iteration 534/4000 [0m

                       Computation: 3191 steps/s (collection: 0.479s, learning 2.088s)
               Value function loss: 2033.1303
                    Surrogate loss: 0.0127
             Mean action noise std: 0.98
                       Mean reward: 557.11
               Mean episode length: 280.94
                 Mean success rate: 8.50
                  Mean reward/step: 2.25
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4382720
                    Iteration time: 2.57s
                        Total time: 1377.52s
                               ETA: 8924.3s

################################################################################
                     [1m Learning iteration 535/4000 [0m

                       Computation: 3179 steps/s (collection: 0.531s, learning 2.045s)
               Value function loss: 1740.5671
                    Surrogate loss: 0.0126
             Mean action noise std: 0.98
                       Mean reward: 518.80
               Mean episode length: 261.32
                 Mean success rate: 8.00
                  Mean reward/step: 2.17
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 2.58s
                        Total time: 1380.09s
                               ETA: 8921.7s

################################################################################
                     [1m Learning iteration 536/4000 [0m

                       Computation: 3183 steps/s (collection: 0.532s, learning 2.041s)
               Value function loss: 1918.1547
                    Surrogate loss: 0.0108
             Mean action noise std: 0.98
                       Mean reward: 482.12
               Mean episode length: 241.69
                 Mean success rate: 6.50
                  Mean reward/step: 2.09
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4399104
                    Iteration time: 2.57s
                        Total time: 1382.67s
                               ETA: 8919.1s

################################################################################
                     [1m Learning iteration 537/4000 [0m

                       Computation: 3231 steps/s (collection: 0.473s, learning 2.063s)
               Value function loss: 1943.8031
                    Surrogate loss: 0.0092
             Mean action noise std: 0.98
                       Mean reward: 472.31
               Mean episode length: 233.05
                 Mean success rate: 6.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 2.54s
                        Total time: 1385.20s
                               ETA: 8916.3s

################################################################################
                     [1m Learning iteration 538/4000 [0m

                       Computation: 3210 steps/s (collection: 0.481s, learning 2.071s)
               Value function loss: 2707.7121
                    Surrogate loss: 0.0089
             Mean action noise std: 0.98
                       Mean reward: 516.81
               Mean episode length: 241.05
                 Mean success rate: 7.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4415488
                    Iteration time: 2.55s
                        Total time: 1387.75s
                               ETA: 8913.6s

################################################################################
                     [1m Learning iteration 539/4000 [0m

                       Computation: 3300 steps/s (collection: 0.460s, learning 2.022s)
               Value function loss: 2130.5035
                    Surrogate loss: 0.0112
             Mean action noise std: 0.98
                       Mean reward: 513.97
               Mean episode length: 241.63
                 Mean success rate: 5.50
                  Mean reward/step: 2.06
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 2.48s
                        Total time: 1390.24s
                               ETA: 8910.4s

################################################################################
                     [1m Learning iteration 540/4000 [0m

                       Computation: 3231 steps/s (collection: 0.469s, learning 2.066s)
               Value function loss: 1816.2386
                    Surrogate loss: 0.0133
             Mean action noise std: 0.98
                       Mean reward: 507.23
               Mean episode length: 237.17
                 Mean success rate: 5.50
                  Mean reward/step: 2.05
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4431872
                    Iteration time: 2.54s
                        Total time: 1392.77s
                               ETA: 8907.6s

################################################################################
                     [1m Learning iteration 541/4000 [0m

                       Computation: 3212 steps/s (collection: 0.486s, learning 2.064s)
               Value function loss: 2149.9071
                    Surrogate loss: 0.0099
             Mean action noise std: 0.98
                       Mean reward: 533.71
               Mean episode length: 245.01
                 Mean success rate: 6.50
                  Mean reward/step: 2.25
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 2.55s
                        Total time: 1395.32s
                               ETA: 8904.8s

################################################################################
                     [1m Learning iteration 542/4000 [0m

                       Computation: 3200 steps/s (collection: 0.479s, learning 2.080s)
               Value function loss: 3026.9565
                    Surrogate loss: 0.0115
             Mean action noise std: 0.98
                       Mean reward: 544.61
               Mean episode length: 249.39
                 Mean success rate: 7.50
                  Mean reward/step: 2.24
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4448256
                    Iteration time: 2.56s
                        Total time: 1397.88s
                               ETA: 8902.2s

################################################################################
                     [1m Learning iteration 543/4000 [0m

                       Computation: 3297 steps/s (collection: 0.461s, learning 2.023s)
               Value function loss: 2307.5941
                    Surrogate loss: 0.0140
             Mean action noise std: 0.98
                       Mean reward: 525.97
               Mean episode length: 242.24
                 Mean success rate: 8.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 2.48s
                        Total time: 1400.37s
                               ETA: 8899.0s

################################################################################
                     [1m Learning iteration 544/4000 [0m

                       Computation: 3172 steps/s (collection: 0.515s, learning 2.068s)
               Value function loss: 2292.6883
                    Surrogate loss: 0.0131
             Mean action noise std: 0.98
                       Mean reward: 457.76
               Mean episode length: 223.60
                 Mean success rate: 5.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4464640
                    Iteration time: 2.58s
                        Total time: 1402.95s
                               ETA: 8896.5s

################################################################################
                     [1m Learning iteration 545/4000 [0m

                       Computation: 3299 steps/s (collection: 0.430s, learning 2.053s)
               Value function loss: 3580.7715
                    Surrogate loss: 0.0125
             Mean action noise std: 0.98
                       Mean reward: 442.29
               Mean episode length: 212.37
                 Mean success rate: 5.50
                  Mean reward/step: 2.36
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 2.48s
                        Total time: 1405.43s
                               ETA: 8893.3s

################################################################################
                     [1m Learning iteration 546/4000 [0m

                       Computation: 3268 steps/s (collection: 0.452s, learning 2.054s)
               Value function loss: 2503.3210
                    Surrogate loss: 0.0149
             Mean action noise std: 0.98
                       Mean reward: 459.22
               Mean episode length: 218.19
                 Mean success rate: 5.00
                  Mean reward/step: 2.20
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4481024
                    Iteration time: 2.51s
                        Total time: 1407.94s
                               ETA: 8890.3s

################################################################################
                     [1m Learning iteration 547/4000 [0m

                       Computation: 3266 steps/s (collection: 0.452s, learning 2.056s)
               Value function loss: 2195.7448
                    Surrogate loss: 0.0149
             Mean action noise std: 0.98
                       Mean reward: 490.35
               Mean episode length: 228.59
                 Mean success rate: 5.50
                  Mean reward/step: 2.33
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 2.51s
                        Total time: 1410.45s
                               ETA: 8887.3s

################################################################################
                     [1m Learning iteration 548/4000 [0m

                       Computation: 3132 steps/s (collection: 0.490s, learning 2.126s)
               Value function loss: 3554.8406
                    Surrogate loss: 0.0108
             Mean action noise std: 0.98
                       Mean reward: 508.65
               Mean episode length: 231.59
                 Mean success rate: 6.50
                  Mean reward/step: 2.35
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4497408
                    Iteration time: 2.62s
                        Total time: 1413.06s
                               ETA: 8885.0s

################################################################################
                     [1m Learning iteration 549/4000 [0m

                       Computation: 3147 steps/s (collection: 0.514s, learning 2.089s)
               Value function loss: 4173.6111
                    Surrogate loss: 0.0106
             Mean action noise std: 0.98
                       Mean reward: 534.50
               Mean episode length: 240.44
                 Mean success rate: 7.50
                  Mean reward/step: 2.34
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 2.60s
                        Total time: 1415.66s
                               ETA: 8882.6s

################################################################################
                     [1m Learning iteration 550/4000 [0m

                       Computation: 3221 steps/s (collection: 0.484s, learning 2.059s)
               Value function loss: 3454.6688
                    Surrogate loss: 0.0133
             Mean action noise std: 0.98
                       Mean reward: 557.76
               Mean episode length: 246.44
                 Mean success rate: 9.50
                  Mean reward/step: 2.30
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4513792
                    Iteration time: 2.54s
                        Total time: 1418.21s
                               ETA: 8879.9s

################################################################################
                     [1m Learning iteration 551/4000 [0m

                       Computation: 3236 steps/s (collection: 0.481s, learning 2.050s)
               Value function loss: 3242.5727
                    Surrogate loss: 0.0113
             Mean action noise std: 0.98
                       Mean reward: 575.14
               Mean episode length: 255.79
                 Mean success rate: 9.50
                  Mean reward/step: 2.39
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 2.53s
                        Total time: 1420.74s
                               ETA: 8877.0s

################################################################################
                     [1m Learning iteration 552/4000 [0m

                       Computation: 3231 steps/s (collection: 0.486s, learning 2.049s)
               Value function loss: 2589.9681
                    Surrogate loss: 0.0115
             Mean action noise std: 0.98
                       Mean reward: 552.81
               Mean episode length: 245.25
                 Mean success rate: 10.50
                  Mean reward/step: 2.21
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 4530176
                    Iteration time: 2.53s
                        Total time: 1423.27s
                               ETA: 8874.2s

################################################################################
                     [1m Learning iteration 553/4000 [0m

                       Computation: 3280 steps/s (collection: 0.464s, learning 2.033s)
               Value function loss: 3799.0379
                    Surrogate loss: 0.0107
             Mean action noise std: 0.98
                       Mean reward: 520.37
               Mean episode length: 223.36
                 Mean success rate: 10.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 2.50s
                        Total time: 1425.77s
                               ETA: 8871.2s

################################################################################
                     [1m Learning iteration 554/4000 [0m

                       Computation: 3306 steps/s (collection: 0.453s, learning 2.024s)
               Value function loss: 1963.1273
                    Surrogate loss: 0.0125
             Mean action noise std: 0.98
                       Mean reward: 524.21
               Mean episode length: 222.37
                 Mean success rate: 8.50
                  Mean reward/step: 1.90
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4546560
                    Iteration time: 2.48s
                        Total time: 1428.25s
                               ETA: 8868.0s

################################################################################
                     [1m Learning iteration 555/4000 [0m

                       Computation: 3208 steps/s (collection: 0.501s, learning 2.052s)
               Value function loss: 2058.6041
                    Surrogate loss: 0.0133
             Mean action noise std: 0.98
                       Mean reward: 530.48
               Mean episode length: 221.68
                 Mean success rate: 8.50
                  Mean reward/step: 2.05
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 2.55s
                        Total time: 1430.80s
                               ETA: 8865.3s

################################################################################
                     [1m Learning iteration 556/4000 [0m

                       Computation: 3294 steps/s (collection: 0.428s, learning 2.058s)
               Value function loss: 3166.6153
                    Surrogate loss: 0.0151
             Mean action noise std: 0.98
                       Mean reward: 528.32
               Mean episode length: 221.82
                 Mean success rate: 8.50
                  Mean reward/step: 2.23
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4562944
                    Iteration time: 2.49s
                        Total time: 1433.29s
                               ETA: 8862.2s

################################################################################
                     [1m Learning iteration 557/4000 [0m

                       Computation: 3276 steps/s (collection: 0.467s, learning 2.034s)
               Value function loss: 3693.6208
                    Surrogate loss: 0.0124
             Mean action noise std: 0.98
                       Mean reward: 530.24
               Mean episode length: 224.79
                 Mean success rate: 8.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 2.50s
                        Total time: 1435.79s
                               ETA: 8859.2s

################################################################################
                     [1m Learning iteration 558/4000 [0m

                       Computation: 3244 steps/s (collection: 0.456s, learning 2.069s)
               Value function loss: 2950.1174
                    Surrogate loss: 0.0137
             Mean action noise std: 0.98
                       Mean reward: 493.48
               Mean episode length: 223.81
                 Mean success rate: 6.50
                  Mean reward/step: 2.29
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4579328
                    Iteration time: 2.52s
                        Total time: 1438.31s
                               ETA: 8856.3s

################################################################################
                     [1m Learning iteration 559/4000 [0m

                       Computation: 3126 steps/s (collection: 0.466s, learning 2.154s)
               Value function loss: 2676.3039
                    Surrogate loss: 0.0140
             Mean action noise std: 0.98
                       Mean reward: 516.75
               Mean episode length: 233.04
                 Mean success rate: 6.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 2.62s
                        Total time: 1440.93s
                               ETA: 8854.0s

################################################################################
                     [1m Learning iteration 560/4000 [0m

                       Computation: 3199 steps/s (collection: 0.470s, learning 2.090s)
               Value function loss: 4156.7723
                    Surrogate loss: 0.0140
             Mean action noise std: 0.98
                       Mean reward: 497.67
               Mean episode length: 231.66
                 Mean success rate: 6.00
                  Mean reward/step: 2.43
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4595712
                    Iteration time: 2.56s
                        Total time: 1443.49s
                               ETA: 8851.4s

################################################################################
                     [1m Learning iteration 561/4000 [0m

                       Computation: 3156 steps/s (collection: 0.482s, learning 2.114s)
               Value function loss: 4593.2978
                    Surrogate loss: 0.0106
             Mean action noise std: 0.98
                       Mean reward: 461.65
               Mean episode length: 218.54
                 Mean success rate: 5.50
                  Mean reward/step: 2.35
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 2.60s
                        Total time: 1446.09s
                               ETA: 8848.9s

################################################################################
                     [1m Learning iteration 562/4000 [0m

                       Computation: 3201 steps/s (collection: 0.471s, learning 2.087s)
               Value function loss: 3411.4483
                    Surrogate loss: 0.0118
             Mean action noise std: 0.98
                       Mean reward: 470.35
               Mean episode length: 218.03
                 Mean success rate: 6.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4612096
                    Iteration time: 2.56s
                        Total time: 1448.64s
                               ETA: 8846.3s

################################################################################
                     [1m Learning iteration 563/4000 [0m

                       Computation: 3144 steps/s (collection: 0.486s, learning 2.119s)
               Value function loss: 2795.9701
                    Surrogate loss: 0.0130
             Mean action noise std: 0.98
                       Mean reward: 456.75
               Mean episode length: 207.84
                 Mean success rate: 6.00
                  Mean reward/step: 2.37
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 2.61s
                        Total time: 1451.25s
                               ETA: 8843.9s

################################################################################
                     [1m Learning iteration 564/4000 [0m

                       Computation: 3109 steps/s (collection: 0.468s, learning 2.167s)
               Value function loss: 3343.0165
                    Surrogate loss: 0.0158
             Mean action noise std: 0.98
                       Mean reward: 448.69
               Mean episode length: 201.75
                 Mean success rate: 6.50
                  Mean reward/step: 2.36
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 4628480
                    Iteration time: 2.63s
                        Total time: 1453.88s
                               ETA: 8841.7s

################################################################################
                     [1m Learning iteration 565/4000 [0m

                       Computation: 3203 steps/s (collection: 0.441s, learning 2.115s)
               Value function loss: 3685.6857
                    Surrogate loss: 0.0124
             Mean action noise std: 0.98
                       Mean reward: 448.28
               Mean episode length: 195.83
                 Mean success rate: 6.50
                  Mean reward/step: 2.48
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 2.56s
                        Total time: 1456.44s
                               ETA: 8839.0s

################################################################################
                     [1m Learning iteration 566/4000 [0m

                       Computation: 3157 steps/s (collection: 0.441s, learning 2.153s)
               Value function loss: 4616.5097
                    Surrogate loss: 0.0110
             Mean action noise std: 0.98
                       Mean reward: 517.58
               Mean episode length: 213.16
                 Mean success rate: 8.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 4644864
                    Iteration time: 2.59s
                        Total time: 1459.04s
                               ETA: 8836.6s

################################################################################
                     [1m Learning iteration 567/4000 [0m

                       Computation: 3195 steps/s (collection: 0.454s, learning 2.109s)
               Value function loss: 4632.9666
                    Surrogate loss: 0.0098
             Mean action noise std: 0.98
                       Mean reward: 506.10
               Mean episode length: 203.25
                 Mean success rate: 8.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 2.56s
                        Total time: 1461.60s
                               ETA: 8833.9s

################################################################################
                     [1m Learning iteration 568/4000 [0m

                       Computation: 3160 steps/s (collection: 0.478s, learning 2.114s)
               Value function loss: 3315.9897
                    Surrogate loss: 0.0146
             Mean action noise std: 0.98
                       Mean reward: 529.97
               Mean episode length: 216.28
                 Mean success rate: 9.00
                  Mean reward/step: 2.47
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4661248
                    Iteration time: 2.59s
                        Total time: 1464.19s
                               ETA: 8831.5s

################################################################################
                     [1m Learning iteration 569/4000 [0m

                       Computation: 3231 steps/s (collection: 0.469s, learning 2.066s)
               Value function loss: 4063.9059
                    Surrogate loss: 0.0122
             Mean action noise std: 0.98
                       Mean reward: 600.87
               Mean episode length: 234.12
                 Mean success rate: 10.50
                  Mean reward/step: 2.30
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 2.53s
                        Total time: 1466.73s
                               ETA: 8828.7s

################################################################################
                     [1m Learning iteration 570/4000 [0m

                       Computation: 3183 steps/s (collection: 0.455s, learning 2.118s)
               Value function loss: 3510.2305
                    Surrogate loss: 0.0139
             Mean action noise std: 0.98
                       Mean reward: 582.59
               Mean episode length: 231.31
                 Mean success rate: 11.00
                  Mean reward/step: 2.28
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4677632
                    Iteration time: 2.57s
                        Total time: 1469.30s
                               ETA: 8826.1s

################################################################################
                     [1m Learning iteration 571/4000 [0m

                       Computation: 3148 steps/s (collection: 0.482s, learning 2.120s)
               Value function loss: 2325.9257
                    Surrogate loss: 0.0149
             Mean action noise std: 0.98
                       Mean reward: 581.44
               Mean episode length: 230.63
                 Mean success rate: 11.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 2.60s
                        Total time: 1471.90s
                               ETA: 8823.7s

################################################################################
                     [1m Learning iteration 572/4000 [0m

                       Computation: 3067 steps/s (collection: 0.558s, learning 2.113s)
               Value function loss: 2216.0901
                    Surrogate loss: 0.0166
             Mean action noise std: 0.98
                       Mean reward: 540.78
               Mean episode length: 221.84
                 Mean success rate: 10.50
                  Mean reward/step: 2.08
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4694016
                    Iteration time: 2.67s
                        Total time: 1474.57s
                               ETA: 8821.7s

################################################################################
                     [1m Learning iteration 573/4000 [0m

                       Computation: 3145 steps/s (collection: 0.462s, learning 2.143s)
               Value function loss: 3585.4515
                    Surrogate loss: 0.0152
             Mean action noise std: 0.98
                       Mean reward: 498.85
               Mean episode length: 216.76
                 Mean success rate: 8.50
                  Mean reward/step: 2.27
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 2.60s
                        Total time: 1477.18s
                               ETA: 8819.3s

################################################################################
                     [1m Learning iteration 574/4000 [0m

                       Computation: 3199 steps/s (collection: 0.465s, learning 2.095s)
               Value function loss: 2997.9668
                    Surrogate loss: 0.0112
             Mean action noise std: 0.98
                       Mean reward: 496.95
               Mean episode length: 212.18
                 Mean success rate: 9.50
                  Mean reward/step: 2.51
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 4710400
                    Iteration time: 2.56s
                        Total time: 1479.74s
                               ETA: 8816.7s

################################################################################
                     [1m Learning iteration 575/4000 [0m

                       Computation: 3088 steps/s (collection: 0.510s, learning 2.143s)
               Value function loss: 4725.1860
                    Surrogate loss: 0.0079
             Mean action noise std: 0.98
                       Mean reward: 451.32
               Mean episode length: 205.10
                 Mean success rate: 7.50
                  Mean reward/step: 2.53
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 2.65s
                        Total time: 1482.39s
                               ETA: 8814.5s

################################################################################
                     [1m Learning iteration 576/4000 [0m

                       Computation: 3120 steps/s (collection: 0.506s, learning 2.119s)
               Value function loss: 4286.0948
                    Surrogate loss: 0.0112
             Mean action noise std: 0.98
                       Mean reward: 501.07
               Mean episode length: 221.47
                 Mean success rate: 9.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4726784
                    Iteration time: 2.63s
                        Total time: 1485.01s
                               ETA: 8812.3s

################################################################################
                     [1m Learning iteration 577/4000 [0m

                       Computation: 3077 steps/s (collection: 0.529s, learning 2.132s)
               Value function loss: 3829.3541
                    Surrogate loss: 0.0134
             Mean action noise std: 0.98
                       Mean reward: 483.50
               Mean episode length: 218.54
                 Mean success rate: 9.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 2.66s
                        Total time: 1487.68s
                               ETA: 8810.2s

################################################################################
                     [1m Learning iteration 578/4000 [0m

                       Computation: 3193 steps/s (collection: 0.484s, learning 2.082s)
               Value function loss: 2310.1619
                    Surrogate loss: 0.0143
             Mean action noise std: 0.98
                       Mean reward: 516.88
               Mean episode length: 223.00
                 Mean success rate: 9.50
                  Mean reward/step: 2.16
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 4743168
                    Iteration time: 2.57s
                        Total time: 1490.24s
                               ETA: 8807.6s

################################################################################
                     [1m Learning iteration 579/4000 [0m

                       Computation: 3114 steps/s (collection: 0.490s, learning 2.140s)
               Value function loss: 2810.3525
                    Surrogate loss: 0.0145
             Mean action noise std: 0.98
                       Mean reward: 517.72
               Mean episode length: 224.84
                 Mean success rate: 9.00
                  Mean reward/step: 2.23
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 2.63s
                        Total time: 1492.87s
                               ETA: 8805.4s

################################################################################
                     [1m Learning iteration 580/4000 [0m

                       Computation: 3007 steps/s (collection: 0.550s, learning 2.174s)
               Value function loss: 3448.5187
                    Surrogate loss: 0.0116
             Mean action noise std: 0.98
                       Mean reward: 523.77
               Mean episode length: 230.41
                 Mean success rate: 9.00
                  Mean reward/step: 2.34
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4759552
                    Iteration time: 2.72s
                        Total time: 1495.59s
                               ETA: 8803.7s

################################################################################
                     [1m Learning iteration 581/4000 [0m

                       Computation: 3069 steps/s (collection: 0.533s, learning 2.136s)
               Value function loss: 3040.2237
                    Surrogate loss: 0.0123
             Mean action noise std: 0.98
                       Mean reward: 505.34
               Mean episode length: 229.80
                 Mean success rate: 8.50
                  Mean reward/step: 2.17
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 2.67s
                        Total time: 1498.26s
                               ETA: 8801.7s

################################################################################
                     [1m Learning iteration 582/4000 [0m

                       Computation: 3141 steps/s (collection: 0.528s, learning 2.080s)
               Value function loss: 1889.6226
                    Surrogate loss: 0.0130
             Mean action noise std: 0.98
                       Mean reward: 498.75
               Mean episode length: 224.25
                 Mean success rate: 8.50
                  Mean reward/step: 2.27
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4775936
                    Iteration time: 2.61s
                        Total time: 1500.87s
                               ETA: 8799.3s

################################################################################
                     [1m Learning iteration 583/4000 [0m

                       Computation: 3262 steps/s (collection: 0.483s, learning 2.028s)
               Value function loss: 3738.2680
                    Surrogate loss: 0.0115
             Mean action noise std: 0.97
                       Mean reward: 477.07
               Mean episode length: 223.03
                 Mean success rate: 7.50
                  Mean reward/step: 2.50
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 2.51s
                        Total time: 1503.38s
                               ETA: 8796.3s

################################################################################
                     [1m Learning iteration 584/4000 [0m

                       Computation: 3158 steps/s (collection: 0.500s, learning 2.094s)
               Value function loss: 3531.2962
                    Surrogate loss: 0.0143
             Mean action noise std: 0.98
                       Mean reward: 480.33
               Mean episode length: 223.99
                 Mean success rate: 7.50
                  Mean reward/step: 2.39
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4792320
                    Iteration time: 2.59s
                        Total time: 1505.98s
                               ETA: 8793.9s

################################################################################
                     [1m Learning iteration 585/4000 [0m

                       Computation: 3171 steps/s (collection: 0.533s, learning 2.050s)
               Value function loss: 3525.8201
                    Surrogate loss: 0.0123
             Mean action noise std: 0.98
                       Mean reward: 512.12
               Mean episode length: 227.00
                 Mean success rate: 8.00
                  Mean reward/step: 2.25
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 2.58s
                        Total time: 1508.56s
                               ETA: 8791.3s

################################################################################
                     [1m Learning iteration 586/4000 [0m

                       Computation: 3245 steps/s (collection: 0.479s, learning 2.046s)
               Value function loss: 1664.8464
                    Surrogate loss: 0.0124
             Mean action noise std: 0.97
                       Mean reward: 521.00
               Mean episode length: 225.66
                 Mean success rate: 8.00
                  Mean reward/step: 2.26
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4808704
                    Iteration time: 2.52s
                        Total time: 1511.08s
                               ETA: 8788.5s

################################################################################
                     [1m Learning iteration 587/4000 [0m

                       Computation: 3206 steps/s (collection: 0.463s, learning 2.091s)
               Value function loss: 4099.0209
                    Surrogate loss: 0.0099
             Mean action noise std: 0.97
                       Mean reward: 500.24
               Mean episode length: 224.95
                 Mean success rate: 7.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 2.55s
                        Total time: 1513.64s
                               ETA: 8785.8s

################################################################################
                     [1m Learning iteration 588/4000 [0m

                       Computation: 3136 steps/s (collection: 0.512s, learning 2.100s)
               Value function loss: 3530.7822
                    Surrogate loss: 0.0094
             Mean action noise std: 0.97
                       Mean reward: 508.02
               Mean episode length: 233.76
                 Mean success rate: 7.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 4825088
                    Iteration time: 2.61s
                        Total time: 1516.25s
                               ETA: 8783.4s

################################################################################
                     [1m Learning iteration 589/4000 [0m

                       Computation: 3220 steps/s (collection: 0.487s, learning 2.057s)
               Value function loss: 2424.0146
                    Surrogate loss: 0.0150
             Mean action noise std: 0.97
                       Mean reward: 572.66
               Mean episode length: 243.82
                 Mean success rate: 7.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 2.54s
                        Total time: 1518.79s
                               ETA: 8780.7s

################################################################################
                     [1m Learning iteration 590/4000 [0m

                       Computation: 3199 steps/s (collection: 0.503s, learning 2.058s)
               Value function loss: 3109.2121
                    Surrogate loss: 0.0123
             Mean action noise std: 0.97
                       Mean reward: 548.46
               Mean episode length: 236.37
                 Mean success rate: 6.50
                  Mean reward/step: 2.17
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4841472
                    Iteration time: 2.56s
                        Total time: 1521.35s
                               ETA: 8778.0s

################################################################################
                     [1m Learning iteration 591/4000 [0m

                       Computation: 3200 steps/s (collection: 0.481s, learning 2.079s)
               Value function loss: 1972.9647
                    Surrogate loss: 0.0147
             Mean action noise std: 0.97
                       Mean reward: 563.45
               Mean episode length: 241.92
                 Mean success rate: 7.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 2.56s
                        Total time: 1523.91s
                               ETA: 8775.4s

################################################################################
                     [1m Learning iteration 592/4000 [0m

                       Computation: 3186 steps/s (collection: 0.518s, learning 2.053s)
               Value function loss: 4022.6157
                    Surrogate loss: 0.0113
             Mean action noise std: 0.97
                       Mean reward: 562.13
               Mean episode length: 250.81
                 Mean success rate: 7.00
                  Mean reward/step: 2.47
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4857856
                    Iteration time: 2.57s
                        Total time: 1526.48s
                               ETA: 8772.8s

################################################################################
                     [1m Learning iteration 593/4000 [0m

                       Computation: 3187 steps/s (collection: 0.510s, learning 2.059s)
               Value function loss: 4728.5830
                    Surrogate loss: 0.0101
             Mean action noise std: 0.97
                       Mean reward: 582.16
               Mean episode length: 255.75
                 Mean success rate: 7.50
                  Mean reward/step: 2.56
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 2.57s
                        Total time: 1529.05s
                               ETA: 8770.2s

################################################################################
                     [1m Learning iteration 594/4000 [0m

                       Computation: 3068 steps/s (collection: 0.495s, learning 2.175s)
               Value function loss: 3192.4506
                    Surrogate loss: 0.0129
             Mean action noise std: 0.97
                       Mean reward: 577.29
               Mean episode length: 245.73
                 Mean success rate: 7.50
                  Mean reward/step: 2.44
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4874240
                    Iteration time: 2.67s
                        Total time: 1531.72s
                               ETA: 8768.2s

################################################################################
                     [1m Learning iteration 595/4000 [0m

                       Computation: 3158 steps/s (collection: 0.475s, learning 2.119s)
               Value function loss: 3934.0075
                    Surrogate loss: 0.0128
             Mean action noise std: 0.97
                       Mean reward: 523.39
               Mean episode length: 244.39
                 Mean success rate: 7.00
                  Mean reward/step: 2.77
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 2.59s
                        Total time: 1534.32s
                               ETA: 8765.7s

################################################################################
                     [1m Learning iteration 596/4000 [0m

                       Computation: 3033 steps/s (collection: 0.539s, learning 2.161s)
               Value function loss: 4142.9666
                    Surrogate loss: 0.0143
             Mean action noise std: 0.97
                       Mean reward: 556.57
               Mean episode length: 257.44
                 Mean success rate: 8.00
                  Mean reward/step: 2.73
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4890624
                    Iteration time: 2.70s
                        Total time: 1537.02s
                               ETA: 8763.8s

################################################################################
                     [1m Learning iteration 597/4000 [0m

                       Computation: 3185 steps/s (collection: 0.488s, learning 2.084s)
               Value function loss: 5038.8587
                    Surrogate loss: 0.0099
             Mean action noise std: 0.97
                       Mean reward: 576.41
               Mean episode length: 262.79
                 Mean success rate: 8.00
                  Mean reward/step: 2.59
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 2.57s
                        Total time: 1539.59s
                               ETA: 8761.2s

################################################################################
                     [1m Learning iteration 598/4000 [0m

                       Computation: 3164 steps/s (collection: 0.464s, learning 2.124s)
               Value function loss: 4253.7838
                    Surrogate loss: 0.0125
             Mean action noise std: 0.97
                       Mean reward: 549.54
               Mean episode length: 256.79
                 Mean success rate: 6.50
                  Mean reward/step: 2.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4907008
                    Iteration time: 2.59s
                        Total time: 1542.18s
                               ETA: 8758.7s

################################################################################
                     [1m Learning iteration 599/4000 [0m

                       Computation: 3132 steps/s (collection: 0.481s, learning 2.134s)
               Value function loss: 5573.5661
                    Surrogate loss: 0.0094
             Mean action noise std: 0.97
                       Mean reward: 527.77
               Mean episode length: 240.10
                 Mean success rate: 6.00
                  Mean reward/step: 2.22
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 2.62s
                        Total time: 1544.79s
                               ETA: 8756.4s

################################################################################
                     [1m Learning iteration 600/4000 [0m

                       Computation: 3071 steps/s (collection: 0.525s, learning 2.142s)
               Value function loss: 4335.2201
                    Surrogate loss: 0.0111
             Mean action noise std: 0.97
                       Mean reward: 582.17
               Mean episode length: 256.86
                 Mean success rate: 7.50
                  Mean reward/step: 2.17
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4923392
                    Iteration time: 2.67s
                        Total time: 1547.46s
                               ETA: 8754.4s

################################################################################
                     [1m Learning iteration 601/4000 [0m

                       Computation: 3111 steps/s (collection: 0.533s, learning 2.099s)
               Value function loss: 3387.1964
                    Surrogate loss: 0.0121
             Mean action noise std: 0.97
                       Mean reward: 566.54
               Mean episode length: 252.91
                 Mean success rate: 7.50
                  Mean reward/step: 2.41
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 2.63s
                        Total time: 1550.09s
                               ETA: 8752.1s

################################################################################
                     [1m Learning iteration 602/4000 [0m

                       Computation: 3232 steps/s (collection: 0.464s, learning 2.070s)
               Value function loss: 5367.4655
                    Surrogate loss: 0.0089
             Mean action noise std: 0.97
                       Mean reward: 596.86
               Mean episode length: 262.20
                 Mean success rate: 8.50
                  Mean reward/step: 2.67
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4939776
                    Iteration time: 2.53s
                        Total time: 1552.63s
                               ETA: 8749.3s

################################################################################
                     [1m Learning iteration 603/4000 [0m

                       Computation: 3129 steps/s (collection: 0.506s, learning 2.112s)
               Value function loss: 5968.6325
                    Surrogate loss: 0.0106
             Mean action noise std: 0.97
                       Mean reward: 611.47
               Mean episode length: 258.58
                 Mean success rate: 9.50
                  Mean reward/step: 2.84
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 2.62s
                        Total time: 1555.25s
                               ETA: 8747.0s

################################################################################
                     [1m Learning iteration 604/4000 [0m

                       Computation: 3104 steps/s (collection: 0.478s, learning 2.161s)
               Value function loss: 5526.8536
                    Surrogate loss: 0.0098
             Mean action noise std: 0.97
                       Mean reward: 627.91
               Mean episode length: 257.77
                 Mean success rate: 10.50
                  Mean reward/step: 2.78
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4956160
                    Iteration time: 2.64s
                        Total time: 1557.88s
                               ETA: 8744.8s

################################################################################
                     [1m Learning iteration 605/4000 [0m

                       Computation: 3111 steps/s (collection: 0.537s, learning 2.096s)
               Value function loss: 5161.1896
                    Surrogate loss: 0.0105
             Mean action noise std: 0.97
                       Mean reward: 657.57
               Mean episode length: 260.94
                 Mean success rate: 11.50
                  Mean reward/step: 2.81
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 2.63s
                        Total time: 1560.52s
                               ETA: 8742.5s

################################################################################
                     [1m Learning iteration 606/4000 [0m

                       Computation: 3117 steps/s (collection: 0.529s, learning 2.099s)
               Value function loss: 5394.8116
                    Surrogate loss: 0.0111
             Mean action noise std: 0.97
                       Mean reward: 711.87
               Mean episode length: 269.93
                 Mean success rate: 12.50
                  Mean reward/step: 2.74
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4972544
                    Iteration time: 2.63s
                        Total time: 1563.15s
                               ETA: 8740.2s

################################################################################
                     [1m Learning iteration 607/4000 [0m

                       Computation: 3201 steps/s (collection: 0.474s, learning 2.085s)
               Value function loss: 4161.9282
                    Surrogate loss: 0.0086
             Mean action noise std: 0.97
                       Mean reward: 708.00
               Mean episode length: 257.36
                 Mean success rate: 12.00
                  Mean reward/step: 2.60
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 2.56s
                        Total time: 1565.70s
                               ETA: 8737.6s

################################################################################
                     [1m Learning iteration 608/4000 [0m

                       Computation: 3152 steps/s (collection: 0.495s, learning 2.103s)
               Value function loss: 4503.3588
                    Surrogate loss: 0.0095
             Mean action noise std: 0.97
                       Mean reward: 703.21
               Mean episode length: 253.03
                 Mean success rate: 12.50
                  Mean reward/step: 2.49
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4988928
                    Iteration time: 2.60s
                        Total time: 1568.30s
                               ETA: 8735.1s

################################################################################
                     [1m Learning iteration 609/4000 [0m

                       Computation: 3134 steps/s (collection: 0.514s, learning 2.099s)
               Value function loss: 5427.6179
                    Surrogate loss: 0.0076
             Mean action noise std: 0.97
                       Mean reward: 679.64
               Mean episode length: 247.38
                 Mean success rate: 11.50
                  Mean reward/step: 2.39
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 2.61s
                        Total time: 1570.92s
                               ETA: 8732.7s

################################################################################
                     [1m Learning iteration 610/4000 [0m

                       Computation: 3094 steps/s (collection: 0.518s, learning 2.129s)
               Value function loss: 3840.9746
                    Surrogate loss: 0.0133
             Mean action noise std: 0.97
                       Mean reward: 695.27
               Mean episode length: 254.25
                 Mean success rate: 12.50
                  Mean reward/step: 2.60
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5005312
                    Iteration time: 2.65s
                        Total time: 1573.56s
                               ETA: 8730.6s

################################################################################
                     [1m Learning iteration 611/4000 [0m

                       Computation: 3169 steps/s (collection: 0.484s, learning 2.101s)
               Value function loss: 3053.1138
                    Surrogate loss: 0.0112
             Mean action noise std: 0.97
                       Mean reward: 626.73
               Mean episode length: 241.62
                 Mean success rate: 11.00
                  Mean reward/step: 2.61
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 2.58s
                        Total time: 1576.15s
                               ETA: 8728.0s

################################################################################
                     [1m Learning iteration 612/4000 [0m

                       Computation: 3118 steps/s (collection: 0.488s, learning 2.139s)
               Value function loss: 6445.0032
                    Surrogate loss: 0.0129
             Mean action noise std: 0.97
                       Mean reward: 579.03
               Mean episode length: 238.67
                 Mean success rate: 10.00
                  Mean reward/step: 2.89
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5021696
                    Iteration time: 2.63s
                        Total time: 1578.77s
                               ETA: 8725.8s

################################################################################
                     [1m Learning iteration 613/4000 [0m

                       Computation: 3113 steps/s (collection: 0.520s, learning 2.112s)
               Value function loss: 7241.9109
                    Surrogate loss: 0.0115
             Mean action noise std: 0.97
                       Mean reward: 637.98
               Mean episode length: 243.69
                 Mean success rate: 11.00
                  Mean reward/step: 2.72
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 2.63s
                        Total time: 1581.41s
                               ETA: 8723.5s

################################################################################
                     [1m Learning iteration 614/4000 [0m

                       Computation: 3159 steps/s (collection: 0.492s, learning 2.100s)
               Value function loss: 5709.5482
                    Surrogate loss: 0.0107
             Mean action noise std: 0.97
                       Mean reward: 697.25
               Mean episode length: 243.09
                 Mean success rate: 11.50
                  Mean reward/step: 2.60
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5038080
                    Iteration time: 2.59s
                        Total time: 1584.00s
                               ETA: 8721.0s

################################################################################
                     [1m Learning iteration 615/4000 [0m

                       Computation: 3101 steps/s (collection: 0.501s, learning 2.141s)
               Value function loss: 5826.0201
                    Surrogate loss: 0.0106
             Mean action noise std: 0.97
                       Mean reward: 726.27
               Mean episode length: 248.04
                 Mean success rate: 11.00
                  Mean reward/step: 2.48
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 2.64s
                        Total time: 1586.64s
                               ETA: 8718.8s

################################################################################
                     [1m Learning iteration 616/4000 [0m

                       Computation: 3031 steps/s (collection: 0.543s, learning 2.160s)
               Value function loss: 6638.9605
                    Surrogate loss: 0.0118
             Mean action noise std: 0.97
                       Mean reward: 665.06
               Mean episode length: 234.63
                 Mean success rate: 10.50
                  Mean reward/step: 2.68
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 5054464
                    Iteration time: 2.70s
                        Total time: 1589.34s
                               ETA: 8716.9s

################################################################################
                     [1m Learning iteration 617/4000 [0m

                       Computation: 3138 steps/s (collection: 0.495s, learning 2.115s)
               Value function loss: 3550.0655
                    Surrogate loss: 0.0133
             Mean action noise std: 0.97
                       Mean reward: 663.58
               Mean episode length: 233.69
                 Mean success rate: 11.00
                  Mean reward/step: 2.45
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 2.61s
                        Total time: 1591.95s
                               ETA: 8714.5s

################################################################################
                     [1m Learning iteration 618/4000 [0m

                       Computation: 3227 steps/s (collection: 0.471s, learning 2.068s)
               Value function loss: 5236.6289
                    Surrogate loss: 0.0111
             Mean action noise std: 0.97
                       Mean reward: 647.21
               Mean episode length: 233.55
                 Mean success rate: 10.00
                  Mean reward/step: 2.57
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 5070848
                    Iteration time: 2.54s
                        Total time: 1594.49s
                               ETA: 8711.7s

################################################################################
                     [1m Learning iteration 619/4000 [0m

                       Computation: 3174 steps/s (collection: 0.444s, learning 2.137s)
               Value function loss: 5382.5525
                    Surrogate loss: 0.0112
             Mean action noise std: 0.97
                       Mean reward: 563.29
               Mean episode length: 220.94
                 Mean success rate: 8.50
                  Mean reward/step: 2.60
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 2.58s
                        Total time: 1597.07s
                               ETA: 8709.2s

################################################################################
                     [1m Learning iteration 620/4000 [0m

                       Computation: 3196 steps/s (collection: 0.495s, learning 2.068s)
               Value function loss: 6073.9865
                    Surrogate loss: 0.0092
             Mean action noise std: 0.97
                       Mean reward: 572.84
               Mean episode length: 223.76
                 Mean success rate: 8.50
                  Mean reward/step: 2.40
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 5087232
                    Iteration time: 2.56s
                        Total time: 1599.64s
                               ETA: 8706.6s

################################################################################
                     [1m Learning iteration 621/4000 [0m

                       Computation: 3124 steps/s (collection: 0.493s, learning 2.129s)
               Value function loss: 5504.4201
                    Surrogate loss: 0.0090
             Mean action noise std: 0.97
                       Mean reward: 601.79
               Mean episode length: 224.56
                 Mean success rate: 9.50
                  Mean reward/step: 2.55
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 2.62s
                        Total time: 1602.26s
                               ETA: 8704.2s

################################################################################
                     [1m Learning iteration 622/4000 [0m

                       Computation: 3153 steps/s (collection: 0.491s, learning 2.107s)
               Value function loss: 2896.1069
                    Surrogate loss: 0.0141
             Mean action noise std: 0.97
                       Mean reward: 554.73
               Mean episode length: 224.04
                 Mean success rate: 7.50
                  Mean reward/step: 2.56
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5103616
                    Iteration time: 2.60s
                        Total time: 1604.85s
                               ETA: 8701.8s

################################################################################
                     [1m Learning iteration 623/4000 [0m

                       Computation: 3178 steps/s (collection: 0.479s, learning 2.099s)
               Value function loss: 5350.3927
                    Surrogate loss: 0.0129
             Mean action noise std: 0.97
                       Mean reward: 535.63
               Mean episode length: 219.75
                 Mean success rate: 6.50
                  Mean reward/step: 2.66
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 2.58s
                        Total time: 1607.43s
                               ETA: 8699.2s

################################################################################
                     [1m Learning iteration 624/4000 [0m

                       Computation: 3150 steps/s (collection: 0.472s, learning 2.128s)
               Value function loss: 6337.3084
                    Surrogate loss: 0.0117
             Mean action noise std: 0.97
                       Mean reward: 523.81
               Mean episode length: 228.71
                 Mean success rate: 7.50
                  Mean reward/step: 2.55
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 5120000
                    Iteration time: 2.60s
                        Total time: 1610.03s
                               ETA: 8696.7s

################################################################################
                     [1m Learning iteration 625/4000 [0m

                       Computation: 3165 steps/s (collection: 0.496s, learning 2.092s)
               Value function loss: 3971.0533
                    Surrogate loss: 0.0134
             Mean action noise std: 0.97
                       Mean reward: 476.33
               Mean episode length: 215.87
                 Mean success rate: 7.00
                  Mean reward/step: 2.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 2.59s
                        Total time: 1612.62s
                               ETA: 8694.2s

################################################################################
                     [1m Learning iteration 626/4000 [0m

                       Computation: 3177 steps/s (collection: 0.505s, learning 2.073s)
               Value function loss: 6309.9544
                    Surrogate loss: 0.0132
             Mean action noise std: 0.97
                       Mean reward: 449.19
               Mean episode length: 205.51
                 Mean success rate: 6.50
                  Mean reward/step: 2.67
       Mean episode length/episode: 25.05
--------------------------------------------------------------------------------
                   Total timesteps: 5136384
                    Iteration time: 2.58s
                        Total time: 1615.20s
                               ETA: 8691.7s

################################################################################
                     [1m Learning iteration 627/4000 [0m

                       Computation: 3237 steps/s (collection: 0.466s, learning 2.064s)
               Value function loss: 4736.6374
                    Surrogate loss: 0.0119
             Mean action noise std: 0.97
                       Mean reward: 552.29
               Mean episode length: 204.80
                 Mean success rate: 8.50
                  Mean reward/step: 2.31
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 2.53s
                        Total time: 1617.73s
                               ETA: 8688.9s

################################################################################
                     [1m Learning iteration 628/4000 [0m

                       Computation: 3194 steps/s (collection: 0.478s, learning 2.086s)
               Value function loss: 2837.3782
                    Surrogate loss: 0.0169
             Mean action noise std: 0.97
                       Mean reward: 549.40
               Mean episode length: 194.63
                 Mean success rate: 9.50
                  Mean reward/step: 2.29
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 5152768
                    Iteration time: 2.56s
                        Total time: 1620.29s
                               ETA: 8686.2s

################################################################################
                     [1m Learning iteration 629/4000 [0m

                       Computation: 3220 steps/s (collection: 0.475s, learning 2.069s)
               Value function loss: 4715.8333
                    Surrogate loss: 0.0134
             Mean action noise std: 0.97
                       Mean reward: 522.52
               Mean episode length: 191.50
                 Mean success rate: 9.00
                  Mean reward/step: 2.56
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 2.54s
                        Total time: 1622.84s
                               ETA: 8683.5s

################################################################################
                     [1m Learning iteration 630/4000 [0m

                       Computation: 3055 steps/s (collection: 0.535s, learning 2.146s)
               Value function loss: 3902.9512
                    Surrogate loss: 0.0173
             Mean action noise std: 0.97
                       Mean reward: 509.04
               Mean episode length: 188.04
                 Mean success rate: 9.00
                  Mean reward/step: 2.57
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 5169152
                    Iteration time: 2.68s
                        Total time: 1625.52s
                               ETA: 8681.4s

################################################################################
                     [1m Learning iteration 631/4000 [0m

                       Computation: 3173 steps/s (collection: 0.470s, learning 2.112s)
               Value function loss: 2684.2367
                    Surrogate loss: 0.0174
             Mean action noise std: 0.97
                       Mean reward: 499.15
               Mean episode length: 195.46
                 Mean success rate: 7.50
                  Mean reward/step: 2.24
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 2.58s
                        Total time: 1628.10s
                               ETA: 8678.9s

################################################################################
                     [1m Learning iteration 632/4000 [0m

                       Computation: 3169 steps/s (collection: 0.492s, learning 2.093s)
               Value function loss: 3756.8979
                    Surrogate loss: 0.0137
             Mean action noise std: 0.97
                       Mean reward: 388.42
               Mean episode length: 186.31
                 Mean success rate: 5.50
                  Mean reward/step: 2.28
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 5185536
                    Iteration time: 2.58s
                        Total time: 1630.68s
                               ETA: 8676.4s

################################################################################
                     [1m Learning iteration 633/4000 [0m

                       Computation: 3110 steps/s (collection: 0.487s, learning 2.146s)
               Value function loss: 4070.1301
                    Surrogate loss: 0.0153
             Mean action noise std: 0.97
                       Mean reward: 368.91
               Mean episode length: 180.34
                 Mean success rate: 4.50
                  Mean reward/step: 2.49
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 2.63s
                        Total time: 1633.32s
                               ETA: 8674.1s

################################################################################
                     [1m Learning iteration 634/4000 [0m

                       Computation: 3178 steps/s (collection: 0.473s, learning 2.104s)
               Value function loss: 5926.1659
                    Surrogate loss: 0.0091
             Mean action noise std: 0.97
                       Mean reward: 328.40
               Mean episode length: 180.84
                 Mean success rate: 3.50
                  Mean reward/step: 2.59
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5201920
                    Iteration time: 2.58s
                        Total time: 1635.89s
                               ETA: 8671.5s

################################################################################
                     [1m Learning iteration 635/4000 [0m

                       Computation: 3136 steps/s (collection: 0.485s, learning 2.127s)
               Value function loss: 6426.6114
                    Surrogate loss: 0.0090
             Mean action noise std: 0.97
                       Mean reward: 389.86
               Mean episode length: 183.25
                 Mean success rate: 4.00
                  Mean reward/step: 2.58
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 2.61s
                        Total time: 1638.51s
                               ETA: 8669.1s

################################################################################
                     [1m Learning iteration 636/4000 [0m

                       Computation: 3141 steps/s (collection: 0.509s, learning 2.098s)
               Value function loss: 4850.7996
                    Surrogate loss: 0.0103
             Mean action noise std: 0.97
                       Mean reward: 416.61
               Mean episode length: 190.13
                 Mean success rate: 4.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 5218304
                    Iteration time: 2.61s
                        Total time: 1641.11s
                               ETA: 8666.7s

################################################################################
                     [1m Learning iteration 637/4000 [0m

                       Computation: 3170 steps/s (collection: 0.481s, learning 2.103s)
               Value function loss: 10083.7523
                    Surrogate loss: 0.0098
             Mean action noise std: 0.97
                       Mean reward: 463.02
               Mean episode length: 200.66
                 Mean success rate: 6.50
                  Mean reward/step: 2.76
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 2.58s
                        Total time: 1643.70s
                               ETA: 8664.2s

################################################################################
                     [1m Learning iteration 638/4000 [0m

                       Computation: 3092 steps/s (collection: 0.504s, learning 2.145s)
               Value function loss: 6509.0525
                    Surrogate loss: 0.0132
             Mean action noise std: 0.97
                       Mean reward: 541.04
               Mean episode length: 209.34
                 Mean success rate: 8.50
                  Mean reward/step: 2.76
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5234688
                    Iteration time: 2.65s
                        Total time: 1646.35s
                               ETA: 8662.0s

################################################################################
                     [1m Learning iteration 639/4000 [0m

                       Computation: 3180 steps/s (collection: 0.515s, learning 2.061s)
               Value function loss: 4791.9076
                    Surrogate loss: 0.0158
             Mean action noise std: 0.97
                       Mean reward: 604.22
               Mean episode length: 213.84
                 Mean success rate: 10.00
                  Mean reward/step: 2.87
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 2.58s
                        Total time: 1648.92s
                               ETA: 8659.4s

################################################################################
                     [1m Learning iteration 640/4000 [0m

                       Computation: 3218 steps/s (collection: 0.488s, learning 2.057s)
               Value function loss: 6020.7828
                    Surrogate loss: 0.0143
             Mean action noise std: 0.97
                       Mean reward: 579.67
               Mean episode length: 212.99
                 Mean success rate: 10.00
                  Mean reward/step: 2.95
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5251072
                    Iteration time: 2.54s
                        Total time: 1651.47s
                               ETA: 8656.7s

################################################################################
                     [1m Learning iteration 641/4000 [0m

                       Computation: 3131 steps/s (collection: 0.496s, learning 2.120s)
               Value function loss: 7027.7821
                    Surrogate loss: 0.0150
             Mean action noise std: 0.97
                       Mean reward: 558.76
               Mean episode length: 203.34
                 Mean success rate: 10.00
                  Mean reward/step: 3.14
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 2.62s
                        Total time: 1654.08s
                               ETA: 8654.3s

################################################################################
                     [1m Learning iteration 642/4000 [0m

                       Computation: 3137 steps/s (collection: 0.521s, learning 2.090s)
               Value function loss: 7031.7161
                    Surrogate loss: 0.0137
             Mean action noise std: 0.97
                       Mean reward: 555.37
               Mean episode length: 204.19
                 Mean success rate: 9.50
                  Mean reward/step: 3.35
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 5267456
                    Iteration time: 2.61s
                        Total time: 1656.69s
                               ETA: 8651.9s

################################################################################
                     [1m Learning iteration 643/4000 [0m

                       Computation: 3242 steps/s (collection: 0.476s, learning 2.051s)
               Value function loss: 7450.1569
                    Surrogate loss: 0.0146
             Mean action noise std: 0.97
                       Mean reward: 549.09
               Mean episode length: 204.29
                 Mean success rate: 8.50
                  Mean reward/step: 3.52
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 2.53s
                        Total time: 1659.22s
                               ETA: 8649.1s

################################################################################
                     [1m Learning iteration 644/4000 [0m

                       Computation: 3261 steps/s (collection: 0.463s, learning 2.049s)
               Value function loss: 8102.8801
                    Surrogate loss: 0.0143
             Mean action noise std: 0.97
                       Mean reward: 517.16
               Mean episode length: 207.53
                 Mean success rate: 7.50
                  Mean reward/step: 3.56
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5283840
                    Iteration time: 2.51s
                        Total time: 1661.73s
                               ETA: 8646.2s

################################################################################
                     [1m Learning iteration 645/4000 [0m

                       Computation: 3179 steps/s (collection: 0.499s, learning 2.077s)
               Value function loss: 7386.1278
                    Surrogate loss: 0.0129
             Mean action noise std: 0.97
                       Mean reward: 504.93
               Mean episode length: 200.94
                 Mean success rate: 8.50
                  Mean reward/step: 3.55
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 2.58s
                        Total time: 1664.31s
                               ETA: 8643.6s

################################################################################
                     [1m Learning iteration 646/4000 [0m

                       Computation: 3136 steps/s (collection: 0.542s, learning 2.070s)
               Value function loss: 6932.1664
                    Surrogate loss: 0.0118
             Mean action noise std: 0.97
                       Mean reward: 525.51
               Mean episode length: 203.88
                 Mean success rate: 9.50
                  Mean reward/step: 3.45
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5300224
                    Iteration time: 2.61s
                        Total time: 1666.92s
                               ETA: 8641.2s

################################################################################
                     [1m Learning iteration 647/4000 [0m

                       Computation: 3218 steps/s (collection: 0.487s, learning 2.058s)
               Value function loss: 10707.4689
                    Surrogate loss: 0.0116
             Mean action noise std: 0.97
                       Mean reward: 599.14
               Mean episode length: 213.96
                 Mean success rate: 12.50
                  Mean reward/step: 3.40
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 2.55s
                        Total time: 1669.47s
                               ETA: 8638.5s

################################################################################
                     [1m Learning iteration 648/4000 [0m

                       Computation: 3150 steps/s (collection: 0.522s, learning 2.078s)
               Value function loss: 7647.4205
                    Surrogate loss: 0.0121
             Mean action noise std: 0.97
                       Mean reward: 711.80
               Mean episode length: 230.42
                 Mean success rate: 16.50
                  Mean reward/step: 3.48
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5316608
                    Iteration time: 2.60s
                        Total time: 1672.07s
                               ETA: 8636.0s

################################################################################
                     [1m Learning iteration 649/4000 [0m

                       Computation: 3134 steps/s (collection: 0.484s, learning 2.129s)
               Value function loss: 9574.6922
                    Surrogate loss: 0.0153
             Mean action noise std: 0.97
                       Mean reward: 751.71
               Mean episode length: 223.98
                 Mean success rate: 18.00
                  Mean reward/step: 3.59
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 2.61s
                        Total time: 1674.68s
                               ETA: 8633.6s

################################################################################
                     [1m Learning iteration 650/4000 [0m

                       Computation: 3167 steps/s (collection: 0.531s, learning 2.055s)
               Value function loss: 6235.5534
                    Surrogate loss: 0.0120
             Mean action noise std: 0.97
                       Mean reward: 803.28
               Mean episode length: 233.45
                 Mean success rate: 18.50
                  Mean reward/step: 3.17
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 5332992
                    Iteration time: 2.59s
                        Total time: 1677.27s
                               ETA: 8631.1s

################################################################################
                     [1m Learning iteration 651/4000 [0m

                       Computation: 3115 steps/s (collection: 0.484s, learning 2.146s)
               Value function loss: 6904.2958
                    Surrogate loss: 0.0140
             Mean action noise std: 0.97
                       Mean reward: 857.20
               Mean episode length: 251.21
                 Mean success rate: 18.00
                  Mean reward/step: 3.25
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 2.63s
                        Total time: 1679.90s
                               ETA: 8628.8s

################################################################################
                     [1m Learning iteration 652/4000 [0m

                       Computation: 3232 steps/s (collection: 0.475s, learning 2.060s)
               Value function loss: 2451.0897
                    Surrogate loss: 0.0151
             Mean action noise std: 0.97
                       Mean reward: 859.56
               Mean episode length: 252.90
                 Mean success rate: 18.00
                  Mean reward/step: 3.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5349376
                    Iteration time: 2.53s
                        Total time: 1682.43s
                               ETA: 8626.0s

################################################################################
                     [1m Learning iteration 653/4000 [0m

                       Computation: 3146 steps/s (collection: 0.511s, learning 2.093s)
               Value function loss: 6973.9242
                    Surrogate loss: 0.0131
             Mean action noise std: 0.97
                       Mean reward: 842.43
               Mean episode length: 256.52
                 Mean success rate: 16.50
                  Mean reward/step: 3.35
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 2.60s
                        Total time: 1685.03s
                               ETA: 8623.6s

################################################################################
                     [1m Learning iteration 654/4000 [0m

                       Computation: 3152 steps/s (collection: 0.524s, learning 2.075s)
               Value function loss: 7361.8829
                    Surrogate loss: 0.0112
             Mean action noise std: 0.97
                       Mean reward: 832.22
               Mean episode length: 247.80
                 Mean success rate: 14.50
                  Mean reward/step: 3.05
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 5365760
                    Iteration time: 2.60s
                        Total time: 1687.63s
                               ETA: 8621.1s

################################################################################
                     [1m Learning iteration 655/4000 [0m

                       Computation: 3117 steps/s (collection: 0.525s, learning 2.103s)
               Value function loss: 5944.1234
                    Surrogate loss: 0.0128
             Mean action noise std: 0.97
                       Mean reward: 758.41
               Mean episode length: 244.23
                 Mean success rate: 13.00
                  Mean reward/step: 3.04
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 2.63s
                        Total time: 1690.26s
                               ETA: 8618.8s

################################################################################
                     [1m Learning iteration 656/4000 [0m

                       Computation: 3170 steps/s (collection: 0.510s, learning 2.074s)
               Value function loss: 4799.8041
                    Surrogate loss: 0.0155
             Mean action noise std: 0.97
                       Mean reward: 788.64
               Mean episode length: 245.25
                 Mean success rate: 14.00
                  Mean reward/step: 3.26
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5382144
                    Iteration time: 2.58s
                        Total time: 1692.84s
                               ETA: 8616.2s

################################################################################
                     [1m Learning iteration 657/4000 [0m

                       Computation: 3152 steps/s (collection: 0.505s, learning 2.093s)
               Value function loss: 7813.0498
                    Surrogate loss: 0.0132
             Mean action noise std: 0.97
                       Mean reward: 750.34
               Mean episode length: 231.69
                 Mean success rate: 14.50
                  Mean reward/step: 3.26
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 2.60s
                        Total time: 1695.44s
                               ETA: 8613.8s

################################################################################
                     [1m Learning iteration 658/4000 [0m

                       Computation: 3176 steps/s (collection: 0.477s, learning 2.102s)
               Value function loss: 9628.9532
                    Surrogate loss: 0.0117
             Mean action noise std: 0.97
                       Mean reward: 747.47
               Mean episode length: 223.04
                 Mean success rate: 15.00
                  Mean reward/step: 3.49
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 5398528
                    Iteration time: 2.58s
                        Total time: 1698.02s
                               ETA: 8611.2s

################################################################################
                     [1m Learning iteration 659/4000 [0m

                       Computation: 3204 steps/s (collection: 0.493s, learning 2.063s)
               Value function loss: 8800.8374
                    Surrogate loss: 0.0120
             Mean action noise std: 0.97
                       Mean reward: 829.70
               Mean episode length: 224.91
                 Mean success rate: 16.50
                  Mean reward/step: 3.17
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 2.56s
                        Total time: 1700.58s
                               ETA: 8608.5s

################################################################################
                     [1m Learning iteration 660/4000 [0m

                       Computation: 3182 steps/s (collection: 0.491s, learning 2.083s)
               Value function loss: 5632.0373
                    Surrogate loss: 0.0129
             Mean action noise std: 0.97
                       Mean reward: 812.29
               Mean episode length: 220.96
                 Mean success rate: 16.00
                  Mean reward/step: 3.05
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5414912
                    Iteration time: 2.57s
                        Total time: 1703.15s
                               ETA: 8605.9s

################################################################################
                     [1m Learning iteration 661/4000 [0m

                       Computation: 3251 steps/s (collection: 0.475s, learning 2.044s)
               Value function loss: 9109.3494
                    Surrogate loss: 0.0116
             Mean action noise std: 0.97
                       Mean reward: 842.31
               Mean episode length: 230.11
                 Mean success rate: 15.50
                  Mean reward/step: 3.28
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 2.52s
                        Total time: 1705.67s
                               ETA: 8603.1s

################################################################################
                     [1m Learning iteration 662/4000 [0m

                       Computation: 3211 steps/s (collection: 0.504s, learning 2.047s)
               Value function loss: 9287.1285
                    Surrogate loss: 0.0146
             Mean action noise std: 0.97
                       Mean reward: 865.93
               Mean episode length: 228.18
                 Mean success rate: 15.00
                  Mean reward/step: 3.65
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5431296
                    Iteration time: 2.55s
                        Total time: 1708.22s
                               ETA: 8600.4s

################################################################################
                     [1m Learning iteration 663/4000 [0m

                       Computation: 3151 steps/s (collection: 0.511s, learning 2.088s)
               Value function loss: 8298.4553
                    Surrogate loss: 0.0128
             Mean action noise std: 0.97
                       Mean reward: 868.32
               Mean episode length: 225.26
                 Mean success rate: 14.00
                  Mean reward/step: 3.94
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 2.60s
                        Total time: 1710.82s
                               ETA: 8597.9s

################################################################################
                     [1m Learning iteration 664/4000 [0m

                       Computation: 3145 steps/s (collection: 0.482s, learning 2.122s)
               Value function loss: 9070.3238
                    Surrogate loss: 0.0144
             Mean action noise std: 0.97
                       Mean reward: 869.73
               Mean episode length: 241.10
                 Mean success rate: 14.00
                  Mean reward/step: 4.40
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5447680
                    Iteration time: 2.60s
                        Total time: 1713.43s
                               ETA: 8595.5s

################################################################################
                     [1m Learning iteration 665/4000 [0m

                       Computation: 3201 steps/s (collection: 0.480s, learning 2.078s)
               Value function loss: 10299.7204
                    Surrogate loss: 0.0114
             Mean action noise std: 0.97
                       Mean reward: 794.70
               Mean episode length: 247.08
                 Mean success rate: 12.50
                  Mean reward/step: 4.72
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 2.56s
                        Total time: 1715.98s
                               ETA: 8592.8s

################################################################################
                     [1m Learning iteration 666/4000 [0m

                       Computation: 3238 steps/s (collection: 0.487s, learning 2.042s)
               Value function loss: 12500.2554
                    Surrogate loss: 0.0102
             Mean action noise std: 0.97
                       Mean reward: 776.88
               Mean episode length: 244.99
                 Mean success rate: 12.00
                  Mean reward/step: 4.53
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 5464064
                    Iteration time: 2.53s
                        Total time: 1718.51s
                               ETA: 8590.0s

################################################################################
                     [1m Learning iteration 667/4000 [0m

                       Computation: 3202 steps/s (collection: 0.482s, learning 2.076s)
               Value function loss: 10185.4999
                    Surrogate loss: 0.0139
             Mean action noise std: 0.97
                       Mean reward: 748.79
               Mean episode length: 255.28
                 Mean success rate: 11.50
                  Mean reward/step: 4.45
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 2.56s
                        Total time: 1721.07s
                               ETA: 8587.3s

################################################################################
                     [1m Learning iteration 668/4000 [0m

                       Computation: 3113 steps/s (collection: 0.537s, learning 2.095s)
               Value function loss: 10818.3682
                    Surrogate loss: 0.0144
             Mean action noise std: 0.97
                       Mean reward: 734.29
               Mean episode length: 242.31
                 Mean success rate: 11.50
                  Mean reward/step: 4.38
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5480448
                    Iteration time: 2.63s
                        Total time: 1723.70s
                               ETA: 8585.0s

################################################################################
                     [1m Learning iteration 669/4000 [0m

                       Computation: 3195 steps/s (collection: 0.490s, learning 2.074s)
               Value function loss: 16300.7592
                    Surrogate loss: 0.0119
             Mean action noise std: 0.97
                       Mean reward: 785.36
               Mean episode length: 232.22
                 Mean success rate: 13.00
                  Mean reward/step: 4.60
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 2.56s
                        Total time: 1726.27s
                               ETA: 8582.4s

################################################################################
                     [1m Learning iteration 670/4000 [0m

                       Computation: 3250 steps/s (collection: 0.482s, learning 2.039s)
               Value function loss: 14877.7444
                    Surrogate loss: 0.0119
             Mean action noise std: 0.97
                       Mean reward: 782.33
               Mean episode length: 233.18
                 Mean success rate: 12.50
                  Mean reward/step: 4.71
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5496832
                    Iteration time: 2.52s
                        Total time: 1728.79s
                               ETA: 8579.5s

################################################################################
                     [1m Learning iteration 671/4000 [0m

                       Computation: 3206 steps/s (collection: 0.497s, learning 2.058s)
               Value function loss: 12836.2629
                    Surrogate loss: 0.0118
             Mean action noise std: 0.97
                       Mean reward: 791.54
               Mean episode length: 230.47
                 Mean success rate: 12.50
                  Mean reward/step: 4.80
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 2.55s
                        Total time: 1731.34s
                               ETA: 8576.8s

################################################################################
                     [1m Learning iteration 672/4000 [0m

                       Computation: 3252 steps/s (collection: 0.483s, learning 2.036s)
               Value function loss: 15829.4898
                    Surrogate loss: 0.0113
             Mean action noise std: 0.97
                       Mean reward: 860.69
               Mean episode length: 226.80
                 Mean success rate: 14.00
                  Mean reward/step: 5.16
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5513216
                    Iteration time: 2.52s
                        Total time: 1733.86s
                               ETA: 8574.0s

################################################################################
                     [1m Learning iteration 673/4000 [0m

                       Computation: 3102 steps/s (collection: 0.509s, learning 2.131s)
               Value function loss: 19837.0713
                    Surrogate loss: 0.0107
             Mean action noise std: 0.97
                       Mean reward: 979.22
               Mean episode length: 222.48
                 Mean success rate: 16.00
                  Mean reward/step: 4.80
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 2.64s
                        Total time: 1736.50s
                               ETA: 8571.7s

################################################################################
                     [1m Learning iteration 674/4000 [0m

                       Computation: 3152 steps/s (collection: 0.552s, learning 2.046s)
               Value function loss: 12820.8529
                    Surrogate loss: 0.0134
             Mean action noise std: 0.97
                       Mean reward: 1106.61
               Mean episode length: 234.82
                 Mean success rate: 18.50
                  Mean reward/step: 4.61
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5529600
                    Iteration time: 2.60s
                        Total time: 1739.10s
                               ETA: 8569.2s

################################################################################
                     [1m Learning iteration 675/4000 [0m

                       Computation: 3239 steps/s (collection: 0.452s, learning 2.077s)
               Value function loss: 16272.9065
                    Surrogate loss: 0.0127
             Mean action noise std: 0.97
                       Mean reward: 1165.14
               Mean episode length: 242.01
                 Mean success rate: 17.50
                  Mean reward/step: 4.52
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 2.53s
                        Total time: 1741.63s
                               ETA: 8566.4s

################################################################################
                     [1m Learning iteration 676/4000 [0m

                       Computation: 3242 steps/s (collection: 0.476s, learning 2.051s)
               Value function loss: 14027.4656
                    Surrogate loss: 0.0132
             Mean action noise std: 0.97
                       Mean reward: 1168.73
               Mean episode length: 239.47
                 Mean success rate: 19.00
                  Mean reward/step: 5.22
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5545984
                    Iteration time: 2.53s
                        Total time: 1744.15s
                               ETA: 8563.6s

################################################################################
                     [1m Learning iteration 677/4000 [0m

                       Computation: 3144 steps/s (collection: 0.494s, learning 2.111s)
               Value function loss: 19972.7807
                    Surrogate loss: 0.0086
             Mean action noise std: 0.97
                       Mean reward: 1260.95
               Mean episode length: 239.44
                 Mean success rate: 20.00
                  Mean reward/step: 4.68
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 2.61s
                        Total time: 1746.76s
                               ETA: 8561.2s

################################################################################
                     [1m Learning iteration 678/4000 [0m

                       Computation: 3208 steps/s (collection: 0.494s, learning 2.059s)
               Value function loss: 13353.8847
                    Surrogate loss: 0.0143
             Mean action noise std: 0.97
                       Mean reward: 1335.34
               Mean episode length: 251.19
                 Mean success rate: 21.00
                  Mean reward/step: 4.30
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 5562368
                    Iteration time: 2.55s
                        Total time: 1749.31s
                               ETA: 8558.5s

################################################################################
                     [1m Learning iteration 679/4000 [0m

                       Computation: 3216 steps/s (collection: 0.478s, learning 2.069s)
               Value function loss: 13003.5599
                    Surrogate loss: 0.0135
             Mean action noise std: 0.97
                       Mean reward: 1262.03
               Mean episode length: 253.78
                 Mean success rate: 20.50
                  Mean reward/step: 4.30
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 2.55s
                        Total time: 1751.86s
                               ETA: 8555.8s

################################################################################
                     [1m Learning iteration 680/4000 [0m

                       Computation: 3231 steps/s (collection: 0.451s, learning 2.085s)
               Value function loss: 14404.8848
                    Surrogate loss: 0.0149
             Mean action noise std: 0.97
                       Mean reward: 1215.13
               Mean episode length: 257.01
                 Mean success rate: 20.00
                  Mean reward/step: 4.33
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 5578752
                    Iteration time: 2.54s
                        Total time: 1754.39s
                               ETA: 8553.0s

################################################################################
                     [1m Learning iteration 681/4000 [0m

                       Computation: 3217 steps/s (collection: 0.489s, learning 2.057s)
               Value function loss: 16467.8431
                    Surrogate loss: 0.0156
             Mean action noise std: 0.97
                       Mean reward: 1098.80
               Mean episode length: 257.99
                 Mean success rate: 19.50
                  Mean reward/step: 4.34
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 2.55s
                        Total time: 1756.94s
                               ETA: 8550.3s

################################################################################
                     [1m Learning iteration 682/4000 [0m

                       Computation: 3200 steps/s (collection: 0.495s, learning 2.065s)
               Value function loss: 21960.9954
                    Surrogate loss: 0.0123
             Mean action noise std: 0.97
                       Mean reward: 1093.93
               Mean episode length: 260.52
                 Mean success rate: 18.00
                  Mean reward/step: 4.43
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 5595136
                    Iteration time: 2.56s
                        Total time: 1759.50s
                               ETA: 8547.6s

################################################################################
                     [1m Learning iteration 683/4000 [0m

                       Computation: 3207 steps/s (collection: 0.498s, learning 2.056s)
               Value function loss: 13989.0203
                    Surrogate loss: 0.0157
             Mean action noise std: 0.97
                       Mean reward: 1117.50
               Mean episode length: 273.96
                 Mean success rate: 18.00
                  Mean reward/step: 4.58
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 2.55s
                        Total time: 1762.05s
                               ETA: 8544.9s

################################################################################
                     [1m Learning iteration 684/4000 [0m

                       Computation: 3216 steps/s (collection: 0.485s, learning 2.062s)
               Value function loss: 10749.5469
                    Surrogate loss: 0.0160
             Mean action noise std: 0.97
                       Mean reward: 1100.02
               Mean episode length: 264.60
                 Mean success rate: 19.00
                  Mean reward/step: 4.52
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5611520
                    Iteration time: 2.55s
                        Total time: 1764.60s
                               ETA: 8542.2s

################################################################################
                     [1m Learning iteration 685/4000 [0m

                       Computation: 3143 steps/s (collection: 0.571s, learning 2.035s)
               Value function loss: 13808.9994
                    Surrogate loss: 0.0138
             Mean action noise std: 0.97
                       Mean reward: 1059.35
               Mean episode length: 255.25
                 Mean success rate: 17.50
                  Mean reward/step: 4.32
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 2.61s
                        Total time: 1767.21s
                               ETA: 8539.8s

################################################################################
                     [1m Learning iteration 686/4000 [0m

                       Computation: 3197 steps/s (collection: 0.508s, learning 2.054s)
               Value function loss: 13127.2750
                    Surrogate loss: 0.0113
             Mean action noise std: 0.97
                       Mean reward: 1092.51
               Mean episode length: 253.41
                 Mean success rate: 17.50
                  Mean reward/step: 4.69
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5627904
                    Iteration time: 2.56s
                        Total time: 1769.77s
                               ETA: 8537.1s

################################################################################
                     [1m Learning iteration 687/4000 [0m

                       Computation: 3222 steps/s (collection: 0.500s, learning 2.042s)
               Value function loss: 15134.8392
                    Surrogate loss: 0.0142
             Mean action noise std: 0.97
                       Mean reward: 1135.91
               Mean episode length: 251.73
                 Mean success rate: 17.00
                  Mean reward/step: 4.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 2.54s
                        Total time: 1772.31s
                               ETA: 8534.4s

################################################################################
                     [1m Learning iteration 688/4000 [0m

                       Computation: 3200 steps/s (collection: 0.494s, learning 2.066s)
               Value function loss: 15287.1135
                    Surrogate loss: 0.0133
             Mean action noise std: 0.97
                       Mean reward: 1093.08
               Mean episode length: 241.31
                 Mean success rate: 17.50
                  Mean reward/step: 4.91
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5644288
                    Iteration time: 2.56s
                        Total time: 1774.87s
                               ETA: 8531.7s

################################################################################
                     [1m Learning iteration 689/4000 [0m

                       Computation: 3237 steps/s (collection: 0.481s, learning 2.050s)
               Value function loss: 12772.5670
                    Surrogate loss: 0.0161
             Mean action noise std: 0.97
                       Mean reward: 1121.57
               Mean episode length: 239.21
                 Mean success rate: 18.00
                  Mean reward/step: 4.90
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 2.53s
                        Total time: 1777.40s
                               ETA: 8528.9s

################################################################################
                     [1m Learning iteration 690/4000 [0m

                       Computation: 3199 steps/s (collection: 0.489s, learning 2.071s)
               Value function loss: 22902.2533
                    Surrogate loss: 0.0113
             Mean action noise std: 0.97
                       Mean reward: 1227.20
               Mean episode length: 244.69
                 Mean success rate: 19.00
                  Mean reward/step: 4.75
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 5660672
                    Iteration time: 2.56s
                        Total time: 1779.96s
                               ETA: 8526.3s

################################################################################
                     [1m Learning iteration 691/4000 [0m

                       Computation: 3173 steps/s (collection: 0.499s, learning 2.082s)
               Value function loss: 10421.6724
                    Surrogate loss: 0.0131
             Mean action noise std: 0.97
                       Mean reward: 1224.11
               Mean episode length: 248.00
                 Mean success rate: 19.00
                  Mean reward/step: 4.96
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 2.58s
                        Total time: 1782.54s
                               ETA: 8523.7s

################################################################################
                     [1m Learning iteration 692/4000 [0m

                       Computation: 3199 steps/s (collection: 0.500s, learning 2.060s)
               Value function loss: 14482.0354
                    Surrogate loss: 0.0129
             Mean action noise std: 0.97
                       Mean reward: 1184.23
               Mean episode length: 246.88
                 Mean success rate: 19.00
                  Mean reward/step: 4.97
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5677056
                    Iteration time: 2.56s
                        Total time: 1785.10s
                               ETA: 8521.1s

################################################################################
                     [1m Learning iteration 693/4000 [0m

                       Computation: 3195 steps/s (collection: 0.508s, learning 2.055s)
               Value function loss: 15092.9858
                    Surrogate loss: 0.0133
             Mean action noise std: 0.97
                       Mean reward: 1232.02
               Mean episode length: 246.91
                 Mean success rate: 21.00
                  Mean reward/step: 4.65
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 2.56s
                        Total time: 1787.67s
                               ETA: 8518.5s

################################################################################
                     [1m Learning iteration 694/4000 [0m

                       Computation: 3209 steps/s (collection: 0.506s, learning 2.047s)
               Value function loss: 9657.6029
                    Surrogate loss: 0.0177
             Mean action noise std: 0.97
                       Mean reward: 1185.27
               Mean episode length: 247.55
                 Mean success rate: 20.00
                  Mean reward/step: 4.66
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5693440
                    Iteration time: 2.55s
                        Total time: 1790.22s
                               ETA: 8515.8s

################################################################################
                     [1m Learning iteration 695/4000 [0m

                       Computation: 3284 steps/s (collection: 0.475s, learning 2.019s)
               Value function loss: 16379.0895
                    Surrogate loss: 0.0137
             Mean action noise std: 0.97
                       Mean reward: 1109.67
               Mean episode length: 240.35
                 Mean success rate: 18.00
                  Mean reward/step: 4.84
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 2.49s
                        Total time: 1792.71s
                               ETA: 8512.8s

################################################################################
                     [1m Learning iteration 696/4000 [0m

                       Computation: 3226 steps/s (collection: 0.461s, learning 2.078s)
               Value function loss: 11958.2915
                    Surrogate loss: 0.0134
             Mean action noise std: 0.97
                       Mean reward: 1067.53
               Mean episode length: 244.66
                 Mean success rate: 18.50
                  Mean reward/step: 4.95
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5709824
                    Iteration time: 2.54s
                        Total time: 1795.25s
                               ETA: 8510.1s

################################################################################
                     [1m Learning iteration 697/4000 [0m

                       Computation: 3204 steps/s (collection: 0.497s, learning 2.060s)
               Value function loss: 19089.9863
                    Surrogate loss: 0.0127
             Mean action noise std: 0.97
                       Mean reward: 1152.98
               Mean episode length: 241.62
                 Mean success rate: 20.50
                  Mean reward/step: 5.07
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 2.56s
                        Total time: 1797.81s
                               ETA: 8507.4s

################################################################################
                     [1m Learning iteration 698/4000 [0m

                       Computation: 3196 steps/s (collection: 0.450s, learning 2.113s)
               Value function loss: 16739.1071
                    Surrogate loss: 0.0121
             Mean action noise std: 0.97
                       Mean reward: 1268.52
               Mean episode length: 249.93
                 Mean success rate: 22.50
                  Mean reward/step: 4.57
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5726208
                    Iteration time: 2.56s
                        Total time: 1800.37s
                               ETA: 8504.8s

################################################################################
                     [1m Learning iteration 699/4000 [0m

                       Computation: 3221 steps/s (collection: 0.432s, learning 2.111s)
               Value function loss: 15209.4218
                    Surrogate loss: 0.0110
             Mean action noise std: 0.97
                       Mean reward: 1195.37
               Mean episode length: 251.11
                 Mean success rate: 19.50
                  Mean reward/step: 4.48
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 2.54s
                        Total time: 1802.91s
                               ETA: 8502.0s

################################################################################
                     [1m Learning iteration 700/4000 [0m

                       Computation: 3270 steps/s (collection: 0.463s, learning 2.042s)
               Value function loss: 9969.8022
                    Surrogate loss: 0.0172
             Mean action noise std: 0.97
                       Mean reward: 1191.23
               Mean episode length: 253.78
                 Mean success rate: 19.50
                  Mean reward/step: 4.62
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5742592
                    Iteration time: 2.50s
                        Total time: 1805.42s
                               ETA: 8499.1s

################################################################################
                     [1m Learning iteration 701/4000 [0m

                       Computation: 3224 steps/s (collection: 0.453s, learning 2.088s)
               Value function loss: 16021.1816
                    Surrogate loss: 0.0140
             Mean action noise std: 0.97
                       Mean reward: 1301.86
               Mean episode length: 255.79
                 Mean success rate: 21.50
                  Mean reward/step: 4.58
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 2.54s
                        Total time: 1807.96s
                               ETA: 8496.4s

################################################################################
                     [1m Learning iteration 702/4000 [0m

                       Computation: 3196 steps/s (collection: 0.508s, learning 2.055s)
               Value function loss: 8624.0405
                    Surrogate loss: 0.0156
             Mean action noise std: 0.97
                       Mean reward: 1282.08
               Mean episode length: 256.77
                 Mean success rate: 21.00
                  Mean reward/step: 4.53
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5758976
                    Iteration time: 2.56s
                        Total time: 1810.52s
                               ETA: 8493.7s

################################################################################
                     [1m Learning iteration 703/4000 [0m

                       Computation: 3207 steps/s (collection: 0.451s, learning 2.104s)
               Value function loss: 14449.6811
                    Surrogate loss: 0.0122
             Mean action noise std: 0.97
                       Mean reward: 1298.32
               Mean episode length: 258.86
                 Mean success rate: 22.00
                  Mean reward/step: 4.76
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 2.55s
                        Total time: 1813.08s
                               ETA: 8491.1s

################################################################################
                     [1m Learning iteration 704/4000 [0m

                       Computation: 3223 steps/s (collection: 0.456s, learning 2.085s)
               Value function loss: 11393.9569
                    Surrogate loss: 0.0149
             Mean action noise std: 0.97
                       Mean reward: 1030.74
               Mean episode length: 252.62
                 Mean success rate: 17.00
                  Mean reward/step: 5.14
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5775360
                    Iteration time: 2.54s
                        Total time: 1815.62s
                               ETA: 8488.3s

################################################################################
                     [1m Learning iteration 705/4000 [0m

                       Computation: 3234 steps/s (collection: 0.457s, learning 2.076s)
               Value function loss: 19471.8874
                    Surrogate loss: 0.0126
             Mean action noise std: 0.97
                       Mean reward: 1127.57
               Mean episode length: 245.47
                 Mean success rate: 19.50
                  Mean reward/step: 5.27
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 2.53s
                        Total time: 1818.15s
                               ETA: 8485.6s

################################################################################
                     [1m Learning iteration 706/4000 [0m

                       Computation: 3109 steps/s (collection: 0.502s, learning 2.133s)
               Value function loss: 19076.6967
                    Surrogate loss: 0.0128
             Mean action noise std: 0.97
                       Mean reward: 1308.84
               Mean episode length: 250.04
                 Mean success rate: 23.00
                  Mean reward/step: 5.11
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5791744
                    Iteration time: 2.63s
                        Total time: 1820.78s
                               ETA: 8483.3s

################################################################################
                     [1m Learning iteration 707/4000 [0m

                       Computation: 3195 steps/s (collection: 0.470s, learning 2.093s)
               Value function loss: 14678.5897
                    Surrogate loss: 0.0125
             Mean action noise std: 0.97
                       Mean reward: 1183.31
               Mean episode length: 250.72
                 Mean success rate: 21.00
                  Mean reward/step: 4.99
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 2.56s
                        Total time: 1823.35s
                               ETA: 8480.6s

################################################################################
                     [1m Learning iteration 708/4000 [0m

                       Computation: 3234 steps/s (collection: 0.429s, learning 2.104s)
               Value function loss: 17495.9989
                    Surrogate loss: 0.0126
             Mean action noise std: 0.97
                       Mean reward: 1216.24
               Mean episode length: 248.12
                 Mean success rate: 21.00
                  Mean reward/step: 5.10
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5808128
                    Iteration time: 2.53s
                        Total time: 1825.88s
                               ETA: 8477.9s

################################################################################
                     [1m Learning iteration 709/4000 [0m

                       Computation: 3155 steps/s (collection: 0.487s, learning 2.109s)
               Value function loss: 21283.6112
                    Surrogate loss: 0.0113
             Mean action noise std: 0.97
                       Mean reward: 1279.69
               Mean episode length: 254.05
                 Mean success rate: 21.50
                  Mean reward/step: 4.76
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 2.60s
                        Total time: 1828.48s
                               ETA: 8475.4s

################################################################################
                     [1m Learning iteration 710/4000 [0m

                       Computation: 3165 steps/s (collection: 0.483s, learning 2.104s)
               Value function loss: 16965.7921
                    Surrogate loss: 0.0118
             Mean action noise std: 0.97
                       Mean reward: 1316.38
               Mean episode length: 251.66
                 Mean success rate: 22.00
                  Mean reward/step: 4.22
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5824512
                    Iteration time: 2.59s
                        Total time: 1831.06s
                               ETA: 8472.9s

################################################################################
                     [1m Learning iteration 711/4000 [0m

                       Computation: 3115 steps/s (collection: 0.498s, learning 2.131s)
               Value function loss: 14359.1780
                    Surrogate loss: 0.0136
             Mean action noise std: 0.97
                       Mean reward: 1242.27
               Mean episode length: 261.66
                 Mean success rate: 20.50
                  Mean reward/step: 4.32
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 2.63s
                        Total time: 1833.69s
                               ETA: 8470.5s

################################################################################
                     [1m Learning iteration 712/4000 [0m

                       Computation: 3191 steps/s (collection: 0.475s, learning 2.092s)
               Value function loss: 14226.1681
                    Surrogate loss: 0.0147
             Mean action noise std: 0.97
                       Mean reward: 1283.36
               Mean episode length: 266.39
                 Mean success rate: 19.50
                  Mean reward/step: 4.25
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5840896
                    Iteration time: 2.57s
                        Total time: 1836.26s
                               ETA: 8467.9s

################################################################################
                     [1m Learning iteration 713/4000 [0m

                       Computation: 3116 steps/s (collection: 0.536s, learning 2.093s)
               Value function loss: 16136.1599
                    Surrogate loss: 0.0128
             Mean action noise std: 0.97
                       Mean reward: 1277.71
               Mean episode length: 267.44
                 Mean success rate: 19.50
                  Mean reward/step: 4.29
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 2.63s
                        Total time: 1838.89s
                               ETA: 8465.6s

################################################################################
                     [1m Learning iteration 714/4000 [0m

                       Computation: 3095 steps/s (collection: 0.528s, learning 2.119s)
               Value function loss: 7689.3571
                    Surrogate loss: 0.0148
             Mean action noise std: 0.97
                       Mean reward: 1257.58
               Mean episode length: 262.03
                 Mean success rate: 19.50
                  Mean reward/step: 4.50
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5857280
                    Iteration time: 2.65s
                        Total time: 1841.54s
                               ETA: 8463.3s

################################################################################
                     [1m Learning iteration 715/4000 [0m

                       Computation: 3135 steps/s (collection: 0.482s, learning 2.130s)
               Value function loss: 14911.7901
                    Surrogate loss: 0.0105
             Mean action noise std: 0.97
                       Mean reward: 1248.81
               Mean episode length: 257.93
                 Mean success rate: 18.50
                  Mean reward/step: 4.88
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 2.61s
                        Total time: 1844.15s
                               ETA: 8460.9s

################################################################################
                     [1m Learning iteration 716/4000 [0m

                       Computation: 3191 steps/s (collection: 0.471s, learning 2.096s)
               Value function loss: 17805.8054
                    Surrogate loss: 0.0113
             Mean action noise std: 0.97
                       Mean reward: 1193.71
               Mean episode length: 249.16
                 Mean success rate: 18.50
                  Mean reward/step: 4.85
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5873664
                    Iteration time: 2.57s
                        Total time: 1846.72s
                               ETA: 8458.3s

################################################################################
                     [1m Learning iteration 717/4000 [0m

                       Computation: 3102 steps/s (collection: 0.493s, learning 2.148s)
               Value function loss: 19960.7773
                    Surrogate loss: 0.0123
             Mean action noise std: 0.97
                       Mean reward: 1093.98
               Mean episode length: 236.77
                 Mean success rate: 17.00
                  Mean reward/step: 4.97
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 2.64s
                        Total time: 1849.36s
                               ETA: 8456.0s

################################################################################
                     [1m Learning iteration 718/4000 [0m

                       Computation: 3102 steps/s (collection: 0.551s, learning 2.089s)
               Value function loss: 12962.1852
                    Surrogate loss: 0.0113
             Mean action noise std: 0.97
                       Mean reward: 928.95
               Mean episode length: 221.91
                 Mean success rate: 14.50
                  Mean reward/step: 5.15
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5890048
                    Iteration time: 2.64s
                        Total time: 1852.00s
                               ETA: 8453.8s

################################################################################
                     [1m Learning iteration 719/4000 [0m

                       Computation: 3134 steps/s (collection: 0.494s, learning 2.120s)
               Value function loss: 13601.0166
                    Surrogate loss: 0.0141
             Mean action noise std: 0.97
                       Mean reward: 941.72
               Mean episode length: 215.56
                 Mean success rate: 13.50
                  Mean reward/step: 4.79
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 2.61s
                        Total time: 1854.61s
                               ETA: 8451.3s

################################################################################
                     [1m Learning iteration 720/4000 [0m

                       Computation: 3135 steps/s (collection: 0.520s, learning 2.093s)
               Value function loss: 14432.3696
                    Surrogate loss: 0.0123
             Mean action noise std: 0.97
                       Mean reward: 894.81
               Mean episode length: 212.79
                 Mean success rate: 13.50
                  Mean reward/step: 4.86
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5906432
                    Iteration time: 2.61s
                        Total time: 1857.22s
                               ETA: 8448.9s

################################################################################
                     [1m Learning iteration 721/4000 [0m

                       Computation: 3243 steps/s (collection: 0.490s, learning 2.035s)
               Value function loss: 19211.7622
                    Surrogate loss: 0.0116
             Mean action noise std: 0.97
                       Mean reward: 948.86
               Mean episode length: 214.78
                 Mean success rate: 12.50
                  Mean reward/step: 4.73
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 2.53s
                        Total time: 1859.75s
                               ETA: 8446.1s

################################################################################
                     [1m Learning iteration 722/4000 [0m

                       Computation: 3276 steps/s (collection: 0.473s, learning 2.026s)
               Value function loss: 11618.6079
                    Surrogate loss: 0.0131
             Mean action noise std: 0.97
                       Mean reward: 945.46
               Mean episode length: 215.38
                 Mean success rate: 12.50
                  Mean reward/step: 4.97
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5922816
                    Iteration time: 2.50s
                        Total time: 1862.25s
                               ETA: 8443.2s

################################################################################
                     [1m Learning iteration 723/4000 [0m

                       Computation: 3151 steps/s (collection: 0.501s, learning 2.099s)
               Value function loss: 20497.0965
                    Surrogate loss: 0.0129
             Mean action noise std: 0.97
                       Mean reward: 971.76
               Mean episode length: 218.72
                 Mean success rate: 14.00
                  Mean reward/step: 5.35
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 2.60s
                        Total time: 1864.85s
                               ETA: 8440.7s

################################################################################
                     [1m Learning iteration 724/4000 [0m

                       Computation: 3252 steps/s (collection: 0.485s, learning 2.033s)
               Value function loss: 13317.4399
                    Surrogate loss: 0.0131
             Mean action noise std: 0.97
                       Mean reward: 866.92
               Mean episode length: 218.01
                 Mean success rate: 12.00
                  Mean reward/step: 5.22
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5939200
                    Iteration time: 2.52s
                        Total time: 1867.37s
                               ETA: 8437.9s

################################################################################
                     [1m Learning iteration 725/4000 [0m

                       Computation: 3179 steps/s (collection: 0.461s, learning 2.115s)
               Value function loss: 20696.3387
                    Surrogate loss: 0.0132
             Mean action noise std: 0.97
                       Mean reward: 968.58
               Mean episode length: 218.47
                 Mean success rate: 13.00
                  Mean reward/step: 4.92
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 2.58s
                        Total time: 1869.94s
                               ETA: 8435.3s

################################################################################
                     [1m Learning iteration 726/4000 [0m

                       Computation: 3195 steps/s (collection: 0.508s, learning 2.055s)
               Value function loss: 13403.8127
                    Surrogate loss: 0.0125
             Mean action noise std: 0.97
                       Mean reward: 971.14
               Mean episode length: 215.57
                 Mean success rate: 13.00
                  Mean reward/step: 5.61
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5955584
                    Iteration time: 2.56s
                        Total time: 1872.50s
                               ETA: 8432.7s

################################################################################
                     [1m Learning iteration 727/4000 [0m

                       Computation: 3127 steps/s (collection: 0.559s, learning 2.060s)
               Value function loss: 20862.6136
                    Surrogate loss: 0.0113
             Mean action noise std: 0.97
                       Mean reward: 1093.31
               Mean episode length: 220.81
                 Mean success rate: 16.00
                  Mean reward/step: 5.55
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 2.62s
                        Total time: 1875.12s
                               ETA: 8430.3s

################################################################################
                     [1m Learning iteration 728/4000 [0m

                       Computation: 3219 steps/s (collection: 0.482s, learning 2.063s)
               Value function loss: 14844.0739
                    Surrogate loss: 0.0129
             Mean action noise std: 0.97
                       Mean reward: 1163.31
               Mean episode length: 219.78
                 Mean success rate: 17.00
                  Mean reward/step: 5.25
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5971968
                    Iteration time: 2.54s
                        Total time: 1877.67s
                               ETA: 8427.6s

################################################################################
                     [1m Learning iteration 729/4000 [0m

                       Computation: 3231 steps/s (collection: 0.485s, learning 2.050s)
               Value function loss: 18048.9437
                    Surrogate loss: 0.0131
             Mean action noise std: 0.97
                       Mean reward: 1249.97
               Mean episode length: 223.00
                 Mean success rate: 18.00
                  Mean reward/step: 5.08
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 2.54s
                        Total time: 1880.20s
                               ETA: 8424.9s

################################################################################
                     [1m Learning iteration 730/4000 [0m

                       Computation: 3176 steps/s (collection: 0.500s, learning 2.079s)
               Value function loss: 24005.4079
                    Surrogate loss: 0.0125
             Mean action noise std: 0.97
                       Mean reward: 1345.37
               Mean episode length: 227.94
                 Mean success rate: 19.50
                  Mean reward/step: 5.19
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 5988352
                    Iteration time: 2.58s
                        Total time: 1882.78s
                               ETA: 8422.3s

################################################################################
                     [1m Learning iteration 731/4000 [0m

                       Computation: 3235 steps/s (collection: 0.500s, learning 2.032s)
               Value function loss: 15565.2504
                    Surrogate loss: 0.0130
             Mean action noise std: 0.97
                       Mean reward: 1389.12
               Mean episode length: 234.33
                 Mean success rate: 21.50
                  Mean reward/step: 4.90
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 2.53s
                        Total time: 1885.32s
                               ETA: 8419.5s

################################################################################
                     [1m Learning iteration 732/4000 [0m

                       Computation: 3213 steps/s (collection: 0.482s, learning 2.067s)
               Value function loss: 7700.1830
                    Surrogate loss: 0.0202
             Mean action noise std: 0.96
                       Mean reward: 1275.70
               Mean episode length: 238.91
                 Mean success rate: 19.50
                  Mean reward/step: 5.34
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6004736
                    Iteration time: 2.55s
                        Total time: 1887.86s
                               ETA: 8416.8s

################################################################################
                     [1m Learning iteration 733/4000 [0m

                       Computation: 3128 steps/s (collection: 0.493s, learning 2.126s)
               Value function loss: 11890.9447
                    Surrogate loss: 0.0258
             Mean action noise std: 0.97
                       Mean reward: 1187.83
               Mean episode length: 236.91
                 Mean success rate: 17.50
                  Mean reward/step: 5.60
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 2.62s
                        Total time: 1890.48s
                               ETA: 8414.5s

################################################################################
                     [1m Learning iteration 734/4000 [0m

                       Computation: 3099 steps/s (collection: 0.508s, learning 2.135s)
               Value function loss: 25481.8443
                    Surrogate loss: 0.0142
             Mean action noise std: 0.96
                       Mean reward: 1163.40
               Mean episode length: 249.09
                 Mean success rate: 17.50
                  Mean reward/step: 5.38
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 6021120
                    Iteration time: 2.64s
                        Total time: 1893.13s
                               ETA: 8412.2s

################################################################################
                     [1m Learning iteration 735/4000 [0m

                       Computation: 3091 steps/s (collection: 0.512s, learning 2.138s)
               Value function loss: 19906.5554
                    Surrogate loss: 0.0090
             Mean action noise std: 0.96
                       Mean reward: 1278.07
               Mean episode length: 243.30
                 Mean success rate: 18.00
                  Mean reward/step: 5.23
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 2.65s
                        Total time: 1895.78s
                               ETA: 8409.9s

################################################################################
                     [1m Learning iteration 736/4000 [0m

                       Computation: 3156 steps/s (collection: 0.519s, learning 2.076s)
               Value function loss: 28951.0906
                    Surrogate loss: 0.0085
             Mean action noise std: 0.96
                       Mean reward: 1308.48
               Mean episode length: 246.45
                 Mean success rate: 19.00
                  Mean reward/step: 5.18
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6037504
                    Iteration time: 2.60s
                        Total time: 1898.37s
                               ETA: 8407.4s

################################################################################
                     [1m Learning iteration 737/4000 [0m

                       Computation: 3148 steps/s (collection: 0.506s, learning 2.097s)
               Value function loss: 11893.5245
                    Surrogate loss: 0.0135
             Mean action noise std: 0.96
                       Mean reward: 1295.43
               Mean episode length: 241.45
                 Mean success rate: 18.00
                  Mean reward/step: 5.13
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 2.60s
                        Total time: 1900.97s
                               ETA: 8405.0s

################################################################################
                     [1m Learning iteration 738/4000 [0m

                       Computation: 3092 steps/s (collection: 0.516s, learning 2.133s)
               Value function loss: 22395.1719
                    Surrogate loss: 0.0148
             Mean action noise std: 0.96
                       Mean reward: 1307.70
               Mean episode length: 245.47
                 Mean success rate: 18.00
                  Mean reward/step: 5.53
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6053888
                    Iteration time: 2.65s
                        Total time: 1903.62s
                               ETA: 8402.7s

################################################################################
                     [1m Learning iteration 739/4000 [0m

                       Computation: 3141 steps/s (collection: 0.498s, learning 2.110s)
               Value function loss: 17640.7819
                    Surrogate loss: 0.0110
             Mean action noise std: 0.96
                       Mean reward: 1291.64
               Mean episode length: 237.19
                 Mean success rate: 17.50
                  Mean reward/step: 5.44
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 2.61s
                        Total time: 1906.23s
                               ETA: 8400.3s

################################################################################
                     [1m Learning iteration 740/4000 [0m

                       Computation: 3167 steps/s (collection: 0.476s, learning 2.110s)
               Value function loss: 18991.0037
                    Surrogate loss: 0.0080
             Mean action noise std: 0.96
                       Mean reward: 1276.26
               Mean episode length: 232.01
                 Mean success rate: 17.50
                  Mean reward/step: 5.99
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6070272
                    Iteration time: 2.59s
                        Total time: 1908.82s
                               ETA: 8397.8s

################################################################################
                     [1m Learning iteration 741/4000 [0m

                       Computation: 3214 steps/s (collection: 0.465s, learning 2.083s)
               Value function loss: 23192.0256
                    Surrogate loss: 0.0091
             Mean action noise std: 0.96
                       Mean reward: 1200.42
               Mean episode length: 236.41
                 Mean success rate: 17.00
                  Mean reward/step: 5.84
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 2.55s
                        Total time: 1911.36s
                               ETA: 8395.1s

################################################################################
                     [1m Learning iteration 742/4000 [0m

                       Computation: 3082 steps/s (collection: 0.538s, learning 2.119s)
               Value function loss: 25985.5396
                    Surrogate loss: 0.0136
             Mean action noise std: 0.96
                       Mean reward: 1157.64
               Mean episode length: 246.59
                 Mean success rate: 16.00
                  Mean reward/step: 5.55
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6086656
                    Iteration time: 2.66s
                        Total time: 1914.02s
                               ETA: 8392.8s

################################################################################
                     [1m Learning iteration 743/4000 [0m

                       Computation: 3082 steps/s (collection: 0.506s, learning 2.152s)
               Value function loss: 21906.1423
                    Surrogate loss: 0.0110
             Mean action noise std: 0.96
                       Mean reward: 1314.27
               Mean episode length: 250.81
                 Mean success rate: 18.00
                  Mean reward/step: 5.00
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 2.66s
                        Total time: 1916.68s
                               ETA: 8390.6s

################################################################################
                     [1m Learning iteration 744/4000 [0m

                       Computation: 3105 steps/s (collection: 0.515s, learning 2.123s)
               Value function loss: 18610.0838
                    Surrogate loss: 0.0084
             Mean action noise std: 0.96
                       Mean reward: 1359.68
               Mean episode length: 241.89
                 Mean success rate: 18.50
                  Mean reward/step: 5.01
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6103040
                    Iteration time: 2.64s
                        Total time: 1919.32s
                               ETA: 8388.3s

################################################################################
                     [1m Learning iteration 745/4000 [0m

                       Computation: 3177 steps/s (collection: 0.496s, learning 2.082s)
               Value function loss: 25140.5580
                    Surrogate loss: 0.0113
             Mean action noise std: 0.96
                       Mean reward: 1493.20
               Mean episode length: 252.10
                 Mean success rate: 20.50
                  Mean reward/step: 4.53
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 2.58s
                        Total time: 1921.90s
                               ETA: 8385.7s

################################################################################
                     [1m Learning iteration 746/4000 [0m

                       Computation: 3103 steps/s (collection: 0.514s, learning 2.126s)
               Value function loss: 10249.6559
                    Surrogate loss: 0.0132
             Mean action noise std: 0.96
                       Mean reward: 1470.51
               Mean episode length: 244.66
                 Mean success rate: 20.00
                  Mean reward/step: 4.45
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6119424
                    Iteration time: 2.64s
                        Total time: 1924.53s
                               ETA: 8383.4s

################################################################################
                     [1m Learning iteration 747/4000 [0m

                       Computation: 3125 steps/s (collection: 0.494s, learning 2.128s)
               Value function loss: 16124.4104
                    Surrogate loss: 0.0116
             Mean action noise std: 0.96
                       Mean reward: 1340.65
               Mean episode length: 229.90
                 Mean success rate: 18.00
                  Mean reward/step: 4.53
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 2.62s
                        Total time: 1927.16s
                               ETA: 8381.1s

################################################################################
                     [1m Learning iteration 748/4000 [0m

                       Computation: 3160 steps/s (collection: 0.507s, learning 2.085s)
               Value function loss: 16056.6887
                    Surrogate loss: 0.0117
             Mean action noise std: 0.96
                       Mean reward: 1255.23
               Mean episode length: 221.41
                 Mean success rate: 17.50
                  Mean reward/step: 4.10
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6135808
                    Iteration time: 2.59s
                        Total time: 1929.75s
                               ETA: 8378.6s

################################################################################
                     [1m Learning iteration 749/4000 [0m

                       Computation: 3142 steps/s (collection: 0.513s, learning 2.094s)
               Value function loss: 19973.5544
                    Surrogate loss: 0.0131
             Mean action noise std: 0.96
                       Mean reward: 1195.58
               Mean episode length: 223.74
                 Mean success rate: 17.00
                  Mean reward/step: 3.99
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 2.61s
                        Total time: 1932.36s
                               ETA: 8376.1s

################################################################################
                     [1m Learning iteration 750/4000 [0m

                       Computation: 3098 steps/s (collection: 0.545s, learning 2.099s)
               Value function loss: 10705.5564
                    Surrogate loss: 0.0132
             Mean action noise std: 0.96
                       Mean reward: 1081.94
               Mean episode length: 212.36
                 Mean success rate: 16.00
                  Mean reward/step: 4.11
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6152192
                    Iteration time: 2.64s
                        Total time: 1935.00s
                               ETA: 8373.8s

################################################################################
                     [1m Learning iteration 751/4000 [0m

                       Computation: 3093 steps/s (collection: 0.543s, learning 2.105s)
               Value function loss: 12751.1152
                    Surrogate loss: 0.0147
             Mean action noise std: 0.96
                       Mean reward: 1022.84
               Mean episode length: 212.07
                 Mean success rate: 16.00
                  Mean reward/step: 4.41
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 2.65s
                        Total time: 1937.65s
                               ETA: 8371.6s

################################################################################
                     [1m Learning iteration 752/4000 [0m

                       Computation: 3068 steps/s (collection: 0.528s, learning 2.141s)
               Value function loss: 17101.0578
                    Surrogate loss: 0.0110
             Mean action noise std: 0.96
                       Mean reward: 1085.12
               Mean episode length: 214.51
                 Mean success rate: 16.00
                  Mean reward/step: 4.84
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6168576
                    Iteration time: 2.67s
                        Total time: 1940.32s
                               ETA: 8369.4s

################################################################################
                     [1m Learning iteration 753/4000 [0m

                       Computation: 3219 steps/s (collection: 0.473s, learning 2.071s)
               Value function loss: 18863.9759
                    Surrogate loss: 0.0131
             Mean action noise std: 0.96
                       Mean reward: 911.82
               Mean episode length: 209.81
                 Mean success rate: 12.00
                  Mean reward/step: 5.05
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 2.54s
                        Total time: 1942.86s
                               ETA: 8366.7s

################################################################################
                     [1m Learning iteration 754/4000 [0m

                       Computation: 3155 steps/s (collection: 0.498s, learning 2.098s)
               Value function loss: 16139.7845
                    Surrogate loss: 0.0122
             Mean action noise std: 0.96
                       Mean reward: 903.56
               Mean episode length: 203.68
                 Mean success rate: 11.00
                  Mean reward/step: 5.13
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6184960
                    Iteration time: 2.60s
                        Total time: 1945.46s
                               ETA: 8364.2s

################################################################################
                     [1m Learning iteration 755/4000 [0m

                       Computation: 3121 steps/s (collection: 0.537s, learning 2.087s)
               Value function loss: 19562.5277
                    Surrogate loss: 0.0108
             Mean action noise std: 0.96
                       Mean reward: 873.94
               Mean episode length: 212.37
                 Mean success rate: 10.00
                  Mean reward/step: 5.17
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 2.62s
                        Total time: 1948.08s
                               ETA: 8361.8s

################################################################################
                     [1m Learning iteration 756/4000 [0m

                       Computation: 3122 steps/s (collection: 0.494s, learning 2.130s)
               Value function loss: 13687.2454
                    Surrogate loss: 0.0113
             Mean action noise std: 0.96
                       Mean reward: 923.12
               Mean episode length: 220.76
                 Mean success rate: 10.00
                  Mean reward/step: 5.43
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6201344
                    Iteration time: 2.62s
                        Total time: 1950.70s
                               ETA: 8359.4s

################################################################################
                     [1m Learning iteration 757/4000 [0m

                       Computation: 3177 steps/s (collection: 0.474s, learning 2.104s)
               Value function loss: 20896.3883
                    Surrogate loss: 0.0115
             Mean action noise std: 0.96
                       Mean reward: 886.22
               Mean episode length: 220.38
                 Mean success rate: 10.00
                  Mean reward/step: 5.99
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 2.58s
                        Total time: 1953.28s
                               ETA: 8356.9s

################################################################################
                     [1m Learning iteration 758/4000 [0m

                       Computation: 3127 steps/s (collection: 0.453s, learning 2.166s)
               Value function loss: 28919.5719
                    Surrogate loss: 0.0082
             Mean action noise std: 0.96
                       Mean reward: 958.43
               Mean episode length: 224.07
                 Mean success rate: 13.50
                  Mean reward/step: 5.77
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 6217728
                    Iteration time: 2.62s
                        Total time: 1955.90s
                               ETA: 8354.5s

################################################################################
                     [1m Learning iteration 759/4000 [0m

                       Computation: 3091 steps/s (collection: 0.525s, learning 2.125s)
               Value function loss: 20089.2226
                    Surrogate loss: 0.0133
             Mean action noise std: 0.96
                       Mean reward: 932.30
               Mean episode length: 220.44
                 Mean success rate: 14.00
                  Mean reward/step: 6.07
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 2.65s
                        Total time: 1958.55s
                               ETA: 8352.2s

################################################################################
                     [1m Learning iteration 760/4000 [0m

                       Computation: 3150 steps/s (collection: 0.479s, learning 2.121s)
               Value function loss: 26730.2590
                    Surrogate loss: 0.0131
             Mean action noise std: 0.96
                       Mean reward: 1030.56
               Mean episode length: 219.26
                 Mean success rate: 16.00
                  Mean reward/step: 6.64
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6234112
                    Iteration time: 2.60s
                        Total time: 1961.15s
                               ETA: 8349.7s

################################################################################
                     [1m Learning iteration 761/4000 [0m

                       Computation: 3098 steps/s (collection: 0.499s, learning 2.146s)
               Value function loss: 18341.3359
                    Surrogate loss: 0.0204
             Mean action noise std: 0.96
                       Mean reward: 1046.82
               Mean episode length: 217.51
                 Mean success rate: 16.50
                  Mean reward/step: 7.05
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 2.64s
                        Total time: 1963.80s
                               ETA: 8347.4s

################################################################################
                     [1m Learning iteration 762/4000 [0m

                       Computation: 3247 steps/s (collection: 0.456s, learning 2.067s)
               Value function loss: 22238.5263
                    Surrogate loss: 0.0231
             Mean action noise std: 0.96
                       Mean reward: 1023.66
               Mean episode length: 202.54
                 Mean success rate: 16.50
                  Mean reward/step: 7.03
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 6250496
                    Iteration time: 2.52s
                        Total time: 1966.32s
                               ETA: 8344.6s

################################################################################
                     [1m Learning iteration 763/4000 [0m

                       Computation: 3087 steps/s (collection: 0.530s, learning 2.123s)
               Value function loss: 24164.6412
                    Surrogate loss: 0.0109
             Mean action noise std: 0.96
                       Mean reward: 1059.27
               Mean episode length: 202.90
                 Mean success rate: 15.00
                  Mean reward/step: 7.05
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 2.65s
                        Total time: 1968.97s
                               ETA: 8342.4s

################################################################################
                     [1m Learning iteration 764/4000 [0m

                       Computation: 3146 steps/s (collection: 0.487s, learning 2.116s)
               Value function loss: 30420.3403
                    Surrogate loss: 0.0104
             Mean action noise std: 0.96
                       Mean reward: 1163.63
               Mean episode length: 198.44
                 Mean success rate: 17.00
                  Mean reward/step: 6.94
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 6266880
                    Iteration time: 2.60s
                        Total time: 1971.58s
                               ETA: 8339.9s

################################################################################
                     [1m Learning iteration 765/4000 [0m

                       Computation: 3114 steps/s (collection: 0.509s, learning 2.121s)
               Value function loss: 26767.0570
                    Surrogate loss: 0.0109
             Mean action noise std: 0.96
                       Mean reward: 1188.57
               Mean episode length: 197.31
                 Mean success rate: 16.50
                  Mean reward/step: 6.45
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 2.63s
                        Total time: 1974.21s
                               ETA: 8337.5s

################################################################################
                     [1m Learning iteration 766/4000 [0m

                       Computation: 3218 steps/s (collection: 0.485s, learning 2.060s)
               Value function loss: 18131.0228
                    Surrogate loss: 0.0110
             Mean action noise std: 0.96
                       Mean reward: 1277.14
               Mean episode length: 204.41
                 Mean success rate: 17.00
                  Mean reward/step: 6.48
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6283264
                    Iteration time: 2.55s
                        Total time: 1976.75s
                               ETA: 8334.8s

################################################################################
                     [1m Learning iteration 767/4000 [0m

                       Computation: 3240 steps/s (collection: 0.472s, learning 2.056s)
               Value function loss: 23044.5188
                    Surrogate loss: 0.0123
             Mean action noise std: 0.96
                       Mean reward: 1341.75
               Mean episode length: 210.53
                 Mean success rate: 18.00
                  Mean reward/step: 6.59
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 2.53s
                        Total time: 1979.28s
                               ETA: 8332.0s

################################################################################
                     [1m Learning iteration 768/4000 [0m

                       Computation: 3153 steps/s (collection: 0.497s, learning 2.101s)
               Value function loss: 26156.1006
                    Surrogate loss: 0.0131
             Mean action noise std: 0.96
                       Mean reward: 1472.41
               Mean episode length: 215.29
                 Mean success rate: 19.50
                  Mean reward/step: 6.73
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6299648
                    Iteration time: 2.60s
                        Total time: 1981.88s
                               ETA: 8329.6s

################################################################################
                     [1m Learning iteration 769/4000 [0m

                       Computation: 3190 steps/s (collection: 0.470s, learning 2.098s)
               Value function loss: 34560.0916
                    Surrogate loss: 0.0096
             Mean action noise std: 0.96
                       Mean reward: 1667.09
               Mean episode length: 224.96
                 Mean success rate: 22.00
                  Mean reward/step: 6.96
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 2.57s
                        Total time: 1984.44s
                               ETA: 8326.9s

################################################################################
                     [1m Learning iteration 770/4000 [0m

                       Computation: 3205 steps/s (collection: 0.470s, learning 2.085s)
               Value function loss: 24398.6252
                    Surrogate loss: 0.0126
             Mean action noise std: 0.96
                       Mean reward: 1576.62
               Mean episode length: 224.91
                 Mean success rate: 20.50
                  Mean reward/step: 6.85
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6316032
                    Iteration time: 2.56s
                        Total time: 1987.00s
                               ETA: 8324.3s

################################################################################
                     [1m Learning iteration 771/4000 [0m

                       Computation: 3227 steps/s (collection: 0.473s, learning 2.066s)
               Value function loss: 33543.8279
                    Surrogate loss: 0.0121
             Mean action noise std: 0.96
                       Mean reward: 1812.18
               Mean episode length: 239.47
                 Mean success rate: 23.50
                  Mean reward/step: 6.93
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 2.54s
                        Total time: 1989.54s
                               ETA: 8321.5s

################################################################################
                     [1m Learning iteration 772/4000 [0m

                       Computation: 3194 steps/s (collection: 0.483s, learning 2.081s)
               Value function loss: 20095.8768
                    Surrogate loss: 0.0128
             Mean action noise std: 0.96
                       Mean reward: 1846.85
               Mean episode length: 242.50
                 Mean success rate: 25.00
                  Mean reward/step: 7.05
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6332416
                    Iteration time: 2.56s
                        Total time: 1992.10s
                               ETA: 8318.9s

################################################################################
                     [1m Learning iteration 773/4000 [0m

                       Computation: 3187 steps/s (collection: 0.470s, learning 2.100s)
               Value function loss: 33908.7099
                    Surrogate loss: 0.0116
             Mean action noise std: 0.96
                       Mean reward: 1886.47
               Mean episode length: 249.59
                 Mean success rate: 25.50
                  Mean reward/step: 6.89
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 2.57s
                        Total time: 1994.67s
                               ETA: 8316.3s

################################################################################
                     [1m Learning iteration 774/4000 [0m

                       Computation: 3158 steps/s (collection: 0.492s, learning 2.101s)
               Value function loss: 19059.3474
                    Surrogate loss: 0.0136
             Mean action noise std: 0.96
                       Mean reward: 1755.32
               Mean episode length: 249.75
                 Mean success rate: 22.00
                  Mean reward/step: 6.97
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6348800
                    Iteration time: 2.59s
                        Total time: 1997.27s
                               ETA: 8313.8s

################################################################################
                     [1m Learning iteration 775/4000 [0m

                       Computation: 3124 steps/s (collection: 0.531s, learning 2.091s)
               Value function loss: 18509.8607
                    Surrogate loss: 0.0117
             Mean action noise std: 0.96
                       Mean reward: 1512.72
               Mean episode length: 238.56
                 Mean success rate: 20.00
                  Mean reward/step: 7.12
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 2.62s
                        Total time: 1999.89s
                               ETA: 8311.4s

################################################################################
                     [1m Learning iteration 776/4000 [0m

                       Computation: 3223 steps/s (collection: 0.460s, learning 2.082s)
               Value function loss: 27365.6771
                    Surrogate loss: 0.0178
             Mean action noise std: 0.96
                       Mean reward: 1539.01
               Mean episode length: 235.51
                 Mean success rate: 20.50
                  Mean reward/step: 7.36
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6365184
                    Iteration time: 2.54s
                        Total time: 2002.43s
                               ETA: 8308.7s

################################################################################
                     [1m Learning iteration 777/4000 [0m

                       Computation: 3113 steps/s (collection: 0.505s, learning 2.126s)
               Value function loss: 30468.3121
                    Surrogate loss: 0.0129
             Mean action noise std: 0.96
                       Mean reward: 1401.91
               Mean episode length: 234.62
                 Mean success rate: 18.00
                  Mean reward/step: 7.14
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 2.63s
                        Total time: 2005.06s
                               ETA: 8306.3s

################################################################################
                     [1m Learning iteration 778/4000 [0m

                       Computation: 3117 steps/s (collection: 0.528s, learning 2.099s)
               Value function loss: 23939.5226
                    Surrogate loss: 0.0106
             Mean action noise std: 0.96
                       Mean reward: 1468.41
               Mean episode length: 226.53
                 Mean success rate: 19.00
                  Mean reward/step: 6.75
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6381568
                    Iteration time: 2.63s
                        Total time: 2007.69s
                               ETA: 8303.9s

################################################################################
                     [1m Learning iteration 779/4000 [0m

                       Computation: 3197 steps/s (collection: 0.474s, learning 2.088s)
               Value function loss: 22866.5346
                    Surrogate loss: 0.0135
             Mean action noise std: 0.96
                       Mean reward: 1577.51
               Mean episode length: 222.53
                 Mean success rate: 20.50
                  Mean reward/step: 6.75
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 2.56s
                        Total time: 2010.25s
                               ETA: 8301.3s

################################################################################
                     [1m Learning iteration 780/4000 [0m

                       Computation: 3226 steps/s (collection: 0.469s, learning 2.069s)
               Value function loss: 26091.5483
                    Surrogate loss: 0.0135
             Mean action noise std: 0.96
                       Mean reward: 1689.74
               Mean episode length: 238.94
                 Mean success rate: 23.00
                  Mean reward/step: 6.79
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6397952
                    Iteration time: 2.54s
                        Total time: 2012.79s
                               ETA: 8298.6s

################################################################################
                     [1m Learning iteration 781/4000 [0m

                       Computation: 3230 steps/s (collection: 0.455s, learning 2.081s)
               Value function loss: 25165.6591
                    Surrogate loss: 0.0133
             Mean action noise std: 0.96
                       Mean reward: 1750.04
               Mean episode length: 241.01
                 Mean success rate: 22.50
                  Mean reward/step: 6.52
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 2.54s
                        Total time: 2015.32s
                               ETA: 8295.8s

################################################################################
                     [1m Learning iteration 782/4000 [0m

                       Computation: 3190 steps/s (collection: 0.489s, learning 2.078s)
               Value function loss: 27907.5369
                    Surrogate loss: 0.0097
             Mean action noise std: 0.96
                       Mean reward: 1608.24
               Mean episode length: 239.98
                 Mean success rate: 20.50
                  Mean reward/step: 6.36
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 6414336
                    Iteration time: 2.57s
                        Total time: 2017.89s
                               ETA: 8293.2s

################################################################################
                     [1m Learning iteration 783/4000 [0m

                       Computation: 3246 steps/s (collection: 0.489s, learning 2.034s)
               Value function loss: 19818.3893
                    Surrogate loss: 0.0133
             Mean action noise std: 0.96
                       Mean reward: 1486.31
               Mean episode length: 235.28
                 Mean success rate: 20.00
                  Mean reward/step: 6.09
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 2.52s
                        Total time: 2020.41s
                               ETA: 8290.4s

################################################################################
                     [1m Learning iteration 784/4000 [0m

                       Computation: 3196 steps/s (collection: 0.474s, learning 2.089s)
               Value function loss: 24070.6391
                    Surrogate loss: 0.0125
             Mean action noise std: 0.96
                       Mean reward: 1345.55
               Mean episode length: 232.71
                 Mean success rate: 17.00
                  Mean reward/step: 6.37
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6430720
                    Iteration time: 2.56s
                        Total time: 2022.98s
                               ETA: 8287.8s

################################################################################
                     [1m Learning iteration 785/4000 [0m

                       Computation: 3178 steps/s (collection: 0.467s, learning 2.110s)
               Value function loss: 14426.4266
                    Surrogate loss: 0.0136
             Mean action noise std: 0.96
                       Mean reward: 1178.59
               Mean episode length: 211.37
                 Mean success rate: 13.50
                  Mean reward/step: 6.54
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 2.58s
                        Total time: 2025.55s
                               ETA: 8285.2s

################################################################################
                     [1m Learning iteration 786/4000 [0m

                       Computation: 3188 steps/s (collection: 0.498s, learning 2.072s)
               Value function loss: 28626.7502
                    Surrogate loss: 0.0125
             Mean action noise std: 0.96
                       Mean reward: 1086.68
               Mean episode length: 193.66
                 Mean success rate: 13.00
                  Mean reward/step: 6.96
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 6447104
                    Iteration time: 2.57s
                        Total time: 2028.12s
                               ETA: 8282.6s

################################################################################
                     [1m Learning iteration 787/4000 [0m

                       Computation: 3137 steps/s (collection: 0.542s, learning 2.069s)
               Value function loss: 26392.9664
                    Surrogate loss: 0.0111
             Mean action noise std: 0.96
                       Mean reward: 1234.96
               Mean episode length: 198.07
                 Mean success rate: 14.00
                  Mean reward/step: 6.86
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 2.61s
                        Total time: 2030.73s
                               ETA: 8280.1s

################################################################################
                     [1m Learning iteration 788/4000 [0m

                       Computation: 3249 steps/s (collection: 0.450s, learning 2.072s)
               Value function loss: 18129.3289
                    Surrogate loss: 0.0139
             Mean action noise std: 0.96
                       Mean reward: 1255.83
               Mean episode length: 200.26
                 Mean success rate: 13.50
                  Mean reward/step: 7.46
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6463488
                    Iteration time: 2.52s
                        Total time: 2033.26s
                               ETA: 8277.3s

################################################################################
                     [1m Learning iteration 789/4000 [0m

                       Computation: 3164 steps/s (collection: 0.452s, learning 2.137s)
               Value function loss: 40568.7500
                    Surrogate loss: 0.0117
             Mean action noise std: 0.96
                       Mean reward: 1399.12
               Mean episode length: 199.21
                 Mean success rate: 16.00
                  Mean reward/step: 7.59
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 2.59s
                        Total time: 2035.84s
                               ETA: 8274.8s

################################################################################
                     [1m Learning iteration 790/4000 [0m

                       Computation: 3209 steps/s (collection: 0.491s, learning 2.061s)
               Value function loss: 23555.5441
                    Surrogate loss: 0.0131
             Mean action noise std: 0.96
                       Mean reward: 1526.70
               Mean episode length: 199.13
                 Mean success rate: 18.00
                  Mean reward/step: 7.42
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6479872
                    Iteration time: 2.55s
                        Total time: 2038.40s
                               ETA: 8272.1s

################################################################################
                     [1m Learning iteration 791/4000 [0m

                       Computation: 3242 steps/s (collection: 0.456s, learning 2.070s)
               Value function loss: 20248.9014
                    Surrogate loss: 0.0138
             Mean action noise std: 0.96
                       Mean reward: 1575.37
               Mean episode length: 209.84
                 Mean success rate: 19.00
                  Mean reward/step: 7.77
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 2.53s
                        Total time: 2040.92s
                               ETA: 8269.3s

################################################################################
                     [1m Learning iteration 792/4000 [0m

                       Computation: 3061 steps/s (collection: 0.534s, learning 2.142s)
               Value function loss: 28190.8399
                    Surrogate loss: 0.0127
             Mean action noise std: 0.96
                       Mean reward: 1476.40
               Mean episode length: 204.13
                 Mean success rate: 17.50
                  Mean reward/step: 7.49
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6496256
                    Iteration time: 2.68s
                        Total time: 2043.60s
                               ETA: 8267.2s

################################################################################
                     [1m Learning iteration 793/4000 [0m

                       Computation: 3160 steps/s (collection: 0.534s, learning 2.058s)
               Value function loss: 27022.5220
                    Surrogate loss: 0.0109
             Mean action noise std: 0.96
                       Mean reward: 1511.29
               Mean episode length: 203.96
                 Mean success rate: 18.00
                  Mean reward/step: 7.43
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 2.59s
                        Total time: 2046.19s
                               ETA: 8264.7s

################################################################################
                     [1m Learning iteration 794/4000 [0m

                       Computation: 3170 steps/s (collection: 0.494s, learning 2.090s)
               Value function loss: 29844.2751
                    Surrogate loss: 0.0129
             Mean action noise std: 0.96
                       Mean reward: 1654.75
               Mean episode length: 221.71
                 Mean success rate: 20.00
                  Mean reward/step: 7.46
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6512640
                    Iteration time: 2.58s
                        Total time: 2048.77s
                               ETA: 8262.1s

################################################################################
                     [1m Learning iteration 795/4000 [0m

                       Computation: 3139 steps/s (collection: 0.555s, learning 2.054s)
               Value function loss: 29059.8161
                    Surrogate loss: 0.0100
             Mean action noise std: 0.96
                       Mean reward: 1532.31
               Mean episode length: 218.35
                 Mean success rate: 18.50
                  Mean reward/step: 7.08
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 2.61s
                        Total time: 2051.38s
                               ETA: 8259.7s

################################################################################
                     [1m Learning iteration 796/4000 [0m

                       Computation: 3191 steps/s (collection: 0.521s, learning 2.046s)
               Value function loss: 26407.9049
                    Surrogate loss: 0.0131
             Mean action noise std: 0.96
                       Mean reward: 1598.49
               Mean episode length: 219.35
                 Mean success rate: 18.50
                  Mean reward/step: 6.90
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6529024
                    Iteration time: 2.57s
                        Total time: 2053.95s
                               ETA: 8257.0s

################################################################################
                     [1m Learning iteration 797/4000 [0m

                       Computation: 3195 steps/s (collection: 0.501s, learning 2.063s)
               Value function loss: 17040.0391
                    Surrogate loss: 0.0134
             Mean action noise std: 0.96
                       Mean reward: 1725.99
               Mean episode length: 236.18
                 Mean success rate: 20.50
                  Mean reward/step: 7.26
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 2.56s
                        Total time: 2056.51s
                               ETA: 8254.4s

################################################################################
                     [1m Learning iteration 798/4000 [0m

                       Computation: 3148 steps/s (collection: 0.511s, learning 2.090s)
               Value function loss: 45268.7733
                    Surrogate loss: 0.0094
             Mean action noise std: 0.96
                       Mean reward: 1845.73
               Mean episode length: 237.22
                 Mean success rate: 21.50
                  Mean reward/step: 7.34
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6545408
                    Iteration time: 2.60s
                        Total time: 2059.12s
                               ETA: 8251.9s

################################################################################
                     [1m Learning iteration 799/4000 [0m

                       Computation: 3182 steps/s (collection: 0.514s, learning 2.061s)
               Value function loss: 29810.0684
                    Surrogate loss: 0.0112
             Mean action noise std: 0.96
                       Mean reward: 1875.06
               Mean episode length: 244.55
                 Mean success rate: 23.50
                  Mean reward/step: 7.07
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 2.57s
                        Total time: 2061.69s
                               ETA: 8249.3s

################################################################################
                     [1m Learning iteration 800/4000 [0m

                       Computation: 3115 steps/s (collection: 0.479s, learning 2.151s)
               Value function loss: 32409.0226
                    Surrogate loss: 0.0103
             Mean action noise std: 0.96
                       Mean reward: 1954.56
               Mean episode length: 250.84
                 Mean success rate: 24.00
                  Mean reward/step: 7.27
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6561792
                    Iteration time: 2.63s
                        Total time: 2064.32s
                               ETA: 8247.0s

################################################################################
                     [1m Learning iteration 801/4000 [0m

                       Computation: 3246 steps/s (collection: 0.444s, learning 2.079s)
               Value function loss: 18981.7817
                    Surrogate loss: 0.0139
             Mean action noise std: 0.96
                       Mean reward: 1947.37
               Mean episode length: 252.92
                 Mean success rate: 23.00
                  Mean reward/step: 7.27
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 2.52s
                        Total time: 2066.84s
                               ETA: 8244.2s

################################################################################
                     [1m Learning iteration 802/4000 [0m

                       Computation: 3213 steps/s (collection: 0.451s, learning 2.098s)
               Value function loss: 35767.9925
                    Surrogate loss: 0.0078
             Mean action noise std: 0.96
                       Mean reward: 2115.56
               Mean episode length: 268.00
                 Mean success rate: 26.00
                  Mean reward/step: 7.35
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6578176
                    Iteration time: 2.55s
                        Total time: 2069.39s
                               ETA: 8241.5s

################################################################################
                     [1m Learning iteration 803/4000 [0m

                       Computation: 3228 steps/s (collection: 0.472s, learning 2.065s)
               Value function loss: 32877.8329
                    Surrogate loss: 0.0105
             Mean action noise std: 0.96
                       Mean reward: 2151.93
               Mean episode length: 278.46
                 Mean success rate: 26.00
                  Mean reward/step: 7.12
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 2.54s
                        Total time: 2071.93s
                               ETA: 8238.8s

################################################################################
                     [1m Learning iteration 804/4000 [0m

                       Computation: 3164 steps/s (collection: 0.498s, learning 2.091s)
               Value function loss: 24850.7674
                    Surrogate loss: 0.0125
             Mean action noise std: 0.96
                       Mean reward: 2080.44
               Mean episode length: 272.01
                 Mean success rate: 25.00
                  Mean reward/step: 7.41
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6594560
                    Iteration time: 2.59s
                        Total time: 2074.52s
                               ETA: 8236.2s

################################################################################
                     [1m Learning iteration 805/4000 [0m

                       Computation: 3062 steps/s (collection: 0.496s, learning 2.178s)
               Value function loss: 41372.4334
                    Surrogate loss: 0.0091
             Mean action noise std: 0.96
                       Mean reward: 1991.22
               Mean episode length: 280.67
                 Mean success rate: 25.00
                  Mean reward/step: 7.62
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 2.67s
                        Total time: 2077.19s
                               ETA: 8234.0s

################################################################################
                     [1m Learning iteration 806/4000 [0m

                       Computation: 3141 steps/s (collection: 0.532s, learning 2.075s)
               Value function loss: 22668.2096
                    Surrogate loss: 0.0160
             Mean action noise std: 0.96
                       Mean reward: 1885.30
               Mean episode length: 271.52
                 Mean success rate: 23.00
                  Mean reward/step: 7.71
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 6610944
                    Iteration time: 2.61s
                        Total time: 2079.80s
                               ETA: 8231.6s

################################################################################
                     [1m Learning iteration 807/4000 [0m

                       Computation: 3162 steps/s (collection: 0.523s, learning 2.067s)
               Value function loss: 31845.5701
                    Surrogate loss: 0.0160
             Mean action noise std: 0.95
                       Mean reward: 1808.86
               Mean episode length: 255.75
                 Mean success rate: 22.50
                  Mean reward/step: 7.81
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 2.59s
                        Total time: 2082.39s
                               ETA: 8229.0s

################################################################################
                     [1m Learning iteration 808/4000 [0m

                       Computation: 3145 steps/s (collection: 0.473s, learning 2.131s)
               Value function loss: 25738.1584
                    Surrogate loss: 0.0114
             Mean action noise std: 0.95
                       Mean reward: 1730.14
               Mean episode length: 245.12
                 Mean success rate: 21.00
                  Mean reward/step: 7.51
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6627328
                    Iteration time: 2.60s
                        Total time: 2084.99s
                               ETA: 8226.6s

################################################################################
                     [1m Learning iteration 809/4000 [0m

                       Computation: 3134 steps/s (collection: 0.526s, learning 2.087s)
               Value function loss: 24239.7013
                    Surrogate loss: 0.0134
             Mean action noise std: 0.95
                       Mean reward: 1497.32
               Mean episode length: 233.28
                 Mean success rate: 19.00
                  Mean reward/step: 7.92
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 2.61s
                        Total time: 2087.61s
                               ETA: 8224.1s

################################################################################
                     [1m Learning iteration 810/4000 [0m

                       Computation: 3213 steps/s (collection: 0.490s, learning 2.059s)
               Value function loss: 36347.1439
                    Surrogate loss: 0.0117
             Mean action noise std: 0.95
                       Mean reward: 1786.79
               Mean episode length: 250.57
                 Mean success rate: 20.50
                  Mean reward/step: 7.65
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6643712
                    Iteration time: 2.55s
                        Total time: 2090.16s
                               ETA: 8221.5s

################################################################################
                     [1m Learning iteration 811/4000 [0m

                       Computation: 3190 steps/s (collection: 0.500s, learning 2.068s)
               Value function loss: 22567.9839
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 1708.00
               Mean episode length: 234.22
                 Mean success rate: 19.50
                  Mean reward/step: 7.34
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 2.57s
                        Total time: 2092.72s
                               ETA: 8218.8s

################################################################################
                     [1m Learning iteration 812/4000 [0m

                       Computation: 3266 steps/s (collection: 0.448s, learning 2.060s)
               Value function loss: 27337.3412
                    Surrogate loss: 0.0109
             Mean action noise std: 0.95
                       Mean reward: 1838.45
               Mean episode length: 243.38
                 Mean success rate: 22.00
                  Mean reward/step: 7.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6660096
                    Iteration time: 2.51s
                        Total time: 2095.23s
                               ETA: 8216.0s

################################################################################
                     [1m Learning iteration 813/4000 [0m

                       Computation: 3100 steps/s (collection: 0.551s, learning 2.091s)
               Value function loss: 47887.5561
                    Surrogate loss: 0.0103
             Mean action noise std: 0.95
                       Mean reward: 1876.40
               Mean episode length: 248.25
                 Mean success rate: 22.00
                  Mean reward/step: 7.45
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 2.64s
                        Total time: 2097.87s
                               ETA: 8213.7s

################################################################################
                     [1m Learning iteration 814/4000 [0m

                       Computation: 3230 steps/s (collection: 0.483s, learning 2.053s)
               Value function loss: 27077.8094
                    Surrogate loss: 0.0102
             Mean action noise std: 0.95
                       Mean reward: 1955.37
               Mean episode length: 257.54
                 Mean success rate: 23.50
                  Mean reward/step: 7.30
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6676480
                    Iteration time: 2.54s
                        Total time: 2100.41s
                               ETA: 8210.9s

################################################################################
                     [1m Learning iteration 815/4000 [0m

                       Computation: 3279 steps/s (collection: 0.477s, learning 2.021s)
               Value function loss: 30709.7744
                    Surrogate loss: 0.0100
             Mean action noise std: 0.95
                       Mean reward: 1981.78
               Mean episode length: 258.64
                 Mean success rate: 24.50
                  Mean reward/step: 7.26
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 2.50s
                        Total time: 2102.91s
                               ETA: 8208.0s

################################################################################
                     [1m Learning iteration 816/4000 [0m

                       Computation: 3227 steps/s (collection: 0.465s, learning 2.074s)
               Value function loss: 39584.6264
                    Surrogate loss: 0.0088
             Mean action noise std: 0.95
                       Mean reward: 2161.10
               Mean episode length: 258.23
                 Mean success rate: 27.00
                  Mean reward/step: 7.00
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6692864
                    Iteration time: 2.54s
                        Total time: 2105.45s
                               ETA: 8205.3s

################################################################################
                     [1m Learning iteration 817/4000 [0m

                       Computation: 3276 steps/s (collection: 0.449s, learning 2.052s)
               Value function loss: 26516.1801
                    Surrogate loss: 0.0095
             Mean action noise std: 0.95
                       Mean reward: 1974.64
               Mean episode length: 235.70
                 Mean success rate: 26.00
                  Mean reward/step: 6.36
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 2.50s
                        Total time: 2107.95s
                               ETA: 8202.4s

################################################################################
                     [1m Learning iteration 818/4000 [0m

                       Computation: 3234 steps/s (collection: 0.493s, learning 2.040s)
               Value function loss: 27174.9244
                    Surrogate loss: 0.0162
             Mean action noise std: 0.95
                       Mean reward: 1815.90
               Mean episode length: 224.47
                 Mean success rate: 25.00
                  Mean reward/step: 6.48
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6709248
                    Iteration time: 2.53s
                        Total time: 2110.48s
                               ETA: 8199.7s

################################################################################
                     [1m Learning iteration 819/4000 [0m

                       Computation: 3241 steps/s (collection: 0.458s, learning 2.069s)
               Value function loss: 26016.8645
                    Surrogate loss: 0.0148
             Mean action noise std: 0.95
                       Mean reward: 1749.19
               Mean episode length: 222.26
                 Mean success rate: 24.00
                  Mean reward/step: 6.68
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 2.53s
                        Total time: 2113.01s
                               ETA: 8196.9s

################################################################################
                     [1m Learning iteration 820/4000 [0m

                       Computation: 3322 steps/s (collection: 0.436s, learning 2.030s)
               Value function loss: 22534.7797
                    Surrogate loss: 0.0127
             Mean action noise std: 0.95
                       Mean reward: 1708.63
               Mean episode length: 226.51
                 Mean success rate: 23.50
                  Mean reward/step: 7.18
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6725632
                    Iteration time: 2.47s
                        Total time: 2115.47s
                               ETA: 8193.9s

################################################################################
                     [1m Learning iteration 821/4000 [0m

                       Computation: 3210 steps/s (collection: 0.488s, learning 2.063s)
               Value function loss: 42144.5966
                    Surrogate loss: 0.0122
             Mean action noise std: 0.95
                       Mean reward: 1614.13
               Mean episode length: 227.38
                 Mean success rate: 24.00
                  Mean reward/step: 6.90
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 2.55s
                        Total time: 2118.02s
                               ETA: 8191.2s

################################################################################
                     [1m Learning iteration 822/4000 [0m

                       Computation: 3276 steps/s (collection: 0.468s, learning 2.033s)
               Value function loss: 27475.5061
                    Surrogate loss: 0.0122
             Mean action noise std: 0.95
                       Mean reward: 1663.37
               Mean episode length: 240.03
                 Mean success rate: 23.00
                  Mean reward/step: 6.60
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6742016
                    Iteration time: 2.50s
                        Total time: 2120.52s
                               ETA: 8188.4s

################################################################################
                     [1m Learning iteration 823/4000 [0m

                       Computation: 3253 steps/s (collection: 0.462s, learning 2.056s)
               Value function loss: 20307.2566
                    Surrogate loss: 0.0141
             Mean action noise std: 0.95
                       Mean reward: 1684.40
               Mean episode length: 250.94
                 Mean success rate: 23.00
                  Mean reward/step: 6.71
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 2.52s
                        Total time: 2123.04s
                               ETA: 8185.6s

################################################################################
                     [1m Learning iteration 824/4000 [0m

                       Computation: 3203 steps/s (collection: 0.490s, learning 2.067s)
               Value function loss: 24380.9348
                    Surrogate loss: 0.0155
             Mean action noise std: 0.95
                       Mean reward: 1803.70
               Mean episode length: 256.61
                 Mean success rate: 25.00
                  Mean reward/step: 7.04
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6758400
                    Iteration time: 2.56s
                        Total time: 2125.60s
                               ETA: 8182.9s

################################################################################
                     [1m Learning iteration 825/4000 [0m

                       Computation: 3291 steps/s (collection: 0.453s, learning 2.035s)
               Value function loss: 32972.4124
                    Surrogate loss: 0.0112
             Mean action noise std: 0.95
                       Mean reward: 1741.80
               Mean episode length: 265.71
                 Mean success rate: 24.00
                  Mean reward/step: 7.23
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 2.49s
                        Total time: 2128.09s
                               ETA: 8180.0s

################################################################################
                     [1m Learning iteration 826/4000 [0m

                       Computation: 3240 steps/s (collection: 0.471s, learning 2.057s)
               Value function loss: 27859.9757
                    Surrogate loss: 0.0118
             Mean action noise std: 0.95
                       Mean reward: 1761.75
               Mean episode length: 255.69
                 Mean success rate: 23.00
                  Mean reward/step: 7.47
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 6774784
                    Iteration time: 2.53s
                        Total time: 2130.61s
                               ETA: 8177.2s

################################################################################
                     [1m Learning iteration 827/4000 [0m

                       Computation: 3233 steps/s (collection: 0.505s, learning 2.028s)
               Value function loss: 25120.9461
                    Surrogate loss: 0.0120
             Mean action noise std: 0.95
                       Mean reward: 1494.05
               Mean episode length: 235.34
                 Mean success rate: 19.00
                  Mean reward/step: 7.15
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 2.53s
                        Total time: 2133.15s
                               ETA: 8174.5s

################################################################################
                     [1m Learning iteration 828/4000 [0m

                       Computation: 3248 steps/s (collection: 0.471s, learning 2.051s)
               Value function loss: 22346.9397
                    Surrogate loss: 0.0124
             Mean action noise std: 0.95
                       Mean reward: 1337.00
               Mean episode length: 226.94
                 Mean success rate: 18.50
                  Mean reward/step: 7.05
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6791168
                    Iteration time: 2.52s
                        Total time: 2135.67s
                               ETA: 8171.7s

################################################################################
                     [1m Learning iteration 829/4000 [0m

                       Computation: 3266 steps/s (collection: 0.456s, learning 2.052s)
               Value function loss: 42836.1497
                    Surrogate loss: 0.0115
             Mean action noise std: 0.95
                       Mean reward: 1352.93
               Mean episode length: 221.75
                 Mean success rate: 19.00
                  Mean reward/step: 7.50
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 2.51s
                        Total time: 2138.18s
                               ETA: 8168.9s

################################################################################
                     [1m Learning iteration 830/4000 [0m

                       Computation: 3216 steps/s (collection: 0.470s, learning 2.076s)
               Value function loss: 34448.4438
                    Surrogate loss: 0.0117
             Mean action noise std: 0.95
                       Mean reward: 1468.44
               Mean episode length: 220.50
                 Mean success rate: 20.50
                  Mean reward/step: 7.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 6807552
                    Iteration time: 2.55s
                        Total time: 2140.72s
                               ETA: 8166.2s

################################################################################
                     [1m Learning iteration 831/4000 [0m

                       Computation: 3201 steps/s (collection: 0.504s, learning 2.054s)
               Value function loss: 22878.4127
                    Surrogate loss: 0.0138
             Mean action noise std: 0.95
                       Mean reward: 1430.37
               Mean episode length: 222.39
                 Mean success rate: 19.50
                  Mean reward/step: 7.04
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 2.56s
                        Total time: 2143.28s
                               ETA: 8163.5s

################################################################################
                     [1m Learning iteration 832/4000 [0m

                       Computation: 3275 steps/s (collection: 0.469s, learning 2.032s)
               Value function loss: 36446.0061
                    Surrogate loss: 0.0106
             Mean action noise std: 0.95
                       Mean reward: 1643.34
               Mean episode length: 236.19
                 Mean success rate: 22.50
                  Mean reward/step: 7.17
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6823936
                    Iteration time: 2.50s
                        Total time: 2145.78s
                               ETA: 8160.7s

################################################################################
                     [1m Learning iteration 833/4000 [0m

                       Computation: 3241 steps/s (collection: 0.475s, learning 2.052s)
               Value function loss: 27698.9212
                    Surrogate loss: 0.0138
             Mean action noise std: 0.95
                       Mean reward: 1621.94
               Mean episode length: 234.63
                 Mean success rate: 22.00
                  Mean reward/step: 7.02
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 2.53s
                        Total time: 2148.31s
                               ETA: 8157.9s

################################################################################
                     [1m Learning iteration 834/4000 [0m

                       Computation: 3233 steps/s (collection: 0.496s, learning 2.037s)
               Value function loss: 40462.5130
                    Surrogate loss: 0.0127
             Mean action noise std: 0.95
                       Mean reward: 1840.78
               Mean episode length: 238.62
                 Mean success rate: 24.00
                  Mean reward/step: 7.52
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 6840320
                    Iteration time: 2.53s
                        Total time: 2150.84s
                               ETA: 8155.2s

################################################################################
                     [1m Learning iteration 835/4000 [0m

                       Computation: 3243 steps/s (collection: 0.498s, learning 2.028s)
               Value function loss: 29833.0988
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 1665.58
               Mean episode length: 225.34
                 Mean success rate: 21.00
                  Mean reward/step: 8.14
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 2.53s
                        Total time: 2153.37s
                               ETA: 8152.4s

################################################################################
                     [1m Learning iteration 836/4000 [0m

                       Computation: 3237 steps/s (collection: 0.473s, learning 2.057s)
               Value function loss: 26478.7965
                    Surrogate loss: 0.0118
             Mean action noise std: 0.95
                       Mean reward: 1450.48
               Mean episode length: 222.25
                 Mean success rate: 19.50
                  Mean reward/step: 8.80
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6856704
                    Iteration time: 2.53s
                        Total time: 2155.90s
                               ETA: 8149.7s

################################################################################
                     [1m Learning iteration 837/4000 [0m

                       Computation: 3308 steps/s (collection: 0.437s, learning 2.039s)
               Value function loss: 42393.8252
                    Surrogate loss: 0.0098
             Mean action noise std: 0.95
                       Mean reward: 1688.53
               Mean episode length: 231.17
                 Mean success rate: 22.00
                  Mean reward/step: 7.99
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 2.48s
                        Total time: 2158.38s
                               ETA: 8146.7s

################################################################################
                     [1m Learning iteration 838/4000 [0m

                       Computation: 3194 steps/s (collection: 0.473s, learning 2.091s)
               Value function loss: 31992.5170
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 1737.11
               Mean episode length: 230.32
                 Mean success rate: 23.00
                  Mean reward/step: 7.92
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6873088
                    Iteration time: 2.56s
                        Total time: 2160.94s
                               ETA: 8144.1s

################################################################################
                     [1m Learning iteration 839/4000 [0m

                       Computation: 3261 steps/s (collection: 0.443s, learning 2.069s)
               Value function loss: 23895.3580
                    Surrogate loss: 0.0159
             Mean action noise std: 0.95
                       Mean reward: 1545.89
               Mean episode length: 217.02
                 Mean success rate: 22.00
                  Mean reward/step: 7.48
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 2.51s
                        Total time: 2163.45s
                               ETA: 8141.3s

################################################################################
                     [1m Learning iteration 840/4000 [0m

                       Computation: 3080 steps/s (collection: 0.559s, learning 2.100s)
               Value function loss: 36412.2709
                    Surrogate loss: 0.0153
             Mean action noise std: 0.95
                       Mean reward: 1639.58
               Mean episode length: 227.49
                 Mean success rate: 24.00
                  Mean reward/step: 7.76
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6889472
                    Iteration time: 2.66s
                        Total time: 2166.11s
                               ETA: 8139.0s

################################################################################
                     [1m Learning iteration 841/4000 [0m

                       Computation: 3279 steps/s (collection: 0.442s, learning 2.056s)
               Value function loss: 29310.7958
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 1832.47
               Mean episode length: 243.59
                 Mean success rate: 27.00
                  Mean reward/step: 7.63
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 2.50s
                        Total time: 2168.61s
                               ETA: 8136.2s

################################################################################
                     [1m Learning iteration 842/4000 [0m

                       Computation: 3196 steps/s (collection: 0.445s, learning 2.118s)
               Value function loss: 33047.5682
                    Surrogate loss: 0.0135
             Mean action noise std: 0.95
                       Mean reward: 1997.73
               Mean episode length: 251.94
                 Mean success rate: 29.00
                  Mean reward/step: 7.29
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6905856
                    Iteration time: 2.56s
                        Total time: 2171.17s
                               ETA: 8133.5s

################################################################################
                     [1m Learning iteration 843/4000 [0m

                       Computation: 3231 steps/s (collection: 0.446s, learning 2.089s)
               Value function loss: 23417.8893
                    Surrogate loss: 0.0190
             Mean action noise std: 0.95
                       Mean reward: 1898.92
               Mean episode length: 251.72
                 Mean success rate: 28.00
                  Mean reward/step: 7.35
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 2.53s
                        Total time: 2173.71s
                               ETA: 8130.8s

################################################################################
                     [1m Learning iteration 844/4000 [0m

                       Computation: 3238 steps/s (collection: 0.448s, learning 2.082s)
               Value function loss: 36794.1324
                    Surrogate loss: 0.0160
             Mean action noise std: 0.95
                       Mean reward: 1988.25
               Mean episode length: 253.41
                 Mean success rate: 29.50
                  Mean reward/step: 7.11
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6922240
                    Iteration time: 2.53s
                        Total time: 2176.24s
                               ETA: 8128.1s

################################################################################
                     [1m Learning iteration 845/4000 [0m

                       Computation: 3254 steps/s (collection: 0.438s, learning 2.079s)
               Value function loss: 26695.3951
                    Surrogate loss: 0.0117
             Mean action noise std: 0.95
                       Mean reward: 1952.77
               Mean episode length: 255.18
                 Mean success rate: 29.00
                  Mean reward/step: 6.91
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 2.52s
                        Total time: 2178.76s
                               ETA: 8125.3s

################################################################################
                     [1m Learning iteration 846/4000 [0m

                       Computation: 3201 steps/s (collection: 0.467s, learning 2.092s)
               Value function loss: 28799.8289
                    Surrogate loss: 0.0097
             Mean action noise std: 0.95
                       Mean reward: 2263.00
               Mean episode length: 268.13
                 Mean success rate: 32.00
                  Mean reward/step: 6.91
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 6938624
                    Iteration time: 2.56s
                        Total time: 2181.32s
                               ETA: 8122.6s

################################################################################
                     [1m Learning iteration 847/4000 [0m

                       Computation: 3246 steps/s (collection: 0.444s, learning 2.079s)
               Value function loss: 27847.9738
                    Surrogate loss: 0.0118
             Mean action noise std: 0.95
                       Mean reward: 2379.31
               Mean episode length: 280.56
                 Mean success rate: 32.50
                  Mean reward/step: 6.33
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 2.52s
                        Total time: 2183.84s
                               ETA: 8119.9s

################################################################################
                     [1m Learning iteration 848/4000 [0m

                       Computation: 3116 steps/s (collection: 0.457s, learning 2.171s)
               Value function loss: 31632.6236
                    Surrogate loss: 0.0129
             Mean action noise std: 0.95
                       Mean reward: 2361.29
               Mean episode length: 279.61
                 Mean success rate: 33.00
                  Mean reward/step: 6.55
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6955008
                    Iteration time: 2.63s
                        Total time: 2186.47s
                               ETA: 8117.5s

################################################################################
                     [1m Learning iteration 849/4000 [0m

                       Computation: 3198 steps/s (collection: 0.470s, learning 2.091s)
               Value function loss: 24802.9391
                    Surrogate loss: 0.0124
             Mean action noise std: 0.95
                       Mean reward: 2251.33
               Mean episode length: 265.57
                 Mean success rate: 32.50
                  Mean reward/step: 6.57
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 2.56s
                        Total time: 2189.03s
                               ETA: 8114.9s

################################################################################
                     [1m Learning iteration 850/4000 [0m

                       Computation: 3052 steps/s (collection: 0.502s, learning 2.182s)
               Value function loss: 41832.2552
                    Surrogate loss: 0.0115
             Mean action noise std: 0.95
                       Mean reward: 2526.44
               Mean episode length: 283.83
                 Mean success rate: 36.50
                  Mean reward/step: 6.57
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 6971392
                    Iteration time: 2.68s
                        Total time: 2191.71s
                               ETA: 8112.7s

################################################################################
                     [1m Learning iteration 851/4000 [0m

                       Computation: 3153 steps/s (collection: 0.479s, learning 2.119s)
               Value function loss: 31682.2982
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 2370.85
               Mean episode length: 276.71
                 Mean success rate: 34.50
                  Mean reward/step: 6.60
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 2.60s
                        Total time: 2194.31s
                               ETA: 8110.2s

################################################################################
                     [1m Learning iteration 852/4000 [0m

                       Computation: 3177 steps/s (collection: 0.472s, learning 2.107s)
               Value function loss: 25804.3332
                    Surrogate loss: 0.0150
             Mean action noise std: 0.95
                       Mean reward: 2142.21
               Mean episode length: 274.13
                 Mean success rate: 31.50
                  Mean reward/step: 6.50
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6987776
                    Iteration time: 2.58s
                        Total time: 2196.89s
                               ETA: 8107.6s

################################################################################
                     [1m Learning iteration 853/4000 [0m

                       Computation: 3141 steps/s (collection: 0.493s, learning 2.114s)
               Value function loss: 34641.2262
                    Surrogate loss: 0.0116
             Mean action noise std: 0.95
                       Mean reward: 2011.95
               Mean episode length: 268.90
                 Mean success rate: 31.00
                  Mean reward/step: 6.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 2.61s
                        Total time: 2199.50s
                               ETA: 8105.2s

################################################################################
                     [1m Learning iteration 854/4000 [0m

                       Computation: 3085 steps/s (collection: 0.538s, learning 2.117s)
               Value function loss: 24391.7794
                    Surrogate loss: 0.0154
             Mean action noise std: 0.95
                       Mean reward: 1971.31
               Mean episode length: 277.88
                 Mean success rate: 31.50
                  Mean reward/step: 7.31
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7004160
                    Iteration time: 2.65s
                        Total time: 2202.15s
                               ETA: 8102.9s

################################################################################
                     [1m Learning iteration 855/4000 [0m

                       Computation: 3015 steps/s (collection: 0.591s, learning 2.126s)
               Value function loss: 23582.2013
                    Surrogate loss: 0.0141
             Mean action noise std: 0.95
                       Mean reward: 1919.14
               Mean episode length: 280.06
                 Mean success rate: 30.50
                  Mean reward/step: 7.29
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 2.72s
                        Total time: 2204.87s
                               ETA: 8100.8s

################################################################################
                     [1m Learning iteration 856/4000 [0m

                       Computation: 3173 steps/s (collection: 0.498s, learning 2.083s)
               Value function loss: 23023.5159
                    Surrogate loss: 0.0170
             Mean action noise std: 0.95
                       Mean reward: 1843.45
               Mean episode length: 271.88
                 Mean success rate: 28.00
                  Mean reward/step: 7.48
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 7020544
                    Iteration time: 2.58s
                        Total time: 2207.45s
                               ETA: 8098.3s

################################################################################
                     [1m Learning iteration 857/4000 [0m

                       Computation: 3185 steps/s (collection: 0.470s, learning 2.101s)
               Value function loss: 28727.6984
                    Surrogate loss: 0.0123
             Mean action noise std: 0.95
                       Mean reward: 1736.13
               Mean episode length: 272.97
                 Mean success rate: 29.00
                  Mean reward/step: 7.09
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 2.57s
                        Total time: 2210.02s
                               ETA: 8095.7s

################################################################################
                     [1m Learning iteration 858/4000 [0m

                       Computation: 3139 steps/s (collection: 0.477s, learning 2.132s)
               Value function loss: 28891.8626
                    Surrogate loss: 0.0114
             Mean action noise std: 0.95
                       Mean reward: 1650.52
               Mean episode length: 272.56
                 Mean success rate: 28.00
                  Mean reward/step: 7.00
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 7036928
                    Iteration time: 2.61s
                        Total time: 2212.63s
                               ETA: 8093.2s

################################################################################
                     [1m Learning iteration 859/4000 [0m

                       Computation: 3045 steps/s (collection: 0.543s, learning 2.146s)
               Value function loss: 26093.6340
                    Surrogate loss: 0.0130
             Mean action noise std: 0.95
                       Mean reward: 1333.88
               Mean episode length: 249.01
                 Mean success rate: 24.00
                  Mean reward/step: 7.59
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 2.69s
                        Total time: 2215.32s
                               ETA: 8091.1s

################################################################################
                     [1m Learning iteration 860/4000 [0m

                       Computation: 3127 steps/s (collection: 0.517s, learning 2.103s)
               Value function loss: 28504.4454
                    Surrogate loss: 0.0152
             Mean action noise std: 0.95
                       Mean reward: 1423.80
               Mean episode length: 249.78
                 Mean success rate: 24.00
                  Mean reward/step: 8.10
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7053312
                    Iteration time: 2.62s
                        Total time: 2217.94s
                               ETA: 8088.6s

################################################################################
                     [1m Learning iteration 861/4000 [0m

                       Computation: 3200 steps/s (collection: 0.506s, learning 2.054s)
               Value function loss: 28140.6078
                    Surrogate loss: 0.0117
             Mean action noise std: 0.95
                       Mean reward: 1468.18
               Mean episode length: 240.42
                 Mean success rate: 24.00
                  Mean reward/step: 7.96
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 2.56s
                        Total time: 2220.50s
                               ETA: 8086.0s

################################################################################
                     [1m Learning iteration 862/4000 [0m

                       Computation: 3253 steps/s (collection: 0.459s, learning 2.059s)
               Value function loss: 27575.2719
                    Surrogate loss: 0.0156
             Mean action noise std: 0.95
                       Mean reward: 1478.08
               Mean episode length: 239.85
                 Mean success rate: 24.00
                  Mean reward/step: 7.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 7069696
                    Iteration time: 2.52s
                        Total time: 2223.02s
                               ETA: 8083.2s

################################################################################
                     [1m Learning iteration 863/4000 [0m

                       Computation: 3258 steps/s (collection: 0.488s, learning 2.026s)
               Value function loss: 35007.3305
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 1621.76
               Mean episode length: 243.46
                 Mean success rate: 25.00
                  Mean reward/step: 7.56
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 2.51s
                        Total time: 2225.53s
                               ETA: 8080.4s

################################################################################
                     [1m Learning iteration 864/4000 [0m

                       Computation: 3173 steps/s (collection: 0.541s, learning 2.040s)
               Value function loss: 36507.6265
                    Surrogate loss: 0.0085
             Mean action noise std: 0.95
                       Mean reward: 1743.49
               Mean episode length: 242.22
                 Mean success rate: 26.00
                  Mean reward/step: 6.78
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 7086080
                    Iteration time: 2.58s
                        Total time: 2228.11s
                               ETA: 8077.9s

################################################################################
                     [1m Learning iteration 865/4000 [0m

                       Computation: 3185 steps/s (collection: 0.499s, learning 2.073s)
               Value function loss: 24337.6641
                    Surrogate loss: 0.0153
             Mean action noise std: 0.95
                       Mean reward: 1873.90
               Mean episode length: 247.66
                 Mean success rate: 28.00
                  Mean reward/step: 6.60
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 2.57s
                        Total time: 2230.68s
                               ETA: 8075.3s

################################################################################
                     [1m Learning iteration 866/4000 [0m

                       Computation: 3197 steps/s (collection: 0.507s, learning 2.055s)
               Value function loss: 21131.5189
                    Surrogate loss: 0.0190
             Mean action noise std: 0.95
                       Mean reward: 1739.29
               Mean episode length: 238.72
                 Mean success rate: 26.00
                  Mean reward/step: 6.55
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 7102464
                    Iteration time: 2.56s
                        Total time: 2233.25s
                               ETA: 8072.7s

################################################################################
                     [1m Learning iteration 867/4000 [0m

                       Computation: 3206 steps/s (collection: 0.517s, learning 2.038s)
               Value function loss: 31021.6066
                    Surrogate loss: 0.0127
             Mean action noise std: 0.95
                       Mean reward: 1761.78
               Mean episode length: 247.79
                 Mean success rate: 26.00
                  Mean reward/step: 6.70
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 2.55s
                        Total time: 2235.80s
                               ETA: 8070.0s

################################################################################
                     [1m Learning iteration 868/4000 [0m

                       Computation: 3193 steps/s (collection: 0.485s, learning 2.080s)
               Value function loss: 24138.0525
                    Surrogate loss: 0.0121
             Mean action noise std: 0.95
                       Mean reward: 1777.78
               Mean episode length: 246.69
                 Mean success rate: 26.00
                  Mean reward/step: 6.26
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 7118848
                    Iteration time: 2.56s
                        Total time: 2238.37s
                               ETA: 8067.4s

################################################################################
                     [1m Learning iteration 869/4000 [0m

                       Computation: 3197 steps/s (collection: 0.504s, learning 2.059s)
               Value function loss: 22979.3267
                    Surrogate loss: 0.0139
             Mean action noise std: 0.95
                       Mean reward: 1714.18
               Mean episode length: 239.26
                 Mean success rate: 25.00
                  Mean reward/step: 6.29
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 2.56s
                        Total time: 2240.93s
                               ETA: 8064.8s

################################################################################
                     [1m Learning iteration 870/4000 [0m

                       Computation: 3248 steps/s (collection: 0.495s, learning 2.027s)
               Value function loss: 14595.6637
                    Surrogate loss: 0.0148
             Mean action noise std: 0.95
                       Mean reward: 1582.53
               Mean episode length: 231.34
                 Mean success rate: 24.50
                  Mean reward/step: 6.15
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7135232
                    Iteration time: 2.52s
                        Total time: 2243.45s
                               ETA: 8062.0s

################################################################################
                     [1m Learning iteration 871/4000 [0m

                       Computation: 3163 steps/s (collection: 0.539s, learning 2.050s)
               Value function loss: 25075.9614
                    Surrogate loss: 0.0133
             Mean action noise std: 0.95
                       Mean reward: 1563.39
               Mean episode length: 227.75
                 Mean success rate: 22.50
                  Mean reward/step: 6.51
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 2.59s
                        Total time: 2246.04s
                               ETA: 8059.5s

################################################################################
                     [1m Learning iteration 872/4000 [0m

                       Computation: 3148 steps/s (collection: 0.512s, learning 2.089s)
               Value function loss: 29533.1334
                    Surrogate loss: 0.0119
             Mean action noise std: 0.95
                       Mean reward: 1751.00
               Mean episode length: 225.46
                 Mean success rate: 26.00
                  Mean reward/step: 5.97
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 7151616
                    Iteration time: 2.60s
                        Total time: 2248.64s
                               ETA: 8057.0s

################################################################################
                     [1m Learning iteration 873/4000 [0m

                       Computation: 3092 steps/s (collection: 0.512s, learning 2.138s)
               Value function loss: 25262.7442
                    Surrogate loss: 0.0148
             Mean action noise std: 0.95
                       Mean reward: 1613.83
               Mean episode length: 214.51
                 Mean success rate: 25.00
                  Mean reward/step: 5.66
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 2.65s
                        Total time: 2251.29s
                               ETA: 8054.7s

################################################################################
                     [1m Learning iteration 874/4000 [0m

                       Computation: 3092 steps/s (collection: 0.532s, learning 2.117s)
               Value function loss: 22662.9894
                    Surrogate loss: 0.0126
             Mean action noise std: 0.95
                       Mean reward: 1502.18
               Mean episode length: 211.69
                 Mean success rate: 24.50
                  Mean reward/step: 5.53
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 7168000
                    Iteration time: 2.65s
                        Total time: 2253.94s
                               ETA: 8052.4s

################################################################################
                     [1m Learning iteration 875/4000 [0m

                       Computation: 3122 steps/s (collection: 0.503s, learning 2.120s)
               Value function loss: 18973.0146
                    Surrogate loss: 0.0175
             Mean action noise std: 0.95
                       Mean reward: 1364.00
               Mean episode length: 202.76
                 Mean success rate: 22.50
                  Mean reward/step: 6.05
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 2.62s
                        Total time: 2256.56s
                               ETA: 8049.9s

################################################################################
                     [1m Learning iteration 876/4000 [0m

                       Computation: 3161 steps/s (collection: 0.481s, learning 2.110s)
               Value function loss: 19936.5412
                    Surrogate loss: 0.0158
             Mean action noise std: 0.95
                       Mean reward: 1289.03
               Mean episode length: 198.57
                 Mean success rate: 21.50
                  Mean reward/step: 6.34
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 7184384
                    Iteration time: 2.59s
                        Total time: 2259.15s
                               ETA: 8047.4s

################################################################################
                     [1m Learning iteration 877/4000 [0m

                       Computation: 3180 steps/s (collection: 0.466s, learning 2.110s)
               Value function loss: 21267.4854
                    Surrogate loss: 0.0200
             Mean action noise std: 0.95
                       Mean reward: 1108.16
               Mean episode length: 193.54
                 Mean success rate: 18.50
                  Mean reward/step: 6.64
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 2.58s
                        Total time: 2261.73s
                               ETA: 8044.9s

################################################################################
                     [1m Learning iteration 878/4000 [0m

                       Computation: 2982 steps/s (collection: 0.564s, learning 2.183s)
               Value function loss: 20669.1861
                    Surrogate loss: 0.0197
             Mean action noise std: 0.95
                       Mean reward: 1023.24
               Mean episode length: 177.86
                 Mean success rate: 16.50
                  Mean reward/step: 6.85
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7200768
                    Iteration time: 2.75s
                        Total time: 2264.48s
                               ETA: 8042.9s

################################################################################
                     [1m Learning iteration 879/4000 [0m

                       Computation: 3055 steps/s (collection: 0.566s, learning 2.116s)
               Value function loss: 29074.1459
                    Surrogate loss: 0.0147
             Mean action noise std: 0.95
                       Mean reward: 1074.02
               Mean episode length: 195.22
                 Mean success rate: 17.00
                  Mean reward/step: 7.48
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 2.68s
                        Total time: 2267.16s
                               ETA: 8040.7s

################################################################################
                     [1m Learning iteration 880/4000 [0m

                       Computation: 3091 steps/s (collection: 0.509s, learning 2.140s)
               Value function loss: 39382.8377
                    Surrogate loss: 0.0118
             Mean action noise std: 0.95
                       Mean reward: 1177.16
               Mean episode length: 202.74
                 Mean success rate: 18.50
                  Mean reward/step: 7.07
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7217152
                    Iteration time: 2.65s
                        Total time: 2269.81s
                               ETA: 8038.4s

################################################################################
                     [1m Learning iteration 881/4000 [0m

                       Computation: 3155 steps/s (collection: 0.464s, learning 2.132s)
               Value function loss: 28595.1939
                    Surrogate loss: 0.0154
             Mean action noise std: 0.95
                       Mean reward: 1282.07
               Mean episode length: 216.68
                 Mean success rate: 20.00
                  Mean reward/step: 7.11
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 2.60s
                        Total time: 2272.40s
                               ETA: 8035.9s

################################################################################
                     [1m Learning iteration 882/4000 [0m

                       Computation: 3105 steps/s (collection: 0.480s, learning 2.158s)
               Value function loss: 42155.9561
                    Surrogate loss: 0.0122
             Mean action noise std: 0.95
                       Mean reward: 1553.40
               Mean episode length: 231.33
                 Mean success rate: 24.00
                  Mean reward/step: 7.17
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7233536
                    Iteration time: 2.64s
                        Total time: 2275.04s
                               ETA: 8033.5s

################################################################################
                     [1m Learning iteration 883/4000 [0m

                       Computation: 3108 steps/s (collection: 0.495s, learning 2.141s)
               Value function loss: 14867.5565
                    Surrogate loss: 0.0163
             Mean action noise std: 0.95
                       Mean reward: 1566.96
               Mean episode length: 236.33
                 Mean success rate: 25.50
                  Mean reward/step: 7.18
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 2.64s
                        Total time: 2277.68s
                               ETA: 8031.1s

################################################################################
                     [1m Learning iteration 884/4000 [0m

                       Computation: 3188 steps/s (collection: 0.475s, learning 2.094s)
               Value function loss: 17028.4057
                    Surrogate loss: 0.0156
             Mean action noise std: 0.95
                       Mean reward: 1511.24
               Mean episode length: 230.25
                 Mean success rate: 25.50
                  Mean reward/step: 7.47
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7249920
                    Iteration time: 2.57s
                        Total time: 2280.25s
                               ETA: 8028.5s

################################################################################
                     [1m Learning iteration 885/4000 [0m

                       Computation: 3115 steps/s (collection: 0.516s, learning 2.113s)
               Value function loss: 25219.1060
                    Surrogate loss: 0.0143
             Mean action noise std: 0.95
                       Mean reward: 1461.91
               Mean episode length: 219.25
                 Mean success rate: 24.50
                  Mean reward/step: 7.26
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 2.63s
                        Total time: 2282.87s
                               ETA: 8026.1s

################################################################################
                     [1m Learning iteration 886/4000 [0m

                       Computation: 3135 steps/s (collection: 0.497s, learning 2.116s)
               Value function loss: 40087.7732
                    Surrogate loss: 0.0094
             Mean action noise std: 0.95
                       Mean reward: 1531.93
               Mean episode length: 218.75
                 Mean success rate: 26.50
                  Mean reward/step: 7.08
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7266304
                    Iteration time: 2.61s
                        Total time: 2285.49s
                               ETA: 8023.7s

################################################################################
                     [1m Learning iteration 887/4000 [0m

                       Computation: 3147 steps/s (collection: 0.557s, learning 2.045s)
               Value function loss: 32379.4460
                    Surrogate loss: 0.0120
             Mean action noise std: 0.95
                       Mean reward: 1469.37
               Mean episode length: 207.34
                 Mean success rate: 25.00
                  Mean reward/step: 6.64
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 2.60s
                        Total time: 2288.09s
                               ETA: 8021.2s

################################################################################
                     [1m Learning iteration 888/4000 [0m

                       Computation: 3108 steps/s (collection: 0.535s, learning 2.100s)
               Value function loss: 30906.9040
                    Surrogate loss: 0.0125
             Mean action noise std: 0.95
                       Mean reward: 1511.01
               Mean episode length: 208.75
                 Mean success rate: 25.00
                  Mean reward/step: 6.45
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7282688
                    Iteration time: 2.63s
                        Total time: 2290.72s
                               ETA: 8018.8s

################################################################################
                     [1m Learning iteration 889/4000 [0m

                       Computation: 3175 steps/s (collection: 0.498s, learning 2.082s)
               Value function loss: 33264.2581
                    Surrogate loss: 0.0126
             Mean action noise std: 0.95
                       Mean reward: 1567.11
               Mean episode length: 217.99
                 Mean success rate: 26.50
                  Mean reward/step: 6.78
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 2.58s
                        Total time: 2293.30s
                               ETA: 8016.3s

################################################################################
                     [1m Learning iteration 890/4000 [0m

                       Computation: 3171 steps/s (collection: 0.450s, learning 2.133s)
               Value function loss: 28988.3404
                    Surrogate loss: 0.0130
             Mean action noise std: 0.95
                       Mean reward: 1649.52
               Mean episode length: 216.71
                 Mean success rate: 25.00
                  Mean reward/step: 6.68
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 7299072
                    Iteration time: 2.58s
                        Total time: 2295.89s
                               ETA: 8013.7s

################################################################################
                     [1m Learning iteration 891/4000 [0m

                       Computation: 3043 steps/s (collection: 0.553s, learning 2.138s)
               Value function loss: 24741.0612
                    Surrogate loss: 0.0154
             Mean action noise std: 0.95
                       Mean reward: 1361.67
               Mean episode length: 203.09
                 Mean success rate: 20.00
                  Mean reward/step: 7.25
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 2.69s
                        Total time: 2298.58s
                               ETA: 8011.5s

################################################################################
                     [1m Learning iteration 892/4000 [0m

                       Computation: 3114 steps/s (collection: 0.514s, learning 2.116s)
               Value function loss: 29047.8245
                    Surrogate loss: 0.0156
             Mean action noise std: 0.95
                       Mean reward: 1311.59
               Mean episode length: 201.28
                 Mean success rate: 19.00
                  Mean reward/step: 7.04
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7315456
                    Iteration time: 2.63s
                        Total time: 2301.21s
                               ETA: 8009.1s

################################################################################
                     [1m Learning iteration 893/4000 [0m

                       Computation: 3214 steps/s (collection: 0.467s, learning 2.081s)
               Value function loss: 21192.1519
                    Surrogate loss: 0.0161
             Mean action noise std: 0.95
                       Mean reward: 1156.15
               Mean episode length: 183.81
                 Mean success rate: 16.00
                  Mean reward/step: 7.60
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 2.55s
                        Total time: 2303.76s
                               ETA: 8006.5s

################################################################################
                     [1m Learning iteration 894/4000 [0m

                       Computation: 3099 steps/s (collection: 0.468s, learning 2.175s)
               Value function loss: 33358.7832
                    Surrogate loss: 0.0116
             Mean action noise std: 0.95
                       Mean reward: 1217.67
               Mean episode length: 184.59
                 Mean success rate: 16.00
                  Mean reward/step: 7.76
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7331840
                    Iteration time: 2.64s
                        Total time: 2306.40s
                               ETA: 8004.1s

################################################################################
                     [1m Learning iteration 895/4000 [0m

                       Computation: 3137 steps/s (collection: 0.533s, learning 2.078s)
               Value function loss: 27104.7554
                    Surrogate loss: 0.0132
             Mean action noise std: 0.95
                       Mean reward: 1187.73
               Mean episode length: 190.96
                 Mean success rate: 16.50
                  Mean reward/step: 8.19
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 2.61s
                        Total time: 2309.01s
                               ETA: 8001.7s

################################################################################
                     [1m Learning iteration 896/4000 [0m

                       Computation: 3226 steps/s (collection: 0.502s, learning 2.037s)
               Value function loss: 30417.0671
                    Surrogate loss: 0.0177
             Mean action noise std: 0.95
                       Mean reward: 1304.49
               Mean episode length: 194.31
                 Mean success rate: 19.00
                  Mean reward/step: 7.81
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 7348224
                    Iteration time: 2.54s
                        Total time: 2311.55s
                               ETA: 7998.9s

################################################################################
                     [1m Learning iteration 897/4000 [0m

                       Computation: 3242 steps/s (collection: 0.487s, learning 2.039s)
               Value function loss: 24538.8664
                    Surrogate loss: 0.0179
             Mean action noise std: 0.95
                       Mean reward: 1388.37
               Mean episode length: 201.22
                 Mean success rate: 21.00
                  Mean reward/step: 7.17
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 2.53s
                        Total time: 2314.08s
                               ETA: 7996.2s

################################################################################
                     [1m Learning iteration 898/4000 [0m

                       Computation: 3173 steps/s (collection: 0.489s, learning 2.093s)
               Value function loss: 25609.2315
                    Surrogate loss: 0.0164
             Mean action noise std: 0.95
                       Mean reward: 1534.70
               Mean episode length: 221.28
                 Mean success rate: 24.00
                  Mean reward/step: 7.38
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7364608
                    Iteration time: 2.58s
                        Total time: 2316.66s
                               ETA: 7993.6s

################################################################################
                     [1m Learning iteration 899/4000 [0m

                       Computation: 3128 steps/s (collection: 0.516s, learning 2.102s)
               Value function loss: 29633.1998
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 1575.79
               Mean episode length: 226.81
                 Mean success rate: 25.00
                  Mean reward/step: 7.08
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 2.62s
                        Total time: 2319.28s
                               ETA: 7991.2s

################################################################################
                     [1m Learning iteration 900/4000 [0m

                       Computation: 3123 steps/s (collection: 0.509s, learning 2.114s)
               Value function loss: 25234.7823
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 1578.98
               Mean episode length: 223.59
                 Mean success rate: 25.00
                  Mean reward/step: 7.39
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7380992
                    Iteration time: 2.62s
                        Total time: 2321.90s
                               ETA: 7988.8s

################################################################################
                     [1m Learning iteration 901/4000 [0m

                       Computation: 3160 steps/s (collection: 0.481s, learning 2.111s)
               Value function loss: 34507.6216
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 1616.25
               Mean episode length: 236.00
                 Mean success rate: 26.00
                  Mean reward/step: 7.34
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 2.59s
                        Total time: 2324.49s
                               ETA: 7986.2s

################################################################################
                     [1m Learning iteration 902/4000 [0m

                       Computation: 3135 steps/s (collection: 0.513s, learning 2.100s)
               Value function loss: 35074.7840
                    Surrogate loss: 0.0121
             Mean action noise std: 0.95
                       Mean reward: 1734.10
               Mean episode length: 245.63
                 Mean success rate: 27.00
                  Mean reward/step: 7.18
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7397376
                    Iteration time: 2.61s
                        Total time: 2327.10s
                               ETA: 7983.8s

################################################################################
                     [1m Learning iteration 903/4000 [0m

                       Computation: 3151 steps/s (collection: 0.470s, learning 2.129s)
               Value function loss: 21654.1083
                    Surrogate loss: 0.0148
             Mean action noise std: 0.95
                       Mean reward: 1789.25
               Mean episode length: 237.41
                 Mean success rate: 27.50
                  Mean reward/step: 7.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 2.60s
                        Total time: 2329.70s
                               ETA: 7981.3s

################################################################################
                     [1m Learning iteration 904/4000 [0m

                       Computation: 3123 steps/s (collection: 0.539s, learning 2.084s)
               Value function loss: 28927.4399
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 1838.86
               Mean episode length: 230.34
                 Mean success rate: 27.50
                  Mean reward/step: 7.34
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7413760
                    Iteration time: 2.62s
                        Total time: 2332.33s
                               ETA: 7978.9s

################################################################################
                     [1m Learning iteration 905/4000 [0m

                       Computation: 3201 steps/s (collection: 0.458s, learning 2.101s)
               Value function loss: 28531.3096
                    Surrogate loss: 0.0157
             Mean action noise std: 0.95
                       Mean reward: 1923.85
               Mean episode length: 231.38
                 Mean success rate: 27.50
                  Mean reward/step: 7.36
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 2.56s
                        Total time: 2334.88s
                               ETA: 7976.2s

################################################################################
                     [1m Learning iteration 906/4000 [0m

                       Computation: 3147 steps/s (collection: 0.480s, learning 2.123s)
               Value function loss: 24847.6750
                    Surrogate loss: 0.0146
             Mean action noise std: 0.95
                       Mean reward: 2088.30
               Mean episode length: 242.01
                 Mean success rate: 29.00
                  Mean reward/step: 7.12
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7430144
                    Iteration time: 2.60s
                        Total time: 2337.49s
                               ETA: 7973.7s

################################################################################
                     [1m Learning iteration 907/4000 [0m

                       Computation: 3152 steps/s (collection: 0.520s, learning 2.079s)
               Value function loss: 24013.0003
                    Surrogate loss: 0.0166
             Mean action noise std: 0.95
                       Mean reward: 2048.16
               Mean episode length: 241.50
                 Mean success rate: 27.00
                  Mean reward/step: 7.50
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 2.60s
                        Total time: 2340.09s
                               ETA: 7971.2s

################################################################################
                     [1m Learning iteration 908/4000 [0m

                       Computation: 3170 steps/s (collection: 0.525s, learning 2.059s)
               Value function loss: 21497.0132
                    Surrogate loss: 0.0229
             Mean action noise std: 0.95
                       Mean reward: 1911.92
               Mean episode length: 231.47
                 Mean success rate: 24.50
                  Mean reward/step: 8.13
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7446528
                    Iteration time: 2.58s
                        Total time: 2342.67s
                               ETA: 7968.7s

################################################################################
                     [1m Learning iteration 909/4000 [0m

                       Computation: 3195 steps/s (collection: 0.486s, learning 2.078s)
               Value function loss: 31673.4924
                    Surrogate loss: 0.0164
             Mean action noise std: 0.95
                       Mean reward: 1851.11
               Mean episode length: 229.71
                 Mean success rate: 23.50
                  Mean reward/step: 8.10
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 2.56s
                        Total time: 2345.23s
                               ETA: 7966.1s

################################################################################
                     [1m Learning iteration 910/4000 [0m

                       Computation: 3062 steps/s (collection: 0.510s, learning 2.165s)
               Value function loss: 25729.0397
                    Surrogate loss: 0.0133
             Mean action noise std: 0.95
                       Mean reward: 1555.80
               Mean episode length: 229.45
                 Mean success rate: 20.00
                  Mean reward/step: 7.56
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7462912
                    Iteration time: 2.68s
                        Total time: 2347.91s
                               ETA: 7963.8s

################################################################################
                     [1m Learning iteration 911/4000 [0m

                       Computation: 3187 steps/s (collection: 0.482s, learning 2.087s)
               Value function loss: 27703.4074
                    Surrogate loss: 0.0148
             Mean action noise std: 0.95
                       Mean reward: 1540.72
               Mean episode length: 232.03
                 Mean success rate: 20.50
                  Mean reward/step: 7.86
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 2.57s
                        Total time: 2350.48s
                               ETA: 7961.2s

################################################################################
                     [1m Learning iteration 912/4000 [0m

                       Computation: 3119 steps/s (collection: 0.505s, learning 2.122s)
               Value function loss: 45441.8935
                    Surrogate loss: 0.0101
             Mean action noise std: 0.95
                       Mean reward: 1634.12
               Mean episode length: 242.93
                 Mean success rate: 23.50
                  Mean reward/step: 7.87
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7479296
                    Iteration time: 2.63s
                        Total time: 2353.10s
                               ETA: 7958.8s

################################################################################
                     [1m Learning iteration 913/4000 [0m

                       Computation: 3150 steps/s (collection: 0.480s, learning 2.120s)
               Value function loss: 23566.5963
                    Surrogate loss: 0.0110
             Mean action noise std: 0.95
                       Mean reward: 1792.50
               Mean episode length: 252.66
                 Mean success rate: 26.50
                  Mean reward/step: 7.74
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 2.60s
                        Total time: 2355.70s
                               ETA: 7956.3s

################################################################################
                     [1m Learning iteration 914/4000 [0m

                       Computation: 3097 steps/s (collection: 0.477s, learning 2.168s)
               Value function loss: 28074.3765
                    Surrogate loss: 0.0112
             Mean action noise std: 0.95
                       Mean reward: 1950.93
               Mean episode length: 255.34
                 Mean success rate: 26.50
                  Mean reward/step: 7.57
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7495680
                    Iteration time: 2.64s
                        Total time: 2358.35s
                               ETA: 7954.0s

################################################################################
                     [1m Learning iteration 915/4000 [0m

                       Computation: 3118 steps/s (collection: 0.511s, learning 2.116s)
               Value function loss: 43618.2076
                    Surrogate loss: 0.0128
             Mean action noise std: 0.95
                       Mean reward: 2054.95
               Mean episode length: 264.69
                 Mean success rate: 27.00
                  Mean reward/step: 7.57
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 2.63s
                        Total time: 2360.98s
                               ETA: 7951.5s

################################################################################
                     [1m Learning iteration 916/4000 [0m

                       Computation: 3171 steps/s (collection: 0.476s, learning 2.106s)
               Value function loss: 32853.4880
                    Surrogate loss: 0.0108
             Mean action noise std: 0.95
                       Mean reward: 2258.56
               Mean episode length: 269.39
                 Mean success rate: 30.00
                  Mean reward/step: 7.40
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7512064
                    Iteration time: 2.58s
                        Total time: 2363.56s
                               ETA: 7949.0s

################################################################################
                     [1m Learning iteration 917/4000 [0m

                       Computation: 3168 steps/s (collection: 0.475s, learning 2.111s)
               Value function loss: 33622.7066
                    Surrogate loss: 0.0149
             Mean action noise std: 0.95
                       Mean reward: 2387.77
               Mean episode length: 270.62
                 Mean success rate: 31.00
                  Mean reward/step: 8.09
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 2.59s
                        Total time: 2366.14s
                               ETA: 7946.4s

################################################################################
                     [1m Learning iteration 918/4000 [0m

                       Computation: 3231 steps/s (collection: 0.463s, learning 2.072s)
               Value function loss: 19471.7248
                    Surrogate loss: 0.0182
             Mean action noise std: 0.95
                       Mean reward: 2242.95
               Mean episode length: 262.58
                 Mean success rate: 29.00
                  Mean reward/step: 8.09
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7528448
                    Iteration time: 2.54s
                        Total time: 2368.68s
                               ETA: 7943.7s

################################################################################
                     [1m Learning iteration 919/4000 [0m

                       Computation: 3216 steps/s (collection: 0.515s, learning 2.032s)
               Value function loss: 22464.2536
                    Surrogate loss: 0.0146
             Mean action noise std: 0.95
                       Mean reward: 2156.39
               Mean episode length: 258.05
                 Mean success rate: 25.50
                  Mean reward/step: 8.05
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 2.55s
                        Total time: 2371.23s
                               ETA: 7941.0s

################################################################################
                     [1m Learning iteration 920/4000 [0m

                       Computation: 3151 steps/s (collection: 0.538s, learning 2.061s)
               Value function loss: 37150.0245
                    Surrogate loss: 0.0154
             Mean action noise std: 0.95
                       Mean reward: 1998.81
               Mean episode length: 258.14
                 Mean success rate: 25.00
                  Mean reward/step: 8.07
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7544832
                    Iteration time: 2.60s
                        Total time: 2373.83s
                               ETA: 7938.5s

################################################################################
                     [1m Learning iteration 921/4000 [0m

                       Computation: 3186 steps/s (collection: 0.490s, learning 2.081s)
               Value function loss: 21790.1048
                    Surrogate loss: 0.0134
             Mean action noise std: 0.95
                       Mean reward: 1937.40
               Mean episode length: 259.41
                 Mean success rate: 25.00
                  Mean reward/step: 8.21
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 2.57s
                        Total time: 2376.40s
                               ETA: 7935.9s

################################################################################
                     [1m Learning iteration 922/4000 [0m

                       Computation: 3197 steps/s (collection: 0.485s, learning 2.077s)
               Value function loss: 32384.8471
                    Surrogate loss: 0.0132
             Mean action noise std: 0.95
                       Mean reward: 1738.06
               Mean episode length: 250.34
                 Mean success rate: 23.50
                  Mean reward/step: 7.92
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7561216
                    Iteration time: 2.56s
                        Total time: 2378.96s
                               ETA: 7933.3s

################################################################################
                     [1m Learning iteration 923/4000 [0m

                       Computation: 3140 steps/s (collection: 0.520s, learning 2.088s)
               Value function loss: 35642.5426
                    Surrogate loss: 0.0134
             Mean action noise std: 0.95
                       Mean reward: 1686.69
               Mean episode length: 245.44
                 Mean success rate: 23.00
                  Mean reward/step: 8.14
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 2.61s
                        Total time: 2381.57s
                               ETA: 7930.8s

################################################################################
                     [1m Learning iteration 924/4000 [0m

                       Computation: 3189 steps/s (collection: 0.480s, learning 2.088s)
               Value function loss: 21604.0981
                    Surrogate loss: 0.0130
             Mean action noise std: 0.95
                       Mean reward: 1683.90
               Mean episode length: 242.48
                 Mean success rate: 23.00
                  Mean reward/step: 7.87
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7577600
                    Iteration time: 2.57s
                        Total time: 2384.14s
                               ETA: 7928.2s

################################################################################
                     [1m Learning iteration 925/4000 [0m

                       Computation: 3189 steps/s (collection: 0.481s, learning 2.087s)
               Value function loss: 24780.2385
                    Surrogate loss: 0.0170
             Mean action noise std: 0.95
                       Mean reward: 1836.55
               Mean episode length: 247.99
                 Mean success rate: 25.50
                  Mean reward/step: 8.02
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 2.57s
                        Total time: 2386.70s
                               ETA: 7925.6s

################################################################################
                     [1m Learning iteration 926/4000 [0m

                       Computation: 3193 steps/s (collection: 0.465s, learning 2.100s)
               Value function loss: 30149.1812
                    Surrogate loss: 0.0246
             Mean action noise std: 0.95
                       Mean reward: 1853.99
               Mean episode length: 237.72
                 Mean success rate: 25.00
                  Mean reward/step: 7.61
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7593984
                    Iteration time: 2.57s
                        Total time: 2389.27s
                               ETA: 7923.0s

################################################################################
                     [1m Learning iteration 927/4000 [0m

                       Computation: 3135 steps/s (collection: 0.545s, learning 2.067s)
               Value function loss: 27439.0332
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 1774.18
               Mean episode length: 232.48
                 Mean success rate: 24.00
                  Mean reward/step: 6.89
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 2.61s
                        Total time: 2391.88s
                               ETA: 7920.5s

################################################################################
                     [1m Learning iteration 928/4000 [0m

                       Computation: 3177 steps/s (collection: 0.472s, learning 2.106s)
               Value function loss: 30371.9106
                    Surrogate loss: 0.0134
             Mean action noise std: 0.95
                       Mean reward: 1762.40
               Mean episode length: 234.21
                 Mean success rate: 23.00
                  Mean reward/step: 6.89
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7610368
                    Iteration time: 2.58s
                        Total time: 2394.46s
                               ETA: 7918.0s

################################################################################
                     [1m Learning iteration 929/4000 [0m

                       Computation: 3150 steps/s (collection: 0.499s, learning 2.101s)
               Value function loss: 21005.2951
                    Surrogate loss: 0.0143
             Mean action noise std: 0.95
                       Mean reward: 1732.63
               Mean episode length: 230.13
                 Mean success rate: 21.50
                  Mean reward/step: 6.86
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 2.60s
                        Total time: 2397.06s
                               ETA: 7915.5s

################################################################################
                     [1m Learning iteration 930/4000 [0m

                       Computation: 3266 steps/s (collection: 0.449s, learning 2.059s)
               Value function loss: 42727.4202
                    Surrogate loss: 0.0116
             Mean action noise std: 0.95
                       Mean reward: 1973.68
               Mean episode length: 237.49
                 Mean success rate: 24.00
                  Mean reward/step: 6.53
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 7626752
                    Iteration time: 2.51s
                        Total time: 2399.57s
                               ETA: 7912.6s

################################################################################
                     [1m Learning iteration 931/4000 [0m

                       Computation: 3235 steps/s (collection: 0.468s, learning 2.064s)
               Value function loss: 26645.1760
                    Surrogate loss: 0.0134
             Mean action noise std: 0.95
                       Mean reward: 1883.66
               Mean episode length: 239.60
                 Mean success rate: 23.00
                  Mean reward/step: 6.27
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 2.53s
                        Total time: 2402.10s
                               ETA: 7909.9s

################################################################################
                     [1m Learning iteration 932/4000 [0m

                       Computation: 3170 steps/s (collection: 0.474s, learning 2.110s)
               Value function loss: 11844.3147
                    Surrogate loss: 0.0152
             Mean action noise std: 0.95
                       Mean reward: 1583.17
               Mean episode length: 218.79
                 Mean success rate: 19.50
                  Mean reward/step: 6.48
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7643136
                    Iteration time: 2.58s
                        Total time: 2404.68s
                               ETA: 7907.4s

################################################################################
                     [1m Learning iteration 933/4000 [0m

                       Computation: 3153 steps/s (collection: 0.508s, learning 2.089s)
               Value function loss: 20434.6934
                    Surrogate loss: 0.0120
             Mean action noise std: 0.95
                       Mean reward: 1487.31
               Mean episode length: 210.29
                 Mean success rate: 19.50
                  Mean reward/step: 6.82
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 2.60s
                        Total time: 2407.28s
                               ETA: 7904.9s

################################################################################
                     [1m Learning iteration 934/4000 [0m

                       Computation: 3197 steps/s (collection: 0.504s, learning 2.058s)
               Value function loss: 13384.4341
                    Surrogate loss: 0.0219
             Mean action noise std: 0.95
                       Mean reward: 1159.06
               Mean episode length: 194.12
                 Mean success rate: 15.50
                  Mean reward/step: 7.13
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7659520
                    Iteration time: 2.56s
                        Total time: 2409.84s
                               ETA: 7902.2s

################################################################################
                     [1m Learning iteration 935/4000 [0m

                       Computation: 3192 steps/s (collection: 0.493s, learning 2.073s)
               Value function loss: 30940.5023
                    Surrogate loss: 0.0119
             Mean action noise std: 0.95
                       Mean reward: 1190.25
               Mean episode length: 181.77
                 Mean success rate: 15.50
                  Mean reward/step: 7.02
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 2.57s
                        Total time: 2412.41s
                               ETA: 7899.6s

################################################################################
                     [1m Learning iteration 936/4000 [0m

                       Computation: 3183 steps/s (collection: 0.508s, learning 2.065s)
               Value function loss: 14841.0704
                    Surrogate loss: 0.0148
             Mean action noise std: 0.95
                       Mean reward: 1154.85
               Mean episode length: 178.57
                 Mean success rate: 15.00
                  Mean reward/step: 6.74
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7675904
                    Iteration time: 2.57s
                        Total time: 2414.98s
                               ETA: 7897.0s

################################################################################
                     [1m Learning iteration 937/4000 [0m

                       Computation: 3253 steps/s (collection: 0.473s, learning 2.045s)
               Value function loss: 25150.9378
                    Surrogate loss: 0.0147
             Mean action noise std: 0.95
                       Mean reward: 1378.30
               Mean episode length: 190.59
                 Mean success rate: 18.00
                  Mean reward/step: 7.11
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 2.52s
                        Total time: 2417.50s
                               ETA: 7894.2s

################################################################################
                     [1m Learning iteration 938/4000 [0m

                       Computation: 3171 steps/s (collection: 0.499s, learning 2.084s)
               Value function loss: 22637.3660
                    Surrogate loss: 0.0129
             Mean action noise std: 0.95
                       Mean reward: 1292.32
               Mean episode length: 193.54
                 Mean success rate: 16.50
                  Mean reward/step: 7.72
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7692288
                    Iteration time: 2.58s
                        Total time: 2420.08s
                               ETA: 7891.7s

################################################################################
                     [1m Learning iteration 939/4000 [0m

                       Computation: 3223 steps/s (collection: 0.467s, learning 2.075s)
               Value function loss: 22420.1928
                    Surrogate loss: 0.0142
             Mean action noise std: 0.95
                       Mean reward: 1486.90
               Mean episode length: 209.91
                 Mean success rate: 18.50
                  Mean reward/step: 8.22
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 2.54s
                        Total time: 2422.63s
                               ETA: 7889.0s

################################################################################
                     [1m Learning iteration 940/4000 [0m

                       Computation: 3199 steps/s (collection: 0.492s, learning 2.069s)
               Value function loss: 28045.5009
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 1470.90
               Mean episode length: 202.60
                 Mean success rate: 18.00
                  Mean reward/step: 8.43
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7708672
                    Iteration time: 2.56s
                        Total time: 2425.19s
                               ETA: 7886.4s

################################################################################
                     [1m Learning iteration 941/4000 [0m

                       Computation: 3148 steps/s (collection: 0.505s, learning 2.096s)
               Value function loss: 23535.6836
                    Surrogate loss: 0.0127
             Mean action noise std: 0.95
                       Mean reward: 1292.71
               Mean episode length: 204.44
                 Mean success rate: 16.00
                  Mean reward/step: 8.46
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 2.60s
                        Total time: 2427.79s
                               ETA: 7883.9s

################################################################################
                     [1m Learning iteration 942/4000 [0m

                       Computation: 3118 steps/s (collection: 0.551s, learning 2.076s)
               Value function loss: 27199.8414
                    Surrogate loss: 0.0123
             Mean action noise std: 0.95
                       Mean reward: 1443.39
               Mean episode length: 215.38
                 Mean success rate: 17.50
                  Mean reward/step: 8.53
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7725056
                    Iteration time: 2.63s
                        Total time: 2430.41s
                               ETA: 7881.5s

################################################################################
                     [1m Learning iteration 943/4000 [0m

                       Computation: 3158 steps/s (collection: 0.497s, learning 2.097s)
               Value function loss: 36934.9128
                    Surrogate loss: 0.0116
             Mean action noise std: 0.95
                       Mean reward: 1511.02
               Mean episode length: 213.01
                 Mean success rate: 18.50
                  Mean reward/step: 8.98
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 2.59s
                        Total time: 2433.01s
                               ETA: 7878.9s

################################################################################
                     [1m Learning iteration 944/4000 [0m

                       Computation: 3134 steps/s (collection: 0.557s, learning 2.056s)
               Value function loss: 33676.0065
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 1755.61
               Mean episode length: 230.61
                 Mean success rate: 21.50
                  Mean reward/step: 9.07
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7741440
                    Iteration time: 2.61s
                        Total time: 2435.62s
                               ETA: 7876.5s

################################################################################
                     [1m Learning iteration 945/4000 [0m

                       Computation: 3147 steps/s (collection: 0.504s, learning 2.099s)
               Value function loss: 25039.7935
                    Surrogate loss: 0.0159
             Mean action noise std: 0.95
                       Mean reward: 1774.69
               Mean episode length: 232.04
                 Mean success rate: 21.50
                  Mean reward/step: 8.63
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 2.60s
                        Total time: 2438.22s
                               ETA: 7874.0s

################################################################################
                     [1m Learning iteration 946/4000 [0m

                       Computation: 3163 steps/s (collection: 0.489s, learning 2.101s)
               Value function loss: 23102.3775
                    Surrogate loss: 0.0174
             Mean action noise std: 0.95
                       Mean reward: 1784.26
               Mean episode length: 237.85
                 Mean success rate: 22.00
                  Mean reward/step: 8.68
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7757824
                    Iteration time: 2.59s
                        Total time: 2440.81s
                               ETA: 7871.4s

################################################################################
                     [1m Learning iteration 947/4000 [0m

                       Computation: 3142 steps/s (collection: 0.532s, learning 2.076s)
               Value function loss: 49585.6752
                    Surrogate loss: 0.0112
             Mean action noise std: 0.95
                       Mean reward: 1917.28
               Mean episode length: 239.76
                 Mean success rate: 24.00
                  Mean reward/step: 8.77
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 2.61s
                        Total time: 2443.42s
                               ETA: 7868.9s

################################################################################
                     [1m Learning iteration 948/4000 [0m

                       Computation: 3139 steps/s (collection: 0.520s, learning 2.090s)
               Value function loss: 37456.5978
                    Surrogate loss: 0.0097
             Mean action noise std: 0.95
                       Mean reward: 2134.01
               Mean episode length: 246.19
                 Mean success rate: 26.00
                  Mean reward/step: 8.29
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7774208
                    Iteration time: 2.61s
                        Total time: 2446.03s
                               ETA: 7866.5s

################################################################################
                     [1m Learning iteration 949/4000 [0m

                       Computation: 3192 steps/s (collection: 0.506s, learning 2.060s)
               Value function loss: 32239.2496
                    Surrogate loss: 0.0152
             Mean action noise std: 0.95
                       Mean reward: 2041.99
               Mean episode length: 243.93
                 Mean success rate: 26.00
                  Mean reward/step: 7.74
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 2.57s
                        Total time: 2448.60s
                               ETA: 7863.9s

################################################################################
                     [1m Learning iteration 950/4000 [0m

                       Computation: 3087 steps/s (collection: 0.545s, learning 2.108s)
               Value function loss: 29035.6789
                    Surrogate loss: 0.0132
             Mean action noise std: 0.95
                       Mean reward: 2042.36
               Mean episode length: 236.63
                 Mean success rate: 27.00
                  Mean reward/step: 7.74
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7790592
                    Iteration time: 2.65s
                        Total time: 2451.25s
                               ETA: 7861.5s

################################################################################
                     [1m Learning iteration 951/4000 [0m

                       Computation: 3198 steps/s (collection: 0.497s, learning 2.065s)
               Value function loss: 31431.8359
                    Surrogate loss: 0.0151
             Mean action noise std: 0.95
                       Mean reward: 2195.26
               Mean episode length: 240.50
                 Mean success rate: 29.00
                  Mean reward/step: 7.59
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 2.56s
                        Total time: 2453.81s
                               ETA: 7858.9s

################################################################################
                     [1m Learning iteration 952/4000 [0m

                       Computation: 3183 steps/s (collection: 0.490s, learning 2.084s)
               Value function loss: 25830.6658
                    Surrogate loss: 0.0147
             Mean action noise std: 0.95
                       Mean reward: 2140.73
               Mean episode length: 242.25
                 Mean success rate: 26.00
                  Mean reward/step: 7.54
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7806976
                    Iteration time: 2.57s
                        Total time: 2456.38s
                               ETA: 7856.3s

################################################################################
                     [1m Learning iteration 953/4000 [0m

                       Computation: 3156 steps/s (collection: 0.506s, learning 2.090s)
               Value function loss: 24395.9578
                    Surrogate loss: 0.0157
             Mean action noise std: 0.95
                       Mean reward: 2066.31
               Mean episode length: 240.99
                 Mean success rate: 26.50
                  Mean reward/step: 7.52
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 2.60s
                        Total time: 2458.98s
                               ETA: 7853.8s

################################################################################
                     [1m Learning iteration 954/4000 [0m

                       Computation: 3255 steps/s (collection: 0.463s, learning 2.053s)
               Value function loss: 16120.0182
                    Surrogate loss: 0.0148
             Mean action noise std: 0.95
                       Mean reward: 1773.87
               Mean episode length: 238.12
                 Mean success rate: 25.00
                  Mean reward/step: 7.92
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 7823360
                    Iteration time: 2.52s
                        Total time: 2461.50s
                               ETA: 7851.0s

################################################################################
                     [1m Learning iteration 955/4000 [0m

                       Computation: 3233 steps/s (collection: 0.461s, learning 2.072s)
               Value function loss: 28995.6338
                    Surrogate loss: 0.0148
             Mean action noise std: 0.95
                       Mean reward: 1818.33
               Mean episode length: 231.44
                 Mean success rate: 25.50
                  Mean reward/step: 7.71
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 2.53s
                        Total time: 2464.03s
                               ETA: 7848.3s

################################################################################
                     [1m Learning iteration 956/4000 [0m

                       Computation: 3250 steps/s (collection: 0.467s, learning 2.053s)
               Value function loss: 24063.5672
                    Surrogate loss: 0.0192
             Mean action noise std: 0.95
                       Mean reward: 1680.35
               Mean episode length: 215.40
                 Mean success rate: 24.00
                  Mean reward/step: 7.40
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 7839744
                    Iteration time: 2.52s
                        Total time: 2466.55s
                               ETA: 7845.5s

################################################################################
                     [1m Learning iteration 957/4000 [0m

                       Computation: 3243 steps/s (collection: 0.477s, learning 2.049s)
               Value function loss: 34995.8351
                    Surrogate loss: 0.0186
             Mean action noise std: 0.95
                       Mean reward: 1693.39
               Mean episode length: 210.87
                 Mean success rate: 24.50
                  Mean reward/step: 7.56
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 2.53s
                        Total time: 2469.07s
                               ETA: 7842.8s

################################################################################
                     [1m Learning iteration 958/4000 [0m

                       Computation: 3189 steps/s (collection: 0.483s, learning 2.085s)
               Value function loss: 37500.4432
                    Surrogate loss: 0.0130
             Mean action noise std: 0.95
                       Mean reward: 1539.27
               Mean episode length: 200.09
                 Mean success rate: 22.50
                  Mean reward/step: 7.70
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7856128
                    Iteration time: 2.57s
                        Total time: 2471.64s
                               ETA: 7840.2s

################################################################################
                     [1m Learning iteration 959/4000 [0m

                       Computation: 3261 steps/s (collection: 0.470s, learning 2.043s)
               Value function loss: 28859.2185
                    Surrogate loss: 0.0110
             Mean action noise std: 0.95
                       Mean reward: 1449.00
               Mean episode length: 191.47
                 Mean success rate: 19.00
                  Mean reward/step: 8.11
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 2.51s
                        Total time: 2474.15s
                               ETA: 7837.4s

################################################################################
                     [1m Learning iteration 960/4000 [0m

                       Computation: 3263 steps/s (collection: 0.468s, learning 2.043s)
               Value function loss: 19300.7021
                    Surrogate loss: 0.0156
             Mean action noise std: 0.95
                       Mean reward: 1471.29
               Mean episode length: 192.44
                 Mean success rate: 18.00
                  Mean reward/step: 8.34
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7872512
                    Iteration time: 2.51s
                        Total time: 2476.67s
                               ETA: 7834.6s

################################################################################
                     [1m Learning iteration 961/4000 [0m

                       Computation: 3234 steps/s (collection: 0.472s, learning 2.060s)
               Value function loss: 35214.7504
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 1584.41
               Mean episode length: 202.99
                 Mean success rate: 20.50
                  Mean reward/step: 8.48
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 2.53s
                        Total time: 2479.20s
                               ETA: 7831.9s

################################################################################
                     [1m Learning iteration 962/4000 [0m

                       Computation: 3180 steps/s (collection: 0.505s, learning 2.071s)
               Value function loss: 27726.0343
                    Surrogate loss: 0.0143
             Mean action noise std: 0.95
                       Mean reward: 1494.05
               Mean episode length: 199.40
                 Mean success rate: 19.50
                  Mean reward/step: 8.35
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7888896
                    Iteration time: 2.58s
                        Total time: 2481.77s
                               ETA: 7829.3s

################################################################################
                     [1m Learning iteration 963/4000 [0m

                       Computation: 3179 steps/s (collection: 0.508s, learning 2.068s)
               Value function loss: 24732.2195
                    Surrogate loss: 0.0126
             Mean action noise std: 0.95
                       Mean reward: 1508.71
               Mean episode length: 204.44
                 Mean success rate: 20.50
                  Mean reward/step: 8.37
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 2.58s
                        Total time: 2484.35s
                               ETA: 7826.7s

################################################################################
                     [1m Learning iteration 964/4000 [0m

                       Computation: 3205 steps/s (collection: 0.477s, learning 2.079s)
               Value function loss: 32463.2888
                    Surrogate loss: 0.0127
             Mean action noise std: 0.95
                       Mean reward: 1579.61
               Mean episode length: 206.25
                 Mean success rate: 22.50
                  Mean reward/step: 8.48
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7905280
                    Iteration time: 2.56s
                        Total time: 2486.91s
                               ETA: 7824.1s

################################################################################
                     [1m Learning iteration 965/4000 [0m

                       Computation: 3169 steps/s (collection: 0.490s, learning 2.095s)
               Value function loss: 24117.8617
                    Surrogate loss: 0.0108
             Mean action noise std: 0.95
                       Mean reward: 1684.71
               Mean episode length: 215.28
                 Mean success rate: 25.00
                  Mean reward/step: 8.48
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 2.58s
                        Total time: 2489.49s
                               ETA: 7821.5s

################################################################################
                     [1m Learning iteration 966/4000 [0m

                       Computation: 3168 steps/s (collection: 0.520s, learning 2.066s)
               Value function loss: 35364.9153
                    Surrogate loss: 0.0097
             Mean action noise std: 0.95
                       Mean reward: 1821.72
               Mean episode length: 212.04
                 Mean success rate: 26.00
                  Mean reward/step: 8.26
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7921664
                    Iteration time: 2.59s
                        Total time: 2492.08s
                               ETA: 7819.0s

################################################################################
                     [1m Learning iteration 967/4000 [0m

                       Computation: 3164 steps/s (collection: 0.524s, learning 2.064s)
               Value function loss: 30813.7460
                    Surrogate loss: 0.0106
             Mean action noise std: 0.95
                       Mean reward: 1915.45
               Mean episode length: 218.19
                 Mean success rate: 26.50
                  Mean reward/step: 7.88
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 2.59s
                        Total time: 2494.67s
                               ETA: 7816.4s

################################################################################
                     [1m Learning iteration 968/4000 [0m

                       Computation: 3213 steps/s (collection: 0.480s, learning 2.069s)
               Value function loss: 30564.2621
                    Surrogate loss: 0.0131
             Mean action noise std: 0.94
                       Mean reward: 1954.98
               Mean episode length: 224.71
                 Mean success rate: 27.50
                  Mean reward/step: 7.78
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 7938048
                    Iteration time: 2.55s
                        Total time: 2497.21s
                               ETA: 7813.8s

################################################################################
                     [1m Learning iteration 969/4000 [0m

                       Computation: 3203 steps/s (collection: 0.504s, learning 2.054s)
               Value function loss: 23110.4783
                    Surrogate loss: 0.0128
             Mean action noise std: 0.94
                       Mean reward: 1884.41
               Mean episode length: 215.99
                 Mean success rate: 26.00
                  Mean reward/step: 8.04
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 2.56s
                        Total time: 2499.77s
                               ETA: 7811.1s

################################################################################
                     [1m Learning iteration 970/4000 [0m

                       Computation: 3153 steps/s (collection: 0.524s, learning 2.074s)
               Value function loss: 31400.3489
                    Surrogate loss: 0.0110
             Mean action noise std: 0.95
                       Mean reward: 2030.37
               Mean episode length: 221.00
                 Mean success rate: 27.00
                  Mean reward/step: 8.06
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7954432
                    Iteration time: 2.60s
                        Total time: 2502.37s
                               ETA: 7808.6s

################################################################################
                     [1m Learning iteration 971/4000 [0m

                       Computation: 3193 steps/s (collection: 0.508s, learning 2.057s)
               Value function loss: 40392.8919
                    Surrogate loss: 0.0101
             Mean action noise std: 0.95
                       Mean reward: 2018.31
               Mean episode length: 229.50
                 Mean success rate: 26.00
                  Mean reward/step: 8.35
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 2.56s
                        Total time: 2504.93s
                               ETA: 7806.0s

################################################################################
                     [1m Learning iteration 972/4000 [0m

                       Computation: 3157 steps/s (collection: 0.512s, learning 2.082s)
               Value function loss: 26887.1223
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 1954.35
               Mean episode length: 224.65
                 Mean success rate: 25.50
                  Mean reward/step: 8.46
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7970816
                    Iteration time: 2.59s
                        Total time: 2507.53s
                               ETA: 7803.5s

################################################################################
                     [1m Learning iteration 973/4000 [0m

                       Computation: 3159 steps/s (collection: 0.501s, learning 2.092s)
               Value function loss: 20183.9359
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 1763.77
               Mean episode length: 216.65
                 Mean success rate: 22.50
                  Mean reward/step: 8.61
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 2.59s
                        Total time: 2510.12s
                               ETA: 7801.0s

################################################################################
                     [1m Learning iteration 974/4000 [0m

                       Computation: 3198 steps/s (collection: 0.516s, learning 2.045s)
               Value function loss: 40274.6944
                    Surrogate loss: 0.0141
             Mean action noise std: 0.95
                       Mean reward: 1996.98
               Mean episode length: 229.20
                 Mean success rate: 24.50
                  Mean reward/step: 9.25
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7987200
                    Iteration time: 2.56s
                        Total time: 2512.68s
                               ETA: 7798.3s

################################################################################
                     [1m Learning iteration 975/4000 [0m

                       Computation: 3190 steps/s (collection: 0.507s, learning 2.060s)
               Value function loss: 32320.8793
                    Surrogate loss: 0.0113
             Mean action noise std: 0.95
                       Mean reward: 1811.53
               Mean episode length: 230.02
                 Mean success rate: 21.00
                  Mean reward/step: 8.80
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 2.57s
                        Total time: 2515.25s
                               ETA: 7795.7s

################################################################################
                     [1m Learning iteration 976/4000 [0m

                       Computation: 3152 steps/s (collection: 0.523s, learning 2.075s)
               Value function loss: 28575.6970
                    Surrogate loss: 0.0123
             Mean action noise std: 0.95
                       Mean reward: 1849.86
               Mean episode length: 221.75
                 Mean success rate: 21.00
                  Mean reward/step: 8.57
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8003584
                    Iteration time: 2.60s
                        Total time: 2517.85s
                               ETA: 7793.2s

################################################################################
                     [1m Learning iteration 977/4000 [0m

                       Computation: 3211 steps/s (collection: 0.487s, learning 2.064s)
               Value function loss: 26390.0922
                    Surrogate loss: 0.0170
             Mean action noise std: 0.94
                       Mean reward: 1760.78
               Mean episode length: 218.44
                 Mean success rate: 21.00
                  Mean reward/step: 8.37
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 2.55s
                        Total time: 2520.40s
                               ETA: 7790.6s

################################################################################
                     [1m Learning iteration 978/4000 [0m

                       Computation: 3284 steps/s (collection: 0.437s, learning 2.057s)
               Value function loss: 33344.2397
                    Surrogate loss: 0.0124
             Mean action noise std: 0.94
                       Mean reward: 1876.43
               Mean episode length: 221.35
                 Mean success rate: 23.00
                  Mean reward/step: 7.92
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 8019968
                    Iteration time: 2.49s
                        Total time: 2522.89s
                               ETA: 7787.7s

################################################################################
                     [1m Learning iteration 979/4000 [0m

                       Computation: 3170 steps/s (collection: 0.477s, learning 2.107s)
               Value function loss: 36782.5242
                    Surrogate loss: 0.0131
             Mean action noise std: 0.94
                       Mean reward: 1769.13
               Mean episode length: 220.76
                 Mean success rate: 23.50
                  Mean reward/step: 7.65
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 2.58s
                        Total time: 2525.48s
                               ETA: 7785.2s

################################################################################
                     [1m Learning iteration 980/4000 [0m

                       Computation: 3195 steps/s (collection: 0.473s, learning 2.090s)
               Value function loss: 24841.0419
                    Surrogate loss: 0.0152
             Mean action noise std: 0.95
                       Mean reward: 1756.68
               Mean episode length: 219.01
                 Mean success rate: 24.00
                  Mean reward/step: 7.82
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 8036352
                    Iteration time: 2.56s
                        Total time: 2528.04s
                               ETA: 7782.6s

################################################################################
                     [1m Learning iteration 981/4000 [0m

                       Computation: 3163 steps/s (collection: 0.479s, learning 2.110s)
               Value function loss: 19019.0869
                    Surrogate loss: 0.0192
             Mean action noise std: 0.94
                       Mean reward: 1608.19
               Mean episode length: 218.62
                 Mean success rate: 22.50
                  Mean reward/step: 8.22
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 2.59s
                        Total time: 2530.63s
                               ETA: 7780.0s

################################################################################
                     [1m Learning iteration 982/4000 [0m

                       Computation: 3213 steps/s (collection: 0.459s, learning 2.090s)
               Value function loss: 31141.4720
                    Surrogate loss: 0.0126
             Mean action noise std: 0.94
                       Mean reward: 1624.17
               Mean episode length: 217.99
                 Mean success rate: 22.50
                  Mean reward/step: 8.36
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8052736
                    Iteration time: 2.55s
                        Total time: 2533.18s
                               ETA: 7777.4s

################################################################################
                     [1m Learning iteration 983/4000 [0m

                       Computation: 3243 steps/s (collection: 0.462s, learning 2.064s)
               Value function loss: 50958.6714
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 2004.63
               Mean episode length: 224.34
                 Mean success rate: 26.50
                  Mean reward/step: 7.95
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 2.53s
                        Total time: 2535.71s
                               ETA: 7774.6s

################################################################################
                     [1m Learning iteration 984/4000 [0m

                       Computation: 3177 steps/s (collection: 0.479s, learning 2.099s)
               Value function loss: 35146.4194
                    Surrogate loss: 0.0104
             Mean action noise std: 0.94
                       Mean reward: 1878.79
               Mean episode length: 226.96
                 Mean success rate: 26.00
                  Mean reward/step: 7.46
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 8069120
                    Iteration time: 2.58s
                        Total time: 2538.28s
                               ETA: 7772.0s

################################################################################
                     [1m Learning iteration 985/4000 [0m

                       Computation: 3163 steps/s (collection: 0.452s, learning 2.138s)
               Value function loss: 24180.5043
                    Surrogate loss: 0.0139
             Mean action noise std: 0.94
                       Mean reward: 1997.68
               Mean episode length: 225.44
                 Mean success rate: 26.50
                  Mean reward/step: 6.94
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 2.59s
                        Total time: 2540.87s
                               ETA: 7769.5s

################################################################################
                     [1m Learning iteration 986/4000 [0m

                       Computation: 3200 steps/s (collection: 0.449s, learning 2.111s)
               Value function loss: 25207.0885
                    Surrogate loss: 0.0137
             Mean action noise std: 0.94
                       Mean reward: 2064.88
               Mean episode length: 224.16
                 Mean success rate: 26.50
                  Mean reward/step: 7.09
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8085504
                    Iteration time: 2.56s
                        Total time: 2543.43s
                               ETA: 7766.9s

################################################################################
                     [1m Learning iteration 987/4000 [0m

                       Computation: 3217 steps/s (collection: 0.453s, learning 2.094s)
               Value function loss: 33398.7382
                    Surrogate loss: 0.0129
             Mean action noise std: 0.94
                       Mean reward: 2244.58
               Mean episode length: 234.71
                 Mean success rate: 29.00
                  Mean reward/step: 7.18
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 2.55s
                        Total time: 2545.98s
                               ETA: 7764.2s

################################################################################
                     [1m Learning iteration 988/4000 [0m

                       Computation: 3253 steps/s (collection: 0.445s, learning 2.073s)
               Value function loss: 26329.7245
                    Surrogate loss: 0.0109
             Mean action noise std: 0.94
                       Mean reward: 2329.60
               Mean episode length: 237.99
                 Mean success rate: 30.50
                  Mean reward/step: 7.20
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 8101888
                    Iteration time: 2.52s
                        Total time: 2548.50s
                               ETA: 7761.4s

################################################################################
                     [1m Learning iteration 989/4000 [0m

                       Computation: 3103 steps/s (collection: 0.539s, learning 2.100s)
               Value function loss: 21894.2256
                    Surrogate loss: 0.0110
             Mean action noise std: 0.94
                       Mean reward: 2198.02
               Mean episode length: 233.40
                 Mean success rate: 29.00
                  Mean reward/step: 7.22
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 2.64s
                        Total time: 2551.14s
                               ETA: 7759.1s

################################################################################
                     [1m Learning iteration 990/4000 [0m

                       Computation: 3206 steps/s (collection: 0.486s, learning 2.069s)
               Value function loss: 29866.7916
                    Surrogate loss: 0.0131
             Mean action noise std: 0.94
                       Mean reward: 1996.47
               Mean episode length: 241.46
                 Mean success rate: 26.00
                  Mean reward/step: 7.21
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 8118272
                    Iteration time: 2.55s
                        Total time: 2553.69s
                               ETA: 7756.4s

################################################################################
                     [1m Learning iteration 991/4000 [0m

                       Computation: 3097 steps/s (collection: 0.522s, learning 2.122s)
               Value function loss: 33056.1395
                    Surrogate loss: 0.0134
             Mean action noise std: 0.94
                       Mean reward: 1763.72
               Mean episode length: 243.04
                 Mean success rate: 23.00
                  Mean reward/step: 7.17
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 2.64s
                        Total time: 2556.34s
                               ETA: 7754.0s

################################################################################
                     [1m Learning iteration 992/4000 [0m

                       Computation: 3131 steps/s (collection: 0.527s, learning 2.089s)
               Value function loss: 23604.7193
                    Surrogate loss: 0.0139
             Mean action noise std: 0.94
                       Mean reward: 1783.48
               Mean episode length: 253.88
                 Mean success rate: 24.00
                  Mean reward/step: 7.48
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8134656
                    Iteration time: 2.62s
                        Total time: 2558.95s
                               ETA: 7751.6s

################################################################################
                     [1m Learning iteration 993/4000 [0m

                       Computation: 3120 steps/s (collection: 0.515s, learning 2.111s)
               Value function loss: 33778.4417
                    Surrogate loss: 0.0128
             Mean action noise std: 0.94
                       Mean reward: 1547.43
               Mean episode length: 241.21
                 Mean success rate: 22.50
                  Mean reward/step: 7.35
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 2.63s
                        Total time: 2561.58s
                               ETA: 7749.2s

################################################################################
                     [1m Learning iteration 994/4000 [0m

                       Computation: 3198 steps/s (collection: 0.497s, learning 2.064s)
               Value function loss: 38094.3552
                    Surrogate loss: 0.0145
             Mean action noise std: 0.94
                       Mean reward: 1639.41
               Mean episode length: 247.72
                 Mean success rate: 23.50
                  Mean reward/step: 7.82
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8151040
                    Iteration time: 2.56s
                        Total time: 2564.14s
                               ETA: 7746.5s

################################################################################
                     [1m Learning iteration 995/4000 [0m

                       Computation: 3172 steps/s (collection: 0.488s, learning 2.094s)
               Value function loss: 30992.9944
                    Surrogate loss: 0.0147
             Mean action noise std: 0.94
                       Mean reward: 1787.06
               Mean episode length: 249.41
                 Mean success rate: 26.00
                  Mean reward/step: 7.99
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 2.58s
                        Total time: 2566.72s
                               ETA: 7744.0s

################################################################################
                     [1m Learning iteration 996/4000 [0m

                       Computation: 3149 steps/s (collection: 0.517s, learning 2.084s)
               Value function loss: 31223.0343
                    Surrogate loss: 0.0150
             Mean action noise std: 0.94
                       Mean reward: 1769.53
               Mean episode length: 236.48
                 Mean success rate: 26.00
                  Mean reward/step: 8.09
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8167424
                    Iteration time: 2.60s
                        Total time: 2569.32s
                               ETA: 7741.5s

################################################################################
                     [1m Learning iteration 997/4000 [0m

                       Computation: 3172 steps/s (collection: 0.486s, learning 2.097s)
               Value function loss: 32657.7234
                    Surrogate loss: 0.0171
             Mean action noise std: 0.94
                       Mean reward: 1782.29
               Mean episode length: 233.31
                 Mean success rate: 26.00
                  Mean reward/step: 8.53
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 2.58s
                        Total time: 2571.90s
                               ETA: 7738.9s

################################################################################
                     [1m Learning iteration 998/4000 [0m

                       Computation: 3157 steps/s (collection: 0.514s, learning 2.080s)
               Value function loss: 21381.6675
                    Surrogate loss: 0.0211
             Mean action noise std: 0.94
                       Mean reward: 1810.87
               Mean episode length: 235.18
                 Mean success rate: 26.00
                  Mean reward/step: 8.91
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8183808
                    Iteration time: 2.59s
                        Total time: 2574.50s
                               ETA: 7736.4s

################################################################################
                     [1m Learning iteration 999/4000 [0m

                       Computation: 3148 steps/s (collection: 0.503s, learning 2.099s)
               Value function loss: 36253.8975
                    Surrogate loss: 0.0166
             Mean action noise std: 0.94
                       Mean reward: 1983.52
               Mean episode length: 252.28
                 Mean success rate: 28.50
                  Mean reward/step: 9.33
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 2.60s
                        Total time: 2577.10s
                               ETA: 7733.9s

################################################################################
                     [1m Learning iteration 1000/4000 [0m

                       Computation: 3126 steps/s (collection: 0.510s, learning 2.110s)
               Value function loss: 43323.5915
                    Surrogate loss: 0.0127
             Mean action noise std: 0.94
                       Mean reward: 2151.90
               Mean episode length: 259.56
                 Mean success rate: 29.50
                  Mean reward/step: 8.74
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8200192
                    Iteration time: 2.62s
                        Total time: 2579.72s
                               ETA: 7731.4s

################################################################################
                     [1m Learning iteration 1001/4000 [0m

                       Computation: 3197 steps/s (collection: 0.462s, learning 2.100s)
               Value function loss: 38130.3307
                    Surrogate loss: 0.0127
             Mean action noise std: 0.94
                       Mean reward: 1958.81
               Mean episode length: 252.66
                 Mean success rate: 28.00
                  Mean reward/step: 8.54
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 2.56s
                        Total time: 2582.28s
                               ETA: 7728.8s

################################################################################
                     [1m Learning iteration 1002/4000 [0m

                       Computation: 3155 steps/s (collection: 0.510s, learning 2.086s)
               Value function loss: 26262.2370
                    Surrogate loss: 0.0137
             Mean action noise std: 0.94
                       Mean reward: 2026.40
               Mean episode length: 250.37
                 Mean success rate: 29.50
                  Mean reward/step: 8.41
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 8216576
                    Iteration time: 2.60s
                        Total time: 2584.88s
                               ETA: 7726.3s

################################################################################
                     [1m Learning iteration 1003/4000 [0m

                       Computation: 3143 steps/s (collection: 0.534s, learning 2.072s)
               Value function loss: 35451.3444
                    Surrogate loss: 0.0157
             Mean action noise std: 0.94
                       Mean reward: 1933.62
               Mean episode length: 252.64
                 Mean success rate: 28.50
                  Mean reward/step: 8.48
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 2.61s
                        Total time: 2587.48s
                               ETA: 7723.8s

################################################################################
                     [1m Learning iteration 1004/4000 [0m

                       Computation: 3186 steps/s (collection: 0.510s, learning 2.060s)
               Value function loss: 22777.1203
                    Surrogate loss: 0.0147
             Mean action noise std: 0.94
                       Mean reward: 2091.14
               Mean episode length: 254.69
                 Mean success rate: 31.50
                  Mean reward/step: 8.57
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8232960
                    Iteration time: 2.57s
                        Total time: 2590.05s
                               ETA: 7721.2s

################################################################################
                     [1m Learning iteration 1005/4000 [0m

                       Computation: 3264 steps/s (collection: 0.458s, learning 2.051s)
               Value function loss: 26173.3249
                    Surrogate loss: 0.0151
             Mean action noise std: 0.94
                       Mean reward: 2142.46
               Mean episode length: 255.60
                 Mean success rate: 32.00
                  Mean reward/step: 8.67
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 2.51s
                        Total time: 2592.56s
                               ETA: 7718.4s

################################################################################
                     [1m Learning iteration 1006/4000 [0m

                       Computation: 3197 steps/s (collection: 0.492s, learning 2.069s)
               Value function loss: 37338.8497
                    Surrogate loss: 0.0191
             Mean action noise std: 0.94
                       Mean reward: 2121.25
               Mean episode length: 258.31
                 Mean success rate: 31.50
                  Mean reward/step: 8.07
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8249344
                    Iteration time: 2.56s
                        Total time: 2595.13s
                               ETA: 7715.8s

################################################################################
                     [1m Learning iteration 1007/4000 [0m

                       Computation: 3146 steps/s (collection: 0.532s, learning 2.071s)
               Value function loss: 35979.5221
                    Surrogate loss: 0.0127
             Mean action noise std: 0.94
                       Mean reward: 2212.27
               Mean episode length: 256.19
                 Mean success rate: 33.00
                  Mean reward/step: 7.67
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 2.60s
                        Total time: 2597.73s
                               ETA: 7713.3s

################################################################################
                     [1m Learning iteration 1008/4000 [0m

                       Computation: 3155 steps/s (collection: 0.506s, learning 2.090s)
               Value function loss: 30695.4047
                    Surrogate loss: 0.0114
             Mean action noise std: 0.94
                       Mean reward: 2535.63
               Mean episode length: 266.38
                 Mean success rate: 37.00
                  Mean reward/step: 8.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8265728
                    Iteration time: 2.60s
                        Total time: 2600.33s
                               ETA: 7710.8s

################################################################################
                     [1m Learning iteration 1009/4000 [0m

                       Computation: 3127 steps/s (collection: 0.511s, learning 2.108s)
               Value function loss: 43410.7653
                    Surrogate loss: 0.0135
             Mean action noise std: 0.94
                       Mean reward: 2748.94
               Mean episode length: 280.91
                 Mean success rate: 39.50
                  Mean reward/step: 8.12
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 2.62s
                        Total time: 2602.94s
                               ETA: 7708.3s

################################################################################
                     [1m Learning iteration 1010/4000 [0m

                       Computation: 3192 steps/s (collection: 0.509s, learning 2.056s)
               Value function loss: 44281.2812
                    Surrogate loss: 0.0147
             Mean action noise std: 0.94
                       Mean reward: 2689.83
               Mean episode length: 286.38
                 Mean success rate: 40.00
                  Mean reward/step: 8.30
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 8282112
                    Iteration time: 2.57s
                        Total time: 2605.51s
                               ETA: 7705.7s

################################################################################
                     [1m Learning iteration 1011/4000 [0m

                       Computation: 3176 steps/s (collection: 0.502s, learning 2.077s)
               Value function loss: 32755.2491
                    Surrogate loss: 0.0142
             Mean action noise std: 0.94
                       Mean reward: 2505.15
               Mean episode length: 291.23
                 Mean success rate: 36.00
                  Mean reward/step: 8.15
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 2.58s
                        Total time: 2608.09s
                               ETA: 7703.1s

################################################################################
                     [1m Learning iteration 1012/4000 [0m

                       Computation: 3219 steps/s (collection: 0.500s, learning 2.044s)
               Value function loss: 26782.1313
                    Surrogate loss: 0.0174
             Mean action noise std: 0.94
                       Mean reward: 2501.84
               Mean episode length: 292.64
                 Mean success rate: 36.50
                  Mean reward/step: 8.55
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8298496
                    Iteration time: 2.54s
                        Total time: 2610.63s
                               ETA: 7700.5s

################################################################################
                     [1m Learning iteration 1013/4000 [0m

                       Computation: 3184 steps/s (collection: 0.497s, learning 2.076s)
               Value function loss: 30695.6571
                    Surrogate loss: 0.0120
             Mean action noise std: 0.94
                       Mean reward: 2228.89
               Mean episode length: 278.17
                 Mean success rate: 32.50
                  Mean reward/step: 8.53
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 2.57s
                        Total time: 2613.21s
                               ETA: 7697.9s

################################################################################
                     [1m Learning iteration 1014/4000 [0m

                       Computation: 3131 steps/s (collection: 0.505s, learning 2.111s)
               Value function loss: 31342.5217
                    Surrogate loss: 0.0117
             Mean action noise std: 0.94
                       Mean reward: 1741.99
               Mean episode length: 255.53
                 Mean success rate: 25.00
                  Mean reward/step: 8.89
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 8314880
                    Iteration time: 2.62s
                        Total time: 2615.82s
                               ETA: 7695.4s

################################################################################
                     [1m Learning iteration 1015/4000 [0m

                       Computation: 3116 steps/s (collection: 0.517s, learning 2.112s)
               Value function loss: 40481.1444
                    Surrogate loss: 0.0123
             Mean action noise std: 0.94
                       Mean reward: 1667.47
               Mean episode length: 257.56
                 Mean success rate: 24.50
                  Mean reward/step: 8.88
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 2.63s
                        Total time: 2618.45s
                               ETA: 7693.0s

################################################################################
                     [1m Learning iteration 1016/4000 [0m

                       Computation: 3172 steps/s (collection: 0.501s, learning 2.081s)
               Value function loss: 37911.1630
                    Surrogate loss: 0.0129
             Mean action noise std: 0.94
                       Mean reward: 1907.68
               Mean episode length: 258.88
                 Mean success rate: 26.50
                  Mean reward/step: 9.05
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8331264
                    Iteration time: 2.58s
                        Total time: 2621.03s
                               ETA: 7690.4s

################################################################################
                     [1m Learning iteration 1017/4000 [0m

                       Computation: 3130 steps/s (collection: 0.542s, learning 2.075s)
               Value function loss: 34994.6361
                    Surrogate loss: 0.0143
             Mean action noise std: 0.94
                       Mean reward: 1820.70
               Mean episode length: 242.66
                 Mean success rate: 24.00
                  Mean reward/step: 8.93
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 2.62s
                        Total time: 2623.65s
                               ETA: 7688.0s

################################################################################
                     [1m Learning iteration 1018/4000 [0m

                       Computation: 3182 steps/s (collection: 0.451s, learning 2.123s)
               Value function loss: 41648.3292
                    Surrogate loss: 0.0124
             Mean action noise std: 0.94
                       Mean reward: 2020.11
               Mean episode length: 245.36
                 Mean success rate: 24.50
                  Mean reward/step: 9.41
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8347648
                    Iteration time: 2.57s
                        Total time: 2626.22s
                               ETA: 7685.4s

################################################################################
                     [1m Learning iteration 1019/4000 [0m

                       Computation: 3109 steps/s (collection: 0.534s, learning 2.100s)
               Value function loss: 40811.1962
                    Surrogate loss: 0.0142
             Mean action noise std: 0.94
                       Mean reward: 2332.83
               Mean episode length: 259.70
                 Mean success rate: 28.50
                  Mean reward/step: 9.60
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 2.63s
                        Total time: 2628.86s
                               ETA: 7683.0s

################################################################################
                     [1m Learning iteration 1020/4000 [0m

                       Computation: 3100 steps/s (collection: 0.552s, learning 2.090s)
               Value function loss: 37956.4839
                    Surrogate loss: 0.0121
             Mean action noise std: 0.94
                       Mean reward: 2457.54
               Mean episode length: 260.50
                 Mean success rate: 30.50
                  Mean reward/step: 9.52
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8364032
                    Iteration time: 2.64s
                        Total time: 2631.50s
                               ETA: 7680.6s

################################################################################
                     [1m Learning iteration 1021/4000 [0m

                       Computation: 3202 steps/s (collection: 0.479s, learning 2.079s)
               Value function loss: 36695.4867
                    Surrogate loss: 0.0137
             Mean action noise std: 0.94
                       Mean reward: 2209.11
               Mean episode length: 243.57
                 Mean success rate: 27.50
                  Mean reward/step: 10.33
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 2.56s
                        Total time: 2634.06s
                               ETA: 7677.9s

################################################################################
                     [1m Learning iteration 1022/4000 [0m

                       Computation: 3140 steps/s (collection: 0.514s, learning 2.095s)
               Value function loss: 39255.3000
                    Surrogate loss: 0.0127
             Mean action noise std: 0.94
                       Mean reward: 2168.73
               Mean episode length: 239.99
                 Mean success rate: 28.00
                  Mean reward/step: 10.38
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 8380416
                    Iteration time: 2.61s
                        Total time: 2636.67s
                               ETA: 7675.5s

################################################################################
                     [1m Learning iteration 1023/4000 [0m

                       Computation: 3128 steps/s (collection: 0.533s, learning 2.085s)
               Value function loss: 25632.8057
                    Surrogate loss: 0.0173
             Mean action noise std: 0.94
                       Mean reward: 2285.60
               Mean episode length: 240.26
                 Mean success rate: 29.50
                  Mean reward/step: 10.63
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 2.62s
                        Total time: 2639.29s
                               ETA: 7673.0s

################################################################################
                     [1m Learning iteration 1024/4000 [0m

                       Computation: 3103 steps/s (collection: 0.534s, learning 2.105s)
               Value function loss: 58525.4714
                    Surrogate loss: 0.0125
             Mean action noise std: 0.94
                       Mean reward: 2166.00
               Mean episode length: 233.49
                 Mean success rate: 29.50
                  Mean reward/step: 11.05
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 8396800
                    Iteration time: 2.64s
                        Total time: 2641.93s
                               ETA: 7670.6s

################################################################################
                     [1m Learning iteration 1025/4000 [0m

                       Computation: 3141 steps/s (collection: 0.511s, learning 2.097s)
               Value function loss: 29184.9342
                    Surrogate loss: 0.0172
             Mean action noise std: 0.94
                       Mean reward: 1856.79
               Mean episode length: 219.93
                 Mean success rate: 26.00
                  Mean reward/step: 11.02
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 2.61s
                        Total time: 2644.53s
                               ETA: 7668.1s

################################################################################
                     [1m Learning iteration 1026/4000 [0m

                       Computation: 3137 steps/s (collection: 0.507s, learning 2.104s)
               Value function loss: 47104.3334
                    Surrogate loss: 0.0133
             Mean action noise std: 0.94
                       Mean reward: 2110.87
               Mean episode length: 236.38
                 Mean success rate: 28.50
                  Mean reward/step: 10.91
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8413184
                    Iteration time: 2.61s
                        Total time: 2647.14s
                               ETA: 7665.6s

################################################################################
                     [1m Learning iteration 1027/4000 [0m

                       Computation: 3092 steps/s (collection: 0.548s, learning 2.101s)
               Value function loss: 45670.5965
                    Surrogate loss: 0.0161
             Mean action noise std: 0.94
                       Mean reward: 2320.44
               Mean episode length: 247.25
                 Mean success rate: 31.00
                  Mean reward/step: 10.53
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 2.65s
                        Total time: 2649.79s
                               ETA: 7663.3s

################################################################################
                     [1m Learning iteration 1028/4000 [0m

                       Computation: 3153 steps/s (collection: 0.526s, learning 2.072s)
               Value function loss: 44366.2691
                    Surrogate loss: 0.0123
             Mean action noise std: 0.94
                       Mean reward: 2181.69
               Mean episode length: 237.09
                 Mean success rate: 28.50
                  Mean reward/step: 11.20
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 8429568
                    Iteration time: 2.60s
                        Total time: 2652.39s
                               ETA: 7660.7s

################################################################################
                     [1m Learning iteration 1029/4000 [0m

                       Computation: 3203 steps/s (collection: 0.488s, learning 2.070s)
               Value function loss: 46557.6069
                    Surrogate loss: 0.0112
             Mean action noise std: 0.94
                       Mean reward: 2451.53
               Mean episode length: 240.37
                 Mean success rate: 30.00
                  Mean reward/step: 11.30
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 2.56s
                        Total time: 2654.95s
                               ETA: 7658.1s

################################################################################
                     [1m Learning iteration 1030/4000 [0m

                       Computation: 3181 steps/s (collection: 0.519s, learning 2.056s)
               Value function loss: 28515.8886
                    Surrogate loss: 0.0169
             Mean action noise std: 0.94
                       Mean reward: 2278.72
               Mean episode length: 229.12
                 Mean success rate: 28.00
                  Mean reward/step: 11.29
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8445952
                    Iteration time: 2.58s
                        Total time: 2657.52s
                               ETA: 7655.5s

################################################################################
                     [1m Learning iteration 1031/4000 [0m

                       Computation: 3167 steps/s (collection: 0.509s, learning 2.077s)
               Value function loss: 57582.7304
                    Surrogate loss: 0.0115
             Mean action noise std: 0.94
                       Mean reward: 2648.91
               Mean episode length: 250.25
                 Mean success rate: 32.50
                  Mean reward/step: 11.35
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 2.59s
                        Total time: 2660.11s
                               ETA: 7653.0s

################################################################################
                     [1m Learning iteration 1032/4000 [0m

                       Computation: 3085 steps/s (collection: 0.520s, learning 2.135s)
               Value function loss: 34204.2409
                    Surrogate loss: 0.0125
             Mean action noise std: 0.94
                       Mean reward: 2811.32
               Mean episode length: 255.61
                 Mean success rate: 34.50
                  Mean reward/step: 10.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8462336
                    Iteration time: 2.66s
                        Total time: 2662.76s
                               ETA: 7650.6s

################################################################################
                     [1m Learning iteration 1033/4000 [0m

                       Computation: 3039 steps/s (collection: 0.558s, learning 2.137s)
               Value function loss: 48957.2957
                    Surrogate loss: 0.0133
             Mean action noise std: 0.94
                       Mean reward: 2619.27
               Mean episode length: 235.61
                 Mean success rate: 31.00
                  Mean reward/step: 11.45
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 2.70s
                        Total time: 2665.46s
                               ETA: 7648.4s

################################################################################
                     [1m Learning iteration 1034/4000 [0m

                       Computation: 3028 steps/s (collection: 0.583s, learning 2.122s)
               Value function loss: 46351.5804
                    Surrogate loss: 0.0120
             Mean action noise std: 0.94
                       Mean reward: 2833.87
               Mean episode length: 246.35
                 Mean success rate: 32.50
                  Mean reward/step: 10.85
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 8478720
                    Iteration time: 2.70s
                        Total time: 2668.16s
                               ETA: 7646.2s

################################################################################
                     [1m Learning iteration 1035/4000 [0m

                       Computation: 3068 steps/s (collection: 0.509s, learning 2.161s)
               Value function loss: 34433.5667
                    Surrogate loss: 0.0149
             Mean action noise std: 0.94
                       Mean reward: 2699.30
               Mean episode length: 249.31
                 Mean success rate: 33.00
                  Mean reward/step: 10.83
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 2.67s
                        Total time: 2670.83s
                               ETA: 7643.8s

################################################################################
                     [1m Learning iteration 1036/4000 [0m

                       Computation: 3116 steps/s (collection: 0.508s, learning 2.121s)
               Value function loss: 50504.7854
                    Surrogate loss: 0.0138
             Mean action noise std: 0.94
                       Mean reward: 2799.81
               Mean episode length: 253.18
                 Mean success rate: 33.00
                  Mean reward/step: 11.64
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8495104
                    Iteration time: 2.63s
                        Total time: 2673.46s
                               ETA: 7641.4s

################################################################################
                     [1m Learning iteration 1037/4000 [0m

                       Computation: 3137 steps/s (collection: 0.491s, learning 2.120s)
               Value function loss: 46369.8156
                    Surrogate loss: 0.0131
             Mean action noise std: 0.94
                       Mean reward: 2598.73
               Mean episode length: 236.68
                 Mean success rate: 31.00
                  Mean reward/step: 11.86
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 2.61s
                        Total time: 2676.07s
                               ETA: 7638.9s

################################################################################
                     [1m Learning iteration 1038/4000 [0m

                       Computation: 3114 steps/s (collection: 0.531s, learning 2.099s)
               Value function loss: 37008.1363
                    Surrogate loss: 0.0140
             Mean action noise std: 0.94
                       Mean reward: 2374.60
               Mean episode length: 223.00
                 Mean success rate: 28.50
                  Mean reward/step: 11.84
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 8511488
                    Iteration time: 2.63s
                        Total time: 2678.70s
                               ETA: 7636.5s

################################################################################
                     [1m Learning iteration 1039/4000 [0m

                       Computation: 3120 steps/s (collection: 0.490s, learning 2.135s)
               Value function loss: 46626.1211
                    Surrogate loss: 0.0122
             Mean action noise std: 0.94
                       Mean reward: 2442.35
               Mean episode length: 230.78
                 Mean success rate: 29.50
                  Mean reward/step: 11.98
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 2.63s
                        Total time: 2681.33s
                               ETA: 7634.1s

################################################################################
                     [1m Learning iteration 1040/4000 [0m

                       Computation: 3175 steps/s (collection: 0.482s, learning 2.097s)
               Value function loss: 60153.0653
                    Surrogate loss: 0.0157
             Mean action noise std: 0.94
                       Mean reward: 2495.41
               Mean episode length: 238.25
                 Mean success rate: 31.00
                  Mean reward/step: 11.87
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 8527872
                    Iteration time: 2.58s
                        Total time: 2683.91s
                               ETA: 7631.5s

################################################################################
                     [1m Learning iteration 1041/4000 [0m

                       Computation: 3083 steps/s (collection: 0.511s, learning 2.146s)
               Value function loss: 33228.8330
                    Surrogate loss: 0.0125
             Mean action noise std: 0.94
                       Mean reward: 2435.12
               Mean episode length: 230.43
                 Mean success rate: 28.50
                  Mean reward/step: 11.74
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 2.66s
                        Total time: 2686.57s
                               ETA: 7629.1s

################################################################################
                     [1m Learning iteration 1042/4000 [0m

                       Computation: 3082 steps/s (collection: 0.520s, learning 2.138s)
               Value function loss: 63112.5550
                    Surrogate loss: 0.0112
             Mean action noise std: 0.94
                       Mean reward: 2775.67
               Mean episode length: 240.41
                 Mean success rate: 31.00
                  Mean reward/step: 11.87
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8544256
                    Iteration time: 2.66s
                        Total time: 2689.22s
                               ETA: 7626.8s

################################################################################
                     [1m Learning iteration 1043/4000 [0m

                       Computation: 3052 steps/s (collection: 0.544s, learning 2.140s)
               Value function loss: 47995.3358
                    Surrogate loss: 0.0129
             Mean action noise std: 0.94
                       Mean reward: 2843.19
               Mean episode length: 244.41
                 Mean success rate: 33.00
                  Mean reward/step: 11.56
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 2.68s
                        Total time: 2691.91s
                               ETA: 7624.5s

################################################################################
                     [1m Learning iteration 1044/4000 [0m

                       Computation: 3130 steps/s (collection: 0.486s, learning 2.131s)
               Value function loss: 43100.0038
                    Surrogate loss: 0.0122
             Mean action noise std: 0.94
                       Mean reward: 3001.37
               Mean episode length: 250.75
                 Mean success rate: 35.50
                  Mean reward/step: 11.87
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8560640
                    Iteration time: 2.62s
                        Total time: 2694.52s
                               ETA: 7622.0s

################################################################################
                     [1m Learning iteration 1045/4000 [0m

                       Computation: 3091 steps/s (collection: 0.518s, learning 2.131s)
               Value function loss: 37291.8092
                    Surrogate loss: 0.0154
             Mean action noise std: 0.94
                       Mean reward: 2704.40
               Mean episode length: 235.05
                 Mean success rate: 32.00
                  Mean reward/step: 12.18
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 2.65s
                        Total time: 2697.17s
                               ETA: 7619.6s

################################################################################
                     [1m Learning iteration 1046/4000 [0m

                       Computation: 3042 steps/s (collection: 0.557s, learning 2.135s)
               Value function loss: 54022.7590
                    Surrogate loss: 0.0120
             Mean action noise std: 0.94
                       Mean reward: 2702.96
               Mean episode length: 227.35
                 Mean success rate: 33.50
                  Mean reward/step: 11.64
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 8577024
                    Iteration time: 2.69s
                        Total time: 2699.87s
                               ETA: 7617.4s

################################################################################
                     [1m Learning iteration 1047/4000 [0m

                       Computation: 3290 steps/s (collection: 0.462s, learning 2.028s)
               Value function loss: 28080.3418
                    Surrogate loss: 0.0147
             Mean action noise std: 0.94
                       Mean reward: 2348.46
               Mean episode length: 207.11
                 Mean success rate: 31.50
                  Mean reward/step: 11.38
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 2.49s
                        Total time: 2702.36s
                               ETA: 7614.6s

################################################################################
                     [1m Learning iteration 1048/4000 [0m

                       Computation: 3297 steps/s (collection: 0.429s, learning 2.055s)
               Value function loss: 41816.6710
                    Surrogate loss: 0.0132
             Mean action noise std: 0.94
                       Mean reward: 2276.38
               Mean episode length: 203.97
                 Mean success rate: 30.50
                  Mean reward/step: 11.70
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 8593408
                    Iteration time: 2.48s
                        Total time: 2704.84s
                               ETA: 7611.7s

################################################################################
                     [1m Learning iteration 1049/4000 [0m

                       Computation: 3152 steps/s (collection: 0.493s, learning 2.106s)
               Value function loss: 71931.5772
                    Surrogate loss: 0.0097
             Mean action noise std: 0.94
                       Mean reward: 2417.38
               Mean episode length: 204.10
                 Mean success rate: 30.50
                  Mean reward/step: 11.01
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 2.60s
                        Total time: 2707.44s
                               ETA: 7609.2s

################################################################################
                     [1m Learning iteration 1050/4000 [0m

                       Computation: 3115 steps/s (collection: 0.551s, learning 2.079s)
               Value function loss: 52780.9365
                    Surrogate loss: 0.0098
             Mean action noise std: 0.94
                       Mean reward: 2624.53
               Mean episode length: 208.90
                 Mean success rate: 33.50
                  Mean reward/step: 10.01
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8609792
                    Iteration time: 2.63s
                        Total time: 2710.07s
                               ETA: 7606.8s

################################################################################
                     [1m Learning iteration 1051/4000 [0m

                       Computation: 3165 steps/s (collection: 0.486s, learning 2.101s)
               Value function loss: 35513.9820
                    Surrogate loss: 0.0148
             Mean action noise std: 0.94
                       Mean reward: 2221.39
               Mean episode length: 200.81
                 Mean success rate: 28.00
                  Mean reward/step: 9.79
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 2.59s
                        Total time: 2712.66s
                               ETA: 7604.2s

################################################################################
                     [1m Learning iteration 1052/4000 [0m

                       Computation: 3159 steps/s (collection: 0.480s, learning 2.113s)
               Value function loss: 44074.0865
                    Surrogate loss: 0.0133
             Mean action noise std: 0.94
                       Mean reward: 2357.12
               Mean episode length: 208.94
                 Mean success rate: 28.00
                  Mean reward/step: 10.48
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8626176
                    Iteration time: 2.59s
                        Total time: 2715.25s
                               ETA: 7601.7s

################################################################################
                     [1m Learning iteration 1053/4000 [0m

                       Computation: 3086 steps/s (collection: 0.532s, learning 2.123s)
               Value function loss: 42441.1151
                    Surrogate loss: 0.0147
             Mean action noise std: 0.94
                       Mean reward: 2262.97
               Mean episode length: 206.78
                 Mean success rate: 26.00
                  Mean reward/step: 10.71
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 2.65s
                        Total time: 2717.90s
                               ETA: 7599.3s

################################################################################
                     [1m Learning iteration 1054/4000 [0m

                       Computation: 3139 steps/s (collection: 0.506s, learning 2.103s)
               Value function loss: 27324.2465
                    Surrogate loss: 0.0173
             Mean action noise std: 0.94
                       Mean reward: 1975.37
               Mean episode length: 196.44
                 Mean success rate: 24.00
                  Mean reward/step: 11.04
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 8642560
                    Iteration time: 2.61s
                        Total time: 2720.51s
                               ETA: 7596.8s

################################################################################
                     [1m Learning iteration 1055/4000 [0m

                       Computation: 3242 steps/s (collection: 0.488s, learning 2.038s)
               Value function loss: 55635.3575
                    Surrogate loss: 0.0104
             Mean action noise std: 0.94
                       Mean reward: 2095.56
               Mean episode length: 204.00
                 Mean success rate: 24.50
                  Mean reward/step: 11.33
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 2.53s
                        Total time: 2723.04s
                               ETA: 7594.1s

################################################################################
                     [1m Learning iteration 1056/4000 [0m

                       Computation: 3149 steps/s (collection: 0.513s, learning 2.088s)
               Value function loss: 42836.9518
                    Surrogate loss: 0.0134
             Mean action noise std: 0.94
                       Mean reward: 2363.43
               Mean episode length: 213.59
                 Mean success rate: 28.00
                  Mean reward/step: 11.75
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8658944
                    Iteration time: 2.60s
                        Total time: 2725.64s
                               ETA: 7591.6s

################################################################################
                     [1m Learning iteration 1057/4000 [0m

                       Computation: 3182 steps/s (collection: 0.512s, learning 2.062s)
               Value function loss: 28774.2457
                    Surrogate loss: 0.0138
             Mean action noise std: 0.94
                       Mean reward: 2410.14
               Mean episode length: 214.25
                 Mean success rate: 28.50
                  Mean reward/step: 11.54
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 2.57s
                        Total time: 2728.21s
                               ETA: 7589.0s

################################################################################
                     [1m Learning iteration 1058/4000 [0m

                       Computation: 2996 steps/s (collection: 0.562s, learning 2.172s)
               Value function loss: 59295.4705
                    Surrogate loss: 0.0101
             Mean action noise std: 0.94
                       Mean reward: 2515.44
               Mean episode length: 220.91
                 Mean success rate: 31.00
                  Mean reward/step: 11.44
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8675328
                    Iteration time: 2.73s
                        Total time: 2730.95s
                               ETA: 7586.8s

################################################################################
                     [1m Learning iteration 1059/4000 [0m

                       Computation: 2755 steps/s (collection: 0.536s, learning 2.437s)
               Value function loss: 54836.5992
                    Surrogate loss: 0.0122
             Mean action noise std: 0.94
                       Mean reward: 2923.66
               Mean episode length: 238.65
                 Mean success rate: 35.50
                  Mean reward/step: 11.14
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 2.97s
                        Total time: 2733.92s
                               ETA: 7585.3s

################################################################################
                     [1m Learning iteration 1060/4000 [0m

                       Computation: 3074 steps/s (collection: 0.541s, learning 2.123s)
               Value function loss: 46639.9078
                    Surrogate loss: 0.0131
             Mean action noise std: 0.94
                       Mean reward: 3063.29
               Mean episode length: 243.34
                 Mean success rate: 38.50
                  Mean reward/step: 10.62
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8691712
                    Iteration time: 2.66s
                        Total time: 2736.59s
                               ETA: 7583.0s

################################################################################
                     [1m Learning iteration 1061/4000 [0m

                       Computation: 2991 steps/s (collection: 0.532s, learning 2.206s)
               Value function loss: 35395.9232
                    Surrogate loss: 0.0151
             Mean action noise std: 0.94
                       Mean reward: 2993.21
               Mean episode length: 247.37
                 Mean success rate: 40.00
                  Mean reward/step: 11.05
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 2.74s
                        Total time: 2739.32s
                               ETA: 7580.9s

################################################################################
                     [1m Learning iteration 1062/4000 [0m

                       Computation: 2723 steps/s (collection: 0.714s, learning 2.294s)
               Value function loss: 28285.9698
                    Surrogate loss: 0.0139
             Mean action noise std: 0.94
                       Mean reward: 2574.13
               Mean episode length: 232.83
                 Mean success rate: 35.00
                  Mean reward/step: 11.15
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 8708096
                    Iteration time: 3.01s
                        Total time: 2742.33s
                               ETA: 7579.5s

################################################################################
                     [1m Learning iteration 1063/4000 [0m

                       Computation: 2839 steps/s (collection: 0.622s, learning 2.263s)
               Value function loss: 46327.1411
                    Surrogate loss: 0.0117
             Mean action noise std: 0.94
                       Mean reward: 2631.88
               Mean episode length: 235.08
                 Mean success rate: 36.00
                  Mean reward/step: 11.40
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 2.88s
                        Total time: 2745.22s
                               ETA: 7577.7s

################################################################################
                     [1m Learning iteration 1064/4000 [0m

                       Computation: 2665 steps/s (collection: 0.644s, learning 2.430s)
               Value function loss: 52886.3188
                    Surrogate loss: 0.0130
             Mean action noise std: 0.94
                       Mean reward: 2609.65
               Mean episode length: 238.24
                 Mean success rate: 35.00
                  Mean reward/step: 11.58
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8724480
                    Iteration time: 3.07s
                        Total time: 2748.29s
                               ETA: 7576.5s

################################################################################
                     [1m Learning iteration 1065/4000 [0m

                       Computation: 2920 steps/s (collection: 0.537s, learning 2.268s)
               Value function loss: 65392.5248
                    Surrogate loss: 0.0145
             Mean action noise std: 0.94
                       Mean reward: 2639.74
               Mean episode length: 237.54
                 Mean success rate: 36.50
                  Mean reward/step: 10.99
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 2.80s
                        Total time: 2751.09s
                               ETA: 7574.5s

################################################################################
                     [1m Learning iteration 1066/4000 [0m

                       Computation: 3018 steps/s (collection: 0.621s, learning 2.092s)
               Value function loss: 42355.0954
                    Surrogate loss: 0.0125
             Mean action noise std: 0.94
                       Mean reward: 2477.77
               Mean episode length: 227.87
                 Mean success rate: 34.00
                  Mean reward/step: 10.72
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8740864
                    Iteration time: 2.71s
                        Total time: 2753.81s
                               ETA: 7572.3s

################################################################################
                     [1m Learning iteration 1067/4000 [0m

                       Computation: 2995 steps/s (collection: 0.581s, learning 2.154s)
               Value function loss: 39886.1432
                    Surrogate loss: 0.0153
             Mean action noise std: 0.94
                       Mean reward: 2641.05
               Mean episode length: 237.53
                 Mean success rate: 35.50
                  Mean reward/step: 10.73
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 2.73s
                        Total time: 2756.54s
                               ETA: 7570.2s

################################################################################
                     [1m Learning iteration 1068/4000 [0m

                       Computation: 2938 steps/s (collection: 0.657s, learning 2.131s)
               Value function loss: 44841.3009
                    Surrogate loss: 0.0132
             Mean action noise std: 0.94
                       Mean reward: 2799.89
               Mean episode length: 243.81
                 Mean success rate: 38.00
                  Mean reward/step: 10.47
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8757248
                    Iteration time: 2.79s
                        Total time: 2759.33s
                               ETA: 7568.2s

################################################################################
                     [1m Learning iteration 1069/4000 [0m

                       Computation: 3000 steps/s (collection: 0.565s, learning 2.165s)
               Value function loss: 52066.8426
                    Surrogate loss: 0.0113
             Mean action noise std: 0.94
                       Mean reward: 3002.18
               Mean episode length: 254.04
                 Mean success rate: 39.50
                  Mean reward/step: 9.96
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 2.73s
                        Total time: 2762.06s
                               ETA: 7566.0s

################################################################################
                     [1m Learning iteration 1070/4000 [0m

                       Computation: 3024 steps/s (collection: 0.612s, learning 2.096s)
               Value function loss: 38222.7629
                    Surrogate loss: 0.0130
             Mean action noise std: 0.94
                       Mean reward: 3118.51
               Mean episode length: 261.54
                 Mean success rate: 41.50
                  Mean reward/step: 10.02
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8773632
                    Iteration time: 2.71s
                        Total time: 2764.77s
                               ETA: 7563.7s

################################################################################
                     [1m Learning iteration 1071/4000 [0m

                       Computation: 3048 steps/s (collection: 0.573s, learning 2.115s)
               Value function loss: 53709.8345
                    Surrogate loss: 0.0121
             Mean action noise std: 0.94
                       Mean reward: 2726.37
               Mean episode length: 244.62
                 Mean success rate: 36.50
                  Mean reward/step: 9.89
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 2.69s
                        Total time: 2767.46s
                               ETA: 7561.5s

################################################################################
                     [1m Learning iteration 1072/4000 [0m

                       Computation: 3051 steps/s (collection: 0.578s, learning 2.106s)
               Value function loss: 34791.0260
                    Surrogate loss: 0.0137
             Mean action noise std: 0.94
                       Mean reward: 2862.37
               Mean episode length: 252.59
                 Mean success rate: 37.00
                  Mean reward/step: 10.02
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8790016
                    Iteration time: 2.68s
                        Total time: 2770.14s
                               ETA: 7559.2s

################################################################################
                     [1m Learning iteration 1073/4000 [0m

                       Computation: 3069 steps/s (collection: 0.548s, learning 2.121s)
               Value function loss: 39065.9187
                    Surrogate loss: 0.0160
             Mean action noise std: 0.94
                       Mean reward: 2735.87
               Mean episode length: 246.84
                 Mean success rate: 34.50
                  Mean reward/step: 10.04
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 2.67s
                        Total time: 2772.81s
                               ETA: 7556.8s

################################################################################
                     [1m Learning iteration 1074/4000 [0m

                       Computation: 3086 steps/s (collection: 0.512s, learning 2.142s)
               Value function loss: 43743.7765
                    Surrogate loss: 0.0139
             Mean action noise std: 0.94
                       Mean reward: 2759.00
               Mean episode length: 248.37
                 Mean success rate: 35.50
                  Mean reward/step: 10.27
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 8806400
                    Iteration time: 2.65s
                        Total time: 2775.46s
                               ETA: 7554.4s

################################################################################
                     [1m Learning iteration 1075/4000 [0m

                       Computation: 3032 steps/s (collection: 0.546s, learning 2.155s)
               Value function loss: 29336.9797
                    Surrogate loss: 0.0145
             Mean action noise std: 0.94
                       Mean reward: 2401.79
               Mean episode length: 236.70
                 Mean success rate: 34.00
                  Mean reward/step: 10.41
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 2.70s
                        Total time: 2778.16s
                               ETA: 7552.2s

################################################################################
                     [1m Learning iteration 1076/4000 [0m

                       Computation: 3052 steps/s (collection: 0.561s, learning 2.123s)
               Value function loss: 34301.8724
                    Surrogate loss: 0.0197
             Mean action noise std: 0.94
                       Mean reward: 2206.23
               Mean episode length: 224.25
                 Mean success rate: 31.50
                  Mean reward/step: 10.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8822784
                    Iteration time: 2.68s
                        Total time: 2780.85s
                               ETA: 7549.9s

################################################################################
                     [1m Learning iteration 1077/4000 [0m

                       Computation: 3148 steps/s (collection: 0.524s, learning 2.078s)
               Value function loss: 44341.0393
                    Surrogate loss: 0.0162
             Mean action noise std: 0.94
                       Mean reward: 2250.43
               Mean episode length: 227.43
                 Mean success rate: 31.50
                  Mean reward/step: 11.10
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 2.60s
                        Total time: 2783.45s
                               ETA: 7547.3s

################################################################################
                     [1m Learning iteration 1078/4000 [0m

                       Computation: 3223 steps/s (collection: 0.506s, learning 2.035s)
               Value function loss: 51751.3950
                    Surrogate loss: 0.0127
             Mean action noise std: 0.94
                       Mean reward: 2307.81
               Mean episode length: 233.62
                 Mean success rate: 31.50
                  Mean reward/step: 11.09
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8839168
                    Iteration time: 2.54s
                        Total time: 2785.99s
                               ETA: 7544.6s

################################################################################
                     [1m Learning iteration 1079/4000 [0m

                       Computation: 3161 steps/s (collection: 0.512s, learning 2.079s)
               Value function loss: 43570.4433
                    Surrogate loss: 0.0119
             Mean action noise std: 0.94
                       Mean reward: 2107.24
               Mean episode length: 226.48
                 Mean success rate: 29.50
                  Mean reward/step: 10.90
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 2.59s
                        Total time: 2788.58s
                               ETA: 7542.1s

################################################################################
                     [1m Learning iteration 1080/4000 [0m

                       Computation: 3234 steps/s (collection: 0.463s, learning 2.069s)
               Value function loss: 49297.3296
                    Surrogate loss: 0.0116
             Mean action noise std: 0.94
                       Mean reward: 2330.05
               Mean episode length: 237.84
                 Mean success rate: 31.50
                  Mean reward/step: 10.75
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8855552
                    Iteration time: 2.53s
                        Total time: 2791.11s
                               ETA: 7539.4s

################################################################################
                     [1m Learning iteration 1081/4000 [0m

                       Computation: 3188 steps/s (collection: 0.498s, learning 2.072s)
               Value function loss: 32485.9437
                    Surrogate loss: 0.0150
             Mean action noise std: 0.94
                       Mean reward: 2382.96
               Mean episode length: 239.00
                 Mean success rate: 30.50
                  Mean reward/step: 10.65
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 2.57s
                        Total time: 2793.68s
                               ETA: 7536.7s

################################################################################
                     [1m Learning iteration 1082/4000 [0m

                       Computation: 3164 steps/s (collection: 0.504s, learning 2.085s)
               Value function loss: 40577.2625
                    Surrogate loss: 0.0123
             Mean action noise std: 0.94
                       Mean reward: 2462.56
               Mean episode length: 241.64
                 Mean success rate: 32.50
                  Mean reward/step: 10.30
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8871936
                    Iteration time: 2.59s
                        Total time: 2796.27s
                               ETA: 7534.2s

################################################################################
                     [1m Learning iteration 1083/4000 [0m

                       Computation: 3305 steps/s (collection: 0.459s, learning 2.020s)
               Value function loss: 34282.8588
                    Surrogate loss: 0.0151
             Mean action noise std: 0.94
                       Mean reward: 2416.46
               Mean episode length: 244.08
                 Mean success rate: 32.00
                  Mean reward/step: 10.55
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 2.48s
                        Total time: 2798.75s
                               ETA: 7531.3s

################################################################################
                     [1m Learning iteration 1084/4000 [0m

                       Computation: 3252 steps/s (collection: 0.452s, learning 2.066s)
               Value function loss: 49627.2536
                    Surrogate loss: 0.0138
             Mean action noise std: 0.94
                       Mean reward: 2216.48
               Mean episode length: 234.90
                 Mean success rate: 31.00
                  Mean reward/step: 11.54
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8888320
                    Iteration time: 2.52s
                        Total time: 2801.27s
                               ETA: 7528.6s

################################################################################
                     [1m Learning iteration 1085/4000 [0m

                       Computation: 3161 steps/s (collection: 0.497s, learning 2.094s)
               Value function loss: 53876.7708
                    Surrogate loss: 0.0119
             Mean action noise std: 0.94
                       Mean reward: 2503.59
               Mean episode length: 246.01
                 Mean success rate: 33.00
                  Mean reward/step: 11.45
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 2.59s
                        Total time: 2803.86s
                               ETA: 7526.0s

################################################################################
                     [1m Learning iteration 1086/4000 [0m

                       Computation: 3150 steps/s (collection: 0.532s, learning 2.068s)
               Value function loss: 43871.9534
                    Surrogate loss: 0.0121
             Mean action noise std: 0.94
                       Mean reward: 2398.56
               Mean episode length: 232.47
                 Mean success rate: 30.50
                  Mean reward/step: 11.29
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 8904704
                    Iteration time: 2.60s
                        Total time: 2806.46s
                               ETA: 7523.5s

################################################################################
                     [1m Learning iteration 1087/4000 [0m

                       Computation: 3269 steps/s (collection: 0.444s, learning 2.061s)
               Value function loss: 49969.7492
                    Surrogate loss: 0.0134
             Mean action noise std: 0.94
                       Mean reward: 2576.79
               Mean episode length: 237.37
                 Mean success rate: 31.50
                  Mean reward/step: 11.14
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 2.51s
                        Total time: 2808.97s
                               ETA: 7520.7s

################################################################################
                     [1m Learning iteration 1088/4000 [0m

                       Computation: 3236 steps/s (collection: 0.466s, learning 2.065s)
               Value function loss: 39702.8646
                    Surrogate loss: 0.0238
             Mean action noise std: 0.94
                       Mean reward: 2595.92
               Mean episode length: 240.06
                 Mean success rate: 31.00
                  Mean reward/step: 11.48
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8921088
                    Iteration time: 2.53s
                        Total time: 2811.50s
                               ETA: 7518.0s

################################################################################
                     [1m Learning iteration 1089/4000 [0m

                       Computation: 3190 steps/s (collection: 0.504s, learning 2.064s)
               Value function loss: 51402.8877
                    Surrogate loss: 0.0171
             Mean action noise std: 0.93
                       Mean reward: 2670.54
               Mean episode length: 235.73
                 Mean success rate: 31.00
                  Mean reward/step: 11.50
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 2.57s
                        Total time: 2814.06s
                               ETA: 7515.4s

################################################################################
                     [1m Learning iteration 1090/4000 [0m

                       Computation: 3174 steps/s (collection: 0.508s, learning 2.073s)
               Value function loss: 59258.7550
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 2502.12
               Mean episode length: 225.21
                 Mean success rate: 29.50
                  Mean reward/step: 11.64
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 8937472
                    Iteration time: 2.58s
                        Total time: 2816.64s
                               ETA: 7512.8s

################################################################################
                     [1m Learning iteration 1091/4000 [0m

                       Computation: 3141 steps/s (collection: 0.523s, learning 2.084s)
               Value function loss: 43086.4085
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 2613.82
               Mean episode length: 225.40
                 Mean success rate: 31.00
                  Mean reward/step: 10.88
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 2.61s
                        Total time: 2819.25s
                               ETA: 7510.3s

################################################################################
                     [1m Learning iteration 1092/4000 [0m

                       Computation: 3212 steps/s (collection: 0.491s, learning 2.059s)
               Value function loss: 43625.0855
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 2831.98
               Mean episode length: 233.03
                 Mean success rate: 35.00
                  Mean reward/step: 11.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8953856
                    Iteration time: 2.55s
                        Total time: 2821.80s
                               ETA: 7507.6s

################################################################################
                     [1m Learning iteration 1093/4000 [0m

                       Computation: 3219 steps/s (collection: 0.458s, learning 2.086s)
               Value function loss: 60945.5184
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 2929.78
               Mean episode length: 241.94
                 Mean success rate: 37.00
                  Mean reward/step: 12.15
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 2.54s
                        Total time: 2824.35s
                               ETA: 7504.9s

################################################################################
                     [1m Learning iteration 1094/4000 [0m

                       Computation: 3180 steps/s (collection: 0.524s, learning 2.052s)
               Value function loss: 26156.0734
                    Surrogate loss: 0.0174
             Mean action noise std: 0.93
                       Mean reward: 2858.36
               Mean episode length: 241.34
                 Mean success rate: 35.50
                  Mean reward/step: 12.15
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8970240
                    Iteration time: 2.58s
                        Total time: 2826.92s
                               ETA: 7502.3s

################################################################################
                     [1m Learning iteration 1095/4000 [0m

                       Computation: 3254 steps/s (collection: 0.477s, learning 2.040s)
               Value function loss: 49966.2476
                    Surrogate loss: 0.0117
             Mean action noise std: 0.93
                       Mean reward: 2886.60
               Mean episode length: 240.92
                 Mean success rate: 36.50
                  Mean reward/step: 12.78
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 2.52s
                        Total time: 2829.44s
                               ETA: 7499.6s

################################################################################
                     [1m Learning iteration 1096/4000 [0m

                       Computation: 3282 steps/s (collection: 0.476s, learning 2.020s)
               Value function loss: 28171.9362
                    Surrogate loss: 0.0171
             Mean action noise std: 0.93
                       Mean reward: 2849.15
               Mean episode length: 237.09
                 Mean success rate: 36.00
                  Mean reward/step: 13.05
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8986624
                    Iteration time: 2.50s
                        Total time: 2831.94s
                               ETA: 7496.8s

################################################################################
                     [1m Learning iteration 1097/4000 [0m

                       Computation: 3095 steps/s (collection: 0.559s, learning 2.087s)
               Value function loss: 45522.6187
                    Surrogate loss: 0.0135
             Mean action noise std: 0.93
                       Mean reward: 2966.50
               Mean episode length: 246.24
                 Mean success rate: 37.00
                  Mean reward/step: 13.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 2.65s
                        Total time: 2834.58s
                               ETA: 7494.3s

################################################################################
                     [1m Learning iteration 1098/4000 [0m

                       Computation: 3192 steps/s (collection: 0.500s, learning 2.066s)
               Value function loss: 67716.9936
                    Surrogate loss: 0.0117
             Mean action noise std: 0.93
                       Mean reward: 3072.42
               Mean episode length: 264.69
                 Mean success rate: 39.00
                  Mean reward/step: 13.26
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 9003008
                    Iteration time: 2.57s
                        Total time: 2837.15s
                               ETA: 7491.7s

################################################################################
                     [1m Learning iteration 1099/4000 [0m

                       Computation: 3224 steps/s (collection: 0.465s, learning 2.076s)
               Value function loss: 34943.1518
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 2754.19
               Mean episode length: 251.50
                 Mean success rate: 34.50
                  Mean reward/step: 12.77
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 2.54s
                        Total time: 2839.69s
                               ETA: 7489.0s

################################################################################
                     [1m Learning iteration 1100/4000 [0m

                       Computation: 3220 steps/s (collection: 0.474s, learning 2.070s)
               Value function loss: 54102.4528
                    Surrogate loss: 0.0120
             Mean action noise std: 0.93
                       Mean reward: 2834.03
               Mean episode length: 247.06
                 Mean success rate: 35.50
                  Mean reward/step: 13.18
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 9019392
                    Iteration time: 2.54s
                        Total time: 2842.23s
                               ETA: 7486.4s

################################################################################
                     [1m Learning iteration 1101/4000 [0m

                       Computation: 3158 steps/s (collection: 0.505s, learning 2.089s)
               Value function loss: 42384.0497
                    Surrogate loss: 0.0173
             Mean action noise std: 0.93
                       Mean reward: 2871.04
               Mean episode length: 250.57
                 Mean success rate: 35.50
                  Mean reward/step: 13.31
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 2.59s
                        Total time: 2844.83s
                               ETA: 7483.8s

################################################################################
                     [1m Learning iteration 1102/4000 [0m

                       Computation: 3173 steps/s (collection: 0.500s, learning 2.081s)
               Value function loss: 76248.5493
                    Surrogate loss: 0.0182
             Mean action noise std: 0.93
                       Mean reward: 3184.27
               Mean episode length: 268.30
                 Mean success rate: 40.00
                  Mean reward/step: 12.63
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 9035776
                    Iteration time: 2.58s
                        Total time: 2847.41s
                               ETA: 7481.2s

################################################################################
                     [1m Learning iteration 1103/4000 [0m

                       Computation: 3171 steps/s (collection: 0.507s, learning 2.076s)
               Value function loss: 36731.2295
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 2969.13
               Mean episode length: 247.93
                 Mean success rate: 37.00
                  Mean reward/step: 12.51
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 2.58s
                        Total time: 2849.99s
                               ETA: 7478.6s

################################################################################
                     [1m Learning iteration 1104/4000 [0m

                       Computation: 3023 steps/s (collection: 0.553s, learning 2.157s)
               Value function loss: 35547.2178
                    Surrogate loss: 0.0173
             Mean action noise std: 0.93
                       Mean reward: 3024.21
               Mean episode length: 246.53
                 Mean success rate: 36.00
                  Mean reward/step: 12.65
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9052160
                    Iteration time: 2.71s
                        Total time: 2852.70s
                               ETA: 7476.4s

################################################################################
                     [1m Learning iteration 1105/4000 [0m

                       Computation: 3034 steps/s (collection: 0.562s, learning 2.137s)
               Value function loss: 65002.7391
                    Surrogate loss: 0.0131
             Mean action noise std: 0.93
                       Mean reward: 3459.47
               Mean episode length: 262.21
                 Mean success rate: 41.00
                  Mean reward/step: 12.26
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 2.70s
                        Total time: 2855.40s
                               ETA: 7474.1s

################################################################################
                     [1m Learning iteration 1106/4000 [0m

                       Computation: 3100 steps/s (collection: 0.562s, learning 2.081s)
               Value function loss: 71444.5099
                    Surrogate loss: 0.0123
             Mean action noise std: 0.94
                       Mean reward: 3615.42
               Mean episode length: 270.25
                 Mean success rate: 41.50
                  Mean reward/step: 11.89
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 9068544
                    Iteration time: 2.64s
                        Total time: 2858.04s
                               ETA: 7471.7s

################################################################################
                     [1m Learning iteration 1107/4000 [0m

                       Computation: 3160 steps/s (collection: 0.521s, learning 2.071s)
               Value function loss: 32635.3155
                    Surrogate loss: 0.0177
             Mean action noise std: 0.94
                       Mean reward: 3495.28
               Mean episode length: 262.50
                 Mean success rate: 39.50
                  Mean reward/step: 11.38
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 2.59s
                        Total time: 2860.63s
                               ETA: 7469.1s

################################################################################
                     [1m Learning iteration 1108/4000 [0m

                       Computation: 3030 steps/s (collection: 0.596s, learning 2.107s)
               Value function loss: 50201.6199
                    Surrogate loss: 0.0154
             Mean action noise std: 0.94
                       Mean reward: 3494.36
               Mean episode length: 262.58
                 Mean success rate: 40.00
                  Mean reward/step: 12.42
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9084928
                    Iteration time: 2.70s
                        Total time: 2863.34s
                               ETA: 7466.9s

################################################################################
                     [1m Learning iteration 1109/4000 [0m

                       Computation: 3056 steps/s (collection: 0.569s, learning 2.111s)
               Value function loss: 40859.4639
                    Surrogate loss: 0.0190
             Mean action noise std: 0.94
                       Mean reward: 3305.50
               Mean episode length: 253.47
                 Mean success rate: 38.00
                  Mean reward/step: 12.76
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 2.68s
                        Total time: 2866.02s
                               ETA: 7464.6s

################################################################################
                     [1m Learning iteration 1110/4000 [0m

                       Computation: 3105 steps/s (collection: 0.583s, learning 2.055s)
               Value function loss: 45658.0701
                    Surrogate loss: 0.0159
             Mean action noise std: 0.94
                       Mean reward: 3334.58
               Mean episode length: 254.00
                 Mean success rate: 38.00
                  Mean reward/step: 12.98
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9101312
                    Iteration time: 2.64s
                        Total time: 2868.66s
                               ETA: 7462.1s

################################################################################
                     [1m Learning iteration 1111/4000 [0m

                       Computation: 3036 steps/s (collection: 0.609s, learning 2.088s)
               Value function loss: 47178.9025
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 3351.39
               Mean episode length: 256.09
                 Mean success rate: 39.00
                  Mean reward/step: 12.57
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 2.70s
                        Total time: 2871.35s
                               ETA: 7459.8s

################################################################################
                     [1m Learning iteration 1112/4000 [0m

                       Computation: 3046 steps/s (collection: 0.580s, learning 2.109s)
               Value function loss: 58010.0514
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 3294.95
               Mean episode length: 251.04
                 Mean success rate: 38.50
                  Mean reward/step: 12.30
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9117696
                    Iteration time: 2.69s
                        Total time: 2874.04s
                               ETA: 7457.5s

################################################################################
                     [1m Learning iteration 1113/4000 [0m

                       Computation: 3097 steps/s (collection: 0.548s, learning 2.097s)
               Value function loss: 59371.4617
                    Surrogate loss: 0.0133
             Mean action noise std: 0.94
                       Mean reward: 3126.60
               Mean episode length: 245.54
                 Mean success rate: 37.00
                  Mean reward/step: 12.44
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 2.64s
                        Total time: 2876.69s
                               ETA: 7455.1s

################################################################################
                     [1m Learning iteration 1114/4000 [0m

                       Computation: 3049 steps/s (collection: 0.565s, learning 2.122s)
               Value function loss: 57171.7188
                    Surrogate loss: 0.0118
             Mean action noise std: 0.94
                       Mean reward: 3092.60
               Mean episode length: 251.28
                 Mean success rate: 35.50
                  Mean reward/step: 12.80
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 9134080
                    Iteration time: 2.69s
                        Total time: 2879.37s
                               ETA: 7452.8s

################################################################################
                     [1m Learning iteration 1115/4000 [0m

                       Computation: 3174 steps/s (collection: 0.522s, learning 2.059s)
               Value function loss: 40085.6440
                    Surrogate loss: 0.0145
             Mean action noise std: 0.94
                       Mean reward: 3145.19
               Mean episode length: 256.32
                 Mean success rate: 38.00
                  Mean reward/step: 12.16
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 2.58s
                        Total time: 2881.95s
                               ETA: 7450.2s

################################################################################
                     [1m Learning iteration 1116/4000 [0m

                       Computation: 3248 steps/s (collection: 0.475s, learning 2.047s)
               Value function loss: 39617.6518
                    Surrogate loss: 0.0165
             Mean action noise std: 0.94
                       Mean reward: 3229.28
               Mean episode length: 260.99
                 Mean success rate: 41.00
                  Mean reward/step: 12.09
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9150464
                    Iteration time: 2.52s
                        Total time: 2884.48s
                               ETA: 7447.5s

################################################################################
                     [1m Learning iteration 1117/4000 [0m

                       Computation: 3217 steps/s (collection: 0.470s, learning 2.076s)
               Value function loss: 31545.8581
                    Surrogate loss: 0.0156
             Mean action noise std: 0.94
                       Mean reward: 2924.40
               Mean episode length: 245.49
                 Mean success rate: 37.00
                  Mean reward/step: 12.10
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 2.55s
                        Total time: 2887.02s
                               ETA: 7444.8s

################################################################################
                     [1m Learning iteration 1118/4000 [0m

                       Computation: 3174 steps/s (collection: 0.519s, learning 2.061s)
               Value function loss: 45947.2109
                    Surrogate loss: 0.0147
             Mean action noise std: 0.94
                       Mean reward: 3003.24
               Mean episode length: 254.64
                 Mean success rate: 39.00
                  Mean reward/step: 12.03
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9166848
                    Iteration time: 2.58s
                        Total time: 2889.60s
                               ETA: 7442.2s

################################################################################
                     [1m Learning iteration 1119/4000 [0m

                       Computation: 3134 steps/s (collection: 0.540s, learning 2.074s)
               Value function loss: 64637.9996
                    Surrogate loss: 0.0150
             Mean action noise std: 0.94
                       Mean reward: 3150.67
               Mean episode length: 253.42
                 Mean success rate: 41.00
                  Mean reward/step: 11.74
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 2.61s
                        Total time: 2892.22s
                               ETA: 7439.7s

################################################################################
                     [1m Learning iteration 1120/4000 [0m

                       Computation: 3191 steps/s (collection: 0.514s, learning 2.052s)
               Value function loss: 52262.0264
                    Surrogate loss: 0.0127
             Mean action noise std: 0.94
                       Mean reward: 3135.62
               Mean episode length: 248.46
                 Mean success rate: 41.00
                  Mean reward/step: 10.58
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9183232
                    Iteration time: 2.57s
                        Total time: 2894.78s
                               ETA: 7437.1s

################################################################################
                     [1m Learning iteration 1121/4000 [0m

                       Computation: 3155 steps/s (collection: 0.537s, learning 2.059s)
               Value function loss: 37861.7245
                    Surrogate loss: 0.0171
             Mean action noise std: 0.94
                       Mean reward: 3089.09
               Mean episode length: 246.58
                 Mean success rate: 40.00
                  Mean reward/step: 10.88
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 2.60s
                        Total time: 2897.38s
                               ETA: 7434.5s

################################################################################
                     [1m Learning iteration 1122/4000 [0m

                       Computation: 3130 steps/s (collection: 0.555s, learning 2.061s)
               Value function loss: 41386.8143
                    Surrogate loss: 0.0113
             Mean action noise std: 0.94
                       Mean reward: 2953.66
               Mean episode length: 245.69
                 Mean success rate: 38.00
                  Mean reward/step: 11.45
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 9199616
                    Iteration time: 2.62s
                        Total time: 2900.00s
                               ETA: 7432.0s

################################################################################
                     [1m Learning iteration 1123/4000 [0m

                       Computation: 3142 steps/s (collection: 0.536s, learning 2.071s)
               Value function loss: 38930.3446
                    Surrogate loss: 0.0129
             Mean action noise std: 0.93
                       Mean reward: 3048.79
               Mean episode length: 250.81
                 Mean success rate: 39.50
                  Mean reward/step: 11.63
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 2.61s
                        Total time: 2902.60s
                               ETA: 7429.5s

################################################################################
                     [1m Learning iteration 1124/4000 [0m

                       Computation: 3243 steps/s (collection: 0.492s, learning 2.033s)
               Value function loss: 50361.0906
                    Surrogate loss: 0.0170
             Mean action noise std: 0.93
                       Mean reward: 2959.27
               Mean episode length: 244.25
                 Mean success rate: 37.50
                  Mean reward/step: 11.89
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9216000
                    Iteration time: 2.53s
                        Total time: 2905.13s
                               ETA: 7426.8s

################################################################################
                     [1m Learning iteration 1125/4000 [0m

                       Computation: 3168 steps/s (collection: 0.527s, learning 2.058s)
               Value function loss: 46844.6271
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 2811.25
               Mean episode length: 239.44
                 Mean success rate: 34.50
                  Mean reward/step: 11.63
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 2.59s
                        Total time: 2907.71s
                               ETA: 7424.2s

################################################################################
                     [1m Learning iteration 1126/4000 [0m

                       Computation: 3127 steps/s (collection: 0.565s, learning 2.054s)
               Value function loss: 67726.0777
                    Surrogate loss: 0.0126
             Mean action noise std: 0.93
                       Mean reward: 2874.14
               Mean episode length: 244.39
                 Mean success rate: 36.00
                  Mean reward/step: 12.16
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 9232384
                    Iteration time: 2.62s
                        Total time: 2910.33s
                               ETA: 7421.7s

################################################################################
                     [1m Learning iteration 1127/4000 [0m

                       Computation: 3218 steps/s (collection: 0.506s, learning 2.039s)
               Value function loss: 39967.4233
                    Surrogate loss: 0.0172
             Mean action noise std: 0.93
                       Mean reward: 2659.33
               Mean episode length: 234.77
                 Mean success rate: 33.00
                  Mean reward/step: 12.05
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 2.55s
                        Total time: 2912.88s
                               ETA: 7419.1s

################################################################################
                     [1m Learning iteration 1128/4000 [0m

                       Computation: 3179 steps/s (collection: 0.528s, learning 2.048s)
               Value function loss: 37631.8792
                    Surrogate loss: 0.0140
             Mean action noise std: 0.93
                       Mean reward: 2649.92
               Mean episode length: 231.31
                 Mean success rate: 33.50
                  Mean reward/step: 12.73
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 9248768
                    Iteration time: 2.58s
                        Total time: 2915.45s
                               ETA: 7416.5s

################################################################################
                     [1m Learning iteration 1129/4000 [0m

                       Computation: 3156 steps/s (collection: 0.553s, learning 2.042s)
               Value function loss: 43966.8436
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 2503.22
               Mean episode length: 223.81
                 Mean success rate: 31.50
                  Mean reward/step: 13.09
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 2.60s
                        Total time: 2918.05s
                               ETA: 7413.9s

################################################################################
                     [1m Learning iteration 1130/4000 [0m

                       Computation: 3162 steps/s (collection: 0.507s, learning 2.083s)
               Value function loss: 58631.5057
                    Surrogate loss: 0.0133
             Mean action noise std: 0.93
                       Mean reward: 2868.71
               Mean episode length: 239.82
                 Mean success rate: 35.50
                  Mean reward/step: 12.45
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9265152
                    Iteration time: 2.59s
                        Total time: 2920.64s
                               ETA: 7411.4s

################################################################################
                     [1m Learning iteration 1131/4000 [0m

                       Computation: 3139 steps/s (collection: 0.519s, learning 2.090s)
               Value function loss: 27283.9415
                    Surrogate loss: 0.0270
             Mean action noise std: 0.93
                       Mean reward: 2802.66
               Mean episode length: 240.34
                 Mean success rate: 37.00
                  Mean reward/step: 12.25
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 2.61s
                        Total time: 2923.25s
                               ETA: 7408.8s

################################################################################
                     [1m Learning iteration 1132/4000 [0m

                       Computation: 3121 steps/s (collection: 0.495s, learning 2.130s)
               Value function loss: 37507.4742
                    Surrogate loss: 0.0214
             Mean action noise std: 0.93
                       Mean reward: 2716.90
               Mean episode length: 237.90
                 Mean success rate: 36.00
                  Mean reward/step: 13.38
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9281536
                    Iteration time: 2.62s
                        Total time: 2925.87s
                               ETA: 7406.4s

################################################################################
                     [1m Learning iteration 1133/4000 [0m

                       Computation: 3125 steps/s (collection: 0.491s, learning 2.130s)
               Value function loss: 76896.3705
                    Surrogate loss: 0.0126
             Mean action noise std: 0.93
                       Mean reward: 3086.20
               Mean episode length: 254.82
                 Mean success rate: 39.50
                  Mean reward/step: 13.46
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 2.62s
                        Total time: 2928.49s
                               ETA: 7403.9s

################################################################################
                     [1m Learning iteration 1134/4000 [0m

                       Computation: 3029 steps/s (collection: 0.567s, learning 2.137s)
               Value function loss: 44784.5177
                    Surrogate loss: 0.0140
             Mean action noise std: 0.93
                       Mean reward: 3128.16
               Mean episode length: 256.69
                 Mean success rate: 39.50
                  Mean reward/step: 13.06
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9297920
                    Iteration time: 2.70s
                        Total time: 2931.20s
                               ETA: 7401.6s

################################################################################
                     [1m Learning iteration 1135/4000 [0m

                       Computation: 3094 steps/s (collection: 0.545s, learning 2.103s)
               Value function loss: 52161.5516
                    Surrogate loss: 0.0131
             Mean action noise std: 0.93
                       Mean reward: 3365.59
               Mean episode length: 261.94
                 Mean success rate: 41.00
                  Mean reward/step: 14.22
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 2.65s
                        Total time: 2933.85s
                               ETA: 7399.2s

################################################################################
                     [1m Learning iteration 1136/4000 [0m

                       Computation: 3129 steps/s (collection: 0.512s, learning 2.106s)
               Value function loss: 45017.6648
                    Surrogate loss: 0.0122
             Mean action noise std: 0.93
                       Mean reward: 3455.06
               Mean episode length: 267.76
                 Mean success rate: 41.50
                  Mean reward/step: 14.81
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9314304
                    Iteration time: 2.62s
                        Total time: 2936.46s
                               ETA: 7396.7s

################################################################################
                     [1m Learning iteration 1137/4000 [0m

                       Computation: 3173 steps/s (collection: 0.521s, learning 2.060s)
               Value function loss: 75723.1580
                    Surrogate loss: 0.0126
             Mean action noise std: 0.93
                       Mean reward: 3318.12
               Mean episode length: 264.02
                 Mean success rate: 40.50
                  Mean reward/step: 15.05
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 2.58s
                        Total time: 2939.04s
                               ETA: 7394.1s

################################################################################
                     [1m Learning iteration 1138/4000 [0m

                       Computation: 3138 steps/s (collection: 0.562s, learning 2.048s)
               Value function loss: 50556.3092
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 3554.32
               Mean episode length: 269.08
                 Mean success rate: 43.00
                  Mean reward/step: 14.48
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 9330688
                    Iteration time: 2.61s
                        Total time: 2941.65s
                               ETA: 7391.6s

################################################################################
                     [1m Learning iteration 1139/4000 [0m

                       Computation: 3133 steps/s (collection: 0.517s, learning 2.097s)
               Value function loss: 53212.8550
                    Surrogate loss: 0.0117
             Mean action noise std: 0.93
                       Mean reward: 3618.74
               Mean episode length: 269.82
                 Mean success rate: 43.50
                  Mean reward/step: 14.73
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 2.61s
                        Total time: 2944.27s
                               ETA: 7389.1s

################################################################################
                     [1m Learning iteration 1140/4000 [0m

                       Computation: 3144 steps/s (collection: 0.532s, learning 2.073s)
               Value function loss: 58247.7829
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 3553.15
               Mean episode length: 270.71
                 Mean success rate: 45.50
                  Mean reward/step: 15.00
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9347072
                    Iteration time: 2.60s
                        Total time: 2946.87s
                               ETA: 7386.6s

################################################################################
                     [1m Learning iteration 1141/4000 [0m

                       Computation: 3140 steps/s (collection: 0.555s, learning 2.053s)
               Value function loss: 59099.5146
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 3615.36
               Mean episode length: 276.20
                 Mean success rate: 46.50
                  Mean reward/step: 15.36
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 2.61s
                        Total time: 2949.48s
                               ETA: 7384.0s

################################################################################
                     [1m Learning iteration 1142/4000 [0m

                       Computation: 3162 steps/s (collection: 0.538s, learning 2.053s)
               Value function loss: 56416.8427
                    Surrogate loss: 0.0121
             Mean action noise std: 0.93
                       Mean reward: 3739.35
               Mean episode length: 279.42
                 Mean success rate: 48.00
                  Mean reward/step: 15.12
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9363456
                    Iteration time: 2.59s
                        Total time: 2952.07s
                               ETA: 7381.5s

################################################################################
                     [1m Learning iteration 1143/4000 [0m

                       Computation: 3250 steps/s (collection: 0.501s, learning 2.020s)
               Value function loss: 75303.6860
                    Surrogate loss: 0.0105
             Mean action noise std: 0.93
                       Mean reward: 4073.36
               Mean episode length: 286.29
                 Mean success rate: 50.50
                  Mean reward/step: 14.89
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 2.52s
                        Total time: 2954.59s
                               ETA: 7378.7s

################################################################################
                     [1m Learning iteration 1144/4000 [0m

                       Computation: 3247 steps/s (collection: 0.520s, learning 2.003s)
               Value function loss: 81836.2479
                    Surrogate loss: 0.0113
             Mean action noise std: 0.93
                       Mean reward: 4138.94
               Mean episode length: 286.29
                 Mean success rate: 52.00
                  Mean reward/step: 14.02
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 9379840
                    Iteration time: 2.52s
                        Total time: 2957.12s
                               ETA: 7376.0s

################################################################################
                     [1m Learning iteration 1145/4000 [0m

                       Computation: 3181 steps/s (collection: 0.545s, learning 2.030s)
               Value function loss: 56441.5694
                    Surrogate loss: 0.0135
             Mean action noise std: 0.93
                       Mean reward: 4113.89
               Mean episode length: 285.13
                 Mean success rate: 51.50
                  Mean reward/step: 13.86
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 2.58s
                        Total time: 2959.69s
                               ETA: 7373.4s

################################################################################
                     [1m Learning iteration 1146/4000 [0m

                       Computation: 3213 steps/s (collection: 0.506s, learning 2.043s)
               Value function loss: 62713.5372
                    Surrogate loss: 0.0113
             Mean action noise std: 0.93
                       Mean reward: 4147.29
               Mean episode length: 280.27
                 Mean success rate: 49.50
                  Mean reward/step: 13.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9396224
                    Iteration time: 2.55s
                        Total time: 2962.24s
                               ETA: 7370.7s

################################################################################
                     [1m Learning iteration 1147/4000 [0m

                       Computation: 3241 steps/s (collection: 0.493s, learning 2.034s)
               Value function loss: 46060.0567
                    Surrogate loss: 0.0187
             Mean action noise std: 0.93
                       Mean reward: 4011.28
               Mean episode length: 276.14
                 Mean success rate: 48.00
                  Mean reward/step: 13.42
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 2.53s
                        Total time: 2964.77s
                               ETA: 7368.0s

################################################################################
                     [1m Learning iteration 1148/4000 [0m

                       Computation: 3295 steps/s (collection: 0.451s, learning 2.035s)
               Value function loss: 58832.9844
                    Surrogate loss: 0.0218
             Mean action noise std: 0.93
                       Mean reward: 3962.41
               Mean episode length: 273.82
                 Mean success rate: 47.00
                  Mean reward/step: 14.10
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9412608
                    Iteration time: 2.49s
                        Total time: 2967.25s
                               ETA: 7365.2s

################################################################################
                     [1m Learning iteration 1149/4000 [0m

                       Computation: 3135 steps/s (collection: 0.523s, learning 2.089s)
               Value function loss: 56730.9085
                    Surrogate loss: 0.0206
             Mean action noise std: 0.93
                       Mean reward: 4067.49
               Mean episode length: 277.56
                 Mean success rate: 48.00
                  Mean reward/step: 13.69
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 2.61s
                        Total time: 2969.87s
                               ETA: 7362.7s

################################################################################
                     [1m Learning iteration 1150/4000 [0m

                       Computation: 3154 steps/s (collection: 0.533s, learning 2.065s)
               Value function loss: 50168.8204
                    Surrogate loss: 0.0169
             Mean action noise std: 0.93
                       Mean reward: 3986.56
               Mean episode length: 278.23
                 Mean success rate: 47.50
                  Mean reward/step: 13.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9428992
                    Iteration time: 2.60s
                        Total time: 2972.46s
                               ETA: 7360.1s

################################################################################
                     [1m Learning iteration 1151/4000 [0m

                       Computation: 3179 steps/s (collection: 0.525s, learning 2.051s)
               Value function loss: 48756.7690
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 3931.91
               Mean episode length: 277.60
                 Mean success rate: 46.00
                  Mean reward/step: 14.10
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 2.58s
                        Total time: 2975.04s
                               ETA: 7357.5s

################################################################################
                     [1m Learning iteration 1152/4000 [0m

                       Computation: 3261 steps/s (collection: 0.468s, learning 2.044s)
               Value function loss: 46665.8312
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 3863.65
               Mean episode length: 277.68
                 Mean success rate: 45.50
                  Mean reward/step: 14.17
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9445376
                    Iteration time: 2.51s
                        Total time: 2977.55s
                               ETA: 7354.8s

################################################################################
                     [1m Learning iteration 1153/4000 [0m

                       Computation: 3123 steps/s (collection: 0.529s, learning 2.094s)
               Value function loss: 74874.6201
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 4267.74
               Mean episode length: 292.65
                 Mean success rate: 50.00
                  Mean reward/step: 13.85
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 2.62s
                        Total time: 2980.17s
                               ETA: 7352.3s

################################################################################
                     [1m Learning iteration 1154/4000 [0m

                       Computation: 3179 steps/s (collection: 0.491s, learning 2.085s)
               Value function loss: 42780.1951
                    Surrogate loss: 0.0119
             Mean action noise std: 0.93
                       Mean reward: 4272.53
               Mean episode length: 297.76
                 Mean success rate: 50.50
                  Mean reward/step: 13.84
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9461760
                    Iteration time: 2.58s
                        Total time: 2982.75s
                               ETA: 7349.7s

################################################################################
                     [1m Learning iteration 1155/4000 [0m

                       Computation: 2983 steps/s (collection: 0.562s, learning 2.183s)
               Value function loss: 74667.7190
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 4617.92
               Mean episode length: 312.50
                 Mean success rate: 54.50
                  Mean reward/step: 14.02
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 2.75s
                        Total time: 2985.50s
                               ETA: 7347.5s

################################################################################
                     [1m Learning iteration 1156/4000 [0m

                       Computation: 3194 steps/s (collection: 0.489s, learning 2.075s)
               Value function loss: 45919.7190
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 4745.53
               Mean episode length: 323.12
                 Mean success rate: 56.50
                  Mean reward/step: 13.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9478144
                    Iteration time: 2.56s
                        Total time: 2988.06s
                               ETA: 7344.9s

################################################################################
                     [1m Learning iteration 1157/4000 [0m

                       Computation: 3137 steps/s (collection: 0.537s, learning 2.075s)
               Value function loss: 38793.0793
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 4914.43
               Mean episode length: 329.50
                 Mean success rate: 58.00
                  Mean reward/step: 14.24
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 2.61s
                        Total time: 2990.67s
                               ETA: 7342.4s

################################################################################
                     [1m Learning iteration 1158/4000 [0m

                       Computation: 3135 steps/s (collection: 0.515s, learning 2.098s)
               Value function loss: 61726.5567
                    Surrogate loss: 0.0164
             Mean action noise std: 0.93
                       Mean reward: 4949.87
               Mean episode length: 330.86
                 Mean success rate: 58.00
                  Mean reward/step: 14.05
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9494528
                    Iteration time: 2.61s
                        Total time: 2993.28s
                               ETA: 7339.9s

################################################################################
                     [1m Learning iteration 1159/4000 [0m

                       Computation: 3214 steps/s (collection: 0.496s, learning 2.053s)
               Value function loss: 51155.7562
                    Surrogate loss: 0.0152
             Mean action noise std: 0.93
                       Mean reward: 4971.45
               Mean episode length: 340.04
                 Mean success rate: 58.50
                  Mean reward/step: 14.13
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 2.55s
                        Total time: 2995.83s
                               ETA: 7337.2s

################################################################################
                     [1m Learning iteration 1160/4000 [0m

                       Computation: 3191 steps/s (collection: 0.505s, learning 2.062s)
               Value function loss: 50666.2798
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 5187.82
               Mean episode length: 360.87
                 Mean success rate: 62.00
                  Mean reward/step: 13.58
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9510912
                    Iteration time: 2.57s
                        Total time: 2998.40s
                               ETA: 7334.6s

################################################################################
                     [1m Learning iteration 1161/4000 [0m

                       Computation: 3163 steps/s (collection: 0.533s, learning 2.057s)
               Value function loss: 62004.7036
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 5442.65
               Mean episode length: 374.13
                 Mean success rate: 64.00
                  Mean reward/step: 12.85
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 2.59s
                        Total time: 3000.99s
                               ETA: 7332.0s

################################################################################
                     [1m Learning iteration 1162/4000 [0m

                       Computation: 3253 steps/s (collection: 0.467s, learning 2.051s)
               Value function loss: 52777.3401
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 5474.67
               Mean episode length: 376.08
                 Mean success rate: 64.00
                  Mean reward/step: 12.72
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9527296
                    Iteration time: 2.52s
                        Total time: 3003.51s
                               ETA: 7329.3s

################################################################################
                     [1m Learning iteration 1163/4000 [0m

                       Computation: 3319 steps/s (collection: 0.448s, learning 2.020s)
               Value function loss: 40151.7096
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 5286.71
               Mean episode length: 378.93
                 Mean success rate: 64.00
                  Mean reward/step: 12.90
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 2.47s
                        Total time: 3005.98s
                               ETA: 7326.4s

################################################################################
                     [1m Learning iteration 1164/4000 [0m

                       Computation: 3255 steps/s (collection: 0.488s, learning 2.028s)
               Value function loss: 67563.6239
                    Surrogate loss: 0.0127
             Mean action noise std: 0.93
                       Mean reward: 5556.83
               Mean episode length: 393.77
                 Mean success rate: 65.50
                  Mean reward/step: 12.61
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9543680
                    Iteration time: 2.52s
                        Total time: 3008.49s
                               ETA: 7323.7s

################################################################################
                     [1m Learning iteration 1165/4000 [0m

                       Computation: 3229 steps/s (collection: 0.504s, learning 2.033s)
               Value function loss: 39257.4625
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 5264.44
               Mean episode length: 383.56
                 Mean success rate: 63.00
                  Mean reward/step: 12.52
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 2.54s
                        Total time: 3011.03s
                               ETA: 7321.0s

################################################################################
                     [1m Learning iteration 1166/4000 [0m

                       Computation: 3249 steps/s (collection: 0.470s, learning 2.051s)
               Value function loss: 39869.3761
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 5155.19
               Mean episode length: 377.48
                 Mean success rate: 61.50
                  Mean reward/step: 12.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9560064
                    Iteration time: 2.52s
                        Total time: 3013.55s
                               ETA: 7318.3s

################################################################################
                     [1m Learning iteration 1167/4000 [0m

                       Computation: 3223 steps/s (collection: 0.500s, learning 2.041s)
               Value function loss: 70262.8674
                    Surrogate loss: 0.0159
             Mean action noise std: 0.93
                       Mean reward: 4931.31
               Mean episode length: 379.74
                 Mean success rate: 60.50
                  Mean reward/step: 12.81
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 2.54s
                        Total time: 3016.09s
                               ETA: 7315.6s

################################################################################
                     [1m Learning iteration 1168/4000 [0m

                       Computation: 3254 steps/s (collection: 0.487s, learning 2.030s)
               Value function loss: 60141.5612
                    Surrogate loss: 0.0131
             Mean action noise std: 0.93
                       Mean reward: 4893.58
               Mean episode length: 373.21
                 Mean success rate: 60.00
                  Mean reward/step: 12.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9576448
                    Iteration time: 2.52s
                        Total time: 3018.61s
                               ETA: 7312.8s

################################################################################
                     [1m Learning iteration 1169/4000 [0m

                       Computation: 3120 steps/s (collection: 0.491s, learning 2.135s)
               Value function loss: 60507.3904
                    Surrogate loss: 0.0167
             Mean action noise std: 0.93
                       Mean reward: 4712.48
               Mean episode length: 362.66
                 Mean success rate: 58.00
                  Mean reward/step: 13.82
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 2.63s
                        Total time: 3021.23s
                               ETA: 7310.4s

################################################################################
                     [1m Learning iteration 1170/4000 [0m

                       Computation: 3000 steps/s (collection: 0.551s, learning 2.179s)
               Value function loss: 47919.2631
                    Surrogate loss: 0.0180
             Mean action noise std: 0.93
                       Mean reward: 4771.37
               Mean episode length: 361.80
                 Mean success rate: 58.50
                  Mean reward/step: 13.45
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9592832
                    Iteration time: 2.73s
                        Total time: 3023.96s
                               ETA: 7308.1s

################################################################################
                     [1m Learning iteration 1171/4000 [0m

                       Computation: 3225 steps/s (collection: 0.520s, learning 2.020s)
               Value function loss: 60705.7522
                    Surrogate loss: 0.0176
             Mean action noise std: 0.93
                       Mean reward: 4900.23
               Mean episode length: 361.38
                 Mean success rate: 58.50
                  Mean reward/step: 13.39
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 2.54s
                        Total time: 3026.50s
                               ETA: 7305.4s

################################################################################
                     [1m Learning iteration 1172/4000 [0m

                       Computation: 3299 steps/s (collection: 0.461s, learning 2.021s)
               Value function loss: 43474.0061
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 4687.20
               Mean episode length: 352.08
                 Mean success rate: 56.50
                  Mean reward/step: 13.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9609216
                    Iteration time: 2.48s
                        Total time: 3028.99s
                               ETA: 7302.6s

################################################################################
                     [1m Learning iteration 1173/4000 [0m

                       Computation: 3034 steps/s (collection: 0.534s, learning 2.166s)
               Value function loss: 48969.4622
                    Surrogate loss: 0.0182
             Mean action noise std: 0.93
                       Mean reward: 4490.60
               Mean episode length: 341.00
                 Mean success rate: 56.00
                  Mean reward/step: 13.63
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 2.70s
                        Total time: 3031.69s
                               ETA: 7300.3s

################################################################################
                     [1m Learning iteration 1174/4000 [0m

                       Computation: 3216 steps/s (collection: 0.472s, learning 2.075s)
               Value function loss: 57470.8655
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 4567.58
               Mean episode length: 338.23
                 Mean success rate: 57.50
                  Mean reward/step: 13.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9625600
                    Iteration time: 2.55s
                        Total time: 3034.23s
                               ETA: 7297.7s

################################################################################
                     [1m Learning iteration 1175/4000 [0m

                       Computation: 3197 steps/s (collection: 0.521s, learning 2.041s)
               Value function loss: 58566.8690
                    Surrogate loss: 0.0166
             Mean action noise std: 0.93
                       Mean reward: 4468.95
               Mean episode length: 332.63
                 Mean success rate: 56.00
                  Mean reward/step: 13.34
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 2.56s
                        Total time: 3036.80s
                               ETA: 7295.0s

################################################################################
                     [1m Learning iteration 1176/4000 [0m

                       Computation: 3218 steps/s (collection: 0.525s, learning 2.021s)
               Value function loss: 55583.3420
                    Surrogate loss: 0.0187
             Mean action noise std: 0.93
                       Mean reward: 4641.48
               Mean episode length: 333.99
                 Mean success rate: 57.50
                  Mean reward/step: 12.63
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9641984
                    Iteration time: 2.55s
                        Total time: 3039.34s
                               ETA: 7292.4s

################################################################################
                     [1m Learning iteration 1177/4000 [0m

                       Computation: 3244 steps/s (collection: 0.489s, learning 2.036s)
               Value function loss: 59581.5996
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 4548.39
               Mean episode length: 336.99
                 Mean success rate: 55.50
                  Mean reward/step: 11.86
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 2.52s
                        Total time: 3041.87s
                               ETA: 7289.6s

################################################################################
                     [1m Learning iteration 1178/4000 [0m

                       Computation: 3163 steps/s (collection: 0.558s, learning 2.032s)
               Value function loss: 42963.7347
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 4450.93
               Mean episode length: 344.05
                 Mean success rate: 56.00
                  Mean reward/step: 11.60
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9658368
                    Iteration time: 2.59s
                        Total time: 3044.46s
                               ETA: 7287.1s

################################################################################
                     [1m Learning iteration 1179/4000 [0m

                       Computation: 3283 steps/s (collection: 0.469s, learning 2.026s)
               Value function loss: 46673.2390
                    Surrogate loss: 0.0178
             Mean action noise std: 0.93
                       Mean reward: 4193.79
               Mean episode length: 331.72
                 Mean success rate: 53.50
                  Mean reward/step: 11.71
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 2.50s
                        Total time: 3046.95s
                               ETA: 7284.3s

################################################################################
                     [1m Learning iteration 1180/4000 [0m

                       Computation: 3023 steps/s (collection: 0.628s, learning 2.081s)
               Value function loss: 52401.0824
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 4141.29
               Mean episode length: 325.54
                 Mean success rate: 52.50
                  Mean reward/step: 12.60
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9674752
                    Iteration time: 2.71s
                        Total time: 3049.66s
                               ETA: 7282.0s

################################################################################
                     [1m Learning iteration 1181/4000 [0m

                       Computation: 2905 steps/s (collection: 0.574s, learning 2.246s)
               Value function loss: 58487.1394
                    Surrogate loss: 0.0160
             Mean action noise std: 0.93
                       Mean reward: 4143.98
               Mean episode length: 329.38
                 Mean success rate: 53.00
                  Mean reward/step: 13.20
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 2.82s
                        Total time: 3052.48s
                               ETA: 7280.0s

################################################################################
                     [1m Learning iteration 1182/4000 [0m

                       Computation: 3047 steps/s (collection: 0.551s, learning 2.137s)
               Value function loss: 54078.4641
                    Surrogate loss: 0.0133
             Mean action noise std: 0.93
                       Mean reward: 4071.77
               Mean episode length: 332.77
                 Mean success rate: 53.50
                  Mean reward/step: 13.02
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9691136
                    Iteration time: 2.69s
                        Total time: 3055.17s
                               ETA: 7277.7s

################################################################################
                     [1m Learning iteration 1183/4000 [0m

                       Computation: 3214 steps/s (collection: 0.517s, learning 2.032s)
               Value function loss: 46603.2083
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 3965.31
               Mean episode length: 331.22
                 Mean success rate: 51.00
                  Mean reward/step: 12.87
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 2.55s
                        Total time: 3057.72s
                               ETA: 7275.0s

################################################################################
                     [1m Learning iteration 1184/4000 [0m

                       Computation: 3149 steps/s (collection: 0.490s, learning 2.110s)
               Value function loss: 52553.1642
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 3791.10
               Mean episode length: 324.31
                 Mean success rate: 49.50
                  Mean reward/step: 13.86
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9707520
                    Iteration time: 2.60s
                        Total time: 3060.32s
                               ETA: 7272.4s

################################################################################
                     [1m Learning iteration 1185/4000 [0m

                       Computation: 3098 steps/s (collection: 0.570s, learning 2.074s)
               Value function loss: 57780.2157
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 3964.09
               Mean episode length: 323.43
                 Mean success rate: 52.50
                  Mean reward/step: 14.05
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 2.64s
                        Total time: 3062.96s
                               ETA: 7270.0s

################################################################################
                     [1m Learning iteration 1186/4000 [0m

                       Computation: 2883 steps/s (collection: 0.737s, learning 2.104s)
               Value function loss: 49064.9987
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 4156.14
               Mean episode length: 328.38
                 Mean success rate: 54.50
                  Mean reward/step: 13.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9723904
                    Iteration time: 2.84s
                        Total time: 3065.80s
                               ETA: 7268.0s

################################################################################
                     [1m Learning iteration 1187/4000 [0m

                       Computation: 3283 steps/s (collection: 0.440s, learning 2.056s)
               Value function loss: 45083.6661
                    Surrogate loss: 0.0133
             Mean action noise std: 0.93
                       Mean reward: 4133.05
               Mean episode length: 329.54
                 Mean success rate: 55.00
                  Mean reward/step: 13.21
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 2.50s
                        Total time: 3068.30s
                               ETA: 7265.3s

################################################################################
                     [1m Learning iteration 1188/4000 [0m

                       Computation: 3263 steps/s (collection: 0.463s, learning 2.047s)
               Value function loss: 42632.0088
                    Surrogate loss: 0.0132
             Mean action noise std: 0.93
                       Mean reward: 4207.04
               Mean episode length: 330.31
                 Mean success rate: 55.50
                  Mean reward/step: 13.37
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9740288
                    Iteration time: 2.51s
                        Total time: 3070.81s
                               ETA: 7262.5s

################################################################################
                     [1m Learning iteration 1189/4000 [0m

                       Computation: 3163 steps/s (collection: 0.527s, learning 2.062s)
               Value function loss: 52478.9685
                    Surrogate loss: 0.0119
             Mean action noise std: 0.93
                       Mean reward: 4223.50
               Mean episode length: 332.57
                 Mean success rate: 55.00
                  Mean reward/step: 13.23
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 2.59s
                        Total time: 3073.40s
                               ETA: 7259.9s

################################################################################
                     [1m Learning iteration 1190/4000 [0m

                       Computation: 3273 steps/s (collection: 0.480s, learning 2.022s)
               Value function loss: 42806.8079
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 4139.73
               Mean episode length: 326.18
                 Mean success rate: 52.50
                  Mean reward/step: 12.98
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9756672
                    Iteration time: 2.50s
                        Total time: 3075.90s
                               ETA: 7257.2s

################################################################################
                     [1m Learning iteration 1191/4000 [0m

                       Computation: 3248 steps/s (collection: 0.478s, learning 2.044s)
               Value function loss: 70274.5617
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 4105.35
               Mean episode length: 320.63
                 Mean success rate: 52.00
                  Mean reward/step: 13.14
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 2.52s
                        Total time: 3078.42s
                               ETA: 7254.4s

################################################################################
                     [1m Learning iteration 1192/4000 [0m

                       Computation: 3253 steps/s (collection: 0.464s, learning 2.054s)
               Value function loss: 54263.6637
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 4221.80
               Mean episode length: 317.45
                 Mean success rate: 52.00
                  Mean reward/step: 12.82
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9773056
                    Iteration time: 2.52s
                        Total time: 3080.94s
                               ETA: 7251.7s

################################################################################
                     [1m Learning iteration 1193/4000 [0m

                       Computation: 3241 steps/s (collection: 0.485s, learning 2.043s)
               Value function loss: 45148.2529
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 4412.78
               Mean episode length: 327.83
                 Mean success rate: 54.00
                  Mean reward/step: 13.33
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 2.53s
                        Total time: 3083.47s
                               ETA: 7249.0s

################################################################################
                     [1m Learning iteration 1194/4000 [0m

                       Computation: 3245 steps/s (collection: 0.470s, learning 2.054s)
               Value function loss: 54670.4942
                    Surrogate loss: 0.0123
             Mean action noise std: 0.93
                       Mean reward: 4595.30
               Mean episode length: 336.32
                 Mean success rate: 55.00
                  Mean reward/step: 13.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9789440
                    Iteration time: 2.52s
                        Total time: 3085.99s
                               ETA: 7246.3s

################################################################################
                     [1m Learning iteration 1195/4000 [0m

                       Computation: 3264 steps/s (collection: 0.492s, learning 2.017s)
               Value function loss: 57930.1041
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 4526.31
               Mean episode length: 333.22
                 Mean success rate: 53.50
                  Mean reward/step: 13.40
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 2.51s
                        Total time: 3088.50s
                               ETA: 7243.5s

################################################################################
                     [1m Learning iteration 1196/4000 [0m

                       Computation: 3256 steps/s (collection: 0.480s, learning 2.035s)
               Value function loss: 58138.5632
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 4500.96
               Mean episode length: 326.43
                 Mean success rate: 54.50
                  Mean reward/step: 13.49
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9805824
                    Iteration time: 2.52s
                        Total time: 3091.02s
                               ETA: 7240.8s

################################################################################
                     [1m Learning iteration 1197/4000 [0m

                       Computation: 3256 steps/s (collection: 0.481s, learning 2.035s)
               Value function loss: 63646.6623
                    Surrogate loss: 0.0140
             Mean action noise std: 0.93
                       Mean reward: 4646.40
               Mean episode length: 333.31
                 Mean success rate: 55.00
                  Mean reward/step: 13.70
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 2.52s
                        Total time: 3093.53s
                               ETA: 7238.0s

################################################################################
                     [1m Learning iteration 1198/4000 [0m

                       Computation: 3341 steps/s (collection: 0.437s, learning 2.014s)
               Value function loss: 54276.6250
                    Surrogate loss: 0.0140
             Mean action noise std: 0.93
                       Mean reward: 4758.35
               Mean episode length: 342.24
                 Mean success rate: 56.50
                  Mean reward/step: 13.58
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9822208
                    Iteration time: 2.45s
                        Total time: 3095.98s
                               ETA: 7235.2s

################################################################################
                     [1m Learning iteration 1199/4000 [0m

                       Computation: 3289 steps/s (collection: 0.454s, learning 2.037s)
               Value function loss: 50242.5131
                    Surrogate loss: 0.0159
             Mean action noise std: 0.93
                       Mean reward: 4545.42
               Mean episode length: 337.74
                 Mean success rate: 56.00
                  Mean reward/step: 13.87
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 2.49s
                        Total time: 3098.47s
                               ETA: 7232.4s

################################################################################
                     [1m Learning iteration 1200/4000 [0m

                       Computation: 3261 steps/s (collection: 0.477s, learning 2.035s)
               Value function loss: 52637.8138
                    Surrogate loss: 0.0129
             Mean action noise std: 0.94
                       Mean reward: 4544.31
               Mean episode length: 338.32
                 Mean success rate: 56.00
                  Mean reward/step: 14.93
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9838592
                    Iteration time: 2.51s
                        Total time: 3100.99s
                               ETA: 7229.6s

################################################################################
                     [1m Learning iteration 1201/4000 [0m

                       Computation: 3291 steps/s (collection: 0.475s, learning 2.014s)
               Value function loss: 59156.3232
                    Surrogate loss: 0.0155
             Mean action noise std: 0.94
                       Mean reward: 4537.57
               Mean episode length: 337.62
                 Mean success rate: 55.50
                  Mean reward/step: 15.55
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 2.49s
                        Total time: 3103.47s
                               ETA: 7226.8s

################################################################################
                     [1m Learning iteration 1202/4000 [0m

                       Computation: 3214 steps/s (collection: 0.499s, learning 2.049s)
               Value function loss: 56168.7248
                    Surrogate loss: 0.0134
             Mean action noise std: 0.94
                       Mean reward: 4320.21
               Mean episode length: 330.07
                 Mean success rate: 53.00
                  Mean reward/step: 15.71
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9854976
                    Iteration time: 2.55s
                        Total time: 3106.02s
                               ETA: 7224.2s

################################################################################
                     [1m Learning iteration 1203/4000 [0m

                       Computation: 3266 steps/s (collection: 0.460s, learning 2.048s)
               Value function loss: 42626.9009
                    Surrogate loss: 0.0133
             Mean action noise std: 0.94
                       Mean reward: 4300.40
               Mean episode length: 327.93
                 Mean success rate: 52.00
                  Mean reward/step: 16.38
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 2.51s
                        Total time: 3108.53s
                               ETA: 7221.4s

################################################################################
                     [1m Learning iteration 1204/4000 [0m

                       Computation: 3236 steps/s (collection: 0.483s, learning 2.048s)
               Value function loss: 60940.2781
                    Surrogate loss: 0.0119
             Mean action noise std: 0.94
                       Mean reward: 4476.06
               Mean episode length: 335.10
                 Mean success rate: 53.50
                  Mean reward/step: 17.13
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9871360
                    Iteration time: 2.53s
                        Total time: 3111.06s
                               ETA: 7218.7s

################################################################################
                     [1m Learning iteration 1205/4000 [0m

                       Computation: 3253 steps/s (collection: 0.481s, learning 2.036s)
               Value function loss: 50367.2500
                    Surrogate loss: 0.0128
             Mean action noise std: 0.93
                       Mean reward: 4664.55
               Mean episode length: 344.13
                 Mean success rate: 54.50
                  Mean reward/step: 16.94
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 2.52s
                        Total time: 3113.58s
                               ETA: 7216.0s

################################################################################
                     [1m Learning iteration 1206/4000 [0m

                       Computation: 3217 steps/s (collection: 0.480s, learning 2.065s)
               Value function loss: 55032.8792
                    Surrogate loss: 0.0128
             Mean action noise std: 0.94
                       Mean reward: 4441.42
               Mean episode length: 331.11
                 Mean success rate: 53.00
                  Mean reward/step: 16.94
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9887744
                    Iteration time: 2.55s
                        Total time: 3116.13s
                               ETA: 7213.3s

################################################################################
                     [1m Learning iteration 1207/4000 [0m

                       Computation: 3271 steps/s (collection: 0.463s, learning 2.041s)
               Value function loss: 92548.8182
                    Surrogate loss: 0.0127
             Mean action noise std: 0.93
                       Mean reward: 4933.96
               Mean episode length: 344.60
                 Mean success rate: 59.00
                  Mean reward/step: 16.15
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 2.50s
                        Total time: 3118.63s
                               ETA: 7210.5s

################################################################################
                     [1m Learning iteration 1208/4000 [0m

                       Computation: 3315 steps/s (collection: 0.445s, learning 2.026s)
               Value function loss: 48780.7584
                    Surrogate loss: 0.0144
             Mean action noise std: 0.94
                       Mean reward: 4971.43
               Mean episode length: 337.17
                 Mean success rate: 57.00
                  Mean reward/step: 15.87
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9904128
                    Iteration time: 2.47s
                        Total time: 3121.10s
                               ETA: 7207.7s

################################################################################
                     [1m Learning iteration 1209/4000 [0m

                       Computation: 3172 steps/s (collection: 0.508s, learning 2.075s)
               Value function loss: 71603.5762
                    Surrogate loss: 0.0152
             Mean action noise std: 0.94
                       Mean reward: 4942.16
               Mean episode length: 337.87
                 Mean success rate: 56.50
                  Mean reward/step: 16.12
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 2.58s
                        Total time: 3123.68s
                               ETA: 7205.1s

################################################################################
                     [1m Learning iteration 1210/4000 [0m

                       Computation: 3239 steps/s (collection: 0.468s, learning 2.061s)
               Value function loss: 69848.3935
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 4992.58
               Mean episode length: 336.93
                 Mean success rate: 57.50
                  Mean reward/step: 15.97
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9920512
                    Iteration time: 2.53s
                        Total time: 3126.21s
                               ETA: 7202.4s

################################################################################
                     [1m Learning iteration 1211/4000 [0m

                       Computation: 3264 steps/s (collection: 0.485s, learning 2.024s)
               Value function loss: 73462.5218
                    Surrogate loss: 0.0160
             Mean action noise std: 0.93
                       Mean reward: 5296.69
               Mean episode length: 346.81
                 Mean success rate: 60.50
                  Mean reward/step: 16.30
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 2.51s
                        Total time: 3128.72s
                               ETA: 7199.7s

################################################################################
                     [1m Learning iteration 1212/4000 [0m

                       Computation: 3210 steps/s (collection: 0.478s, learning 2.073s)
               Value function loss: 64622.7345
                    Surrogate loss: 0.0162
             Mean action noise std: 0.93
                       Mean reward: 5427.46
               Mean episode length: 350.94
                 Mean success rate: 61.50
                  Mean reward/step: 16.28
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9936896
                    Iteration time: 2.55s
                        Total time: 3131.27s
                               ETA: 7197.0s

################################################################################
                     [1m Learning iteration 1213/4000 [0m

                       Computation: 3248 steps/s (collection: 0.468s, learning 2.053s)
               Value function loss: 63119.7879
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 5549.39
               Mean episode length: 357.28
                 Mean success rate: 62.50
                  Mean reward/step: 16.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 2.52s
                        Total time: 3133.79s
                               ETA: 7194.3s

################################################################################
                     [1m Learning iteration 1214/4000 [0m

                       Computation: 3260 steps/s (collection: 0.468s, learning 2.045s)
               Value function loss: 60286.4362
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 5734.56
               Mean episode length: 358.57
                 Mean success rate: 63.00
                  Mean reward/step: 15.98
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9953280
                    Iteration time: 2.51s
                        Total time: 3136.31s
                               ETA: 7191.6s

################################################################################
                     [1m Learning iteration 1215/4000 [0m

                       Computation: 3082 steps/s (collection: 0.590s, learning 2.068s)
               Value function loss: 56359.4886
                    Surrogate loss: 0.0122
             Mean action noise std: 0.93
                       Mean reward: 5381.17
               Mean episode length: 339.02
                 Mean success rate: 58.00
                  Mean reward/step: 15.94
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 2.66s
                        Total time: 3138.96s
                               ETA: 7189.2s

################################################################################
                     [1m Learning iteration 1216/4000 [0m

                       Computation: 3108 steps/s (collection: 0.547s, learning 2.088s)
               Value function loss: 77657.0872
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 5717.88
               Mean episode length: 350.44
                 Mean success rate: 60.50
                  Mean reward/step: 15.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9969664
                    Iteration time: 2.64s
                        Total time: 3141.60s
                               ETA: 7186.7s

################################################################################
                     [1m Learning iteration 1217/4000 [0m

                       Computation: 3133 steps/s (collection: 0.525s, learning 2.090s)
               Value function loss: 68447.6131
                    Surrogate loss: 0.0126
             Mean action noise std: 0.93
                       Mean reward: 5973.73
               Mean episode length: 359.31
                 Mean success rate: 62.00
                  Mean reward/step: 15.42
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 2.61s
                        Total time: 3144.21s
                               ETA: 7184.2s

################################################################################
                     [1m Learning iteration 1218/4000 [0m

                       Computation: 3105 steps/s (collection: 0.538s, learning 2.099s)
               Value function loss: 83116.9096
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 6373.25
               Mean episode length: 370.34
                 Mean success rate: 65.50
                  Mean reward/step: 15.59
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9986048
                    Iteration time: 2.64s
                        Total time: 3146.85s
                               ETA: 7181.7s

################################################################################
                     [1m Learning iteration 1219/4000 [0m

                       Computation: 3043 steps/s (collection: 0.589s, learning 2.102s)
               Value function loss: 37440.6188
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 5974.14
               Mean episode length: 357.22
                 Mean success rate: 62.00
                  Mean reward/step: 15.44
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 2.69s
                        Total time: 3149.54s
                               ETA: 7179.4s

################################################################################
                     [1m Learning iteration 1220/4000 [0m

                       Computation: 3065 steps/s (collection: 0.588s, learning 2.084s)
               Value function loss: 70330.1762
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 5833.03
               Mean episode length: 352.30
                 Mean success rate: 61.50
                  Mean reward/step: 16.58
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10002432
                    Iteration time: 2.67s
                        Total time: 3152.22s
                               ETA: 7177.0s

################################################################################
                     [1m Learning iteration 1221/4000 [0m

                       Computation: 3084 steps/s (collection: 0.587s, learning 2.068s)
               Value function loss: 69105.4980
                    Surrogate loss: 0.0117
             Mean action noise std: 0.93
                       Mean reward: 5781.09
               Mean episode length: 347.07
                 Mean success rate: 61.00
                  Mean reward/step: 17.16
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 2.66s
                        Total time: 3154.87s
                               ETA: 7174.6s

################################################################################
                     [1m Learning iteration 1222/4000 [0m

                       Computation: 3115 steps/s (collection: 0.558s, learning 2.071s)
               Value function loss: 65631.7617
                    Surrogate loss: 0.0126
             Mean action noise std: 0.93
                       Mean reward: 5506.77
               Mean episode length: 334.70
                 Mean success rate: 58.00
                  Mean reward/step: 17.60
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10018816
                    Iteration time: 2.63s
                        Total time: 3157.50s
                               ETA: 7172.1s

################################################################################
                     [1m Learning iteration 1223/4000 [0m

                       Computation: 3079 steps/s (collection: 0.561s, learning 2.099s)
               Value function loss: 69754.5517
                    Surrogate loss: 0.0116
             Mean action noise std: 0.93
                       Mean reward: 5573.24
               Mean episode length: 340.47
                 Mean success rate: 60.50
                  Mean reward/step: 17.89
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 2.66s
                        Total time: 3160.16s
                               ETA: 7169.7s

################################################################################
                     [1m Learning iteration 1224/4000 [0m

                       Computation: 3070 steps/s (collection: 0.603s, learning 2.065s)
               Value function loss: 70411.3409
                    Surrogate loss: 0.0124
             Mean action noise std: 0.93
                       Mean reward: 5823.09
               Mean episode length: 353.54
                 Mean success rate: 62.50
                  Mean reward/step: 17.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10035200
                    Iteration time: 2.67s
                        Total time: 3162.83s
                               ETA: 7167.4s

################################################################################
                     [1m Learning iteration 1225/4000 [0m

                       Computation: 3091 steps/s (collection: 0.572s, learning 2.078s)
               Value function loss: 56717.9295
                    Surrogate loss: 0.0144
             Mean action noise std: 0.93
                       Mean reward: 5298.65
               Mean episode length: 340.01
                 Mean success rate: 58.50
                  Mean reward/step: 17.41
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 2.65s
                        Total time: 3165.48s
                               ETA: 7164.9s

################################################################################
                     [1m Learning iteration 1226/4000 [0m

                       Computation: 3122 steps/s (collection: 0.569s, learning 2.055s)
               Value function loss: 61199.5979
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 5377.49
               Mean episode length: 341.51
                 Mean success rate: 60.50
                  Mean reward/step: 17.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10051584
                    Iteration time: 2.62s
                        Total time: 3168.10s
                               ETA: 7162.4s

################################################################################
                     [1m Learning iteration 1227/4000 [0m

                       Computation: 3035 steps/s (collection: 0.590s, learning 2.108s)
               Value function loss: 71921.7761
                    Surrogate loss: 0.0123
             Mean action noise std: 0.93
                       Mean reward: 5669.66
               Mean episode length: 350.02
                 Mean success rate: 62.50
                  Mean reward/step: 17.57
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 2.70s
                        Total time: 3170.80s
                               ETA: 7160.1s

################################################################################
                     [1m Learning iteration 1228/4000 [0m

                       Computation: 3029 steps/s (collection: 0.607s, learning 2.097s)
               Value function loss: 52290.4626
                    Surrogate loss: 0.0129
             Mean action noise std: 0.93
                       Mean reward: 5464.81
               Mean episode length: 339.70
                 Mean success rate: 60.50
                  Mean reward/step: 16.91
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10067968
                    Iteration time: 2.70s
                        Total time: 3173.50s
                               ETA: 7157.8s

################################################################################
                     [1m Learning iteration 1229/4000 [0m

                       Computation: 3069 steps/s (collection: 0.578s, learning 2.090s)
               Value function loss: 62387.7752
                    Surrogate loss: 0.0129
             Mean action noise std: 0.93
                       Mean reward: 5477.68
               Mean episode length: 341.92
                 Mean success rate: 61.50
                  Mean reward/step: 17.39
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 2.67s
                        Total time: 3176.17s
                               ETA: 7155.4s

################################################################################
                     [1m Learning iteration 1230/4000 [0m

                       Computation: 3078 steps/s (collection: 0.582s, learning 2.079s)
               Value function loss: 88397.2998
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 5726.07
               Mean episode length: 352.62
                 Mean success rate: 64.50
                  Mean reward/step: 17.44
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10084352
                    Iteration time: 2.66s
                        Total time: 3178.83s
                               ETA: 7153.0s

################################################################################
                     [1m Learning iteration 1231/4000 [0m

                       Computation: 3108 steps/s (collection: 0.579s, learning 2.057s)
               Value function loss: 58909.3957
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 5812.25
               Mean episode length: 351.62
                 Mean success rate: 63.50
                  Mean reward/step: 17.22
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 2.64s
                        Total time: 3181.47s
                               ETA: 7150.6s

################################################################################
                     [1m Learning iteration 1232/4000 [0m

                       Computation: 3190 steps/s (collection: 0.523s, learning 2.044s)
               Value function loss: 64126.4653
                    Surrogate loss: 0.0135
             Mean action noise std: 0.93
                       Mean reward: 5661.46
               Mean episode length: 340.64
                 Mean success rate: 62.50
                  Mean reward/step: 16.87
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10100736
                    Iteration time: 2.57s
                        Total time: 3184.04s
                               ETA: 7147.9s

################################################################################
                     [1m Learning iteration 1233/4000 [0m

                       Computation: 3221 steps/s (collection: 0.495s, learning 2.048s)
               Value function loss: 91634.0639
                    Surrogate loss: 0.0129
             Mean action noise std: 0.93
                       Mean reward: 5937.01
               Mean episode length: 345.48
                 Mean success rate: 65.50
                  Mean reward/step: 16.99
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 2.54s
                        Total time: 3186.58s
                               ETA: 7145.3s

################################################################################
                     [1m Learning iteration 1234/4000 [0m

                       Computation: 3121 steps/s (collection: 0.549s, learning 2.076s)
               Value function loss: 71370.3177
                    Surrogate loss: 0.0125
             Mean action noise std: 0.93
                       Mean reward: 5448.20
               Mean episode length: 323.13
                 Mean success rate: 60.00
                  Mean reward/step: 16.52
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 10117120
                    Iteration time: 2.62s
                        Total time: 3189.20s
                               ETA: 7142.8s

################################################################################
                     [1m Learning iteration 1235/4000 [0m

                       Computation: 3188 steps/s (collection: 0.497s, learning 2.072s)
               Value function loss: 32748.0677
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 5209.00
               Mean episode length: 315.94
                 Mean success rate: 58.00
                  Mean reward/step: 16.52
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 2.57s
                        Total time: 3191.77s
                               ETA: 7140.2s

################################################################################
                     [1m Learning iteration 1236/4000 [0m

                       Computation: 3169 steps/s (collection: 0.526s, learning 2.059s)
               Value function loss: 65070.1080
                    Surrogate loss: 0.0128
             Mean action noise std: 0.93
                       Mean reward: 5434.20
               Mean episode length: 321.21
                 Mean success rate: 59.50
                  Mean reward/step: 16.87
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10133504
                    Iteration time: 2.58s
                        Total time: 3194.36s
                               ETA: 7137.6s

################################################################################
                     [1m Learning iteration 1237/4000 [0m

                       Computation: 3245 steps/s (collection: 0.472s, learning 2.053s)
               Value function loss: 49004.1936
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 5108.31
               Mean episode length: 301.35
                 Mean success rate: 54.00
                  Mean reward/step: 17.61
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 2.52s
                        Total time: 3196.88s
                               ETA: 7134.9s

################################################################################
                     [1m Learning iteration 1238/4000 [0m

                       Computation: 3233 steps/s (collection: 0.471s, learning 2.063s)
               Value function loss: 84111.6586
                    Surrogate loss: 0.0124
             Mean action noise std: 0.93
                       Mean reward: 5246.00
               Mean episode length: 303.00
                 Mean success rate: 54.50
                  Mean reward/step: 17.86
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10149888
                    Iteration time: 2.53s
                        Total time: 3199.42s
                               ETA: 7132.2s

################################################################################
                     [1m Learning iteration 1239/4000 [0m

                       Computation: 3244 steps/s (collection: 0.474s, learning 2.051s)
               Value function loss: 61455.9922
                    Surrogate loss: 0.0152
             Mean action noise std: 0.93
                       Mean reward: 5441.96
               Mean episode length: 314.27
                 Mean success rate: 57.50
                  Mean reward/step: 17.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 2.52s
                        Total time: 3201.94s
                               ETA: 7129.5s

################################################################################
                     [1m Learning iteration 1240/4000 [0m

                       Computation: 3264 steps/s (collection: 0.455s, learning 2.055s)
               Value function loss: 83056.8687
                    Surrogate loss: 0.0107
             Mean action noise std: 0.94
                       Mean reward: 5782.56
               Mean episode length: 326.80
                 Mean success rate: 60.00
                  Mean reward/step: 17.94
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10166272
                    Iteration time: 2.51s
                        Total time: 3204.45s
                               ETA: 7126.7s

################################################################################
                     [1m Learning iteration 1241/4000 [0m

                       Computation: 3187 steps/s (collection: 0.494s, learning 2.076s)
               Value function loss: 80694.2825
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 5984.91
               Mean episode length: 330.39
                 Mean success rate: 61.00
                  Mean reward/step: 17.46
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 2.57s
                        Total time: 3207.02s
                               ETA: 7124.1s

################################################################################
                     [1m Learning iteration 1242/4000 [0m

                       Computation: 3082 steps/s (collection: 0.591s, learning 2.067s)
               Value function loss: 66912.4514
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 6080.02
               Mean episode length: 338.62
                 Mean success rate: 62.50
                  Mean reward/step: 17.47
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10182656
                    Iteration time: 2.66s
                        Total time: 3209.68s
                               ETA: 7121.7s

################################################################################
                     [1m Learning iteration 1243/4000 [0m

                       Computation: 3090 steps/s (collection: 0.569s, learning 2.082s)
               Value function loss: 76492.7054
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 5995.92
               Mean episode length: 338.75
                 Mean success rate: 61.00
                  Mean reward/step: 17.45
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 2.65s
                        Total time: 3212.33s
                               ETA: 7119.3s

################################################################################
                     [1m Learning iteration 1244/4000 [0m

                       Computation: 3092 steps/s (collection: 0.551s, learning 2.098s)
               Value function loss: 60095.9905
                    Surrogate loss: 0.0116
             Mean action noise std: 0.93
                       Mean reward: 5872.97
               Mean episode length: 333.40
                 Mean success rate: 59.50
                  Mean reward/step: 16.64
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 10199040
                    Iteration time: 2.65s
                        Total time: 3214.98s
                               ETA: 7116.9s

################################################################################
                     [1m Learning iteration 1245/4000 [0m

                       Computation: 3185 steps/s (collection: 0.495s, learning 2.076s)
               Value function loss: 67617.3739
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 6154.89
               Mean episode length: 345.69
                 Mean success rate: 63.00
                  Mean reward/step: 16.79
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 2.57s
                        Total time: 3217.55s
                               ETA: 7114.2s

################################################################################
                     [1m Learning iteration 1246/4000 [0m

                       Computation: 2934 steps/s (collection: 0.690s, learning 2.102s)
               Value function loss: 64549.9075
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 5905.22
               Mean episode length: 338.26
                 Mean success rate: 61.00
                  Mean reward/step: 17.04
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10215424
                    Iteration time: 2.79s
                        Total time: 3220.34s
                               ETA: 7112.1s

################################################################################
                     [1m Learning iteration 1247/4000 [0m

                       Computation: 3134 steps/s (collection: 0.518s, learning 2.095s)
               Value function loss: 61895.7597
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 5404.69
               Mean episode length: 323.92
                 Mean success rate: 58.00
                  Mean reward/step: 17.43
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 2.61s
                        Total time: 3222.95s
                               ETA: 7109.6s

################################################################################
                     [1m Learning iteration 1248/4000 [0m

                       Computation: 3162 steps/s (collection: 0.522s, learning 2.068s)
               Value function loss: 57277.3021
                    Surrogate loss: 0.0173
             Mean action noise std: 0.94
                       Mean reward: 5324.30
               Mean episode length: 321.40
                 Mean success rate: 57.50
                  Mean reward/step: 17.88
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10231808
                    Iteration time: 2.59s
                        Total time: 3225.55s
                               ETA: 7107.0s

################################################################################
                     [1m Learning iteration 1249/4000 [0m

                       Computation: 2858 steps/s (collection: 0.617s, learning 2.248s)
               Value function loss: 78336.1101
                    Surrogate loss: 0.0134
             Mean action noise std: 0.94
                       Mean reward: 5391.59
               Mean episode length: 324.38
                 Mean success rate: 59.00
                  Mean reward/step: 17.87
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 2.87s
                        Total time: 3228.41s
                               ETA: 7105.1s

################################################################################
                     [1m Learning iteration 1250/4000 [0m

                       Computation: 3112 steps/s (collection: 0.560s, learning 2.072s)
               Value function loss: 78426.1183
                    Surrogate loss: 0.0140
             Mean action noise std: 0.94
                       Mean reward: 5718.73
               Mean episode length: 330.45
                 Mean success rate: 62.00
                  Mean reward/step: 16.96
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10248192
                    Iteration time: 2.63s
                        Total time: 3231.04s
                               ETA: 7102.6s

################################################################################
                     [1m Learning iteration 1251/4000 [0m

                       Computation: 3150 steps/s (collection: 0.531s, learning 2.069s)
               Value function loss: 73574.7062
                    Surrogate loss: 0.0162
             Mean action noise std: 0.94
                       Mean reward: 5732.40
               Mean episode length: 333.88
                 Mean success rate: 62.50
                  Mean reward/step: 16.46
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 2.60s
                        Total time: 3233.64s
                               ETA: 7100.1s

################################################################################
                     [1m Learning iteration 1252/4000 [0m

                       Computation: 3211 steps/s (collection: 0.500s, learning 2.050s)
               Value function loss: 65078.5453
                    Surrogate loss: 0.0120
             Mean action noise std: 0.94
                       Mean reward: 5775.79
               Mean episode length: 336.50
                 Mean success rate: 62.50
                  Mean reward/step: 16.62
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10264576
                    Iteration time: 2.55s
                        Total time: 3236.19s
                               ETA: 7097.4s

################################################################################
                     [1m Learning iteration 1253/4000 [0m

                       Computation: 3184 steps/s (collection: 0.521s, learning 2.051s)
               Value function loss: 55046.6923
                    Surrogate loss: 0.0135
             Mean action noise std: 0.94
                       Mean reward: 5812.35
               Mean episode length: 337.94
                 Mean success rate: 63.00
                  Mean reward/step: 17.24
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 2.57s
                        Total time: 3238.77s
                               ETA: 7094.8s

################################################################################
                     [1m Learning iteration 1254/4000 [0m

                       Computation: 3185 steps/s (collection: 0.503s, learning 2.068s)
               Value function loss: 87545.1099
                    Surrogate loss: 0.0174
             Mean action noise std: 0.94
                       Mean reward: 5820.85
               Mean episode length: 334.80
                 Mean success rate: 62.50
                  Mean reward/step: 17.37
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 10280960
                    Iteration time: 2.57s
                        Total time: 3241.34s
                               ETA: 7092.2s

################################################################################
                     [1m Learning iteration 1255/4000 [0m

                       Computation: 3176 steps/s (collection: 0.504s, learning 2.076s)
               Value function loss: 47886.3669
                    Surrogate loss: 0.0128
             Mean action noise std: 0.94
                       Mean reward: 5712.57
               Mean episode length: 325.19
                 Mean success rate: 59.50
                  Mean reward/step: 17.26
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 2.58s
                        Total time: 3243.92s
                               ETA: 7089.6s

################################################################################
                     [1m Learning iteration 1256/4000 [0m

                       Computation: 3169 steps/s (collection: 0.526s, learning 2.058s)
               Value function loss: 67561.1592
                    Surrogate loss: 0.0165
             Mean action noise std: 0.94
                       Mean reward: 5564.77
               Mean episode length: 319.28
                 Mean success rate: 57.00
                  Mean reward/step: 18.02
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10297344
                    Iteration time: 2.58s
                        Total time: 3246.50s
                               ETA: 7087.0s

################################################################################
                     [1m Learning iteration 1257/4000 [0m

                       Computation: 3108 steps/s (collection: 0.531s, learning 2.104s)
               Value function loss: 90404.0549
                    Surrogate loss: 0.0152
             Mean action noise std: 0.94
                       Mean reward: 5350.89
               Mean episode length: 315.62
                 Mean success rate: 55.00
                  Mean reward/step: 17.55
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 2.64s
                        Total time: 3249.14s
                               ETA: 7084.6s

################################################################################
                     [1m Learning iteration 1258/4000 [0m

                       Computation: 3122 steps/s (collection: 0.524s, learning 2.099s)
               Value function loss: 73271.8969
                    Surrogate loss: 0.0152
             Mean action noise std: 0.94
                       Mean reward: 5120.42
               Mean episode length: 304.85
                 Mean success rate: 51.50
                  Mean reward/step: 17.67
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10313728
                    Iteration time: 2.62s
                        Total time: 3251.76s
                               ETA: 7082.1s

################################################################################
                     [1m Learning iteration 1259/4000 [0m

                       Computation: 3173 steps/s (collection: 0.511s, learning 2.070s)
               Value function loss: 81929.6591
                    Surrogate loss: 0.0130
             Mean action noise std: 0.94
                       Mean reward: 5264.93
               Mean episode length: 307.46
                 Mean success rate: 52.00
                  Mean reward/step: 18.13
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 2.58s
                        Total time: 3254.34s
                               ETA: 7079.5s

################################################################################
                     [1m Learning iteration 1260/4000 [0m

                       Computation: 3188 steps/s (collection: 0.530s, learning 2.039s)
               Value function loss: 75023.8071
                    Surrogate loss: 0.0150
             Mean action noise std: 0.94
                       Mean reward: 5281.94
               Mean episode length: 312.75
                 Mean success rate: 53.50
                  Mean reward/step: 17.68
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10330112
                    Iteration time: 2.57s
                        Total time: 3256.91s
                               ETA: 7076.9s

################################################################################
                     [1m Learning iteration 1261/4000 [0m

                       Computation: 3096 steps/s (collection: 0.532s, learning 2.113s)
               Value function loss: 80945.3061
                    Surrogate loss: 0.0130
             Mean action noise std: 0.94
                       Mean reward: 5767.96
               Mean episode length: 331.75
                 Mean success rate: 58.50
                  Mean reward/step: 18.16
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 2.65s
                        Total time: 3259.56s
                               ETA: 7074.4s

################################################################################
                     [1m Learning iteration 1262/4000 [0m

                       Computation: 3281 steps/s (collection: 0.455s, learning 2.041s)
               Value function loss: 70465.2208
                    Surrogate loss: 0.0149
             Mean action noise std: 0.94
                       Mean reward: 5809.55
               Mean episode length: 337.35
                 Mean success rate: 60.50
                  Mean reward/step: 18.22
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10346496
                    Iteration time: 2.50s
                        Total time: 3262.05s
                               ETA: 7071.7s

################################################################################
                     [1m Learning iteration 1263/4000 [0m

                       Computation: 3213 steps/s (collection: 0.492s, learning 2.058s)
               Value function loss: 62028.5714
                    Surrogate loss: 0.0140
             Mean action noise std: 0.94
                       Mean reward: 5907.76
               Mean episode length: 341.18
                 Mean success rate: 62.00
                  Mean reward/step: 17.87
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 2.55s
                        Total time: 3264.60s
                               ETA: 7069.0s

################################################################################
                     [1m Learning iteration 1264/4000 [0m

                       Computation: 3251 steps/s (collection: 0.447s, learning 2.073s)
               Value function loss: 57123.9278
                    Surrogate loss: 0.0158
             Mean action noise std: 0.94
                       Mean reward: 6026.96
               Mean episode length: 345.95
                 Mean success rate: 64.00
                  Mean reward/step: 18.51
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10362880
                    Iteration time: 2.52s
                        Total time: 3267.12s
                               ETA: 7066.3s

################################################################################
                     [1m Learning iteration 1265/4000 [0m

                       Computation: 3059 steps/s (collection: 0.536s, learning 2.141s)
               Value function loss: 56863.3279
                    Surrogate loss: 0.0132
             Mean action noise std: 0.93
                       Mean reward: 5876.66
               Mean episode length: 335.77
                 Mean success rate: 62.50
                  Mean reward/step: 19.33
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 2.68s
                        Total time: 3269.80s
                               ETA: 7063.9s

################################################################################
                     [1m Learning iteration 1266/4000 [0m

                       Computation: 3058 steps/s (collection: 0.572s, learning 2.107s)
               Value function loss: 58563.4195
                    Surrogate loss: 0.0132
             Mean action noise std: 0.93
                       Mean reward: 6168.51
               Mean episode length: 342.74
                 Mean success rate: 65.50
                  Mean reward/step: 19.17
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10379264
                    Iteration time: 2.68s
                        Total time: 3272.48s
                               ETA: 7061.5s

################################################################################
                     [1m Learning iteration 1267/4000 [0m

                       Computation: 3180 steps/s (collection: 0.470s, learning 2.106s)
               Value function loss: 74429.6807
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 6348.00
               Mean episode length: 350.88
                 Mean success rate: 67.00
                  Mean reward/step: 19.37
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 2.58s
                        Total time: 3275.05s
                               ETA: 7058.9s

################################################################################
                     [1m Learning iteration 1268/4000 [0m

                       Computation: 3198 steps/s (collection: 0.466s, learning 2.096s)
               Value function loss: 59924.2996
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 6486.62
               Mean episode length: 354.11
                 Mean success rate: 68.00
                  Mean reward/step: 18.56
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10395648
                    Iteration time: 2.56s
                        Total time: 3277.61s
                               ETA: 7056.3s

################################################################################
                     [1m Learning iteration 1269/4000 [0m

                       Computation: 3059 steps/s (collection: 0.537s, learning 2.140s)
               Value function loss: 59876.1556
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 6525.03
               Mean episode length: 354.45
                 Mean success rate: 69.00
                  Mean reward/step: 18.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 2.68s
                        Total time: 3280.29s
                               ETA: 7053.9s

################################################################################
                     [1m Learning iteration 1270/4000 [0m

                       Computation: 3104 steps/s (collection: 0.513s, learning 2.126s)
               Value function loss: 87497.5327
                    Surrogate loss: 0.0168
             Mean action noise std: 0.93
                       Mean reward: 6336.41
               Mean episode length: 352.42
                 Mean success rate: 67.00
                  Mean reward/step: 18.29
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 10412032
                    Iteration time: 2.64s
                        Total time: 3282.93s
                               ETA: 7051.5s

################################################################################
                     [1m Learning iteration 1271/4000 [0m

                       Computation: 3123 steps/s (collection: 0.491s, learning 2.132s)
               Value function loss: 64258.6132
                    Surrogate loss: 0.0201
             Mean action noise std: 0.93
                       Mean reward: 6362.84
               Mean episode length: 351.44
                 Mean success rate: 65.00
                  Mean reward/step: 18.05
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 2.62s
                        Total time: 3285.55s
                               ETA: 7049.0s

################################################################################
                     [1m Learning iteration 1272/4000 [0m

                       Computation: 3212 steps/s (collection: 0.477s, learning 2.073s)
               Value function loss: 79659.4412
                    Surrogate loss: 0.0162
             Mean action noise std: 0.93
                       Mean reward: 6478.18
               Mean episode length: 350.14
                 Mean success rate: 65.50
                  Mean reward/step: 17.15
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10428416
                    Iteration time: 2.55s
                        Total time: 3288.10s
                               ETA: 7046.3s

################################################################################
                     [1m Learning iteration 1273/4000 [0m

                       Computation: 3119 steps/s (collection: 0.497s, learning 2.129s)
               Value function loss: 81436.3575
                    Surrogate loss: 0.0173
             Mean action noise std: 0.93
                       Mean reward: 6445.53
               Mean episode length: 353.62
                 Mean success rate: 65.00
                  Mean reward/step: 16.50
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 2.63s
                        Total time: 3290.73s
                               ETA: 7043.8s

################################################################################
                     [1m Learning iteration 1274/4000 [0m

                       Computation: 3099 steps/s (collection: 0.532s, learning 2.111s)
               Value function loss: 71421.1873
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 6548.74
               Mean episode length: 359.58
                 Mean success rate: 65.50
                  Mean reward/step: 15.98
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10444800
                    Iteration time: 2.64s
                        Total time: 3293.37s
                               ETA: 7041.4s

################################################################################
                     [1m Learning iteration 1275/4000 [0m

                       Computation: 3172 steps/s (collection: 0.438s, learning 2.144s)
               Value function loss: 70200.0428
                    Surrogate loss: 0.0188
             Mean action noise std: 0.93
                       Mean reward: 6532.34
               Mean episode length: 363.67
                 Mean success rate: 64.50
                  Mean reward/step: 16.05
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 2.58s
                        Total time: 3295.95s
                               ETA: 7038.8s

################################################################################
                     [1m Learning iteration 1276/4000 [0m

                       Computation: 3173 steps/s (collection: 0.513s, learning 2.068s)
               Value function loss: 82607.1349
                    Surrogate loss: 0.0144
             Mean action noise std: 0.93
                       Mean reward: 6535.84
               Mean episode length: 363.95
                 Mean success rate: 64.00
                  Mean reward/step: 16.03
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10461184
                    Iteration time: 2.58s
                        Total time: 3298.54s
                               ETA: 7036.2s

################################################################################
                     [1m Learning iteration 1277/4000 [0m

                       Computation: 3126 steps/s (collection: 0.533s, learning 2.087s)
               Value function loss: 57227.0226
                    Surrogate loss: 0.0157
             Mean action noise std: 0.93
                       Mean reward: 6562.86
               Mean episode length: 360.88
                 Mean success rate: 63.50
                  Mean reward/step: 16.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 2.62s
                        Total time: 3301.16s
                               ETA: 7033.7s

################################################################################
                     [1m Learning iteration 1278/4000 [0m

                       Computation: 3122 steps/s (collection: 0.537s, learning 2.087s)
               Value function loss: 75236.0159
                    Surrogate loss: 0.0170
             Mean action noise std: 0.93
                       Mean reward: 6686.10
               Mean episode length: 360.20
                 Mean success rate: 64.50
                  Mean reward/step: 17.07
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10477568
                    Iteration time: 2.62s
                        Total time: 3303.78s
                               ETA: 7031.2s

################################################################################
                     [1m Learning iteration 1279/4000 [0m

                       Computation: 3191 steps/s (collection: 0.493s, learning 2.073s)
               Value function loss: 38430.6078
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 6226.34
               Mean episode length: 343.52
                 Mean success rate: 62.00
                  Mean reward/step: 17.13
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 2.57s
                        Total time: 3306.35s
                               ETA: 7028.6s

################################################################################
                     [1m Learning iteration 1280/4000 [0m

                       Computation: 3128 steps/s (collection: 0.486s, learning 2.132s)
               Value function loss: 50500.3532
                    Surrogate loss: 0.0167
             Mean action noise std: 0.93
                       Mean reward: 6256.30
               Mean episode length: 350.27
                 Mean success rate: 62.00
                  Mean reward/step: 17.61
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10493952
                    Iteration time: 2.62s
                        Total time: 3308.96s
                               ETA: 7026.1s

################################################################################
                     [1m Learning iteration 1281/4000 [0m

                       Computation: 2875 steps/s (collection: 0.644s, learning 2.205s)
               Value function loss: 70103.3521
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 5792.70
               Mean episode length: 329.56
                 Mean success rate: 57.00
                  Mean reward/step: 18.00
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 2.85s
                        Total time: 3311.81s
                               ETA: 7024.0s

################################################################################
                     [1m Learning iteration 1282/4000 [0m

                       Computation: 3058 steps/s (collection: 0.561s, learning 2.118s)
               Value function loss: 44640.6682
                    Surrogate loss: 0.0170
             Mean action noise std: 0.93
                       Mean reward: 5335.00
               Mean episode length: 314.87
                 Mean success rate: 53.00
                  Mean reward/step: 18.52
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10510336
                    Iteration time: 2.68s
                        Total time: 3314.49s
                               ETA: 7021.7s

################################################################################
                     [1m Learning iteration 1283/4000 [0m

                       Computation: 3170 steps/s (collection: 0.518s, learning 2.067s)
               Value function loss: 85179.6246
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 5391.92
               Mean episode length: 311.23
                 Mean success rate: 53.50
                  Mean reward/step: 18.84
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 2.58s
                        Total time: 3317.08s
                               ETA: 7019.1s

################################################################################
                     [1m Learning iteration 1284/4000 [0m

                       Computation: 3120 steps/s (collection: 0.544s, learning 2.082s)
               Value function loss: 49764.6422
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 4854.19
               Mean episode length: 297.25
                 Mean success rate: 49.00
                  Mean reward/step: 18.90
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10526720
                    Iteration time: 2.63s
                        Total time: 3319.70s
                               ETA: 7016.6s

################################################################################
                     [1m Learning iteration 1285/4000 [0m

                       Computation: 3144 steps/s (collection: 0.510s, learning 2.095s)
               Value function loss: 79846.3880
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 4770.14
               Mean episode length: 299.26
                 Mean success rate: 48.50
                  Mean reward/step: 19.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 2.61s
                        Total time: 3322.31s
                               ETA: 7014.0s

################################################################################
                     [1m Learning iteration 1286/4000 [0m

                       Computation: 3135 steps/s (collection: 0.506s, learning 2.106s)
               Value function loss: 72737.9382
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 4730.88
               Mean episode length: 301.68
                 Mean success rate: 48.50
                  Mean reward/step: 19.47
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10543104
                    Iteration time: 2.61s
                        Total time: 3324.92s
                               ETA: 7011.5s

################################################################################
                     [1m Learning iteration 1287/4000 [0m

                       Computation: 3162 steps/s (collection: 0.504s, learning 2.086s)
               Value function loss: 91625.6001
                    Surrogate loss: 0.0140
             Mean action noise std: 0.93
                       Mean reward: 5140.40
               Mean episode length: 315.60
                 Mean success rate: 52.00
                  Mean reward/step: 19.78
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 2.59s
                        Total time: 3327.51s
                               ETA: 7009.0s

################################################################################
                     [1m Learning iteration 1288/4000 [0m

                       Computation: 3050 steps/s (collection: 0.538s, learning 2.148s)
               Value function loss: 79355.8630
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 5521.35
               Mean episode length: 324.25
                 Mean success rate: 54.50
                  Mean reward/step: 19.62
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10559488
                    Iteration time: 2.69s
                        Total time: 3330.20s
                               ETA: 7006.6s

################################################################################
                     [1m Learning iteration 1289/4000 [0m

                       Computation: 2999 steps/s (collection: 0.616s, learning 2.115s)
               Value function loss: 100376.7928
                    Surrogate loss: 0.0129
             Mean action noise std: 0.93
                       Mean reward: 6103.59
               Mean episode length: 342.40
                 Mean success rate: 59.50
                  Mean reward/step: 19.27
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 2.73s
                        Total time: 3332.93s
                               ETA: 7004.3s

################################################################################
                     [1m Learning iteration 1290/4000 [0m

                       Computation: 3142 steps/s (collection: 0.514s, learning 2.093s)
               Value function loss: 65294.8080
                    Surrogate loss: 0.0154
             Mean action noise std: 0.93
                       Mean reward: 6270.87
               Mean episode length: 348.43
                 Mean success rate: 61.50
                  Mean reward/step: 19.25
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10575872
                    Iteration time: 2.61s
                        Total time: 3335.53s
                               ETA: 7001.8s

################################################################################
                     [1m Learning iteration 1291/4000 [0m

                       Computation: 3106 steps/s (collection: 0.527s, learning 2.110s)
               Value function loss: 68691.1122
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 6758.55
               Mean episode length: 366.81
                 Mean success rate: 66.50
                  Mean reward/step: 19.27
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 2.64s
                        Total time: 3338.17s
                               ETA: 6999.3s

################################################################################
                     [1m Learning iteration 1292/4000 [0m

                       Computation: 3114 steps/s (collection: 0.518s, learning 2.113s)
               Value function loss: 83296.9890
                    Surrogate loss: 0.0107
             Mean action noise std: 0.93
                       Mean reward: 6703.20
               Mean episode length: 361.37
                 Mean success rate: 65.50
                  Mean reward/step: 18.88
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10592256
                    Iteration time: 2.63s
                        Total time: 3340.80s
                               ETA: 6996.8s

################################################################################
                     [1m Learning iteration 1293/4000 [0m

                       Computation: 3114 steps/s (collection: 0.510s, learning 2.120s)
               Value function loss: 68433.5062
                    Surrogate loss: 0.0135
             Mean action noise std: 0.93
                       Mean reward: 7193.77
               Mean episode length: 377.65
                 Mean success rate: 69.50
                  Mean reward/step: 18.83
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 2.63s
                        Total time: 3343.43s
                               ETA: 6994.3s

################################################################################
                     [1m Learning iteration 1294/4000 [0m

                       Computation: 3182 steps/s (collection: 0.512s, learning 2.063s)
               Value function loss: 92519.0708
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 7523.77
               Mean episode length: 385.68
                 Mean success rate: 72.50
                  Mean reward/step: 18.86
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10608640
                    Iteration time: 2.57s
                        Total time: 3346.01s
                               ETA: 6991.7s

################################################################################
                     [1m Learning iteration 1295/4000 [0m

                       Computation: 3234 steps/s (collection: 0.495s, learning 2.037s)
               Value function loss: 70470.5306
                    Surrogate loss: 0.0106
             Mean action noise std: 0.93
                       Mean reward: 7304.53
               Mean episode length: 375.81
                 Mean success rate: 71.50
                  Mean reward/step: 18.67
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 2.53s
                        Total time: 3348.54s
                               ETA: 6989.0s

################################################################################
                     [1m Learning iteration 1296/4000 [0m

                       Computation: 3074 steps/s (collection: 0.600s, learning 2.065s)
               Value function loss: 77543.4526
                    Surrogate loss: 0.0122
             Mean action noise std: 0.93
                       Mean reward: 7276.38
               Mean episode length: 368.30
                 Mean success rate: 70.00
                  Mean reward/step: 18.94
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10625024
                    Iteration time: 2.66s
                        Total time: 3351.20s
                               ETA: 6986.6s

################################################################################
                     [1m Learning iteration 1297/4000 [0m

                       Computation: 3178 steps/s (collection: 0.509s, learning 2.069s)
               Value function loss: 64004.7819
                    Surrogate loss: 0.0103
             Mean action noise std: 0.93
                       Mean reward: 6981.85
               Mean episode length: 359.32
                 Mean success rate: 68.00
                  Mean reward/step: 19.04
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 2.58s
                        Total time: 3353.78s
                               ETA: 6984.0s

################################################################################
                     [1m Learning iteration 1298/4000 [0m

                       Computation: 3184 steps/s (collection: 0.451s, learning 2.122s)
               Value function loss: 93474.3057
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 6636.22
               Mean episode length: 346.43
                 Mean success rate: 64.50
                  Mean reward/step: 19.93
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10641408
                    Iteration time: 2.57s
                        Total time: 3356.35s
                               ETA: 6981.4s

################################################################################
                     [1m Learning iteration 1299/4000 [0m

                       Computation: 3231 steps/s (collection: 0.462s, learning 2.072s)
               Value function loss: 74684.3891
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 6614.31
               Mean episode length: 346.05
                 Mean success rate: 63.50
                  Mean reward/step: 19.49
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 2.53s
                        Total time: 3358.89s
                               ETA: 6978.7s

################################################################################
                     [1m Learning iteration 1300/4000 [0m

                       Computation: 3106 steps/s (collection: 0.531s, learning 2.106s)
               Value function loss: 54243.9678
                    Surrogate loss: 0.0142
             Mean action noise std: 0.93
                       Mean reward: 6560.18
               Mean episode length: 345.94
                 Mean success rate: 63.50
                  Mean reward/step: 18.82
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10657792
                    Iteration time: 2.64s
                        Total time: 3361.52s
                               ETA: 6976.3s

################################################################################
                     [1m Learning iteration 1301/4000 [0m

                       Computation: 3134 steps/s (collection: 0.511s, learning 2.103s)
               Value function loss: 84860.3487
                    Surrogate loss: 0.0124
             Mean action noise std: 0.93
                       Mean reward: 6511.66
               Mean episode length: 344.87
                 Mean success rate: 64.00
                  Mean reward/step: 18.53
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 2.61s
                        Total time: 3364.14s
                               ETA: 6973.7s

################################################################################
                     [1m Learning iteration 1302/4000 [0m

                       Computation: 3233 steps/s (collection: 0.469s, learning 2.064s)
               Value function loss: 60135.2176
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 6281.15
               Mean episode length: 334.96
                 Mean success rate: 61.50
                  Mean reward/step: 18.21
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10674176
                    Iteration time: 2.53s
                        Total time: 3366.67s
                               ETA: 6971.1s

################################################################################
                     [1m Learning iteration 1303/4000 [0m

                       Computation: 3178 steps/s (collection: 0.509s, learning 2.068s)
               Value function loss: 78805.9116
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 6591.09
               Mean episode length: 343.44
                 Mean success rate: 63.50
                  Mean reward/step: 17.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 2.58s
                        Total time: 3369.25s
                               ETA: 6968.5s

################################################################################
                     [1m Learning iteration 1304/4000 [0m

                       Computation: 3128 steps/s (collection: 0.532s, learning 2.086s)
               Value function loss: 74464.4896
                    Surrogate loss: 0.0129
             Mean action noise std: 0.93
                       Mean reward: 6416.22
               Mean episode length: 341.71
                 Mean success rate: 61.50
                  Mean reward/step: 18.13
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10690560
                    Iteration time: 2.62s
                        Total time: 3371.87s
                               ETA: 6965.9s

################################################################################
                     [1m Learning iteration 1305/4000 [0m

                       Computation: 3137 steps/s (collection: 0.525s, learning 2.086s)
               Value function loss: 77996.8353
                    Surrogate loss: 0.0109
             Mean action noise std: 0.93
                       Mean reward: 6389.99
               Mean episode length: 341.90
                 Mean success rate: 62.00
                  Mean reward/step: 18.90
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 2.61s
                        Total time: 3374.48s
                               ETA: 6963.4s

################################################################################
                     [1m Learning iteration 1306/4000 [0m

                       Computation: 3149 steps/s (collection: 0.502s, learning 2.099s)
               Value function loss: 70987.9654
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 6424.60
               Mean episode length: 342.00
                 Mean success rate: 62.00
                  Mean reward/step: 18.44
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10706944
                    Iteration time: 2.60s
                        Total time: 3377.08s
                               ETA: 6960.9s

################################################################################
                     [1m Learning iteration 1307/4000 [0m

                       Computation: 3162 steps/s (collection: 0.512s, learning 2.078s)
               Value function loss: 96509.8237
                    Surrogate loss: 0.0127
             Mean action noise std: 0.93
                       Mean reward: 6531.08
               Mean episode length: 347.67
                 Mean success rate: 63.50
                  Mean reward/step: 18.12
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 2.59s
                        Total time: 3379.67s
                               ETA: 6958.3s

################################################################################
                     [1m Learning iteration 1308/4000 [0m

                       Computation: 3101 steps/s (collection: 0.544s, learning 2.098s)
               Value function loss: 67606.6478
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 6561.05
               Mean episode length: 350.10
                 Mean success rate: 63.00
                  Mean reward/step: 17.20
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10723328
                    Iteration time: 2.64s
                        Total time: 3382.31s
                               ETA: 6955.8s

################################################################################
                     [1m Learning iteration 1309/4000 [0m

                       Computation: 3128 steps/s (collection: 0.512s, learning 2.107s)
               Value function loss: 76869.6822
                    Surrogate loss: 0.0188
             Mean action noise std: 0.93
                       Mean reward: 6318.87
               Mean episode length: 337.69
                 Mean success rate: 59.50
                  Mean reward/step: 17.67
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 2.62s
                        Total time: 3384.93s
                               ETA: 6953.3s

################################################################################
                     [1m Learning iteration 1310/4000 [0m

                       Computation: 3171 steps/s (collection: 0.509s, learning 2.075s)
               Value function loss: 72638.4761
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 6345.91
               Mean episode length: 341.83
                 Mean success rate: 59.50
                  Mean reward/step: 17.67
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10739712
                    Iteration time: 2.58s
                        Total time: 3387.51s
                               ETA: 6950.7s

################################################################################
                     [1m Learning iteration 1311/4000 [0m

                       Computation: 3187 steps/s (collection: 0.503s, learning 2.067s)
               Value function loss: 62326.1227
                    Surrogate loss: 0.0152
             Mean action noise std: 0.93
                       Mean reward: 6095.81
               Mean episode length: 334.29
                 Mean success rate: 57.00
                  Mean reward/step: 18.22
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 2.57s
                        Total time: 3390.08s
                               ETA: 6948.1s

################################################################################
                     [1m Learning iteration 1312/4000 [0m

                       Computation: 3103 steps/s (collection: 0.528s, learning 2.111s)
               Value function loss: 85935.0191
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 6304.46
               Mean episode length: 337.12
                 Mean success rate: 58.50
                  Mean reward/step: 18.35
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10756096
                    Iteration time: 2.64s
                        Total time: 3392.72s
                               ETA: 6945.6s

################################################################################
                     [1m Learning iteration 1313/4000 [0m

                       Computation: 3091 steps/s (collection: 0.565s, learning 2.085s)
               Value function loss: 72374.0065
                    Surrogate loss: 0.0132
             Mean action noise std: 0.93
                       Mean reward: 6103.09
               Mean episode length: 329.75
                 Mean success rate: 57.00
                  Mean reward/step: 17.90
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 2.65s
                        Total time: 3395.37s
                               ETA: 6943.2s

################################################################################
                     [1m Learning iteration 1314/4000 [0m

                       Computation: 3121 steps/s (collection: 0.573s, learning 2.051s)
               Value function loss: 91294.9543
                    Surrogate loss: 0.0102
             Mean action noise std: 0.93
                       Mean reward: 6033.72
               Mean episode length: 331.83
                 Mean success rate: 57.00
                  Mean reward/step: 17.44
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10772480
                    Iteration time: 2.62s
                        Total time: 3398.00s
                               ETA: 6940.7s

################################################################################
                     [1m Learning iteration 1315/4000 [0m

                       Computation: 3060 steps/s (collection: 0.576s, learning 2.101s)
               Value function loss: 57050.9821
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 5644.60
               Mean episode length: 319.46
                 Mean success rate: 54.00
                  Mean reward/step: 17.74
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 2.68s
                        Total time: 3400.67s
                               ETA: 6938.3s

################################################################################
                     [1m Learning iteration 1316/4000 [0m

                       Computation: 3072 steps/s (collection: 0.572s, learning 2.095s)
               Value function loss: 69512.9252
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 5613.50
               Mean episode length: 318.25
                 Mean success rate: 53.50
                  Mean reward/step: 18.40
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10788864
                    Iteration time: 2.67s
                        Total time: 3403.34s
                               ETA: 6935.9s

################################################################################
                     [1m Learning iteration 1317/4000 [0m

                       Computation: 3283 steps/s (collection: 0.475s, learning 2.020s)
               Value function loss: 77825.8637
                    Surrogate loss: 0.0128
             Mean action noise std: 0.93
                       Mean reward: 5735.60
               Mean episode length: 320.68
                 Mean success rate: 55.00
                  Mean reward/step: 18.16
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 2.49s
                        Total time: 3405.83s
                               ETA: 6933.1s

################################################################################
                     [1m Learning iteration 1318/4000 [0m

                       Computation: 3143 steps/s (collection: 0.517s, learning 2.090s)
               Value function loss: 39711.0968
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 5631.11
               Mean episode length: 315.31
                 Mean success rate: 55.50
                  Mean reward/step: 18.35
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10805248
                    Iteration time: 2.61s
                        Total time: 3408.44s
                               ETA: 6930.6s

################################################################################
                     [1m Learning iteration 1319/4000 [0m

                       Computation: 3235 steps/s (collection: 0.500s, learning 2.032s)
               Value function loss: 82346.0806
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 5990.20
               Mean episode length: 330.60
                 Mean success rate: 60.00
                  Mean reward/step: 18.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 2.53s
                        Total time: 3410.97s
                               ETA: 6927.9s

################################################################################
                     [1m Learning iteration 1320/4000 [0m

                       Computation: 3210 steps/s (collection: 0.508s, learning 2.044s)
               Value function loss: 78736.4737
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 5817.81
               Mean episode length: 323.48
                 Mean success rate: 58.50
                  Mean reward/step: 18.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10821632
                    Iteration time: 2.55s
                        Total time: 3413.52s
                               ETA: 6925.2s

################################################################################
                     [1m Learning iteration 1321/4000 [0m

                       Computation: 3293 steps/s (collection: 0.464s, learning 2.023s)
               Value function loss: 58057.7077
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 5845.16
               Mean episode length: 323.38
                 Mean success rate: 58.00
                  Mean reward/step: 18.46
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 2.49s
                        Total time: 3416.01s
                               ETA: 6922.5s

################################################################################
                     [1m Learning iteration 1322/4000 [0m

                       Computation: 3212 steps/s (collection: 0.479s, learning 2.071s)
               Value function loss: 46506.3147
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 5923.75
               Mean episode length: 328.70
                 Mean success rate: 58.00
                  Mean reward/step: 19.26
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 10838016
                    Iteration time: 2.55s
                        Total time: 3418.56s
                               ETA: 6919.8s

################################################################################
                     [1m Learning iteration 1323/4000 [0m

                       Computation: 3174 steps/s (collection: 0.529s, learning 2.052s)
               Value function loss: 78003.9963
                    Surrogate loss: 0.0144
             Mean action noise std: 0.93
                       Mean reward: 5620.95
               Mean episode length: 319.49
                 Mean success rate: 54.50
                  Mean reward/step: 19.17
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 2.58s
                        Total time: 3421.14s
                               ETA: 6917.2s

################################################################################
                     [1m Learning iteration 1324/4000 [0m

                       Computation: 3164 steps/s (collection: 0.503s, learning 2.086s)
               Value function loss: 58180.8417
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 5693.91
               Mean episode length: 316.10
                 Mean success rate: 55.50
                  Mean reward/step: 18.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10854400
                    Iteration time: 2.59s
                        Total time: 3423.73s
                               ETA: 6914.6s

################################################################################
                     [1m Learning iteration 1325/4000 [0m

                       Computation: 3111 steps/s (collection: 0.578s, learning 2.054s)
               Value function loss: 87781.2614
                    Surrogate loss: 0.0131
             Mean action noise std: 0.93
                       Mean reward: 5368.02
               Mean episode length: 302.65
                 Mean success rate: 53.00
                  Mean reward/step: 18.29
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 2.63s
                        Total time: 3426.36s
                               ETA: 6912.2s

################################################################################
                     [1m Learning iteration 1326/4000 [0m

                       Computation: 3233 steps/s (collection: 0.496s, learning 2.038s)
               Value function loss: 73143.7869
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 5299.31
               Mean episode length: 297.50
                 Mean success rate: 51.00
                  Mean reward/step: 18.22
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10870784
                    Iteration time: 2.53s
                        Total time: 3428.90s
                               ETA: 6909.5s

################################################################################
                     [1m Learning iteration 1327/4000 [0m

                       Computation: 3225 steps/s (collection: 0.510s, learning 2.030s)
               Value function loss: 84979.6554
                    Surrogate loss: 0.0133
             Mean action noise std: 0.93
                       Mean reward: 5442.84
               Mean episode length: 306.12
                 Mean success rate: 52.50
                  Mean reward/step: 18.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 2.54s
                        Total time: 3431.44s
                               ETA: 6906.8s

################################################################################
                     [1m Learning iteration 1328/4000 [0m

                       Computation: 3226 steps/s (collection: 0.486s, learning 2.054s)
               Value function loss: 49742.6864
                    Surrogate loss: 0.0094
             Mean action noise std: 0.93
                       Mean reward: 5512.79
               Mean episode length: 305.82
                 Mean success rate: 52.50
                  Mean reward/step: 18.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10887168
                    Iteration time: 2.54s
                        Total time: 3433.98s
                               ETA: 6904.1s

################################################################################
                     [1m Learning iteration 1329/4000 [0m

                       Computation: 3234 steps/s (collection: 0.504s, learning 2.029s)
               Value function loss: 115603.9090
                    Surrogate loss: 0.0113
             Mean action noise std: 0.93
                       Mean reward: 5712.76
               Mean episode length: 310.35
                 Mean success rate: 55.00
                  Mean reward/step: 18.65
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 2.53s
                        Total time: 3436.51s
                               ETA: 6901.4s

################################################################################
                     [1m Learning iteration 1330/4000 [0m

                       Computation: 3297 steps/s (collection: 0.470s, learning 2.014s)
               Value function loss: 76082.2126
                    Surrogate loss: 0.0112
             Mean action noise std: 0.93
                       Mean reward: 6010.26
               Mean episode length: 315.08
                 Mean success rate: 57.00
                  Mean reward/step: 18.51
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10903552
                    Iteration time: 2.48s
                        Total time: 3438.99s
                               ETA: 6898.7s

################################################################################
                     [1m Learning iteration 1331/4000 [0m

                       Computation: 3149 steps/s (collection: 0.537s, learning 2.064s)
               Value function loss: 68457.2479
                    Surrogate loss: 0.0127
             Mean action noise std: 0.93
                       Mean reward: 5809.12
               Mean episode length: 314.60
                 Mean success rate: 56.50
                  Mean reward/step: 18.65
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 2.60s
                        Total time: 3441.59s
                               ETA: 6896.1s

################################################################################
                     [1m Learning iteration 1332/4000 [0m

                       Computation: 3231 steps/s (collection: 0.524s, learning 2.011s)
               Value function loss: 96084.1963
                    Surrogate loss: 0.0158
             Mean action noise std: 0.93
                       Mean reward: 5972.05
               Mean episode length: 325.68
                 Mean success rate: 58.00
                  Mean reward/step: 18.81
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10919936
                    Iteration time: 2.53s
                        Total time: 3444.13s
                               ETA: 6893.4s

################################################################################
                     [1m Learning iteration 1333/4000 [0m

                       Computation: 3264 steps/s (collection: 0.484s, learning 2.025s)
               Value function loss: 60717.6026
                    Surrogate loss: 0.0095
             Mean action noise std: 0.93
                       Mean reward: 6031.44
               Mean episode length: 328.20
                 Mean success rate: 57.50
                  Mean reward/step: 18.27
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 2.51s
                        Total time: 3446.64s
                               ETA: 6890.7s

################################################################################
                     [1m Learning iteration 1334/4000 [0m

                       Computation: 3218 steps/s (collection: 0.495s, learning 2.050s)
               Value function loss: 69982.8381
                    Surrogate loss: 0.0114
             Mean action noise std: 0.93
                       Mean reward: 6220.61
               Mean episode length: 332.80
                 Mean success rate: 59.00
                  Mean reward/step: 18.81
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10936320
                    Iteration time: 2.55s
                        Total time: 3449.18s
                               ETA: 6888.0s

################################################################################
                     [1m Learning iteration 1335/4000 [0m

                       Computation: 3251 steps/s (collection: 0.497s, learning 2.022s)
               Value function loss: 64575.1911
                    Surrogate loss: 0.0135
             Mean action noise std: 0.93
                       Mean reward: 6013.10
               Mean episode length: 323.54
                 Mean success rate: 56.50
                  Mean reward/step: 18.58
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 2.52s
                        Total time: 3451.70s
                               ETA: 6885.3s

################################################################################
                     [1m Learning iteration 1336/4000 [0m

                       Computation: 3273 steps/s (collection: 0.468s, learning 2.034s)
               Value function loss: 70639.5811
                    Surrogate loss: 0.0128
             Mean action noise std: 0.93
                       Mean reward: 6121.59
               Mean episode length: 326.27
                 Mean success rate: 57.00
                  Mean reward/step: 18.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10952704
                    Iteration time: 2.50s
                        Total time: 3454.20s
                               ETA: 6882.6s

################################################################################
                     [1m Learning iteration 1337/4000 [0m

                       Computation: 3250 steps/s (collection: 0.482s, learning 2.038s)
               Value function loss: 48914.1498
                    Surrogate loss: 0.0113
             Mean action noise std: 0.93
                       Mean reward: 5906.45
               Mean episode length: 318.78
                 Mean success rate: 56.00
                  Mean reward/step: 18.45
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 2.52s
                        Total time: 3456.72s
                               ETA: 6879.9s

################################################################################
                     [1m Learning iteration 1338/4000 [0m

                       Computation: 3182 steps/s (collection: 0.513s, learning 2.061s)
               Value function loss: 87994.2535
                    Surrogate loss: 0.0098
             Mean action noise std: 0.93
                       Mean reward: 5594.45
               Mean episode length: 306.31
                 Mean success rate: 53.50
                  Mean reward/step: 19.51
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 10969088
                    Iteration time: 2.57s
                        Total time: 3459.30s
                               ETA: 6877.3s

################################################################################
                     [1m Learning iteration 1339/4000 [0m

                       Computation: 3162 steps/s (collection: 0.505s, learning 2.085s)
               Value function loss: 64860.5867
                    Surrogate loss: 0.0120
             Mean action noise std: 0.93
                       Mean reward: 5828.54
               Mean episode length: 315.32
                 Mean success rate: 56.00
                  Mean reward/step: 18.94
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 2.59s
                        Total time: 3461.89s
                               ETA: 6874.7s

################################################################################
                     [1m Learning iteration 1340/4000 [0m

                       Computation: 3091 steps/s (collection: 0.545s, learning 2.104s)
               Value function loss: 99896.0689
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 5834.15
               Mean episode length: 308.01
                 Mean success rate: 55.00
                  Mean reward/step: 18.79
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10985472
                    Iteration time: 2.65s
                        Total time: 3464.54s
                               ETA: 6872.2s

################################################################################
                     [1m Learning iteration 1341/4000 [0m

                       Computation: 3266 steps/s (collection: 0.480s, learning 2.028s)
               Value function loss: 86479.5635
                    Surrogate loss: 0.0116
             Mean action noise std: 0.93
                       Mean reward: 5923.41
               Mean episode length: 313.45
                 Mean success rate: 55.50
                  Mean reward/step: 17.74
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 2.51s
                        Total time: 3467.05s
                               ETA: 6869.5s

################################################################################
                     [1m Learning iteration 1342/4000 [0m

                       Computation: 3150 steps/s (collection: 0.513s, learning 2.087s)
               Value function loss: 80841.3158
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 5880.82
               Mean episode length: 312.81
                 Mean success rate: 55.50
                  Mean reward/step: 17.93
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11001856
                    Iteration time: 2.60s
                        Total time: 3469.65s
                               ETA: 6867.0s

################################################################################
                     [1m Learning iteration 1343/4000 [0m

                       Computation: 3152 steps/s (collection: 0.512s, learning 2.087s)
               Value function loss: 90955.8625
                    Surrogate loss: 0.0144
             Mean action noise std: 0.93
                       Mean reward: 5916.20
               Mean episode length: 313.96
                 Mean success rate: 56.00
                  Mean reward/step: 17.69
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 2.60s
                        Total time: 3472.25s
                               ETA: 6864.4s

################################################################################
                     [1m Learning iteration 1344/4000 [0m

                       Computation: 3188 steps/s (collection: 0.533s, learning 2.036s)
               Value function loss: 51203.6401
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 6060.73
               Mean episode length: 323.26
                 Mean success rate: 57.00
                  Mean reward/step: 17.87
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11018240
                    Iteration time: 2.57s
                        Total time: 3474.82s
                               ETA: 6861.8s

################################################################################
                     [1m Learning iteration 1345/4000 [0m

                       Computation: 3152 steps/s (collection: 0.508s, learning 2.090s)
               Value function loss: 83508.7312
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 6304.16
               Mean episode length: 335.55
                 Mean success rate: 58.50
                  Mean reward/step: 18.37
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 2.60s
                        Total time: 3477.41s
                               ETA: 6859.2s

################################################################################
                     [1m Learning iteration 1346/4000 [0m

                       Computation: 3189 steps/s (collection: 0.508s, learning 2.060s)
               Value function loss: 54756.0917
                    Surrogate loss: 0.0122
             Mean action noise std: 0.93
                       Mean reward: 5910.47
               Mean episode length: 318.43
                 Mean success rate: 55.50
                  Mean reward/step: 18.64
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11034624
                    Iteration time: 2.57s
                        Total time: 3479.98s
                               ETA: 6856.6s

################################################################################
                     [1m Learning iteration 1347/4000 [0m

                       Computation: 3167 steps/s (collection: 0.536s, learning 2.051s)
               Value function loss: 63027.8359
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 5853.05
               Mean episode length: 316.68
                 Mean success rate: 55.00
                  Mean reward/step: 19.41
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 2.59s
                        Total time: 3482.57s
                               ETA: 6854.0s

################################################################################
                     [1m Learning iteration 1348/4000 [0m

                       Computation: 3089 steps/s (collection: 0.579s, learning 2.073s)
               Value function loss: 103934.2983
                    Surrogate loss: 0.0115
             Mean action noise std: 0.93
                       Mean reward: 5820.95
               Mean episode length: 315.31
                 Mean success rate: 55.00
                  Mean reward/step: 19.25
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 11051008
                    Iteration time: 2.65s
                        Total time: 3485.22s
                               ETA: 6851.6s

################################################################################
                     [1m Learning iteration 1349/4000 [0m

                       Computation: 3268 steps/s (collection: 0.490s, learning 2.017s)
               Value function loss: 55706.9688
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 5902.05
               Mean episode length: 322.47
                 Mean success rate: 56.50
                  Mean reward/step: 18.97
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 2.51s
                        Total time: 3487.73s
                               ETA: 6848.9s

################################################################################
                     [1m Learning iteration 1350/4000 [0m

                       Computation: 3207 steps/s (collection: 0.521s, learning 2.033s)
               Value function loss: 96350.8636
                    Surrogate loss: 0.0162
             Mean action noise std: 0.93
                       Mean reward: 6002.78
               Mean episode length: 326.69
                 Mean success rate: 57.00
                  Mean reward/step: 19.32
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11067392
                    Iteration time: 2.55s
                        Total time: 3490.28s
                               ETA: 6846.2s

################################################################################
                     [1m Learning iteration 1351/4000 [0m

                       Computation: 3174 steps/s (collection: 0.546s, learning 2.035s)
               Value function loss: 67114.2208
                    Surrogate loss: 0.0113
             Mean action noise std: 0.93
                       Mean reward: 5915.97
               Mean episode length: 322.81
                 Mean success rate: 56.50
                  Mean reward/step: 19.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 2.58s
                        Total time: 3492.86s
                               ETA: 6843.6s

################################################################################
                     [1m Learning iteration 1352/4000 [0m

                       Computation: 3203 steps/s (collection: 0.506s, learning 2.051s)
               Value function loss: 63928.9180
                    Surrogate loss: 0.0094
             Mean action noise std: 0.93
                       Mean reward: 6091.87
               Mean episode length: 323.71
                 Mean success rate: 58.00
                  Mean reward/step: 19.91
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11083776
                    Iteration time: 2.56s
                        Total time: 3495.42s
                               ETA: 6841.0s

################################################################################
                     [1m Learning iteration 1353/4000 [0m

                       Computation: 3211 steps/s (collection: 0.515s, learning 2.036s)
               Value function loss: 61822.6070
                    Surrogate loss: 0.0099
             Mean action noise std: 0.93
                       Mean reward: 5801.28
               Mean episode length: 310.78
                 Mean success rate: 55.00
                  Mean reward/step: 20.23
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 2.55s
                        Total time: 3497.97s
                               ETA: 6838.3s

################################################################################
                     [1m Learning iteration 1354/4000 [0m

                       Computation: 3211 steps/s (collection: 0.510s, learning 2.041s)
               Value function loss: 85653.6582
                    Surrogate loss: 0.0123
             Mean action noise std: 0.93
                       Mean reward: 5716.80
               Mean episode length: 307.20
                 Mean success rate: 54.00
                  Mean reward/step: 20.61
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 11100160
                    Iteration time: 2.55s
                        Total time: 3500.52s
                               ETA: 6835.7s

################################################################################
                     [1m Learning iteration 1355/4000 [0m

                       Computation: 3245 steps/s (collection: 0.483s, learning 2.041s)
               Value function loss: 79267.2147
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 6136.02
               Mean episode length: 322.70
                 Mean success rate: 57.50
                  Mean reward/step: 20.19
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 2.52s
                        Total time: 3503.04s
                               ETA: 6833.0s

################################################################################
                     [1m Learning iteration 1356/4000 [0m

                       Computation: 3239 steps/s (collection: 0.489s, learning 2.040s)
               Value function loss: 111680.1565
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 6382.88
               Mean episode length: 334.28
                 Mean success rate: 60.00
                  Mean reward/step: 18.88
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11116544
                    Iteration time: 2.53s
                        Total time: 3505.57s
                               ETA: 6830.3s

################################################################################
                     [1m Learning iteration 1357/4000 [0m

                       Computation: 3267 steps/s (collection: 0.460s, learning 2.047s)
               Value function loss: 60681.1852
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 6301.98
               Mean episode length: 324.64
                 Mean success rate: 58.50
                  Mean reward/step: 18.19
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 2.51s
                        Total time: 3508.08s
                               ETA: 6827.6s

################################################################################
                     [1m Learning iteration 1358/4000 [0m

                       Computation: 3173 steps/s (collection: 0.486s, learning 2.095s)
               Value function loss: 70666.9804
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 6475.43
               Mean episode length: 328.12
                 Mean success rate: 60.00
                  Mean reward/step: 18.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11132928
                    Iteration time: 2.58s
                        Total time: 3510.66s
                               ETA: 6825.0s

################################################################################
                     [1m Learning iteration 1359/4000 [0m

                       Computation: 3232 steps/s (collection: 0.494s, learning 2.041s)
               Value function loss: 70155.3518
                    Surrogate loss: 0.0128
             Mean action noise std: 0.93
                       Mean reward: 6046.66
               Mean episode length: 321.02
                 Mean success rate: 56.50
                  Mean reward/step: 18.21
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 2.53s
                        Total time: 3513.20s
                               ETA: 6822.3s

################################################################################
                     [1m Learning iteration 1360/4000 [0m

                       Computation: 3324 steps/s (collection: 0.465s, learning 2.000s)
               Value function loss: 99028.4826
                    Surrogate loss: 0.0111
             Mean action noise std: 0.93
                       Mean reward: 6308.92
               Mean episode length: 327.50
                 Mean success rate: 58.50
                  Mean reward/step: 18.98
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 11149312
                    Iteration time: 2.46s
                        Total time: 3515.66s
                               ETA: 6819.5s

################################################################################
                     [1m Learning iteration 1361/4000 [0m

                       Computation: 3318 steps/s (collection: 0.458s, learning 2.011s)
               Value function loss: 59861.0221
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 6749.62
               Mean episode length: 344.44
                 Mean success rate: 61.50
                  Mean reward/step: 18.52
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 2.47s
                        Total time: 3518.13s
                               ETA: 6816.7s

################################################################################
                     [1m Learning iteration 1362/4000 [0m

                       Computation: 3270 steps/s (collection: 0.467s, learning 2.038s)
               Value function loss: 84004.7023
                    Surrogate loss: 0.0151
             Mean action noise std: 0.93
                       Mean reward: 6498.67
               Mean episode length: 332.02
                 Mean success rate: 59.00
                  Mean reward/step: 18.57
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11165696
                    Iteration time: 2.50s
                        Total time: 3520.63s
                               ETA: 6814.0s

################################################################################
                     [1m Learning iteration 1363/4000 [0m

                       Computation: 3289 steps/s (collection: 0.451s, learning 2.039s)
               Value function loss: 72598.0273
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 5647.44
               Mean episode length: 302.43
                 Mean success rate: 52.00
                  Mean reward/step: 18.92
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 2.49s
                        Total time: 3523.12s
                               ETA: 6811.2s

################################################################################
                     [1m Learning iteration 1364/4000 [0m

                       Computation: 3269 steps/s (collection: 0.494s, learning 2.011s)
               Value function loss: 91098.8543
                    Surrogate loss: 0.0129
             Mean action noise std: 0.93
                       Mean reward: 5769.09
               Mean episode length: 308.08
                 Mean success rate: 54.00
                  Mean reward/step: 17.91
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 11182080
                    Iteration time: 2.51s
                        Total time: 3525.63s
                               ETA: 6808.5s

################################################################################
                     [1m Learning iteration 1365/4000 [0m

                       Computation: 3227 steps/s (collection: 0.498s, learning 2.040s)
               Value function loss: 59737.2315
                    Surrogate loss: 0.0112
             Mean action noise std: 0.93
                       Mean reward: 5287.56
               Mean episode length: 283.89
                 Mean success rate: 50.00
                  Mean reward/step: 17.45
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 2.54s
                        Total time: 3528.17s
                               ETA: 6805.8s

################################################################################
                     [1m Learning iteration 1366/4000 [0m

                       Computation: 3281 steps/s (collection: 0.448s, learning 2.048s)
               Value function loss: 81308.8345
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 5686.57
               Mean episode length: 301.50
                 Mean success rate: 54.00
                  Mean reward/step: 17.67
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11198464
                    Iteration time: 2.50s
                        Total time: 3530.66s
                               ETA: 6803.0s

################################################################################
                     [1m Learning iteration 1367/4000 [0m

                       Computation: 3270 steps/s (collection: 0.464s, learning 2.040s)
               Value function loss: 55169.4067
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 5764.48
               Mean episode length: 305.17
                 Mean success rate: 55.00
                  Mean reward/step: 18.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 2.50s
                        Total time: 3533.17s
                               ETA: 6800.3s

################################################################################
                     [1m Learning iteration 1368/4000 [0m

                       Computation: 3283 steps/s (collection: 0.470s, learning 2.025s)
               Value function loss: 61636.1895
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 5234.78
               Mean episode length: 285.33
                 Mean success rate: 52.00
                  Mean reward/step: 19.82
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11214848
                    Iteration time: 2.49s
                        Total time: 3535.66s
                               ETA: 6797.6s

################################################################################
                     [1m Learning iteration 1369/4000 [0m

                       Computation: 3289 steps/s (collection: 0.461s, learning 2.030s)
               Value function loss: 66054.7479
                    Surrogate loss: 0.0107
             Mean action noise std: 0.93
                       Mean reward: 5317.19
               Mean episode length: 293.31
                 Mean success rate: 52.50
                  Mean reward/step: 20.64
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 2.49s
                        Total time: 3538.15s
                               ETA: 6794.8s

################################################################################
                     [1m Learning iteration 1370/4000 [0m

                       Computation: 3243 steps/s (collection: 0.503s, learning 2.023s)
               Value function loss: 81386.1140
                    Surrogate loss: 0.0112
             Mean action noise std: 0.93
                       Mean reward: 5823.38
               Mean episode length: 306.31
                 Mean success rate: 56.00
                  Mean reward/step: 20.20
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11231232
                    Iteration time: 2.53s
                        Total time: 3540.68s
                               ETA: 6792.1s

################################################################################
                     [1m Learning iteration 1371/4000 [0m

                       Computation: 3247 steps/s (collection: 0.487s, learning 2.036s)
               Value function loss: 82511.4975
                    Surrogate loss: 0.0141
             Mean action noise std: 0.92
                       Mean reward: 5490.67
               Mean episode length: 293.56
                 Mean success rate: 53.00
                  Mean reward/step: 19.78
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 2.52s
                        Total time: 3543.20s
                               ETA: 6789.4s

################################################################################
                     [1m Learning iteration 1372/4000 [0m

                       Computation: 3188 steps/s (collection: 0.509s, learning 2.060s)
               Value function loss: 102280.4027
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 5667.49
               Mean episode length: 300.00
                 Mean success rate: 54.50
                  Mean reward/step: 19.47
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 11247616
                    Iteration time: 2.57s
                        Total time: 3545.77s
                               ETA: 6786.8s

################################################################################
                     [1m Learning iteration 1373/4000 [0m

                       Computation: 3203 steps/s (collection: 0.507s, learning 2.050s)
               Value function loss: 58949.4139
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 5765.89
               Mean episode length: 306.55
                 Mean success rate: 55.50
                  Mean reward/step: 19.15
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 2.56s
                        Total time: 3548.33s
                               ETA: 6784.2s

################################################################################
                     [1m Learning iteration 1374/4000 [0m

                       Computation: 3221 steps/s (collection: 0.488s, learning 2.055s)
               Value function loss: 71848.2395
                    Surrogate loss: 0.0132
             Mean action noise std: 0.92
                       Mean reward: 5688.75
               Mean episode length: 298.31
                 Mean success rate: 54.00
                  Mean reward/step: 19.47
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11264000
                    Iteration time: 2.54s
                        Total time: 3550.87s
                               ETA: 6781.5s

################################################################################
                     [1m Learning iteration 1375/4000 [0m

                       Computation: 3127 steps/s (collection: 0.548s, learning 2.071s)
               Value function loss: 70922.2322
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 5637.11
               Mean episode length: 298.40
                 Mean success rate: 53.50
                  Mean reward/step: 19.77
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 2.62s
                        Total time: 3553.49s
                               ETA: 6779.0s

################################################################################
                     [1m Learning iteration 1376/4000 [0m

                       Computation: 3235 steps/s (collection: 0.504s, learning 2.028s)
               Value function loss: 80577.1653
                    Surrogate loss: 0.0114
             Mean action noise std: 0.92
                       Mean reward: 5686.36
               Mean episode length: 294.89
                 Mean success rate: 55.00
                  Mean reward/step: 19.05
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11280384
                    Iteration time: 2.53s
                        Total time: 3556.02s
                               ETA: 6776.3s

################################################################################
                     [1m Learning iteration 1377/4000 [0m

                       Computation: 3186 steps/s (collection: 0.528s, learning 2.043s)
               Value function loss: 59783.3222
                    Surrogate loss: 0.0110
             Mean action noise std: 0.92
                       Mean reward: 5565.40
               Mean episode length: 294.25
                 Mean success rate: 54.00
                  Mean reward/step: 19.14
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 2.57s
                        Total time: 3558.59s
                               ETA: 6773.7s

################################################################################
                     [1m Learning iteration 1378/4000 [0m

                       Computation: 3217 steps/s (collection: 0.495s, learning 2.051s)
               Value function loss: 85644.4399
                    Surrogate loss: 0.0118
             Mean action noise std: 0.92
                       Mean reward: 5705.50
               Mean episode length: 300.70
                 Mean success rate: 55.00
                  Mean reward/step: 19.37
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11296768
                    Iteration time: 2.55s
                        Total time: 3561.14s
                               ETA: 6771.1s

################################################################################
                     [1m Learning iteration 1379/4000 [0m

                       Computation: 3293 steps/s (collection: 0.460s, learning 2.027s)
               Value function loss: 110519.1096
                    Surrogate loss: 0.0112
             Mean action noise std: 0.92
                       Mean reward: 5986.88
               Mean episode length: 315.57
                 Mean success rate: 57.50
                  Mean reward/step: 19.48
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 2.49s
                        Total time: 3563.63s
                               ETA: 6768.3s

################################################################################
                     [1m Learning iteration 1380/4000 [0m

                       Computation: 3318 steps/s (collection: 0.444s, learning 2.024s)
               Value function loss: 80586.8319
                    Surrogate loss: 0.0113
             Mean action noise std: 0.92
                       Mean reward: 5976.26
               Mean episode length: 313.74
                 Mean success rate: 56.50
                  Mean reward/step: 18.27
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11313152
                    Iteration time: 2.47s
                        Total time: 3566.10s
                               ETA: 6765.5s

################################################################################
                     [1m Learning iteration 1381/4000 [0m

                       Computation: 3229 steps/s (collection: 0.501s, learning 2.036s)
               Value function loss: 55664.7646
                    Surrogate loss: 0.0163
             Mean action noise std: 0.92
                       Mean reward: 5943.87
               Mean episode length: 310.51
                 Mean success rate: 57.00
                  Mean reward/step: 18.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 2.54s
                        Total time: 3568.63s
                               ETA: 6762.8s

################################################################################
                     [1m Learning iteration 1382/4000 [0m

                       Computation: 3260 steps/s (collection: 0.437s, learning 2.075s)
               Value function loss: 79994.5274
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 6114.39
               Mean episode length: 313.95
                 Mean success rate: 57.00
                  Mean reward/step: 18.83
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11329536
                    Iteration time: 2.51s
                        Total time: 3571.15s
                               ETA: 6760.1s

################################################################################
                     [1m Learning iteration 1383/4000 [0m

                       Computation: 3221 steps/s (collection: 0.510s, learning 2.033s)
               Value function loss: 51540.4163
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 5905.78
               Mean episode length: 305.36
                 Mean success rate: 54.50
                  Mean reward/step: 19.20
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 2.54s
                        Total time: 3573.69s
                               ETA: 6757.5s

################################################################################
                     [1m Learning iteration 1384/4000 [0m

                       Computation: 3225 steps/s (collection: 0.476s, learning 2.064s)
               Value function loss: 57000.4870
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 5886.68
               Mean episode length: 303.04
                 Mean success rate: 53.50
                  Mean reward/step: 19.44
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11345920
                    Iteration time: 2.54s
                        Total time: 3576.23s
                               ETA: 6754.8s

################################################################################
                     [1m Learning iteration 1385/4000 [0m

                       Computation: 3279 steps/s (collection: 0.463s, learning 2.035s)
               Value function loss: 89660.3154
                    Surrogate loss: 0.0113
             Mean action noise std: 0.92
                       Mean reward: 5450.13
               Mean episode length: 286.17
                 Mean success rate: 49.50
                  Mean reward/step: 19.08
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 2.50s
                        Total time: 3578.73s
                               ETA: 6752.1s

################################################################################
                     [1m Learning iteration 1386/4000 [0m

                       Computation: 3289 steps/s (collection: 0.465s, learning 2.025s)
               Value function loss: 62788.9860
                    Surrogate loss: 0.0124
             Mean action noise std: 0.92
                       Mean reward: 5087.72
               Mean episode length: 271.41
                 Mean success rate: 46.50
                  Mean reward/step: 19.13
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11362304
                    Iteration time: 2.49s
                        Total time: 3581.22s
                               ETA: 6749.3s

################################################################################
                     [1m Learning iteration 1387/4000 [0m

                       Computation: 3258 steps/s (collection: 0.471s, learning 2.043s)
               Value function loss: 83178.7504
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 5040.82
               Mean episode length: 266.61
                 Mean success rate: 47.00
                  Mean reward/step: 18.96
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 2.51s
                        Total time: 3583.73s
                               ETA: 6746.6s

################################################################################
                     [1m Learning iteration 1388/4000 [0m

                       Computation: 3199 steps/s (collection: 0.495s, learning 2.065s)
               Value function loss: 86098.0743
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 5404.21
               Mean episode length: 281.26
                 Mean success rate: 50.00
                  Mean reward/step: 18.32
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11378688
                    Iteration time: 2.56s
                        Total time: 3586.29s
                               ETA: 6744.0s

################################################################################
                     [1m Learning iteration 1389/4000 [0m

                       Computation: 3237 steps/s (collection: 0.495s, learning 2.036s)
               Value function loss: 64992.3652
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 5142.19
               Mean episode length: 271.90
                 Mean success rate: 48.00
                  Mean reward/step: 18.24
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 2.53s
                        Total time: 3588.82s
                               ETA: 6741.3s

################################################################################
                     [1m Learning iteration 1390/4000 [0m

                       Computation: 3241 steps/s (collection: 0.484s, learning 2.043s)
               Value function loss: 65170.1021
                    Surrogate loss: 0.0165
             Mean action noise std: 0.92
                       Mean reward: 5256.54
               Mean episode length: 278.72
                 Mean success rate: 48.50
                  Mean reward/step: 18.20
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11395072
                    Iteration time: 2.53s
                        Total time: 3591.35s
                               ETA: 6738.6s

################################################################################
                     [1m Learning iteration 1391/4000 [0m

                       Computation: 3190 steps/s (collection: 0.545s, learning 2.022s)
               Value function loss: 99564.6955
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 5503.31
               Mean episode length: 289.25
                 Mean success rate: 50.50
                  Mean reward/step: 17.90
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 2.57s
                        Total time: 3593.92s
                               ETA: 6736.0s

################################################################################
                     [1m Learning iteration 1392/4000 [0m

                       Computation: 3241 steps/s (collection: 0.469s, learning 2.059s)
               Value function loss: 74950.2079
                    Surrogate loss: 0.0162
             Mean action noise std: 0.92
                       Mean reward: 5536.89
               Mean episode length: 291.00
                 Mean success rate: 50.50
                  Mean reward/step: 17.27
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11411456
                    Iteration time: 2.53s
                        Total time: 3596.44s
                               ETA: 6733.3s

################################################################################
                     [1m Learning iteration 1393/4000 [0m

                       Computation: 3192 steps/s (collection: 0.464s, learning 2.102s)
               Value function loss: 63217.0641
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 5366.39
               Mean episode length: 284.17
                 Mean success rate: 49.00
                  Mean reward/step: 17.76
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 2.57s
                        Total time: 3599.01s
                               ETA: 6730.7s

################################################################################
                     [1m Learning iteration 1394/4000 [0m

                       Computation: 3209 steps/s (collection: 0.471s, learning 2.082s)
               Value function loss: 99057.1675
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 5482.54
               Mean episode length: 287.13
                 Mean success rate: 49.00
                  Mean reward/step: 18.37
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11427840
                    Iteration time: 2.55s
                        Total time: 3601.56s
                               ETA: 6728.1s

################################################################################
                     [1m Learning iteration 1395/4000 [0m

                       Computation: 3235 steps/s (collection: 0.449s, learning 2.083s)
               Value function loss: 105871.7502
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 5316.60
               Mean episode length: 281.26
                 Mean success rate: 48.00
                  Mean reward/step: 17.49
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 2.53s
                        Total time: 3604.09s
                               ETA: 6725.4s

################################################################################
                     [1m Learning iteration 1396/4000 [0m

                       Computation: 3269 steps/s (collection: 0.436s, learning 2.069s)
               Value function loss: 78943.1940
                    Surrogate loss: 0.0126
             Mean action noise std: 0.92
                       Mean reward: 5485.36
               Mean episode length: 290.72
                 Mean success rate: 50.50
                  Mean reward/step: 16.97
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11444224
                    Iteration time: 2.51s
                        Total time: 3606.60s
                               ETA: 6722.7s

################################################################################
                     [1m Learning iteration 1397/4000 [0m

                       Computation: 3235 steps/s (collection: 0.456s, learning 2.076s)
               Value function loss: 86907.1713
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 5557.36
               Mean episode length: 294.03
                 Mean success rate: 52.00
                  Mean reward/step: 17.32
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 2.53s
                        Total time: 3609.13s
                               ETA: 6720.0s

################################################################################
                     [1m Learning iteration 1398/4000 [0m

                       Computation: 3230 steps/s (collection: 0.454s, learning 2.083s)
               Value function loss: 53667.6677
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 5341.36
               Mean episode length: 295.59
                 Mean success rate: 51.00
                  Mean reward/step: 17.07
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11460608
                    Iteration time: 2.54s
                        Total time: 3611.67s
                               ETA: 6717.3s

################################################################################
                     [1m Learning iteration 1399/4000 [0m

                       Computation: 3196 steps/s (collection: 0.448s, learning 2.115s)
               Value function loss: 81944.3116
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 5198.72
               Mean episode length: 291.51
                 Mean success rate: 51.50
                  Mean reward/step: 17.43
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 2.56s
                        Total time: 3614.23s
                               ETA: 6714.7s

################################################################################
                     [1m Learning iteration 1400/4000 [0m

                       Computation: 3305 steps/s (collection: 0.441s, learning 2.037s)
               Value function loss: 63127.8519
                    Surrogate loss: 0.0127
             Mean action noise std: 0.92
                       Mean reward: 5258.32
               Mean episode length: 292.79
                 Mean success rate: 52.00
                  Mean reward/step: 16.45
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11476992
                    Iteration time: 2.48s
                        Total time: 3616.71s
                               ETA: 6711.9s

################################################################################
                     [1m Learning iteration 1401/4000 [0m

                       Computation: 3232 steps/s (collection: 0.462s, learning 2.072s)
               Value function loss: 82177.6198
                    Surrogate loss: 0.0108
             Mean action noise std: 0.92
                       Mean reward: 4675.83
               Mean episode length: 281.04
                 Mean success rate: 48.00
                  Mean reward/step: 15.97
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 2.53s
                        Total time: 3619.24s
                               ETA: 6709.3s

################################################################################
                     [1m Learning iteration 1402/4000 [0m

                       Computation: 3197 steps/s (collection: 0.513s, learning 2.049s)
               Value function loss: 72915.2493
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 4253.24
               Mean episode length: 262.85
                 Mean success rate: 44.00
                  Mean reward/step: 16.60
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 11493376
                    Iteration time: 2.56s
                        Total time: 3621.80s
                               ETA: 6706.7s

################################################################################
                     [1m Learning iteration 1403/4000 [0m

                       Computation: 3230 steps/s (collection: 0.438s, learning 2.098s)
               Value function loss: 90562.5049
                    Surrogate loss: 0.0180
             Mean action noise std: 0.92
                       Mean reward: 4217.89
               Mean episode length: 261.86
                 Mean success rate: 43.50
                  Mean reward/step: 16.76
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 2.54s
                        Total time: 3624.34s
                               ETA: 6704.0s

################################################################################
                     [1m Learning iteration 1404/4000 [0m

                       Computation: 3160 steps/s (collection: 0.468s, learning 2.124s)
               Value function loss: 59764.0729
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 4298.44
               Mean episode length: 258.32
                 Mean success rate: 43.50
                  Mean reward/step: 15.79
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11509760
                    Iteration time: 2.59s
                        Total time: 3626.93s
                               ETA: 6701.4s

################################################################################
                     [1m Learning iteration 1405/4000 [0m

                       Computation: 3072 steps/s (collection: 0.536s, learning 2.130s)
               Value function loss: 98787.6320
                    Surrogate loss: 0.0141
             Mean action noise std: 0.92
                       Mean reward: 4293.93
               Mean episode length: 255.90
                 Mean success rate: 41.50
                  Mean reward/step: 16.33
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 2.67s
                        Total time: 3629.60s
                               ETA: 6699.0s

################################################################################
                     [1m Learning iteration 1406/4000 [0m

                       Computation: 3105 steps/s (collection: 0.505s, learning 2.133s)
               Value function loss: 72513.1584
                    Surrogate loss: 0.0124
             Mean action noise std: 0.92
                       Mean reward: 4499.80
               Mean episode length: 264.61
                 Mean success rate: 44.50
                  Mean reward/step: 16.91
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11526144
                    Iteration time: 2.64s
                        Total time: 3632.24s
                               ETA: 6696.5s

################################################################################
                     [1m Learning iteration 1407/4000 [0m

                       Computation: 3052 steps/s (collection: 0.548s, learning 2.136s)
               Value function loss: 54197.8999
                    Surrogate loss: 0.0139
             Mean action noise std: 0.92
                       Mean reward: 4399.51
               Mean episode length: 262.25
                 Mean success rate: 44.00
                  Mean reward/step: 17.00
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 2.68s
                        Total time: 3634.92s
                               ETA: 6694.1s

################################################################################
                     [1m Learning iteration 1408/4000 [0m

                       Computation: 3118 steps/s (collection: 0.494s, learning 2.133s)
               Value function loss: 79960.8624
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 4952.80
               Mean episode length: 277.96
                 Mean success rate: 48.50
                  Mean reward/step: 17.14
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11542528
                    Iteration time: 2.63s
                        Total time: 3637.55s
                               ETA: 6691.6s

################################################################################
                     [1m Learning iteration 1409/4000 [0m

                       Computation: 3218 steps/s (collection: 0.465s, learning 2.080s)
               Value function loss: 72771.1729
                    Surrogate loss: 0.0196
             Mean action noise std: 0.92
                       Mean reward: 4865.89
               Mean episode length: 277.00
                 Mean success rate: 48.50
                  Mean reward/step: 17.77
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 2.55s
                        Total time: 3640.09s
                               ETA: 6689.0s

################################################################################
                     [1m Learning iteration 1410/4000 [0m

                       Computation: 3178 steps/s (collection: 0.488s, learning 2.090s)
               Value function loss: 68453.5977
                    Surrogate loss: 0.0184
             Mean action noise std: 0.92
                       Mean reward: 5258.36
               Mean episode length: 293.29
                 Mean success rate: 52.50
                  Mean reward/step: 17.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11558912
                    Iteration time: 2.58s
                        Total time: 3642.67s
                               ETA: 6686.4s

################################################################################
                     [1m Learning iteration 1411/4000 [0m

                       Computation: 3183 steps/s (collection: 0.487s, learning 2.087s)
               Value function loss: 85857.7760
                    Surrogate loss: 0.0172
             Mean action noise std: 0.92
                       Mean reward: 5247.06
               Mean episode length: 297.64
                 Mean success rate: 52.50
                  Mean reward/step: 16.68
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 2.57s
                        Total time: 3645.24s
                               ETA: 6683.8s

################################################################################
                     [1m Learning iteration 1412/4000 [0m

                       Computation: 3104 steps/s (collection: 0.516s, learning 2.122s)
               Value function loss: 44432.7454
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 5130.98
               Mean episode length: 292.39
                 Mean success rate: 52.50
                  Mean reward/step: 16.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11575296
                    Iteration time: 2.64s
                        Total time: 3647.88s
                               ETA: 6681.3s

################################################################################
                     [1m Learning iteration 1413/4000 [0m

                       Computation: 3165 steps/s (collection: 0.498s, learning 2.090s)
               Value function loss: 54831.6595
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 5093.49
               Mean episode length: 290.92
                 Mean success rate: 53.00
                  Mean reward/step: 17.25
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 2.59s
                        Total time: 3650.47s
                               ETA: 6678.8s

################################################################################
                     [1m Learning iteration 1414/4000 [0m

                       Computation: 3168 steps/s (collection: 0.496s, learning 2.089s)
               Value function loss: 43683.1116
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 5193.00
               Mean episode length: 296.89
                 Mean success rate: 55.00
                  Mean reward/step: 17.62
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11591680
                    Iteration time: 2.59s
                        Total time: 3653.05s
                               ETA: 6676.2s

################################################################################
                     [1m Learning iteration 1415/4000 [0m

                       Computation: 3118 steps/s (collection: 0.469s, learning 2.158s)
               Value function loss: 57417.7061
                    Surrogate loss: 0.0170
             Mean action noise std: 0.92
                       Mean reward: 5102.37
               Mean episode length: 295.19
                 Mean success rate: 53.50
                  Mean reward/step: 18.35
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 2.63s
                        Total time: 3655.68s
                               ETA: 6673.7s

################################################################################
                     [1m Learning iteration 1416/4000 [0m

                       Computation: 3265 steps/s (collection: 0.492s, learning 2.016s)
               Value function loss: 95425.2494
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 5194.98
               Mean episode length: 303.39
                 Mean success rate: 55.50
                  Mean reward/step: 17.11
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 11608064
                    Iteration time: 2.51s
                        Total time: 3658.19s
                               ETA: 6671.0s

################################################################################
                     [1m Learning iteration 1417/4000 [0m

                       Computation: 3264 steps/s (collection: 0.491s, learning 2.018s)
               Value function loss: 89612.9504
                    Surrogate loss: 0.0156
             Mean action noise std: 0.92
                       Mean reward: 5693.49
               Mean episode length: 320.45
                 Mean success rate: 59.00
                  Mean reward/step: 15.88
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 2.51s
                        Total time: 3660.70s
                               ETA: 6668.3s

################################################################################
                     [1m Learning iteration 1418/4000 [0m

                       Computation: 3255 steps/s (collection: 0.490s, learning 2.026s)
               Value function loss: 96647.0927
                    Surrogate loss: 0.0107
             Mean action noise std: 0.92
                       Mean reward: 5463.93
               Mean episode length: 310.78
                 Mean success rate: 57.50
                  Mean reward/step: 15.56
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 11624448
                    Iteration time: 2.52s
                        Total time: 3663.22s
                               ETA: 6665.6s

################################################################################
                     [1m Learning iteration 1419/4000 [0m

                       Computation: 3225 steps/s (collection: 0.497s, learning 2.043s)
               Value function loss: 66183.4904
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 5505.70
               Mean episode length: 316.57
                 Mean success rate: 57.50
                  Mean reward/step: 14.84
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 2.54s
                        Total time: 3665.76s
                               ETA: 6662.9s

################################################################################
                     [1m Learning iteration 1420/4000 [0m

                       Computation: 3180 steps/s (collection: 0.524s, learning 2.052s)
               Value function loss: 56131.6576
                    Surrogate loss: 0.0172
             Mean action noise std: 0.92
                       Mean reward: 5581.94
               Mean episode length: 327.25
                 Mean success rate: 59.50
                  Mean reward/step: 14.89
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11640832
                    Iteration time: 2.58s
                        Total time: 3668.33s
                               ETA: 6660.3s

################################################################################
                     [1m Learning iteration 1421/4000 [0m

                       Computation: 3249 steps/s (collection: 0.477s, learning 2.044s)
               Value function loss: 76582.2878
                    Surrogate loss: 0.0176
             Mean action noise std: 0.92
                       Mean reward: 5302.93
               Mean episode length: 313.94
                 Mean success rate: 56.00
                  Mean reward/step: 15.01
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 2.52s
                        Total time: 3670.85s
                               ETA: 6657.6s

################################################################################
                     [1m Learning iteration 1422/4000 [0m

                       Computation: 3296 steps/s (collection: 0.438s, learning 2.047s)
               Value function loss: 50904.2964
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 5113.27
               Mean episode length: 313.30
                 Mean success rate: 54.50
                  Mean reward/step: 15.97
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11657216
                    Iteration time: 2.49s
                        Total time: 3673.34s
                               ETA: 6654.9s

################################################################################
                     [1m Learning iteration 1423/4000 [0m

                       Computation: 3191 steps/s (collection: 0.498s, learning 2.069s)
               Value function loss: 55237.7741
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 4632.42
               Mean episode length: 289.38
                 Mean success rate: 48.50
                  Mean reward/step: 16.58
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 2.57s
                        Total time: 3675.90s
                               ETA: 6652.3s

################################################################################
                     [1m Learning iteration 1424/4000 [0m

                       Computation: 3285 steps/s (collection: 0.458s, learning 2.036s)
               Value function loss: 56124.7154
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 4363.27
               Mean episode length: 281.32
                 Mean success rate: 46.50
                  Mean reward/step: 17.26
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11673600
                    Iteration time: 2.49s
                        Total time: 3678.40s
                               ETA: 6649.5s

################################################################################
                     [1m Learning iteration 1425/4000 [0m

                       Computation: 3166 steps/s (collection: 0.515s, learning 2.071s)
               Value function loss: 81846.7644
                    Surrogate loss: 0.0106
             Mean action noise std: 0.92
                       Mean reward: 4508.42
               Mean episode length: 282.65
                 Mean success rate: 48.00
                  Mean reward/step: 17.48
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 2.59s
                        Total time: 3680.99s
                               ETA: 6646.9s

################################################################################
                     [1m Learning iteration 1426/4000 [0m

                       Computation: 3262 steps/s (collection: 0.477s, learning 2.034s)
               Value function loss: 57657.7859
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 4173.89
               Mean episode length: 270.17
                 Mean success rate: 46.50
                  Mean reward/step: 17.17
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 11689984
                    Iteration time: 2.51s
                        Total time: 3683.50s
                               ETA: 6644.2s

################################################################################
                     [1m Learning iteration 1427/4000 [0m

                       Computation: 3190 steps/s (collection: 0.505s, learning 2.063s)
               Value function loss: 69042.1036
                    Surrogate loss: 0.0123
             Mean action noise std: 0.92
                       Mean reward: 4339.16
               Mean episode length: 270.71
                 Mean success rate: 46.50
                  Mean reward/step: 16.87
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 2.57s
                        Total time: 3686.06s
                               ETA: 6641.6s

################################################################################
                     [1m Learning iteration 1428/4000 [0m

                       Computation: 3192 steps/s (collection: 0.467s, learning 2.099s)
               Value function loss: 49266.6925
                    Surrogate loss: 0.0152
             Mean action noise std: 0.92
                       Mean reward: 4044.60
               Mean episode length: 255.89
                 Mean success rate: 44.00
                  Mean reward/step: 17.06
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11706368
                    Iteration time: 2.57s
                        Total time: 3688.63s
                               ETA: 6639.0s

################################################################################
                     [1m Learning iteration 1429/4000 [0m

                       Computation: 3156 steps/s (collection: 0.498s, learning 2.097s)
               Value function loss: 62345.6525
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 4132.53
               Mean episode length: 254.82
                 Mean success rate: 45.00
                  Mean reward/step: 17.11
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 2.59s
                        Total time: 3691.22s
                               ETA: 6636.5s

################################################################################
                     [1m Learning iteration 1430/4000 [0m

                       Computation: 3270 steps/s (collection: 0.460s, learning 2.045s)
               Value function loss: 52592.3651
                    Surrogate loss: 0.0137
             Mean action noise std: 0.92
                       Mean reward: 3781.03
               Mean episode length: 244.28
                 Mean success rate: 43.50
                  Mean reward/step: 16.86
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 11722752
                    Iteration time: 2.50s
                        Total time: 3693.73s
                               ETA: 6633.7s

################################################################################
                     [1m Learning iteration 1431/4000 [0m

                       Computation: 3207 steps/s (collection: 0.513s, learning 2.041s)
               Value function loss: 54944.9605
                    Surrogate loss: 0.0102
             Mean action noise std: 0.92
                       Mean reward: 3758.69
               Mean episode length: 241.33
                 Mean success rate: 42.50
                  Mean reward/step: 17.14
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 2.55s
                        Total time: 3696.28s
                               ETA: 6631.1s

################################################################################
                     [1m Learning iteration 1432/4000 [0m

                       Computation: 3238 steps/s (collection: 0.480s, learning 2.050s)
               Value function loss: 97872.9738
                    Surrogate loss: 0.0115
             Mean action noise std: 0.92
                       Mean reward: 3869.29
               Mean episode length: 242.40
                 Mean success rate: 43.00
                  Mean reward/step: 16.50
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 11739136
                    Iteration time: 2.53s
                        Total time: 3698.81s
                               ETA: 6628.4s

################################################################################
                     [1m Learning iteration 1433/4000 [0m

                       Computation: 3220 steps/s (collection: 0.476s, learning 2.068s)
               Value function loss: 51885.6562
                    Surrogate loss: 0.0120
             Mean action noise std: 0.92
                       Mean reward: 3794.93
               Mean episode length: 246.01
                 Mean success rate: 43.00
                  Mean reward/step: 16.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 2.54s
                        Total time: 3701.36s
                               ETA: 6625.8s

################################################################################
                     [1m Learning iteration 1434/4000 [0m

                       Computation: 3182 steps/s (collection: 0.490s, learning 2.084s)
               Value function loss: 97814.7812
                    Surrogate loss: 0.0103
             Mean action noise std: 0.92
                       Mean reward: 4507.99
               Mean episode length: 276.45
                 Mean success rate: 49.00
                  Mean reward/step: 16.73
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11755520
                    Iteration time: 2.57s
                        Total time: 3703.93s
                               ETA: 6623.2s

################################################################################
                     [1m Learning iteration 1435/4000 [0m

                       Computation: 3202 steps/s (collection: 0.464s, learning 2.095s)
               Value function loss: 63728.3327
                    Surrogate loss: 0.0126
             Mean action noise std: 0.92
                       Mean reward: 4844.86
               Mean episode length: 287.73
                 Mean success rate: 52.50
                  Mean reward/step: 16.96
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 2.56s
                        Total time: 3706.49s
                               ETA: 6620.6s

################################################################################
                     [1m Learning iteration 1436/4000 [0m

                       Computation: 3210 steps/s (collection: 0.486s, learning 2.066s)
               Value function loss: 75206.2034
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 5205.50
               Mean episode length: 303.95
                 Mean success rate: 54.50
                  Mean reward/step: 17.38
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11771904
                    Iteration time: 2.55s
                        Total time: 3709.04s
                               ETA: 6617.9s

################################################################################
                     [1m Learning iteration 1437/4000 [0m

                       Computation: 3193 steps/s (collection: 0.508s, learning 2.057s)
               Value function loss: 62825.7460
                    Surrogate loss: 0.0112
             Mean action noise std: 0.92
                       Mean reward: 5575.38
               Mean episode length: 318.25
                 Mean success rate: 58.00
                  Mean reward/step: 17.31
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 2.57s
                        Total time: 3711.61s
                               ETA: 6615.3s

################################################################################
                     [1m Learning iteration 1438/4000 [0m

                       Computation: 3185 steps/s (collection: 0.512s, learning 2.060s)
               Value function loss: 78863.7330
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 5956.63
               Mean episode length: 333.72
                 Mean success rate: 62.00
                  Mean reward/step: 17.93
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11788288
                    Iteration time: 2.57s
                        Total time: 3714.18s
                               ETA: 6612.7s

################################################################################
                     [1m Learning iteration 1439/4000 [0m

                       Computation: 3193 steps/s (collection: 0.458s, learning 2.107s)
               Value function loss: 46023.7082
                    Surrogate loss: 0.0128
             Mean action noise std: 0.92
                       Mean reward: 5838.86
               Mean episode length: 325.68
                 Mean success rate: 60.50
                  Mean reward/step: 18.41
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 2.56s
                        Total time: 3716.74s
                               ETA: 6610.1s

################################################################################
                     [1m Learning iteration 1440/4000 [0m

                       Computation: 3216 steps/s (collection: 0.473s, learning 2.074s)
               Value function loss: 49253.8056
                    Surrogate loss: 0.0132
             Mean action noise std: 0.92
                       Mean reward: 5742.99
               Mean episode length: 322.43
                 Mean success rate: 60.00
                  Mean reward/step: 19.31
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11804672
                    Iteration time: 2.55s
                        Total time: 3719.29s
                               ETA: 6607.5s

################################################################################
                     [1m Learning iteration 1441/4000 [0m

                       Computation: 3258 steps/s (collection: 0.435s, learning 2.079s)
               Value function loss: 67491.5008
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 5779.54
               Mean episode length: 320.03
                 Mean success rate: 61.00
                  Mean reward/step: 19.34
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 2.51s
                        Total time: 3721.80s
                               ETA: 6604.8s

################################################################################
                     [1m Learning iteration 1442/4000 [0m

                       Computation: 3322 steps/s (collection: 0.432s, learning 2.033s)
               Value function loss: 65689.0770
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 5324.25
               Mean episode length: 302.14
                 Mean success rate: 56.50
                  Mean reward/step: 19.03
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11821056
                    Iteration time: 2.47s
                        Total time: 3724.27s
                               ETA: 6602.0s

################################################################################
                     [1m Learning iteration 1443/4000 [0m

                       Computation: 3157 steps/s (collection: 0.503s, learning 2.091s)
               Value function loss: 78926.5568
                    Surrogate loss: 0.0157
             Mean action noise std: 0.92
                       Mean reward: 5281.55
               Mean episode length: 295.01
                 Mean success rate: 55.50
                  Mean reward/step: 18.49
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 2.59s
                        Total time: 3726.86s
                               ETA: 6599.4s

################################################################################
                     [1m Learning iteration 1444/4000 [0m

                       Computation: 3227 steps/s (collection: 0.473s, learning 2.065s)
               Value function loss: 60272.5282
                    Surrogate loss: 0.0148
             Mean action noise std: 0.92
                       Mean reward: 5674.90
               Mean episode length: 311.37
                 Mean success rate: 60.00
                  Mean reward/step: 17.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11837440
                    Iteration time: 2.54s
                        Total time: 3729.40s
                               ETA: 6596.8s

################################################################################
                     [1m Learning iteration 1445/4000 [0m

                       Computation: 3179 steps/s (collection: 0.487s, learning 2.089s)
               Value function loss: 77325.1276
                    Surrogate loss: 0.0120
             Mean action noise std: 0.92
                       Mean reward: 5762.57
               Mean episode length: 315.31
                 Mean success rate: 61.50
                  Mean reward/step: 17.58
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 2.58s
                        Total time: 3731.98s
                               ETA: 6594.2s

################################################################################
                     [1m Learning iteration 1446/4000 [0m

                       Computation: 3162 steps/s (collection: 0.502s, learning 2.088s)
               Value function loss: 82364.3083
                    Surrogate loss: 0.0126
             Mean action noise std: 0.92
                       Mean reward: 5823.60
               Mean episode length: 314.30
                 Mean success rate: 61.00
                  Mean reward/step: 16.28
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 11853824
                    Iteration time: 2.59s
                        Total time: 3734.57s
                               ETA: 6591.6s

################################################################################
                     [1m Learning iteration 1447/4000 [0m

                       Computation: 3174 steps/s (collection: 0.499s, learning 2.082s)
               Value function loss: 59596.2177
                    Surrogate loss: 0.0182
             Mean action noise std: 0.92
                       Mean reward: 5300.47
               Mean episode length: 299.05
                 Mean success rate: 56.00
                  Mean reward/step: 16.06
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 2.58s
                        Total time: 3737.15s
                               ETA: 6589.0s

################################################################################
                     [1m Learning iteration 1448/4000 [0m

                       Computation: 3087 steps/s (collection: 0.549s, learning 2.105s)
               Value function loss: 67414.2188
                    Surrogate loss: 0.0182
             Mean action noise std: 0.92
                       Mean reward: 5548.68
               Mean episode length: 316.06
                 Mean success rate: 57.00
                  Mean reward/step: 16.04
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11870208
                    Iteration time: 2.65s
                        Total time: 3739.80s
                               ETA: 6586.6s

################################################################################
                     [1m Learning iteration 1449/4000 [0m

                       Computation: 3220 steps/s (collection: 0.460s, learning 2.083s)
               Value function loss: 88019.2366
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 5412.91
               Mean episode length: 310.49
                 Mean success rate: 56.00
                  Mean reward/step: 16.25
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 2.54s
                        Total time: 3742.35s
                               ETA: 6583.9s

################################################################################
                     [1m Learning iteration 1450/4000 [0m

                       Computation: 3176 steps/s (collection: 0.489s, learning 2.090s)
               Value function loss: 91807.4632
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 5457.02
               Mean episode length: 316.73
                 Mean success rate: 56.00
                  Mean reward/step: 16.59
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11886592
                    Iteration time: 2.58s
                        Total time: 3744.93s
                               ETA: 6581.4s

################################################################################
                     [1m Learning iteration 1451/4000 [0m

                       Computation: 3248 steps/s (collection: 0.476s, learning 2.046s)
               Value function loss: 58509.8230
                    Surrogate loss: 0.0157
             Mean action noise std: 0.92
                       Mean reward: 5319.72
               Mean episode length: 311.18
                 Mean success rate: 54.50
                  Mean reward/step: 17.39
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 2.52s
                        Total time: 3747.45s
                               ETA: 6578.7s

################################################################################
                     [1m Learning iteration 1452/4000 [0m

                       Computation: 3110 steps/s (collection: 0.523s, learning 2.110s)
               Value function loss: 93363.6152
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 5242.53
               Mean episode length: 303.51
                 Mean success rate: 53.00
                  Mean reward/step: 18.45
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11902976
                    Iteration time: 2.63s
                        Total time: 3750.08s
                               ETA: 6576.2s

################################################################################
                     [1m Learning iteration 1453/4000 [0m

                       Computation: 3164 steps/s (collection: 0.463s, learning 2.125s)
               Value function loss: 70962.8959
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 5110.79
               Mean episode length: 297.48
                 Mean success rate: 51.00
                  Mean reward/step: 19.26
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 2.59s
                        Total time: 3752.67s
                               ETA: 6573.6s

################################################################################
                     [1m Learning iteration 1454/4000 [0m

                       Computation: 3238 steps/s (collection: 0.451s, learning 2.079s)
               Value function loss: 72550.5058
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 5638.45
               Mean episode length: 318.86
                 Mean success rate: 56.00
                  Mean reward/step: 19.20
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11919360
                    Iteration time: 2.53s
                        Total time: 3755.20s
                               ETA: 6571.0s

################################################################################
                     [1m Learning iteration 1455/4000 [0m

                       Computation: 3200 steps/s (collection: 0.478s, learning 2.081s)
               Value function loss: 49434.3863
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 5654.76
               Mean episode length: 317.67
                 Mean success rate: 55.50
                  Mean reward/step: 18.88
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 2.56s
                        Total time: 3757.76s
                               ETA: 6568.3s

################################################################################
                     [1m Learning iteration 1456/4000 [0m

                       Computation: 3148 steps/s (collection: 0.517s, learning 2.085s)
               Value function loss: 50422.9710
                    Surrogate loss: 0.0169
             Mean action noise std: 0.92
                       Mean reward: 5541.39
               Mean episode length: 309.08
                 Mean success rate: 54.50
                  Mean reward/step: 19.27
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11935744
                    Iteration time: 2.60s
                        Total time: 3760.36s
                               ETA: 6565.8s

################################################################################
                     [1m Learning iteration 1457/4000 [0m

                       Computation: 3187 steps/s (collection: 0.511s, learning 2.059s)
               Value function loss: 35546.6341
                    Surrogate loss: 0.0193
             Mean action noise std: 0.92
                       Mean reward: 5295.87
               Mean episode length: 301.88
                 Mean success rate: 53.00
                  Mean reward/step: 20.35
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 2.57s
                        Total time: 3762.93s
                               ETA: 6563.2s

################################################################################
                     [1m Learning iteration 1458/4000 [0m

                       Computation: 3231 steps/s (collection: 0.506s, learning 2.029s)
               Value function loss: 76612.8062
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 5228.29
               Mean episode length: 299.75
                 Mean success rate: 52.50
                  Mean reward/step: 20.43
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11952128
                    Iteration time: 2.54s
                        Total time: 3765.46s
                               ETA: 6560.5s

################################################################################
                     [1m Learning iteration 1459/4000 [0m

                       Computation: 3154 steps/s (collection: 0.522s, learning 2.075s)
               Value function loss: 98878.5414
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 5348.80
               Mean episode length: 301.32
                 Mean success rate: 53.00
                  Mean reward/step: 20.36
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 2.60s
                        Total time: 3768.06s
                               ETA: 6558.0s

################################################################################
                     [1m Learning iteration 1460/4000 [0m

                       Computation: 3122 steps/s (collection: 0.537s, learning 2.086s)
               Value function loss: 35996.5870
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 5214.65
               Mean episode length: 296.05
                 Mean success rate: 51.50
                  Mean reward/step: 20.32
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 11968512
                    Iteration time: 2.62s
                        Total time: 3770.68s
                               ETA: 6555.5s

################################################################################
                     [1m Learning iteration 1461/4000 [0m

                       Computation: 3199 steps/s (collection: 0.516s, learning 2.044s)
               Value function loss: 122549.9802
                    Surrogate loss: 0.0172
             Mean action noise std: 0.91
                       Mean reward: 5511.57
               Mean episode length: 309.13
                 Mean success rate: 54.50
                  Mean reward/step: 20.45
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 2.56s
                        Total time: 3773.24s
                               ETA: 6552.9s

################################################################################
                     [1m Learning iteration 1462/4000 [0m

                       Computation: 3171 steps/s (collection: 0.537s, learning 2.046s)
               Value function loss: 107375.4228
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 5628.57
               Mean episode length: 315.24
                 Mean success rate: 57.00
                  Mean reward/step: 18.90
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 11984896
                    Iteration time: 2.58s
                        Total time: 3775.83s
                               ETA: 6550.3s

################################################################################
                     [1m Learning iteration 1463/4000 [0m

                       Computation: 3211 steps/s (collection: 0.501s, learning 2.050s)
               Value function loss: 125835.4755
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 6016.30
               Mean episode length: 325.27
                 Mean success rate: 59.50
                  Mean reward/step: 17.50
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 2.55s
                        Total time: 3778.38s
                               ETA: 6547.6s

################################################################################
                     [1m Learning iteration 1464/4000 [0m

                       Computation: 3175 steps/s (collection: 0.527s, learning 2.053s)
               Value function loss: 61238.6266
                    Surrogate loss: 0.0196
             Mean action noise std: 0.92
                       Mean reward: 6257.99
               Mean episode length: 336.69
                 Mean success rate: 61.50
                  Mean reward/step: 16.87
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12001280
                    Iteration time: 2.58s
                        Total time: 3780.96s
                               ETA: 6545.1s

################################################################################
                     [1m Learning iteration 1465/4000 [0m

                       Computation: 3191 steps/s (collection: 0.531s, learning 2.036s)
               Value function loss: 101094.1883
                    Surrogate loss: 0.0169
             Mean action noise std: 0.92
                       Mean reward: 6669.90
               Mean episode length: 350.83
                 Mean success rate: 64.00
                  Mean reward/step: 17.23
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 2.57s
                        Total time: 3783.53s
                               ETA: 6542.5s

################################################################################
                     [1m Learning iteration 1466/4000 [0m

                       Computation: 3200 steps/s (collection: 0.516s, learning 2.043s)
               Value function loss: 74516.9370
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 6756.24
               Mean episode length: 350.91
                 Mean success rate: 64.00
                  Mean reward/step: 17.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12017664
                    Iteration time: 2.56s
                        Total time: 3786.09s
                               ETA: 6539.8s

################################################################################
                     [1m Learning iteration 1467/4000 [0m

                       Computation: 3158 steps/s (collection: 0.503s, learning 2.091s)
               Value function loss: 63910.6771
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 6814.59
               Mean episode length: 350.99
                 Mean success rate: 65.50
                  Mean reward/step: 17.86
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 2.59s
                        Total time: 3788.68s
                               ETA: 6537.3s

################################################################################
                     [1m Learning iteration 1468/4000 [0m

                       Computation: 3206 steps/s (collection: 0.520s, learning 2.035s)
               Value function loss: 70661.2818
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 6569.70
               Mean episode length: 344.39
                 Mean success rate: 63.50
                  Mean reward/step: 17.89
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12034048
                    Iteration time: 2.55s
                        Total time: 3791.23s
                               ETA: 6534.7s

################################################################################
                     [1m Learning iteration 1469/4000 [0m

                       Computation: 3175 steps/s (collection: 0.529s, learning 2.051s)
               Value function loss: 59957.4557
                    Surrogate loss: 0.0165
             Mean action noise std: 0.91
                       Mean reward: 6767.41
               Mean episode length: 355.45
                 Mean success rate: 65.50
                  Mean reward/step: 17.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 2.58s
                        Total time: 3793.81s
                               ETA: 6532.1s

################################################################################
                     [1m Learning iteration 1470/4000 [0m

                       Computation: 3153 steps/s (collection: 0.498s, learning 2.100s)
               Value function loss: 59041.0420
                    Surrogate loss: 0.0184
             Mean action noise std: 0.91
                       Mean reward: 6674.48
               Mean episode length: 348.56
                 Mean success rate: 64.50
                  Mean reward/step: 18.14
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12050432
                    Iteration time: 2.60s
                        Total time: 3796.41s
                               ETA: 6529.5s

################################################################################
                     [1m Learning iteration 1471/4000 [0m

                       Computation: 3030 steps/s (collection: 0.531s, learning 2.173s)
               Value function loss: 59140.8285
                    Surrogate loss: 0.0177
             Mean action noise std: 0.91
                       Mean reward: 6796.61
               Mean episode length: 352.13
                 Mean success rate: 66.00
                  Mean reward/step: 18.29
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 2.70s
                        Total time: 3799.12s
                               ETA: 6527.1s

################################################################################
                     [1m Learning iteration 1472/4000 [0m

                       Computation: 3211 steps/s (collection: 0.478s, learning 2.073s)
               Value function loss: 73570.3060
                    Surrogate loss: 0.0128
             Mean action noise std: 0.92
                       Mean reward: 6647.71
               Mean episode length: 348.98
                 Mean success rate: 65.00
                  Mean reward/step: 18.78
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12066816
                    Iteration time: 2.55s
                        Total time: 3801.67s
                               ETA: 6524.5s

################################################################################
                     [1m Learning iteration 1473/4000 [0m

                       Computation: 3211 steps/s (collection: 0.502s, learning 2.049s)
               Value function loss: 76239.0775
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 6600.89
               Mean episode length: 348.54
                 Mean success rate: 65.50
                  Mean reward/step: 19.54
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 2.55s
                        Total time: 3804.22s
                               ETA: 6521.9s

################################################################################
                     [1m Learning iteration 1474/4000 [0m

                       Computation: 3176 steps/s (collection: 0.506s, learning 2.073s)
               Value function loss: 49432.0667
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 6193.65
               Mean episode length: 336.19
                 Mean success rate: 62.00
                  Mean reward/step: 19.10
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12083200
                    Iteration time: 2.58s
                        Total time: 3806.80s
                               ETA: 6519.3s

################################################################################
                     [1m Learning iteration 1475/4000 [0m

                       Computation: 3259 steps/s (collection: 0.459s, learning 2.054s)
               Value function loss: 89026.6254
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 6252.17
               Mean episode length: 341.75
                 Mean success rate: 63.50
                  Mean reward/step: 18.61
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 2.51s
                        Total time: 3809.31s
                               ETA: 6516.6s

################################################################################
                     [1m Learning iteration 1476/4000 [0m

                       Computation: 3195 steps/s (collection: 0.480s, learning 2.084s)
               Value function loss: 55594.4256
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 6116.62
               Mean episode length: 335.00
                 Mean success rate: 62.00
                  Mean reward/step: 18.76
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12099584
                    Iteration time: 2.56s
                        Total time: 3811.87s
                               ETA: 6514.0s

################################################################################
                     [1m Learning iteration 1477/4000 [0m

                       Computation: 3166 steps/s (collection: 0.559s, learning 2.028s)
               Value function loss: 108318.2002
                    Surrogate loss: 0.0173
             Mean action noise std: 0.91
                       Mean reward: 6263.23
               Mean episode length: 340.81
                 Mean success rate: 63.00
                  Mean reward/step: 18.04
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 2.59s
                        Total time: 3814.46s
                               ETA: 6511.4s

################################################################################
                     [1m Learning iteration 1478/4000 [0m

                       Computation: 3263 steps/s (collection: 0.487s, learning 2.023s)
               Value function loss: 97850.4352
                    Surrogate loss: 0.0148
             Mean action noise std: 0.92
                       Mean reward: 6318.95
               Mean episode length: 344.47
                 Mean success rate: 64.00
                  Mean reward/step: 17.31
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 12115968
                    Iteration time: 2.51s
                        Total time: 3816.97s
                               ETA: 6508.7s

################################################################################
                     [1m Learning iteration 1479/4000 [0m

                       Computation: 3185 steps/s (collection: 0.536s, learning 2.035s)
               Value function loss: 74611.8881
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 6260.42
               Mean episode length: 339.94
                 Mean success rate: 61.50
                  Mean reward/step: 16.29
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 2.57s
                        Total time: 3819.54s
                               ETA: 6506.1s

################################################################################
                     [1m Learning iteration 1480/4000 [0m

                       Computation: 3310 steps/s (collection: 0.449s, learning 2.025s)
               Value function loss: 64469.4314
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 6364.07
               Mean episode length: 344.33
                 Mean success rate: 62.50
                  Mean reward/step: 17.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12132352
                    Iteration time: 2.47s
                        Total time: 3822.02s
                               ETA: 6503.4s

################################################################################
                     [1m Learning iteration 1481/4000 [0m

                       Computation: 3288 steps/s (collection: 0.452s, learning 2.039s)
               Value function loss: 92785.3327
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 6195.77
               Mean episode length: 336.70
                 Mean success rate: 61.50
                  Mean reward/step: 17.76
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 2.49s
                        Total time: 3824.51s
                               ETA: 6500.6s

################################################################################
                     [1m Learning iteration 1482/4000 [0m

                       Computation: 3254 steps/s (collection: 0.490s, learning 2.026s)
               Value function loss: 63151.4995
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 6143.45
               Mean episode length: 334.94
                 Mean success rate: 60.50
                  Mean reward/step: 18.13
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12148736
                    Iteration time: 2.52s
                        Total time: 3827.03s
                               ETA: 6497.9s

################################################################################
                     [1m Learning iteration 1483/4000 [0m

                       Computation: 3058 steps/s (collection: 0.559s, learning 2.119s)
               Value function loss: 81518.8389
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 6161.10
               Mean episode length: 335.57
                 Mean success rate: 59.50
                  Mean reward/step: 17.84
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 2.68s
                        Total time: 3829.70s
                               ETA: 6495.5s

################################################################################
                     [1m Learning iteration 1484/4000 [0m

                       Computation: 3062 steps/s (collection: 0.586s, learning 2.089s)
               Value function loss: 67948.9462
                    Surrogate loss: 0.0165
             Mean action noise std: 0.91
                       Mean reward: 5807.33
               Mean episode length: 325.32
                 Mean success rate: 57.50
                  Mean reward/step: 18.33
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12165120
                    Iteration time: 2.68s
                        Total time: 3832.38s
                               ETA: 6493.1s

################################################################################
                     [1m Learning iteration 1485/4000 [0m

                       Computation: 3131 steps/s (collection: 0.527s, learning 2.089s)
               Value function loss: 71731.1094
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 5802.28
               Mean episode length: 325.03
                 Mean success rate: 57.00
                  Mean reward/step: 18.39
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 2.62s
                        Total time: 3834.99s
                               ETA: 6490.6s

################################################################################
                     [1m Learning iteration 1486/4000 [0m

                       Computation: 3068 steps/s (collection: 0.552s, learning 2.118s)
               Value function loss: 67639.1497
                    Surrogate loss: 0.0160
             Mean action noise std: 0.91
                       Mean reward: 5523.36
               Mean episode length: 317.88
                 Mean success rate: 54.50
                  Mean reward/step: 17.90
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12181504
                    Iteration time: 2.67s
                        Total time: 3837.66s
                               ETA: 6488.2s

################################################################################
                     [1m Learning iteration 1487/4000 [0m

                       Computation: 3045 steps/s (collection: 0.604s, learning 2.085s)
               Value function loss: 43369.2406
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 5337.30
               Mean episode length: 309.50
                 Mean success rate: 52.50
                  Mean reward/step: 18.80
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 2.69s
                        Total time: 3840.35s
                               ETA: 6485.8s

################################################################################
                     [1m Learning iteration 1488/4000 [0m

                       Computation: 3066 steps/s (collection: 0.579s, learning 2.092s)
               Value function loss: 65006.0924
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 5245.52
               Mean episode length: 304.11
                 Mean success rate: 50.50
                  Mean reward/step: 19.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12197888
                    Iteration time: 2.67s
                        Total time: 3843.02s
                               ETA: 6483.3s

################################################################################
                     [1m Learning iteration 1489/4000 [0m

                       Computation: 3057 steps/s (collection: 0.582s, learning 2.097s)
               Value function loss: 66235.2071
                    Surrogate loss: 0.0160
             Mean action noise std: 0.91
                       Mean reward: 5138.92
               Mean episode length: 295.87
                 Mean success rate: 50.00
                  Mean reward/step: 19.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 2.68s
                        Total time: 3845.70s
                               ETA: 6480.9s

################################################################################
                     [1m Learning iteration 1490/4000 [0m

                       Computation: 3036 steps/s (collection: 0.595s, learning 2.103s)
               Value function loss: 82788.5401
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 5301.50
               Mean episode length: 298.64
                 Mean success rate: 51.50
                  Mean reward/step: 19.24
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12214272
                    Iteration time: 2.70s
                        Total time: 3848.40s
                               ETA: 6478.5s

################################################################################
                     [1m Learning iteration 1491/4000 [0m

                       Computation: 3070 steps/s (collection: 0.548s, learning 2.120s)
               Value function loss: 69042.2288
                    Surrogate loss: 0.0232
             Mean action noise std: 0.91
                       Mean reward: 5161.80
               Mean episode length: 298.81
                 Mean success rate: 51.50
                  Mean reward/step: 18.48
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 2.67s
                        Total time: 3851.07s
                               ETA: 6476.1s

################################################################################
                     [1m Learning iteration 1492/4000 [0m

                       Computation: 3080 steps/s (collection: 0.555s, learning 2.104s)
               Value function loss: 54288.0383
                    Surrogate loss: 0.0174
             Mean action noise std: 0.91
                       Mean reward: 5275.48
               Mean episode length: 305.70
                 Mean success rate: 52.00
                  Mean reward/step: 19.20
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12230656
                    Iteration time: 2.66s
                        Total time: 3853.73s
                               ETA: 6473.6s

################################################################################
                     [1m Learning iteration 1493/4000 [0m

                       Computation: 3048 steps/s (collection: 0.575s, learning 2.112s)
               Value function loss: 128995.2729
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 5716.08
               Mean episode length: 314.14
                 Mean success rate: 54.00
                  Mean reward/step: 18.10
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 2.69s
                        Total time: 3856.42s
                               ETA: 6471.2s

################################################################################
                     [1m Learning iteration 1494/4000 [0m

                       Computation: 3074 steps/s (collection: 0.564s, learning 2.101s)
               Value function loss: 81037.4688
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 5970.81
               Mean episode length: 322.98
                 Mean success rate: 56.50
                  Mean reward/step: 16.60
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 12247040
                    Iteration time: 2.66s
                        Total time: 3859.08s
                               ETA: 6468.8s

################################################################################
                     [1m Learning iteration 1495/4000 [0m

                       Computation: 3045 steps/s (collection: 0.593s, learning 2.096s)
               Value function loss: 74415.8004
                    Surrogate loss: 0.0248
             Mean action noise std: 0.91
                       Mean reward: 5966.34
               Mean episode length: 322.61
                 Mean success rate: 56.50
                  Mean reward/step: 16.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 2.69s
                        Total time: 3861.77s
                               ETA: 6466.4s

################################################################################
                     [1m Learning iteration 1496/4000 [0m

                       Computation: 2995 steps/s (collection: 0.627s, learning 2.107s)
               Value function loss: 82553.7877
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 6020.65
               Mean episode length: 320.89
                 Mean success rate: 56.00
                  Mean reward/step: 16.78
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12263424
                    Iteration time: 2.73s
                        Total time: 3864.51s
                               ETA: 6464.1s

################################################################################
                     [1m Learning iteration 1497/4000 [0m

                       Computation: 3071 steps/s (collection: 0.576s, learning 2.092s)
               Value function loss: 78945.9784
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 5865.81
               Mean episode length: 313.04
                 Mean success rate: 54.00
                  Mean reward/step: 16.11
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 2.67s
                        Total time: 3867.17s
                               ETA: 6461.6s

################################################################################
                     [1m Learning iteration 1498/4000 [0m

                       Computation: 3121 steps/s (collection: 0.548s, learning 2.076s)
               Value function loss: 66897.1998
                    Surrogate loss: 0.0167
             Mean action noise std: 0.91
                       Mean reward: 5983.88
               Mean episode length: 317.79
                 Mean success rate: 55.00
                  Mean reward/step: 16.58
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12279808
                    Iteration time: 2.62s
                        Total time: 3869.80s
                               ETA: 6459.1s

################################################################################
                     [1m Learning iteration 1499/4000 [0m

                       Computation: 3069 steps/s (collection: 0.571s, learning 2.098s)
               Value function loss: 92277.0321
                    Surrogate loss: 0.0184
             Mean action noise std: 0.91
                       Mean reward: 5968.98
               Mean episode length: 318.00
                 Mean success rate: 56.00
                  Mean reward/step: 16.88
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 2.67s
                        Total time: 3872.47s
                               ETA: 6456.7s

################################################################################
                     [1m Learning iteration 1500/4000 [0m

                       Computation: 3063 steps/s (collection: 0.589s, learning 2.084s)
               Value function loss: 78845.8021
                    Surrogate loss: 0.0163
             Mean action noise std: 0.91
                       Mean reward: 5849.09
               Mean episode length: 310.85
                 Mean success rate: 55.00
                  Mean reward/step: 17.17
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12296192
                    Iteration time: 2.67s
                        Total time: 3875.14s
                               ETA: 6454.3s

################################################################################
                     [1m Learning iteration 1501/4000 [0m

                       Computation: 3088 steps/s (collection: 0.543s, learning 2.110s)
               Value function loss: 86107.6724
                    Surrogate loss: 0.0156
             Mean action noise std: 0.91
                       Mean reward: 5561.60
               Mean episode length: 300.44
                 Mean success rate: 53.50
                  Mean reward/step: 17.66
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 2.65s
                        Total time: 3877.79s
                               ETA: 6451.8s

################################################################################
                     [1m Learning iteration 1502/4000 [0m

                       Computation: 2993 steps/s (collection: 0.628s, learning 2.108s)
               Value function loss: 42304.7134
                    Surrogate loss: 0.0187
             Mean action noise std: 0.91
                       Mean reward: 5121.31
               Mean episode length: 288.20
                 Mean success rate: 49.50
                  Mean reward/step: 17.72
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12312576
                    Iteration time: 2.74s
                        Total time: 3880.53s
                               ETA: 6449.5s

################################################################################
                     [1m Learning iteration 1503/4000 [0m

                       Computation: 3095 steps/s (collection: 0.545s, learning 2.101s)
               Value function loss: 54392.8455
                    Surrogate loss: 0.0185
             Mean action noise std: 0.91
                       Mean reward: 5085.96
               Mean episode length: 287.26
                 Mean success rate: 49.00
                  Mean reward/step: 18.85
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 2.65s
                        Total time: 3883.17s
                               ETA: 6447.0s

################################################################################
                     [1m Learning iteration 1504/4000 [0m

                       Computation: 3058 steps/s (collection: 0.576s, learning 2.102s)
               Value function loss: 67018.9641
                    Surrogate loss: 0.0110
             Mean action noise std: 0.91
                       Mean reward: 4811.22
               Mean episode length: 279.52
                 Mean success rate: 47.00
                  Mean reward/step: 19.29
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12328960
                    Iteration time: 2.68s
                        Total time: 3885.85s
                               ETA: 6444.6s

################################################################################
                     [1m Learning iteration 1505/4000 [0m

                       Computation: 3176 steps/s (collection: 0.546s, learning 2.033s)
               Value function loss: 74993.3164
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 4842.04
               Mean episode length: 280.98
                 Mean success rate: 47.50
                  Mean reward/step: 19.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 2.58s
                        Total time: 3888.43s
                               ETA: 6442.0s

################################################################################
                     [1m Learning iteration 1506/4000 [0m

                       Computation: 3177 steps/s (collection: 0.528s, learning 2.050s)
               Value function loss: 87067.9832
                    Surrogate loss: 0.0177
             Mean action noise std: 0.91
                       Mean reward: 4663.09
               Mean episode length: 275.31
                 Mean success rate: 46.00
                  Mean reward/step: 20.21
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 12345344
                    Iteration time: 2.58s
                        Total time: 3891.01s
                               ETA: 6439.4s

################################################################################
                     [1m Learning iteration 1507/4000 [0m

                       Computation: 3126 steps/s (collection: 0.538s, learning 2.082s)
               Value function loss: 48822.5950
                    Surrogate loss: 0.0188
             Mean action noise std: 0.91
                       Mean reward: 4197.91
               Mean episode length: 254.33
                 Mean success rate: 41.50
                  Mean reward/step: 20.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 2.62s
                        Total time: 3893.63s
                               ETA: 6436.9s

################################################################################
                     [1m Learning iteration 1508/4000 [0m

                       Computation: 3118 steps/s (collection: 0.554s, learning 2.073s)
               Value function loss: 102136.9015
                    Surrogate loss: 0.0170
             Mean action noise std: 0.91
                       Mean reward: 4511.08
               Mean episode length: 272.16
                 Mean success rate: 43.50
                  Mean reward/step: 20.93
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12361728
                    Iteration time: 2.63s
                        Total time: 3896.26s
                               ETA: 6434.4s

################################################################################
                     [1m Learning iteration 1509/4000 [0m

                       Computation: 3168 steps/s (collection: 0.532s, learning 2.054s)
               Value function loss: 105357.9773
                    Surrogate loss: 0.0160
             Mean action noise std: 0.91
                       Mean reward: 5098.12
               Mean episode length: 293.02
                 Mean success rate: 48.50
                  Mean reward/step: 19.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 2.59s
                        Total time: 3898.84s
                               ETA: 6431.8s

################################################################################
                     [1m Learning iteration 1510/4000 [0m

                       Computation: 3214 steps/s (collection: 0.491s, learning 2.058s)
               Value function loss: 68839.0431
                    Surrogate loss: 0.0161
             Mean action noise std: 0.91
                       Mean reward: 5403.38
               Mean episode length: 302.14
                 Mean success rate: 51.50
                  Mean reward/step: 18.79
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12378112
                    Iteration time: 2.55s
                        Total time: 3901.39s
                               ETA: 6429.2s

################################################################################
                     [1m Learning iteration 1511/4000 [0m

                       Computation: 3185 steps/s (collection: 0.507s, learning 2.065s)
               Value function loss: 77421.1040
                    Surrogate loss: 0.0160
             Mean action noise std: 0.91
                       Mean reward: 5588.77
               Mean episode length: 307.91
                 Mean success rate: 53.50
                  Mean reward/step: 19.11
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 2.57s
                        Total time: 3903.96s
                               ETA: 6426.6s

################################################################################
                     [1m Learning iteration 1512/4000 [0m

                       Computation: 3124 steps/s (collection: 0.557s, learning 2.065s)
               Value function loss: 127575.5176
                    Surrogate loss: 0.0167
             Mean action noise std: 0.91
                       Mean reward: 5891.06
               Mean episode length: 321.87
                 Mean success rate: 56.00
                  Mean reward/step: 19.33
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 12394496
                    Iteration time: 2.62s
                        Total time: 3906.58s
                               ETA: 6424.0s

################################################################################
                     [1m Learning iteration 1513/4000 [0m

                       Computation: 3250 steps/s (collection: 0.464s, learning 2.056s)
               Value function loss: 67849.2313
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 5904.97
               Mean episode length: 322.59
                 Mean success rate: 55.50
                  Mean reward/step: 18.79
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 2.52s
                        Total time: 3909.11s
                               ETA: 6421.4s

################################################################################
                     [1m Learning iteration 1514/4000 [0m

                       Computation: 3170 steps/s (collection: 0.522s, learning 2.062s)
               Value function loss: 62249.1132
                    Surrogate loss: 0.0179
             Mean action noise std: 0.91
                       Mean reward: 6182.69
               Mean episode length: 328.55
                 Mean success rate: 57.00
                  Mean reward/step: 19.14
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12410880
                    Iteration time: 2.58s
                        Total time: 3911.69s
                               ETA: 6418.8s

################################################################################
                     [1m Learning iteration 1515/4000 [0m

                       Computation: 3148 steps/s (collection: 0.532s, learning 2.070s)
               Value function loss: 92257.1460
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 6621.50
               Mean episode length: 339.52
                 Mean success rate: 60.00
                  Mean reward/step: 19.39
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 2.60s
                        Total time: 3914.29s
                               ETA: 6416.2s

################################################################################
                     [1m Learning iteration 1516/4000 [0m

                       Computation: 3118 steps/s (collection: 0.568s, learning 2.059s)
               Value function loss: 80772.1015
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 5981.42
               Mean episode length: 313.00
                 Mean success rate: 55.00
                  Mean reward/step: 19.63
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 12427264
                    Iteration time: 2.63s
                        Total time: 3916.92s
                               ETA: 6413.7s

################################################################################
                     [1m Learning iteration 1517/4000 [0m

                       Computation: 3124 steps/s (collection: 0.535s, learning 2.087s)
               Value function loss: 86781.1392
                    Surrogate loss: 0.0179
             Mean action noise std: 0.91
                       Mean reward: 6019.99
               Mean episode length: 309.02
                 Mean success rate: 54.50
                  Mean reward/step: 19.24
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 2.62s
                        Total time: 3919.54s
                               ETA: 6411.2s

################################################################################
                     [1m Learning iteration 1518/4000 [0m

                       Computation: 3114 steps/s (collection: 0.536s, learning 2.094s)
               Value function loss: 65943.1249
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 6042.38
               Mean episode length: 308.80
                 Mean success rate: 53.00
                  Mean reward/step: 18.81
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12443648
                    Iteration time: 2.63s
                        Total time: 3922.17s
                               ETA: 6408.7s

################################################################################
                     [1m Learning iteration 1519/4000 [0m

                       Computation: 3123 steps/s (collection: 0.534s, learning 2.088s)
               Value function loss: 59388.8896
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 5472.13
               Mean episode length: 290.04
                 Mean success rate: 48.50
                  Mean reward/step: 19.10
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 2.62s
                        Total time: 3924.79s
                               ETA: 6406.2s

################################################################################
                     [1m Learning iteration 1520/4000 [0m

                       Computation: 3125 steps/s (collection: 0.531s, learning 2.090s)
               Value function loss: 86607.4544
                    Surrogate loss: 0.0167
             Mean action noise std: 0.91
                       Mean reward: 5600.50
               Mean episode length: 288.80
                 Mean success rate: 49.00
                  Mean reward/step: 20.08
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12460032
                    Iteration time: 2.62s
                        Total time: 3927.41s
                               ETA: 6403.7s

################################################################################
                     [1m Learning iteration 1521/4000 [0m

                       Computation: 3081 steps/s (collection: 0.532s, learning 2.127s)
               Value function loss: 65229.8672
                    Surrogate loss: 0.0156
             Mean action noise std: 0.91
                       Mean reward: 5784.11
               Mean episode length: 293.32
                 Mean success rate: 50.00
                  Mean reward/step: 20.32
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 2.66s
                        Total time: 3930.07s
                               ETA: 6401.2s

################################################################################
                     [1m Learning iteration 1522/4000 [0m

                       Computation: 3103 steps/s (collection: 0.544s, learning 2.096s)
               Value function loss: 87106.6641
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 5740.13
               Mean episode length: 297.42
                 Mean success rate: 51.50
                  Mean reward/step: 20.00
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12476416
                    Iteration time: 2.64s
                        Total time: 3932.71s
                               ETA: 6398.7s

################################################################################
                     [1m Learning iteration 1523/4000 [0m

                       Computation: 3097 steps/s (collection: 0.541s, learning 2.104s)
               Value function loss: 95554.5870
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 5880.11
               Mean episode length: 298.57
                 Mean success rate: 52.50
                  Mean reward/step: 20.33
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 2.64s
                        Total time: 3935.36s
                               ETA: 6396.2s

################################################################################
                     [1m Learning iteration 1524/4000 [0m

                       Computation: 3058 steps/s (collection: 0.552s, learning 2.127s)
               Value function loss: 94750.5863
                    Surrogate loss: 0.0184
             Mean action noise std: 0.91
                       Mean reward: 6117.08
               Mean episode length: 309.10
                 Mean success rate: 54.50
                  Mean reward/step: 19.86
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 12492800
                    Iteration time: 2.68s
                        Total time: 3938.03s
                               ETA: 6393.8s

################################################################################
                     [1m Learning iteration 1525/4000 [0m

                       Computation: 3085 steps/s (collection: 0.591s, learning 2.064s)
               Value function loss: 95905.2321
                    Surrogate loss: 0.0197
             Mean action noise std: 0.91
                       Mean reward: 6337.49
               Mean episode length: 317.04
                 Mean success rate: 57.00
                  Mean reward/step: 19.76
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 2.65s
                        Total time: 3940.69s
                               ETA: 6391.4s

################################################################################
                     [1m Learning iteration 1526/4000 [0m

                       Computation: 3245 steps/s (collection: 0.477s, learning 2.047s)
               Value function loss: 57521.2650
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 6361.90
               Mean episode length: 320.91
                 Mean success rate: 58.50
                  Mean reward/step: 20.45
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12509184
                    Iteration time: 2.52s
                        Total time: 3943.21s
                               ETA: 6388.7s

################################################################################
                     [1m Learning iteration 1527/4000 [0m

                       Computation: 3206 steps/s (collection: 0.514s, learning 2.040s)
               Value function loss: 80981.6514
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 6982.72
               Mean episode length: 338.10
                 Mean success rate: 64.00
                  Mean reward/step: 21.12
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 2.55s
                        Total time: 3945.77s
                               ETA: 6386.0s

################################################################################
                     [1m Learning iteration 1528/4000 [0m

                       Computation: 3201 steps/s (collection: 0.522s, learning 2.037s)
               Value function loss: 106353.2976
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 7557.42
               Mean episode length: 357.58
                 Mean success rate: 68.00
                  Mean reward/step: 21.34
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12525568
                    Iteration time: 2.56s
                        Total time: 3948.33s
                               ETA: 6383.4s

################################################################################
                     [1m Learning iteration 1529/4000 [0m

                       Computation: 3219 steps/s (collection: 0.472s, learning 2.073s)
               Value function loss: 64738.4350
                    Surrogate loss: 0.0175
             Mean action noise std: 0.91
                       Mean reward: 7434.56
               Mean episode length: 355.93
                 Mean success rate: 67.50
                  Mean reward/step: 21.37
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 2.54s
                        Total time: 3950.87s
                               ETA: 6380.8s

################################################################################
                     [1m Learning iteration 1530/4000 [0m

                       Computation: 3132 steps/s (collection: 0.576s, learning 2.040s)
               Value function loss: 92283.1596
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 7274.66
               Mean episode length: 353.45
                 Mean success rate: 66.50
                  Mean reward/step: 21.47
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12541952
                    Iteration time: 2.62s
                        Total time: 3953.49s
                               ETA: 6378.3s

################################################################################
                     [1m Learning iteration 1531/4000 [0m

                       Computation: 3250 steps/s (collection: 0.513s, learning 2.007s)
               Value function loss: 75187.5163
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 7303.71
               Mean episode length: 355.38
                 Mean success rate: 66.00
                  Mean reward/step: 20.96
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 2.52s
                        Total time: 3956.01s
                               ETA: 6375.6s

################################################################################
                     [1m Learning iteration 1532/4000 [0m

                       Computation: 3204 steps/s (collection: 0.516s, learning 2.041s)
               Value function loss: 82725.3704
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 7414.06
               Mean episode length: 363.36
                 Mean success rate: 68.00
                  Mean reward/step: 20.00
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12558336
                    Iteration time: 2.56s
                        Total time: 3958.56s
                               ETA: 6373.0s

################################################################################
                     [1m Learning iteration 1533/4000 [0m

                       Computation: 3199 steps/s (collection: 0.455s, learning 2.106s)
               Value function loss: 104277.8271
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 7722.38
               Mean episode length: 372.93
                 Mean success rate: 70.00
                  Mean reward/step: 19.97
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 2.56s
                        Total time: 3961.12s
                               ETA: 6370.3s

################################################################################
                     [1m Learning iteration 1534/4000 [0m

                       Computation: 3235 steps/s (collection: 0.467s, learning 2.065s)
               Value function loss: 68160.7405
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 7744.00
               Mean episode length: 374.37
                 Mean success rate: 70.00
                  Mean reward/step: 20.12
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12574720
                    Iteration time: 2.53s
                        Total time: 3963.66s
                               ETA: 6367.7s

################################################################################
                     [1m Learning iteration 1535/4000 [0m

                       Computation: 3230 steps/s (collection: 0.475s, learning 2.060s)
               Value function loss: 82835.5810
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 7812.14
               Mean episode length: 379.56
                 Mean success rate: 71.50
                  Mean reward/step: 20.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 2.54s
                        Total time: 3966.19s
                               ETA: 6365.0s

################################################################################
                     [1m Learning iteration 1536/4000 [0m

                       Computation: 3115 steps/s (collection: 0.501s, learning 2.129s)
               Value function loss: 83754.2160
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 7959.50
               Mean episode length: 386.09
                 Mean success rate: 73.00
                  Mean reward/step: 21.17
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12591104
                    Iteration time: 2.63s
                        Total time: 3968.82s
                               ETA: 6362.5s

################################################################################
                     [1m Learning iteration 1537/4000 [0m

                       Computation: 3233 steps/s (collection: 0.510s, learning 2.024s)
               Value function loss: 82152.0391
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 7714.66
               Mean episode length: 377.18
                 Mean success rate: 71.50
                  Mean reward/step: 21.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 2.53s
                        Total time: 3971.35s
                               ETA: 6359.8s

################################################################################
                     [1m Learning iteration 1538/4000 [0m

                       Computation: 3222 steps/s (collection: 0.491s, learning 2.051s)
               Value function loss: 80731.9535
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 7725.53
               Mean episode length: 375.97
                 Mean success rate: 71.50
                  Mean reward/step: 20.49
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12607488
                    Iteration time: 2.54s
                        Total time: 3973.90s
                               ETA: 6357.2s

################################################################################
                     [1m Learning iteration 1539/4000 [0m

                       Computation: 3171 steps/s (collection: 0.496s, learning 2.087s)
               Value function loss: 137288.0676
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 7784.95
               Mean episode length: 382.18
                 Mean success rate: 72.50
                  Mean reward/step: 19.66
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 2.58s
                        Total time: 3976.48s
                               ETA: 6354.6s

################################################################################
                     [1m Learning iteration 1540/4000 [0m

                       Computation: 3234 steps/s (collection: 0.454s, learning 2.078s)
               Value function loss: 53845.9823
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 7764.85
               Mean episode length: 380.23
                 Mean success rate: 72.50
                  Mean reward/step: 18.42
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12623872
                    Iteration time: 2.53s
                        Total time: 3979.01s
                               ETA: 6352.0s

################################################################################
                     [1m Learning iteration 1541/4000 [0m

                       Computation: 3270 steps/s (collection: 0.472s, learning 2.033s)
               Value function loss: 75888.5033
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 7961.10
               Mean episode length: 384.04
                 Mean success rate: 73.50
                  Mean reward/step: 18.83
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 2.50s
                        Total time: 3981.52s
                               ETA: 6349.3s

################################################################################
                     [1m Learning iteration 1542/4000 [0m

                       Computation: 3275 steps/s (collection: 0.440s, learning 2.061s)
               Value function loss: 56375.8952
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 7675.96
               Mean episode length: 375.02
                 Mean success rate: 71.50
                  Mean reward/step: 19.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12640256
                    Iteration time: 2.50s
                        Total time: 3984.02s
                               ETA: 6346.5s

################################################################################
                     [1m Learning iteration 1543/4000 [0m

                       Computation: 3231 steps/s (collection: 0.421s, learning 2.114s)
               Value function loss: 79707.3084
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 7447.67
               Mean episode length: 372.89
                 Mean success rate: 70.50
                  Mean reward/step: 20.09
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 2.54s
                        Total time: 3986.55s
                               ETA: 6343.9s

################################################################################
                     [1m Learning iteration 1544/4000 [0m

                       Computation: 3114 steps/s (collection: 0.503s, learning 2.127s)
               Value function loss: 90819.4595
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 7101.40
               Mean episode length: 360.83
                 Mean success rate: 67.50
                  Mean reward/step: 20.13
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12656640
                    Iteration time: 2.63s
                        Total time: 3989.18s
                               ETA: 6341.4s

################################################################################
                     [1m Learning iteration 1545/4000 [0m

                       Computation: 3105 steps/s (collection: 0.540s, learning 2.098s)
               Value function loss: 50172.5528
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 6994.89
               Mean episode length: 354.92
                 Mean success rate: 65.00
                  Mean reward/step: 20.73
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 2.64s
                        Total time: 3991.82s
                               ETA: 6338.9s

################################################################################
                     [1m Learning iteration 1546/4000 [0m

                       Computation: 3124 steps/s (collection: 0.480s, learning 2.141s)
               Value function loss: 104311.3580
                    Surrogate loss: 0.0191
             Mean action noise std: 0.91
                       Mean reward: 7099.85
               Mean episode length: 357.00
                 Mean success rate: 66.00
                  Mean reward/step: 20.77
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12673024
                    Iteration time: 2.62s
                        Total time: 3994.44s
                               ETA: 6336.4s

################################################################################
                     [1m Learning iteration 1547/4000 [0m

                       Computation: 3147 steps/s (collection: 0.464s, learning 2.138s)
               Value function loss: 82222.2570
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 7071.65
               Mean episode length: 350.94
                 Mean success rate: 65.00
                  Mean reward/step: 21.10
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 2.60s
                        Total time: 3997.05s
                               ETA: 6333.8s

################################################################################
                     [1m Learning iteration 1548/4000 [0m

                       Computation: 3101 steps/s (collection: 0.476s, learning 2.164s)
               Value function loss: 86611.3432
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 7222.54
               Mean episode length: 354.92
                 Mean success rate: 66.50
                  Mean reward/step: 20.89
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12689408
                    Iteration time: 2.64s
                        Total time: 3999.69s
                               ETA: 6331.3s

################################################################################
                     [1m Learning iteration 1549/4000 [0m

                       Computation: 3178 steps/s (collection: 0.465s, learning 2.112s)
               Value function loss: 92778.8378
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 7345.50
               Mean episode length: 363.13
                 Mean success rate: 67.50
                  Mean reward/step: 20.05
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 2.58s
                        Total time: 4002.26s
                               ETA: 6328.7s

################################################################################
                     [1m Learning iteration 1550/4000 [0m

                       Computation: 3189 steps/s (collection: 0.472s, learning 2.097s)
               Value function loss: 90787.7560
                    Surrogate loss: 0.0159
             Mean action noise std: 0.91
                       Mean reward: 7086.18
               Mean episode length: 357.69
                 Mean success rate: 66.50
                  Mean reward/step: 19.66
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12705792
                    Iteration time: 2.57s
                        Total time: 4004.83s
                               ETA: 6326.1s

################################################################################
                     [1m Learning iteration 1551/4000 [0m

                       Computation: 3167 steps/s (collection: 0.452s, learning 2.134s)
               Value function loss: 78045.4999
                    Surrogate loss: 0.0169
             Mean action noise std: 0.91
                       Mean reward: 7323.47
               Mean episode length: 366.35
                 Mean success rate: 69.00
                  Mean reward/step: 19.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 2.59s
                        Total time: 4007.42s
                               ETA: 6323.6s

################################################################################
                     [1m Learning iteration 1552/4000 [0m

                       Computation: 3100 steps/s (collection: 0.519s, learning 2.124s)
               Value function loss: 69881.8669
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 7366.23
               Mean episode length: 359.89
                 Mean success rate: 68.00
                  Mean reward/step: 19.29
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12722176
                    Iteration time: 2.64s
                        Total time: 4010.06s
                               ETA: 6321.1s

################################################################################
                     [1m Learning iteration 1553/4000 [0m

                       Computation: 3179 steps/s (collection: 0.473s, learning 2.104s)
               Value function loss: 79133.7097
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 7141.99
               Mean episode length: 355.54
                 Mean success rate: 67.50
                  Mean reward/step: 19.93
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 2.58s
                        Total time: 4012.64s
                               ETA: 6318.5s

################################################################################
                     [1m Learning iteration 1554/4000 [0m

                       Computation: 3104 steps/s (collection: 0.476s, learning 2.163s)
               Value function loss: 104360.3061
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 7305.10
               Mean episode length: 362.75
                 Mean success rate: 69.00
                  Mean reward/step: 19.85
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12738560
                    Iteration time: 2.64s
                        Total time: 4015.28s
                               ETA: 6316.0s

################################################################################
                     [1m Learning iteration 1555/4000 [0m

                       Computation: 3184 steps/s (collection: 0.483s, learning 2.090s)
               Value function loss: 106840.2544
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 7325.67
               Mean episode length: 365.55
                 Mean success rate: 69.50
                  Mean reward/step: 19.20
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 2.57s
                        Total time: 4017.85s
                               ETA: 6313.4s

################################################################################
                     [1m Learning iteration 1556/4000 [0m

                       Computation: 3181 steps/s (collection: 0.501s, learning 2.073s)
               Value function loss: 73759.0904
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 7272.77
               Mean episode length: 364.44
                 Mean success rate: 69.00
                  Mean reward/step: 19.18
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12754944
                    Iteration time: 2.57s
                        Total time: 4020.42s
                               ETA: 6310.8s

################################################################################
                     [1m Learning iteration 1557/4000 [0m

                       Computation: 3214 steps/s (collection: 0.484s, learning 2.064s)
               Value function loss: 56531.8065
                    Surrogate loss: 0.0163
             Mean action noise std: 0.91
                       Mean reward: 6941.68
               Mean episode length: 350.07
                 Mean success rate: 66.50
                  Mean reward/step: 19.68
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 2.55s
                        Total time: 4022.97s
                               ETA: 6308.2s

################################################################################
                     [1m Learning iteration 1558/4000 [0m

                       Computation: 3204 steps/s (collection: 0.458s, learning 2.098s)
               Value function loss: 84474.5385
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 6735.19
               Mean episode length: 339.77
                 Mean success rate: 64.50
                  Mean reward/step: 19.71
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 12771328
                    Iteration time: 2.56s
                        Total time: 4025.53s
                               ETA: 6305.5s

################################################################################
                     [1m Learning iteration 1559/4000 [0m

                       Computation: 3230 steps/s (collection: 0.464s, learning 2.072s)
               Value function loss: 93094.6545
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 6833.33
               Mean episode length: 342.65
                 Mean success rate: 65.00
                  Mean reward/step: 19.77
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 2.54s
                        Total time: 4028.06s
                               ETA: 6302.9s

################################################################################
                     [1m Learning iteration 1560/4000 [0m

                       Computation: 3208 steps/s (collection: 0.454s, learning 2.099s)
               Value function loss: 85505.8181
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 6910.41
               Mean episode length: 346.30
                 Mean success rate: 65.50
                  Mean reward/step: 20.00
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12787712
                    Iteration time: 2.55s
                        Total time: 4030.62s
                               ETA: 6300.3s

################################################################################
                     [1m Learning iteration 1561/4000 [0m

                       Computation: 3242 steps/s (collection: 0.469s, learning 2.057s)
               Value function loss: 61144.2001
                    Surrogate loss: 0.0163
             Mean action noise std: 0.91
                       Mean reward: 6915.77
               Mean episode length: 343.56
                 Mean success rate: 65.50
                  Mean reward/step: 20.01
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 2.53s
                        Total time: 4033.14s
                               ETA: 6297.6s

################################################################################
                     [1m Learning iteration 1562/4000 [0m

                       Computation: 3192 steps/s (collection: 0.468s, learning 2.098s)
               Value function loss: 80767.5438
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 6851.07
               Mean episode length: 342.79
                 Mean success rate: 65.00
                  Mean reward/step: 19.67
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12804096
                    Iteration time: 2.57s
                        Total time: 4035.71s
                               ETA: 6295.0s

################################################################################
                     [1m Learning iteration 1563/4000 [0m

                       Computation: 3181 steps/s (collection: 0.522s, learning 2.053s)
               Value function loss: 120959.1180
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 6739.24
               Mean episode length: 342.22
                 Mean success rate: 65.00
                  Mean reward/step: 19.72
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 2.57s
                        Total time: 4038.28s
                               ETA: 6292.4s

################################################################################
                     [1m Learning iteration 1564/4000 [0m

                       Computation: 3184 steps/s (collection: 0.515s, learning 2.057s)
               Value function loss: 69195.2188
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 7026.43
               Mean episode length: 346.98
                 Mean success rate: 67.00
                  Mean reward/step: 19.46
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12820480
                    Iteration time: 2.57s
                        Total time: 4040.86s
                               ETA: 6289.8s

################################################################################
                     [1m Learning iteration 1565/4000 [0m

                       Computation: 3134 steps/s (collection: 0.534s, learning 2.080s)
               Value function loss: 80804.2576
                    Surrogate loss: 0.0148
             Mean action noise std: 0.91
                       Mean reward: 6844.84
               Mean episode length: 342.66
                 Mean success rate: 66.00
                  Mean reward/step: 19.63
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 2.61s
                        Total time: 4043.47s
                               ETA: 6287.3s

################################################################################
                     [1m Learning iteration 1566/4000 [0m

                       Computation: 3178 steps/s (collection: 0.486s, learning 2.092s)
               Value function loss: 68995.4642
                    Surrogate loss: 0.0107
             Mean action noise std: 0.91
                       Mean reward: 6866.82
               Mean episode length: 348.68
                 Mean success rate: 67.50
                  Mean reward/step: 19.79
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12836864
                    Iteration time: 2.58s
                        Total time: 4046.05s
                               ETA: 6284.7s

################################################################################
                     [1m Learning iteration 1567/4000 [0m

                       Computation: 3186 steps/s (collection: 0.460s, learning 2.111s)
               Value function loss: 88257.1861
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 6781.79
               Mean episode length: 346.30
                 Mean success rate: 66.50
                  Mean reward/step: 20.22
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 2.57s
                        Total time: 4048.62s
                               ETA: 6282.1s

################################################################################
                     [1m Learning iteration 1568/4000 [0m

                       Computation: 3229 steps/s (collection: 0.451s, learning 2.085s)
               Value function loss: 65120.4673
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 6668.03
               Mean episode length: 341.31
                 Mean success rate: 66.00
                  Mean reward/step: 19.82
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12853248
                    Iteration time: 2.54s
                        Total time: 4051.16s
                               ETA: 6279.4s

################################################################################
                     [1m Learning iteration 1569/4000 [0m

                       Computation: 3141 steps/s (collection: 0.499s, learning 2.108s)
               Value function loss: 68392.3694
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 6580.35
               Mean episode length: 337.88
                 Mean success rate: 64.50
                  Mean reward/step: 19.50
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 2.61s
                        Total time: 4053.76s
                               ETA: 6276.9s

################################################################################
                     [1m Learning iteration 1570/4000 [0m

                       Computation: 3086 steps/s (collection: 0.524s, learning 2.129s)
               Value function loss: 98138.7747
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 6734.88
               Mean episode length: 342.43
                 Mean success rate: 65.50
                  Mean reward/step: 19.34
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12869632
                    Iteration time: 2.65s
                        Total time: 4056.42s
                               ETA: 6274.4s

################################################################################
                     [1m Learning iteration 1571/4000 [0m

                       Computation: 3056 steps/s (collection: 0.518s, learning 2.162s)
               Value function loss: 64658.9551
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 6775.01
               Mean episode length: 349.55
                 Mean success rate: 67.00
                  Mean reward/step: 18.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 2.68s
                        Total time: 4059.10s
                               ETA: 6272.0s

################################################################################
                     [1m Learning iteration 1572/4000 [0m

                       Computation: 3071 steps/s (collection: 0.490s, learning 2.177s)
               Value function loss: 52249.5734
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 6676.07
               Mean episode length: 341.02
                 Mean success rate: 65.50
                  Mean reward/step: 19.18
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12886016
                    Iteration time: 2.67s
                        Total time: 4061.76s
                               ETA: 6269.5s

################################################################################
                     [1m Learning iteration 1573/4000 [0m

                       Computation: 3207 steps/s (collection: 0.459s, learning 2.095s)
               Value function loss: 75167.6820
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 6606.04
               Mean episode length: 339.93
                 Mean success rate: 64.00
                  Mean reward/step: 20.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 2.55s
                        Total time: 4064.32s
                               ETA: 6266.9s

################################################################################
                     [1m Learning iteration 1574/4000 [0m

                       Computation: 3160 steps/s (collection: 0.489s, learning 2.103s)
               Value function loss: 66397.3645
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 6846.65
               Mean episode length: 348.58
                 Mean success rate: 65.50
                  Mean reward/step: 20.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12902400
                    Iteration time: 2.59s
                        Total time: 4066.91s
                               ETA: 6264.3s

################################################################################
                     [1m Learning iteration 1575/4000 [0m

                       Computation: 3132 steps/s (collection: 0.514s, learning 2.101s)
               Value function loss: 89972.4961
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 7136.60
               Mean episode length: 354.05
                 Mean success rate: 65.50
                  Mean reward/step: 20.69
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 2.62s
                        Total time: 4069.53s
                               ETA: 6261.8s

################################################################################
                     [1m Learning iteration 1576/4000 [0m

                       Computation: 3174 steps/s (collection: 0.482s, learning 2.099s)
               Value function loss: 66522.1214
                    Surrogate loss: 0.0104
             Mean action noise std: 0.91
                       Mean reward: 7043.76
               Mean episode length: 353.46
                 Mean success rate: 66.00
                  Mean reward/step: 20.67
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12918784
                    Iteration time: 2.58s
                        Total time: 4072.11s
                               ETA: 6259.2s

################################################################################
                     [1m Learning iteration 1577/4000 [0m

                       Computation: 3214 steps/s (collection: 0.487s, learning 2.062s)
               Value function loss: 105571.1789
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 7264.52
               Mean episode length: 365.24
                 Mean success rate: 68.00
                  Mean reward/step: 20.89
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 2.55s
                        Total time: 4074.66s
                               ETA: 6256.6s

################################################################################
                     [1m Learning iteration 1578/4000 [0m

                       Computation: 3149 steps/s (collection: 0.487s, learning 2.113s)
               Value function loss: 96737.1084
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 7689.14
               Mean episode length: 382.00
                 Mean success rate: 71.00
                  Mean reward/step: 20.87
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12935168
                    Iteration time: 2.60s
                        Total time: 4077.26s
                               ETA: 6254.0s

################################################################################
                     [1m Learning iteration 1579/4000 [0m

                       Computation: 3103 steps/s (collection: 0.526s, learning 2.113s)
               Value function loss: 93439.3103
                    Surrogate loss: 0.0179
             Mean action noise std: 0.91
                       Mean reward: 7795.16
               Mean episode length: 388.13
                 Mean success rate: 72.50
                  Mean reward/step: 20.05
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 2.64s
                        Total time: 4079.90s
                               ETA: 6251.5s

################################################################################
                     [1m Learning iteration 1580/4000 [0m

                       Computation: 3217 steps/s (collection: 0.465s, learning 2.081s)
               Value function loss: 44588.9562
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 7841.85
               Mean episode length: 387.35
                 Mean success rate: 72.50
                  Mean reward/step: 19.86
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 12951552
                    Iteration time: 2.55s
                        Total time: 4082.44s
                               ETA: 6248.9s

################################################################################
                     [1m Learning iteration 1581/4000 [0m

                       Computation: 3188 steps/s (collection: 0.487s, learning 2.083s)
               Value function loss: 80986.0638
                    Surrogate loss: 0.0148
             Mean action noise std: 0.91
                       Mean reward: 7766.51
               Mean episode length: 383.53
                 Mean success rate: 71.50
                  Mean reward/step: 20.24
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 2.57s
                        Total time: 4085.01s
                               ETA: 6246.3s

################################################################################
                     [1m Learning iteration 1582/4000 [0m

                       Computation: 3218 steps/s (collection: 0.493s, learning 2.052s)
               Value function loss: 58167.8406
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 7561.82
               Mean episode length: 379.32
                 Mean success rate: 71.50
                  Mean reward/step: 20.39
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12967936
                    Iteration time: 2.55s
                        Total time: 4087.56s
                               ETA: 6243.7s

################################################################################
                     [1m Learning iteration 1583/4000 [0m

                       Computation: 3098 steps/s (collection: 0.483s, learning 2.161s)
               Value function loss: 79317.5562
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 7718.22
               Mean episode length: 383.71
                 Mean success rate: 72.00
                  Mean reward/step: 21.35
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 2.64s
                        Total time: 4090.20s
                               ETA: 6241.2s

################################################################################
                     [1m Learning iteration 1584/4000 [0m

                       Computation: 3141 steps/s (collection: 0.483s, learning 2.125s)
               Value function loss: 86836.1149
                    Surrogate loss: 0.0183
             Mean action noise std: 0.91
                       Mean reward: 7735.75
               Mean episode length: 381.63
                 Mean success rate: 72.50
                  Mean reward/step: 21.35
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12984320
                    Iteration time: 2.61s
                        Total time: 4092.81s
                               ETA: 6238.6s

################################################################################
                     [1m Learning iteration 1585/4000 [0m

                       Computation: 3172 steps/s (collection: 0.460s, learning 2.123s)
               Value function loss: 99782.7886
                    Surrogate loss: 0.0112
             Mean action noise std: 0.91
                       Mean reward: 7572.03
               Mean episode length: 375.19
                 Mean success rate: 70.50
                  Mean reward/step: 20.76
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 2.58s
                        Total time: 4095.39s
                               ETA: 6236.0s

################################################################################
                     [1m Learning iteration 1586/4000 [0m

                       Computation: 3226 steps/s (collection: 0.469s, learning 2.070s)
               Value function loss: 128021.9777
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 7465.98
               Mean episode length: 369.09
                 Mean success rate: 68.50
                  Mean reward/step: 19.68
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 13000704
                    Iteration time: 2.54s
                        Total time: 4097.93s
                               ETA: 6233.4s

################################################################################
                     [1m Learning iteration 1587/4000 [0m

                       Computation: 3127 steps/s (collection: 0.494s, learning 2.125s)
               Value function loss: 58373.3239
                    Surrogate loss: 0.0180
             Mean action noise std: 0.91
                       Mean reward: 7284.54
               Mean episode length: 362.39
                 Mean success rate: 67.50
                  Mean reward/step: 19.02
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 2.62s
                        Total time: 4100.55s
                               ETA: 6230.9s

################################################################################
                     [1m Learning iteration 1588/4000 [0m

                       Computation: 3154 steps/s (collection: 0.496s, learning 2.100s)
               Value function loss: 82493.3044
                    Surrogate loss: 0.0161
             Mean action noise std: 0.91
                       Mean reward: 7092.70
               Mean episode length: 354.67
                 Mean success rate: 65.50
                  Mean reward/step: 19.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13017088
                    Iteration time: 2.60s
                        Total time: 4103.15s
                               ETA: 6228.3s

################################################################################
                     [1m Learning iteration 1589/4000 [0m

                       Computation: 3166 steps/s (collection: 0.522s, learning 2.065s)
               Value function loss: 90949.8896
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 6875.52
               Mean episode length: 346.31
                 Mean success rate: 63.50
                  Mean reward/step: 19.73
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 2.59s
                        Total time: 4105.73s
                               ETA: 6225.7s

################################################################################
                     [1m Learning iteration 1590/4000 [0m

                       Computation: 3179 steps/s (collection: 0.480s, learning 2.096s)
               Value function loss: 73792.6573
                    Surrogate loss: 0.0163
             Mean action noise std: 0.91
                       Mean reward: 7055.37
               Mean episode length: 352.32
                 Mean success rate: 63.50
                  Mean reward/step: 20.14
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13033472
                    Iteration time: 2.58s
                        Total time: 4108.31s
                               ETA: 6223.1s

################################################################################
                     [1m Learning iteration 1591/4000 [0m

                       Computation: 3228 steps/s (collection: 0.478s, learning 2.059s)
               Value function loss: 83062.2828
                    Surrogate loss: 0.0166
             Mean action noise std: 0.91
                       Mean reward: 7227.66
               Mean episode length: 359.81
                 Mean success rate: 65.00
                  Mean reward/step: 20.80
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 2.54s
                        Total time: 4110.85s
                               ETA: 6220.5s

################################################################################
                     [1m Learning iteration 1592/4000 [0m

                       Computation: 3170 steps/s (collection: 0.470s, learning 2.113s)
               Value function loss: 58418.1369
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 7187.39
               Mean episode length: 358.52
                 Mean success rate: 65.00
                  Mean reward/step: 21.08
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13049856
                    Iteration time: 2.58s
                        Total time: 4113.43s
                               ETA: 6217.9s

################################################################################
                     [1m Learning iteration 1593/4000 [0m

                       Computation: 3182 steps/s (collection: 0.469s, learning 2.105s)
               Value function loss: 113176.2453
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 7283.85
               Mean episode length: 361.96
                 Mean success rate: 65.50
                  Mean reward/step: 21.13
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 2.57s
                        Total time: 4116.00s
                               ETA: 6215.3s

################################################################################
                     [1m Learning iteration 1594/4000 [0m

                       Computation: 3213 steps/s (collection: 0.480s, learning 2.069s)
               Value function loss: 87287.4961
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 7496.95
               Mean episode length: 371.52
                 Mean success rate: 67.00
                  Mean reward/step: 20.83
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13066240
                    Iteration time: 2.55s
                        Total time: 4118.55s
                               ETA: 6212.7s

################################################################################
                     [1m Learning iteration 1595/4000 [0m

                       Computation: 3173 steps/s (collection: 0.462s, learning 2.119s)
               Value function loss: 84024.7222
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 7653.50
               Mean episode length: 377.72
                 Mean success rate: 68.50
                  Mean reward/step: 19.90
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 2.58s
                        Total time: 4121.13s
                               ETA: 6210.1s

################################################################################
                     [1m Learning iteration 1596/4000 [0m

                       Computation: 3220 steps/s (collection: 0.469s, learning 2.074s)
               Value function loss: 74077.9840
                    Surrogate loss: 0.0170
             Mean action noise std: 0.91
                       Mean reward: 7713.25
               Mean episode length: 373.97
                 Mean success rate: 68.50
                  Mean reward/step: 19.94
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13082624
                    Iteration time: 2.54s
                        Total time: 4123.68s
                               ETA: 6207.5s

################################################################################
                     [1m Learning iteration 1597/4000 [0m

                       Computation: 3187 steps/s (collection: 0.469s, learning 2.101s)
               Value function loss: 65135.1680
                    Surrogate loss: 0.0111
             Mean action noise std: 0.91
                       Mean reward: 7843.37
               Mean episode length: 380.68
                 Mean success rate: 69.50
                  Mean reward/step: 20.16
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 2.57s
                        Total time: 4126.25s
                               ETA: 6204.9s

################################################################################
                     [1m Learning iteration 1598/4000 [0m

                       Computation: 3247 steps/s (collection: 0.457s, learning 2.066s)
               Value function loss: 101627.2950
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 7524.73
               Mean episode length: 369.31
                 Mean success rate: 67.50
                  Mean reward/step: 20.36
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 13099008
                    Iteration time: 2.52s
                        Total time: 4128.77s
                               ETA: 6202.2s

################################################################################
                     [1m Learning iteration 1599/4000 [0m

                       Computation: 3201 steps/s (collection: 0.463s, learning 2.096s)
               Value function loss: 66375.2616
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 7767.98
               Mean episode length: 376.52
                 Mean success rate: 70.00
                  Mean reward/step: 20.37
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 2.56s
                        Total time: 4131.33s
                               ETA: 6199.6s

################################################################################
                     [1m Learning iteration 1600/4000 [0m

                       Computation: 3127 steps/s (collection: 0.518s, learning 2.101s)
               Value function loss: 79963.4356
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 7333.80
               Mean episode length: 361.96
                 Mean success rate: 67.00
                  Mean reward/step: 20.55
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13115392
                    Iteration time: 2.62s
                        Total time: 4133.95s
                               ETA: 6197.0s

################################################################################
                     [1m Learning iteration 1601/4000 [0m

                       Computation: 3182 steps/s (collection: 0.508s, learning 2.066s)
               Value function loss: 113767.7864
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 7600.09
               Mean episode length: 370.55
                 Mean success rate: 69.00
                  Mean reward/step: 20.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 2.57s
                        Total time: 4136.52s
                               ETA: 6194.5s

################################################################################
                     [1m Learning iteration 1602/4000 [0m

                       Computation: 3181 steps/s (collection: 0.477s, learning 2.098s)
               Value function loss: 130500.6508
                    Surrogate loss: 0.0126
             Mean action noise std: 0.91
                       Mean reward: 7691.82
               Mean episode length: 375.81
                 Mean success rate: 71.00
                  Mean reward/step: 19.43
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13131776
                    Iteration time: 2.58s
                        Total time: 4139.10s
                               ETA: 6191.9s

################################################################################
                     [1m Learning iteration 1603/4000 [0m

                       Computation: 3158 steps/s (collection: 0.491s, learning 2.103s)
               Value function loss: 78824.0308
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 7871.53
               Mean episode length: 379.05
                 Mean success rate: 72.50
                  Mean reward/step: 19.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 2.59s
                        Total time: 4141.69s
                               ETA: 6189.3s

################################################################################
                     [1m Learning iteration 1604/4000 [0m

                       Computation: 3193 steps/s (collection: 0.467s, learning 2.098s)
               Value function loss: 73327.8780
                    Surrogate loss: 0.0105
             Mean action noise std: 0.91
                       Mean reward: 7720.92
               Mean episode length: 370.24
                 Mean success rate: 71.00
                  Mean reward/step: 19.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13148160
                    Iteration time: 2.57s
                        Total time: 4144.26s
                               ETA: 6186.7s

################################################################################
                     [1m Learning iteration 1605/4000 [0m

                       Computation: 3207 steps/s (collection: 0.469s, learning 2.085s)
               Value function loss: 79847.5759
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 7827.27
               Mean episode length: 380.52
                 Mean success rate: 72.50
                  Mean reward/step: 20.16
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 2.55s
                        Total time: 4146.81s
                               ETA: 6184.1s

################################################################################
                     [1m Learning iteration 1606/4000 [0m

                       Computation: 3308 steps/s (collection: 0.430s, learning 2.045s)
               Value function loss: 68897.2426
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 7512.34
               Mean episode length: 366.69
                 Mean success rate: 69.50
                  Mean reward/step: 20.43
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13164544
                    Iteration time: 2.48s
                        Total time: 4149.29s
                               ETA: 6181.3s

################################################################################
                     [1m Learning iteration 1607/4000 [0m

                       Computation: 3195 steps/s (collection: 0.459s, learning 2.104s)
               Value function loss: 79582.3141
                    Surrogate loss: 0.0161
             Mean action noise std: 0.91
                       Mean reward: 7867.21
               Mean episode length: 383.87
                 Mean success rate: 73.50
                  Mean reward/step: 20.38
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 2.56s
                        Total time: 4151.85s
                               ETA: 6178.7s

################################################################################
                     [1m Learning iteration 1608/4000 [0m

                       Computation: 3127 steps/s (collection: 0.492s, learning 2.127s)
               Value function loss: 69742.0284
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 8004.88
               Mean episode length: 389.91
                 Mean success rate: 74.50
                  Mean reward/step: 21.40
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13180928
                    Iteration time: 2.62s
                        Total time: 4154.47s
                               ETA: 6176.2s

################################################################################
                     [1m Learning iteration 1609/4000 [0m

                       Computation: 3117 steps/s (collection: 0.515s, learning 2.113s)
               Value function loss: 87788.2304
                    Surrogate loss: 0.0110
             Mean action noise std: 0.91
                       Mean reward: 7937.80
               Mean episode length: 390.20
                 Mean success rate: 74.00
                  Mean reward/step: 21.76
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 2.63s
                        Total time: 4157.10s
                               ETA: 6173.7s

################################################################################
                     [1m Learning iteration 1610/4000 [0m

                       Computation: 3216 steps/s (collection: 0.471s, learning 2.076s)
               Value function loss: 94971.4102
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 7601.90
               Mean episode length: 379.45
                 Mean success rate: 72.00
                  Mean reward/step: 21.47
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13197312
                    Iteration time: 2.55s
                        Total time: 4159.64s
                               ETA: 6171.0s

################################################################################
                     [1m Learning iteration 1611/4000 [0m

                       Computation: 3255 steps/s (collection: 0.460s, learning 2.056s)
               Value function loss: 62658.7255
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 7174.95
               Mean episode length: 365.92
                 Mean success rate: 68.50
                  Mean reward/step: 20.96
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 2.52s
                        Total time: 4162.16s
                               ETA: 6168.4s

################################################################################
                     [1m Learning iteration 1612/4000 [0m

                       Computation: 3225 steps/s (collection: 0.459s, learning 2.080s)
               Value function loss: 90941.5098
                    Surrogate loss: 0.0112
             Mean action noise std: 0.91
                       Mean reward: 7066.14
               Mean episode length: 356.44
                 Mean success rate: 66.00
                  Mean reward/step: 21.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13213696
                    Iteration time: 2.54s
                        Total time: 4164.70s
                               ETA: 6165.7s

################################################################################
                     [1m Learning iteration 1613/4000 [0m

                       Computation: 3158 steps/s (collection: 0.491s, learning 2.103s)
               Value function loss: 65907.1815
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 6932.61
               Mean episode length: 354.49
                 Mean success rate: 66.00
                  Mean reward/step: 21.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 2.59s
                        Total time: 4167.29s
                               ETA: 6163.2s

################################################################################
                     [1m Learning iteration 1614/4000 [0m

                       Computation: 3215 steps/s (collection: 0.463s, learning 2.084s)
               Value function loss: 111727.0580
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 7025.32
               Mean episode length: 354.06
                 Mean success rate: 67.00
                  Mean reward/step: 20.87
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13230080
                    Iteration time: 2.55s
                        Total time: 4169.84s
                               ETA: 6160.5s

################################################################################
                     [1m Learning iteration 1615/4000 [0m

                       Computation: 3178 steps/s (collection: 0.484s, learning 2.094s)
               Value function loss: 77001.2072
                    Surrogate loss: 0.0111
             Mean action noise std: 0.91
                       Mean reward: 7274.44
               Mean episode length: 359.74
                 Mean success rate: 68.00
                  Mean reward/step: 20.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 2.58s
                        Total time: 4172.42s
                               ETA: 6157.9s

################################################################################
                     [1m Learning iteration 1616/4000 [0m

                       Computation: 3174 steps/s (collection: 0.518s, learning 2.063s)
               Value function loss: 78948.8762
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 7188.23
               Mean episode length: 350.13
                 Mean success rate: 67.00
                  Mean reward/step: 20.73
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13246464
                    Iteration time: 2.58s
                        Total time: 4175.00s
                               ETA: 6155.3s

################################################################################
                     [1m Learning iteration 1617/4000 [0m

                       Computation: 3183 steps/s (collection: 0.479s, learning 2.094s)
               Value function loss: 122924.0420
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 7472.54
               Mean episode length: 357.56
                 Mean success rate: 69.00
                  Mean reward/step: 20.56
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 2.57s
                        Total time: 4177.57s
                               ETA: 6152.8s

################################################################################
                     [1m Learning iteration 1618/4000 [0m

                       Computation: 3198 steps/s (collection: 0.474s, learning 2.087s)
               Value function loss: 64033.5961
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 7398.29
               Mean episode length: 357.87
                 Mean success rate: 68.50
                  Mean reward/step: 19.89
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13262848
                    Iteration time: 2.56s
                        Total time: 4180.13s
                               ETA: 6150.1s

################################################################################
                     [1m Learning iteration 1619/4000 [0m

                       Computation: 3241 steps/s (collection: 0.460s, learning 2.067s)
               Value function loss: 93574.7773
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 7588.49
               Mean episode length: 357.31
                 Mean success rate: 69.00
                  Mean reward/step: 19.71
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 2.53s
                        Total time: 4182.66s
                               ETA: 6147.5s

################################################################################
                     [1m Learning iteration 1620/4000 [0m

                       Computation: 3229 steps/s (collection: 0.486s, learning 2.050s)
               Value function loss: 96516.8574
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 7695.87
               Mean episode length: 366.54
                 Mean success rate: 70.50
                  Mean reward/step: 19.98
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13279232
                    Iteration time: 2.54s
                        Total time: 4185.20s
                               ETA: 6144.8s

################################################################################
                     [1m Learning iteration 1621/4000 [0m

                       Computation: 3183 steps/s (collection: 0.507s, learning 2.067s)
               Value function loss: 74327.5088
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 7591.17
               Mean episode length: 361.24
                 Mean success rate: 69.00
                  Mean reward/step: 20.13
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 2.57s
                        Total time: 4187.77s
                               ETA: 6142.2s

################################################################################
                     [1m Learning iteration 1622/4000 [0m

                       Computation: 3238 steps/s (collection: 0.488s, learning 2.042s)
               Value function loss: 73674.5592
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 7278.95
               Mean episode length: 348.46
                 Mean success rate: 65.50
                  Mean reward/step: 20.19
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13295616
                    Iteration time: 2.53s
                        Total time: 4190.30s
                               ETA: 6139.6s

################################################################################
                     [1m Learning iteration 1623/4000 [0m

                       Computation: 3152 steps/s (collection: 0.467s, learning 2.131s)
               Value function loss: 53908.9268
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 7081.76
               Mean episode length: 341.97
                 Mean success rate: 64.00
                  Mean reward/step: 20.42
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 2.60s
                        Total time: 4192.90s
                               ETA: 6137.0s

################################################################################
                     [1m Learning iteration 1624/4000 [0m

                       Computation: 3199 steps/s (collection: 0.490s, learning 2.071s)
               Value function loss: 91373.9291
                    Surrogate loss: 0.0180
             Mean action noise std: 0.91
                       Mean reward: 7150.66
               Mean episode length: 347.14
                 Mean success rate: 64.00
                  Mean reward/step: 21.42
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13312000
                    Iteration time: 2.56s
                        Total time: 4195.46s
                               ETA: 6134.4s

################################################################################
                     [1m Learning iteration 1625/4000 [0m

                       Computation: 3203 steps/s (collection: 0.510s, learning 2.047s)
               Value function loss: 87283.0182
                    Surrogate loss: 0.0172
             Mean action noise std: 0.91
                       Mean reward: 6769.21
               Mean episode length: 334.47
                 Mean success rate: 62.00
                  Mean reward/step: 21.47
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 2.56s
                        Total time: 4198.02s
                               ETA: 6131.8s

################################################################################
                     [1m Learning iteration 1626/4000 [0m

                       Computation: 3267 steps/s (collection: 0.445s, learning 2.062s)
               Value function loss: 103373.5506
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 6901.32
               Mean episode length: 334.20
                 Mean success rate: 62.50
                  Mean reward/step: 20.62
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13328384
                    Iteration time: 2.51s
                        Total time: 4200.52s
                               ETA: 6129.1s

################################################################################
                     [1m Learning iteration 1627/4000 [0m

                       Computation: 3249 steps/s (collection: 0.477s, learning 2.044s)
               Value function loss: 58039.9107
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 6427.65
               Mean episode length: 322.28
                 Mean success rate: 60.00
                  Mean reward/step: 20.39
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 2.52s
                        Total time: 4203.04s
                               ETA: 6126.4s

################################################################################
                     [1m Learning iteration 1628/4000 [0m

                       Computation: 3265 steps/s (collection: 0.457s, learning 2.052s)
               Value function loss: 65412.6232
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 6317.16
               Mean episode length: 317.81
                 Mean success rate: 59.00
                  Mean reward/step: 20.67
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13344768
                    Iteration time: 2.51s
                        Total time: 4205.55s
                               ETA: 6123.7s

################################################################################
                     [1m Learning iteration 1629/4000 [0m

                       Computation: 3244 steps/s (collection: 0.456s, learning 2.069s)
               Value function loss: 103145.7707
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 6650.13
               Mean episode length: 329.76
                 Mean success rate: 62.50
                  Mean reward/step: 21.03
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 2.52s
                        Total time: 4208.08s
                               ETA: 6121.1s

################################################################################
                     [1m Learning iteration 1630/4000 [0m

                       Computation: 3238 steps/s (collection: 0.483s, learning 2.046s)
               Value function loss: 66224.1093
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 6620.59
               Mean episode length: 328.25
                 Mean success rate: 63.00
                  Mean reward/step: 20.71
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13361152
                    Iteration time: 2.53s
                        Total time: 4210.61s
                               ETA: 6118.4s

################################################################################
                     [1m Learning iteration 1631/4000 [0m

                       Computation: 3286 steps/s (collection: 0.451s, learning 2.042s)
               Value function loss: 71051.3301
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 6495.66
               Mean episode length: 325.70
                 Mean success rate: 62.50
                  Mean reward/step: 20.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 2.49s
                        Total time: 4213.10s
                               ETA: 6115.7s

################################################################################
                     [1m Learning iteration 1632/4000 [0m

                       Computation: 3311 steps/s (collection: 0.437s, learning 2.036s)
               Value function loss: 86767.5258
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 6675.96
               Mean episode length: 329.20
                 Mean success rate: 64.00
                  Mean reward/step: 20.22
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13377536
                    Iteration time: 2.47s
                        Total time: 4215.57s
                               ETA: 6113.0s

################################################################################
                     [1m Learning iteration 1633/4000 [0m

                       Computation: 3357 steps/s (collection: 0.438s, learning 2.003s)
               Value function loss: 119306.6525
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 6994.96
               Mean episode length: 335.82
                 Mean success rate: 64.50
                  Mean reward/step: 20.23
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 2.44s
                        Total time: 4218.01s
                               ETA: 6110.2s

################################################################################
                     [1m Learning iteration 1634/4000 [0m

                       Computation: 3314 steps/s (collection: 0.448s, learning 2.024s)
               Value function loss: 77755.6234
                    Surrogate loss: 0.0114
             Mean action noise std: 0.91
                       Mean reward: 6658.88
               Mean episode length: 325.66
                 Mean success rate: 62.50
                  Mean reward/step: 19.60
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13393920
                    Iteration time: 2.47s
                        Total time: 4220.49s
                               ETA: 6107.4s

################################################################################
                     [1m Learning iteration 1635/4000 [0m

                       Computation: 3221 steps/s (collection: 0.462s, learning 2.080s)
               Value function loss: 73504.3864
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 6990.87
               Mean episode length: 335.89
                 Mean success rate: 65.50
                  Mean reward/step: 19.78
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 2.54s
                        Total time: 4223.03s
                               ETA: 6104.8s

################################################################################
                     [1m Learning iteration 1636/4000 [0m

                       Computation: 3225 steps/s (collection: 0.468s, learning 2.073s)
               Value function loss: 82944.4721
                    Surrogate loss: 0.0173
             Mean action noise std: 0.91
                       Mean reward: 7077.18
               Mean episode length: 335.90
                 Mean success rate: 66.00
                  Mean reward/step: 20.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13410304
                    Iteration time: 2.54s
                        Total time: 4225.57s
                               ETA: 6102.2s

################################################################################
                     [1m Learning iteration 1637/4000 [0m

                       Computation: 3171 steps/s (collection: 0.474s, learning 2.109s)
               Value function loss: 99876.0881
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 7474.59
               Mean episode length: 353.34
                 Mean success rate: 69.00
                  Mean reward/step: 19.99
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 2.58s
                        Total time: 4228.15s
                               ETA: 6099.6s

################################################################################
                     [1m Learning iteration 1638/4000 [0m

                       Computation: 3242 steps/s (collection: 0.483s, learning 2.043s)
               Value function loss: 64441.7272
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 7259.40
               Mean episode length: 345.56
                 Mean success rate: 67.00
                  Mean reward/step: 20.19
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13426688
                    Iteration time: 2.53s
                        Total time: 4230.68s
                               ETA: 6096.9s

################################################################################
                     [1m Learning iteration 1639/4000 [0m

                       Computation: 3172 steps/s (collection: 0.512s, learning 2.070s)
               Value function loss: 37907.3863
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 7087.56
               Mean episode length: 341.82
                 Mean success rate: 64.50
                  Mean reward/step: 20.89
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 2.58s
                        Total time: 4233.26s
                               ETA: 6094.3s

################################################################################
                     [1m Learning iteration 1640/4000 [0m

                       Computation: 3194 steps/s (collection: 0.496s, learning 2.068s)
               Value function loss: 127041.9826
                    Surrogate loss: 0.0103
             Mean action noise std: 0.91
                       Mean reward: 6997.60
               Mean episode length: 344.38
                 Mean success rate: 65.00
                  Mean reward/step: 21.05
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13443072
                    Iteration time: 2.56s
                        Total time: 4235.82s
                               ETA: 6091.7s

################################################################################
                     [1m Learning iteration 1641/4000 [0m

                       Computation: 3198 steps/s (collection: 0.493s, learning 2.068s)
               Value function loss: 59938.7407
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 6983.24
               Mean episode length: 341.60
                 Mean success rate: 65.00
                  Mean reward/step: 20.24
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 2.56s
                        Total time: 4238.39s
                               ETA: 6089.1s

################################################################################
                     [1m Learning iteration 1642/4000 [0m

                       Computation: 3154 steps/s (collection: 0.490s, learning 2.106s)
               Value function loss: 88675.5115
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 6978.24
               Mean episode length: 346.01
                 Mean success rate: 66.00
                  Mean reward/step: 20.13
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13459456
                    Iteration time: 2.60s
                        Total time: 4240.98s
                               ETA: 6086.6s

################################################################################
                     [1m Learning iteration 1643/4000 [0m

                       Computation: 3233 steps/s (collection: 0.485s, learning 2.048s)
               Value function loss: 66447.5935
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 7156.48
               Mean episode length: 352.90
                 Mean success rate: 68.00
                  Mean reward/step: 20.78
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 2.53s
                        Total time: 4243.52s
                               ETA: 6083.9s

################################################################################
                     [1m Learning iteration 1644/4000 [0m

                       Computation: 3267 steps/s (collection: 0.441s, learning 2.066s)
               Value function loss: 65255.6101
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 7273.05
               Mean episode length: 356.82
                 Mean success rate: 68.00
                  Mean reward/step: 21.19
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13475840
                    Iteration time: 2.51s
                        Total time: 4246.02s
                               ETA: 6081.2s

################################################################################
                     [1m Learning iteration 1645/4000 [0m

                       Computation: 3275 steps/s (collection: 0.444s, learning 2.057s)
               Value function loss: 106683.8313
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 6987.46
               Mean episode length: 340.46
                 Mean success rate: 65.00
                  Mean reward/step: 20.54
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 2.50s
                        Total time: 4248.52s
                               ETA: 6078.5s

################################################################################
                     [1m Learning iteration 1646/4000 [0m

                       Computation: 3220 steps/s (collection: 0.457s, learning 2.087s)
               Value function loss: 86152.4747
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 6590.89
               Mean episode length: 327.70
                 Mean success rate: 62.50
                  Mean reward/step: 19.44
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13492224
                    Iteration time: 2.54s
                        Total time: 4251.07s
                               ETA: 6075.9s

################################################################################
                     [1m Learning iteration 1647/4000 [0m

                       Computation: 3241 steps/s (collection: 0.466s, learning 2.061s)
               Value function loss: 89860.3037
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 7000.44
               Mean episode length: 339.77
                 Mean success rate: 66.00
                  Mean reward/step: 19.41
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 2.53s
                        Total time: 4253.59s
                               ETA: 6073.2s

################################################################################
                     [1m Learning iteration 1648/4000 [0m

                       Computation: 3270 steps/s (collection: 0.436s, learning 2.068s)
               Value function loss: 67506.3247
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 6883.84
               Mean episode length: 331.34
                 Mean success rate: 64.00
                  Mean reward/step: 19.44
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13508608
                    Iteration time: 2.50s
                        Total time: 4256.10s
                               ETA: 6070.6s

################################################################################
                     [1m Learning iteration 1649/4000 [0m

                       Computation: 3260 steps/s (collection: 0.472s, learning 2.040s)
               Value function loss: 99153.6789
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 6493.51
               Mean episode length: 320.01
                 Mean success rate: 61.00
                  Mean reward/step: 19.46
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 2.51s
                        Total time: 4258.61s
                               ETA: 6067.9s

################################################################################
                     [1m Learning iteration 1650/4000 [0m

                       Computation: 3233 steps/s (collection: 0.455s, learning 2.079s)
               Value function loss: 96117.4049
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 6513.52
               Mean episode length: 321.67
                 Mean success rate: 60.00
                  Mean reward/step: 19.72
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13524992
                    Iteration time: 2.53s
                        Total time: 4261.14s
                               ETA: 6065.2s

################################################################################
                     [1m Learning iteration 1651/4000 [0m

                       Computation: 3199 steps/s (collection: 0.502s, learning 2.059s)
               Value function loss: 68014.7776
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 5729.19
               Mean episode length: 292.49
                 Mean success rate: 54.00
                  Mean reward/step: 19.94
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 2.56s
                        Total time: 4263.70s
                               ETA: 6062.6s

################################################################################
                     [1m Learning iteration 1652/4000 [0m

                       Computation: 3210 steps/s (collection: 0.502s, learning 2.050s)
               Value function loss: 81755.1848
                    Surrogate loss: 0.0164
             Mean action noise std: 0.91
                       Mean reward: 5622.52
               Mean episode length: 293.90
                 Mean success rate: 52.50
                  Mean reward/step: 19.63
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13541376
                    Iteration time: 2.55s
                        Total time: 4266.26s
                               ETA: 6060.0s

################################################################################
                     [1m Learning iteration 1653/4000 [0m

                       Computation: 3221 steps/s (collection: 0.505s, learning 2.038s)
               Value function loss: 79623.5486
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 5895.04
               Mean episode length: 303.60
                 Mean success rate: 54.00
                  Mean reward/step: 19.93
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 2.54s
                        Total time: 4268.80s
                               ETA: 6057.4s

################################################################################
                     [1m Learning iteration 1654/4000 [0m

                       Computation: 3284 steps/s (collection: 0.451s, learning 2.043s)
               Value function loss: 60129.3846
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 5835.46
               Mean episode length: 300.99
                 Mean success rate: 53.00
                  Mean reward/step: 20.82
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13557760
                    Iteration time: 2.49s
                        Total time: 4271.29s
                               ETA: 6054.7s

################################################################################
                     [1m Learning iteration 1655/4000 [0m

                       Computation: 3212 steps/s (collection: 0.517s, learning 2.033s)
               Value function loss: 88056.5602
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 5558.96
               Mean episode length: 289.04
                 Mean success rate: 50.00
                  Mean reward/step: 21.95
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 2.55s
                        Total time: 4273.84s
                               ETA: 6052.0s

################################################################################
                     [1m Learning iteration 1656/4000 [0m

                       Computation: 3133 steps/s (collection: 0.535s, learning 2.080s)
               Value function loss: 80424.5605
                    Surrogate loss: 0.0163
             Mean action noise std: 0.91
                       Mean reward: 5780.30
               Mean episode length: 293.61
                 Mean success rate: 50.50
                  Mean reward/step: 21.13
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13574144
                    Iteration time: 2.61s
                        Total time: 4276.46s
                               ETA: 6049.5s

################################################################################
                     [1m Learning iteration 1657/4000 [0m

                       Computation: 3286 steps/s (collection: 0.482s, learning 2.011s)
               Value function loss: 47135.7805
                    Surrogate loss: 0.0165
             Mean action noise std: 0.91
                       Mean reward: 5892.74
               Mean episode length: 294.47
                 Mean success rate: 51.00
                  Mean reward/step: 21.29
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 2.49s
                        Total time: 4278.95s
                               ETA: 6046.8s

################################################################################
                     [1m Learning iteration 1658/4000 [0m

                       Computation: 3260 steps/s (collection: 0.487s, learning 2.026s)
               Value function loss: 96795.8746
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 5851.05
               Mean episode length: 294.50
                 Mean success rate: 53.00
                  Mean reward/step: 21.81
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13590528
                    Iteration time: 2.51s
                        Total time: 4281.46s
                               ETA: 6044.1s

################################################################################
                     [1m Learning iteration 1659/4000 [0m

                       Computation: 3246 steps/s (collection: 0.495s, learning 2.028s)
               Value function loss: 95737.5777
                    Surrogate loss: 0.0185
             Mean action noise std: 0.91
                       Mean reward: 6180.47
               Mean episode length: 310.12
                 Mean success rate: 55.50
                  Mean reward/step: 21.90
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 2.52s
                        Total time: 4283.99s
                               ETA: 6041.5s

################################################################################
                     [1m Learning iteration 1660/4000 [0m

                       Computation: 3296 steps/s (collection: 0.484s, learning 2.001s)
               Value function loss: 64628.2262
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 6231.63
               Mean episode length: 311.72
                 Mean success rate: 56.50
                  Mean reward/step: 21.91
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13606912
                    Iteration time: 2.48s
                        Total time: 4286.47s
                               ETA: 6038.7s

################################################################################
                     [1m Learning iteration 1661/4000 [0m

                       Computation: 3306 steps/s (collection: 0.482s, learning 1.995s)
               Value function loss: 112694.7346
                    Surrogate loss: 0.0184
             Mean action noise std: 0.91
                       Mean reward: 6265.85
               Mean episode length: 311.06
                 Mean success rate: 57.00
                  Mean reward/step: 21.51
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 2.48s
                        Total time: 4288.95s
                               ETA: 6036.0s

################################################################################
                     [1m Learning iteration 1662/4000 [0m

                       Computation: 3344 steps/s (collection: 0.456s, learning 1.994s)
               Value function loss: 91501.7471
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 6653.97
               Mean episode length: 320.43
                 Mean success rate: 60.50
                  Mean reward/step: 20.87
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13623296
                    Iteration time: 2.45s
                        Total time: 4291.40s
                               ETA: 6033.2s

################################################################################
                     [1m Learning iteration 1663/4000 [0m

                       Computation: 3385 steps/s (collection: 0.421s, learning 1.999s)
               Value function loss: 72849.0138
                    Surrogate loss: 0.0161
             Mean action noise std: 0.91
                       Mean reward: 7057.88
               Mean episode length: 337.33
                 Mean success rate: 64.00
                  Mean reward/step: 20.76
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 2.42s
                        Total time: 4293.82s
                               ETA: 6030.4s

################################################################################
                     [1m Learning iteration 1664/4000 [0m

                       Computation: 3377 steps/s (collection: 0.430s, learning 1.996s)
               Value function loss: 98512.2469
                    Surrogate loss: 0.0167
             Mean action noise std: 0.91
                       Mean reward: 6699.02
               Mean episode length: 327.25
                 Mean success rate: 62.00
                  Mean reward/step: 20.75
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 13639680
                    Iteration time: 2.43s
                        Total time: 4296.24s
                               ETA: 6027.6s

################################################################################
                     [1m Learning iteration 1665/4000 [0m

                       Computation: 3383 steps/s (collection: 0.426s, learning 1.995s)
               Value function loss: 102678.6648
                    Surrogate loss: 0.0246
             Mean action noise std: 0.91
                       Mean reward: 7111.11
               Mean episode length: 340.96
                 Mean success rate: 64.50
                  Mean reward/step: 20.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 2.42s
                        Total time: 4298.66s
                               ETA: 6024.8s

################################################################################
                     [1m Learning iteration 1666/4000 [0m

                       Computation: 3315 steps/s (collection: 0.455s, learning 2.016s)
               Value function loss: 97784.0094
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 7021.68
               Mean episode length: 333.81
                 Mean success rate: 63.00
                  Mean reward/step: 19.83
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 13656064
                    Iteration time: 2.47s
                        Total time: 4301.13s
                               ETA: 6022.1s

################################################################################
                     [1m Learning iteration 1667/4000 [0m

                       Computation: 3379 steps/s (collection: 0.431s, learning 1.993s)
               Value function loss: 104498.7316
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 7174.79
               Mean episode length: 336.69
                 Mean success rate: 63.50
                  Mean reward/step: 19.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 2.42s
                        Total time: 4303.56s
                               ETA: 6019.3s

################################################################################
                     [1m Learning iteration 1668/4000 [0m

                       Computation: 3361 steps/s (collection: 0.415s, learning 2.022s)
               Value function loss: 90561.3832
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 7550.65
               Mean episode length: 350.54
                 Mean success rate: 66.50
                  Mean reward/step: 19.10
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13672448
                    Iteration time: 2.44s
                        Total time: 4306.00s
                               ETA: 6016.5s

################################################################################
                     [1m Learning iteration 1669/4000 [0m

                       Computation: 3346 steps/s (collection: 0.443s, learning 2.005s)
               Value function loss: 83773.7198
                    Surrogate loss: 0.0126
             Mean action noise std: 0.91
                       Mean reward: 7273.41
               Mean episode length: 340.14
                 Mean success rate: 64.50
                  Mean reward/step: 19.34
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 2.45s
                        Total time: 4308.44s
                               ETA: 6013.8s

################################################################################
                     [1m Learning iteration 1670/4000 [0m

                       Computation: 3280 steps/s (collection: 0.480s, learning 2.018s)
               Value function loss: 63884.3362
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 7140.27
               Mean episode length: 336.60
                 Mean success rate: 63.00
                  Mean reward/step: 20.17
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13688832
                    Iteration time: 2.50s
                        Total time: 4310.94s
                               ETA: 6011.1s

################################################################################
                     [1m Learning iteration 1671/4000 [0m

                       Computation: 3331 steps/s (collection: 0.444s, learning 2.015s)
               Value function loss: 113253.2393
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 7425.04
               Mean episode length: 347.35
                 Mean success rate: 66.00
                  Mean reward/step: 19.96
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 2.46s
                        Total time: 4313.40s
                               ETA: 6008.3s

################################################################################
                     [1m Learning iteration 1672/4000 [0m

                       Computation: 3361 steps/s (collection: 0.413s, learning 2.024s)
               Value function loss: 75449.9375
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 7541.38
               Mean episode length: 349.58
                 Mean success rate: 66.50
                  Mean reward/step: 18.94
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13705216
                    Iteration time: 2.44s
                        Total time: 4315.84s
                               ETA: 6005.5s

################################################################################
                     [1m Learning iteration 1673/4000 [0m

                       Computation: 3213 steps/s (collection: 0.515s, learning 2.034s)
               Value function loss: 57527.5871
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 7070.74
               Mean episode length: 332.23
                 Mean success rate: 63.50
                  Mean reward/step: 19.58
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 2.55s
                        Total time: 4318.39s
                               ETA: 6002.9s

################################################################################
                     [1m Learning iteration 1674/4000 [0m

                       Computation: 3243 steps/s (collection: 0.439s, learning 2.086s)
               Value function loss: 95871.3523
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 7270.98
               Mean episode length: 345.39
                 Mean success rate: 67.00
                  Mean reward/step: 19.79
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13721600
                    Iteration time: 2.53s
                        Total time: 4320.91s
                               ETA: 6000.3s

################################################################################
                     [1m Learning iteration 1675/4000 [0m

                       Computation: 3273 steps/s (collection: 0.434s, learning 2.068s)
               Value function loss: 51462.9127
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 7199.82
               Mean episode length: 344.00
                 Mean success rate: 65.00
                  Mean reward/step: 20.12
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 2.50s
                        Total time: 4323.41s
                               ETA: 5997.6s

################################################################################
                     [1m Learning iteration 1676/4000 [0m

                       Computation: 3295 steps/s (collection: 0.459s, learning 2.027s)
               Value function loss: 110628.3627
                    Surrogate loss: 0.0126
             Mean action noise std: 0.91
                       Mean reward: 6961.89
               Mean episode length: 336.61
                 Mean success rate: 64.00
                  Mean reward/step: 20.33
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13737984
                    Iteration time: 2.49s
                        Total time: 4325.90s
                               ETA: 5994.9s

################################################################################
                     [1m Learning iteration 1677/4000 [0m

                       Computation: 3270 steps/s (collection: 0.463s, learning 2.042s)
               Value function loss: 70660.7785
                    Surrogate loss: 0.0167
             Mean action noise std: 0.91
                       Mean reward: 7039.81
               Mean episode length: 341.67
                 Mean success rate: 64.50
                  Mean reward/step: 19.92
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 2.51s
                        Total time: 4328.41s
                               ETA: 5992.2s

################################################################################
                     [1m Learning iteration 1678/4000 [0m

                       Computation: 3300 steps/s (collection: 0.443s, learning 2.040s)
               Value function loss: 102028.9475
                    Surrogate loss: 0.0161
             Mean action noise std: 0.91
                       Mean reward: 6771.26
               Mean episode length: 334.40
                 Mean success rate: 62.50
                  Mean reward/step: 20.07
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13754368
                    Iteration time: 2.48s
                        Total time: 4330.89s
                               ETA: 5989.5s

################################################################################
                     [1m Learning iteration 1679/4000 [0m

                       Computation: 3244 steps/s (collection: 0.488s, learning 2.037s)
               Value function loss: 62438.9304
                    Surrogate loss: 0.0156
             Mean action noise std: 0.91
                       Mean reward: 6598.65
               Mean episode length: 333.07
                 Mean success rate: 62.00
                  Mean reward/step: 19.79
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 2.53s
                        Total time: 4333.41s
                               ETA: 5986.8s

################################################################################
                     [1m Learning iteration 1680/4000 [0m

                       Computation: 3224 steps/s (collection: 0.481s, learning 2.060s)
               Value function loss: 110798.4527
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 6788.48
               Mean episode length: 341.19
                 Mean success rate: 63.00
                  Mean reward/step: 19.76
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13770752
                    Iteration time: 2.54s
                        Total time: 4335.95s
                               ETA: 5984.2s

################################################################################
                     [1m Learning iteration 1681/4000 [0m

                       Computation: 3289 steps/s (collection: 0.446s, learning 2.044s)
               Value function loss: 94767.7086
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 7099.08
               Mean episode length: 359.48
                 Mean success rate: 66.50
                  Mean reward/step: 19.91
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 2.49s
                        Total time: 4338.44s
                               ETA: 5981.5s

################################################################################
                     [1m Learning iteration 1682/4000 [0m

                       Computation: 3189 steps/s (collection: 0.483s, learning 2.085s)
               Value function loss: 87038.5063
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 7020.72
               Mean episode length: 357.36
                 Mean success rate: 65.50
                  Mean reward/step: 19.82
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13787136
                    Iteration time: 2.57s
                        Total time: 4341.01s
                               ETA: 5978.9s

################################################################################
                     [1m Learning iteration 1683/4000 [0m

                       Computation: 3250 steps/s (collection: 0.506s, learning 2.014s)
               Value function loss: 78488.5509
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 7253.36
               Mean episode length: 364.94
                 Mean success rate: 67.50
                  Mean reward/step: 20.62
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 2.52s
                        Total time: 4343.53s
                               ETA: 5976.2s

################################################################################
                     [1m Learning iteration 1684/4000 [0m

                       Computation: 3171 steps/s (collection: 0.479s, learning 2.104s)
               Value function loss: 84768.4580
                    Surrogate loss: 0.0148
             Mean action noise std: 0.91
                       Mean reward: 7358.68
               Mean episode length: 370.60
                 Mean success rate: 68.00
                  Mean reward/step: 20.46
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13803520
                    Iteration time: 2.58s
                        Total time: 4346.12s
                               ETA: 5973.7s

################################################################################
                     [1m Learning iteration 1685/4000 [0m

                       Computation: 3200 steps/s (collection: 0.436s, learning 2.124s)
               Value function loss: 50141.2323
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 7001.97
               Mean episode length: 356.60
                 Mean success rate: 65.50
                  Mean reward/step: 20.13
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 2.56s
                        Total time: 4348.67s
                               ETA: 5971.0s

################################################################################
                     [1m Learning iteration 1686/4000 [0m

                       Computation: 3087 steps/s (collection: 0.468s, learning 2.186s)
               Value function loss: 92569.2795
                    Surrogate loss: 0.0148
             Mean action noise std: 0.91
                       Mean reward: 6835.09
               Mean episode length: 344.68
                 Mean success rate: 64.00
                  Mean reward/step: 19.99
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13819904
                    Iteration time: 2.65s
                        Total time: 4351.33s
                               ETA: 5968.6s

################################################################################
                     [1m Learning iteration 1687/4000 [0m

                       Computation: 3107 steps/s (collection: 0.467s, learning 2.169s)
               Value function loss: 94072.5643
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 7025.59
               Mean episode length: 349.69
                 Mean success rate: 65.50
                  Mean reward/step: 19.61
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 2.64s
                        Total time: 4353.96s
                               ETA: 5966.1s

################################################################################
                     [1m Learning iteration 1688/4000 [0m

                       Computation: 3185 steps/s (collection: 0.459s, learning 2.113s)
               Value function loss: 61569.8440
                    Surrogate loss: 0.0148
             Mean action noise std: 0.91
                       Mean reward: 6502.68
               Mean episode length: 333.29
                 Mean success rate: 61.50
                  Mean reward/step: 20.34
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13836288
                    Iteration time: 2.57s
                        Total time: 4356.54s
                               ETA: 5963.5s

################################################################################
                     [1m Learning iteration 1689/4000 [0m

                       Computation: 3241 steps/s (collection: 0.464s, learning 2.063s)
               Value function loss: 74623.0290
                    Surrogate loss: 0.0169
             Mean action noise std: 0.91
                       Mean reward: 6466.59
               Mean episode length: 331.48
                 Mean success rate: 61.50
                  Mean reward/step: 21.03
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 2.53s
                        Total time: 4359.06s
                               ETA: 5960.8s

################################################################################
                     [1m Learning iteration 1690/4000 [0m

                       Computation: 3072 steps/s (collection: 0.519s, learning 2.148s)
               Value function loss: 81947.8173
                    Surrogate loss: 0.0168
             Mean action noise std: 0.91
                       Mean reward: 6586.27
               Mean episode length: 335.60
                 Mean success rate: 62.50
                  Mean reward/step: 21.37
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13852672
                    Iteration time: 2.67s
                        Total time: 4361.73s
                               ETA: 5958.4s

################################################################################
                     [1m Learning iteration 1691/4000 [0m

                       Computation: 3116 steps/s (collection: 0.490s, learning 2.139s)
               Value function loss: 73509.0486
                    Surrogate loss: 0.0179
             Mean action noise std: 0.91
                       Mean reward: 6259.38
               Mean episode length: 317.33
                 Mean success rate: 58.50
                  Mean reward/step: 21.91
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 2.63s
                        Total time: 4364.36s
                               ETA: 5955.9s

################################################################################
                     [1m Learning iteration 1692/4000 [0m

                       Computation: 3183 steps/s (collection: 0.446s, learning 2.128s)
               Value function loss: 92667.9492
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 6425.70
               Mean episode length: 322.05
                 Mean success rate: 61.00
                  Mean reward/step: 21.40
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13869056
                    Iteration time: 2.57s
                        Total time: 4366.93s
                               ETA: 5953.3s

################################################################################
                     [1m Learning iteration 1693/4000 [0m

                       Computation: 3119 steps/s (collection: 0.481s, learning 2.145s)
               Value function loss: 83734.3277
                    Surrogate loss: 0.0178
             Mean action noise std: 0.91
                       Mean reward: 6319.62
               Mean episode length: 321.87
                 Mean success rate: 60.00
                  Mean reward/step: 20.90
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 2.63s
                        Total time: 4369.56s
                               ETA: 5950.7s

################################################################################
                     [1m Learning iteration 1694/4000 [0m

                       Computation: 3101 steps/s (collection: 0.497s, learning 2.144s)
               Value function loss: 84183.0012
                    Surrogate loss: 0.0156
             Mean action noise std: 0.91
                       Mean reward: 6780.74
               Mean episode length: 338.96
                 Mean success rate: 63.00
                  Mean reward/step: 20.48
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13885440
                    Iteration time: 2.64s
                        Total time: 4372.20s
                               ETA: 5948.3s

################################################################################
                     [1m Learning iteration 1695/4000 [0m

                       Computation: 3084 steps/s (collection: 0.514s, learning 2.141s)
               Value function loss: 107388.2973
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 6824.43
               Mean episode length: 340.76
                 Mean success rate: 62.50
                  Mean reward/step: 20.49
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 2.66s
                        Total time: 4374.85s
                               ETA: 5945.8s

################################################################################
                     [1m Learning iteration 1696/4000 [0m

                       Computation: 3245 steps/s (collection: 0.488s, learning 2.036s)
               Value function loss: 82481.0385
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 7235.16
               Mean episode length: 351.40
                 Mean success rate: 65.00
                  Mean reward/step: 20.78
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13901824
                    Iteration time: 2.52s
                        Total time: 4377.38s
                               ETA: 5943.1s

################################################################################
                     [1m Learning iteration 1697/4000 [0m

                       Computation: 3316 steps/s (collection: 0.444s, learning 2.027s)
               Value function loss: 106192.1644
                    Surrogate loss: 0.0169
             Mean action noise std: 0.91
                       Mean reward: 7640.38
               Mean episode length: 362.06
                 Mean success rate: 67.50
                  Mean reward/step: 21.19
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 2.47s
                        Total time: 4379.85s
                               ETA: 5940.4s

################################################################################
                     [1m Learning iteration 1698/4000 [0m

                       Computation: 3227 steps/s (collection: 0.491s, learning 2.047s)
               Value function loss: 67601.1604
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 7852.26
               Mean episode length: 369.48
                 Mean success rate: 68.50
                  Mean reward/step: 21.50
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13918208
                    Iteration time: 2.54s
                        Total time: 4382.39s
                               ETA: 5937.8s

################################################################################
                     [1m Learning iteration 1699/4000 [0m

                       Computation: 3235 steps/s (collection: 0.460s, learning 2.072s)
               Value function loss: 71425.5697
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 7757.89
               Mean episode length: 364.05
                 Mean success rate: 67.50
                  Mean reward/step: 21.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 2.53s
                        Total time: 4384.92s
                               ETA: 5935.1s

################################################################################
                     [1m Learning iteration 1700/4000 [0m

                       Computation: 3255 steps/s (collection: 0.473s, learning 2.043s)
               Value function loss: 84871.8713
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 7972.02
               Mean episode length: 374.25
                 Mean success rate: 69.50
                  Mean reward/step: 21.60
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13934592
                    Iteration time: 2.52s
                        Total time: 4387.44s
                               ETA: 5932.5s

################################################################################
                     [1m Learning iteration 1701/4000 [0m

                       Computation: 3253 steps/s (collection: 0.484s, learning 2.034s)
               Value function loss: 80487.9097
                    Surrogate loss: 0.0107
             Mean action noise std: 0.91
                       Mean reward: 7946.09
               Mean episode length: 376.13
                 Mean success rate: 69.00
                  Mean reward/step: 21.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 2.52s
                        Total time: 4389.95s
                               ETA: 5929.8s

################################################################################
                     [1m Learning iteration 1702/4000 [0m

                       Computation: 3243 steps/s (collection: 0.479s, learning 2.047s)
               Value function loss: 107321.1744
                    Surrogate loss: 0.0126
             Mean action noise std: 0.91
                       Mean reward: 8229.89
               Mean episode length: 383.18
                 Mean success rate: 71.00
                  Mean reward/step: 22.55
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13950976
                    Iteration time: 2.53s
                        Total time: 4392.48s
                               ETA: 5927.1s

################################################################################
                     [1m Learning iteration 1703/4000 [0m

                       Computation: 3231 steps/s (collection: 0.477s, learning 2.058s)
               Value function loss: 88205.7230
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 8333.30
               Mean episode length: 386.56
                 Mean success rate: 72.50
                  Mean reward/step: 22.51
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 2.53s
                        Total time: 4395.01s
                               ETA: 5924.5s

################################################################################
                     [1m Learning iteration 1704/4000 [0m

                       Computation: 3264 steps/s (collection: 0.452s, learning 2.058s)
               Value function loss: 65405.6438
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 8396.04
               Mean episode length: 391.46
                 Mean success rate: 73.50
                  Mean reward/step: 21.96
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13967360
                    Iteration time: 2.51s
                        Total time: 4397.52s
                               ETA: 5921.8s

################################################################################
                     [1m Learning iteration 1705/4000 [0m

                       Computation: 3261 steps/s (collection: 0.449s, learning 2.063s)
               Value function loss: 65939.4346
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 8349.03
               Mean episode length: 392.15
                 Mean success rate: 73.00
                  Mean reward/step: 22.22
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 2.51s
                        Total time: 4400.03s
                               ETA: 5919.2s

################################################################################
                     [1m Learning iteration 1706/4000 [0m

                       Computation: 3266 steps/s (collection: 0.466s, learning 2.042s)
               Value function loss: 78864.7195
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 8252.23
               Mean episode length: 386.87
                 Mean success rate: 72.00
                  Mean reward/step: 22.59
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13983744
                    Iteration time: 2.51s
                        Total time: 4402.54s
                               ETA: 5916.5s

################################################################################
                     [1m Learning iteration 1707/4000 [0m

                       Computation: 3286 steps/s (collection: 0.460s, learning 2.032s)
               Value function loss: 107778.1740
                    Surrogate loss: 0.0166
             Mean action noise std: 0.91
                       Mean reward: 8247.20
               Mean episode length: 390.44
                 Mean success rate: 73.00
                  Mean reward/step: 22.24
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 2.49s
                        Total time: 4405.04s
                               ETA: 5913.8s

################################################################################
                     [1m Learning iteration 1708/4000 [0m

                       Computation: 3211 steps/s (collection: 0.466s, learning 2.085s)
               Value function loss: 76510.8376
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 8213.69
               Mean episode length: 388.41
                 Mean success rate: 73.00
                  Mean reward/step: 21.83
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14000128
                    Iteration time: 2.55s
                        Total time: 4407.59s
                               ETA: 5911.2s

################################################################################
                     [1m Learning iteration 1709/4000 [0m

                       Computation: 3129 steps/s (collection: 0.477s, learning 2.141s)
               Value function loss: 106177.2574
                    Surrogate loss: 0.0159
             Mean action noise std: 0.91
                       Mean reward: 8125.29
               Mean episode length: 384.97
                 Mean success rate: 72.00
                  Mean reward/step: 21.90
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 2.62s
                        Total time: 4410.20s
                               ETA: 5908.6s

################################################################################
                     [1m Learning iteration 1710/4000 [0m

                       Computation: 3103 steps/s (collection: 0.509s, learning 2.131s)
               Value function loss: 114661.3182
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 8433.78
               Mean episode length: 391.79
                 Mean success rate: 74.50
                  Mean reward/step: 21.08
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14016512
                    Iteration time: 2.64s
                        Total time: 4412.84s
                               ETA: 5906.1s

################################################################################
                     [1m Learning iteration 1711/4000 [0m

                       Computation: 3079 steps/s (collection: 0.538s, learning 2.122s)
               Value function loss: 102067.1865
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 8653.73
               Mean episode length: 395.33
                 Mean success rate: 75.00
                  Mean reward/step: 20.07
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 2.66s
                        Total time: 4415.50s
                               ETA: 5903.7s

################################################################################
                     [1m Learning iteration 1712/4000 [0m

                       Computation: 3207 steps/s (collection: 0.452s, learning 2.101s)
               Value function loss: 69396.0067
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 8449.14
               Mean episode length: 388.81
                 Mean success rate: 74.50
                  Mean reward/step: 19.85
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14032896
                    Iteration time: 2.55s
                        Total time: 4418.06s
                               ETA: 5901.1s

################################################################################
                     [1m Learning iteration 1713/4000 [0m

                       Computation: 3204 steps/s (collection: 0.465s, learning 2.092s)
               Value function loss: 81655.7166
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 8728.38
               Mean episode length: 394.61
                 Mean success rate: 76.00
                  Mean reward/step: 19.84
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 2.56s
                        Total time: 4420.61s
                               ETA: 5898.5s

################################################################################
                     [1m Learning iteration 1714/4000 [0m

                       Computation: 3148 steps/s (collection: 0.482s, learning 2.120s)
               Value function loss: 89238.8035
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 8584.68
               Mean episode length: 386.35
                 Mean success rate: 75.50
                  Mean reward/step: 20.25
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14049280
                    Iteration time: 2.60s
                        Total time: 4423.22s
                               ETA: 5895.9s

################################################################################
                     [1m Learning iteration 1715/4000 [0m

                       Computation: 3116 steps/s (collection: 0.498s, learning 2.130s)
               Value function loss: 67796.0238
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 8321.78
               Mean episode length: 376.55
                 Mean success rate: 73.00
                  Mean reward/step: 20.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 2.63s
                        Total time: 4425.84s
                               ETA: 5893.4s

################################################################################
                     [1m Learning iteration 1716/4000 [0m

                       Computation: 3147 steps/s (collection: 0.486s, learning 2.117s)
               Value function loss: 70449.6184
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 8249.81
               Mean episode length: 375.70
                 Mean success rate: 72.00
                  Mean reward/step: 20.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14065664
                    Iteration time: 2.60s
                        Total time: 4428.45s
                               ETA: 5890.8s

################################################################################
                     [1m Learning iteration 1717/4000 [0m

                       Computation: 3211 steps/s (collection: 0.454s, learning 2.097s)
               Value function loss: 79009.0432
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 7733.19
               Mean episode length: 361.79
                 Mean success rate: 68.00
                  Mean reward/step: 20.80
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 2.55s
                        Total time: 4431.00s
                               ETA: 5888.2s

################################################################################
                     [1m Learning iteration 1718/4000 [0m

                       Computation: 3007 steps/s (collection: 0.580s, learning 2.144s)
               Value function loss: 99760.7421
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 7621.61
               Mean episode length: 360.62
                 Mean success rate: 68.00
                  Mean reward/step: 20.54
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14082048
                    Iteration time: 2.72s
                        Total time: 4433.72s
                               ETA: 5885.8s

################################################################################
                     [1m Learning iteration 1719/4000 [0m

                       Computation: 3227 steps/s (collection: 0.484s, learning 2.054s)
               Value function loss: 70653.9979
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 7271.12
               Mean episode length: 352.85
                 Mean success rate: 65.50
                  Mean reward/step: 20.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 2.54s
                        Total time: 4436.26s
                               ETA: 5883.2s

################################################################################
                     [1m Learning iteration 1720/4000 [0m

                       Computation: 3292 steps/s (collection: 0.441s, learning 2.047s)
               Value function loss: 70237.2021
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 6878.68
               Mean episode length: 343.02
                 Mean success rate: 62.50
                  Mean reward/step: 20.69
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14098432
                    Iteration time: 2.49s
                        Total time: 4438.75s
                               ETA: 5880.5s

################################################################################
                     [1m Learning iteration 1721/4000 [0m

                       Computation: 3235 steps/s (collection: 0.452s, learning 2.080s)
               Value function loss: 65517.9785
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 6814.62
               Mean episode length: 337.44
                 Mean success rate: 60.50
                  Mean reward/step: 21.94
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 2.53s
                        Total time: 4441.28s
                               ETA: 5877.9s

################################################################################
                     [1m Learning iteration 1722/4000 [0m

                       Computation: 3276 steps/s (collection: 0.453s, learning 2.048s)
               Value function loss: 67554.1824
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 6718.08
               Mean episode length: 333.82
                 Mean success rate: 60.00
                  Mean reward/step: 22.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14114816
                    Iteration time: 2.50s
                        Total time: 4443.78s
                               ETA: 5875.2s

################################################################################
                     [1m Learning iteration 1723/4000 [0m

                       Computation: 3192 steps/s (collection: 0.488s, learning 2.078s)
               Value function loss: 75550.7749
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 6918.63
               Mean episode length: 342.10
                 Mean success rate: 61.50
                  Mean reward/step: 21.48
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 2.57s
                        Total time: 4446.35s
                               ETA: 5872.6s

################################################################################
                     [1m Learning iteration 1724/4000 [0m

                       Computation: 3253 steps/s (collection: 0.473s, learning 2.045s)
               Value function loss: 64560.7620
                    Surrogate loss: 0.0101
             Mean action noise std: 0.91
                       Mean reward: 6902.23
               Mean episode length: 341.44
                 Mean success rate: 61.50
                  Mean reward/step: 21.85
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14131200
                    Iteration time: 2.52s
                        Total time: 4448.86s
                               ETA: 5869.9s

################################################################################
                     [1m Learning iteration 1725/4000 [0m

                       Computation: 3194 steps/s (collection: 0.498s, learning 2.066s)
               Value function loss: 112396.9217
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 7166.69
               Mean episode length: 345.81
                 Mean success rate: 63.50
                  Mean reward/step: 21.58
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 2.56s
                        Total time: 4451.43s
                               ETA: 5867.3s

################################################################################
                     [1m Learning iteration 1726/4000 [0m

                       Computation: 3164 steps/s (collection: 0.467s, learning 2.122s)
               Value function loss: 86239.8504
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 6994.16
               Mean episode length: 342.85
                 Mean success rate: 62.50
                  Mean reward/step: 20.83
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14147584
                    Iteration time: 2.59s
                        Total time: 4454.02s
                               ETA: 5864.8s

################################################################################
                     [1m Learning iteration 1727/4000 [0m

                       Computation: 3294 steps/s (collection: 0.441s, learning 2.046s)
               Value function loss: 93224.0477
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 7126.14
               Mean episode length: 342.94
                 Mean success rate: 63.50
                  Mean reward/step: 20.85
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 2.49s
                        Total time: 4456.50s
                               ETA: 5862.1s

################################################################################
                     [1m Learning iteration 1728/4000 [0m

                       Computation: 3272 steps/s (collection: 0.431s, learning 2.072s)
               Value function loss: 80194.3044
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 7307.50
               Mean episode length: 348.21
                 Mean success rate: 64.50
                  Mean reward/step: 21.05
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14163968
                    Iteration time: 2.50s
                        Total time: 4459.01s
                               ETA: 5859.4s

################################################################################
                     [1m Learning iteration 1729/4000 [0m

                       Computation: 3300 steps/s (collection: 0.434s, learning 2.048s)
               Value function loss: 91728.5192
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 7717.88
               Mean episode length: 359.46
                 Mean success rate: 68.00
                  Mean reward/step: 21.48
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 2.48s
                        Total time: 4461.49s
                               ETA: 5856.7s

################################################################################
                     [1m Learning iteration 1730/4000 [0m

                       Computation: 3201 steps/s (collection: 0.476s, learning 2.083s)
               Value function loss: 76298.2885
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 7643.50
               Mean episode length: 358.42
                 Mean success rate: 68.50
                  Mean reward/step: 21.60
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14180352
                    Iteration time: 2.56s
                        Total time: 4464.05s
                               ETA: 5854.1s

################################################################################
                     [1m Learning iteration 1731/4000 [0m

                       Computation: 3287 steps/s (collection: 0.442s, learning 2.050s)
               Value function loss: 56748.5418
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 7240.71
               Mean episode length: 344.27
                 Mean success rate: 65.00
                  Mean reward/step: 22.55
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 2.49s
                        Total time: 4466.54s
                               ETA: 5851.4s

################################################################################
                     [1m Learning iteration 1732/4000 [0m

                       Computation: 3247 steps/s (collection: 0.440s, learning 2.082s)
               Value function loss: 61812.2360
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 7257.04
               Mean episode length: 345.86
                 Mean success rate: 66.00
                  Mean reward/step: 22.51
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14196736
                    Iteration time: 2.52s
                        Total time: 4469.06s
                               ETA: 5848.7s

################################################################################
                     [1m Learning iteration 1733/4000 [0m

                       Computation: 3188 steps/s (collection: 0.456s, learning 2.113s)
               Value function loss: 106056.7402
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 7526.91
               Mean episode length: 351.38
                 Mean success rate: 67.00
                  Mean reward/step: 22.18
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 2.57s
                        Total time: 4471.63s
                               ETA: 5846.1s

################################################################################
                     [1m Learning iteration 1734/4000 [0m

                       Computation: 3225 steps/s (collection: 0.458s, learning 2.081s)
               Value function loss: 105452.4443
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 7873.10
               Mean episode length: 361.27
                 Mean success rate: 69.00
                  Mean reward/step: 21.46
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14213120
                    Iteration time: 2.54s
                        Total time: 4474.17s
                               ETA: 5843.5s

################################################################################
                     [1m Learning iteration 1735/4000 [0m

                       Computation: 3162 steps/s (collection: 0.493s, learning 2.098s)
               Value function loss: 73738.4884
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 7632.90
               Mean episode length: 352.04
                 Mean success rate: 66.00
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 2.59s
                        Total time: 4476.76s
                               ETA: 5840.9s

################################################################################
                     [1m Learning iteration 1736/4000 [0m

                       Computation: 3134 steps/s (collection: 0.477s, learning 2.136s)
               Value function loss: 94974.9725
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 8081.22
               Mean episode length: 363.82
                 Mean success rate: 69.00
                  Mean reward/step: 22.53
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14229504
                    Iteration time: 2.61s
                        Total time: 4479.37s
                               ETA: 5838.4s

################################################################################
                     [1m Learning iteration 1737/4000 [0m

                       Computation: 3201 steps/s (collection: 0.485s, learning 2.073s)
               Value function loss: 86046.8992
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 7686.68
               Mean episode length: 348.58
                 Mean success rate: 66.00
                  Mean reward/step: 22.46
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 2.56s
                        Total time: 4481.93s
                               ETA: 5835.8s

################################################################################
                     [1m Learning iteration 1738/4000 [0m

                       Computation: 3211 steps/s (collection: 0.459s, learning 2.091s)
               Value function loss: 103657.9031
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 7585.23
               Mean episode length: 349.00
                 Mean success rate: 66.00
                  Mean reward/step: 22.56
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14245888
                    Iteration time: 2.55s
                        Total time: 4484.48s
                               ETA: 5833.2s

################################################################################
                     [1m Learning iteration 1739/4000 [0m

                       Computation: 3141 steps/s (collection: 0.458s, learning 2.149s)
               Value function loss: 89808.8251
                    Surrogate loss: 0.0168
             Mean action noise std: 0.91
                       Mean reward: 7751.85
               Mean episode length: 352.11
                 Mean success rate: 66.00
                  Mean reward/step: 21.93
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 2.61s
                        Total time: 4487.09s
                               ETA: 5830.6s

################################################################################
                     [1m Learning iteration 1740/4000 [0m

                       Computation: 3183 steps/s (collection: 0.458s, learning 2.115s)
               Value function loss: 105685.0062
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 7978.69
               Mean episode length: 363.48
                 Mean success rate: 68.50
                  Mean reward/step: 21.50
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14262272
                    Iteration time: 2.57s
                        Total time: 4489.66s
                               ETA: 5828.1s

################################################################################
                     [1m Learning iteration 1741/4000 [0m

                       Computation: 3200 steps/s (collection: 0.456s, learning 2.104s)
               Value function loss: 89895.2202
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 7905.68
               Mean episode length: 364.20
                 Mean success rate: 68.00
                  Mean reward/step: 20.66
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 2.56s
                        Total time: 4492.22s
                               ETA: 5825.5s

################################################################################
                     [1m Learning iteration 1742/4000 [0m

                       Computation: 3240 steps/s (collection: 0.432s, learning 2.096s)
               Value function loss: 82439.3094
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 7767.72
               Mean episode length: 359.60
                 Mean success rate: 67.50
                  Mean reward/step: 21.02
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14278656
                    Iteration time: 2.53s
                        Total time: 4494.75s
                               ETA: 5822.8s

################################################################################
                     [1m Learning iteration 1743/4000 [0m

                       Computation: 3230 steps/s (collection: 0.457s, learning 2.079s)
               Value function loss: 79215.1518
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 7904.55
               Mean episode length: 366.18
                 Mean success rate: 69.50
                  Mean reward/step: 20.33
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 2.54s
                        Total time: 4497.29s
                               ETA: 5820.2s

################################################################################
                     [1m Learning iteration 1744/4000 [0m

                       Computation: 3254 steps/s (collection: 0.468s, learning 2.049s)
               Value function loss: 77289.9950
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 8200.93
               Mean episode length: 375.50
                 Mean success rate: 71.50
                  Mean reward/step: 20.38
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14295040
                    Iteration time: 2.52s
                        Total time: 4499.80s
                               ETA: 5817.5s

################################################################################
                     [1m Learning iteration 1745/4000 [0m

                       Computation: 3198 steps/s (collection: 0.474s, learning 2.088s)
               Value function loss: 76654.9216
                    Surrogate loss: 0.0165
             Mean action noise std: 0.91
                       Mean reward: 7922.36
               Mean episode length: 368.94
                 Mean success rate: 70.00
                  Mean reward/step: 21.12
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 2.56s
                        Total time: 4502.37s
                               ETA: 5814.9s

################################################################################
                     [1m Learning iteration 1746/4000 [0m

                       Computation: 3223 steps/s (collection: 0.464s, learning 2.077s)
               Value function loss: 75973.7585
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 7873.15
               Mean episode length: 367.69
                 Mean success rate: 69.50
                  Mean reward/step: 21.60
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14311424
                    Iteration time: 2.54s
                        Total time: 4504.91s
                               ETA: 5812.3s

################################################################################
                     [1m Learning iteration 1747/4000 [0m

                       Computation: 3237 steps/s (collection: 0.452s, learning 2.079s)
               Value function loss: 87740.9509
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 7576.91
               Mean episode length: 354.69
                 Mean success rate: 67.00
                  Mean reward/step: 21.64
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 2.53s
                        Total time: 4507.44s
                               ETA: 5809.6s

################################################################################
                     [1m Learning iteration 1748/4000 [0m

                       Computation: 3260 steps/s (collection: 0.459s, learning 2.053s)
               Value function loss: 110888.9719
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 7568.32
               Mean episode length: 352.14
                 Mean success rate: 67.00
                  Mean reward/step: 20.69
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14327808
                    Iteration time: 2.51s
                        Total time: 4509.95s
                               ETA: 5807.0s

################################################################################
                     [1m Learning iteration 1749/4000 [0m

                       Computation: 3182 steps/s (collection: 0.480s, learning 2.093s)
               Value function loss: 97371.1098
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 7333.99
               Mean episode length: 341.38
                 Mean success rate: 65.00
                  Mean reward/step: 20.09
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 2.57s
                        Total time: 4512.52s
                               ETA: 5804.4s

################################################################################
                     [1m Learning iteration 1750/4000 [0m

                       Computation: 3230 steps/s (collection: 0.451s, learning 2.085s)
               Value function loss: 56668.8982
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 7270.55
               Mean episode length: 338.24
                 Mean success rate: 64.00
                  Mean reward/step: 20.43
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14344192
                    Iteration time: 2.54s
                        Total time: 4515.06s
                               ETA: 5801.8s

################################################################################
                     [1m Learning iteration 1751/4000 [0m

                       Computation: 3219 steps/s (collection: 0.471s, learning 2.073s)
               Value function loss: 77997.1066
                    Surrogate loss: 0.0163
             Mean action noise std: 0.91
                       Mean reward: 6874.92
               Mean episode length: 327.79
                 Mean success rate: 60.50
                  Mean reward/step: 21.43
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 2.54s
                        Total time: 4517.60s
                               ETA: 5799.1s

################################################################################
                     [1m Learning iteration 1752/4000 [0m

                       Computation: 3252 steps/s (collection: 0.454s, learning 2.064s)
               Value function loss: 88336.0927
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 6700.67
               Mean episode length: 319.96
                 Mean success rate: 60.00
                  Mean reward/step: 21.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14360576
                    Iteration time: 2.52s
                        Total time: 4520.12s
                               ETA: 5796.5s

################################################################################
                     [1m Learning iteration 1753/4000 [0m

                       Computation: 3234 steps/s (collection: 0.467s, learning 2.065s)
               Value function loss: 68871.1448
                    Surrogate loss: 0.0116
             Mean action noise std: 0.91
                       Mean reward: 6867.95
               Mean episode length: 325.08
                 Mean success rate: 60.50
                  Mean reward/step: 21.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 2.53s
                        Total time: 4522.66s
                               ETA: 5793.8s

################################################################################
                     [1m Learning iteration 1754/4000 [0m

                       Computation: 3202 steps/s (collection: 0.476s, learning 2.082s)
               Value function loss: 94466.9175
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 6854.43
               Mean episode length: 327.62
                 Mean success rate: 60.50
                  Mean reward/step: 21.52
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14376960
                    Iteration time: 2.56s
                        Total time: 4525.21s
                               ETA: 5791.2s

################################################################################
                     [1m Learning iteration 1755/4000 [0m

                       Computation: 3188 steps/s (collection: 0.474s, learning 2.096s)
               Value function loss: 81198.5617
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 7430.75
               Mean episode length: 346.79
                 Mean success rate: 65.50
                  Mean reward/step: 20.98
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 2.57s
                        Total time: 4527.78s
                               ETA: 5788.7s

################################################################################
                     [1m Learning iteration 1756/4000 [0m

                       Computation: 3209 steps/s (collection: 0.475s, learning 2.077s)
               Value function loss: 115965.5002
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 7517.32
               Mean episode length: 355.02
                 Mean success rate: 67.50
                  Mean reward/step: 20.97
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 14393344
                    Iteration time: 2.55s
                        Total time: 4530.33s
                               ETA: 5786.0s

################################################################################
                     [1m Learning iteration 1757/4000 [0m

                       Computation: 3212 steps/s (collection: 0.464s, learning 2.086s)
               Value function loss: 98813.6172
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 7410.62
               Mean episode length: 349.95
                 Mean success rate: 66.50
                  Mean reward/step: 20.26
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 2.55s
                        Total time: 4532.88s
                               ETA: 5783.4s

################################################################################
                     [1m Learning iteration 1758/4000 [0m

                       Computation: 3260 steps/s (collection: 0.447s, learning 2.066s)
               Value function loss: 90330.6379
                    Surrogate loss: 0.0210
             Mean action noise std: 0.91
                       Mean reward: 7485.23
               Mean episode length: 355.10
                 Mean success rate: 67.00
                  Mean reward/step: 20.06
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14409728
                    Iteration time: 2.51s
                        Total time: 4535.40s
                               ETA: 5780.8s

################################################################################
                     [1m Learning iteration 1759/4000 [0m

                       Computation: 3206 steps/s (collection: 0.475s, learning 2.080s)
               Value function loss: 65538.8931
                    Surrogate loss: 0.0176
             Mean action noise std: 0.91
                       Mean reward: 7203.21
               Mean episode length: 343.62
                 Mean success rate: 65.00
                  Mean reward/step: 20.60
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 2.55s
                        Total time: 4537.95s
                               ETA: 5778.2s

################################################################################
                     [1m Learning iteration 1760/4000 [0m

                       Computation: 3270 steps/s (collection: 0.449s, learning 2.056s)
               Value function loss: 84728.1139
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 7373.29
               Mean episode length: 345.71
                 Mean success rate: 66.00
                  Mean reward/step: 21.27
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14426112
                    Iteration time: 2.50s
                        Total time: 4540.46s
                               ETA: 5775.5s

################################################################################
                     [1m Learning iteration 1761/4000 [0m

                       Computation: 3247 steps/s (collection: 0.464s, learning 2.058s)
               Value function loss: 86802.8484
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 7381.11
               Mean episode length: 348.94
                 Mean success rate: 66.00
                  Mean reward/step: 21.78
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 2.52s
                        Total time: 4542.98s
                               ETA: 5772.8s

################################################################################
                     [1m Learning iteration 1762/4000 [0m

                       Computation: 3256 steps/s (collection: 0.466s, learning 2.049s)
               Value function loss: 76211.6381
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 7363.80
               Mean episode length: 350.73
                 Mean success rate: 65.50
                  Mean reward/step: 21.50
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14442496
                    Iteration time: 2.52s
                        Total time: 4545.49s
                               ETA: 5770.2s

################################################################################
                     [1m Learning iteration 1763/4000 [0m

                       Computation: 3274 steps/s (collection: 0.462s, learning 2.040s)
               Value function loss: 59109.3141
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 7023.47
               Mean episode length: 340.70
                 Mean success rate: 62.50
                  Mean reward/step: 21.67
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 2.50s
                        Total time: 4548.00s
                               ETA: 5767.5s

################################################################################
                     [1m Learning iteration 1764/4000 [0m

                       Computation: 3250 steps/s (collection: 0.463s, learning 2.057s)
               Value function loss: 105418.6800
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 7287.94
               Mean episode length: 343.30
                 Mean success rate: 62.50
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14458880
                    Iteration time: 2.52s
                        Total time: 4550.52s
                               ETA: 5764.8s

################################################################################
                     [1m Learning iteration 1765/4000 [0m

                       Computation: 3300 steps/s (collection: 0.428s, learning 2.054s)
               Value function loss: 84648.9047
                    Surrogate loss: 0.0115
             Mean action noise std: 0.91
                       Mean reward: 7466.22
               Mean episode length: 353.36
                 Mean success rate: 64.00
                  Mean reward/step: 21.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 2.48s
                        Total time: 4553.00s
                               ETA: 5762.1s

################################################################################
                     [1m Learning iteration 1766/4000 [0m

                       Computation: 3266 steps/s (collection: 0.468s, learning 2.040s)
               Value function loss: 100313.8026
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 7617.99
               Mean episode length: 359.77
                 Mean success rate: 65.50
                  Mean reward/step: 22.41
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14475264
                    Iteration time: 2.51s
                        Total time: 4555.51s
                               ETA: 5759.5s

################################################################################
                     [1m Learning iteration 1767/4000 [0m

                       Computation: 3162 steps/s (collection: 0.496s, learning 2.094s)
               Value function loss: 102211.7281
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 7516.69
               Mean episode length: 354.23
                 Mean success rate: 64.00
                  Mean reward/step: 22.17
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 2.59s
                        Total time: 4558.10s
                               ETA: 5756.9s

################################################################################
                     [1m Learning iteration 1768/4000 [0m

                       Computation: 3246 steps/s (collection: 0.439s, learning 2.084s)
               Value function loss: 77772.4772
                    Surrogate loss: 0.0168
             Mean action noise std: 0.91
                       Mean reward: 7821.90
               Mean episode length: 363.67
                 Mean success rate: 66.50
                  Mean reward/step: 21.77
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14491648
                    Iteration time: 2.52s
                        Total time: 4560.62s
                               ETA: 5754.3s

################################################################################
                     [1m Learning iteration 1769/4000 [0m

                       Computation: 3158 steps/s (collection: 0.506s, learning 2.088s)
               Value function loss: 70127.9991
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 7695.70
               Mean episode length: 364.50
                 Mean success rate: 66.00
                  Mean reward/step: 21.76
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 2.59s
                        Total time: 4563.21s
                               ETA: 5751.7s

################################################################################
                     [1m Learning iteration 1770/4000 [0m

                       Computation: 3211 steps/s (collection: 0.465s, learning 2.086s)
               Value function loss: 85136.3597
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 7837.79
               Mean episode length: 368.77
                 Mean success rate: 67.00
                  Mean reward/step: 21.32
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14508032
                    Iteration time: 2.55s
                        Total time: 4565.76s
                               ETA: 5749.1s

################################################################################
                     [1m Learning iteration 1771/4000 [0m

                       Computation: 3161 steps/s (collection: 0.521s, learning 2.071s)
               Value function loss: 59633.2937
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 7665.49
               Mean episode length: 367.34
                 Mean success rate: 66.50
                  Mean reward/step: 21.78
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 2.59s
                        Total time: 4568.36s
                               ETA: 5746.5s

################################################################################
                     [1m Learning iteration 1772/4000 [0m

                       Computation: 3155 steps/s (collection: 0.528s, learning 2.068s)
               Value function loss: 113399.6055
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 8272.01
               Mean episode length: 383.65
                 Mean success rate: 71.50
                  Mean reward/step: 21.62
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14524416
                    Iteration time: 2.60s
                        Total time: 4570.95s
                               ETA: 5744.0s

################################################################################
                     [1m Learning iteration 1773/4000 [0m

                       Computation: 3208 steps/s (collection: 0.470s, learning 2.083s)
               Value function loss: 111622.0322
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 8062.33
               Mean episode length: 377.53
                 Mean success rate: 70.00
                  Mean reward/step: 21.33
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 2.55s
                        Total time: 4573.50s
                               ETA: 5741.4s

################################################################################
                     [1m Learning iteration 1774/4000 [0m

                       Computation: 3221 steps/s (collection: 0.488s, learning 2.055s)
               Value function loss: 90222.7019
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 8149.21
               Mean episode length: 378.58
                 Mean success rate: 70.50
                  Mean reward/step: 20.29
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14540800
                    Iteration time: 2.54s
                        Total time: 4576.05s
                               ETA: 5738.8s

################################################################################
                     [1m Learning iteration 1775/4000 [0m

                       Computation: 3208 steps/s (collection: 0.463s, learning 2.090s)
               Value function loss: 98508.7933
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 7895.42
               Mean episode length: 366.96
                 Mean success rate: 67.50
                  Mean reward/step: 20.63
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 2.55s
                        Total time: 4578.60s
                               ETA: 5736.1s

################################################################################
                     [1m Learning iteration 1776/4000 [0m

                       Computation: 3230 steps/s (collection: 0.482s, learning 2.054s)
               Value function loss: 90757.9005
                    Surrogate loss: 0.0160
             Mean action noise std: 0.91
                       Mean reward: 7816.66
               Mean episode length: 365.88
                 Mean success rate: 67.50
                  Mean reward/step: 21.06
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14557184
                    Iteration time: 2.54s
                        Total time: 4581.14s
                               ETA: 5733.5s

################################################################################
                     [1m Learning iteration 1777/4000 [0m

                       Computation: 3223 steps/s (collection: 0.469s, learning 2.072s)
               Value function loss: 96182.2918
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 7855.38
               Mean episode length: 361.90
                 Mean success rate: 67.00
                  Mean reward/step: 21.26
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 2.54s
                        Total time: 4583.68s
                               ETA: 5730.9s

################################################################################
                     [1m Learning iteration 1778/4000 [0m

                       Computation: 3191 steps/s (collection: 0.497s, learning 2.070s)
               Value function loss: 100886.2252
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 7807.39
               Mean episode length: 358.32
                 Mean success rate: 66.50
                  Mean reward/step: 21.26
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14573568
                    Iteration time: 2.57s
                        Total time: 4586.24s
                               ETA: 5728.3s

################################################################################
                     [1m Learning iteration 1779/4000 [0m

                       Computation: 3276 steps/s (collection: 0.449s, learning 2.051s)
               Value function loss: 76630.3299
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 7974.85
               Mean episode length: 361.80
                 Mean success rate: 67.50
                  Mean reward/step: 21.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 2.50s
                        Total time: 4588.75s
                               ETA: 5725.6s

################################################################################
                     [1m Learning iteration 1780/4000 [0m

                       Computation: 3193 steps/s (collection: 0.461s, learning 2.105s)
               Value function loss: 62942.3002
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 8018.53
               Mean episode length: 363.35
                 Mean success rate: 68.00
                  Mean reward/step: 22.21
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14589952
                    Iteration time: 2.57s
                        Total time: 4591.31s
                               ETA: 5723.0s

################################################################################
                     [1m Learning iteration 1781/4000 [0m

                       Computation: 3253 steps/s (collection: 0.442s, learning 2.075s)
               Value function loss: 67641.2174
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 7757.47
               Mean episode length: 355.44
                 Mean success rate: 65.50
                  Mean reward/step: 22.19
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 2.52s
                        Total time: 4593.83s
                               ETA: 5720.4s

################################################################################
                     [1m Learning iteration 1782/4000 [0m

                       Computation: 3117 steps/s (collection: 0.521s, learning 2.107s)
               Value function loss: 90860.1047
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 7787.75
               Mean episode length: 361.60
                 Mean success rate: 67.00
                  Mean reward/step: 22.82
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14606336
                    Iteration time: 2.63s
                        Total time: 4596.46s
                               ETA: 5717.9s

################################################################################
                     [1m Learning iteration 1783/4000 [0m

                       Computation: 3178 steps/s (collection: 0.491s, learning 2.086s)
               Value function loss: 100829.5924
                    Surrogate loss: 0.0148
             Mean action noise std: 0.91
                       Mean reward: 7860.25
               Mean episode length: 363.23
                 Mean success rate: 67.50
                  Mean reward/step: 22.87
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 2.58s
                        Total time: 4599.03s
                               ETA: 5715.3s

################################################################################
                     [1m Learning iteration 1784/4000 [0m

                       Computation: 3158 steps/s (collection: 0.509s, learning 2.085s)
               Value function loss: 84400.8092
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 7886.13
               Mean episode length: 367.14
                 Mean success rate: 68.50
                  Mean reward/step: 22.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14622720
                    Iteration time: 2.59s
                        Total time: 4601.63s
                               ETA: 5712.7s

################################################################################
                     [1m Learning iteration 1785/4000 [0m

                       Computation: 3276 steps/s (collection: 0.458s, learning 2.042s)
               Value function loss: 99221.1941
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 8474.09
               Mean episode length: 389.18
                 Mean success rate: 73.50
                  Mean reward/step: 22.14
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 2.50s
                        Total time: 4604.13s
                               ETA: 5710.0s

################################################################################
                     [1m Learning iteration 1786/4000 [0m

                       Computation: 3183 steps/s (collection: 0.491s, learning 2.082s)
               Value function loss: 80051.4153
                    Surrogate loss: 0.0148
             Mean action noise std: 0.91
                       Mean reward: 8371.18
               Mean episode length: 383.80
                 Mean success rate: 72.50
                  Mean reward/step: 22.14
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14639104
                    Iteration time: 2.57s
                        Total time: 4606.70s
                               ETA: 5707.5s

################################################################################
                     [1m Learning iteration 1787/4000 [0m

                       Computation: 3177 steps/s (collection: 0.468s, learning 2.111s)
               Value function loss: 112239.0152
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 8424.30
               Mean episode length: 386.25
                 Mean success rate: 73.00
                  Mean reward/step: 22.07
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 2.58s
                        Total time: 4609.28s
                               ETA: 5704.9s

################################################################################
                     [1m Learning iteration 1788/4000 [0m

                       Computation: 3149 steps/s (collection: 0.518s, learning 2.083s)
               Value function loss: 76059.2797
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 8378.51
               Mean episode length: 386.36
                 Mean success rate: 73.00
                  Mean reward/step: 21.48
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14655488
                    Iteration time: 2.60s
                        Total time: 4611.88s
                               ETA: 5702.3s

################################################################################
                     [1m Learning iteration 1789/4000 [0m

                       Computation: 3232 steps/s (collection: 0.444s, learning 2.090s)
               Value function loss: 105985.4127
                    Surrogate loss: 0.0174
             Mean action noise std: 0.91
                       Mean reward: 8451.08
               Mean episode length: 386.08
                 Mean success rate: 73.50
                  Mean reward/step: 21.26
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 2.53s
                        Total time: 4614.41s
                               ETA: 5699.7s

################################################################################
                     [1m Learning iteration 1790/4000 [0m

                       Computation: 3205 steps/s (collection: 0.501s, learning 2.055s)
               Value function loss: 89754.7305
                    Surrogate loss: 0.0171
             Mean action noise std: 0.91
                       Mean reward: 8525.64
               Mean episode length: 389.80
                 Mean success rate: 74.00
                  Mean reward/step: 21.52
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14671872
                    Iteration time: 2.56s
                        Total time: 4616.97s
                               ETA: 5697.1s

################################################################################
                     [1m Learning iteration 1791/4000 [0m

                       Computation: 3224 steps/s (collection: 0.453s, learning 2.087s)
               Value function loss: 86187.6285
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 8826.92
               Mean episode length: 398.65
                 Mean success rate: 76.50
                  Mean reward/step: 21.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 2.54s
                        Total time: 4619.51s
                               ETA: 5694.5s

################################################################################
                     [1m Learning iteration 1792/4000 [0m

                       Computation: 3192 steps/s (collection: 0.498s, learning 2.069s)
               Value function loss: 93167.1168
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 8680.80
               Mean episode length: 390.79
                 Mean success rate: 74.00
                  Mean reward/step: 21.20
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14688256
                    Iteration time: 2.57s
                        Total time: 4622.08s
                               ETA: 5691.9s

################################################################################
                     [1m Learning iteration 1793/4000 [0m

                       Computation: 3213 steps/s (collection: 0.480s, learning 2.069s)
               Value function loss: 72964.2940
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 8483.70
               Mean episode length: 380.66
                 Mean success rate: 71.50
                  Mean reward/step: 20.95
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 2.55s
                        Total time: 4624.63s
                               ETA: 5689.3s

################################################################################
                     [1m Learning iteration 1794/4000 [0m

                       Computation: 3231 steps/s (collection: 0.494s, learning 2.041s)
               Value function loss: 113389.3500
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 7899.33
               Mean episode length: 359.66
                 Mean success rate: 67.00
                  Mean reward/step: 21.04
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 14704640
                    Iteration time: 2.53s
                        Total time: 4627.16s
                               ETA: 5686.6s

################################################################################
                     [1m Learning iteration 1795/4000 [0m

                       Computation: 3257 steps/s (collection: 0.462s, learning 2.052s)
               Value function loss: 85036.6603
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 8159.48
               Mean episode length: 369.23
                 Mean success rate: 69.50
                  Mean reward/step: 21.21
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 2.51s
                        Total time: 4629.68s
                               ETA: 5684.0s

################################################################################
                     [1m Learning iteration 1796/4000 [0m

                       Computation: 3247 steps/s (collection: 0.440s, learning 2.083s)
               Value function loss: 74632.5183
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 8014.13
               Mean episode length: 367.16
                 Mean success rate: 68.00
                  Mean reward/step: 22.12
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14721024
                    Iteration time: 2.52s
                        Total time: 4632.20s
                               ETA: 5681.3s

################################################################################
                     [1m Learning iteration 1797/4000 [0m

                       Computation: 3218 steps/s (collection: 0.457s, learning 2.088s)
               Value function loss: 79269.6501
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 8013.56
               Mean episode length: 366.05
                 Mean success rate: 67.50
                  Mean reward/step: 22.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 2.55s
                        Total time: 4634.74s
                               ETA: 5678.7s

################################################################################
                     [1m Learning iteration 1798/4000 [0m

                       Computation: 3288 steps/s (collection: 0.442s, learning 2.049s)
               Value function loss: 67394.5716
                    Surrogate loss: 0.0107
             Mean action noise std: 0.91
                       Mean reward: 7950.45
               Mean episode length: 365.36
                 Mean success rate: 67.50
                  Mean reward/step: 22.96
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14737408
                    Iteration time: 2.49s
                        Total time: 4637.23s
                               ETA: 5676.0s

################################################################################
                     [1m Learning iteration 1799/4000 [0m

                       Computation: 3287 steps/s (collection: 0.446s, learning 2.046s)
               Value function loss: 97821.9859
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 7877.89
               Mean episode length: 363.47
                 Mean success rate: 68.00
                  Mean reward/step: 23.06
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 2.49s
                        Total time: 4639.73s
                               ETA: 5673.4s

################################################################################
                     [1m Learning iteration 1800/4000 [0m

                       Computation: 3216 steps/s (collection: 0.483s, learning 2.063s)
               Value function loss: 67513.8237
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 7307.07
               Mean episode length: 345.92
                 Mean success rate: 64.50
                  Mean reward/step: 22.17
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14753792
                    Iteration time: 2.55s
                        Total time: 4642.27s
                               ETA: 5670.7s

################################################################################
                     [1m Learning iteration 1801/4000 [0m

                       Computation: 3237 steps/s (collection: 0.445s, learning 2.086s)
               Value function loss: 74486.7703
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 7483.55
               Mean episode length: 357.66
                 Mean success rate: 67.00
                  Mean reward/step: 22.16
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 2.53s
                        Total time: 4644.80s
                               ETA: 5668.1s

################################################################################
                     [1m Learning iteration 1802/4000 [0m

                       Computation: 3321 steps/s (collection: 0.451s, learning 2.016s)
               Value function loss: 77439.3229
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 7561.53
               Mean episode length: 356.25
                 Mean success rate: 68.00
                  Mean reward/step: 23.14
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14770176
                    Iteration time: 2.47s
                        Total time: 4647.27s
                               ETA: 5665.4s

################################################################################
                     [1m Learning iteration 1803/4000 [0m

                       Computation: 3241 steps/s (collection: 0.461s, learning 2.066s)
               Value function loss: 112515.2563
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 7738.74
               Mean episode length: 362.01
                 Mean success rate: 68.50
                  Mean reward/step: 23.04
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 2.53s
                        Total time: 4649.80s
                               ETA: 5662.8s

################################################################################
                     [1m Learning iteration 1804/4000 [0m

                       Computation: 3250 steps/s (collection: 0.466s, learning 2.054s)
               Value function loss: 80791.4300
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 7497.99
               Mean episode length: 352.80
                 Mean success rate: 66.50
                  Mean reward/step: 22.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14786560
                    Iteration time: 2.52s
                        Total time: 4652.32s
                               ETA: 5660.1s

################################################################################
                     [1m Learning iteration 1805/4000 [0m

                       Computation: 3212 steps/s (collection: 0.497s, learning 2.053s)
               Value function loss: 80468.3117
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 7668.59
               Mean episode length: 355.71
                 Mean success rate: 67.50
                  Mean reward/step: 22.74
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 2.55s
                        Total time: 4654.87s
                               ETA: 5657.5s

################################################################################
                     [1m Learning iteration 1806/4000 [0m

                       Computation: 3326 steps/s (collection: 0.433s, learning 2.030s)
               Value function loss: 93723.6547
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 7933.04
               Mean episode length: 366.18
                 Mean success rate: 70.00
                  Mean reward/step: 23.10
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14802944
                    Iteration time: 2.46s
                        Total time: 4657.33s
                               ETA: 5654.8s

################################################################################
                     [1m Learning iteration 1807/4000 [0m

                       Computation: 3256 steps/s (collection: 0.446s, learning 2.070s)
               Value function loss: 77072.1139
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 7777.98
               Mean episode length: 360.54
                 Mean success rate: 69.00
                  Mean reward/step: 22.97
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 2.52s
                        Total time: 4659.85s
                               ETA: 5652.1s

################################################################################
                     [1m Learning iteration 1808/4000 [0m

                       Computation: 3235 steps/s (collection: 0.475s, learning 2.058s)
               Value function loss: 108402.2924
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 7727.93
               Mean episode length: 354.28
                 Mean success rate: 67.00
                  Mean reward/step: 22.18
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 14819328
                    Iteration time: 2.53s
                        Total time: 4662.38s
                               ETA: 5649.5s

################################################################################
                     [1m Learning iteration 1809/4000 [0m

                       Computation: 3282 steps/s (collection: 0.471s, learning 2.025s)
               Value function loss: 84596.7842
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 8021.37
               Mean episode length: 358.64
                 Mean success rate: 68.00
                  Mean reward/step: 22.12
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 2.50s
                        Total time: 4664.87s
                               ETA: 5646.8s

################################################################################
                     [1m Learning iteration 1810/4000 [0m

                       Computation: 3151 steps/s (collection: 0.533s, learning 2.066s)
               Value function loss: 95167.8892
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 8050.43
               Mean episode length: 357.58
                 Mean success rate: 67.00
                  Mean reward/step: 22.09
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14835712
                    Iteration time: 2.60s
                        Total time: 4667.47s
                               ETA: 5644.3s

################################################################################
                     [1m Learning iteration 1811/4000 [0m

                       Computation: 3206 steps/s (collection: 0.496s, learning 2.059s)
               Value function loss: 62421.9670
                    Surrogate loss: 0.0115
             Mean action noise std: 0.91
                       Mean reward: 8184.44
               Mean episode length: 362.50
                 Mean success rate: 68.00
                  Mean reward/step: 22.04
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 2.55s
                        Total time: 4670.03s
                               ETA: 5641.7s

################################################################################
                     [1m Learning iteration 1812/4000 [0m

                       Computation: 3195 steps/s (collection: 0.492s, learning 2.072s)
               Value function loss: 95136.4553
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 8484.39
               Mean episode length: 372.85
                 Mean success rate: 70.50
                  Mean reward/step: 22.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14852096
                    Iteration time: 2.56s
                        Total time: 4672.59s
                               ETA: 5639.1s

################################################################################
                     [1m Learning iteration 1813/4000 [0m

                       Computation: 3261 steps/s (collection: 0.482s, learning 2.029s)
               Value function loss: 89518.6370
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 8671.00
               Mean episode length: 379.38
                 Mean success rate: 71.50
                  Mean reward/step: 22.35
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 2.51s
                        Total time: 4675.10s
                               ETA: 5636.4s

################################################################################
                     [1m Learning iteration 1814/4000 [0m

                       Computation: 3173 steps/s (collection: 0.477s, learning 2.105s)
               Value function loss: 102581.6945
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 8597.46
               Mean episode length: 376.40
                 Mean success rate: 71.00
                  Mean reward/step: 22.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14868480
                    Iteration time: 2.58s
                        Total time: 4677.68s
                               ETA: 5633.8s

################################################################################
                     [1m Learning iteration 1815/4000 [0m

                       Computation: 3232 steps/s (collection: 0.448s, learning 2.086s)
               Value function loss: 96039.5437
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 8626.32
               Mean episode length: 372.66
                 Mean success rate: 70.50
                  Mean reward/step: 22.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 2.53s
                        Total time: 4680.22s
                               ETA: 5631.2s

################################################################################
                     [1m Learning iteration 1816/4000 [0m

                       Computation: 3210 steps/s (collection: 0.441s, learning 2.111s)
               Value function loss: 86624.4721
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 8945.70
               Mean episode length: 381.83
                 Mean success rate: 72.00
                  Mean reward/step: 23.07
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14884864
                    Iteration time: 2.55s
                        Total time: 4682.77s
                               ETA: 5628.6s

################################################################################
                     [1m Learning iteration 1817/4000 [0m

                       Computation: 3215 steps/s (collection: 0.475s, learning 2.073s)
               Value function loss: 83113.5497
                    Surrogate loss: 0.0126
             Mean action noise std: 0.91
                       Mean reward: 8542.97
               Mean episode length: 370.16
                 Mean success rate: 69.50
                  Mean reward/step: 22.59
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 2.55s
                        Total time: 4685.32s
                               ETA: 5626.0s

################################################################################
                     [1m Learning iteration 1818/4000 [0m

                       Computation: 3264 steps/s (collection: 0.438s, learning 2.072s)
               Value function loss: 108596.1882
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 8647.87
               Mean episode length: 374.56
                 Mean success rate: 71.00
                  Mean reward/step: 22.77
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14901248
                    Iteration time: 2.51s
                        Total time: 4687.83s
                               ETA: 5623.3s

################################################################################
                     [1m Learning iteration 1819/4000 [0m

                       Computation: 3154 steps/s (collection: 0.482s, learning 2.115s)
               Value function loss: 86175.5887
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 8632.49
               Mean episode length: 375.94
                 Mean success rate: 71.50
                  Mean reward/step: 22.29
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 2.60s
                        Total time: 4690.42s
                               ETA: 5620.8s

################################################################################
                     [1m Learning iteration 1820/4000 [0m

                       Computation: 3145 steps/s (collection: 0.517s, learning 2.087s)
               Value function loss: 81412.8539
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 8701.46
               Mean episode length: 380.43
                 Mean success rate: 72.50
                  Mean reward/step: 22.19
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14917632
                    Iteration time: 2.60s
                        Total time: 4693.03s
                               ETA: 5618.2s

################################################################################
                     [1m Learning iteration 1821/4000 [0m

                       Computation: 3204 steps/s (collection: 0.473s, learning 2.084s)
               Value function loss: 90993.9170
                    Surrogate loss: 0.0191
             Mean action noise std: 0.91
                       Mean reward: 8675.11
               Mean episode length: 382.31
                 Mean success rate: 73.00
                  Mean reward/step: 22.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 2.56s
                        Total time: 4695.58s
                               ETA: 5615.6s

################################################################################
                     [1m Learning iteration 1822/4000 [0m

                       Computation: 3153 steps/s (collection: 0.482s, learning 2.115s)
               Value function loss: 90130.3917
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 8374.73
               Mean episode length: 374.12
                 Mean success rate: 70.50
                  Mean reward/step: 22.40
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14934016
                    Iteration time: 2.60s
                        Total time: 4698.18s
                               ETA: 5613.1s

################################################################################
                     [1m Learning iteration 1823/4000 [0m

                       Computation: 3233 steps/s (collection: 0.472s, learning 2.062s)
               Value function loss: 85491.6813
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 8553.54
               Mean episode length: 381.40
                 Mean success rate: 72.00
                  Mean reward/step: 22.84
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.53s
                        Total time: 4700.72s
                               ETA: 5610.4s

################################################################################
                     [1m Learning iteration 1824/4000 [0m

                       Computation: 3135 steps/s (collection: 0.482s, learning 2.131s)
               Value function loss: 113317.4421
                    Surrogate loss: 0.0186
             Mean action noise std: 0.91
                       Mean reward: 8574.23
               Mean episode length: 386.11
                 Mean success rate: 73.00
                  Mean reward/step: 22.04
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14950400
                    Iteration time: 2.61s
                        Total time: 4703.33s
                               ETA: 5607.9s

################################################################################
                     [1m Learning iteration 1825/4000 [0m

                       Computation: 3123 steps/s (collection: 0.501s, learning 2.121s)
               Value function loss: 106080.9887
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 8532.91
               Mean episode length: 382.25
                 Mean success rate: 73.00
                  Mean reward/step: 21.24
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 2.62s
                        Total time: 4705.95s
                               ETA: 5605.4s

################################################################################
                     [1m Learning iteration 1826/4000 [0m

                       Computation: 3079 steps/s (collection: 0.490s, learning 2.171s)
               Value function loss: 77821.3393
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 8641.28
               Mean episode length: 388.67
                 Mean success rate: 74.00
                  Mean reward/step: 20.11
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14966784
                    Iteration time: 2.66s
                        Total time: 4708.61s
                               ETA: 5602.9s

################################################################################
                     [1m Learning iteration 1827/4000 [0m

                       Computation: 3064 steps/s (collection: 0.530s, learning 2.143s)
               Value function loss: 82130.1843
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 8491.36
               Mean episode length: 382.50
                 Mean success rate: 72.50
                  Mean reward/step: 20.93
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 2.67s
                        Total time: 4711.28s
                               ETA: 5600.4s

################################################################################
                     [1m Learning iteration 1828/4000 [0m

                       Computation: 3157 steps/s (collection: 0.469s, learning 2.125s)
               Value function loss: 87320.1667
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 8473.72
               Mean episode length: 381.13
                 Mean success rate: 72.00
                  Mean reward/step: 21.18
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14983168
                    Iteration time: 2.59s
                        Total time: 4713.88s
                               ETA: 5597.9s

################################################################################
                     [1m Learning iteration 1829/4000 [0m

                       Computation: 3125 steps/s (collection: 0.464s, learning 2.158s)
               Value function loss: 71501.4959
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 8339.73
               Mean episode length: 375.38
                 Mean success rate: 70.50
                  Mean reward/step: 21.57
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 2.62s
                        Total time: 4716.50s
                               ETA: 5595.4s

################################################################################
                     [1m Learning iteration 1830/4000 [0m

                       Computation: 3068 steps/s (collection: 0.543s, learning 2.127s)
               Value function loss: 94658.5524
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 8289.82
               Mean episode length: 370.09
                 Mean success rate: 70.00
                  Mean reward/step: 22.10
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14999552
                    Iteration time: 2.67s
                        Total time: 4719.17s
                               ETA: 5592.9s

################################################################################
                     [1m Learning iteration 1831/4000 [0m

                       Computation: 3140 steps/s (collection: 0.480s, learning 2.129s)
               Value function loss: 59136.0233
                    Surrogate loss: 0.0160
             Mean action noise std: 0.91
                       Mean reward: 8121.76
               Mean episode length: 360.11
                 Mean success rate: 69.00
                  Mean reward/step: 21.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 2.61s
                        Total time: 4721.78s
                               ETA: 5590.4s

################################################################################
                     [1m Learning iteration 1832/4000 [0m

                       Computation: 3093 steps/s (collection: 0.471s, learning 2.178s)
               Value function loss: 84685.2823
                    Surrogate loss: 0.0156
             Mean action noise std: 0.91
                       Mean reward: 7831.38
               Mean episode length: 353.52
                 Mean success rate: 67.00
                  Mean reward/step: 21.94
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15015936
                    Iteration time: 2.65s
                        Total time: 4724.43s
                               ETA: 5587.9s

################################################################################
                     [1m Learning iteration 1833/4000 [0m

                       Computation: 3173 steps/s (collection: 0.498s, learning 2.083s)
               Value function loss: 111037.4146
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 8077.77
               Mean episode length: 364.16
                 Mean success rate: 69.00
                  Mean reward/step: 21.48
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 2.58s
                        Total time: 4727.01s
                               ETA: 5585.3s

################################################################################
                     [1m Learning iteration 1834/4000 [0m

                       Computation: 3042 steps/s (collection: 0.538s, learning 2.154s)
               Value function loss: 107152.5470
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 7919.96
               Mean episode length: 360.15
                 Mean success rate: 68.00
                  Mean reward/step: 20.95
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15032320
                    Iteration time: 2.69s
                        Total time: 4729.70s
                               ETA: 5582.9s

################################################################################
                     [1m Learning iteration 1835/4000 [0m

                       Computation: 3075 steps/s (collection: 0.531s, learning 2.132s)
               Value function loss: 81910.2864
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 7913.04
               Mean episode length: 361.40
                 Mean success rate: 68.00
                  Mean reward/step: 21.00
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 2.66s
                        Total time: 4732.36s
                               ETA: 5580.4s

################################################################################
                     [1m Learning iteration 1836/4000 [0m

                       Computation: 3126 steps/s (collection: 0.503s, learning 2.117s)
               Value function loss: 94849.5716
                    Surrogate loss: 0.0170
             Mean action noise std: 0.91
                       Mean reward: 7732.05
               Mean episode length: 354.80
                 Mean success rate: 66.50
                  Mean reward/step: 21.35
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15048704
                    Iteration time: 2.62s
                        Total time: 4734.98s
                               ETA: 5577.8s

################################################################################
                     [1m Learning iteration 1837/4000 [0m

                       Computation: 3312 steps/s (collection: 0.441s, learning 2.032s)
               Value function loss: 71258.2232
                    Surrogate loss: 0.0112
             Mean action noise std: 0.91
                       Mean reward: 7605.55
               Mean episode length: 357.19
                 Mean success rate: 67.00
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 2.47s
                        Total time: 4737.46s
                               ETA: 5575.1s

################################################################################
                     [1m Learning iteration 1838/4000 [0m

                       Computation: 3270 steps/s (collection: 0.438s, learning 2.067s)
               Value function loss: 62787.7786
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 7598.29
               Mean episode length: 355.41
                 Mean success rate: 67.50
                  Mean reward/step: 22.30
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15065088
                    Iteration time: 2.50s
                        Total time: 4739.96s
                               ETA: 5572.5s

################################################################################
                     [1m Learning iteration 1839/4000 [0m

                       Computation: 3246 steps/s (collection: 0.480s, learning 2.043s)
               Value function loss: 81823.9945
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 7575.29
               Mean episode length: 358.06
                 Mean success rate: 67.50
                  Mean reward/step: 22.79
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 2.52s
                        Total time: 4742.49s
                               ETA: 5569.8s

################################################################################
                     [1m Learning iteration 1840/4000 [0m

                       Computation: 3285 steps/s (collection: 0.419s, learning 2.075s)
               Value function loss: 95330.5382
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 7824.67
               Mean episode length: 367.11
                 Mean success rate: 69.00
                  Mean reward/step: 22.87
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15081472
                    Iteration time: 2.49s
                        Total time: 4744.98s
                               ETA: 5567.2s

################################################################################
                     [1m Learning iteration 1841/4000 [0m

                       Computation: 3280 steps/s (collection: 0.443s, learning 2.054s)
               Value function loss: 105873.1207
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 8003.57
               Mean episode length: 369.74
                 Mean success rate: 70.50
                  Mean reward/step: 22.11
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 2.50s
                        Total time: 4747.48s
                               ETA: 5564.5s

################################################################################
                     [1m Learning iteration 1842/4000 [0m

                       Computation: 3306 steps/s (collection: 0.438s, learning 2.040s)
               Value function loss: 66569.9652
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 7783.65
               Mean episode length: 359.29
                 Mean success rate: 67.50
                  Mean reward/step: 21.87
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15097856
                    Iteration time: 2.48s
                        Total time: 4749.95s
                               ETA: 5561.8s

################################################################################
                     [1m Learning iteration 1843/4000 [0m

                       Computation: 3310 steps/s (collection: 0.425s, learning 2.049s)
               Value function loss: 114420.6643
                    Surrogate loss: 0.0169
             Mean action noise std: 0.91
                       Mean reward: 7731.68
               Mean episode length: 357.60
                 Mean success rate: 67.50
                  Mean reward/step: 21.99
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 2.47s
                        Total time: 4752.43s
                               ETA: 5559.1s

################################################################################
                     [1m Learning iteration 1844/4000 [0m

                       Computation: 3261 steps/s (collection: 0.454s, learning 2.058s)
               Value function loss: 79387.3081
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 7811.48
               Mean episode length: 361.57
                 Mean success rate: 68.50
                  Mean reward/step: 21.57
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15114240
                    Iteration time: 2.51s
                        Total time: 4754.94s
                               ETA: 5556.5s

################################################################################
                     [1m Learning iteration 1845/4000 [0m

                       Computation: 3258 steps/s (collection: 0.437s, learning 2.077s)
               Value function loss: 85567.1125
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 7795.74
               Mean episode length: 361.75
                 Mean success rate: 68.50
                  Mean reward/step: 21.85
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 2.51s
                        Total time: 4757.45s
                               ETA: 5553.8s

################################################################################
                     [1m Learning iteration 1846/4000 [0m

                       Computation: 3220 steps/s (collection: 0.482s, learning 2.062s)
               Value function loss: 103228.0531
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 7976.40
               Mean episode length: 367.55
                 Mean success rate: 69.00
                  Mean reward/step: 21.98
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15130624
                    Iteration time: 2.54s
                        Total time: 4760.00s
                               ETA: 5551.2s

################################################################################
                     [1m Learning iteration 1847/4000 [0m

                       Computation: 3315 steps/s (collection: 0.428s, learning 2.042s)
               Value function loss: 72344.2434
                    Surrogate loss: 0.0187
             Mean action noise std: 0.91
                       Mean reward: 8024.80
               Mean episode length: 369.73
                 Mean success rate: 69.50
                  Mean reward/step: 22.06
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 2.47s
                        Total time: 4762.47s
                               ETA: 5548.5s

################################################################################
                     [1m Learning iteration 1848/4000 [0m

                       Computation: 3247 steps/s (collection: 0.429s, learning 2.094s)
               Value function loss: 109845.9723
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 7968.87
               Mean episode length: 362.86
                 Mean success rate: 68.00
                  Mean reward/step: 22.05
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15147008
                    Iteration time: 2.52s
                        Total time: 4764.99s
                               ETA: 5545.8s

################################################################################
                     [1m Learning iteration 1849/4000 [0m

                       Computation: 3181 steps/s (collection: 0.467s, learning 2.108s)
               Value function loss: 93159.8808
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 7850.05
               Mean episode length: 359.72
                 Mean success rate: 68.50
                  Mean reward/step: 20.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 2.57s
                        Total time: 4767.57s
                               ETA: 5543.3s

################################################################################
                     [1m Learning iteration 1850/4000 [0m

                       Computation: 3055 steps/s (collection: 0.533s, learning 2.148s)
               Value function loss: 114614.2843
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 7651.14
               Mean episode length: 354.89
                 Mean success rate: 67.50
                  Mean reward/step: 20.16
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 15163392
                    Iteration time: 2.68s
                        Total time: 4770.25s
                               ETA: 5540.8s

################################################################################
                     [1m Learning iteration 1851/4000 [0m

                       Computation: 3066 steps/s (collection: 0.527s, learning 2.144s)
               Value function loss: 62053.1615
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 7747.59
               Mean episode length: 356.13
                 Mean success rate: 68.50
                  Mean reward/step: 20.96
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 2.67s
                        Total time: 4772.92s
                               ETA: 5538.3s

################################################################################
                     [1m Learning iteration 1852/4000 [0m

                       Computation: 3213 steps/s (collection: 0.448s, learning 2.102s)
               Value function loss: 80569.0686
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 7574.92
               Mean episode length: 351.02
                 Mean success rate: 67.00
                  Mean reward/step: 21.43
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15179776
                    Iteration time: 2.55s
                        Total time: 4775.47s
                               ETA: 5535.7s

################################################################################
                     [1m Learning iteration 1853/4000 [0m

                       Computation: 3223 steps/s (collection: 0.447s, learning 2.095s)
               Value function loss: 81267.0800
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 7566.36
               Mean episode length: 349.25
                 Mean success rate: 67.00
                  Mean reward/step: 21.31
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 2.54s
                        Total time: 4778.01s
                               ETA: 5533.1s

################################################################################
                     [1m Learning iteration 1854/4000 [0m

                       Computation: 3130 steps/s (collection: 0.480s, learning 2.137s)
               Value function loss: 60712.1223
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 7564.92
               Mean episode length: 344.18
                 Mean success rate: 66.50
                  Mean reward/step: 21.47
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15196160
                    Iteration time: 2.62s
                        Total time: 4780.63s
                               ETA: 5530.6s

################################################################################
                     [1m Learning iteration 1855/4000 [0m

                       Computation: 3144 steps/s (collection: 0.483s, learning 2.122s)
               Value function loss: 66510.9580
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 7413.84
               Mean episode length: 338.08
                 Mean success rate: 65.00
                  Mean reward/step: 22.32
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 2.60s
                        Total time: 4783.23s
                               ETA: 5528.0s

################################################################################
                     [1m Learning iteration 1856/4000 [0m

                       Computation: 3142 steps/s (collection: 0.476s, learning 2.131s)
               Value function loss: 87104.7552
                    Surrogate loss: 0.0173
             Mean action noise std: 0.91
                       Mean reward: 7667.62
               Mean episode length: 347.85
                 Mean success rate: 67.00
                  Mean reward/step: 22.81
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15212544
                    Iteration time: 2.61s
                        Total time: 4785.84s
                               ETA: 5525.5s

################################################################################
                     [1m Learning iteration 1857/4000 [0m

                       Computation: 3262 steps/s (collection: 0.445s, learning 2.066s)
               Value function loss: 74282.2783
                    Surrogate loss: 0.0173
             Mean action noise std: 0.91
                       Mean reward: 7598.92
               Mean episode length: 346.93
                 Mean success rate: 66.50
                  Mean reward/step: 21.78
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 2.51s
                        Total time: 4788.35s
                               ETA: 5522.8s

################################################################################
                     [1m Learning iteration 1858/4000 [0m

                       Computation: 3188 steps/s (collection: 0.464s, learning 2.105s)
               Value function loss: 88836.4949
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 7808.23
               Mean episode length: 355.50
                 Mean success rate: 67.50
                  Mean reward/step: 22.40
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15228928
                    Iteration time: 2.57s
                        Total time: 4790.92s
                               ETA: 5520.3s

################################################################################
                     [1m Learning iteration 1859/4000 [0m

                       Computation: 3101 steps/s (collection: 0.534s, learning 2.107s)
               Value function loss: 111904.5016
                    Surrogate loss: 0.0112
             Mean action noise std: 0.91
                       Mean reward: 7981.25
               Mean episode length: 358.96
                 Mean success rate: 68.00
                  Mean reward/step: 22.40
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 2.64s
                        Total time: 4793.56s
                               ETA: 5517.7s

################################################################################
                     [1m Learning iteration 1860/4000 [0m

                       Computation: 3238 steps/s (collection: 0.485s, learning 2.045s)
               Value function loss: 66139.7241
                    Surrogate loss: 0.0098
             Mean action noise std: 0.91
                       Mean reward: 8053.60
               Mean episode length: 363.26
                 Mean success rate: 68.50
                  Mean reward/step: 22.32
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15245312
                    Iteration time: 2.53s
                        Total time: 4796.09s
                               ETA: 5515.1s

################################################################################
                     [1m Learning iteration 1861/4000 [0m

                       Computation: 3165 steps/s (collection: 0.520s, learning 2.068s)
               Value function loss: 105175.4225
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 7926.26
               Mean episode length: 363.25
                 Mean success rate: 67.50
                  Mean reward/step: 22.55
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 2.59s
                        Total time: 4798.68s
                               ETA: 5512.6s

################################################################################
                     [1m Learning iteration 1862/4000 [0m

                       Computation: 3194 steps/s (collection: 0.456s, learning 2.108s)
               Value function loss: 83321.9135
                    Surrogate loss: 0.0107
             Mean action noise std: 0.91
                       Mean reward: 7707.55
               Mean episode length: 361.86
                 Mean success rate: 67.00
                  Mean reward/step: 21.79
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15261696
                    Iteration time: 2.56s
                        Total time: 4801.24s
                               ETA: 5510.0s

################################################################################
                     [1m Learning iteration 1863/4000 [0m

                       Computation: 3188 steps/s (collection: 0.501s, learning 2.068s)
               Value function loss: 89260.4336
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 7328.00
               Mean episode length: 347.16
                 Mean success rate: 64.00
                  Mean reward/step: 21.64
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 2.57s
                        Total time: 4803.81s
                               ETA: 5507.4s

################################################################################
                     [1m Learning iteration 1864/4000 [0m

                       Computation: 3118 steps/s (collection: 0.532s, learning 2.095s)
               Value function loss: 123796.1012
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 7177.92
               Mean episode length: 339.74
                 Mean success rate: 63.00
                  Mean reward/step: 21.64
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15278080
                    Iteration time: 2.63s
                        Total time: 4806.44s
                               ETA: 5504.9s

################################################################################
                     [1m Learning iteration 1865/4000 [0m

                       Computation: 3146 steps/s (collection: 0.505s, learning 2.098s)
               Value function loss: 111361.8773
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 7185.95
               Mean episode length: 340.81
                 Mean success rate: 63.50
                  Mean reward/step: 21.35
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 2.60s
                        Total time: 4809.04s
                               ETA: 5502.3s

################################################################################
                     [1m Learning iteration 1866/4000 [0m

                       Computation: 3157 steps/s (collection: 0.487s, learning 2.107s)
               Value function loss: 121533.8871
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 6828.66
               Mean episode length: 330.38
                 Mean success rate: 61.00
                  Mean reward/step: 20.45
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 15294464
                    Iteration time: 2.59s
                        Total time: 4811.64s
                               ETA: 5499.7s

################################################################################
                     [1m Learning iteration 1867/4000 [0m

                       Computation: 3121 steps/s (collection: 0.536s, learning 2.089s)
               Value function loss: 84739.3791
                    Surrogate loss: 0.0174
             Mean action noise std: 0.91
                       Mean reward: 7057.64
               Mean episode length: 334.31
                 Mean success rate: 62.00
                  Mean reward/step: 20.34
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 2.62s
                        Total time: 4814.26s
                               ETA: 5497.2s

################################################################################
                     [1m Learning iteration 1868/4000 [0m

                       Computation: 3129 steps/s (collection: 0.503s, learning 2.115s)
               Value function loss: 81231.7611
                    Surrogate loss: 0.0111
             Mean action noise std: 0.91
                       Mean reward: 7076.67
               Mean episode length: 331.08
                 Mean success rate: 62.50
                  Mean reward/step: 20.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15310848
                    Iteration time: 2.62s
                        Total time: 4816.88s
                               ETA: 5494.7s

################################################################################
                     [1m Learning iteration 1869/4000 [0m

                       Computation: 3167 steps/s (collection: 0.476s, learning 2.110s)
               Value function loss: 81782.1165
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 7147.58
               Mean episode length: 324.19
                 Mean success rate: 61.50
                  Mean reward/step: 20.71
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 2.59s
                        Total time: 4819.46s
                               ETA: 5492.1s

################################################################################
                     [1m Learning iteration 1870/4000 [0m

                       Computation: 3194 steps/s (collection: 0.489s, learning 2.075s)
               Value function loss: 58636.6403
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 7109.02
               Mean episode length: 322.52
                 Mean success rate: 60.00
                  Mean reward/step: 20.87
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15327232
                    Iteration time: 2.56s
                        Total time: 4822.03s
                               ETA: 5489.5s

################################################################################
                     [1m Learning iteration 1871/4000 [0m

                       Computation: 3137 steps/s (collection: 0.505s, learning 2.106s)
               Value function loss: 99977.3254
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 7222.99
               Mean episode length: 330.36
                 Mean success rate: 60.50
                  Mean reward/step: 21.16
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 2.61s
                        Total time: 4824.64s
                               ETA: 5487.0s

################################################################################
                     [1m Learning iteration 1872/4000 [0m

                       Computation: 3090 steps/s (collection: 0.515s, learning 2.135s)
               Value function loss: 94931.2752
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 6828.24
               Mean episode length: 316.96
                 Mean success rate: 57.00
                  Mean reward/step: 20.67
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 15343616
                    Iteration time: 2.65s
                        Total time: 4827.29s
                               ETA: 5484.5s

################################################################################
                     [1m Learning iteration 1873/4000 [0m

                       Computation: 3184 steps/s (collection: 0.483s, learning 2.090s)
               Value function loss: 58652.1814
                    Surrogate loss: 0.0109
             Mean action noise std: 0.91
                       Mean reward: 6548.76
               Mean episode length: 304.65
                 Mean success rate: 54.00
                  Mean reward/step: 20.59
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 2.57s
                        Total time: 4829.86s
                               ETA: 5481.9s

################################################################################
                     [1m Learning iteration 1874/4000 [0m

                       Computation: 3222 steps/s (collection: 0.474s, learning 2.068s)
               Value function loss: 86036.5375
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 6379.46
               Mean episode length: 301.17
                 Mean success rate: 53.00
                  Mean reward/step: 21.27
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15360000
                    Iteration time: 2.54s
                        Total time: 4832.40s
                               ETA: 5479.3s

################################################################################
                     [1m Learning iteration 1875/4000 [0m

                       Computation: 3165 steps/s (collection: 0.490s, learning 2.099s)
               Value function loss: 77016.5021
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 6414.96
               Mean episode length: 301.57
                 Mean success rate: 53.50
                  Mean reward/step: 22.05
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 2.59s
                        Total time: 4834.99s
                               ETA: 5476.7s

################################################################################
                     [1m Learning iteration 1876/4000 [0m

                       Computation: 3099 steps/s (collection: 0.528s, learning 2.115s)
               Value function loss: 87454.2853
                    Surrogate loss: 0.0126
             Mean action noise std: 0.91
                       Mean reward: 6750.03
               Mean episode length: 315.17
                 Mean success rate: 56.00
                  Mean reward/step: 22.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15376384
                    Iteration time: 2.64s
                        Total time: 4837.64s
                               ETA: 5474.2s

################################################################################
                     [1m Learning iteration 1877/4000 [0m

                       Computation: 3224 steps/s (collection: 0.460s, learning 2.081s)
               Value function loss: 94290.6213
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 6946.57
               Mean episode length: 323.24
                 Mean success rate: 57.50
                  Mean reward/step: 22.48
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 2.54s
                        Total time: 4840.18s
                               ETA: 5471.6s

################################################################################
                     [1m Learning iteration 1878/4000 [0m

                       Computation: 3189 steps/s (collection: 0.501s, learning 2.067s)
               Value function loss: 79585.6635
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 6869.53
               Mean episode length: 321.21
                 Mean success rate: 58.00
                  Mean reward/step: 22.33
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15392768
                    Iteration time: 2.57s
                        Total time: 4842.74s
                               ETA: 5469.0s

################################################################################
                     [1m Learning iteration 1879/4000 [0m

                       Computation: 3205 steps/s (collection: 0.465s, learning 2.091s)
               Value function loss: 79587.3033
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 7105.46
               Mean episode length: 326.81
                 Mean success rate: 59.50
                  Mean reward/step: 22.27
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 2.56s
                        Total time: 4845.30s
                               ETA: 5466.4s

################################################################################
                     [1m Learning iteration 1880/4000 [0m

                       Computation: 3083 steps/s (collection: 0.536s, learning 2.120s)
               Value function loss: 108717.4996
                    Surrogate loss: 0.0156
             Mean action noise std: 0.91
                       Mean reward: 7342.92
               Mean episode length: 334.66
                 Mean success rate: 62.50
                  Mean reward/step: 22.07
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15409152
                    Iteration time: 2.66s
                        Total time: 4847.96s
                               ETA: 5463.9s

################################################################################
                     [1m Learning iteration 1881/4000 [0m

                       Computation: 3199 steps/s (collection: 0.488s, learning 2.072s)
               Value function loss: 98756.3062
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 7324.49
               Mean episode length: 336.31
                 Mean success rate: 62.50
                  Mean reward/step: 21.61
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 2.56s
                        Total time: 4850.52s
                               ETA: 5461.3s

################################################################################
                     [1m Learning iteration 1882/4000 [0m

                       Computation: 3229 steps/s (collection: 0.454s, learning 2.083s)
               Value function loss: 117876.5261
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 7897.94
               Mean episode length: 352.96
                 Mean success rate: 67.00
                  Mean reward/step: 21.36
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 15425536
                    Iteration time: 2.54s
                        Total time: 4853.05s
                               ETA: 5458.7s

################################################################################
                     [1m Learning iteration 1883/4000 [0m

                       Computation: 3247 steps/s (collection: 0.465s, learning 2.057s)
               Value function loss: 92590.8332
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 7398.41
               Mean episode length: 342.73
                 Mean success rate: 63.00
                  Mean reward/step: 21.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 2.52s
                        Total time: 4855.58s
                               ETA: 5456.1s

################################################################################
                     [1m Learning iteration 1884/4000 [0m

                       Computation: 3171 steps/s (collection: 0.470s, learning 2.113s)
               Value function loss: 89403.3660
                    Surrogate loss: 0.0126
             Mean action noise std: 0.91
                       Mean reward: 7431.84
               Mean episode length: 346.90
                 Mean success rate: 64.00
                  Mean reward/step: 22.07
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15441920
                    Iteration time: 2.58s
                        Total time: 4858.16s
                               ETA: 5453.5s

################################################################################
                     [1m Learning iteration 1885/4000 [0m

                       Computation: 3167 steps/s (collection: 0.503s, learning 2.083s)
               Value function loss: 88071.3834
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 7494.99
               Mean episode length: 347.91
                 Mean success rate: 64.50
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 2.59s
                        Total time: 4860.75s
                               ETA: 5450.9s

################################################################################
                     [1m Learning iteration 1886/4000 [0m

                       Computation: 3180 steps/s (collection: 0.464s, learning 2.111s)
               Value function loss: 80731.1831
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 7436.17
               Mean episode length: 349.71
                 Mean success rate: 64.00
                  Mean reward/step: 22.51
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15458304
                    Iteration time: 2.58s
                        Total time: 4863.32s
                               ETA: 5448.4s

################################################################################
                     [1m Learning iteration 1887/4000 [0m

                       Computation: 3185 steps/s (collection: 0.505s, learning 2.066s)
               Value function loss: 99972.8432
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 7542.30
               Mean episode length: 352.60
                 Mean success rate: 65.50
                  Mean reward/step: 22.11
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 2.57s
                        Total time: 4865.89s
                               ETA: 5445.8s

################################################################################
                     [1m Learning iteration 1888/4000 [0m

                       Computation: 3138 steps/s (collection: 0.522s, learning 2.088s)
               Value function loss: 103947.1033
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 7549.50
               Mean episode length: 352.71
                 Mean success rate: 64.50
                  Mean reward/step: 21.63
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15474688
                    Iteration time: 2.61s
                        Total time: 4868.50s
                               ETA: 5443.2s

################################################################################
                     [1m Learning iteration 1889/4000 [0m

                       Computation: 3182 steps/s (collection: 0.476s, learning 2.098s)
               Value function loss: 45759.0016
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 7475.32
               Mean episode length: 351.00
                 Mean success rate: 64.00
                  Mean reward/step: 21.97
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 2.57s
                        Total time: 4871.08s
                               ETA: 5440.7s

################################################################################
                     [1m Learning iteration 1890/4000 [0m

                       Computation: 3175 steps/s (collection: 0.494s, learning 2.086s)
               Value function loss: 103895.2181
                    Surrogate loss: 0.0148
             Mean action noise std: 0.91
                       Mean reward: 7294.52
               Mean episode length: 338.67
                 Mean success rate: 62.00
                  Mean reward/step: 22.21
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 15491072
                    Iteration time: 2.58s
                        Total time: 4873.66s
                               ETA: 5438.1s

################################################################################
                     [1m Learning iteration 1891/4000 [0m

                       Computation: 3182 steps/s (collection: 0.503s, learning 2.072s)
               Value function loss: 47870.7614
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 7040.53
               Mean episode length: 331.94
                 Mean success rate: 60.50
                  Mean reward/step: 22.72
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 2.57s
                        Total time: 4876.23s
                               ETA: 5435.5s

################################################################################
                     [1m Learning iteration 1892/4000 [0m

                       Computation: 3161 steps/s (collection: 0.492s, learning 2.100s)
               Value function loss: 107461.1223
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 7269.45
               Mean episode length: 331.32
                 Mean success rate: 62.50
                  Mean reward/step: 23.51
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15507456
                    Iteration time: 2.59s
                        Total time: 4878.82s
                               ETA: 5432.9s

################################################################################
                     [1m Learning iteration 1893/4000 [0m

                       Computation: 3187 steps/s (collection: 0.485s, learning 2.085s)
               Value function loss: 81836.7716
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 6780.64
               Mean episode length: 310.86
                 Mean success rate: 58.50
                  Mean reward/step: 22.44
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 2.57s
                        Total time: 4881.39s
                               ETA: 5430.4s

################################################################################
                     [1m Learning iteration 1894/4000 [0m

                       Computation: 3218 steps/s (collection: 0.464s, learning 2.082s)
               Value function loss: 95430.1393
                    Surrogate loss: 0.0107
             Mean action noise std: 0.91
                       Mean reward: 7054.51
               Mean episode length: 317.87
                 Mean success rate: 60.00
                  Mean reward/step: 22.38
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15523840
                    Iteration time: 2.55s
                        Total time: 4883.94s
                               ETA: 5427.7s

################################################################################
                     [1m Learning iteration 1895/4000 [0m

                       Computation: 3232 steps/s (collection: 0.480s, learning 2.055s)
               Value function loss: 63455.0413
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 6854.40
               Mean episode length: 311.01
                 Mean success rate: 58.50
                  Mean reward/step: 22.26
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 2.53s
                        Total time: 4886.47s
                               ETA: 5425.1s

################################################################################
                     [1m Learning iteration 1896/4000 [0m

                       Computation: 3142 steps/s (collection: 0.533s, learning 2.074s)
               Value function loss: 98853.3163
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 6897.34
               Mean episode length: 315.52
                 Mean success rate: 59.50
                  Mean reward/step: 22.48
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15540224
                    Iteration time: 2.61s
                        Total time: 4889.08s
                               ETA: 5422.6s

################################################################################
                     [1m Learning iteration 1897/4000 [0m

                       Computation: 3207 steps/s (collection: 0.509s, learning 2.046s)
               Value function loss: 124855.7279
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 7325.81
               Mean episode length: 328.48
                 Mean success rate: 63.50
                  Mean reward/step: 21.97
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 2.55s
                        Total time: 4891.63s
                               ETA: 5420.0s

################################################################################
                     [1m Learning iteration 1898/4000 [0m

                       Computation: 3177 steps/s (collection: 0.478s, learning 2.099s)
               Value function loss: 81290.5677
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 7453.01
               Mean episode length: 332.13
                 Mean success rate: 64.50
                  Mean reward/step: 21.31
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15556608
                    Iteration time: 2.58s
                        Total time: 4894.21s
                               ETA: 5417.4s

################################################################################
                     [1m Learning iteration 1899/4000 [0m

                       Computation: 3135 steps/s (collection: 0.501s, learning 2.112s)
               Value function loss: 91002.1231
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 7548.12
               Mean episode length: 335.35
                 Mean success rate: 65.00
                  Mean reward/step: 20.85
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 2.61s
                        Total time: 4896.82s
                               ETA: 5414.9s

################################################################################
                     [1m Learning iteration 1900/4000 [0m

                       Computation: 3128 steps/s (collection: 0.505s, learning 2.113s)
               Value function loss: 90746.5156
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 7322.22
               Mean episode length: 328.95
                 Mean success rate: 63.00
                  Mean reward/step: 20.68
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15572992
                    Iteration time: 2.62s
                        Total time: 4899.44s
                               ETA: 5412.3s

################################################################################
                     [1m Learning iteration 1901/4000 [0m

                       Computation: 3200 steps/s (collection: 0.487s, learning 2.073s)
               Value function loss: 84454.3426
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 7520.39
               Mean episode length: 337.60
                 Mean success rate: 66.00
                  Mean reward/step: 21.00
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 2.56s
                        Total time: 4902.00s
                               ETA: 5409.7s

################################################################################
                     [1m Learning iteration 1902/4000 [0m

                       Computation: 3269 steps/s (collection: 0.438s, learning 2.068s)
               Value function loss: 67744.2911
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 7250.03
               Mean episode length: 326.76
                 Mean success rate: 64.00
                  Mean reward/step: 21.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15589376
                    Iteration time: 2.51s
                        Total time: 4904.51s
                               ETA: 5407.1s

################################################################################
                     [1m Learning iteration 1903/4000 [0m

                       Computation: 3220 steps/s (collection: 0.472s, learning 2.072s)
               Value function loss: 107454.1961
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 7383.59
               Mean episode length: 331.38
                 Mean success rate: 64.50
                  Mean reward/step: 21.31
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 2.54s
                        Total time: 4907.05s
                               ETA: 5404.5s

################################################################################
                     [1m Learning iteration 1904/4000 [0m

                       Computation: 3178 steps/s (collection: 0.491s, learning 2.086s)
               Value function loss: 85628.2401
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 7218.46
               Mean episode length: 326.12
                 Mean success rate: 63.00
                  Mean reward/step: 20.99
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15605760
                    Iteration time: 2.58s
                        Total time: 4909.63s
                               ETA: 5401.9s

################################################################################
                     [1m Learning iteration 1905/4000 [0m

                       Computation: 3205 steps/s (collection: 0.495s, learning 2.060s)
               Value function loss: 41794.4517
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 6608.97
               Mean episode length: 305.90
                 Mean success rate: 59.00
                  Mean reward/step: 21.22
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 2.56s
                        Total time: 4912.18s
                               ETA: 5399.3s

################################################################################
                     [1m Learning iteration 1906/4000 [0m

                       Computation: 3234 steps/s (collection: 0.481s, learning 2.052s)
               Value function loss: 120103.5978
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 6645.67
               Mean episode length: 308.94
                 Mean success rate: 60.00
                  Mean reward/step: 21.20
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 15622144
                    Iteration time: 2.53s
                        Total time: 4914.72s
                               ETA: 5396.7s

################################################################################
                     [1m Learning iteration 1907/4000 [0m

                       Computation: 3194 steps/s (collection: 0.491s, learning 2.073s)
               Value function loss: 60971.5234
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 6212.63
               Mean episode length: 297.17
                 Mean success rate: 57.00
                  Mean reward/step: 21.48
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 2.56s
                        Total time: 4917.28s
                               ETA: 5394.1s

################################################################################
                     [1m Learning iteration 1908/4000 [0m

                       Computation: 3153 steps/s (collection: 0.498s, learning 2.100s)
               Value function loss: 92647.3887
                    Surrogate loss: 0.0100
             Mean action noise std: 0.91
                       Mean reward: 6495.23
               Mean episode length: 308.16
                 Mean success rate: 58.50
                  Mean reward/step: 21.51
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15638528
                    Iteration time: 2.60s
                        Total time: 4919.88s
                               ETA: 5391.5s

################################################################################
                     [1m Learning iteration 1909/4000 [0m

                       Computation: 3142 steps/s (collection: 0.511s, learning 2.096s)
               Value function loss: 89953.3359
                    Surrogate loss: 0.0082
             Mean action noise std: 0.91
                       Mean reward: 6547.44
               Mean episode length: 308.96
                 Mean success rate: 58.00
                  Mean reward/step: 21.09
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 2.61s
                        Total time: 4922.49s
                               ETA: 5389.0s

################################################################################
                     [1m Learning iteration 1910/4000 [0m

                       Computation: 3233 steps/s (collection: 0.461s, learning 2.073s)
               Value function loss: 105774.9694
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 6863.81
               Mean episode length: 320.11
                 Mean success rate: 61.00
                  Mean reward/step: 21.30
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15654912
                    Iteration time: 2.53s
                        Total time: 4925.02s
                               ETA: 5386.3s

################################################################################
                     [1m Learning iteration 1911/4000 [0m

                       Computation: 3213 steps/s (collection: 0.473s, learning 2.077s)
               Value function loss: 87602.3538
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 6465.87
               Mean episode length: 306.29
                 Mean success rate: 58.00
                  Mean reward/step: 21.13
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 2.55s
                        Total time: 4927.57s
                               ETA: 5383.7s

################################################################################
                     [1m Learning iteration 1912/4000 [0m

                       Computation: 3253 steps/s (collection: 0.469s, learning 2.048s)
               Value function loss: 95313.8965
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 6519.28
               Mean episode length: 309.61
                 Mean success rate: 58.00
                  Mean reward/step: 20.45
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15671296
                    Iteration time: 2.52s
                        Total time: 4930.09s
                               ETA: 5381.1s

################################################################################
                     [1m Learning iteration 1913/4000 [0m

                       Computation: 3185 steps/s (collection: 0.504s, learning 2.067s)
               Value function loss: 120676.2104
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 7128.71
               Mean episode length: 330.83
                 Mean success rate: 62.00
                  Mean reward/step: 20.02
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 2.57s
                        Total time: 4932.66s
                               ETA: 5378.5s

################################################################################
                     [1m Learning iteration 1914/4000 [0m

                       Computation: 3177 steps/s (collection: 0.471s, learning 2.107s)
               Value function loss: 65571.7158
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 6855.68
               Mean episode length: 326.21
                 Mean success rate: 60.00
                  Mean reward/step: 19.87
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15687680
                    Iteration time: 2.58s
                        Total time: 4935.24s
                               ETA: 5375.9s

################################################################################
                     [1m Learning iteration 1915/4000 [0m

                       Computation: 3181 steps/s (collection: 0.496s, learning 2.079s)
               Value function loss: 91930.6240
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 7305.05
               Mean episode length: 344.54
                 Mean success rate: 64.00
                  Mean reward/step: 20.40
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 2.57s
                        Total time: 4937.81s
                               ETA: 5373.3s

################################################################################
                     [1m Learning iteration 1916/4000 [0m

                       Computation: 3240 steps/s (collection: 0.473s, learning 2.055s)
               Value function loss: 79535.1028
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 6466.57
               Mean episode length: 312.45
                 Mean success rate: 56.50
                  Mean reward/step: 21.03
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 15704064
                    Iteration time: 2.53s
                        Total time: 4940.34s
                               ETA: 5370.7s

################################################################################
                     [1m Learning iteration 1917/4000 [0m

                       Computation: 3191 steps/s (collection: 0.470s, learning 2.097s)
               Value function loss: 89271.7763
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 6634.34
               Mean episode length: 318.06
                 Mean success rate: 57.00
                  Mean reward/step: 21.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 2.57s
                        Total time: 4942.91s
                               ETA: 5368.1s

################################################################################
                     [1m Learning iteration 1918/4000 [0m

                       Computation: 3247 steps/s (collection: 0.455s, learning 2.067s)
               Value function loss: 72243.5309
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 6593.87
               Mean episode length: 318.11
                 Mean success rate: 57.00
                  Mean reward/step: 21.33
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15720448
                    Iteration time: 2.52s
                        Total time: 4945.43s
                               ETA: 5365.5s

################################################################################
                     [1m Learning iteration 1919/4000 [0m

                       Computation: 3193 steps/s (collection: 0.476s, learning 2.090s)
               Value function loss: 68099.6122
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 6872.74
               Mean episode length: 326.25
                 Mean success rate: 59.00
                  Mean reward/step: 21.48
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 2.57s
                        Total time: 4947.99s
                               ETA: 5362.9s

################################################################################
                     [1m Learning iteration 1920/4000 [0m

                       Computation: 3184 steps/s (collection: 0.478s, learning 2.094s)
               Value function loss: 72892.8213
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 6773.47
               Mean episode length: 323.81
                 Mean success rate: 58.00
                  Mean reward/step: 21.89
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15736832
                    Iteration time: 2.57s
                        Total time: 4950.57s
                               ETA: 5360.3s

################################################################################
                     [1m Learning iteration 1921/4000 [0m

                       Computation: 3224 steps/s (collection: 0.468s, learning 2.072s)
               Value function loss: 91381.9478
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 6569.70
               Mean episode length: 317.92
                 Mean success rate: 57.00
                  Mean reward/step: 21.67
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 2.54s
                        Total time: 4953.11s
                               ETA: 5357.7s

################################################################################
                     [1m Learning iteration 1922/4000 [0m

                       Computation: 3145 steps/s (collection: 0.509s, learning 2.095s)
               Value function loss: 114584.0521
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 6860.41
               Mean episode length: 321.33
                 Mean success rate: 58.50
                  Mean reward/step: 20.65
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 15753216
                    Iteration time: 2.60s
                        Total time: 4955.71s
                               ETA: 5355.2s

################################################################################
                     [1m Learning iteration 1923/4000 [0m

                       Computation: 3279 steps/s (collection: 0.464s, learning 2.034s)
               Value function loss: 83996.7155
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 6892.77
               Mean episode length: 322.33
                 Mean success rate: 58.50
                  Mean reward/step: 19.95
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 2.50s
                        Total time: 4958.21s
                               ETA: 5352.5s

################################################################################
                     [1m Learning iteration 1924/4000 [0m

                       Computation: 3194 steps/s (collection: 0.479s, learning 2.085s)
               Value function loss: 63252.0186
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 6906.60
               Mean episode length: 324.36
                 Mean success rate: 60.00
                  Mean reward/step: 20.55
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15769600
                    Iteration time: 2.56s
                        Total time: 4960.77s
                               ETA: 5349.9s

################################################################################
                     [1m Learning iteration 1925/4000 [0m

                       Computation: 3231 steps/s (collection: 0.467s, learning 2.068s)
               Value function loss: 104388.9800
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 7268.34
               Mean episode length: 340.78
                 Mean success rate: 65.00
                  Mean reward/step: 21.11
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 2.54s
                        Total time: 4963.31s
                               ETA: 5347.3s

################################################################################
                     [1m Learning iteration 1926/4000 [0m

                       Computation: 3247 steps/s (collection: 0.456s, learning 2.067s)
               Value function loss: 73208.8894
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 7383.71
               Mean episode length: 344.28
                 Mean success rate: 65.50
                  Mean reward/step: 20.88
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15785984
                    Iteration time: 2.52s
                        Total time: 4965.83s
                               ETA: 5344.6s

################################################################################
                     [1m Learning iteration 1927/4000 [0m

                       Computation: 3111 steps/s (collection: 0.511s, learning 2.121s)
               Value function loss: 80407.7162
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 7486.67
               Mean episode length: 348.82
                 Mean success rate: 66.00
                  Mean reward/step: 21.02
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 2.63s
                        Total time: 4968.46s
                               ETA: 5342.1s

################################################################################
                     [1m Learning iteration 1928/4000 [0m

                       Computation: 3159 steps/s (collection: 0.529s, learning 2.064s)
               Value function loss: 88339.7695
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 7120.39
               Mean episode length: 334.48
                 Mean success rate: 63.00
                  Mean reward/step: 21.30
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 15802368
                    Iteration time: 2.59s
                        Total time: 4971.06s
                               ETA: 5339.6s

################################################################################
                     [1m Learning iteration 1929/4000 [0m

                       Computation: 3212 steps/s (collection: 0.483s, learning 2.067s)
               Value function loss: 79578.1223
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 7118.47
               Mean episode length: 333.94
                 Mean success rate: 62.50
                  Mean reward/step: 21.43
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 2.55s
                        Total time: 4973.61s
                               ETA: 5337.0s

################################################################################
                     [1m Learning iteration 1930/4000 [0m

                       Computation: 3115 steps/s (collection: 0.533s, learning 2.096s)
               Value function loss: 119199.6259
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 7097.71
               Mean episode length: 333.96
                 Mean success rate: 61.50
                  Mean reward/step: 21.69
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 15818752
                    Iteration time: 2.63s
                        Total time: 4976.24s
                               ETA: 5334.4s

################################################################################
                     [1m Learning iteration 1931/4000 [0m

                       Computation: 3248 steps/s (collection: 0.457s, learning 2.064s)
               Value function loss: 87215.4247
                    Surrogate loss: 0.0100
             Mean action noise std: 0.91
                       Mean reward: 7000.50
               Mean episode length: 328.58
                 Mean success rate: 60.50
                  Mean reward/step: 21.11
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 2.52s
                        Total time: 4978.76s
                               ETA: 5331.8s

################################################################################
                     [1m Learning iteration 1932/4000 [0m

                       Computation: 3176 steps/s (collection: 0.512s, learning 2.067s)
               Value function loss: 81983.3304
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 6754.54
               Mean episode length: 323.39
                 Mean success rate: 58.50
                  Mean reward/step: 20.80
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 15835136
                    Iteration time: 2.58s
                        Total time: 4981.34s
                               ETA: 5329.2s

################################################################################
                     [1m Learning iteration 1933/4000 [0m

                       Computation: 3261 steps/s (collection: 0.476s, learning 2.036s)
               Value function loss: 86218.3064
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 6808.45
               Mean episode length: 324.73
                 Mean success rate: 58.50
                  Mean reward/step: 20.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 2.51s
                        Total time: 4983.85s
                               ETA: 5326.6s

################################################################################
                     [1m Learning iteration 1934/4000 [0m

                       Computation: 3253 steps/s (collection: 0.462s, learning 2.056s)
               Value function loss: 88295.4979
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 6440.59
               Mean episode length: 311.42
                 Mean success rate: 56.50
                  Mean reward/step: 21.08
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15851520
                    Iteration time: 2.52s
                        Total time: 4986.37s
                               ETA: 5323.9s

################################################################################
                     [1m Learning iteration 1935/4000 [0m

                       Computation: 3300 steps/s (collection: 0.450s, learning 2.032s)
               Value function loss: 62797.2890
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 6376.22
               Mean episode length: 311.95
                 Mean success rate: 56.50
                  Mean reward/step: 21.74
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 2.48s
                        Total time: 4988.85s
                               ETA: 5321.3s

################################################################################
                     [1m Learning iteration 1936/4000 [0m

                       Computation: 3175 steps/s (collection: 0.481s, learning 2.099s)
               Value function loss: 64318.0923
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 6253.97
               Mean episode length: 306.84
                 Mean success rate: 56.00
                  Mean reward/step: 22.59
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15867904
                    Iteration time: 2.58s
                        Total time: 4991.43s
                               ETA: 5318.7s

################################################################################
                     [1m Learning iteration 1937/4000 [0m

                       Computation: 3267 steps/s (collection: 0.484s, learning 2.023s)
               Value function loss: 119588.9268
                    Surrogate loss: 0.0175
             Mean action noise std: 0.91
                       Mean reward: 6407.12
               Mean episode length: 313.87
                 Mean success rate: 57.50
                  Mean reward/step: 22.74
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 2.51s
                        Total time: 4993.94s
                               ETA: 5316.0s

################################################################################
                     [1m Learning iteration 1938/4000 [0m

                       Computation: 3319 steps/s (collection: 0.454s, learning 2.014s)
               Value function loss: 96702.3767
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 6759.55
               Mean episode length: 323.13
                 Mean success rate: 60.50
                  Mean reward/step: 21.43
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15884288
                    Iteration time: 2.47s
                        Total time: 4996.40s
                               ETA: 5313.3s

################################################################################
                     [1m Learning iteration 1939/4000 [0m

                       Computation: 3285 steps/s (collection: 0.455s, learning 2.038s)
               Value function loss: 66444.9493
                    Surrogate loss: 0.0173
             Mean action noise std: 0.91
                       Mean reward: 6779.57
               Mean episode length: 322.82
                 Mean success rate: 60.50
                  Mean reward/step: 21.10
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 2.49s
                        Total time: 4998.90s
                               ETA: 5310.7s

################################################################################
                     [1m Learning iteration 1940/4000 [0m

                       Computation: 3223 steps/s (collection: 0.466s, learning 2.075s)
               Value function loss: 77708.3622
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 6892.76
               Mean episode length: 322.76
                 Mean success rate: 60.00
                  Mean reward/step: 20.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15900672
                    Iteration time: 2.54s
                        Total time: 5001.44s
                               ETA: 5308.1s

################################################################################
                     [1m Learning iteration 1941/4000 [0m

                       Computation: 3244 steps/s (collection: 0.447s, learning 2.078s)
               Value function loss: 106810.4439
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 7150.39
               Mean episode length: 331.82
                 Mean success rate: 61.50
                  Mean reward/step: 20.19
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 2.52s
                        Total time: 5003.96s
                               ETA: 5305.4s

################################################################################
                     [1m Learning iteration 1942/4000 [0m

                       Computation: 3267 steps/s (collection: 0.445s, learning 2.062s)
               Value function loss: 78181.4120
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 7388.93
               Mean episode length: 340.60
                 Mean success rate: 63.00
                  Mean reward/step: 20.17
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15917056
                    Iteration time: 2.51s
                        Total time: 5006.47s
                               ETA: 5302.8s

################################################################################
                     [1m Learning iteration 1943/4000 [0m

                       Computation: 3224 steps/s (collection: 0.481s, learning 2.059s)
               Value function loss: 72644.5722
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 7353.28
               Mean episode length: 338.52
                 Mean success rate: 61.50
                  Mean reward/step: 20.27
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 2.54s
                        Total time: 5009.01s
                               ETA: 5300.2s

################################################################################
                     [1m Learning iteration 1944/4000 [0m

                       Computation: 3263 steps/s (collection: 0.473s, learning 2.037s)
               Value function loss: 93009.6862
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 7625.27
               Mean episode length: 349.81
                 Mean success rate: 64.50
                  Mean reward/step: 21.06
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15933440
                    Iteration time: 2.51s
                        Total time: 5011.52s
                               ETA: 5297.5s

################################################################################
                     [1m Learning iteration 1945/4000 [0m

                       Computation: 3206 steps/s (collection: 0.470s, learning 2.085s)
               Value function loss: 48248.8749
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 7020.13
               Mean episode length: 328.02
                 Mean success rate: 60.00
                  Mean reward/step: 21.14
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 2.55s
                        Total time: 5014.07s
                               ETA: 5294.9s

################################################################################
                     [1m Learning iteration 1946/4000 [0m

                       Computation: 3261 steps/s (collection: 0.467s, learning 2.045s)
               Value function loss: 115402.1811
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 6857.55
               Mean episode length: 323.43
                 Mean success rate: 59.00
                  Mean reward/step: 21.37
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 15949824
                    Iteration time: 2.51s
                        Total time: 5016.59s
                               ETA: 5292.3s

################################################################################
                     [1m Learning iteration 1947/4000 [0m

                       Computation: 3280 steps/s (collection: 0.456s, learning 2.041s)
               Value function loss: 120268.2354
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 7005.81
               Mean episode length: 331.30
                 Mean success rate: 61.50
                  Mean reward/step: 20.42
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 2.50s
                        Total time: 5019.08s
                               ETA: 5289.6s

################################################################################
                     [1m Learning iteration 1948/4000 [0m

                       Computation: 3282 steps/s (collection: 0.464s, learning 2.031s)
               Value function loss: 93468.1495
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 6965.12
               Mean episode length: 328.94
                 Mean success rate: 61.50
                  Mean reward/step: 19.83
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15966208
                    Iteration time: 2.50s
                        Total time: 5021.58s
                               ETA: 5287.0s

################################################################################
                     [1m Learning iteration 1949/4000 [0m

                       Computation: 3264 steps/s (collection: 0.469s, learning 2.041s)
               Value function loss: 81423.1995
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 6787.27
               Mean episode length: 321.86
                 Mean success rate: 60.50
                  Mean reward/step: 20.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 2.51s
                        Total time: 5024.09s
                               ETA: 5284.3s

################################################################################
                     [1m Learning iteration 1950/4000 [0m

                       Computation: 3266 steps/s (collection: 0.462s, learning 2.046s)
               Value function loss: 72336.5515
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 6469.31
               Mean episode length: 307.68
                 Mean success rate: 58.00
                  Mean reward/step: 20.39
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 15982592
                    Iteration time: 2.51s
                        Total time: 5026.60s
                               ETA: 5281.7s

################################################################################
                     [1m Learning iteration 1951/4000 [0m

                       Computation: 3299 steps/s (collection: 0.440s, learning 2.042s)
               Value function loss: 78930.8668
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 6254.69
               Mean episode length: 300.14
                 Mean success rate: 54.50
                  Mean reward/step: 20.50
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 2.48s
                        Total time: 5029.08s
                               ETA: 5279.0s

################################################################################
                     [1m Learning iteration 1952/4000 [0m

                       Computation: 3227 steps/s (collection: 0.450s, learning 2.088s)
               Value function loss: 70761.4075
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 6519.32
               Mean episode length: 310.92
                 Mean success rate: 56.00
                  Mean reward/step: 20.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15998976
                    Iteration time: 2.54s
                        Total time: 5031.62s
                               ETA: 5276.4s

################################################################################
                     [1m Learning iteration 1953/4000 [0m

                       Computation: 3172 steps/s (collection: 0.507s, learning 2.075s)
               Value function loss: 75993.0103
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 6544.62
               Mean episode length: 313.73
                 Mean success rate: 56.50
                  Mean reward/step: 21.15
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 2.58s
                        Total time: 5034.20s
                               ETA: 5273.8s

################################################################################
                     [1m Learning iteration 1954/4000 [0m

                       Computation: 3193 steps/s (collection: 0.443s, learning 2.122s)
               Value function loss: 73012.2210
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 6344.78
               Mean episode length: 310.50
                 Mean success rate: 55.50
                  Mean reward/step: 21.84
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16015360
                    Iteration time: 2.56s
                        Total time: 5036.76s
                               ETA: 5271.2s

################################################################################
                     [1m Learning iteration 1955/4000 [0m

                       Computation: 3212 steps/s (collection: 0.464s, learning 2.087s)
               Value function loss: 77219.0958
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 5981.66
               Mean episode length: 297.36
                 Mean success rate: 51.50
                  Mean reward/step: 22.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 2.55s
                        Total time: 5039.31s
                               ETA: 5268.6s

################################################################################
                     [1m Learning iteration 1956/4000 [0m

                       Computation: 3151 steps/s (collection: 0.495s, learning 2.105s)
               Value function loss: 98386.0607
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 6317.04
               Mean episode length: 306.00
                 Mean success rate: 54.50
                  Mean reward/step: 22.06
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16031744
                    Iteration time: 2.60s
                        Total time: 5041.91s
                               ETA: 5266.1s

################################################################################
                     [1m Learning iteration 1957/4000 [0m

                       Computation: 3294 steps/s (collection: 0.432s, learning 2.055s)
               Value function loss: 92542.6264
                    Surrogate loss: 0.0109
             Mean action noise std: 0.91
                       Mean reward: 6553.23
               Mean episode length: 315.70
                 Mean success rate: 57.00
                  Mean reward/step: 21.38
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 2.49s
                        Total time: 5044.40s
                               ETA: 5263.4s

################################################################################
                     [1m Learning iteration 1958/4000 [0m

                       Computation: 3223 steps/s (collection: 0.473s, learning 2.068s)
               Value function loss: 69679.5551
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 6673.66
               Mean episode length: 325.34
                 Mean success rate: 58.00
                  Mean reward/step: 21.92
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16048128
                    Iteration time: 2.54s
                        Total time: 5046.94s
                               ETA: 5260.8s

################################################################################
                     [1m Learning iteration 1959/4000 [0m

                       Computation: 3196 steps/s (collection: 0.495s, learning 2.068s)
               Value function loss: 88967.8606
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 6877.30
               Mean episode length: 332.13
                 Mean success rate: 60.50
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 2.56s
                        Total time: 5049.51s
                               ETA: 5258.2s

################################################################################
                     [1m Learning iteration 1960/4000 [0m

                       Computation: 3168 steps/s (collection: 0.491s, learning 2.094s)
               Value function loss: 76498.2898
                    Surrogate loss: 0.0186
             Mean action noise std: 0.91
                       Mean reward: 7334.95
               Mean episode length: 347.75
                 Mean success rate: 64.50
                  Mean reward/step: 22.22
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16064512
                    Iteration time: 2.59s
                        Total time: 5052.09s
                               ETA: 5255.6s

################################################################################
                     [1m Learning iteration 1961/4000 [0m

                       Computation: 3213 steps/s (collection: 0.448s, learning 2.101s)
               Value function loss: 82141.7744
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 7352.39
               Mean episode length: 342.93
                 Mean success rate: 64.00
                  Mean reward/step: 22.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 2.55s
                        Total time: 5054.64s
                               ETA: 5253.0s

################################################################################
                     [1m Learning iteration 1962/4000 [0m

                       Computation: 3237 steps/s (collection: 0.455s, learning 2.075s)
               Value function loss: 91133.2388
                    Surrogate loss: 0.0098
             Mean action noise std: 0.91
                       Mean reward: 7638.90
               Mean episode length: 349.11
                 Mean success rate: 65.50
                  Mean reward/step: 22.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16080896
                    Iteration time: 2.53s
                        Total time: 5057.17s
                               ETA: 5250.4s

################################################################################
                     [1m Learning iteration 1963/4000 [0m

                       Computation: 3175 steps/s (collection: 0.481s, learning 2.099s)
               Value function loss: 94570.1735
                    Surrogate loss: 0.0107
             Mean action noise std: 0.91
                       Mean reward: 7839.53
               Mean episode length: 358.21
                 Mean success rate: 67.50
                  Mean reward/step: 22.78
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 2.58s
                        Total time: 5059.75s
                               ETA: 5247.8s

################################################################################
                     [1m Learning iteration 1964/4000 [0m

                       Computation: 3159 steps/s (collection: 0.484s, learning 2.109s)
               Value function loss: 93693.2293
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 7761.88
               Mean episode length: 354.57
                 Mean success rate: 67.00
                  Mean reward/step: 22.90
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16097280
                    Iteration time: 2.59s
                        Total time: 5062.34s
                               ETA: 5245.3s

################################################################################
                     [1m Learning iteration 1965/4000 [0m

                       Computation: 3172 steps/s (collection: 0.479s, learning 2.103s)
               Value function loss: 115047.4836
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 7534.55
               Mean episode length: 349.30
                 Mean success rate: 65.50
                  Mean reward/step: 22.18
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 2.58s
                        Total time: 5064.92s
                               ETA: 5242.7s

################################################################################
                     [1m Learning iteration 1966/4000 [0m

                       Computation: 3118 steps/s (collection: 0.513s, learning 2.114s)
               Value function loss: 60803.9742
                    Surrogate loss: 0.0168
             Mean action noise std: 0.91
                       Mean reward: 7540.11
               Mean episode length: 348.90
                 Mean success rate: 65.00
                  Mean reward/step: 21.28
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16113664
                    Iteration time: 2.63s
                        Total time: 5067.55s
                               ETA: 5240.2s

################################################################################
                     [1m Learning iteration 1967/4000 [0m

                       Computation: 3187 steps/s (collection: 0.461s, learning 2.109s)
               Value function loss: 110023.8029
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 7970.45
               Mean episode length: 357.55
                 Mean success rate: 68.00
                  Mean reward/step: 21.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 2.57s
                        Total time: 5070.12s
                               ETA: 5237.6s

################################################################################
                     [1m Learning iteration 1968/4000 [0m

                       Computation: 3190 steps/s (collection: 0.480s, learning 2.088s)
               Value function loss: 98343.0004
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 7698.29
               Mean episode length: 345.06
                 Mean success rate: 65.50
                  Mean reward/step: 21.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16130048
                    Iteration time: 2.57s
                        Total time: 5072.69s
                               ETA: 5235.0s

################################################################################
                     [1m Learning iteration 1969/4000 [0m

                       Computation: 3073 steps/s (collection: 0.534s, learning 2.131s)
               Value function loss: 73994.9356
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 7629.52
               Mean episode length: 347.49
                 Mean success rate: 66.00
                  Mean reward/step: 20.95
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 2.67s
                        Total time: 5075.35s
                               ETA: 5232.5s

################################################################################
                     [1m Learning iteration 1970/4000 [0m

                       Computation: 3112 steps/s (collection: 0.495s, learning 2.137s)
               Value function loss: 96488.6606
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 7281.05
               Mean episode length: 338.92
                 Mean success rate: 63.50
                  Mean reward/step: 20.54
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 16146432
                    Iteration time: 2.63s
                        Total time: 5077.99s
                               ETA: 5230.0s

################################################################################
                     [1m Learning iteration 1971/4000 [0m

                       Computation: 3076 steps/s (collection: 0.525s, learning 2.138s)
               Value function loss: 95634.2093
                    Surrogate loss: 0.0170
             Mean action noise std: 0.91
                       Mean reward: 7179.82
               Mean episode length: 327.17
                 Mean success rate: 61.50
                  Mean reward/step: 20.13
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 2.66s
                        Total time: 5080.65s
                               ETA: 5227.5s

################################################################################
                     [1m Learning iteration 1972/4000 [0m

                       Computation: 3111 steps/s (collection: 0.479s, learning 2.154s)
               Value function loss: 113941.3682
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 7326.41
               Mean episode length: 334.42
                 Mean success rate: 63.00
                  Mean reward/step: 20.21
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 16162816
                    Iteration time: 2.63s
                        Total time: 5083.28s
                               ETA: 5225.0s

################################################################################
                     [1m Learning iteration 1973/4000 [0m

                       Computation: 3181 steps/s (collection: 0.446s, learning 2.129s)
               Value function loss: 75275.2176
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 7442.74
               Mean episode length: 336.92
                 Mean success rate: 63.00
                  Mean reward/step: 20.24
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 2.57s
                        Total time: 5085.86s
                               ETA: 5222.4s

################################################################################
                     [1m Learning iteration 1974/4000 [0m

                       Computation: 3070 steps/s (collection: 0.511s, learning 2.157s)
               Value function loss: 91801.7321
                    Surrogate loss: 0.0166
             Mean action noise std: 0.91
                       Mean reward: 7441.53
               Mean episode length: 336.44
                 Mean success rate: 63.00
                  Mean reward/step: 20.72
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16179200
                    Iteration time: 2.67s
                        Total time: 5088.52s
                               ETA: 5219.9s

################################################################################
                     [1m Learning iteration 1975/4000 [0m

                       Computation: 3173 steps/s (collection: 0.455s, learning 2.127s)
               Value function loss: 66519.2206
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 7343.84
               Mean episode length: 336.67
                 Mean success rate: 63.00
                  Mean reward/step: 21.44
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 2.58s
                        Total time: 5091.11s
                               ETA: 5217.4s

################################################################################
                     [1m Learning iteration 1976/4000 [0m

                       Computation: 3175 steps/s (collection: 0.461s, learning 2.119s)
               Value function loss: 80087.7160
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 7549.73
               Mean episode length: 341.83
                 Mean success rate: 65.00
                  Mean reward/step: 21.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16195584
                    Iteration time: 2.58s
                        Total time: 5093.69s
                               ETA: 5214.8s

################################################################################
                     [1m Learning iteration 1977/4000 [0m

                       Computation: 3291 steps/s (collection: 0.449s, learning 2.040s)
               Value function loss: 92572.6619
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 7583.99
               Mean episode length: 342.30
                 Mean success rate: 65.00
                  Mean reward/step: 21.76
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 2.49s
                        Total time: 5096.17s
                               ETA: 5212.1s

################################################################################
                     [1m Learning iteration 1978/4000 [0m

                       Computation: 3258 steps/s (collection: 0.487s, learning 2.027s)
               Value function loss: 72035.1217
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 7371.83
               Mean episode length: 333.96
                 Mean success rate: 63.00
                  Mean reward/step: 22.83
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16211968
                    Iteration time: 2.51s
                        Total time: 5098.69s
                               ETA: 5209.5s

################################################################################
                     [1m Learning iteration 1979/4000 [0m

                       Computation: 3258 steps/s (collection: 0.453s, learning 2.061s)
               Value function loss: 74907.8448
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 7450.55
               Mean episode length: 337.70
                 Mean success rate: 64.00
                  Mean reward/step: 22.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 2.51s
                        Total time: 5101.20s
                               ETA: 5206.8s

################################################################################
                     [1m Learning iteration 1980/4000 [0m

                       Computation: 3269 steps/s (collection: 0.464s, learning 2.042s)
               Value function loss: 124253.6617
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 7514.22
               Mean episode length: 346.72
                 Mean success rate: 65.50
                  Mean reward/step: 22.43
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 16228352
                    Iteration time: 2.51s
                        Total time: 5103.71s
                               ETA: 5204.2s

################################################################################
                     [1m Learning iteration 1981/4000 [0m

                       Computation: 3245 steps/s (collection: 0.464s, learning 2.060s)
               Value function loss: 82093.5871
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 7244.47
               Mean episode length: 343.07
                 Mean success rate: 64.00
                  Mean reward/step: 21.65
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 2.52s
                        Total time: 5106.23s
                               ETA: 5201.6s

################################################################################
                     [1m Learning iteration 1982/4000 [0m

                       Computation: 3260 steps/s (collection: 0.437s, learning 2.075s)
               Value function loss: 57890.1687
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 7115.65
               Mean episode length: 338.99
                 Mean success rate: 63.50
                  Mean reward/step: 21.30
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16244736
                    Iteration time: 2.51s
                        Total time: 5108.74s
                               ETA: 5198.9s

################################################################################
                     [1m Learning iteration 1983/4000 [0m

                       Computation: 3214 steps/s (collection: 0.487s, learning 2.061s)
               Value function loss: 92269.0104
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 6540.80
               Mean episode length: 324.30
                 Mean success rate: 58.50
                  Mean reward/step: 22.12
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 2.55s
                        Total time: 5111.29s
                               ETA: 5196.3s

################################################################################
                     [1m Learning iteration 1984/4000 [0m

                       Computation: 3191 steps/s (collection: 0.459s, learning 2.107s)
               Value function loss: 85952.6095
                    Surrogate loss: 0.0115
             Mean action noise std: 0.91
                       Mean reward: 6144.43
               Mean episode length: 315.46
                 Mean success rate: 54.50
                  Mean reward/step: 22.93
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16261120
                    Iteration time: 2.57s
                        Total time: 5113.86s
                               ETA: 5193.7s

################################################################################
                     [1m Learning iteration 1985/4000 [0m

                       Computation: 3294 steps/s (collection: 0.444s, learning 2.042s)
               Value function loss: 103252.2881
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 6346.58
               Mean episode length: 318.43
                 Mean success rate: 56.00
                  Mean reward/step: 22.12
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 2.49s
                        Total time: 5116.35s
                               ETA: 5191.1s

################################################################################
                     [1m Learning iteration 1986/4000 [0m

                       Computation: 3244 steps/s (collection: 0.470s, learning 2.055s)
               Value function loss: 100318.7022
                    Surrogate loss: 0.0106
             Mean action noise std: 0.91
                       Mean reward: 6590.76
               Mean episode length: 324.00
                 Mean success rate: 58.00
                  Mean reward/step: 21.28
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16277504
                    Iteration time: 2.53s
                        Total time: 5118.87s
                               ETA: 5188.4s

################################################################################
                     [1m Learning iteration 1987/4000 [0m

                       Computation: 3250 steps/s (collection: 0.441s, learning 2.080s)
               Value function loss: 92864.1054
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 6568.26
               Mean episode length: 314.29
                 Mean success rate: 57.00
                  Mean reward/step: 20.99
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 2.52s
                        Total time: 5121.39s
                               ETA: 5185.8s

################################################################################
                     [1m Learning iteration 1988/4000 [0m

                       Computation: 3321 steps/s (collection: 0.443s, learning 2.023s)
               Value function loss: 113278.4066
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 7033.67
               Mean episode length: 324.14
                 Mean success rate: 59.00
                  Mean reward/step: 20.49
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 16293888
                    Iteration time: 2.47s
                        Total time: 5123.86s
                               ETA: 5183.1s

################################################################################
                     [1m Learning iteration 1989/4000 [0m

                       Computation: 3206 steps/s (collection: 0.477s, learning 2.079s)
               Value function loss: 87055.4555
                    Surrogate loss: 0.0156
             Mean action noise std: 0.91
                       Mean reward: 7216.46
               Mean episode length: 329.40
                 Mean success rate: 60.50
                  Mean reward/step: 21.25
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 2.56s
                        Total time: 5126.41s
                               ETA: 5180.5s

################################################################################
                     [1m Learning iteration 1990/4000 [0m

                       Computation: 3077 steps/s (collection: 0.491s, learning 2.171s)
               Value function loss: 81844.8455
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 7382.79
               Mean episode length: 332.52
                 Mean success rate: 62.00
                  Mean reward/step: 21.55
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16310272
                    Iteration time: 2.66s
                        Total time: 5129.08s
                               ETA: 5178.0s

################################################################################
                     [1m Learning iteration 1991/4000 [0m

                       Computation: 3200 steps/s (collection: 0.469s, learning 2.091s)
               Value function loss: 75952.2309
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 7568.74
               Mean episode length: 335.81
                 Mean success rate: 64.50
                  Mean reward/step: 22.26
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 2.56s
                        Total time: 5131.64s
                               ETA: 5175.4s

################################################################################
                     [1m Learning iteration 1992/4000 [0m

                       Computation: 3222 steps/s (collection: 0.442s, learning 2.100s)
               Value function loss: 57382.4593
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 7488.31
               Mean episode length: 331.50
                 Mean success rate: 64.00
                  Mean reward/step: 22.48
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16326656
                    Iteration time: 2.54s
                        Total time: 5134.18s
                               ETA: 5172.8s

################################################################################
                     [1m Learning iteration 1993/4000 [0m

                       Computation: 3182 steps/s (collection: 0.490s, learning 2.084s)
               Value function loss: 61274.5694
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 7651.37
               Mean episode length: 338.00
                 Mean success rate: 65.50
                  Mean reward/step: 22.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 2.57s
                        Total time: 5136.75s
                               ETA: 5170.2s

################################################################################
                     [1m Learning iteration 1994/4000 [0m

                       Computation: 3205 steps/s (collection: 0.459s, learning 2.096s)
               Value function loss: 65719.0695
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 7514.25
               Mean episode length: 335.81
                 Mean success rate: 64.00
                  Mean reward/step: 22.89
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16343040
                    Iteration time: 2.56s
                        Total time: 5139.31s
                               ETA: 5167.6s

################################################################################
                     [1m Learning iteration 1995/4000 [0m

                       Computation: 3154 steps/s (collection: 0.475s, learning 2.121s)
               Value function loss: 61224.7099
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 7241.35
               Mean episode length: 328.47
                 Mean success rate: 62.00
                  Mean reward/step: 23.06
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 2.60s
                        Total time: 5141.90s
                               ETA: 5165.1s

################################################################################
                     [1m Learning iteration 1996/4000 [0m

                       Computation: 3251 steps/s (collection: 0.483s, learning 2.036s)
               Value function loss: 140612.8590
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 7340.41
               Mean episode length: 330.95
                 Mean success rate: 62.50
                  Mean reward/step: 22.20
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 16359424
                    Iteration time: 2.52s
                        Total time: 5144.42s
                               ETA: 5162.5s

################################################################################
                     [1m Learning iteration 1997/4000 [0m

                       Computation: 3160 steps/s (collection: 0.467s, learning 2.125s)
               Value function loss: 80873.6568
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 7357.90
               Mean episode length: 329.17
                 Mean success rate: 62.00
                  Mean reward/step: 21.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 2.59s
                        Total time: 5147.01s
                               ETA: 5159.9s

################################################################################
                     [1m Learning iteration 1998/4000 [0m

                       Computation: 3127 steps/s (collection: 0.472s, learning 2.147s)
               Value function loss: 91565.0605
                    Surrogate loss: 0.0173
             Mean action noise std: 0.91
                       Mean reward: 7083.31
               Mean episode length: 321.09
                 Mean success rate: 60.00
                  Mean reward/step: 22.09
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16375808
                    Iteration time: 2.62s
                        Total time: 5149.63s
                               ETA: 5157.4s

################################################################################
                     [1m Learning iteration 1999/4000 [0m

                       Computation: 3035 steps/s (collection: 0.581s, learning 2.118s)
               Value function loss: 100385.3166
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 7210.84
               Mean episode length: 328.21
                 Mean success rate: 60.50
                  Mean reward/step: 22.14
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 2.70s
                        Total time: 5152.33s
                               ETA: 5154.9s

################################################################################
                     [1m Learning iteration 2000/4000 [0m

                       Computation: 3206 steps/s (collection: 0.468s, learning 2.087s)
               Value function loss: 101286.2519
                    Surrogate loss: 0.0126
             Mean action noise std: 0.91
                       Mean reward: 7212.17
               Mean episode length: 328.98
                 Mean success rate: 59.50
                  Mean reward/step: 21.53
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16392192
                    Iteration time: 2.55s
                        Total time: 5154.89s
                               ETA: 5152.3s

################################################################################
                     [1m Learning iteration 2001/4000 [0m

                       Computation: 3172 steps/s (collection: 0.479s, learning 2.103s)
               Value function loss: 88698.4717
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 7056.16
               Mean episode length: 323.94
                 Mean success rate: 58.50
                  Mean reward/step: 21.25
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 2.58s
                        Total time: 5157.47s
                               ETA: 5149.7s

################################################################################
                     [1m Learning iteration 2002/4000 [0m

                       Computation: 3103 steps/s (collection: 0.530s, learning 2.110s)
               Value function loss: 130351.8656
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 7766.25
               Mean episode length: 343.54
                 Mean success rate: 63.00
                  Mean reward/step: 21.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16408576
                    Iteration time: 2.64s
                        Total time: 5160.11s
                               ETA: 5147.2s

################################################################################
                     [1m Learning iteration 2003/4000 [0m

                       Computation: 3107 steps/s (collection: 0.528s, learning 2.108s)
               Value function loss: 128301.9291
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 8075.91
               Mean episode length: 354.07
                 Mean success rate: 65.50
                  Mean reward/step: 20.88
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 2.64s
                        Total time: 5162.75s
                               ETA: 5144.7s

################################################################################
                     [1m Learning iteration 2004/4000 [0m

                       Computation: 3174 steps/s (collection: 0.475s, learning 2.106s)
               Value function loss: 60981.5599
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 7807.87
               Mean episode length: 350.17
                 Mean success rate: 64.00
                  Mean reward/step: 20.18
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16424960
                    Iteration time: 2.58s
                        Total time: 5165.33s
                               ETA: 5142.1s

################################################################################
                     [1m Learning iteration 2005/4000 [0m

                       Computation: 3225 steps/s (collection: 0.467s, learning 2.072s)
               Value function loss: 88709.0494
                    Surrogate loss: 0.0126
             Mean action noise std: 0.91
                       Mean reward: 7947.71
               Mean episode length: 356.94
                 Mean success rate: 65.50
                  Mean reward/step: 21.06
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 2.54s
                        Total time: 5167.87s
                               ETA: 5139.5s

################################################################################
                     [1m Learning iteration 2006/4000 [0m

                       Computation: 3143 steps/s (collection: 0.499s, learning 2.107s)
               Value function loss: 90393.0881
                    Surrogate loss: 0.0111
             Mean action noise std: 0.91
                       Mean reward: 7861.11
               Mean episode length: 352.80
                 Mean success rate: 66.00
                  Mean reward/step: 21.60
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 16441344
                    Iteration time: 2.61s
                        Total time: 5170.47s
                               ETA: 5137.0s

################################################################################
                     [1m Learning iteration 2007/4000 [0m

                       Computation: 3158 steps/s (collection: 0.472s, learning 2.122s)
               Value function loss: 51214.3473
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 7746.22
               Mean episode length: 348.43
                 Mean success rate: 65.50
                  Mean reward/step: 21.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 2.59s
                        Total time: 5173.07s
                               ETA: 5134.4s

################################################################################
                     [1m Learning iteration 2008/4000 [0m

                       Computation: 3150 steps/s (collection: 0.497s, learning 2.103s)
               Value function loss: 96349.5974
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 7836.46
               Mean episode length: 349.82
                 Mean success rate: 66.50
                  Mean reward/step: 22.02
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16457728
                    Iteration time: 2.60s
                        Total time: 5175.67s
                               ETA: 5131.9s

################################################################################
                     [1m Learning iteration 2009/4000 [0m

                       Computation: 3209 steps/s (collection: 0.457s, learning 2.095s)
               Value function loss: 69815.8740
                    Surrogate loss: 0.0126
             Mean action noise std: 0.91
                       Mean reward: 7451.16
               Mean episode length: 339.79
                 Mean success rate: 64.00
                  Mean reward/step: 22.72
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 2.55s
                        Total time: 5178.22s
                               ETA: 5129.3s

################################################################################
                     [1m Learning iteration 2010/4000 [0m

                       Computation: 3152 steps/s (collection: 0.479s, learning 2.119s)
               Value function loss: 75103.8132
                    Surrogate loss: 0.0126
             Mean action noise std: 0.91
                       Mean reward: 7011.93
               Mean episode length: 327.55
                 Mean success rate: 61.00
                  Mean reward/step: 22.82
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16474112
                    Iteration time: 2.60s
                        Total time: 5180.82s
                               ETA: 5126.7s

################################################################################
                     [1m Learning iteration 2011/4000 [0m

                       Computation: 3110 steps/s (collection: 0.506s, learning 2.128s)
               Value function loss: 93997.9400
                    Surrogate loss: 0.0107
             Mean action noise std: 0.91
                       Mean reward: 6544.92
               Mean episode length: 313.42
                 Mean success rate: 57.00
                  Mean reward/step: 22.93
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 2.63s
                        Total time: 5183.45s
                               ETA: 5124.2s

################################################################################
                     [1m Learning iteration 2012/4000 [0m

                       Computation: 3207 steps/s (collection: 0.473s, learning 2.082s)
               Value function loss: 118579.8736
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 6593.34
               Mean episode length: 316.96
                 Mean success rate: 58.00
                  Mean reward/step: 22.15
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16490496
                    Iteration time: 2.55s
                        Total time: 5186.00s
                               ETA: 5121.6s

################################################################################
                     [1m Learning iteration 2013/4000 [0m

                       Computation: 3143 steps/s (collection: 0.499s, learning 2.108s)
               Value function loss: 73992.6729
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 6480.63
               Mean episode length: 313.67
                 Mean success rate: 57.00
                  Mean reward/step: 22.07
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 2.61s
                        Total time: 5188.61s
                               ETA: 5119.1s

################################################################################
                     [1m Learning iteration 2014/4000 [0m

                       Computation: 3187 steps/s (collection: 0.469s, learning 2.101s)
               Value function loss: 93969.4669
                    Surrogate loss: 0.0159
             Mean action noise std: 0.91
                       Mean reward: 6569.44
               Mean episode length: 315.14
                 Mean success rate: 57.00
                  Mean reward/step: 22.61
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16506880
                    Iteration time: 2.57s
                        Total time: 5191.18s
                               ETA: 5116.5s

################################################################################
                     [1m Learning iteration 2015/4000 [0m

                       Computation: 3147 steps/s (collection: 0.512s, learning 2.091s)
               Value function loss: 78996.7278
                    Surrogate loss: 0.0115
             Mean action noise std: 0.91
                       Mean reward: 6760.54
               Mean episode length: 322.58
                 Mean success rate: 57.50
                  Mean reward/step: 22.94
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 2.60s
                        Total time: 5193.78s
                               ETA: 5113.9s

################################################################################
                     [1m Learning iteration 2016/4000 [0m

                       Computation: 2927 steps/s (collection: 0.717s, learning 2.081s)
               Value function loss: 110510.6503
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 6985.41
               Mean episode length: 331.05
                 Mean success rate: 59.00
                  Mean reward/step: 22.58
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16523264
                    Iteration time: 2.80s
                        Total time: 5196.58s
                               ETA: 5111.6s

################################################################################
                     [1m Learning iteration 2017/4000 [0m

                       Computation: 3261 steps/s (collection: 0.452s, learning 2.060s)
               Value function loss: 93323.4259
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 7482.17
               Mean episode length: 347.38
                 Mean success rate: 62.50
                  Mean reward/step: 21.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 2.51s
                        Total time: 5199.09s
                               ETA: 5108.9s

################################################################################
                     [1m Learning iteration 2018/4000 [0m

                       Computation: 3204 steps/s (collection: 0.492s, learning 2.064s)
               Value function loss: 108042.1111
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 7937.59
               Mean episode length: 362.30
                 Mean success rate: 66.50
                  Mean reward/step: 21.60
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 16539648
                    Iteration time: 2.56s
                        Total time: 5201.65s
                               ETA: 5106.3s

################################################################################
                     [1m Learning iteration 2019/4000 [0m

                       Computation: 3222 steps/s (collection: 0.462s, learning 2.081s)
               Value function loss: 122750.0767
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 8568.50
               Mean episode length: 381.27
                 Mean success rate: 71.50
                  Mean reward/step: 21.12
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 2.54s
                        Total time: 5204.19s
                               ETA: 5103.7s

################################################################################
                     [1m Learning iteration 2020/4000 [0m

                       Computation: 3132 steps/s (collection: 0.489s, learning 2.127s)
               Value function loss: 42050.3729
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 8183.01
               Mean episode length: 366.33
                 Mean success rate: 68.00
                  Mean reward/step: 21.87
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16556032
                    Iteration time: 2.62s
                        Total time: 5206.81s
                               ETA: 5101.2s

################################################################################
                     [1m Learning iteration 2021/4000 [0m

                       Computation: 3258 steps/s (collection: 0.454s, learning 2.060s)
               Value function loss: 102517.7064
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 8054.97
               Mean episode length: 358.87
                 Mean success rate: 67.00
                  Mean reward/step: 22.48
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 2.51s
                        Total time: 5209.32s
                               ETA: 5098.5s

################################################################################
                     [1m Learning iteration 2022/4000 [0m

                       Computation: 3157 steps/s (collection: 0.467s, learning 2.127s)
               Value function loss: 86330.4346
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 8028.81
               Mean episode length: 357.57
                 Mean success rate: 68.00
                  Mean reward/step: 22.06
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16572416
                    Iteration time: 2.59s
                        Total time: 5211.91s
                               ETA: 5096.0s

################################################################################
                     [1m Learning iteration 2023/4000 [0m

                       Computation: 3211 steps/s (collection: 0.483s, learning 2.067s)
               Value function loss: 51480.9155
                    Surrogate loss: 0.0109
             Mean action noise std: 0.91
                       Mean reward: 7879.73
               Mean episode length: 352.69
                 Mean success rate: 67.00
                  Mean reward/step: 22.32
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 2.55s
                        Total time: 5214.46s
                               ETA: 5093.4s

################################################################################
                     [1m Learning iteration 2024/4000 [0m

                       Computation: 3195 steps/s (collection: 0.493s, learning 2.071s)
               Value function loss: 91644.5479
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 7923.01
               Mean episode length: 357.06
                 Mean success rate: 67.50
                  Mean reward/step: 22.41
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16588800
                    Iteration time: 2.56s
                        Total time: 5217.03s
                               ETA: 5090.8s

################################################################################
                     [1m Learning iteration 2025/4000 [0m

                       Computation: 3145 steps/s (collection: 0.480s, learning 2.124s)
               Value function loss: 111849.6759
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 7981.69
               Mean episode length: 354.05
                 Mean success rate: 67.50
                  Mean reward/step: 22.57
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 2.60s
                        Total time: 5219.63s
                               ETA: 5088.2s

################################################################################
                     [1m Learning iteration 2026/4000 [0m

                       Computation: 3265 steps/s (collection: 0.459s, learning 2.049s)
               Value function loss: 74048.0536
                    Surrogate loss: 0.0168
             Mean action noise std: 0.91
                       Mean reward: 7329.32
               Mean episode length: 332.02
                 Mean success rate: 63.00
                  Mean reward/step: 22.20
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 16605184
                    Iteration time: 2.51s
                        Total time: 5222.14s
                               ETA: 5085.6s

################################################################################
                     [1m Learning iteration 2027/4000 [0m

                       Computation: 3200 steps/s (collection: 0.489s, learning 2.071s)
               Value function loss: 110040.9359
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 7653.71
               Mean episode length: 341.95
                 Mean success rate: 65.00
                  Mean reward/step: 21.31
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 2.56s
                        Total time: 5224.70s
                               ETA: 5083.0s

################################################################################
                     [1m Learning iteration 2028/4000 [0m

                       Computation: 3197 steps/s (collection: 0.489s, learning 2.073s)
               Value function loss: 79700.5336
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 7312.32
               Mean episode length: 333.05
                 Mean success rate: 62.50
                  Mean reward/step: 21.07
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16621568
                    Iteration time: 2.56s
                        Total time: 5227.26s
                               ETA: 5080.4s

################################################################################
                     [1m Learning iteration 2029/4000 [0m

                       Computation: 3195 steps/s (collection: 0.450s, learning 2.114s)
               Value function loss: 80423.9764
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 7557.03
               Mean episode length: 344.68
                 Mean success rate: 65.50
                  Mean reward/step: 21.95
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 2.56s
                        Total time: 5229.83s
                               ETA: 5077.8s

################################################################################
                     [1m Learning iteration 2030/4000 [0m

                       Computation: 3160 steps/s (collection: 0.464s, learning 2.129s)
               Value function loss: 87027.1509
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 7849.90
               Mean episode length: 351.60
                 Mean success rate: 67.00
                  Mean reward/step: 22.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16637952
                    Iteration time: 2.59s
                        Total time: 5232.42s
                               ETA: 5075.3s

################################################################################
                     [1m Learning iteration 2031/4000 [0m

                       Computation: 3172 steps/s (collection: 0.504s, learning 2.079s)
               Value function loss: 82559.6555
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 7932.57
               Mean episode length: 357.04
                 Mean success rate: 67.50
                  Mean reward/step: 22.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 2.58s
                        Total time: 5235.00s
                               ETA: 5072.7s

################################################################################
                     [1m Learning iteration 2032/4000 [0m

                       Computation: 3189 steps/s (collection: 0.495s, learning 2.074s)
               Value function loss: 99023.6570
                    Surrogate loss: 0.0112
             Mean action noise std: 0.91
                       Mean reward: 7782.10
               Mean episode length: 353.35
                 Mean success rate: 67.50
                  Mean reward/step: 22.01
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16654336
                    Iteration time: 2.57s
                        Total time: 5237.57s
                               ETA: 5070.1s

################################################################################
                     [1m Learning iteration 2033/4000 [0m

                       Computation: 3229 steps/s (collection: 0.469s, learning 2.068s)
               Value function loss: 96434.1656
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 7841.54
               Mean episode length: 360.69
                 Mean success rate: 68.50
                  Mean reward/step: 22.57
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 2.54s
                        Total time: 5240.11s
                               ETA: 5067.5s

################################################################################
                     [1m Learning iteration 2034/4000 [0m

                       Computation: 3188 steps/s (collection: 0.485s, learning 2.084s)
               Value function loss: 89382.7138
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 7892.67
               Mean episode length: 362.58
                 Mean success rate: 68.50
                  Mean reward/step: 22.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16670720
                    Iteration time: 2.57s
                        Total time: 5242.68s
                               ETA: 5064.9s

################################################################################
                     [1m Learning iteration 2035/4000 [0m

                       Computation: 3168 steps/s (collection: 0.492s, learning 2.094s)
               Value function loss: 81728.3887
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 8129.78
               Mean episode length: 365.91
                 Mean success rate: 68.50
                  Mean reward/step: 22.13
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 2.59s
                        Total time: 5245.26s
                               ETA: 5062.3s

################################################################################
                     [1m Learning iteration 2036/4000 [0m

                       Computation: 3134 steps/s (collection: 0.542s, learning 2.071s)
               Value function loss: 88358.7527
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 7997.89
               Mean episode length: 364.94
                 Mean success rate: 68.00
                  Mean reward/step: 22.93
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16687104
                    Iteration time: 2.61s
                        Total time: 5247.88s
                               ETA: 5059.8s

################################################################################
                     [1m Learning iteration 2037/4000 [0m

                       Computation: 3252 steps/s (collection: 0.454s, learning 2.065s)
               Value function loss: 129242.5928
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 8176.78
               Mean episode length: 373.25
                 Mean success rate: 70.00
                  Mean reward/step: 23.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 2.52s
                        Total time: 5250.39s
                               ETA: 5057.2s

################################################################################
                     [1m Learning iteration 2038/4000 [0m

                       Computation: 3174 steps/s (collection: 0.489s, learning 2.092s)
               Value function loss: 85507.5068
                    Surrogate loss: 0.0148
             Mean action noise std: 0.91
                       Mean reward: 8238.32
               Mean episode length: 372.79
                 Mean success rate: 69.50
                  Mean reward/step: 22.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16703488
                    Iteration time: 2.58s
                        Total time: 5252.98s
                               ETA: 5054.6s

################################################################################
                     [1m Learning iteration 2039/4000 [0m

                       Computation: 3198 steps/s (collection: 0.477s, learning 2.084s)
               Value function loss: 63365.6434
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 7942.84
               Mean episode length: 366.64
                 Mean success rate: 67.50
                  Mean reward/step: 23.15
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 2.56s
                        Total time: 5255.54s
                               ETA: 5052.0s

################################################################################
                     [1m Learning iteration 2040/4000 [0m

                       Computation: 3212 steps/s (collection: 0.473s, learning 2.077s)
               Value function loss: 85840.0496
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 8066.63
               Mean episode length: 370.65
                 Mean success rate: 68.00
                  Mean reward/step: 23.78
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16719872
                    Iteration time: 2.55s
                        Total time: 5258.09s
                               ETA: 5049.4s

################################################################################
                     [1m Learning iteration 2041/4000 [0m

                       Computation: 3207 steps/s (collection: 0.468s, learning 2.086s)
               Value function loss: 88019.4521
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 8212.46
               Mean episode length: 373.60
                 Mean success rate: 69.00
                  Mean reward/step: 23.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 2.55s
                        Total time: 5260.64s
                               ETA: 5046.8s

################################################################################
                     [1m Learning iteration 2042/4000 [0m

                       Computation: 3182 steps/s (collection: 0.508s, learning 2.066s)
               Value function loss: 138394.5865
                    Surrogate loss: 0.0126
             Mean action noise std: 0.91
                       Mean reward: 7803.05
               Mean episode length: 351.50
                 Mean success rate: 64.50
                  Mean reward/step: 22.95
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 16736256
                    Iteration time: 2.57s
                        Total time: 5263.21s
                               ETA: 5044.2s

################################################################################
                     [1m Learning iteration 2043/4000 [0m

                       Computation: 3212 steps/s (collection: 0.502s, learning 2.048s)
               Value function loss: 102053.8473
                    Surrogate loss: 0.0112
             Mean action noise std: 0.91
                       Mean reward: 7853.10
               Mean episode length: 354.00
                 Mean success rate: 65.00
                  Mean reward/step: 21.47
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 2.55s
                        Total time: 5265.76s
                               ETA: 5041.6s

################################################################################
                     [1m Learning iteration 2044/4000 [0m

                       Computation: 3244 steps/s (collection: 0.473s, learning 2.052s)
               Value function loss: 72273.1480
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 8026.84
               Mean episode length: 360.30
                 Mean success rate: 66.50
                  Mean reward/step: 21.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16752640
                    Iteration time: 2.52s
                        Total time: 5268.29s
                               ETA: 5039.0s

################################################################################
                     [1m Learning iteration 2045/4000 [0m

                       Computation: 3139 steps/s (collection: 0.513s, learning 2.096s)
               Value function loss: 108288.0877
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 7732.82
               Mean episode length: 341.97
                 Mean success rate: 64.00
                  Mean reward/step: 22.17
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 2.61s
                        Total time: 5270.90s
                               ETA: 5036.5s

################################################################################
                     [1m Learning iteration 2046/4000 [0m

                       Computation: 3193 steps/s (collection: 0.486s, learning 2.079s)
               Value function loss: 89874.3449
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 7534.32
               Mean episode length: 334.89
                 Mean success rate: 63.00
                  Mean reward/step: 22.28
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 16769024
                    Iteration time: 2.57s
                        Total time: 5273.46s
                               ETA: 5033.9s

################################################################################
                     [1m Learning iteration 2047/4000 [0m

                       Computation: 3217 steps/s (collection: 0.488s, learning 2.058s)
               Value function loss: 108875.3539
                    Surrogate loss: 0.0163
             Mean action noise std: 0.91
                       Mean reward: 7770.74
               Mean episode length: 343.20
                 Mean success rate: 65.50
                  Mean reward/step: 21.72
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 2.55s
                        Total time: 5276.01s
                               ETA: 5031.3s

################################################################################
                     [1m Learning iteration 2048/4000 [0m

                       Computation: 3172 steps/s (collection: 0.504s, learning 2.078s)
               Value function loss: 84077.1396
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 7816.60
               Mean episode length: 339.65
                 Mean success rate: 65.00
                  Mean reward/step: 21.82
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16785408
                    Iteration time: 2.58s
                        Total time: 5278.59s
                               ETA: 5028.7s

################################################################################
                     [1m Learning iteration 2049/4000 [0m

                       Computation: 3215 steps/s (collection: 0.452s, learning 2.096s)
               Value function loss: 88985.1702
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 7655.56
               Mean episode length: 335.94
                 Mean success rate: 64.00
                  Mean reward/step: 22.34
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 2.55s
                        Total time: 5281.14s
                               ETA: 5026.1s

################################################################################
                     [1m Learning iteration 2050/4000 [0m

                       Computation: 3193 steps/s (collection: 0.498s, learning 2.067s)
               Value function loss: 111074.8815
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 7990.48
               Mean episode length: 348.36
                 Mean success rate: 66.50
                  Mean reward/step: 22.46
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16801792
                    Iteration time: 2.57s
                        Total time: 5283.71s
                               ETA: 5023.5s

################################################################################
                     [1m Learning iteration 2051/4000 [0m

                       Computation: 3178 steps/s (collection: 0.498s, learning 2.080s)
               Value function loss: 68685.2364
                    Surrogate loss: 0.0166
             Mean action noise std: 0.91
                       Mean reward: 7544.36
               Mean episode length: 337.38
                 Mean success rate: 64.50
                  Mean reward/step: 22.29
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 2.58s
                        Total time: 5286.28s
                               ETA: 5020.9s

################################################################################
                     [1m Learning iteration 2052/4000 [0m

                       Computation: 3207 steps/s (collection: 0.451s, learning 2.103s)
               Value function loss: 70670.6554
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 7718.02
               Mean episode length: 340.32
                 Mean success rate: 66.00
                  Mean reward/step: 22.96
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16818176
                    Iteration time: 2.55s
                        Total time: 5288.84s
                               ETA: 5018.3s

################################################################################
                     [1m Learning iteration 2053/4000 [0m

                       Computation: 3199 steps/s (collection: 0.474s, learning 2.087s)
               Value function loss: 110248.3144
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 8127.83
               Mean episode length: 353.38
                 Mean success rate: 69.00
                  Mean reward/step: 22.54
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 2.56s
                        Total time: 5291.40s
                               ETA: 5015.7s

################################################################################
                     [1m Learning iteration 2054/4000 [0m

                       Computation: 3238 steps/s (collection: 0.460s, learning 2.069s)
               Value function loss: 57945.4469
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 8082.12
               Mean episode length: 351.60
                 Mean success rate: 69.00
                  Mean reward/step: 21.93
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16834560
                    Iteration time: 2.53s
                        Total time: 5293.93s
                               ETA: 5013.1s

################################################################################
                     [1m Learning iteration 2055/4000 [0m

                       Computation: 3110 steps/s (collection: 0.481s, learning 2.153s)
               Value function loss: 64892.3837
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 7790.04
               Mean episode length: 344.43
                 Mean success rate: 66.00
                  Mean reward/step: 22.01
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 2.63s
                        Total time: 5296.56s
                               ETA: 5010.6s

################################################################################
                     [1m Learning iteration 2056/4000 [0m

                       Computation: 3192 steps/s (collection: 0.486s, learning 2.080s)
               Value function loss: 68790.4662
                    Surrogate loss: 0.0098
             Mean action noise std: 0.91
                       Mean reward: 7623.95
               Mean episode length: 340.19
                 Mean success rate: 64.50
                  Mean reward/step: 22.80
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16850944
                    Iteration time: 2.57s
                        Total time: 5299.13s
                               ETA: 5008.0s

################################################################################
                     [1m Learning iteration 2057/4000 [0m

                       Computation: 3205 steps/s (collection: 0.479s, learning 2.077s)
               Value function loss: 92634.2995
                    Surrogate loss: 0.0094
             Mean action noise std: 0.91
                       Mean reward: 7487.41
               Mean episode length: 337.38
                 Mean success rate: 63.00
                  Mean reward/step: 23.27
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 2.56s
                        Total time: 5301.68s
                               ETA: 5005.4s

################################################################################
                     [1m Learning iteration 2058/4000 [0m

                       Computation: 3210 steps/s (collection: 0.471s, learning 2.081s)
               Value function loss: 136365.1354
                    Surrogate loss: 0.0100
             Mean action noise std: 0.91
                       Mean reward: 7698.79
               Mean episode length: 344.17
                 Mean success rate: 65.50
                  Mean reward/step: 22.43
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 16867328
                    Iteration time: 2.55s
                        Total time: 5304.23s
                               ETA: 5002.8s

################################################################################
                     [1m Learning iteration 2059/4000 [0m

                       Computation: 3170 steps/s (collection: 0.493s, learning 2.091s)
               Value function loss: 87565.8389
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 7867.63
               Mean episode length: 349.30
                 Mean success rate: 66.00
                  Mean reward/step: 21.80
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 2.58s
                        Total time: 5306.82s
                               ETA: 5000.3s

################################################################################
                     [1m Learning iteration 2060/4000 [0m

                       Computation: 3232 steps/s (collection: 0.467s, learning 2.067s)
               Value function loss: 93440.5717
                    Surrogate loss: 0.0084
             Mean action noise std: 0.91
                       Mean reward: 7921.89
               Mean episode length: 348.90
                 Mean success rate: 66.50
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16883712
                    Iteration time: 2.53s
                        Total time: 5309.35s
                               ETA: 4997.6s

################################################################################
                     [1m Learning iteration 2061/4000 [0m

                       Computation: 3169 steps/s (collection: 0.495s, learning 2.090s)
               Value function loss: 124225.1330
                    Surrogate loss: 0.0108
             Mean action noise std: 0.91
                       Mean reward: 7746.64
               Mean episode length: 344.80
                 Mean success rate: 64.50
                  Mean reward/step: 21.28
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 2.58s
                        Total time: 5311.94s
                               ETA: 4995.1s

################################################################################
                     [1m Learning iteration 2062/4000 [0m

                       Computation: 3205 steps/s (collection: 0.500s, learning 2.056s)
               Value function loss: 108473.7121
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 7750.42
               Mean episode length: 350.80
                 Mean success rate: 64.50
                  Mean reward/step: 20.20
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16900096
                    Iteration time: 2.56s
                        Total time: 5314.49s
                               ETA: 4992.5s

################################################################################
                     [1m Learning iteration 2063/4000 [0m

                       Computation: 3095 steps/s (collection: 0.515s, learning 2.131s)
               Value function loss: 96428.5745
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 7954.19
               Mean episode length: 356.38
                 Mean success rate: 65.50
                  Mean reward/step: 19.64
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 2.65s
                        Total time: 5317.14s
                               ETA: 4990.0s

################################################################################
                     [1m Learning iteration 2064/4000 [0m

                       Computation: 3184 steps/s (collection: 0.497s, learning 2.075s)
               Value function loss: 88401.1632
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 7673.99
               Mean episode length: 346.68
                 Mean success rate: 64.50
                  Mean reward/step: 20.37
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16916480
                    Iteration time: 2.57s
                        Total time: 5319.71s
                               ETA: 4987.4s

################################################################################
                     [1m Learning iteration 2065/4000 [0m

                       Computation: 3135 steps/s (collection: 0.499s, learning 2.113s)
               Value function loss: 79204.0904
                    Surrogate loss: 0.0114
             Mean action noise std: 0.91
                       Mean reward: 7431.28
               Mean episode length: 336.94
                 Mean success rate: 62.50
                  Mean reward/step: 20.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 2.61s
                        Total time: 5322.32s
                               ETA: 4984.8s

################################################################################
                     [1m Learning iteration 2066/4000 [0m

                       Computation: 3210 steps/s (collection: 0.459s, learning 2.092s)
               Value function loss: 85111.2162
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 6941.30
               Mean episode length: 323.32
                 Mean success rate: 59.00
                  Mean reward/step: 21.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16932864
                    Iteration time: 2.55s
                        Total time: 5324.87s
                               ETA: 4982.2s

################################################################################
                     [1m Learning iteration 2067/4000 [0m

                       Computation: 3259 steps/s (collection: 0.469s, learning 2.045s)
               Value function loss: 77299.4029
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 6924.73
               Mean episode length: 324.24
                 Mean success rate: 58.50
                  Mean reward/step: 21.94
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 2.51s
                        Total time: 5327.39s
                               ETA: 4979.6s

################################################################################
                     [1m Learning iteration 2068/4000 [0m

                       Computation: 3255 steps/s (collection: 0.440s, learning 2.076s)
               Value function loss: 94391.6633
                    Surrogate loss: 0.0115
             Mean action noise std: 0.91
                       Mean reward: 6586.79
               Mean episode length: 316.28
                 Mean success rate: 56.50
                  Mean reward/step: 22.36
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16949248
                    Iteration time: 2.52s
                        Total time: 5329.90s
                               ETA: 4977.0s

################################################################################
                     [1m Learning iteration 2069/4000 [0m

                       Computation: 3187 steps/s (collection: 0.507s, learning 2.064s)
               Value function loss: 90899.8537
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 6600.45
               Mean episode length: 317.00
                 Mean success rate: 56.50
                  Mean reward/step: 23.11
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 2.57s
                        Total time: 5332.47s
                               ETA: 4974.4s

################################################################################
                     [1m Learning iteration 2070/4000 [0m

                       Computation: 3143 steps/s (collection: 0.507s, learning 2.099s)
               Value function loss: 69523.6783
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 6688.51
               Mean episode length: 318.51
                 Mean success rate: 57.50
                  Mean reward/step: 23.19
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16965632
                    Iteration time: 2.61s
                        Total time: 5335.08s
                               ETA: 4971.9s

################################################################################
                     [1m Learning iteration 2071/4000 [0m

                       Computation: 3292 steps/s (collection: 0.465s, learning 2.023s)
               Value function loss: 84337.3482
                    Surrogate loss: 0.0167
             Mean action noise std: 0.91
                       Mean reward: 6696.15
               Mean episode length: 315.82
                 Mean success rate: 57.50
                  Mean reward/step: 23.15
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 2.49s
                        Total time: 5337.57s
                               ETA: 4969.2s

################################################################################
                     [1m Learning iteration 2072/4000 [0m

                       Computation: 3268 steps/s (collection: 0.441s, learning 2.065s)
               Value function loss: 70450.6067
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 6715.15
               Mean episode length: 322.41
                 Mean success rate: 58.50
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16982016
                    Iteration time: 2.51s
                        Total time: 5340.07s
                               ETA: 4966.6s

################################################################################
                     [1m Learning iteration 2073/4000 [0m

                       Computation: 3305 steps/s (collection: 0.444s, learning 2.035s)
               Value function loss: 121297.2412
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 7025.71
               Mean episode length: 331.37
                 Mean success rate: 60.50
                  Mean reward/step: 22.57
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 2.48s
                        Total time: 5342.55s
                               ETA: 4963.9s

################################################################################
                     [1m Learning iteration 2074/4000 [0m

                       Computation: 3183 steps/s (collection: 0.479s, learning 2.095s)
               Value function loss: 80327.1933
                    Surrogate loss: 0.0095
             Mean action noise std: 0.91
                       Mean reward: 7421.38
               Mean episode length: 343.06
                 Mean success rate: 63.00
                  Mean reward/step: 21.83
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16998400
                    Iteration time: 2.57s
                        Total time: 5345.13s
                               ETA: 4961.3s

################################################################################
                     [1m Learning iteration 2075/4000 [0m

                       Computation: 3209 steps/s (collection: 0.489s, learning 2.063s)
               Value function loss: 57303.0709
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 7341.21
               Mean episode length: 340.00
                 Mean success rate: 62.00
                  Mean reward/step: 22.16
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.55s
                        Total time: 5347.68s
                               ETA: 4958.7s

################################################################################
                     [1m Learning iteration 2076/4000 [0m

                       Computation: 3226 steps/s (collection: 0.479s, learning 2.061s)
               Value function loss: 91395.2232
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 7623.79
               Mean episode length: 349.30
                 Mean success rate: 64.50
                  Mean reward/step: 23.80
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17014784
                    Iteration time: 2.54s
                        Total time: 5350.22s
                               ETA: 4956.1s

################################################################################
                     [1m Learning iteration 2077/4000 [0m

                       Computation: 3229 steps/s (collection: 0.489s, learning 2.048s)
               Value function loss: 141680.5795
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 8099.50
               Mean episode length: 361.80
                 Mean success rate: 68.50
                  Mean reward/step: 23.04
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 2.54s
                        Total time: 5352.75s
                               ETA: 4953.5s

################################################################################
                     [1m Learning iteration 2078/4000 [0m

                       Computation: 3253 steps/s (collection: 0.476s, learning 2.041s)
               Value function loss: 59306.4367
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 7719.13
               Mean episode length: 344.98
                 Mean success rate: 66.00
                  Mean reward/step: 21.98
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 17031168
                    Iteration time: 2.52s
                        Total time: 5355.27s
                               ETA: 4950.9s

################################################################################
                     [1m Learning iteration 2079/4000 [0m

                       Computation: 3229 steps/s (collection: 0.467s, learning 2.070s)
               Value function loss: 112087.9594
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 7854.25
               Mean episode length: 347.44
                 Mean success rate: 66.50
                  Mean reward/step: 21.75
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 2.54s
                        Total time: 5357.81s
                               ETA: 4948.2s

################################################################################
                     [1m Learning iteration 2080/4000 [0m

                       Computation: 3234 steps/s (collection: 0.483s, learning 2.049s)
               Value function loss: 76998.6527
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 8008.05
               Mean episode length: 354.25
                 Mean success rate: 67.00
                  Mean reward/step: 22.12
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 17047552
                    Iteration time: 2.53s
                        Total time: 5360.34s
                               ETA: 4945.6s

################################################################################
                     [1m Learning iteration 2081/4000 [0m

                       Computation: 3216 steps/s (collection: 0.467s, learning 2.080s)
               Value function loss: 88834.7445
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 7926.48
               Mean episode length: 349.42
                 Mean success rate: 66.00
                  Mean reward/step: 22.18
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 2.55s
                        Total time: 5362.89s
                               ETA: 4943.0s

################################################################################
                     [1m Learning iteration 2082/4000 [0m

                       Computation: 3228 steps/s (collection: 0.492s, learning 2.046s)
               Value function loss: 95960.7288
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 7726.98
               Mean episode length: 344.31
                 Mean success rate: 64.00
                  Mean reward/step: 22.36
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17063936
                    Iteration time: 2.54s
                        Total time: 5365.43s
                               ETA: 4940.4s

################################################################################
                     [1m Learning iteration 2083/4000 [0m

                       Computation: 3240 steps/s (collection: 0.482s, learning 2.047s)
               Value function loss: 69784.8369
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 7914.78
               Mean episode length: 352.29
                 Mean success rate: 66.00
                  Mean reward/step: 22.25
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 2.53s
                        Total time: 5367.95s
                               ETA: 4937.8s

################################################################################
                     [1m Learning iteration 2084/4000 [0m

                       Computation: 3318 steps/s (collection: 0.425s, learning 2.044s)
               Value function loss: 96491.2141
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 7465.59
               Mean episode length: 337.05
                 Mean success rate: 63.00
                  Mean reward/step: 22.52
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 17080320
                    Iteration time: 2.47s
                        Total time: 5370.42s
                               ETA: 4935.1s

################################################################################
                     [1m Learning iteration 2085/4000 [0m

                       Computation: 3256 steps/s (collection: 0.461s, learning 2.055s)
               Value function loss: 72823.3504
                    Surrogate loss: 0.0096
             Mean action noise std: 0.91
                       Mean reward: 7533.52
               Mean episode length: 338.13
                 Mean success rate: 63.50
                  Mean reward/step: 22.43
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 2.52s
                        Total time: 5372.94s
                               ETA: 4932.5s

################################################################################
                     [1m Learning iteration 2086/4000 [0m

                       Computation: 3258 steps/s (collection: 0.478s, learning 2.036s)
               Value function loss: 98794.1719
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 7780.11
               Mean episode length: 347.96
                 Mean success rate: 65.00
                  Mean reward/step: 22.51
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17096704
                    Iteration time: 2.51s
                        Total time: 5375.45s
                               ETA: 4929.9s

################################################################################
                     [1m Learning iteration 2087/4000 [0m

                       Computation: 3248 steps/s (collection: 0.459s, learning 2.063s)
               Value function loss: 90615.2416
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 8023.17
               Mean episode length: 358.17
                 Mean success rate: 67.50
                  Mean reward/step: 22.61
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 2.52s
                        Total time: 5377.97s
                               ETA: 4927.2s

################################################################################
                     [1m Learning iteration 2088/4000 [0m

                       Computation: 3263 steps/s (collection: 0.462s, learning 2.048s)
               Value function loss: 70198.8003
                    Surrogate loss: 0.0174
             Mean action noise std: 0.91
                       Mean reward: 7929.31
               Mean episode length: 350.40
                 Mean success rate: 67.00
                  Mean reward/step: 22.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17113088
                    Iteration time: 2.51s
                        Total time: 5380.49s
                               ETA: 4924.6s

################################################################################
                     [1m Learning iteration 2089/4000 [0m

                       Computation: 3200 steps/s (collection: 0.497s, learning 2.063s)
               Value function loss: 120040.6667
                    Surrogate loss: 0.0104
             Mean action noise std: 0.91
                       Mean reward: 8101.83
               Mean episode length: 352.38
                 Mean success rate: 68.50
                  Mean reward/step: 22.61
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 2.56s
                        Total time: 5383.04s
                               ETA: 4922.0s

################################################################################
                     [1m Learning iteration 2090/4000 [0m

                       Computation: 3240 steps/s (collection: 0.486s, learning 2.042s)
               Value function loss: 52784.1523
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 7977.86
               Mean episode length: 350.80
                 Mean success rate: 68.00
                  Mean reward/step: 22.81
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 17129472
                    Iteration time: 2.53s
                        Total time: 5385.57s
                               ETA: 4919.4s

################################################################################
                     [1m Learning iteration 2091/4000 [0m

                       Computation: 3315 steps/s (collection: 0.444s, learning 2.026s)
               Value function loss: 46761.4470
                    Surrogate loss: 0.0109
             Mean action noise std: 0.91
                       Mean reward: 7710.04
               Mean episode length: 344.25
                 Mean success rate: 67.00
                  Mean reward/step: 23.76
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 2.47s
                        Total time: 5388.04s
                               ETA: 4916.7s

################################################################################
                     [1m Learning iteration 2092/4000 [0m

                       Computation: 3249 steps/s (collection: 0.464s, learning 2.056s)
               Value function loss: 109720.7402
                    Surrogate loss: 0.0112
             Mean action noise std: 0.91
                       Mean reward: 7874.85
               Mean episode length: 347.84
                 Mean success rate: 68.00
                  Mean reward/step: 24.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17145856
                    Iteration time: 2.52s
                        Total time: 5390.56s
                               ETA: 4914.1s

################################################################################
                     [1m Learning iteration 2093/4000 [0m

                       Computation: 3227 steps/s (collection: 0.484s, learning 2.054s)
               Value function loss: 101220.8617
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 8106.73
               Mean episode length: 354.39
                 Mean success rate: 69.50
                  Mean reward/step: 23.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 2.54s
                        Total time: 5393.10s
                               ETA: 4911.5s

################################################################################
                     [1m Learning iteration 2094/4000 [0m

                       Computation: 3215 steps/s (collection: 0.491s, learning 2.056s)
               Value function loss: 128536.2219
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 8285.72
               Mean episode length: 362.22
                 Mean success rate: 70.50
                  Mean reward/step: 22.37
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 17162240
                    Iteration time: 2.55s
                        Total time: 5395.65s
                               ETA: 4908.9s

################################################################################
                     [1m Learning iteration 2095/4000 [0m

                       Computation: 3197 steps/s (collection: 0.510s, learning 2.052s)
               Value function loss: 116176.4803
                    Surrogate loss: 0.0114
             Mean action noise std: 0.91
                       Mean reward: 8010.23
               Mean episode length: 350.70
                 Mean success rate: 68.00
                  Mean reward/step: 21.73
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 2.56s
                        Total time: 5398.21s
                               ETA: 4906.3s

################################################################################
                     [1m Learning iteration 2096/4000 [0m

                       Computation: 3248 steps/s (collection: 0.458s, learning 2.063s)
               Value function loss: 75535.2549
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 8024.08
               Mean episode length: 351.58
                 Mean success rate: 69.50
                  Mean reward/step: 21.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17178624
                    Iteration time: 2.52s
                        Total time: 5400.73s
                               ETA: 4903.7s

################################################################################
                     [1m Learning iteration 2097/4000 [0m

                       Computation: 3204 steps/s (collection: 0.472s, learning 2.084s)
               Value function loss: 94175.9895
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 7803.46
               Mean episode length: 347.02
                 Mean success rate: 67.00
                  Mean reward/step: 21.81
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 2.56s
                        Total time: 5403.29s
                               ETA: 4901.1s

################################################################################
                     [1m Learning iteration 2098/4000 [0m

                       Computation: 3197 steps/s (collection: 0.456s, learning 2.106s)
               Value function loss: 94388.5226
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 8014.14
               Mean episode length: 361.12
                 Mean success rate: 69.50
                  Mean reward/step: 21.90
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17195008
                    Iteration time: 2.56s
                        Total time: 5405.85s
                               ETA: 4898.5s

################################################################################
                     [1m Learning iteration 2099/4000 [0m

                       Computation: 3222 steps/s (collection: 0.449s, learning 2.093s)
               Value function loss: 76021.0348
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 8111.48
               Mean episode length: 356.93
                 Mean success rate: 69.00
                  Mean reward/step: 22.14
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 2.54s
                        Total time: 5408.39s
                               ETA: 4895.9s

################################################################################
                     [1m Learning iteration 2100/4000 [0m

                       Computation: 3179 steps/s (collection: 0.500s, learning 2.076s)
               Value function loss: 96666.3406
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 7987.70
               Mean episode length: 351.32
                 Mean success rate: 67.50
                  Mean reward/step: 22.07
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17211392
                    Iteration time: 2.58s
                        Total time: 5410.97s
                               ETA: 4893.3s

################################################################################
                     [1m Learning iteration 2101/4000 [0m

                       Computation: 3170 steps/s (collection: 0.498s, learning 2.086s)
               Value function loss: 83133.1936
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 7817.94
               Mean episode length: 348.52
                 Mean success rate: 66.00
                  Mean reward/step: 22.27
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 2.58s
                        Total time: 5413.55s
                               ETA: 4890.7s

################################################################################
                     [1m Learning iteration 2102/4000 [0m

                       Computation: 3236 steps/s (collection: 0.416s, learning 2.115s)
               Value function loss: 109376.9754
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 7795.03
               Mean episode length: 346.76
                 Mean success rate: 65.00
                  Mean reward/step: 22.44
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 17227776
                    Iteration time: 2.53s
                        Total time: 5416.08s
                               ETA: 4888.1s

################################################################################
                     [1m Learning iteration 2103/4000 [0m

                       Computation: 3234 steps/s (collection: 0.462s, learning 2.071s)
               Value function loss: 82283.1005
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 7895.06
               Mean episode length: 345.81
                 Mean success rate: 65.50
                  Mean reward/step: 22.25
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 2.53s
                        Total time: 5418.62s
                               ETA: 4885.5s

################################################################################
                     [1m Learning iteration 2104/4000 [0m

                       Computation: 3200 steps/s (collection: 0.487s, learning 2.072s)
               Value function loss: 101484.5423
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 7830.71
               Mean episode length: 343.55
                 Mean success rate: 64.50
                  Mean reward/step: 22.56
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17244160
                    Iteration time: 2.56s
                        Total time: 5421.18s
                               ETA: 4882.9s

################################################################################
                     [1m Learning iteration 2105/4000 [0m

                       Computation: 3197 steps/s (collection: 0.459s, learning 2.103s)
               Value function loss: 95172.6720
                    Surrogate loss: 0.0112
             Mean action noise std: 0.91
                       Mean reward: 8119.63
               Mean episode length: 356.70
                 Mean success rate: 67.50
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 2.56s
                        Total time: 5423.74s
                               ETA: 4880.3s

################################################################################
                     [1m Learning iteration 2106/4000 [0m

                       Computation: 3124 steps/s (collection: 0.472s, learning 2.150s)
               Value function loss: 60432.5923
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 8172.39
               Mean episode length: 359.27
                 Mean success rate: 68.00
                  Mean reward/step: 22.22
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 17260544
                    Iteration time: 2.62s
                        Total time: 5426.36s
                               ETA: 4877.8s

################################################################################
                     [1m Learning iteration 2107/4000 [0m

                       Computation: 3075 steps/s (collection: 0.511s, learning 2.153s)
               Value function loss: 77413.2492
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 8013.67
               Mean episode length: 354.03
                 Mean success rate: 66.50
                  Mean reward/step: 22.40
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 2.66s
                        Total time: 5429.02s
                               ETA: 4875.3s

################################################################################
                     [1m Learning iteration 2108/4000 [0m

                       Computation: 3096 steps/s (collection: 0.525s, learning 2.121s)
               Value function loss: 98861.0868
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 8401.17
               Mean episode length: 366.51
                 Mean success rate: 70.00
                  Mean reward/step: 21.74
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 17276928
                    Iteration time: 2.65s
                        Total time: 5431.67s
                               ETA: 4872.8s

################################################################################
                     [1m Learning iteration 2109/4000 [0m

                       Computation: 3096 steps/s (collection: 0.540s, learning 2.106s)
               Value function loss: 85039.3618
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 8387.62
               Mean episode length: 366.98
                 Mean success rate: 70.00
                  Mean reward/step: 21.21
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 2.65s
                        Total time: 5434.32s
                               ETA: 4870.3s

################################################################################
                     [1m Learning iteration 2110/4000 [0m

                       Computation: 3101 steps/s (collection: 0.477s, learning 2.164s)
               Value function loss: 127816.5426
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 8519.71
               Mean episode length: 370.70
                 Mean success rate: 71.00
                  Mean reward/step: 20.81
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 17293312
                    Iteration time: 2.64s
                        Total time: 5436.96s
                               ETA: 4867.8s

################################################################################
                     [1m Learning iteration 2111/4000 [0m

                       Computation: 3094 steps/s (collection: 0.516s, learning 2.131s)
               Value function loss: 102702.9099
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 8323.80
               Mean episode length: 370.15
                 Mean success rate: 69.50
                  Mean reward/step: 20.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 2.65s
                        Total time: 5439.60s
                               ETA: 4865.3s

################################################################################
                     [1m Learning iteration 2112/4000 [0m

                       Computation: 3063 steps/s (collection: 0.516s, learning 2.158s)
               Value function loss: 97503.5148
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 8227.90
               Mean episode length: 365.66
                 Mean success rate: 68.00
                  Mean reward/step: 20.45
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17309696
                    Iteration time: 2.67s
                        Total time: 5442.28s
                               ETA: 4862.8s

################################################################################
                     [1m Learning iteration 2113/4000 [0m

                       Computation: 3090 steps/s (collection: 0.521s, learning 2.130s)
               Value function loss: 111344.5015
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 8135.33
               Mean episode length: 363.94
                 Mean success rate: 68.00
                  Mean reward/step: 19.94
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 2.65s
                        Total time: 5444.93s
                               ETA: 4860.3s

################################################################################
                     [1m Learning iteration 2114/4000 [0m

                       Computation: 3142 steps/s (collection: 0.481s, learning 2.125s)
               Value function loss: 94333.6440
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 8209.18
               Mean episode length: 365.94
                 Mean success rate: 68.50
                  Mean reward/step: 20.25
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17326080
                    Iteration time: 2.61s
                        Total time: 5447.53s
                               ETA: 4857.7s

################################################################################
                     [1m Learning iteration 2115/4000 [0m

                       Computation: 3062 steps/s (collection: 0.554s, learning 2.121s)
               Value function loss: 77843.7135
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 8040.44
               Mean episode length: 361.27
                 Mean success rate: 67.00
                  Mean reward/step: 20.80
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 2.67s
                        Total time: 5450.21s
                               ETA: 4855.2s

################################################################################
                     [1m Learning iteration 2116/4000 [0m

                       Computation: 3038 steps/s (collection: 0.554s, learning 2.142s)
               Value function loss: 90675.8082
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 8153.54
               Mean episode length: 369.70
                 Mean success rate: 68.00
                  Mean reward/step: 21.10
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17342464
                    Iteration time: 2.70s
                        Total time: 5452.91s
                               ETA: 4852.8s

################################################################################
                     [1m Learning iteration 2117/4000 [0m

                       Computation: 3222 steps/s (collection: 0.461s, learning 2.081s)
               Value function loss: 94102.8016
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 7668.14
               Mean episode length: 357.75
                 Mean success rate: 64.00
                  Mean reward/step: 21.28
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 2.54s
                        Total time: 5455.45s
                               ETA: 4850.1s

################################################################################
                     [1m Learning iteration 2118/4000 [0m

                       Computation: 3229 steps/s (collection: 0.453s, learning 2.083s)
               Value function loss: 91209.6244
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 7524.54
               Mean episode length: 358.65
                 Mean success rate: 64.00
                  Mean reward/step: 21.10
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17358848
                    Iteration time: 2.54s
                        Total time: 5457.98s
                               ETA: 4847.5s

################################################################################
                     [1m Learning iteration 2119/4000 [0m

                       Computation: 3266 steps/s (collection: 0.456s, learning 2.052s)
               Value function loss: 49368.9880
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 7324.96
               Mean episode length: 356.92
                 Mean success rate: 64.00
                  Mean reward/step: 20.75
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 2.51s
                        Total time: 5460.49s
                               ETA: 4844.9s

################################################################################
                     [1m Learning iteration 2120/4000 [0m

                       Computation: 3184 steps/s (collection: 0.437s, learning 2.135s)
               Value function loss: 107656.6331
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 7193.71
               Mean episode length: 353.79
                 Mean success rate: 63.50
                  Mean reward/step: 20.97
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 17375232
                    Iteration time: 2.57s
                        Total time: 5463.06s
                               ETA: 4842.3s

################################################################################
                     [1m Learning iteration 2121/4000 [0m

                       Computation: 3266 steps/s (collection: 0.456s, learning 2.052s)
               Value function loss: 59037.7321
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 7046.54
               Mean episode length: 348.54
                 Mean success rate: 62.00
                  Mean reward/step: 21.25
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 2.51s
                        Total time: 5465.57s
                               ETA: 4839.7s

################################################################################
                     [1m Learning iteration 2122/4000 [0m

                       Computation: 3288 steps/s (collection: 0.450s, learning 2.041s)
               Value function loss: 61782.4464
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 6773.72
               Mean episode length: 339.39
                 Mean success rate: 60.50
                  Mean reward/step: 21.89
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17391616
                    Iteration time: 2.49s
                        Total time: 5468.06s
                               ETA: 4837.0s

################################################################################
                     [1m Learning iteration 2123/4000 [0m

                       Computation: 3225 steps/s (collection: 0.449s, learning 2.091s)
               Value function loss: 88094.1500
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 6607.42
               Mean episode length: 333.63
                 Mean success rate: 59.50
                  Mean reward/step: 22.68
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 2.54s
                        Total time: 5470.60s
                               ETA: 4834.4s

################################################################################
                     [1m Learning iteration 2124/4000 [0m

                       Computation: 3212 steps/s (collection: 0.467s, learning 2.083s)
               Value function loss: 59341.8808
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 6608.97
               Mean episode length: 331.19
                 Mean success rate: 59.00
                  Mean reward/step: 23.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17408000
                    Iteration time: 2.55s
                        Total time: 5473.15s
                               ETA: 4831.8s

################################################################################
                     [1m Learning iteration 2125/4000 [0m

                       Computation: 3180 steps/s (collection: 0.492s, learning 2.084s)
               Value function loss: 79623.2926
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 6783.33
               Mean episode length: 338.64
                 Mean success rate: 61.00
                  Mean reward/step: 23.49
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 2.58s
                        Total time: 5475.73s
                               ETA: 4829.3s

################################################################################
                     [1m Learning iteration 2126/4000 [0m

                       Computation: 3204 steps/s (collection: 0.497s, learning 2.059s)
               Value function loss: 115157.5566
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 7225.96
               Mean episode length: 349.76
                 Mean success rate: 63.50
                  Mean reward/step: 23.25
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17424384
                    Iteration time: 2.56s
                        Total time: 5478.29s
                               ETA: 4826.7s

################################################################################
                     [1m Learning iteration 2127/4000 [0m

                       Computation: 3213 steps/s (collection: 0.522s, learning 2.027s)
               Value function loss: 111115.5459
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 7247.24
               Mean episode length: 347.25
                 Mean success rate: 63.50
                  Mean reward/step: 22.37
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 2.55s
                        Total time: 5480.83s
                               ETA: 4824.1s

################################################################################
                     [1m Learning iteration 2128/4000 [0m

                       Computation: 3233 steps/s (collection: 0.507s, learning 2.026s)
               Value function loss: 103181.0811
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 7818.82
               Mean episode length: 361.58
                 Mean success rate: 67.00
                  Mean reward/step: 21.91
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17440768
                    Iteration time: 2.53s
                        Total time: 5483.37s
                               ETA: 4821.4s

################################################################################
                     [1m Learning iteration 2129/4000 [0m

                       Computation: 3170 steps/s (collection: 0.493s, learning 2.090s)
               Value function loss: 73893.0583
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 7699.20
               Mean episode length: 358.06
                 Mean success rate: 66.00
                  Mean reward/step: 21.38
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 2.58s
                        Total time: 5485.95s
                               ETA: 4818.9s

################################################################################
                     [1m Learning iteration 2130/4000 [0m

                       Computation: 3206 steps/s (collection: 0.459s, learning 2.095s)
               Value function loss: 108056.0139
                    Surrogate loss: 0.0112
             Mean action noise std: 0.91
                       Mean reward: 7912.22
               Mean episode length: 365.56
                 Mean success rate: 67.50
                  Mean reward/step: 22.14
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17457152
                    Iteration time: 2.55s
                        Total time: 5488.51s
                               ETA: 4816.3s

################################################################################
                     [1m Learning iteration 2131/4000 [0m

                       Computation: 3149 steps/s (collection: 0.484s, learning 2.117s)
               Value function loss: 80362.6288
                    Surrogate loss: 0.0114
             Mean action noise std: 0.91
                       Mean reward: 7990.17
               Mean episode length: 364.81
                 Mean success rate: 66.50
                  Mean reward/step: 22.22
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 2.60s
                        Total time: 5491.11s
                               ETA: 4813.7s

################################################################################
                     [1m Learning iteration 2132/4000 [0m

                       Computation: 2996 steps/s (collection: 0.554s, learning 2.180s)
               Value function loss: 97943.8664
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 8227.87
               Mean episode length: 370.78
                 Mean success rate: 68.00
                  Mean reward/step: 22.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17473536
                    Iteration time: 2.73s
                        Total time: 5493.84s
                               ETA: 4811.3s

################################################################################
                     [1m Learning iteration 2133/4000 [0m

                       Computation: 3158 steps/s (collection: 0.484s, learning 2.110s)
               Value function loss: 115404.5870
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 8459.99
               Mean episode length: 377.02
                 Mean success rate: 70.00
                  Mean reward/step: 21.73
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 2.59s
                        Total time: 5496.44s
                               ETA: 4808.7s

################################################################################
                     [1m Learning iteration 2134/4000 [0m

                       Computation: 3119 steps/s (collection: 0.539s, learning 2.087s)
               Value function loss: 100370.9084
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 8088.04
               Mean episode length: 365.86
                 Mean success rate: 66.50
                  Mean reward/step: 21.47
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 17489920
                    Iteration time: 2.63s
                        Total time: 5499.06s
                               ETA: 4806.2s

################################################################################
                     [1m Learning iteration 2135/4000 [0m

                       Computation: 3160 steps/s (collection: 0.499s, learning 2.093s)
               Value function loss: 70785.4553
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 7629.38
               Mean episode length: 352.10
                 Mean success rate: 63.00
                  Mean reward/step: 21.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 2.59s
                        Total time: 5501.65s
                               ETA: 4803.6s

################################################################################
                     [1m Learning iteration 2136/4000 [0m

                       Computation: 3122 steps/s (collection: 0.495s, learning 2.129s)
               Value function loss: 144026.2879
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 7752.20
               Mean episode length: 351.30
                 Mean success rate: 63.00
                  Mean reward/step: 21.00
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 17506304
                    Iteration time: 2.62s
                        Total time: 5504.28s
                               ETA: 4801.1s

################################################################################
                     [1m Learning iteration 2137/4000 [0m

                       Computation: 3080 steps/s (collection: 0.535s, learning 2.125s)
               Value function loss: 66531.9992
                    Surrogate loss: 0.0116
             Mean action noise std: 0.91
                       Mean reward: 7725.18
               Mean episode length: 347.04
                 Mean success rate: 62.50
                  Mean reward/step: 21.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 2.66s
                        Total time: 5506.94s
                               ETA: 4798.6s

################################################################################
                     [1m Learning iteration 2138/4000 [0m

                       Computation: 3092 steps/s (collection: 0.516s, learning 2.133s)
               Value function loss: 82757.1729
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 7989.61
               Mean episode length: 353.09
                 Mean success rate: 64.50
                  Mean reward/step: 21.81
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17522688
                    Iteration time: 2.65s
                        Total time: 5509.59s
                               ETA: 4796.1s

################################################################################
                     [1m Learning iteration 2139/4000 [0m

                       Computation: 3158 steps/s (collection: 0.478s, learning 2.115s)
               Value function loss: 103467.3748
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 7924.91
               Mean episode length: 352.66
                 Mean success rate: 63.50
                  Mean reward/step: 22.23
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 2.59s
                        Total time: 5512.18s
                               ETA: 4793.5s

################################################################################
                     [1m Learning iteration 2140/4000 [0m

                       Computation: 3218 steps/s (collection: 0.475s, learning 2.071s)
               Value function loss: 72636.1982
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 8188.67
               Mean episode length: 364.27
                 Mean success rate: 66.00
                  Mean reward/step: 22.97
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17539072
                    Iteration time: 2.55s
                        Total time: 5514.73s
                               ETA: 4790.9s

################################################################################
                     [1m Learning iteration 2141/4000 [0m

                       Computation: 3075 steps/s (collection: 0.530s, learning 2.134s)
               Value function loss: 106771.2887
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 7704.70
               Mean episode length: 353.71
                 Mean success rate: 62.00
                  Mean reward/step: 23.25
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 2.66s
                        Total time: 5517.39s
                               ETA: 4788.4s

################################################################################
                     [1m Learning iteration 2142/4000 [0m

                       Computation: 3115 steps/s (collection: 0.528s, learning 2.102s)
               Value function loss: 77438.9537
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 7815.48
               Mean episode length: 353.64
                 Mean success rate: 62.00
                  Mean reward/step: 22.86
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17555456
                    Iteration time: 2.63s
                        Total time: 5520.02s
                               ETA: 4785.9s

################################################################################
                     [1m Learning iteration 2143/4000 [0m

                       Computation: 3163 steps/s (collection: 0.492s, learning 2.098s)
               Value function loss: 72680.4323
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 7908.60
               Mean episode length: 351.21
                 Mean success rate: 63.00
                  Mean reward/step: 22.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 2.59s
                        Total time: 5522.61s
                               ETA: 4783.3s

################################################################################
                     [1m Learning iteration 2144/4000 [0m

                       Computation: 3203 steps/s (collection: 0.484s, learning 2.073s)
               Value function loss: 126903.1826
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 8157.00
               Mean episode length: 364.10
                 Mean success rate: 66.00
                  Mean reward/step: 22.87
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 17571840
                    Iteration time: 2.56s
                        Total time: 5525.17s
                               ETA: 4780.7s

################################################################################
                     [1m Learning iteration 2145/4000 [0m

                       Computation: 3032 steps/s (collection: 0.539s, learning 2.162s)
               Value function loss: 74337.6254
                    Surrogate loss: 0.0217
             Mean action noise std: 0.91
                       Mean reward: 7775.55
               Mean episode length: 352.27
                 Mean success rate: 63.50
                  Mean reward/step: 22.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 2.70s
                        Total time: 5527.87s
                               ETA: 4778.3s

################################################################################
                     [1m Learning iteration 2146/4000 [0m

                       Computation: 3142 steps/s (collection: 0.520s, learning 2.086s)
               Value function loss: 83342.8881
                    Surrogate loss: 0.0181
             Mean action noise std: 0.91
                       Mean reward: 8098.98
               Mean episode length: 363.93
                 Mean success rate: 66.00
                  Mean reward/step: 22.25
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 17588224
                    Iteration time: 2.61s
                        Total time: 5530.47s
                               ETA: 4775.7s

################################################################################
                     [1m Learning iteration 2147/4000 [0m

                       Computation: 3094 steps/s (collection: 0.518s, learning 2.129s)
               Value function loss: 101898.3553
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 7810.38
               Mean episode length: 359.12
                 Mean success rate: 64.50
                  Mean reward/step: 21.14
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 2.65s
                        Total time: 5533.12s
                               ETA: 4773.2s

################################################################################
                     [1m Learning iteration 2148/4000 [0m

                       Computation: 3212 steps/s (collection: 0.457s, learning 2.092s)
               Value function loss: 94795.9001
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 7799.08
               Mean episode length: 356.71
                 Mean success rate: 66.00
                  Mean reward/step: 21.19
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17604608
                    Iteration time: 2.55s
                        Total time: 5535.67s
                               ETA: 4770.6s

################################################################################
                     [1m Learning iteration 2149/4000 [0m

                       Computation: 3115 steps/s (collection: 0.496s, learning 2.133s)
               Value function loss: 86475.4175
                    Surrogate loss: 0.0106
             Mean action noise std: 0.91
                       Mean reward: 8045.11
               Mean episode length: 366.59
                 Mean success rate: 68.00
                  Mean reward/step: 21.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 2.63s
                        Total time: 5538.30s
                               ETA: 4768.1s

################################################################################
                     [1m Learning iteration 2150/4000 [0m

                       Computation: 3152 steps/s (collection: 0.507s, learning 2.091s)
               Value function loss: 87318.5264
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 8310.99
               Mean episode length: 374.67
                 Mean success rate: 70.50
                  Mean reward/step: 21.78
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17620992
                    Iteration time: 2.60s
                        Total time: 5540.90s
                               ETA: 4765.5s

################################################################################
                     [1m Learning iteration 2151/4000 [0m

                       Computation: 3151 steps/s (collection: 0.494s, learning 2.105s)
               Value function loss: 87986.6917
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 8048.56
               Mean episode length: 366.99
                 Mean success rate: 69.00
                  Mean reward/step: 21.59
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 2.60s
                        Total time: 5543.50s
                               ETA: 4763.0s

################################################################################
                     [1m Learning iteration 2152/4000 [0m

                       Computation: 3117 steps/s (collection: 0.505s, learning 2.123s)
               Value function loss: 90963.3880
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 7762.73
               Mean episode length: 350.94
                 Mean success rate: 66.00
                  Mean reward/step: 21.14
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 17637376
                    Iteration time: 2.63s
                        Total time: 5546.13s
                               ETA: 4760.4s

################################################################################
                     [1m Learning iteration 2153/4000 [0m

                       Computation: 3143 steps/s (collection: 0.494s, learning 2.112s)
               Value function loss: 56261.7516
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 7634.18
               Mean episode length: 345.27
                 Mean success rate: 64.00
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 2.61s
                        Total time: 5548.73s
                               ETA: 4757.9s

################################################################################
                     [1m Learning iteration 2154/4000 [0m

                       Computation: 3106 steps/s (collection: 0.540s, learning 2.097s)
               Value function loss: 116599.5437
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 7751.71
               Mean episode length: 350.73
                 Mean success rate: 65.00
                  Mean reward/step: 22.82
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17653760
                    Iteration time: 2.64s
                        Total time: 5551.37s
                               ETA: 4755.4s

################################################################################
                     [1m Learning iteration 2155/4000 [0m

                       Computation: 3201 steps/s (collection: 0.494s, learning 2.065s)
               Value function loss: 79169.4075
                    Surrogate loss: 0.0105
             Mean action noise std: 0.91
                       Mean reward: 7313.49
               Mean episode length: 338.72
                 Mean success rate: 61.50
                  Mean reward/step: 22.72
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 2.56s
                        Total time: 5553.93s
                               ETA: 4752.8s

################################################################################
                     [1m Learning iteration 2156/4000 [0m

                       Computation: 3223 steps/s (collection: 0.471s, learning 2.070s)
               Value function loss: 79180.6617
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 7209.76
               Mean episode length: 335.70
                 Mean success rate: 60.50
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17670144
                    Iteration time: 2.54s
                        Total time: 5556.47s
                               ETA: 4750.2s

################################################################################
                     [1m Learning iteration 2157/4000 [0m

                       Computation: 3160 steps/s (collection: 0.472s, learning 2.120s)
               Value function loss: 87439.2168
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 7462.44
               Mean episode length: 345.99
                 Mean success rate: 62.00
                  Mean reward/step: 22.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 2.59s
                        Total time: 5559.06s
                               ETA: 4747.6s

################################################################################
                     [1m Learning iteration 2158/4000 [0m

                       Computation: 3165 steps/s (collection: 0.491s, learning 2.096s)
               Value function loss: 96583.5037
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 7283.38
               Mean episode length: 334.21
                 Mean success rate: 60.50
                  Mean reward/step: 23.19
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 17686528
                    Iteration time: 2.59s
                        Total time: 5561.65s
                               ETA: 4745.0s

################################################################################
                     [1m Learning iteration 2159/4000 [0m

                       Computation: 3166 steps/s (collection: 0.512s, learning 2.074s)
               Value function loss: 80044.1016
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 7392.09
               Mean episode length: 335.54
                 Mean success rate: 60.50
                  Mean reward/step: 23.39
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 2.59s
                        Total time: 5564.24s
                               ETA: 4742.5s

################################################################################
                     [1m Learning iteration 2160/4000 [0m

                       Computation: 3173 steps/s (collection: 0.485s, learning 2.096s)
               Value function loss: 97265.0263
                    Surrogate loss: 0.0166
             Mean action noise std: 0.91
                       Mean reward: 7413.28
               Mean episode length: 339.49
                 Mean success rate: 61.00
                  Mean reward/step: 23.29
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17702912
                    Iteration time: 2.58s
                        Total time: 5566.82s
                               ETA: 4739.9s

################################################################################
                     [1m Learning iteration 2161/4000 [0m

                       Computation: 3128 steps/s (collection: 0.492s, learning 2.126s)
               Value function loss: 80296.2469
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 7811.66
               Mean episode length: 352.59
                 Mean success rate: 64.50
                  Mean reward/step: 23.19
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 2.62s
                        Total time: 5569.44s
                               ETA: 4737.4s

################################################################################
                     [1m Learning iteration 2162/4000 [0m

                       Computation: 3176 steps/s (collection: 0.493s, learning 2.086s)
               Value function loss: 101044.7820
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 8221.36
               Mean episode length: 364.95
                 Mean success rate: 67.50
                  Mean reward/step: 23.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17719296
                    Iteration time: 2.58s
                        Total time: 5572.01s
                               ETA: 4734.8s

################################################################################
                     [1m Learning iteration 2163/4000 [0m

                       Computation: 3221 steps/s (collection: 0.463s, learning 2.080s)
               Value function loss: 79768.0334
                    Surrogate loss: 0.0160
             Mean action noise std: 0.91
                       Mean reward: 8097.57
               Mean episode length: 363.77
                 Mean success rate: 67.00
                  Mean reward/step: 22.92
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 2.54s
                        Total time: 5574.56s
                               ETA: 4732.2s

################################################################################
                     [1m Learning iteration 2164/4000 [0m

                       Computation: 3165 steps/s (collection: 0.472s, learning 2.116s)
               Value function loss: 109682.3700
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 8174.13
               Mean episode length: 367.61
                 Mean success rate: 68.50
                  Mean reward/step: 22.82
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17735680
                    Iteration time: 2.59s
                        Total time: 5577.14s
                               ETA: 4729.6s

################################################################################
                     [1m Learning iteration 2165/4000 [0m

                       Computation: 3164 steps/s (collection: 0.472s, learning 2.116s)
               Value function loss: 90124.1111
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 8478.33
               Mean episode length: 372.98
                 Mean success rate: 70.00
                  Mean reward/step: 22.79
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 2.59s
                        Total time: 5579.73s
                               ETA: 4727.1s

################################################################################
                     [1m Learning iteration 2166/4000 [0m

                       Computation: 3169 steps/s (collection: 0.499s, learning 2.086s)
               Value function loss: 74815.1585
                    Surrogate loss: 0.0167
             Mean action noise std: 0.91
                       Mean reward: 8100.36
               Mean episode length: 360.64
                 Mean success rate: 68.00
                  Mean reward/step: 23.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17752064
                    Iteration time: 2.58s
                        Total time: 5582.32s
                               ETA: 4724.5s

################################################################################
                     [1m Learning iteration 2167/4000 [0m

                       Computation: 3155 steps/s (collection: 0.528s, learning 2.068s)
               Value function loss: 127495.9539
                    Surrogate loss: 0.0104
             Mean action noise std: 0.91
                       Mean reward: 8273.35
               Mean episode length: 365.30
                 Mean success rate: 68.50
                  Mean reward/step: 23.10
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 2.60s
                        Total time: 5584.91s
                               ETA: 4721.9s

################################################################################
                     [1m Learning iteration 2168/4000 [0m

                       Computation: 3314 steps/s (collection: 0.426s, learning 2.045s)
               Value function loss: 94847.1642
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 8446.49
               Mean episode length: 371.44
                 Mean success rate: 70.00
                  Mean reward/step: 22.43
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17768448
                    Iteration time: 2.47s
                        Total time: 5587.39s
                               ETA: 4719.3s

################################################################################
                     [1m Learning iteration 2169/4000 [0m

                       Computation: 3143 steps/s (collection: 0.458s, learning 2.148s)
               Value function loss: 72804.9517
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 8700.49
               Mean episode length: 378.66
                 Mean success rate: 72.00
                  Mean reward/step: 22.51
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 2.61s
                        Total time: 5589.99s
                               ETA: 4716.7s

################################################################################
                     [1m Learning iteration 2170/4000 [0m

                       Computation: 3150 steps/s (collection: 0.495s, learning 2.105s)
               Value function loss: 97309.2986
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 8703.16
               Mean episode length: 374.41
                 Mean success rate: 70.50
                  Mean reward/step: 23.44
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17784832
                    Iteration time: 2.60s
                        Total time: 5592.59s
                               ETA: 4714.2s

################################################################################
                     [1m Learning iteration 2171/4000 [0m

                       Computation: 3157 steps/s (collection: 0.486s, learning 2.108s)
               Value function loss: 102939.2442
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 8590.33
               Mean episode length: 370.45
                 Mean success rate: 69.00
                  Mean reward/step: 23.23
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 2.59s
                        Total time: 5595.19s
                               ETA: 4711.6s

################################################################################
                     [1m Learning iteration 2172/4000 [0m

                       Computation: 3233 steps/s (collection: 0.476s, learning 2.058s)
               Value function loss: 97919.2521
                    Surrogate loss: 0.0110
             Mean action noise std: 0.91
                       Mean reward: 8471.53
               Mean episode length: 362.40
                 Mean success rate: 67.50
                  Mean reward/step: 22.48
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 17801216
                    Iteration time: 2.53s
                        Total time: 5597.72s
                               ETA: 4709.0s

################################################################################
                     [1m Learning iteration 2173/4000 [0m

                       Computation: 3207 steps/s (collection: 0.463s, learning 2.091s)
               Value function loss: 78353.3661
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 8339.45
               Mean episode length: 359.99
                 Mean success rate: 66.50
                  Mean reward/step: 22.98
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 2.55s
                        Total time: 5600.27s
                               ETA: 4706.4s

################################################################################
                     [1m Learning iteration 2174/4000 [0m

                       Computation: 3082 steps/s (collection: 0.504s, learning 2.153s)
               Value function loss: 101266.7433
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 8522.08
               Mean episode length: 366.88
                 Mean success rate: 68.50
                  Mean reward/step: 22.50
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17817600
                    Iteration time: 2.66s
                        Total time: 5602.93s
                               ETA: 4703.9s

################################################################################
                     [1m Learning iteration 2175/4000 [0m

                       Computation: 3242 steps/s (collection: 0.502s, learning 2.025s)
               Value function loss: 76961.3241
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 8862.61
               Mean episode length: 383.11
                 Mean success rate: 71.50
                  Mean reward/step: 22.99
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 2.53s
                        Total time: 5605.46s
                               ETA: 4701.3s

################################################################################
                     [1m Learning iteration 2176/4000 [0m

                       Computation: 3259 steps/s (collection: 0.453s, learning 2.060s)
               Value function loss: 60073.5508
                    Surrogate loss: 0.0106
             Mean action noise std: 0.91
                       Mean reward: 8689.69
               Mean episode length: 377.37
                 Mean success rate: 70.50
                  Mean reward/step: 23.97
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 17833984
                    Iteration time: 2.51s
                        Total time: 5607.97s
                               ETA: 4698.6s

################################################################################
                     [1m Learning iteration 2177/4000 [0m

                       Computation: 3210 steps/s (collection: 0.485s, learning 2.067s)
               Value function loss: 81707.3258
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 8830.53
               Mean episode length: 380.26
                 Mean success rate: 71.50
                  Mean reward/step: 24.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 2.55s
                        Total time: 5610.52s
                               ETA: 4696.0s

################################################################################
                     [1m Learning iteration 2178/4000 [0m

                       Computation: 3164 steps/s (collection: 0.514s, learning 2.075s)
               Value function loss: 77331.3621
                    Surrogate loss: 0.0095
             Mean action noise std: 0.91
                       Mean reward: 8499.29
               Mean episode length: 368.18
                 Mean success rate: 68.50
                  Mean reward/step: 24.29
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17850368
                    Iteration time: 2.59s
                        Total time: 5613.11s
                               ETA: 4693.5s

################################################################################
                     [1m Learning iteration 2179/4000 [0m

                       Computation: 3106 steps/s (collection: 0.514s, learning 2.123s)
               Value function loss: 67292.0882
                    Surrogate loss: 0.0109
             Mean action noise std: 0.91
                       Mean reward: 8403.48
               Mean episode length: 367.79
                 Mean success rate: 68.50
                  Mean reward/step: 24.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 2.64s
                        Total time: 5615.75s
                               ETA: 4691.0s

################################################################################
                     [1m Learning iteration 2180/4000 [0m

                       Computation: 3219 steps/s (collection: 0.481s, learning 2.063s)
               Value function loss: 97108.4505
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 8379.97
               Mean episode length: 368.24
                 Mean success rate: 69.50
                  Mean reward/step: 24.52
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17866752
                    Iteration time: 2.54s
                        Total time: 5618.29s
                               ETA: 4688.4s

################################################################################
                     [1m Learning iteration 2181/4000 [0m

                       Computation: 3129 steps/s (collection: 0.515s, learning 2.102s)
               Value function loss: 88166.8317
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 8696.96
               Mean episode length: 380.89
                 Mean success rate: 72.00
                  Mean reward/step: 24.65
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 2.62s
                        Total time: 5620.91s
                               ETA: 4685.8s

################################################################################
                     [1m Learning iteration 2182/4000 [0m

                       Computation: 3209 steps/s (collection: 0.488s, learning 2.065s)
               Value function loss: 128319.6379
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 9077.79
               Mean episode length: 392.75
                 Mean success rate: 74.50
                  Mean reward/step: 24.38
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17883136
                    Iteration time: 2.55s
                        Total time: 5623.46s
                               ETA: 4683.2s

################################################################################
                     [1m Learning iteration 2183/4000 [0m

                       Computation: 3181 steps/s (collection: 0.512s, learning 2.063s)
               Value function loss: 130427.2879
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 9482.87
               Mean episode length: 401.92
                 Mean success rate: 77.00
                  Mean reward/step: 23.55
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 2.58s
                        Total time: 5626.04s
                               ETA: 4680.6s

################################################################################
                     [1m Learning iteration 2184/4000 [0m

                       Computation: 3215 steps/s (collection: 0.479s, learning 2.069s)
               Value function loss: 94805.1569
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 9126.88
               Mean episode length: 390.25
                 Mean success rate: 74.00
                  Mean reward/step: 22.57
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17899520
                    Iteration time: 2.55s
                        Total time: 5628.59s
                               ETA: 4678.0s

################################################################################
                     [1m Learning iteration 2185/4000 [0m

                       Computation: 3257 steps/s (collection: 0.483s, learning 2.032s)
               Value function loss: 58797.7437
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 9053.92
               Mean episode length: 380.24
                 Mean success rate: 73.00
                  Mean reward/step: 22.80
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 2.51s
                        Total time: 5631.10s
                               ETA: 4675.4s

################################################################################
                     [1m Learning iteration 2186/4000 [0m

                       Computation: 3153 steps/s (collection: 0.516s, learning 2.082s)
               Value function loss: 89103.8465
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 9064.45
               Mean episode length: 383.63
                 Mean success rate: 73.00
                  Mean reward/step: 23.03
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 17915904
                    Iteration time: 2.60s
                        Total time: 5633.70s
                               ETA: 4672.9s

################################################################################
                     [1m Learning iteration 2187/4000 [0m

                       Computation: 3194 steps/s (collection: 0.516s, learning 2.048s)
               Value function loss: 117591.7941
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 9267.72
               Mean episode length: 392.65
                 Mean success rate: 75.00
                  Mean reward/step: 22.66
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 2.56s
                        Total time: 5636.26s
                               ETA: 4670.3s

################################################################################
                     [1m Learning iteration 2188/4000 [0m

                       Computation: 3276 steps/s (collection: 0.425s, learning 2.075s)
               Value function loss: 99854.2205
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 9135.83
               Mean episode length: 387.31
                 Mean success rate: 73.00
                  Mean reward/step: 22.27
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 17932288
                    Iteration time: 2.50s
                        Total time: 5638.76s
                               ETA: 4667.6s

################################################################################
                     [1m Learning iteration 2189/4000 [0m

                       Computation: 3216 steps/s (collection: 0.470s, learning 2.077s)
               Value function loss: 113101.0652
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 9188.14
               Mean episode length: 384.39
                 Mean success rate: 72.50
                  Mean reward/step: 22.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 2.55s
                        Total time: 5641.31s
                               ETA: 4665.0s

################################################################################
                     [1m Learning iteration 2190/4000 [0m

                       Computation: 3183 steps/s (collection: 0.496s, learning 2.077s)
               Value function loss: 73128.0190
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 8730.97
               Mean episode length: 371.24
                 Mean success rate: 70.00
                  Mean reward/step: 22.00
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17948672
                    Iteration time: 2.57s
                        Total time: 5643.88s
                               ETA: 4662.5s

################################################################################
                     [1m Learning iteration 2191/4000 [0m

                       Computation: 3208 steps/s (collection: 0.472s, learning 2.081s)
               Value function loss: 98354.3169
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 8527.47
               Mean episode length: 364.55
                 Mean success rate: 68.50
                  Mean reward/step: 22.31
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 2.55s
                        Total time: 5646.44s
                               ETA: 4659.9s

################################################################################
                     [1m Learning iteration 2192/4000 [0m

                       Computation: 3118 steps/s (collection: 0.528s, learning 2.099s)
               Value function loss: 63874.7047
                    Surrogate loss: 0.0104
             Mean action noise std: 0.91
                       Mean reward: 8296.76
               Mean episode length: 358.69
                 Mean success rate: 67.00
                  Mean reward/step: 22.65
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17965056
                    Iteration time: 2.63s
                        Total time: 5649.06s
                               ETA: 4657.3s

################################################################################
                     [1m Learning iteration 2193/4000 [0m

                       Computation: 3149 steps/s (collection: 0.515s, learning 2.086s)
               Value function loss: 127963.9876
                    Surrogate loss: 0.0110
             Mean action noise std: 0.91
                       Mean reward: 8628.14
               Mean episode length: 366.93
                 Mean success rate: 68.50
                  Mean reward/step: 22.76
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 2.60s
                        Total time: 5651.66s
                               ETA: 4654.8s

################################################################################
                     [1m Learning iteration 2194/4000 [0m

                       Computation: 3196 steps/s (collection: 0.483s, learning 2.080s)
               Value function loss: 88545.4565
                    Surrogate loss: 0.0106
             Mean action noise std: 0.91
                       Mean reward: 8592.50
               Mean episode length: 368.83
                 Mean success rate: 68.50
                  Mean reward/step: 22.24
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17981440
                    Iteration time: 2.56s
                        Total time: 5654.23s
                               ETA: 4652.2s

################################################################################
                     [1m Learning iteration 2195/4000 [0m

                       Computation: 3212 steps/s (collection: 0.485s, learning 2.065s)
               Value function loss: 84439.4839
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 8762.26
               Mean episode length: 374.86
                 Mean success rate: 70.00
                  Mean reward/step: 22.67
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 2.55s
                        Total time: 5656.78s
                               ETA: 4649.6s

################################################################################
                     [1m Learning iteration 2196/4000 [0m

                       Computation: 3196 steps/s (collection: 0.514s, learning 2.048s)
               Value function loss: 69452.4359
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 8738.12
               Mean episode length: 374.71
                 Mean success rate: 70.50
                  Mean reward/step: 22.74
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17997824
                    Iteration time: 2.56s
                        Total time: 5659.34s
                               ETA: 4647.0s

################################################################################
                     [1m Learning iteration 2197/4000 [0m

                       Computation: 3226 steps/s (collection: 0.491s, learning 2.048s)
               Value function loss: 83563.1002
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 8469.76
               Mean episode length: 368.28
                 Mean success rate: 69.50
                  Mean reward/step: 22.85
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 2.54s
                        Total time: 5661.88s
                               ETA: 4644.4s

################################################################################
                     [1m Learning iteration 2198/4000 [0m

                       Computation: 3224 steps/s (collection: 0.491s, learning 2.050s)
               Value function loss: 78048.8509
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 8034.64
               Mean episode length: 360.36
                 Mean success rate: 68.00
                  Mean reward/step: 22.87
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18014208
                    Iteration time: 2.54s
                        Total time: 5664.42s
                               ETA: 4641.8s

################################################################################
                     [1m Learning iteration 2199/4000 [0m

                       Computation: 3258 steps/s (collection: 0.428s, learning 2.086s)
               Value function loss: 112589.9732
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 8151.96
               Mean episode length: 361.57
                 Mean success rate: 68.50
                  Mean reward/step: 22.76
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 2.51s
                        Total time: 5666.93s
                               ETA: 4639.2s

################################################################################
                     [1m Learning iteration 2200/4000 [0m

                       Computation: 3206 steps/s (collection: 0.483s, learning 2.072s)
               Value function loss: 102005.8939
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 7599.04
               Mean episode length: 342.26
                 Mean success rate: 64.00
                  Mean reward/step: 22.24
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 18030592
                    Iteration time: 2.55s
                        Total time: 5669.49s
                               ETA: 4636.6s

################################################################################
                     [1m Learning iteration 2201/4000 [0m

                       Computation: 3126 steps/s (collection: 0.522s, learning 2.098s)
               Value function loss: 107193.1258
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 7479.95
               Mean episode length: 336.00
                 Mean success rate: 63.00
                  Mean reward/step: 21.94
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 2.62s
                        Total time: 5672.11s
                               ETA: 4634.0s

################################################################################
                     [1m Learning iteration 2202/4000 [0m

                       Computation: 3239 steps/s (collection: 0.478s, learning 2.051s)
               Value function loss: 101430.3172
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 7186.86
               Mean episode length: 326.09
                 Mean success rate: 61.50
                  Mean reward/step: 21.86
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 18046976
                    Iteration time: 2.53s
                        Total time: 5674.64s
                               ETA: 4631.4s

################################################################################
                     [1m Learning iteration 2203/4000 [0m

                       Computation: 3223 steps/s (collection: 0.477s, learning 2.064s)
               Value function loss: 121170.4799
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 7659.50
               Mean episode length: 338.43
                 Mean success rate: 64.00
                  Mean reward/step: 20.96
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 2.54s
                        Total time: 5677.18s
                               ETA: 4628.8s

################################################################################
                     [1m Learning iteration 2204/4000 [0m

                       Computation: 3149 steps/s (collection: 0.487s, learning 2.114s)
               Value function loss: 76083.0415
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 7584.58
               Mean episode length: 335.06
                 Mean success rate: 63.00
                  Mean reward/step: 20.74
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18063360
                    Iteration time: 2.60s
                        Total time: 5679.78s
                               ETA: 4626.3s

################################################################################
                     [1m Learning iteration 2205/4000 [0m

                       Computation: 3214 steps/s (collection: 0.490s, learning 2.059s)
               Value function loss: 111125.3588
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 7901.13
               Mean episode length: 344.10
                 Mean success rate: 64.50
                  Mean reward/step: 21.68
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 2.55s
                        Total time: 5682.33s
                               ETA: 4623.7s

################################################################################
                     [1m Learning iteration 2206/4000 [0m

                       Computation: 3168 steps/s (collection: 0.503s, learning 2.082s)
               Value function loss: 73444.1905
                    Surrogate loss: 0.0114
             Mean action noise std: 0.91
                       Mean reward: 8128.29
               Mean episode length: 354.20
                 Mean success rate: 66.50
                  Mean reward/step: 22.15
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 18079744
                    Iteration time: 2.59s
                        Total time: 5684.91s
                               ETA: 4621.1s

################################################################################
                     [1m Learning iteration 2207/4000 [0m

                       Computation: 3225 steps/s (collection: 0.502s, learning 2.038s)
               Value function loss: 75186.8184
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 8122.30
               Mean episode length: 355.12
                 Mean success rate: 66.50
                  Mean reward/step: 21.86
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 2.54s
                        Total time: 5687.45s
                               ETA: 4618.5s

################################################################################
                     [1m Learning iteration 2208/4000 [0m

                       Computation: 3144 steps/s (collection: 0.500s, learning 2.105s)
               Value function loss: 79981.4523
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 8335.02
               Mean episode length: 363.21
                 Mean success rate: 68.00
                  Mean reward/step: 22.83
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18096128
                    Iteration time: 2.61s
                        Total time: 5690.06s
                               ETA: 4615.9s

################################################################################
                     [1m Learning iteration 2209/4000 [0m

                       Computation: 3180 steps/s (collection: 0.459s, learning 2.117s)
               Value function loss: 92516.7980
                    Surrogate loss: 0.0111
             Mean action noise std: 0.91
                       Mean reward: 8677.13
               Mean episode length: 378.09
                 Mean success rate: 71.50
                  Mean reward/step: 21.79
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 2.58s
                        Total time: 5692.63s
                               ETA: 4613.4s

################################################################################
                     [1m Learning iteration 2210/4000 [0m

                       Computation: 3184 steps/s (collection: 0.516s, learning 2.057s)
               Value function loss: 53251.1174
                    Surrogate loss: 0.0099
             Mean action noise std: 0.91
                       Mean reward: 8721.94
               Mean episode length: 383.56
                 Mean success rate: 72.00
                  Mean reward/step: 22.12
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 18112512
                    Iteration time: 2.57s
                        Total time: 5695.20s
                               ETA: 4610.8s

################################################################################
                     [1m Learning iteration 2211/4000 [0m

                       Computation: 3294 steps/s (collection: 0.442s, learning 2.045s)
               Value function loss: 109736.0956
                    Surrogate loss: 0.0109
             Mean action noise std: 0.91
                       Mean reward: 9099.78
               Mean episode length: 395.11
                 Mean success rate: 75.00
                  Mean reward/step: 22.95
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 2.49s
                        Total time: 5697.69s
                               ETA: 4608.1s

################################################################################
                     [1m Learning iteration 2212/4000 [0m

                       Computation: 3278 steps/s (collection: 0.455s, learning 2.043s)
               Value function loss: 48947.8591
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 8898.10
               Mean episode length: 395.70
                 Mean success rate: 73.50
                  Mean reward/step: 23.48
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18128896
                    Iteration time: 2.50s
                        Total time: 5700.19s
                               ETA: 4605.5s

################################################################################
                     [1m Learning iteration 2213/4000 [0m

                       Computation: 3207 steps/s (collection: 0.465s, learning 2.090s)
               Value function loss: 92816.9671
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 8564.22
               Mean episode length: 382.73
                 Mean success rate: 71.50
                  Mean reward/step: 23.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 2.55s
                        Total time: 5702.74s
                               ETA: 4602.9s

################################################################################
                     [1m Learning iteration 2214/4000 [0m

                       Computation: 3244 steps/s (collection: 0.487s, learning 2.038s)
               Value function loss: 89215.6090
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 8443.63
               Mean episode length: 382.41
                 Mean success rate: 70.50
                  Mean reward/step: 23.24
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18145280
                    Iteration time: 2.52s
                        Total time: 5705.27s
                               ETA: 4600.3s

################################################################################
                     [1m Learning iteration 2215/4000 [0m

                       Computation: 3251 steps/s (collection: 0.470s, learning 2.049s)
               Value function loss: 98482.5118
                    Surrogate loss: 0.0110
             Mean action noise std: 0.91
                       Mean reward: 8548.42
               Mean episode length: 385.92
                 Mean success rate: 71.50
                  Mean reward/step: 23.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 2.52s
                        Total time: 5707.79s
                               ETA: 4597.7s

################################################################################
                     [1m Learning iteration 2216/4000 [0m

                       Computation: 3252 steps/s (collection: 0.454s, learning 2.064s)
               Value function loss: 111712.7631
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 8177.20
               Mean episode length: 375.73
                 Mean success rate: 69.50
                  Mean reward/step: 23.51
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 18161664
                    Iteration time: 2.52s
                        Total time: 5710.31s
                               ETA: 4595.0s

################################################################################
                     [1m Learning iteration 2217/4000 [0m

                       Computation: 3214 steps/s (collection: 0.485s, learning 2.063s)
               Value function loss: 86428.2596
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 8364.48
               Mean episode length: 384.06
                 Mean success rate: 71.50
                  Mean reward/step: 23.03
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 2.55s
                        Total time: 5712.86s
                               ETA: 4592.4s

################################################################################
                     [1m Learning iteration 2218/4000 [0m

                       Computation: 3163 steps/s (collection: 0.512s, learning 2.077s)
               Value function loss: 127544.9854
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 8381.35
               Mean episode length: 384.00
                 Mean success rate: 71.00
                  Mean reward/step: 22.88
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18178048
                    Iteration time: 2.59s
                        Total time: 5715.44s
                               ETA: 4589.9s

################################################################################
                     [1m Learning iteration 2219/4000 [0m

                       Computation: 3325 steps/s (collection: 0.447s, learning 2.016s)
               Value function loss: 112397.5344
                    Surrogate loss: 0.0114
             Mean action noise std: 0.91
                       Mean reward: 8402.07
               Mean episode length: 385.00
                 Mean success rate: 71.00
                  Mean reward/step: 23.25
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 2.46s
                        Total time: 5717.91s
                               ETA: 4587.2s

################################################################################
                     [1m Learning iteration 2220/4000 [0m

                       Computation: 3246 steps/s (collection: 0.466s, learning 2.057s)
               Value function loss: 91361.5077
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 8367.93
               Mean episode length: 380.38
                 Mean success rate: 70.00
                  Mean reward/step: 23.32
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18194432
                    Iteration time: 2.52s
                        Total time: 5720.43s
                               ETA: 4584.6s

################################################################################
                     [1m Learning iteration 2221/4000 [0m

                       Computation: 3191 steps/s (collection: 0.480s, learning 2.086s)
               Value function loss: 81182.3526
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 8369.02
               Mean episode length: 376.17
                 Mean success rate: 70.00
                  Mean reward/step: 23.42
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 2.57s
                        Total time: 5723.00s
                               ETA: 4582.0s

################################################################################
                     [1m Learning iteration 2222/4000 [0m

                       Computation: 3226 steps/s (collection: 0.490s, learning 2.049s)
               Value function loss: 89046.4606
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 8530.61
               Mean episode length: 379.85
                 Mean success rate: 71.50
                  Mean reward/step: 23.87
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18210816
                    Iteration time: 2.54s
                        Total time: 5725.54s
                               ETA: 4579.4s

################################################################################
                     [1m Learning iteration 2223/4000 [0m

                       Computation: 3245 steps/s (collection: 0.481s, learning 2.043s)
               Value function loss: 71364.7273
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 8231.29
               Mean episode length: 366.85
                 Mean success rate: 69.00
                  Mean reward/step: 23.54
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 2.52s
                        Total time: 5728.06s
                               ETA: 4576.8s

################################################################################
                     [1m Learning iteration 2224/4000 [0m

                       Computation: 3223 steps/s (collection: 0.491s, learning 2.051s)
               Value function loss: 98331.4508
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 8359.40
               Mean episode length: 367.81
                 Mean success rate: 70.00
                  Mean reward/step: 23.50
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18227200
                    Iteration time: 2.54s
                        Total time: 5730.60s
                               ETA: 4574.2s

################################################################################
                     [1m Learning iteration 2225/4000 [0m

                       Computation: 3275 steps/s (collection: 0.455s, learning 2.046s)
               Value function loss: 122224.9174
                    Surrogate loss: 0.0163
             Mean action noise std: 0.91
                       Mean reward: 8506.65
               Mean episode length: 365.95
                 Mean success rate: 69.50
                  Mean reward/step: 22.75
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 2.50s
                        Total time: 5733.10s
                               ETA: 4571.5s

################################################################################
                     [1m Learning iteration 2226/4000 [0m

                       Computation: 3202 steps/s (collection: 0.469s, learning 2.089s)
               Value function loss: 63911.0391
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 8126.93
               Mean episode length: 353.97
                 Mean success rate: 66.50
                  Mean reward/step: 22.38
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18243584
                    Iteration time: 2.56s
                        Total time: 5735.66s
                               ETA: 4569.0s

################################################################################
                     [1m Learning iteration 2227/4000 [0m

                       Computation: 3293 steps/s (collection: 0.446s, learning 2.041s)
               Value function loss: 82011.1918
                    Surrogate loss: 0.0116
             Mean action noise std: 0.91
                       Mean reward: 8051.80
               Mean episode length: 350.23
                 Mean success rate: 66.50
                  Mean reward/step: 22.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 2.49s
                        Total time: 5738.15s
                               ETA: 4566.3s

################################################################################
                     [1m Learning iteration 2228/4000 [0m

                       Computation: 3271 steps/s (collection: 0.464s, learning 2.039s)
               Value function loss: 100146.8550
                    Surrogate loss: 0.0086
             Mean action noise std: 0.91
                       Mean reward: 7917.27
               Mean episode length: 339.49
                 Mean success rate: 65.00
                  Mean reward/step: 23.78
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 18259968
                    Iteration time: 2.50s
                        Total time: 5740.65s
                               ETA: 4563.7s

################################################################################
                     [1m Learning iteration 2229/4000 [0m

                       Computation: 3219 steps/s (collection: 0.489s, learning 2.055s)
               Value function loss: 99879.7898
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 8123.23
               Mean episode length: 348.50
                 Mean success rate: 67.50
                  Mean reward/step: 23.30
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 2.54s
                        Total time: 5743.20s
                               ETA: 4561.1s

################################################################################
                     [1m Learning iteration 2230/4000 [0m

                       Computation: 3257 steps/s (collection: 0.470s, learning 2.046s)
               Value function loss: 100285.5313
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 8189.49
               Mean episode length: 352.94
                 Mean success rate: 67.50
                  Mean reward/step: 22.74
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 18276352
                    Iteration time: 2.52s
                        Total time: 5745.71s
                               ETA: 4558.5s

################################################################################
                     [1m Learning iteration 2231/4000 [0m

                       Computation: 3273 steps/s (collection: 0.467s, learning 2.035s)
               Value function loss: 102581.1790
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 8239.04
               Mean episode length: 354.44
                 Mean success rate: 67.50
                  Mean reward/step: 22.13
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 2.50s
                        Total time: 5748.21s
                               ETA: 4555.8s

################################################################################
                     [1m Learning iteration 2232/4000 [0m

                       Computation: 3278 steps/s (collection: 0.445s, learning 2.053s)
               Value function loss: 104740.9718
                    Surrogate loss: 0.0164
             Mean action noise std: 0.91
                       Mean reward: 8453.05
               Mean episode length: 363.60
                 Mean success rate: 69.00
                  Mean reward/step: 22.41
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 18292736
                    Iteration time: 2.50s
                        Total time: 5750.71s
                               ETA: 4553.2s

################################################################################
                     [1m Learning iteration 2233/4000 [0m

                       Computation: 3257 steps/s (collection: 0.448s, learning 2.067s)
               Value function loss: 98198.2766
                    Surrogate loss: 0.0115
             Mean action noise std: 0.91
                       Mean reward: 8345.42
               Mean episode length: 360.02
                 Mean success rate: 68.50
                  Mean reward/step: 22.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 2.51s
                        Total time: 5753.23s
                               ETA: 4550.6s

################################################################################
                     [1m Learning iteration 2234/4000 [0m

                       Computation: 3289 steps/s (collection: 0.462s, learning 2.029s)
               Value function loss: 101030.3740
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 8365.23
               Mean episode length: 361.55
                 Mean success rate: 69.00
                  Mean reward/step: 22.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18309120
                    Iteration time: 2.49s
                        Total time: 5755.72s
                               ETA: 4547.9s

################################################################################
                     [1m Learning iteration 2235/4000 [0m

                       Computation: 3223 steps/s (collection: 0.467s, learning 2.074s)
               Value function loss: 90480.4528
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 8662.49
               Mean episode length: 368.30
                 Mean success rate: 71.50
                  Mean reward/step: 22.39
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 2.54s
                        Total time: 5758.26s
                               ETA: 4545.3s

################################################################################
                     [1m Learning iteration 2236/4000 [0m

                       Computation: 3243 steps/s (collection: 0.462s, learning 2.064s)
               Value function loss: 90689.6770
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 8716.75
               Mean episode length: 372.17
                 Mean success rate: 72.00
                  Mean reward/step: 22.48
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18325504
                    Iteration time: 2.53s
                        Total time: 5760.79s
                               ETA: 4542.7s

################################################################################
                     [1m Learning iteration 2237/4000 [0m

                       Computation: 3136 steps/s (collection: 0.477s, learning 2.135s)
               Value function loss: 74214.3528
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 8430.81
               Mean episode length: 366.81
                 Mean success rate: 70.50
                  Mean reward/step: 22.25
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 2.61s
                        Total time: 5763.40s
                               ETA: 4540.2s

################################################################################
                     [1m Learning iteration 2238/4000 [0m

                       Computation: 3193 steps/s (collection: 0.454s, learning 2.111s)
               Value function loss: 114084.7783
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 8486.43
               Mean episode length: 365.81
                 Mean success rate: 70.50
                  Mean reward/step: 22.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18341888
                    Iteration time: 2.57s
                        Total time: 5765.96s
                               ETA: 4537.6s

################################################################################
                     [1m Learning iteration 2239/4000 [0m

                       Computation: 3176 steps/s (collection: 0.458s, learning 2.121s)
               Value function loss: 89641.2883
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 8249.76
               Mean episode length: 362.69
                 Mean success rate: 70.50
                  Mean reward/step: 22.96
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 2.58s
                        Total time: 5768.54s
                               ETA: 4535.0s

################################################################################
                     [1m Learning iteration 2240/4000 [0m

                       Computation: 3273 steps/s (collection: 0.458s, learning 2.044s)
               Value function loss: 127513.7717
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 7930.74
               Mean episode length: 352.91
                 Mean success rate: 67.50
                  Mean reward/step: 22.81
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 18358272
                    Iteration time: 2.50s
                        Total time: 5771.04s
                               ETA: 4532.4s

################################################################################
                     [1m Learning iteration 2241/4000 [0m

                       Computation: 3211 steps/s (collection: 0.472s, learning 2.078s)
               Value function loss: 74776.7321
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 7979.41
               Mean episode length: 351.90
                 Mean success rate: 67.50
                  Mean reward/step: 22.28
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 2.55s
                        Total time: 5773.59s
                               ETA: 4529.8s

################################################################################
                     [1m Learning iteration 2242/4000 [0m

                       Computation: 3193 steps/s (collection: 0.462s, learning 2.103s)
               Value function loss: 71273.8644
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 7928.73
               Mean episode length: 350.14
                 Mean success rate: 67.00
                  Mean reward/step: 22.72
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18374656
                    Iteration time: 2.56s
                        Total time: 5776.16s
                               ETA: 4527.2s

################################################################################
                     [1m Learning iteration 2243/4000 [0m

                       Computation: 3151 steps/s (collection: 0.507s, learning 2.093s)
               Value function loss: 92018.1575
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 7920.02
               Mean episode length: 351.06
                 Mean success rate: 67.00
                  Mean reward/step: 23.13
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 2.60s
                        Total time: 5778.76s
                               ETA: 4524.6s

################################################################################
                     [1m Learning iteration 2244/4000 [0m

                       Computation: 3238 steps/s (collection: 0.433s, learning 2.097s)
               Value function loss: 93236.9116
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 7680.72
               Mean episode length: 349.54
                 Mean success rate: 66.00
                  Mean reward/step: 22.56
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18391040
                    Iteration time: 2.53s
                        Total time: 5781.29s
                               ETA: 4522.0s

################################################################################
                     [1m Learning iteration 2245/4000 [0m

                       Computation: 3177 steps/s (collection: 0.486s, learning 2.092s)
               Value function loss: 104483.3199
                    Surrogate loss: 0.0093
             Mean action noise std: 0.91
                       Mean reward: 7826.85
               Mean episode length: 353.19
                 Mean success rate: 66.00
                  Mean reward/step: 22.27
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 2.58s
                        Total time: 5783.87s
                               ETA: 4519.4s

################################################################################
                     [1m Learning iteration 2246/4000 [0m

                       Computation: 3152 steps/s (collection: 0.479s, learning 2.120s)
               Value function loss: 105327.8316
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 7923.25
               Mean episode length: 353.27
                 Mean success rate: 66.50
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18407424
                    Iteration time: 2.60s
                        Total time: 5786.46s
                               ETA: 4516.9s

################################################################################
                     [1m Learning iteration 2247/4000 [0m

                       Computation: 3165 steps/s (collection: 0.494s, learning 2.094s)
               Value function loss: 85354.0119
                    Surrogate loss: 0.0116
             Mean action noise std: 0.91
                       Mean reward: 8177.02
               Mean episode length: 358.15
                 Mean success rate: 68.00
                  Mean reward/step: 21.36
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 2.59s
                        Total time: 5789.05s
                               ETA: 4514.3s

################################################################################
                     [1m Learning iteration 2248/4000 [0m

                       Computation: 3091 steps/s (collection: 0.472s, learning 2.177s)
               Value function loss: 92235.9822
                    Surrogate loss: 0.0112
             Mean action noise std: 0.91
                       Mean reward: 8153.70
               Mean episode length: 355.54
                 Mean success rate: 67.50
                  Mean reward/step: 21.75
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 18423808
                    Iteration time: 2.65s
                        Total time: 5791.70s
                               ETA: 4511.8s

################################################################################
                     [1m Learning iteration 2249/4000 [0m

                       Computation: 3081 steps/s (collection: 0.519s, learning 2.140s)
               Value function loss: 60750.4266
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 7958.68
               Mean episode length: 349.83
                 Mean success rate: 66.50
                  Mean reward/step: 21.95
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 2.66s
                        Total time: 5794.36s
                               ETA: 4509.3s

################################################################################
                     [1m Learning iteration 2250/4000 [0m

                       Computation: 3186 steps/s (collection: 0.466s, learning 2.105s)
               Value function loss: 95660.0024
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 7904.78
               Mean episode length: 351.22
                 Mean success rate: 65.50
                  Mean reward/step: 22.37
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18440192
                    Iteration time: 2.57s
                        Total time: 5796.93s
                               ETA: 4506.7s

################################################################################
                     [1m Learning iteration 2251/4000 [0m

                       Computation: 3142 steps/s (collection: 0.489s, learning 2.118s)
               Value function loss: 84015.2863
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 8000.71
               Mean episode length: 352.77
                 Mean success rate: 66.50
                  Mean reward/step: 23.17
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 2.61s
                        Total time: 5799.54s
                               ETA: 4504.2s

################################################################################
                     [1m Learning iteration 2252/4000 [0m

                       Computation: 3131 steps/s (collection: 0.485s, learning 2.131s)
               Value function loss: 108141.3813
                    Surrogate loss: 0.0107
             Mean action noise std: 0.91
                       Mean reward: 8403.67
               Mean episode length: 363.88
                 Mean success rate: 69.00
                  Mean reward/step: 23.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18456576
                    Iteration time: 2.62s
                        Total time: 5802.15s
                               ETA: 4501.6s

################################################################################
                     [1m Learning iteration 2253/4000 [0m

                       Computation: 3096 steps/s (collection: 0.493s, learning 2.153s)
               Value function loss: 71101.4648
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 8413.19
               Mean episode length: 364.12
                 Mean success rate: 69.00
                  Mean reward/step: 23.80
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 2.65s
                        Total time: 5804.80s
                               ETA: 4499.1s

################################################################################
                     [1m Learning iteration 2254/4000 [0m

                       Computation: 3190 steps/s (collection: 0.457s, learning 2.111s)
               Value function loss: 84492.9653
                    Surrogate loss: 0.0107
             Mean action noise std: 0.91
                       Mean reward: 8209.62
               Mean episode length: 359.93
                 Mean success rate: 67.50
                  Mean reward/step: 23.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18472960
                    Iteration time: 2.57s
                        Total time: 5807.37s
                               ETA: 4496.5s

################################################################################
                     [1m Learning iteration 2255/4000 [0m

                       Computation: 3131 steps/s (collection: 0.479s, learning 2.137s)
               Value function loss: 147119.1012
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 7952.06
               Mean episode length: 352.33
                 Mean success rate: 65.50
                  Mean reward/step: 23.28
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 2.62s
                        Total time: 5809.98s
                               ETA: 4494.0s

################################################################################
                     [1m Learning iteration 2256/4000 [0m

                       Computation: 3085 steps/s (collection: 0.474s, learning 2.181s)
               Value function loss: 107330.5251
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 8150.59
               Mean episode length: 362.53
                 Mean success rate: 67.50
                  Mean reward/step: 22.21
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18489344
                    Iteration time: 2.65s
                        Total time: 5812.64s
                               ETA: 4491.5s

################################################################################
                     [1m Learning iteration 2257/4000 [0m

                       Computation: 3206 steps/s (collection: 0.475s, learning 2.079s)
               Value function loss: 52733.6235
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 8122.89
               Mean episode length: 363.12
                 Mean success rate: 68.00
                  Mean reward/step: 22.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 2.55s
                        Total time: 5815.19s
                               ETA: 4488.9s

################################################################################
                     [1m Learning iteration 2258/4000 [0m

                       Computation: 3172 steps/s (collection: 0.495s, learning 2.088s)
               Value function loss: 85200.9583
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 7972.82
               Mean episode length: 353.68
                 Mean success rate: 67.50
                  Mean reward/step: 22.87
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18505728
                    Iteration time: 2.58s
                        Total time: 5817.77s
                               ETA: 4486.3s

################################################################################
                     [1m Learning iteration 2259/4000 [0m

                       Computation: 3275 steps/s (collection: 0.468s, learning 2.033s)
               Value function loss: 101935.7264
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 8070.33
               Mean episode length: 353.64
                 Mean success rate: 67.50
                  Mean reward/step: 23.08
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 2.50s
                        Total time: 5820.27s
                               ETA: 4483.7s

################################################################################
                     [1m Learning iteration 2260/4000 [0m

                       Computation: 3250 steps/s (collection: 0.485s, learning 2.036s)
               Value function loss: 90974.8048
                    Surrogate loss: 0.0111
             Mean action noise std: 0.91
                       Mean reward: 7862.40
               Mean episode length: 351.98
                 Mean success rate: 67.00
                  Mean reward/step: 23.11
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18522112
                    Iteration time: 2.52s
                        Total time: 5822.80s
                               ETA: 4481.1s

################################################################################
                     [1m Learning iteration 2261/4000 [0m

                       Computation: 3200 steps/s (collection: 0.511s, learning 2.049s)
               Value function loss: 109351.7832
                    Surrogate loss: 0.0103
             Mean action noise std: 0.91
                       Mean reward: 7513.28
               Mean episode length: 335.60
                 Mean success rate: 63.00
                  Mean reward/step: 23.09
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 2.56s
                        Total time: 5825.35s
                               ETA: 4478.5s

################################################################################
                     [1m Learning iteration 2262/4000 [0m

                       Computation: 3229 steps/s (collection: 0.471s, learning 2.065s)
               Value function loss: 116054.0704
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 7701.43
               Mean episode length: 336.18
                 Mean success rate: 64.50
                  Mean reward/step: 22.56
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 18538496
                    Iteration time: 2.54s
                        Total time: 5827.89s
                               ETA: 4475.9s

################################################################################
                     [1m Learning iteration 2263/4000 [0m

                       Computation: 3247 steps/s (collection: 0.478s, learning 2.045s)
               Value function loss: 82813.4317
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 7447.51
               Mean episode length: 332.72
                 Mean success rate: 63.00
                  Mean reward/step: 22.76
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 2.52s
                        Total time: 5830.41s
                               ETA: 4473.2s

################################################################################
                     [1m Learning iteration 2264/4000 [0m

                       Computation: 3247 steps/s (collection: 0.452s, learning 2.070s)
               Value function loss: 128162.7281
                    Surrogate loss: 0.0097
             Mean action noise std: 0.91
                       Mean reward: 7907.57
               Mean episode length: 348.01
                 Mean success rate: 66.00
                  Mean reward/step: 22.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18554880
                    Iteration time: 2.52s
                        Total time: 5832.94s
                               ETA: 4470.6s

################################################################################
                     [1m Learning iteration 2265/4000 [0m

                       Computation: 3220 steps/s (collection: 0.479s, learning 2.064s)
               Value function loss: 57696.0063
                    Surrogate loss: 0.0116
             Mean action noise std: 0.91
                       Mean reward: 7858.29
               Mean episode length: 343.19
                 Mean success rate: 64.50
                  Mean reward/step: 22.60
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 2.54s
                        Total time: 5835.48s
                               ETA: 4468.0s

################################################################################
                     [1m Learning iteration 2266/4000 [0m

                       Computation: 3186 steps/s (collection: 0.478s, learning 2.093s)
               Value function loss: 109683.2786
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 7881.74
               Mean episode length: 346.54
                 Mean success rate: 64.50
                  Mean reward/step: 23.40
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 18571264
                    Iteration time: 2.57s
                        Total time: 5838.05s
                               ETA: 4465.5s

################################################################################
                     [1m Learning iteration 2267/4000 [0m

                       Computation: 3264 steps/s (collection: 0.458s, learning 2.052s)
               Value function loss: 91350.8789
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 8376.54
               Mean episode length: 363.95
                 Mean success rate: 67.50
                  Mean reward/step: 23.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 2.51s
                        Total time: 5840.56s
                               ETA: 4462.8s

################################################################################
                     [1m Learning iteration 2268/4000 [0m

                       Computation: 3211 steps/s (collection: 0.492s, learning 2.059s)
               Value function loss: 74901.9999
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 8648.99
               Mean episode length: 372.55
                 Mean success rate: 69.50
                  Mean reward/step: 23.53
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 18587648
                    Iteration time: 2.55s
                        Total time: 5843.11s
                               ETA: 4460.2s

################################################################################
                     [1m Learning iteration 2269/4000 [0m

                       Computation: 3123 steps/s (collection: 0.495s, learning 2.128s)
               Value function loss: 77382.0824
                    Surrogate loss: 0.0091
             Mean action noise std: 0.91
                       Mean reward: 8703.03
               Mean episode length: 372.60
                 Mean success rate: 69.50
                  Mean reward/step: 24.65
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 2.62s
                        Total time: 5845.74s
                               ETA: 4457.7s

################################################################################
                     [1m Learning iteration 2270/4000 [0m

                       Computation: 3199 steps/s (collection: 0.466s, learning 2.094s)
               Value function loss: 101941.6744
                    Surrogate loss: 0.0103
             Mean action noise std: 0.91
                       Mean reward: 9001.67
               Mean episode length: 386.17
                 Mean success rate: 72.50
                  Mean reward/step: 25.15
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18604032
                    Iteration time: 2.56s
                        Total time: 5848.30s
                               ETA: 4455.1s

################################################################################
                     [1m Learning iteration 2271/4000 [0m

                       Computation: 3174 steps/s (collection: 0.466s, learning 2.115s)
               Value function loss: 127230.0717
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 9294.05
               Mean episode length: 402.62
                 Mean success rate: 76.00
                  Mean reward/step: 23.99
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 2.58s
                        Total time: 5850.88s
                               ETA: 4452.5s

################################################################################
                     [1m Learning iteration 2272/4000 [0m

                       Computation: 3162 steps/s (collection: 0.466s, learning 2.124s)
               Value function loss: 109801.0140
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 9659.84
               Mean episode length: 412.61
                 Mean success rate: 78.00
                  Mean reward/step: 23.35
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18620416
                    Iteration time: 2.59s
                        Total time: 5853.47s
                               ETA: 4450.0s

################################################################################
                     [1m Learning iteration 2273/4000 [0m

                       Computation: 3086 steps/s (collection: 0.490s, learning 2.164s)
               Value function loss: 61193.3543
                    Surrogate loss: 0.0097
             Mean action noise std: 0.91
                       Mean reward: 9578.58
               Mean episode length: 407.00
                 Mean success rate: 77.00
                  Mean reward/step: 23.50
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 2.65s
                        Total time: 5856.12s
                               ETA: 4447.5s

################################################################################
                     [1m Learning iteration 2274/4000 [0m

                       Computation: 3221 steps/s (collection: 0.483s, learning 2.060s)
               Value function loss: 102950.0091
                    Surrogate loss: 0.0112
             Mean action noise std: 0.91
                       Mean reward: 9706.60
               Mean episode length: 409.25
                 Mean success rate: 78.50
                  Mean reward/step: 24.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18636800
                    Iteration time: 2.54s
                        Total time: 5858.66s
                               ETA: 4444.9s

################################################################################
                     [1m Learning iteration 2275/4000 [0m

                       Computation: 3251 steps/s (collection: 0.466s, learning 2.053s)
               Value function loss: 79361.1830
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 9298.13
               Mean episode length: 397.94
                 Mean success rate: 76.50
                  Mean reward/step: 24.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 2.52s
                        Total time: 5861.18s
                               ETA: 4442.2s

################################################################################
                     [1m Learning iteration 2276/4000 [0m

                       Computation: 3166 steps/s (collection: 0.473s, learning 2.115s)
               Value function loss: 100315.1822
                    Surrogate loss: 0.0148
             Mean action noise std: 0.91
                       Mean reward: 9630.98
               Mean episode length: 407.73
                 Mean success rate: 79.00
                  Mean reward/step: 24.23
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 18653184
                    Iteration time: 2.59s
                        Total time: 5863.77s
                               ETA: 4439.7s

################################################################################
                     [1m Learning iteration 2277/4000 [0m

                       Computation: 3134 steps/s (collection: 0.499s, learning 2.115s)
               Value function loss: 108162.1375
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 9975.11
               Mean episode length: 419.00
                 Mean success rate: 82.50
                  Mean reward/step: 23.11
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 2.61s
                        Total time: 5866.38s
                               ETA: 4437.1s

################################################################################
                     [1m Learning iteration 2278/4000 [0m

                       Computation: 3152 steps/s (collection: 0.502s, learning 2.096s)
               Value function loss: 130765.8932
                    Surrogate loss: 0.0102
             Mean action noise std: 0.91
                       Mean reward: 9902.58
               Mean episode length: 419.00
                 Mean success rate: 82.50
                  Mean reward/step: 22.61
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18669568
                    Iteration time: 2.60s
                        Total time: 5868.98s
                               ETA: 4434.6s

################################################################################
                     [1m Learning iteration 2279/4000 [0m

                       Computation: 3219 steps/s (collection: 0.477s, learning 2.067s)
               Value function loss: 93114.9774
                    Surrogate loss: 0.0095
             Mean action noise std: 0.91
                       Mean reward: 9741.33
               Mean episode length: 414.88
                 Mean success rate: 81.00
                  Mean reward/step: 23.13
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 2.54s
                        Total time: 5871.53s
                               ETA: 4432.0s

################################################################################
                     [1m Learning iteration 2280/4000 [0m

                       Computation: 3165 steps/s (collection: 0.452s, learning 2.136s)
               Value function loss: 69266.9748
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 9555.10
               Mean episode length: 407.07
                 Mean success rate: 79.00
                  Mean reward/step: 23.05
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18685952
                    Iteration time: 2.59s
                        Total time: 5874.11s
                               ETA: 4429.4s

################################################################################
                     [1m Learning iteration 2281/4000 [0m

                       Computation: 3074 steps/s (collection: 0.522s, learning 2.142s)
               Value function loss: 96879.4480
                    Surrogate loss: 0.0106
             Mean action noise std: 0.91
                       Mean reward: 9674.57
               Mean episode length: 404.27
                 Mean success rate: 79.00
                  Mean reward/step: 24.12
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 2.66s
                        Total time: 5876.78s
                               ETA: 4426.9s

################################################################################
                     [1m Learning iteration 2282/4000 [0m

                       Computation: 3071 steps/s (collection: 0.527s, learning 2.140s)
               Value function loss: 95517.4698
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 9641.61
               Mean episode length: 405.30
                 Mean success rate: 79.00
                  Mean reward/step: 24.02
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18702336
                    Iteration time: 2.67s
                        Total time: 5879.45s
                               ETA: 4424.4s

################################################################################
                     [1m Learning iteration 2283/4000 [0m

                       Computation: 3165 steps/s (collection: 0.501s, learning 2.087s)
               Value function loss: 103508.6093
                    Surrogate loss: 0.0119
             Mean action noise std: 0.91
                       Mean reward: 9696.77
               Mean episode length: 408.51
                 Mean success rate: 79.50
                  Mean reward/step: 23.97
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 2.59s
                        Total time: 5882.03s
                               ETA: 4421.8s

################################################################################
                     [1m Learning iteration 2284/4000 [0m

                       Computation: 3144 steps/s (collection: 0.482s, learning 2.123s)
               Value function loss: 69738.1228
                    Surrogate loss: 0.0116
             Mean action noise std: 0.91
                       Mean reward: 9719.08
               Mean episode length: 407.48
                 Mean success rate: 78.50
                  Mean reward/step: 24.38
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18718720
                    Iteration time: 2.60s
                        Total time: 5884.64s
                               ETA: 4419.3s

################################################################################
                     [1m Learning iteration 2285/4000 [0m

                       Computation: 3145 steps/s (collection: 0.474s, learning 2.131s)
               Value function loss: 55859.7904
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 9945.21
               Mean episode length: 415.12
                 Mean success rate: 80.00
                  Mean reward/step: 25.33
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 2.60s
                        Total time: 5887.24s
                               ETA: 4416.7s

################################################################################
                     [1m Learning iteration 2286/4000 [0m

                       Computation: 3075 steps/s (collection: 0.547s, learning 2.117s)
               Value function loss: 112037.4344
                    Surrogate loss: 0.0104
             Mean action noise std: 0.91
                       Mean reward: 9744.38
               Mean episode length: 409.48
                 Mean success rate: 78.50
                  Mean reward/step: 25.90
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 18735104
                    Iteration time: 2.66s
                        Total time: 5889.91s
                               ETA: 4414.2s

################################################################################
                     [1m Learning iteration 2287/4000 [0m

                       Computation: 3167 steps/s (collection: 0.493s, learning 2.093s)
               Value function loss: 99713.6679
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 9912.68
               Mean episode length: 412.60
                 Mean success rate: 79.00
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 2.59s
                        Total time: 5892.49s
                               ETA: 4411.6s

################################################################################
                     [1m Learning iteration 2288/4000 [0m

                       Computation: 3115 steps/s (collection: 0.507s, learning 2.123s)
               Value function loss: 97008.2771
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 9651.67
               Mean episode length: 405.86
                 Mean success rate: 77.50
                  Mean reward/step: 24.34
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18751488
                    Iteration time: 2.63s
                        Total time: 5895.12s
                               ETA: 4409.1s

################################################################################
                     [1m Learning iteration 2289/4000 [0m

                       Computation: 3153 steps/s (collection: 0.468s, learning 2.130s)
               Value function loss: 80865.1979
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 9877.91
               Mean episode length: 410.37
                 Mean success rate: 79.00
                  Mean reward/step: 24.71
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 2.60s
                        Total time: 5897.72s
                               ETA: 4406.6s

################################################################################
                     [1m Learning iteration 2290/4000 [0m

                       Computation: 3175 steps/s (collection: 0.516s, learning 2.064s)
               Value function loss: 69753.9405
                    Surrogate loss: 0.0110
             Mean action noise std: 0.91
                       Mean reward: 9481.60
               Mean episode length: 398.44
                 Mean success rate: 76.00
                  Mean reward/step: 25.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18767872
                    Iteration time: 2.58s
                        Total time: 5900.30s
                               ETA: 4404.0s

################################################################################
                     [1m Learning iteration 2291/4000 [0m

                       Computation: 3190 steps/s (collection: 0.498s, learning 2.069s)
               Value function loss: 119299.0082
                    Surrogate loss: 0.0094
             Mean action noise std: 0.91
                       Mean reward: 9907.08
               Mean episode length: 409.86
                 Mean success rate: 79.00
                  Mean reward/step: 25.20
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 2.57s
                        Total time: 5902.87s
                               ETA: 4401.4s

################################################################################
                     [1m Learning iteration 2292/4000 [0m

                       Computation: 3166 steps/s (collection: 0.532s, learning 2.056s)
               Value function loss: 123259.5561
                    Surrogate loss: 0.0180
             Mean action noise std: 0.91
                       Mean reward: 10195.06
               Mean episode length: 422.45
                 Mean success rate: 81.00
                  Mean reward/step: 25.06
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18784256
                    Iteration time: 2.59s
                        Total time: 5905.46s
                               ETA: 4398.8s

################################################################################
                     [1m Learning iteration 2293/4000 [0m

                       Computation: 3105 steps/s (collection: 0.546s, learning 2.091s)
               Value function loss: 96618.4625
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 10012.81
               Mean episode length: 413.14
                 Mean success rate: 79.00
                  Mean reward/step: 24.29
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 2.64s
                        Total time: 5908.09s
                               ETA: 4396.3s

################################################################################
                     [1m Learning iteration 2294/4000 [0m

                       Computation: 3168 steps/s (collection: 0.517s, learning 2.069s)
               Value function loss: 99626.7000
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 10056.69
               Mean episode length: 414.31
                 Mean success rate: 79.50
                  Mean reward/step: 24.12
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 18800640
                    Iteration time: 2.59s
                        Total time: 5910.68s
                               ETA: 4393.7s

################################################################################
                     [1m Learning iteration 2295/4000 [0m

                       Computation: 3209 steps/s (collection: 0.461s, learning 2.091s)
               Value function loss: 114320.3354
                    Surrogate loss: 0.0110
             Mean action noise std: 0.91
                       Mean reward: 10119.21
               Mean episode length: 415.69
                 Mean success rate: 79.00
                  Mean reward/step: 23.94
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 2.55s
                        Total time: 5913.23s
                               ETA: 4391.1s

################################################################################
                     [1m Learning iteration 2296/4000 [0m

                       Computation: 3170 steps/s (collection: 0.462s, learning 2.122s)
               Value function loss: 83801.6817
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 9995.99
               Mean episode length: 413.88
                 Mean success rate: 78.00
                  Mean reward/step: 23.98
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18817024
                    Iteration time: 2.58s
                        Total time: 5915.81s
                               ETA: 4388.6s

################################################################################
                     [1m Learning iteration 2297/4000 [0m

                       Computation: 3053 steps/s (collection: 0.554s, learning 2.130s)
               Value function loss: 128467.4111
                    Surrogate loss: 0.0111
             Mean action noise std: 0.91
                       Mean reward: 9870.88
               Mean episode length: 409.97
                 Mean success rate: 77.00
                  Mean reward/step: 23.91
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 2.68s
                        Total time: 5918.50s
                               ETA: 4386.1s

################################################################################
                     [1m Learning iteration 2298/4000 [0m

                       Computation: 3114 steps/s (collection: 0.520s, learning 2.110s)
               Value function loss: 94942.6058
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 10137.84
               Mean episode length: 416.07
                 Mean success rate: 78.00
                  Mean reward/step: 23.87
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18833408
                    Iteration time: 2.63s
                        Total time: 5921.13s
                               ETA: 4383.5s

################################################################################
                     [1m Learning iteration 2299/4000 [0m

                       Computation: 3134 steps/s (collection: 0.489s, learning 2.125s)
               Value function loss: 83007.9421
                    Surrogate loss: 0.0115
             Mean action noise std: 0.91
                       Mean reward: 10084.87
               Mean episode length: 411.44
                 Mean success rate: 78.00
                  Mean reward/step: 24.11
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 2.61s
                        Total time: 5923.74s
                               ETA: 4381.0s

################################################################################
                     [1m Learning iteration 2300/4000 [0m

                       Computation: 3214 steps/s (collection: 0.474s, learning 2.074s)
               Value function loss: 73752.1489
                    Surrogate loss: 0.0106
             Mean action noise std: 0.91
                       Mean reward: 9826.52
               Mean episode length: 404.76
                 Mean success rate: 76.50
                  Mean reward/step: 25.33
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 18849792
                    Iteration time: 2.55s
                        Total time: 5926.29s
                               ETA: 4378.4s

################################################################################
                     [1m Learning iteration 2301/4000 [0m

                       Computation: 3159 steps/s (collection: 0.493s, learning 2.099s)
               Value function loss: 77400.8519
                    Surrogate loss: 0.0088
             Mean action noise std: 0.91
                       Mean reward: 9915.73
               Mean episode length: 407.25
                 Mean success rate: 77.00
                  Mean reward/step: 26.40
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 2.59s
                        Total time: 5928.88s
                               ETA: 4375.8s

################################################################################
                     [1m Learning iteration 2302/4000 [0m

                       Computation: 3118 steps/s (collection: 0.514s, learning 2.113s)
               Value function loss: 148903.0752
                    Surrogate loss: 0.0096
             Mean action noise std: 0.91
                       Mean reward: 9725.10
               Mean episode length: 397.26
                 Mean success rate: 75.50
                  Mean reward/step: 25.72
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 18866176
                    Iteration time: 2.63s
                        Total time: 5931.51s
                               ETA: 4373.3s

################################################################################
                     [1m Learning iteration 2303/4000 [0m

                       Computation: 3215 steps/s (collection: 0.459s, learning 2.089s)
               Value function loss: 74301.8046
                    Surrogate loss: 0.0107
             Mean action noise std: 0.91
                       Mean reward: 9711.94
               Mean episode length: 398.75
                 Mean success rate: 76.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 2.55s
                        Total time: 5934.06s
                               ETA: 4370.7s

################################################################################
                     [1m Learning iteration 2304/4000 [0m

                       Computation: 3121 steps/s (collection: 0.496s, learning 2.129s)
               Value function loss: 104450.6947
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 9924.61
               Mean episode length: 407.35
                 Mean success rate: 77.50
                  Mean reward/step: 24.52
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18882560
                    Iteration time: 2.62s
                        Total time: 5936.68s
                               ETA: 4368.2s

################################################################################
                     [1m Learning iteration 2305/4000 [0m

                       Computation: 3178 steps/s (collection: 0.494s, learning 2.083s)
               Value function loss: 91918.8669
                    Surrogate loss: 0.0096
             Mean action noise std: 0.91
                       Mean reward: 9869.78
               Mean episode length: 404.79
                 Mean success rate: 77.50
                  Mean reward/step: 25.11
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 2.58s
                        Total time: 5939.26s
                               ETA: 4365.6s

################################################################################
                     [1m Learning iteration 2306/4000 [0m

                       Computation: 3223 steps/s (collection: 0.453s, learning 2.088s)
               Value function loss: 82211.7681
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 9617.08
               Mean episode length: 397.21
                 Mean success rate: 76.00
                  Mean reward/step: 24.84
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18898944
                    Iteration time: 2.54s
                        Total time: 5941.80s
                               ETA: 4363.0s

################################################################################
                     [1m Learning iteration 2307/4000 [0m

                       Computation: 3198 steps/s (collection: 0.485s, learning 2.076s)
               Value function loss: 105637.7922
                    Surrogate loss: 0.0126
             Mean action noise std: 0.91
                       Mean reward: 9310.33
               Mean episode length: 383.61
                 Mean success rate: 74.00
                  Mean reward/step: 23.93
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 2.56s
                        Total time: 5944.36s
                               ETA: 4360.4s

################################################################################
                     [1m Learning iteration 2308/4000 [0m

                       Computation: 3201 steps/s (collection: 0.504s, learning 2.055s)
               Value function loss: 107979.8012
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 9277.83
               Mean episode length: 379.51
                 Mean success rate: 73.50
                  Mean reward/step: 23.81
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18915328
                    Iteration time: 2.56s
                        Total time: 5946.92s
                               ETA: 4357.8s

################################################################################
                     [1m Learning iteration 2309/4000 [0m

                       Computation: 3231 steps/s (collection: 0.482s, learning 2.054s)
               Value function loss: 116516.7807
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 9659.72
               Mean episode length: 394.96
                 Mean success rate: 75.50
                  Mean reward/step: 24.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 2.54s
                        Total time: 5949.45s
                               ETA: 4355.2s

################################################################################
                     [1m Learning iteration 2310/4000 [0m

                       Computation: 3173 steps/s (collection: 0.465s, learning 2.116s)
               Value function loss: 99786.7597
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 9608.80
               Mean episode length: 392.87
                 Mean success rate: 75.50
                  Mean reward/step: 23.34
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18931712
                    Iteration time: 2.58s
                        Total time: 5952.04s
                               ETA: 4352.6s

################################################################################
                     [1m Learning iteration 2311/4000 [0m

                       Computation: 3209 steps/s (collection: 0.466s, learning 2.086s)
               Value function loss: 107365.1994
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 9287.27
               Mean episode length: 384.95
                 Mean success rate: 75.00
                  Mean reward/step: 22.41
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 2.55s
                        Total time: 5954.59s
                               ETA: 4350.0s

################################################################################
                     [1m Learning iteration 2312/4000 [0m

                       Computation: 3198 steps/s (collection: 0.510s, learning 2.051s)
               Value function loss: 106609.5729
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 9289.20
               Mean episode length: 382.26
                 Mean success rate: 75.00
                  Mean reward/step: 22.84
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18948096
                    Iteration time: 2.56s
                        Total time: 5957.15s
                               ETA: 4347.5s

################################################################################
                     [1m Learning iteration 2313/4000 [0m

                       Computation: 3196 steps/s (collection: 0.455s, learning 2.108s)
               Value function loss: 132393.2270
                    Surrogate loss: 0.0114
             Mean action noise std: 0.91
                       Mean reward: 9329.15
               Mean episode length: 381.34
                 Mean success rate: 74.50
                  Mean reward/step: 22.72
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 2.56s
                        Total time: 5959.71s
                               ETA: 4344.9s

################################################################################
                     [1m Learning iteration 2314/4000 [0m

                       Computation: 3111 steps/s (collection: 0.502s, learning 2.131s)
               Value function loss: 74351.7279
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 9454.74
               Mean episode length: 385.94
                 Mean success rate: 75.50
                  Mean reward/step: 22.66
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18964480
                    Iteration time: 2.63s
                        Total time: 5962.35s
                               ETA: 4342.3s

################################################################################
                     [1m Learning iteration 2315/4000 [0m

                       Computation: 3151 steps/s (collection: 0.493s, learning 2.107s)
               Value function loss: 87352.5909
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 9727.14
               Mean episode length: 393.98
                 Mean success rate: 77.00
                  Mean reward/step: 22.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 2.60s
                        Total time: 5964.95s
                               ETA: 4339.8s

################################################################################
                     [1m Learning iteration 2316/4000 [0m

                       Computation: 3174 steps/s (collection: 0.476s, learning 2.104s)
               Value function loss: 72854.5380
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 9820.60
               Mean episode length: 399.59
                 Mean success rate: 77.50
                  Mean reward/step: 23.85
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 18980864
                    Iteration time: 2.58s
                        Total time: 5967.53s
                               ETA: 4337.2s

################################################################################
                     [1m Learning iteration 2317/4000 [0m

                       Computation: 3236 steps/s (collection: 0.466s, learning 2.065s)
               Value function loss: 65623.2792
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 9913.22
               Mean episode length: 403.04
                 Mean success rate: 78.00
                  Mean reward/step: 25.12
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 2.53s
                        Total time: 5970.06s
                               ETA: 4334.6s

################################################################################
                     [1m Learning iteration 2318/4000 [0m

                       Computation: 3239 steps/s (collection: 0.471s, learning 2.058s)
               Value function loss: 124543.8671
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 9081.12
               Mean episode length: 377.56
                 Mean success rate: 74.00
                  Mean reward/step: 24.26
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 18997248
                    Iteration time: 2.53s
                        Total time: 5972.59s
                               ETA: 4332.0s

################################################################################
                     [1m Learning iteration 2319/4000 [0m

                       Computation: 3217 steps/s (collection: 0.474s, learning 2.072s)
               Value function loss: 73453.1563
                    Surrogate loss: 0.0104
             Mean action noise std: 0.91
                       Mean reward: 9093.99
               Mean episode length: 379.75
                 Mean success rate: 73.50
                  Mean reward/step: 23.90
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 2.55s
                        Total time: 5975.13s
                               ETA: 4329.4s

################################################################################
                     [1m Learning iteration 2320/4000 [0m

                       Computation: 3150 steps/s (collection: 0.501s, learning 2.099s)
               Value function loss: 64484.5040
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 8517.68
               Mean episode length: 359.48
                 Mean success rate: 68.00
                  Mean reward/step: 24.01
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 19013632
                    Iteration time: 2.60s
                        Total time: 5977.73s
                               ETA: 4326.8s

################################################################################
                     [1m Learning iteration 2321/4000 [0m

                       Computation: 3207 steps/s (collection: 0.477s, learning 2.077s)
               Value function loss: 94413.1664
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 8393.21
               Mean episode length: 357.24
                 Mean success rate: 67.50
                  Mean reward/step: 24.25
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 2.55s
                        Total time: 5980.29s
                               ETA: 4324.2s

################################################################################
                     [1m Learning iteration 2322/4000 [0m

                       Computation: 3113 steps/s (collection: 0.511s, learning 2.120s)
               Value function loss: 95317.5992
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 8113.91
               Mean episode length: 349.60
                 Mean success rate: 66.00
                  Mean reward/step: 24.16
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19030016
                    Iteration time: 2.63s
                        Total time: 5982.92s
                               ETA: 4321.7s

################################################################################
                     [1m Learning iteration 2323/4000 [0m

                       Computation: 3190 steps/s (collection: 0.507s, learning 2.061s)
               Value function loss: 94781.7621
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 8288.89
               Mean episode length: 354.42
                 Mean success rate: 67.00
                  Mean reward/step: 23.81
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 2.57s
                        Total time: 5985.48s
                               ETA: 4319.1s

################################################################################
                     [1m Learning iteration 2324/4000 [0m

                       Computation: 3253 steps/s (collection: 0.453s, learning 2.065s)
               Value function loss: 84026.0056
                    Surrogate loss: 0.0101
             Mean action noise std: 0.91
                       Mean reward: 7775.15
               Mean episode length: 341.82
                 Mean success rate: 64.00
                  Mean reward/step: 24.19
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 19046400
                    Iteration time: 2.52s
                        Total time: 5988.00s
                               ETA: 4316.5s

################################################################################
                     [1m Learning iteration 2325/4000 [0m

                       Computation: 3151 steps/s (collection: 0.510s, learning 2.089s)
               Value function loss: 134809.9924
                    Surrogate loss: 0.0112
             Mean action noise std: 0.91
                       Mean reward: 8206.82
               Mean episode length: 350.03
                 Mean success rate: 67.50
                  Mean reward/step: 24.32
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 2.60s
                        Total time: 5990.60s
                               ETA: 4314.0s

################################################################################
                     [1m Learning iteration 2326/4000 [0m

                       Computation: 3182 steps/s (collection: 0.454s, learning 2.120s)
               Value function loss: 137429.5891
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 8797.21
               Mean episode length: 370.33
                 Mean success rate: 71.00
                  Mean reward/step: 23.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19062784
                    Iteration time: 2.57s
                        Total time: 5993.18s
                               ETA: 4311.4s

################################################################################
                     [1m Learning iteration 2327/4000 [0m

                       Computation: 3195 steps/s (collection: 0.478s, learning 2.086s)
               Value function loss: 82910.4930
                    Surrogate loss: 0.0104
             Mean action noise std: 0.91
                       Mean reward: 8800.81
               Mean episode length: 370.89
                 Mean success rate: 71.00
                  Mean reward/step: 22.88
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 2.56s
                        Total time: 5995.74s
                               ETA: 4308.8s

################################################################################
                     [1m Learning iteration 2328/4000 [0m

                       Computation: 3216 steps/s (collection: 0.468s, learning 2.079s)
               Value function loss: 87881.0830
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 8785.13
               Mean episode length: 367.03
                 Mean success rate: 70.50
                  Mean reward/step: 23.41
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19079168
                    Iteration time: 2.55s
                        Total time: 5998.29s
                               ETA: 4306.2s

################################################################################
                     [1m Learning iteration 2329/4000 [0m

                       Computation: 3112 steps/s (collection: 0.506s, learning 2.126s)
               Value function loss: 136268.9682
                    Surrogate loss: 0.0115
             Mean action noise std: 0.91
                       Mean reward: 9389.16
               Mean episode length: 388.58
                 Mean success rate: 75.00
                  Mean reward/step: 22.64
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 2.63s
                        Total time: 6000.92s
                               ETA: 4303.7s

################################################################################
                     [1m Learning iteration 2330/4000 [0m

                       Computation: 3212 steps/s (collection: 0.470s, learning 2.080s)
               Value function loss: 101796.5617
                    Surrogate loss: 0.0126
             Mean action noise std: 0.91
                       Mean reward: 9523.39
               Mean episode length: 390.58
                 Mean success rate: 74.50
                  Mean reward/step: 22.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19095552
                    Iteration time: 2.55s
                        Total time: 6003.47s
                               ETA: 4301.1s

################################################################################
                     [1m Learning iteration 2331/4000 [0m

                       Computation: 3138 steps/s (collection: 0.519s, learning 2.091s)
               Value function loss: 71420.6794
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 9516.48
               Mean episode length: 390.69
                 Mean success rate: 74.00
                  Mean reward/step: 23.25
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 2.61s
                        Total time: 6006.08s
                               ETA: 4298.5s

################################################################################
                     [1m Learning iteration 2332/4000 [0m

                       Computation: 3188 steps/s (collection: 0.498s, learning 2.071s)
               Value function loss: 66556.0188
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 9528.23
               Mean episode length: 389.88
                 Mean success rate: 74.00
                  Mean reward/step: 24.03
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 19111936
                    Iteration time: 2.57s
                        Total time: 6008.65s
                               ETA: 4295.9s

################################################################################
                     [1m Learning iteration 2333/4000 [0m

                       Computation: 3186 steps/s (collection: 0.495s, learning 2.076s)
               Value function loss: 138934.4131
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 9775.73
               Mean episode length: 397.47
                 Mean success rate: 76.00
                  Mean reward/step: 24.02
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 2.57s
                        Total time: 6011.22s
                               ETA: 4293.4s

################################################################################
                     [1m Learning iteration 2334/4000 [0m

                       Computation: 3242 steps/s (collection: 0.463s, learning 2.063s)
               Value function loss: 65775.3396
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 9845.55
               Mean episode length: 400.24
                 Mean success rate: 76.00
                  Mean reward/step: 23.08
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19128320
                    Iteration time: 2.53s
                        Total time: 6013.75s
                               ETA: 4290.7s

################################################################################
                     [1m Learning iteration 2335/4000 [0m

                       Computation: 3249 steps/s (collection: 0.454s, learning 2.067s)
               Value function loss: 107613.0188
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 9141.99
               Mean episode length: 381.34
                 Mean success rate: 72.50
                  Mean reward/step: 23.30
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 2.52s
                        Total time: 6016.27s
                               ETA: 4288.1s

################################################################################
                     [1m Learning iteration 2336/4000 [0m

                       Computation: 3305 steps/s (collection: 0.456s, learning 2.023s)
               Value function loss: 82322.0996
                    Surrogate loss: 0.0159
             Mean action noise std: 0.91
                       Mean reward: 8707.95
               Mean episode length: 367.47
                 Mean success rate: 69.50
                  Mean reward/step: 22.99
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19144704
                    Iteration time: 2.48s
                        Total time: 6018.74s
                               ETA: 4285.5s

################################################################################
                     [1m Learning iteration 2337/4000 [0m

                       Computation: 3195 steps/s (collection: 0.491s, learning 2.072s)
               Value function loss: 106607.9603
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 8699.83
               Mean episode length: 368.93
                 Mean success rate: 70.00
                  Mean reward/step: 22.38
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 2.56s
                        Total time: 6021.31s
                               ETA: 4282.9s

################################################################################
                     [1m Learning iteration 2338/4000 [0m

                       Computation: 3181 steps/s (collection: 0.488s, learning 2.087s)
               Value function loss: 101149.0755
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 8441.68
               Mean episode length: 367.30
                 Mean success rate: 69.00
                  Mean reward/step: 22.17
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 19161088
                    Iteration time: 2.57s
                        Total time: 6023.88s
                               ETA: 4280.3s

################################################################################
                     [1m Learning iteration 2339/4000 [0m

                       Computation: 3203 steps/s (collection: 0.467s, learning 2.090s)
               Value function loss: 64391.1351
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 8398.71
               Mean episode length: 370.27
                 Mean success rate: 69.50
                  Mean reward/step: 22.39
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 2.56s
                        Total time: 6026.44s
                               ETA: 4277.7s

################################################################################
                     [1m Learning iteration 2340/4000 [0m

                       Computation: 3180 steps/s (collection: 0.474s, learning 2.102s)
               Value function loss: 112272.6518
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 8702.63
               Mean episode length: 381.17
                 Mean success rate: 73.00
                  Mean reward/step: 22.89
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 19177472
                    Iteration time: 2.58s
                        Total time: 6029.02s
                               ETA: 4275.2s

################################################################################
                     [1m Learning iteration 2341/4000 [0m

                       Computation: 3178 steps/s (collection: 0.503s, learning 2.074s)
               Value function loss: 106575.8117
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 8692.69
               Mean episode length: 383.69
                 Mean success rate: 74.00
                  Mean reward/step: 22.62
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 2.58s
                        Total time: 6031.59s
                               ETA: 4272.6s

################################################################################
                     [1m Learning iteration 2342/4000 [0m

                       Computation: 3121 steps/s (collection: 0.509s, learning 2.115s)
               Value function loss: 98689.0108
                    Surrogate loss: 0.0169
             Mean action noise std: 0.91
                       Mean reward: 8319.75
               Mean episode length: 371.97
                 Mean success rate: 70.50
                  Mean reward/step: 22.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 19193856
                    Iteration time: 2.62s
                        Total time: 6034.22s
                               ETA: 4270.1s

################################################################################
                     [1m Learning iteration 2343/4000 [0m

                       Computation: 3209 steps/s (collection: 0.452s, learning 2.100s)
               Value function loss: 87859.3504
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 8504.05
               Mean episode length: 378.14
                 Mean success rate: 72.00
                  Mean reward/step: 23.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 2.55s
                        Total time: 6036.77s
                               ETA: 4267.5s

################################################################################
                     [1m Learning iteration 2344/4000 [0m

                       Computation: 3217 steps/s (collection: 0.456s, learning 2.090s)
               Value function loss: 131185.7533
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 8846.99
               Mean episode length: 386.85
                 Mean success rate: 73.00
                  Mean reward/step: 22.89
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 19210240
                    Iteration time: 2.55s
                        Total time: 6039.32s
                               ETA: 4264.9s

################################################################################
                     [1m Learning iteration 2345/4000 [0m

                       Computation: 3144 steps/s (collection: 0.478s, learning 2.127s)
               Value function loss: 78225.5061
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 8803.27
               Mean episode length: 387.26
                 Mean success rate: 73.50
                  Mean reward/step: 22.17
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 2.61s
                        Total time: 6041.92s
                               ETA: 4262.3s

################################################################################
                     [1m Learning iteration 2346/4000 [0m

                       Computation: 3153 steps/s (collection: 0.498s, learning 2.099s)
               Value function loss: 101578.8280
                    Surrogate loss: 0.0148
             Mean action noise std: 0.91
                       Mean reward: 8849.00
               Mean episode length: 388.73
                 Mean success rate: 73.50
                  Mean reward/step: 22.40
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19226624
                    Iteration time: 2.60s
                        Total time: 6044.52s
                               ETA: 4259.8s

################################################################################
                     [1m Learning iteration 2347/4000 [0m

                       Computation: 3176 steps/s (collection: 0.510s, learning 2.069s)
               Value function loss: 75398.7606
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 8305.38
               Mean episode length: 361.62
                 Mean success rate: 68.50
                  Mean reward/step: 22.97
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 2.58s
                        Total time: 6047.10s
                               ETA: 4257.2s

################################################################################
                     [1m Learning iteration 2348/4000 [0m

                       Computation: 3260 steps/s (collection: 0.434s, learning 2.078s)
               Value function loss: 66438.6693
                    Surrogate loss: 0.0104
             Mean action noise std: 0.91
                       Mean reward: 8412.65
               Mean episode length: 362.31
                 Mean success rate: 69.00
                  Mean reward/step: 24.01
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 19243008
                    Iteration time: 2.51s
                        Total time: 6049.61s
                               ETA: 4254.6s

################################################################################
                     [1m Learning iteration 2349/4000 [0m

                       Computation: 3204 steps/s (collection: 0.481s, learning 2.076s)
               Value function loss: 109754.6868
                    Surrogate loss: 0.0111
             Mean action noise std: 0.91
                       Mean reward: 8226.74
               Mean episode length: 353.70
                 Mean success rate: 66.50
                  Mean reward/step: 24.98
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 2.56s
                        Total time: 6052.17s
                               ETA: 4252.0s

################################################################################
                     [1m Learning iteration 2350/4000 [0m

                       Computation: 3220 steps/s (collection: 0.480s, learning 2.064s)
               Value function loss: 56551.1012
                    Surrogate loss: 0.0115
             Mean action noise std: 0.91
                       Mean reward: 8389.15
               Mean episode length: 357.37
                 Mean success rate: 67.50
                  Mean reward/step: 25.16
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 19259392
                    Iteration time: 2.54s
                        Total time: 6054.71s
                               ETA: 4249.4s

################################################################################
                     [1m Learning iteration 2351/4000 [0m

                       Computation: 3270 steps/s (collection: 0.464s, learning 2.041s)
               Value function loss: 112711.4152
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 8567.02
               Mean episode length: 365.28
                 Mean success rate: 70.00
                  Mean reward/step: 24.48
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 2.50s
                        Total time: 6057.22s
                               ETA: 4246.7s

################################################################################
                     [1m Learning iteration 2352/4000 [0m

                       Computation: 3323 steps/s (collection: 0.435s, learning 2.030s)
               Value function loss: 91710.1318
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 8595.66
               Mean episode length: 367.51
                 Mean success rate: 71.50
                  Mean reward/step: 23.93
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 19275776
                    Iteration time: 2.47s
                        Total time: 6059.68s
                               ETA: 4244.1s

################################################################################
                     [1m Learning iteration 2353/4000 [0m

                       Computation: 3211 steps/s (collection: 0.470s, learning 2.081s)
               Value function loss: 86272.1107
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 8439.87
               Mean episode length: 365.02
                 Mean success rate: 70.50
                  Mean reward/step: 23.53
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 2.55s
                        Total time: 6062.23s
                               ETA: 4241.5s

################################################################################
                     [1m Learning iteration 2354/4000 [0m

                       Computation: 3304 steps/s (collection: 0.428s, learning 2.051s)
               Value function loss: 74847.1269
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 8539.83
               Mean episode length: 367.20
                 Mean success rate: 71.50
                  Mean reward/step: 23.35
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19292160
                    Iteration time: 2.48s
                        Total time: 6064.71s
                               ETA: 4238.9s

################################################################################
                     [1m Learning iteration 2355/4000 [0m

                       Computation: 3263 steps/s (collection: 0.434s, learning 2.077s)
               Value function loss: 60796.2427
                    Surrogate loss: 0.0097
             Mean action noise std: 0.91
                       Mean reward: 8772.64
               Mean episode length: 377.10
                 Mean success rate: 73.00
                  Mean reward/step: 24.47
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 2.51s
                        Total time: 6067.22s
                               ETA: 4236.2s

################################################################################
                     [1m Learning iteration 2356/4000 [0m

                       Computation: 3239 steps/s (collection: 0.432s, learning 2.097s)
               Value function loss: 125253.9014
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 8714.10
               Mean episode length: 377.80
                 Mean success rate: 73.50
                  Mean reward/step: 24.83
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 19308544
                    Iteration time: 2.53s
                        Total time: 6069.75s
                               ETA: 4233.6s

################################################################################
                     [1m Learning iteration 2357/4000 [0m

                       Computation: 3250 steps/s (collection: 0.471s, learning 2.049s)
               Value function loss: 93514.2597
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 9344.27
               Mean episode length: 399.44
                 Mean success rate: 78.00
                  Mean reward/step: 24.29
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 2.52s
                        Total time: 6072.27s
                               ETA: 4231.0s

################################################################################
                     [1m Learning iteration 2358/4000 [0m

                       Computation: 3173 steps/s (collection: 0.464s, learning 2.118s)
               Value function loss: 76467.0780
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 9478.35
               Mean episode length: 405.58
                 Mean success rate: 79.00
                  Mean reward/step: 23.87
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19324928
                    Iteration time: 2.58s
                        Total time: 6074.85s
                               ETA: 4228.4s

################################################################################
                     [1m Learning iteration 2359/4000 [0m

                       Computation: 3277 steps/s (collection: 0.442s, learning 2.058s)
               Value function loss: 79209.0381
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 9700.51
               Mean episode length: 414.57
                 Mean success rate: 80.50
                  Mean reward/step: 24.44
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 2.50s
                        Total time: 6077.35s
                               ETA: 4225.8s

################################################################################
                     [1m Learning iteration 2360/4000 [0m

                       Computation: 3206 steps/s (collection: 0.465s, learning 2.090s)
               Value function loss: 123109.1921
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 9565.85
               Mean episode length: 411.65
                 Mean success rate: 79.00
                  Mean reward/step: 23.95
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 19341312
                    Iteration time: 2.55s
                        Total time: 6079.91s
                               ETA: 4223.2s

################################################################################
                     [1m Learning iteration 2361/4000 [0m

                       Computation: 3217 steps/s (collection: 0.452s, learning 2.094s)
               Value function loss: 89473.9341
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 9655.87
               Mean episode length: 411.50
                 Mean success rate: 79.00
                  Mean reward/step: 23.47
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 2.55s
                        Total time: 6082.45s
                               ETA: 4220.6s

################################################################################
                     [1m Learning iteration 2362/4000 [0m

                       Computation: 3237 steps/s (collection: 0.456s, learning 2.074s)
               Value function loss: 84980.1193
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 9601.10
               Mean episode length: 406.50
                 Mean success rate: 78.00
                  Mean reward/step: 23.52
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19357696
                    Iteration time: 2.53s
                        Total time: 6084.98s
                               ETA: 4218.0s

################################################################################
                     [1m Learning iteration 2363/4000 [0m

                       Computation: 3212 steps/s (collection: 0.456s, learning 2.094s)
               Value function loss: 99117.5510
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 9706.10
               Mean episode length: 403.27
                 Mean success rate: 77.50
                  Mean reward/step: 23.33
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 2.55s
                        Total time: 6087.53s
                               ETA: 4215.4s

################################################################################
                     [1m Learning iteration 2364/4000 [0m

                       Computation: 3208 steps/s (collection: 0.456s, learning 2.097s)
               Value function loss: 69142.7448
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 9435.02
               Mean episode length: 394.57
                 Mean success rate: 76.00
                  Mean reward/step: 23.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 19374080
                    Iteration time: 2.55s
                        Total time: 6090.09s
                               ETA: 4212.8s

################################################################################
                     [1m Learning iteration 2365/4000 [0m

                       Computation: 3254 steps/s (collection: 0.470s, learning 2.047s)
               Value function loss: 107462.5313
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 9582.81
               Mean episode length: 395.84
                 Mean success rate: 76.00
                  Mean reward/step: 23.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 2.52s
                        Total time: 6092.60s
                               ETA: 4210.2s

################################################################################
                     [1m Learning iteration 2366/4000 [0m

                       Computation: 3223 steps/s (collection: 0.457s, learning 2.085s)
               Value function loss: 93243.3945
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 9775.56
               Mean episode length: 401.11
                 Mean success rate: 77.50
                  Mean reward/step: 24.57
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19390464
                    Iteration time: 2.54s
                        Total time: 6095.15s
                               ETA: 4207.6s

################################################################################
                     [1m Learning iteration 2367/4000 [0m

                       Computation: 3206 steps/s (collection: 0.466s, learning 2.089s)
               Value function loss: 113383.9680
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 9450.50
               Mean episode length: 387.60
                 Mean success rate: 74.00
                  Mean reward/step: 24.49
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 2.55s
                        Total time: 6097.70s
                               ETA: 4205.0s

################################################################################
                     [1m Learning iteration 2368/4000 [0m

                       Computation: 3233 steps/s (collection: 0.487s, learning 2.046s)
               Value function loss: 91197.6325
                    Surrogate loss: 0.0114
             Mean action noise std: 0.91
                       Mean reward: 9248.36
               Mean episode length: 383.31
                 Mean success rate: 73.00
                  Mean reward/step: 23.95
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 19406848
                    Iteration time: 2.53s
                        Total time: 6100.23s
                               ETA: 4202.4s

################################################################################
                     [1m Learning iteration 2369/4000 [0m

                       Computation: 3267 steps/s (collection: 0.470s, learning 2.037s)
               Value function loss: 96686.4535
                    Surrogate loss: 0.0110
             Mean action noise std: 0.91
                       Mean reward: 9220.24
               Mean episode length: 382.74
                 Mean success rate: 72.50
                  Mean reward/step: 24.01
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 2.51s
                        Total time: 6102.74s
                               ETA: 4199.8s

################################################################################
                     [1m Learning iteration 2370/4000 [0m

                       Computation: 3332 steps/s (collection: 0.447s, learning 2.012s)
               Value function loss: 86827.8058
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 8921.69
               Mean episode length: 374.27
                 Mean success rate: 70.50
                  Mean reward/step: 23.66
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 19423232
                    Iteration time: 2.46s
                        Total time: 6105.20s
                               ETA: 4197.2s

################################################################################
                     [1m Learning iteration 2371/4000 [0m

                       Computation: 3249 steps/s (collection: 0.480s, learning 2.040s)
               Value function loss: 74093.2894
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 8805.94
               Mean episode length: 370.95
                 Mean success rate: 70.50
                  Mean reward/step: 23.91
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 2.52s
                        Total time: 6107.72s
                               ETA: 4194.6s

################################################################################
                     [1m Learning iteration 2372/4000 [0m

                       Computation: 3219 steps/s (collection: 0.473s, learning 2.071s)
               Value function loss: 126823.7859
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 8848.29
               Mean episode length: 375.43
                 Mean success rate: 70.50
                  Mean reward/step: 23.93
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 19439616
                    Iteration time: 2.54s
                        Total time: 6110.26s
                               ETA: 4192.0s

################################################################################
                     [1m Learning iteration 2373/4000 [0m

                       Computation: 3241 steps/s (collection: 0.474s, learning 2.053s)
               Value function loss: 89010.0454
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 8969.92
               Mean episode length: 380.88
                 Mean success rate: 71.50
                  Mean reward/step: 23.55
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 2.53s
                        Total time: 6112.79s
                               ETA: 4189.3s

################################################################################
                     [1m Learning iteration 2374/4000 [0m

                       Computation: 3282 steps/s (collection: 0.459s, learning 2.036s)
               Value function loss: 54257.8645
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 8962.19
               Mean episode length: 380.46
                 Mean success rate: 71.50
                  Mean reward/step: 24.05
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 19456000
                    Iteration time: 2.50s
                        Total time: 6115.29s
                               ETA: 4186.7s

################################################################################
                     [1m Learning iteration 2375/4000 [0m

                       Computation: 3225 steps/s (collection: 0.454s, learning 2.085s)
               Value function loss: 101301.0385
                    Surrogate loss: 0.0103
             Mean action noise std: 0.91
                       Mean reward: 8936.02
               Mean episode length: 379.64
                 Mean success rate: 71.50
                  Mean reward/step: 25.10
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.54s
                        Total time: 6117.83s
                               ETA: 4184.1s

################################################################################
                     [1m Learning iteration 2376/4000 [0m

                       Computation: 3269 steps/s (collection: 0.419s, learning 2.086s)
               Value function loss: 107367.0080
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 9471.97
               Mean episode length: 391.82
                 Mean success rate: 74.50
                  Mean reward/step: 24.09
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19472384
                    Iteration time: 2.51s
                        Total time: 6120.33s
                               ETA: 4181.5s

################################################################################
                     [1m Learning iteration 2377/4000 [0m

                       Computation: 3214 steps/s (collection: 0.421s, learning 2.127s)
               Value function loss: 94298.7716
                    Surrogate loss: 0.0116
             Mean action noise std: 0.91
                       Mean reward: 9504.33
               Mean episode length: 395.72
                 Mean success rate: 75.50
                  Mean reward/step: 23.32
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 2.55s
                        Total time: 6122.88s
                               ETA: 4178.9s

################################################################################
                     [1m Learning iteration 2378/4000 [0m

                       Computation: 3244 steps/s (collection: 0.438s, learning 2.087s)
               Value function loss: 116294.8963
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 9770.15
               Mean episode length: 399.90
                 Mean success rate: 77.50
                  Mean reward/step: 23.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19488768
                    Iteration time: 2.52s
                        Total time: 6125.40s
                               ETA: 4176.3s

################################################################################
                     [1m Learning iteration 2379/4000 [0m

                       Computation: 3214 steps/s (collection: 0.431s, learning 2.118s)
               Value function loss: 73930.1438
                    Surrogate loss: 0.0114
             Mean action noise std: 0.91
                       Mean reward: 9471.85
               Mean episode length: 389.57
                 Mean success rate: 75.00
                  Mean reward/step: 23.13
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 2.55s
                        Total time: 6127.95s
                               ETA: 4173.7s

################################################################################
                     [1m Learning iteration 2380/4000 [0m

                       Computation: 3161 steps/s (collection: 0.469s, learning 2.122s)
               Value function loss: 129986.2795
                    Surrogate loss: 0.0111
             Mean action noise std: 0.91
                       Mean reward: 10028.63
               Mean episode length: 406.80
                 Mean success rate: 79.50
                  Mean reward/step: 23.54
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19505152
                    Iteration time: 2.59s
                        Total time: 6130.54s
                               ETA: 4171.1s

################################################################################
                     [1m Learning iteration 2381/4000 [0m

                       Computation: 3184 steps/s (collection: 0.488s, learning 2.084s)
               Value function loss: 60040.3829
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 10031.51
               Mean episode length: 405.01
                 Mean success rate: 79.50
                  Mean reward/step: 23.85
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 2.57s
                        Total time: 6133.12s
                               ETA: 4168.6s

################################################################################
                     [1m Learning iteration 2382/4000 [0m

                       Computation: 3164 steps/s (collection: 0.474s, learning 2.115s)
               Value function loss: 110397.0883
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 9840.81
               Mean episode length: 404.31
                 Mean success rate: 78.00
                  Mean reward/step: 24.42
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 19521536
                    Iteration time: 2.59s
                        Total time: 6135.70s
                               ETA: 4166.0s

################################################################################
                     [1m Learning iteration 2383/4000 [0m

                       Computation: 3076 steps/s (collection: 0.480s, learning 2.183s)
               Value function loss: 103793.9336
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 10400.16
               Mean episode length: 421.46
                 Mean success rate: 82.00
                  Mean reward/step: 24.48
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 2.66s
                        Total time: 6138.37s
                               ETA: 4163.5s

################################################################################
                     [1m Learning iteration 2384/4000 [0m

                       Computation: 3225 steps/s (collection: 0.450s, learning 2.090s)
               Value function loss: 97948.1586
                    Surrogate loss: 0.0108
             Mean action noise std: 0.91
                       Mean reward: 10393.45
               Mean episode length: 421.27
                 Mean success rate: 81.50
                  Mean reward/step: 24.16
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19537920
                    Iteration time: 2.54s
                        Total time: 6140.91s
                               ETA: 4160.9s

################################################################################
                     [1m Learning iteration 2385/4000 [0m

                       Computation: 3244 steps/s (collection: 0.457s, learning 2.067s)
               Value function loss: 105809.9622
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 10552.65
               Mean episode length: 427.00
                 Mean success rate: 83.00
                  Mean reward/step: 24.29
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 2.52s
                        Total time: 6143.43s
                               ETA: 4158.3s

################################################################################
                     [1m Learning iteration 2386/4000 [0m

                       Computation: 3212 steps/s (collection: 0.467s, learning 2.083s)
               Value function loss: 88296.7431
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 10157.46
               Mean episode length: 418.57
                 Mean success rate: 81.00
                  Mean reward/step: 23.88
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19554304
                    Iteration time: 2.55s
                        Total time: 6145.98s
                               ETA: 4155.7s

################################################################################
                     [1m Learning iteration 2387/4000 [0m

                       Computation: 3145 steps/s (collection: 0.449s, learning 2.155s)
               Value function loss: 105914.9016
                    Surrogate loss: 0.0101
             Mean action noise std: 0.91
                       Mean reward: 10147.93
               Mean episode length: 417.64
                 Mean success rate: 80.50
                  Mean reward/step: 24.34
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 2.60s
                        Total time: 6148.59s
                               ETA: 4153.1s

################################################################################
                     [1m Learning iteration 2388/4000 [0m

                       Computation: 3093 steps/s (collection: 0.505s, learning 2.143s)
               Value function loss: 114071.0808
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 10149.00
               Mean episode length: 422.35
                 Mean success rate: 81.00
                  Mean reward/step: 23.78
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 19570688
                    Iteration time: 2.65s
                        Total time: 6151.23s
                               ETA: 4150.6s

################################################################################
                     [1m Learning iteration 2389/4000 [0m

                       Computation: 3081 steps/s (collection: 0.528s, learning 2.131s)
               Value function loss: 77647.0477
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 10168.01
               Mean episode length: 426.20
                 Mean success rate: 81.50
                  Mean reward/step: 23.60
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 2.66s
                        Total time: 6153.89s
                               ETA: 4148.1s

################################################################################
                     [1m Learning iteration 2390/4000 [0m

                       Computation: 3132 steps/s (collection: 0.478s, learning 2.137s)
               Value function loss: 73663.1560
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 10253.93
               Mean episode length: 428.69
                 Mean success rate: 82.00
                  Mean reward/step: 24.33
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 19587072
                    Iteration time: 2.61s
                        Total time: 6156.51s
                               ETA: 4145.5s

################################################################################
                     [1m Learning iteration 2391/4000 [0m

                       Computation: 3137 steps/s (collection: 0.493s, learning 2.118s)
               Value function loss: 121974.8379
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 9949.47
               Mean episode length: 419.51
                 Mean success rate: 80.00
                  Mean reward/step: 24.91
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 2.61s
                        Total time: 6159.12s
                               ETA: 4143.0s

################################################################################
                     [1m Learning iteration 2392/4000 [0m

                       Computation: 3057 steps/s (collection: 0.505s, learning 2.174s)
               Value function loss: 69708.0674
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 10113.57
               Mean episode length: 421.47
                 Mean success rate: 81.50
                  Mean reward/step: 24.77
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 19603456
                    Iteration time: 2.68s
                        Total time: 6161.80s
                               ETA: 4140.5s

################################################################################
                     [1m Learning iteration 2393/4000 [0m

                       Computation: 3158 steps/s (collection: 0.459s, learning 2.134s)
               Value function loss: 86399.4058
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 9910.35
               Mean episode length: 415.61
                 Mean success rate: 80.00
                  Mean reward/step: 24.90
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 2.59s
                        Total time: 6164.39s
                               ETA: 4137.9s

################################################################################
                     [1m Learning iteration 2394/4000 [0m

                       Computation: 3131 steps/s (collection: 0.478s, learning 2.138s)
               Value function loss: 141892.0210
                    Surrogate loss: 0.0116
             Mean action noise std: 0.91
                       Mean reward: 9743.21
               Mean episode length: 410.32
                 Mean success rate: 78.50
                  Mean reward/step: 24.25
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 19619840
                    Iteration time: 2.62s
                        Total time: 6167.01s
                               ETA: 4135.4s

################################################################################
                     [1m Learning iteration 2395/4000 [0m

                       Computation: 3197 steps/s (collection: 0.478s, learning 2.084s)
               Value function loss: 60405.7237
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 9743.03
               Mean episode length: 410.06
                 Mean success rate: 78.00
                  Mean reward/step: 23.32
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 2.56s
                        Total time: 6169.57s
                               ETA: 4132.8s

################################################################################
                     [1m Learning iteration 2396/4000 [0m

                       Computation: 3057 steps/s (collection: 0.544s, learning 2.136s)
               Value function loss: 111854.2536
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 9779.97
               Mean episode length: 412.55
                 Mean success rate: 78.50
                  Mean reward/step: 23.56
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19636224
                    Iteration time: 2.68s
                        Total time: 6172.25s
                               ETA: 4130.3s

################################################################################
                     [1m Learning iteration 2397/4000 [0m

                       Computation: 3212 steps/s (collection: 0.485s, learning 2.065s)
               Value function loss: 99043.3217
                    Surrogate loss: 0.0095
             Mean action noise std: 0.91
                       Mean reward: 9549.26
               Mean episode length: 403.57
                 Mean success rate: 77.00
                  Mean reward/step: 23.86
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 2.55s
                        Total time: 6174.80s
                               ETA: 4127.7s

################################################################################
                     [1m Learning iteration 2398/4000 [0m

                       Computation: 3249 steps/s (collection: 0.457s, learning 2.064s)
               Value function loss: 117466.6714
                    Surrogate loss: 0.0114
             Mean action noise std: 0.91
                       Mean reward: 9630.77
               Mean episode length: 402.34
                 Mean success rate: 77.00
                  Mean reward/step: 23.07
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 19652608
                    Iteration time: 2.52s
                        Total time: 6177.32s
                               ETA: 4125.1s

################################################################################
                     [1m Learning iteration 2399/4000 [0m

                       Computation: 3188 steps/s (collection: 0.487s, learning 2.082s)
               Value function loss: 86322.8856
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 9652.25
               Mean episode length: 401.02
                 Mean success rate: 77.00
                  Mean reward/step: 23.06
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 2.57s
                        Total time: 6179.89s
                               ETA: 4122.5s

################################################################################
                     [1m Learning iteration 2400/4000 [0m

                       Computation: 3280 steps/s (collection: 0.444s, learning 2.053s)
               Value function loss: 87183.2636
                    Surrogate loss: 0.0094
             Mean action noise std: 0.91
                       Mean reward: 9676.14
               Mean episode length: 401.37
                 Mean success rate: 77.00
                  Mean reward/step: 23.61
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19668992
                    Iteration time: 2.50s
                        Total time: 6182.39s
                               ETA: 4119.9s

################################################################################
                     [1m Learning iteration 2401/4000 [0m

                       Computation: 3243 steps/s (collection: 0.503s, learning 2.022s)
               Value function loss: 136364.2592
                    Surrogate loss: 0.0103
             Mean action noise std: 0.91
                       Mean reward: 10038.35
               Mean episode length: 413.57
                 Mean success rate: 79.50
                  Mean reward/step: 23.14
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 2.53s
                        Total time: 6184.91s
                               ETA: 4117.3s

################################################################################
                     [1m Learning iteration 2402/4000 [0m

                       Computation: 3285 steps/s (collection: 0.462s, learning 2.031s)
               Value function loss: 56451.2080
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 10031.57
               Mean episode length: 414.50
                 Mean success rate: 79.50
                  Mean reward/step: 22.64
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 19685376
                    Iteration time: 2.49s
                        Total time: 6187.41s
                               ETA: 4114.6s

################################################################################
                     [1m Learning iteration 2403/4000 [0m

                       Computation: 3252 steps/s (collection: 0.436s, learning 2.083s)
               Value function loss: 119807.3668
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 10120.61
               Mean episode length: 415.14
                 Mean success rate: 79.50
                  Mean reward/step: 23.20
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 2.52s
                        Total time: 6189.92s
                               ETA: 4112.0s

################################################################################
                     [1m Learning iteration 2404/4000 [0m

                       Computation: 3257 steps/s (collection: 0.469s, learning 2.046s)
               Value function loss: 93083.4323
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 10033.27
               Mean episode length: 413.87
                 Mean success rate: 79.50
                  Mean reward/step: 23.09
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19701760
                    Iteration time: 2.52s
                        Total time: 6192.44s
                               ETA: 4109.4s

################################################################################
                     [1m Learning iteration 2405/4000 [0m

                       Computation: 3314 steps/s (collection: 0.435s, learning 2.037s)
               Value function loss: 61919.3167
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 9875.18
               Mean episode length: 414.91
                 Mean success rate: 79.00
                  Mean reward/step: 23.51
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 2.47s
                        Total time: 6194.91s
                               ETA: 4106.8s

################################################################################
                     [1m Learning iteration 2406/4000 [0m

                       Computation: 3262 steps/s (collection: 0.474s, learning 2.037s)
               Value function loss: 75512.8136
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 9883.38
               Mean episode length: 414.79
                 Mean success rate: 79.00
                  Mean reward/step: 24.63
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 19718144
                    Iteration time: 2.51s
                        Total time: 6197.42s
                               ETA: 4104.2s

################################################################################
                     [1m Learning iteration 2407/4000 [0m

                       Computation: 3279 steps/s (collection: 0.446s, learning 2.052s)
               Value function loss: 76724.4801
                    Surrogate loss: 0.0112
             Mean action noise std: 0.91
                       Mean reward: 9788.68
               Mean episode length: 413.44
                 Mean success rate: 78.00
                  Mean reward/step: 24.60
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 2.50s
                        Total time: 6199.92s
                               ETA: 4101.5s

################################################################################
                     [1m Learning iteration 2408/4000 [0m

                       Computation: 3209 steps/s (collection: 0.478s, learning 2.075s)
               Value function loss: 81967.7052
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 9839.29
               Mean episode length: 418.31
                 Mean success rate: 79.50
                  Mean reward/step: 24.90
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19734528
                    Iteration time: 2.55s
                        Total time: 6202.47s
                               ETA: 4098.9s

################################################################################
                     [1m Learning iteration 2409/4000 [0m

                       Computation: 3159 steps/s (collection: 0.478s, learning 2.115s)
               Value function loss: 124092.4719
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 9792.68
               Mean episode length: 414.59
                 Mean success rate: 78.50
                  Mean reward/step: 24.67
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 2.59s
                        Total time: 6205.07s
                               ETA: 4096.4s

################################################################################
                     [1m Learning iteration 2410/4000 [0m

                       Computation: 3215 steps/s (collection: 0.462s, learning 2.085s)
               Value function loss: 85557.1515
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 9708.48
               Mean episode length: 414.38
                 Mean success rate: 78.00
                  Mean reward/step: 23.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19750912
                    Iteration time: 2.55s
                        Total time: 6207.61s
                               ETA: 4093.8s

################################################################################
                     [1m Learning iteration 2411/4000 [0m

                       Computation: 3145 steps/s (collection: 0.488s, learning 2.117s)
               Value function loss: 67102.9701
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 9236.62
               Mean episode length: 398.45
                 Mean success rate: 75.00
                  Mean reward/step: 24.37
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 2.60s
                        Total time: 6210.22s
                               ETA: 4091.2s

################################################################################
                     [1m Learning iteration 2412/4000 [0m

                       Computation: 3022 steps/s (collection: 0.546s, learning 2.164s)
               Value function loss: 111659.8488
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 9053.96
               Mean episode length: 394.04
                 Mean success rate: 74.00
                  Mean reward/step: 23.99
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19767296
                    Iteration time: 2.71s
                        Total time: 6212.93s
                               ETA: 4088.7s

################################################################################
                     [1m Learning iteration 2413/4000 [0m

                       Computation: 3127 steps/s (collection: 0.482s, learning 2.137s)
               Value function loss: 112330.8932
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 9213.19
               Mean episode length: 398.34
                 Mean success rate: 75.50
                  Mean reward/step: 23.93
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 2.62s
                        Total time: 6215.55s
                               ETA: 4086.2s

################################################################################
                     [1m Learning iteration 2414/4000 [0m

                       Computation: 3216 steps/s (collection: 0.479s, learning 2.068s)
               Value function loss: 96524.8172
                    Surrogate loss: 0.0094
             Mean action noise std: 0.91
                       Mean reward: 9346.31
               Mean episode length: 401.12
                 Mean success rate: 76.00
                  Mean reward/step: 23.48
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19783680
                    Iteration time: 2.55s
                        Total time: 6218.09s
                               ETA: 4083.6s

################################################################################
                     [1m Learning iteration 2415/4000 [0m

                       Computation: 3143 steps/s (collection: 0.477s, learning 2.129s)
               Value function loss: 101629.9271
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 9370.51
               Mean episode length: 395.85
                 Mean success rate: 75.00
                  Mean reward/step: 23.24
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 2.61s
                        Total time: 6220.70s
                               ETA: 4081.0s

################################################################################
                     [1m Learning iteration 2416/4000 [0m

                       Computation: 3085 steps/s (collection: 0.528s, learning 2.127s)
               Value function loss: 96233.3975
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 9639.42
               Mean episode length: 400.69
                 Mean success rate: 77.00
                  Mean reward/step: 23.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19800064
                    Iteration time: 2.65s
                        Total time: 6223.36s
                               ETA: 4078.5s

################################################################################
                     [1m Learning iteration 2417/4000 [0m

                       Computation: 3145 steps/s (collection: 0.529s, learning 2.076s)
               Value function loss: 105774.7722
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 9491.80
               Mean episode length: 395.82
                 Mean success rate: 75.50
                  Mean reward/step: 23.52
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 2.60s
                        Total time: 6225.96s
                               ETA: 4076.0s

################################################################################
                     [1m Learning iteration 2418/4000 [0m

                       Computation: 3204 steps/s (collection: 0.450s, learning 2.107s)
               Value function loss: 61532.3843
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 9072.72
               Mean episode length: 383.55
                 Mean success rate: 72.50
                  Mean reward/step: 24.17
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 19816448
                    Iteration time: 2.56s
                        Total time: 6228.52s
                               ETA: 4073.4s

################################################################################
                     [1m Learning iteration 2419/4000 [0m

                       Computation: 3147 steps/s (collection: 0.493s, learning 2.110s)
               Value function loss: 133154.1705
                    Surrogate loss: 0.0105
             Mean action noise std: 0.91
                       Mean reward: 9316.18
               Mean episode length: 390.57
                 Mean success rate: 75.00
                  Mean reward/step: 24.13
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 2.60s
                        Total time: 6231.12s
                               ETA: 4070.8s

################################################################################
                     [1m Learning iteration 2420/4000 [0m

                       Computation: 3063 steps/s (collection: 0.522s, learning 2.153s)
               Value function loss: 84995.7631
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 9626.34
               Mean episode length: 397.98
                 Mean success rate: 76.50
                  Mean reward/step: 23.48
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19832832
                    Iteration time: 2.67s
                        Total time: 6233.79s
                               ETA: 4068.3s

################################################################################
                     [1m Learning iteration 2421/4000 [0m

                       Computation: 3114 steps/s (collection: 0.508s, learning 2.123s)
               Value function loss: 70962.5959
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 9780.45
               Mean episode length: 401.52
                 Mean success rate: 77.00
                  Mean reward/step: 23.61
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 2.63s
                        Total time: 6236.42s
                               ETA: 4065.8s

################################################################################
                     [1m Learning iteration 2422/4000 [0m

                       Computation: 3154 steps/s (collection: 0.482s, learning 2.114s)
               Value function loss: 127336.4320
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 9679.42
               Mean episode length: 398.06
                 Mean success rate: 75.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19849216
                    Iteration time: 2.60s
                        Total time: 6239.02s
                               ETA: 4063.2s

################################################################################
                     [1m Learning iteration 2423/4000 [0m

                       Computation: 3176 steps/s (collection: 0.503s, learning 2.076s)
               Value function loss: 65222.8224
                    Surrogate loss: 0.0107
             Mean action noise std: 0.91
                       Mean reward: 9226.04
               Mean episode length: 382.57
                 Mean success rate: 72.50
                  Mean reward/step: 24.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 2.58s
                        Total time: 6241.60s
                               ETA: 4060.6s

################################################################################
                     [1m Learning iteration 2424/4000 [0m

                       Computation: 3085 steps/s (collection: 0.501s, learning 2.154s)
               Value function loss: 90715.7522
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 9141.43
               Mean episode length: 377.87
                 Mean success rate: 71.50
                  Mean reward/step: 24.79
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19865600
                    Iteration time: 2.66s
                        Total time: 6244.25s
                               ETA: 4058.1s

################################################################################
                     [1m Learning iteration 2425/4000 [0m

                       Computation: 3057 steps/s (collection: 0.545s, learning 2.134s)
               Value function loss: 130950.5283
                    Surrogate loss: 0.0087
             Mean action noise std: 0.91
                       Mean reward: 9160.09
               Mean episode length: 383.70
                 Mean success rate: 73.00
                  Mean reward/step: 24.05
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 2.68s
                        Total time: 6246.93s
                               ETA: 4055.6s

################################################################################
                     [1m Learning iteration 2426/4000 [0m

                       Computation: 3243 steps/s (collection: 0.448s, learning 2.078s)
               Value function loss: 65109.9220
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 8999.19
               Mean episode length: 380.14
                 Mean success rate: 72.00
                  Mean reward/step: 23.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 19881984
                    Iteration time: 2.53s
                        Total time: 6249.46s
                               ETA: 4053.0s

################################################################################
                     [1m Learning iteration 2427/4000 [0m

                       Computation: 3167 steps/s (collection: 0.491s, learning 2.096s)
               Value function loss: 100760.2053
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 9114.02
               Mean episode length: 381.85
                 Mean success rate: 71.50
                  Mean reward/step: 24.36
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 2.59s
                        Total time: 6252.05s
                               ETA: 4050.4s

################################################################################
                     [1m Learning iteration 2428/4000 [0m

                       Computation: 3068 steps/s (collection: 0.515s, learning 2.155s)
               Value function loss: 70811.2486
                    Surrogate loss: 0.0107
             Mean action noise std: 0.91
                       Mean reward: 9215.27
               Mean episode length: 385.25
                 Mean success rate: 72.50
                  Mean reward/step: 24.24
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19898368
                    Iteration time: 2.67s
                        Total time: 6254.72s
                               ETA: 4047.9s

################################################################################
                     [1m Learning iteration 2429/4000 [0m

                       Computation: 3093 steps/s (collection: 0.523s, learning 2.125s)
               Value function loss: 130746.9521
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 9277.00
               Mean episode length: 388.88
                 Mean success rate: 73.50
                  Mean reward/step: 23.89
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 2.65s
                        Total time: 6257.36s
                               ETA: 4045.4s

################################################################################
                     [1m Learning iteration 2430/4000 [0m

                       Computation: 3105 steps/s (collection: 0.500s, learning 2.137s)
               Value function loss: 89560.3465
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 9103.38
               Mean episode length: 382.91
                 Mean success rate: 73.00
                  Mean reward/step: 23.52
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19914752
                    Iteration time: 2.64s
                        Total time: 6260.00s
                               ETA: 4042.9s

################################################################################
                     [1m Learning iteration 2431/4000 [0m

                       Computation: 3233 steps/s (collection: 0.464s, learning 2.069s)
               Value function loss: 82844.2806
                    Surrogate loss: 0.0098
             Mean action noise std: 0.91
                       Mean reward: 9110.98
               Mean episode length: 383.23
                 Mean success rate: 73.00
                  Mean reward/step: 23.50
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 2.53s
                        Total time: 6262.54s
                               ETA: 4040.3s

################################################################################
                     [1m Learning iteration 2432/4000 [0m

                       Computation: 3160 steps/s (collection: 0.499s, learning 2.093s)
               Value function loss: 100515.6330
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 9248.22
               Mean episode length: 387.13
                 Mean success rate: 74.00
                  Mean reward/step: 24.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19931136
                    Iteration time: 2.59s
                        Total time: 6265.13s
                               ETA: 4037.7s

################################################################################
                     [1m Learning iteration 2433/4000 [0m

                       Computation: 3049 steps/s (collection: 0.589s, learning 2.098s)
               Value function loss: 104400.1747
                    Surrogate loss: 0.0107
             Mean action noise std: 0.91
                       Mean reward: 9524.44
               Mean episode length: 398.85
                 Mean success rate: 76.50
                  Mean reward/step: 24.92
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 2.69s
                        Total time: 6267.81s
                               ETA: 4035.2s

################################################################################
                     [1m Learning iteration 2434/4000 [0m

                       Computation: 3146 steps/s (collection: 0.504s, learning 2.099s)
               Value function loss: 97577.7045
                    Surrogate loss: 0.0118
             Mean action noise std: 0.91
                       Mean reward: 9381.70
               Mean episode length: 392.12
                 Mean success rate: 75.00
                  Mean reward/step: 24.83
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19947520
                    Iteration time: 2.60s
                        Total time: 6270.42s
                               ETA: 4032.6s

################################################################################
                     [1m Learning iteration 2435/4000 [0m

                       Computation: 3140 steps/s (collection: 0.484s, learning 2.125s)
               Value function loss: 100634.0392
                    Surrogate loss: 0.0112
             Mean action noise std: 0.91
                       Mean reward: 9504.79
               Mean episode length: 394.84
                 Mean success rate: 76.00
                  Mean reward/step: 24.32
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 2.61s
                        Total time: 6273.03s
                               ETA: 4030.1s

################################################################################
                     [1m Learning iteration 2436/4000 [0m

                       Computation: 3142 steps/s (collection: 0.473s, learning 2.134s)
               Value function loss: 101579.4955
                    Surrogate loss: 0.0111
             Mean action noise std: 0.91
                       Mean reward: 9707.27
               Mean episode length: 399.91
                 Mean success rate: 78.00
                  Mean reward/step: 23.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 19963904
                    Iteration time: 2.61s
                        Total time: 6275.63s
                               ETA: 4027.5s

################################################################################
                     [1m Learning iteration 2437/4000 [0m

                       Computation: 3149 steps/s (collection: 0.469s, learning 2.132s)
               Value function loss: 54313.4257
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 9632.18
               Mean episode length: 397.51
                 Mean success rate: 77.50
                  Mean reward/step: 23.64
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 2.60s
                        Total time: 6278.23s
                               ETA: 4025.0s

################################################################################
                     [1m Learning iteration 2438/4000 [0m

                       Computation: 3074 steps/s (collection: 0.544s, learning 2.121s)
               Value function loss: 129440.7070
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 9797.73
               Mean episode length: 402.06
                 Mean success rate: 78.50
                  Mean reward/step: 23.86
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 19980288
                    Iteration time: 2.66s
                        Total time: 6280.90s
                               ETA: 4022.5s

################################################################################
                     [1m Learning iteration 2439/4000 [0m

                       Computation: 3151 steps/s (collection: 0.496s, learning 2.103s)
               Value function loss: 49064.3183
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 9720.51
               Mean episode length: 400.57
                 Mean success rate: 78.00
                  Mean reward/step: 23.96
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 2.60s
                        Total time: 6283.50s
                               ETA: 4019.9s

################################################################################
                     [1m Learning iteration 2440/4000 [0m

                       Computation: 3228 steps/s (collection: 0.459s, learning 2.079s)
               Value function loss: 109067.2362
                    Surrogate loss: 0.0156
             Mean action noise std: 0.91
                       Mean reward: 9965.66
               Mean episode length: 406.75
                 Mean success rate: 79.50
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19996672
                    Iteration time: 2.54s
                        Total time: 6286.04s
                               ETA: 4017.3s

################################################################################
                     [1m Learning iteration 2441/4000 [0m

                       Computation: 3096 steps/s (collection: 0.538s, learning 2.108s)
               Value function loss: 105602.1058
                    Surrogate loss: 0.0127
             Mean action noise std: 0.90
                       Mean reward: 10299.70
               Mean episode length: 419.44
                 Mean success rate: 82.00
                  Mean reward/step: 24.31
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 2.65s
                        Total time: 6288.68s
                               ETA: 4014.8s

################################################################################
                     [1m Learning iteration 2442/4000 [0m

                       Computation: 3161 steps/s (collection: 0.484s, learning 2.107s)
               Value function loss: 86582.8032
                    Surrogate loss: 0.0122
             Mean action noise std: 0.90
                       Mean reward: 10306.65
               Mean episode length: 419.75
                 Mean success rate: 82.00
                  Mean reward/step: 25.28
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 20013056
                    Iteration time: 2.59s
                        Total time: 6291.27s
                               ETA: 4012.2s

################################################################################
                     [1m Learning iteration 2443/4000 [0m

                       Computation: 3179 steps/s (collection: 0.483s, learning 2.094s)
               Value function loss: 125491.3017
                    Surrogate loss: 0.0134
             Mean action noise std: 0.90
                       Mean reward: 10394.68
               Mean episode length: 425.56
                 Mean success rate: 83.50
                  Mean reward/step: 24.57
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 2.58s
                        Total time: 6293.85s
                               ETA: 4009.6s

################################################################################
                     [1m Learning iteration 2444/4000 [0m

                       Computation: 3191 steps/s (collection: 0.474s, learning 2.093s)
               Value function loss: 101129.0628
                    Surrogate loss: 0.0144
             Mean action noise std: 0.90
                       Mean reward: 10173.13
               Mean episode length: 420.36
                 Mean success rate: 82.00
                  Mean reward/step: 23.32
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20029440
                    Iteration time: 2.57s
                        Total time: 6296.42s
                               ETA: 4007.0s

################################################################################
                     [1m Learning iteration 2445/4000 [0m

                       Computation: 3102 steps/s (collection: 0.501s, learning 2.139s)
               Value function loss: 114015.8958
                    Surrogate loss: 0.0134
             Mean action noise std: 0.90
                       Mean reward: 10063.11
               Mean episode length: 415.18
                 Mean success rate: 81.00
                  Mean reward/step: 23.17
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 2.64s
                        Total time: 6299.06s
                               ETA: 4004.5s

################################################################################
                     [1m Learning iteration 2446/4000 [0m

                       Computation: 3154 steps/s (collection: 0.514s, learning 2.083s)
               Value function loss: 104757.3396
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 10101.80
               Mean episode length: 419.65
                 Mean success rate: 81.50
                  Mean reward/step: 22.89
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 20045824
                    Iteration time: 2.60s
                        Total time: 6301.65s
                               ETA: 4001.9s

################################################################################
                     [1m Learning iteration 2447/4000 [0m

                       Computation: 3170 steps/s (collection: 0.481s, learning 2.102s)
               Value function loss: 54502.5431
                    Surrogate loss: 0.0119
             Mean action noise std: 0.90
                       Mean reward: 9953.08
               Mean episode length: 414.49
                 Mean success rate: 80.50
                  Mean reward/step: 23.28
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 2.58s
                        Total time: 6304.24s
                               ETA: 3999.4s

################################################################################
                     [1m Learning iteration 2448/4000 [0m

                       Computation: 3201 steps/s (collection: 0.468s, learning 2.091s)
               Value function loss: 109214.2759
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 9599.98
               Mean episode length: 403.54
                 Mean success rate: 77.50
                  Mean reward/step: 24.07
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 20062208
                    Iteration time: 2.56s
                        Total time: 6306.80s
                               ETA: 3996.8s

################################################################################
                     [1m Learning iteration 2449/4000 [0m

                       Computation: 3230 steps/s (collection: 0.476s, learning 2.060s)
               Value function loss: 80766.8764
                    Surrogate loss: 0.0097
             Mean action noise std: 0.91
                       Mean reward: 9492.90
               Mean episode length: 401.64
                 Mean success rate: 77.00
                  Mean reward/step: 24.13
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 2.54s
                        Total time: 6309.33s
                               ETA: 3994.2s

################################################################################
                     [1m Learning iteration 2450/4000 [0m

                       Computation: 3145 steps/s (collection: 0.515s, learning 2.090s)
               Value function loss: 121335.6452
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 9029.89
               Mean episode length: 390.05
                 Mean success rate: 73.00
                  Mean reward/step: 24.24
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 20078592
                    Iteration time: 2.60s
                        Total time: 6311.94s
                               ETA: 3991.6s

################################################################################
                     [1m Learning iteration 2451/4000 [0m

                       Computation: 3200 steps/s (collection: 0.486s, learning 2.074s)
               Value function loss: 99819.0822
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 9166.12
               Mean episode length: 394.78
                 Mean success rate: 74.00
                  Mean reward/step: 24.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 2.56s
                        Total time: 6314.50s
                               ETA: 3989.1s

################################################################################
                     [1m Learning iteration 2452/4000 [0m

                       Computation: 3056 steps/s (collection: 0.547s, learning 2.133s)
               Value function loss: 63071.9032
                    Surrogate loss: 0.0086
             Mean action noise std: 0.90
                       Mean reward: 8839.59
               Mean episode length: 381.60
                 Mean success rate: 71.50
                  Mean reward/step: 24.46
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20094976
                    Iteration time: 2.68s
                        Total time: 6317.18s
                               ETA: 3986.5s

################################################################################
                     [1m Learning iteration 2453/4000 [0m

                       Computation: 3107 steps/s (collection: 0.558s, learning 2.078s)
               Value function loss: 106912.5619
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 9408.01
               Mean episode length: 395.35
                 Mean success rate: 74.50
                  Mean reward/step: 25.64
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 2.64s
                        Total time: 6319.81s
                               ETA: 3984.0s

################################################################################
                     [1m Learning iteration 2454/4000 [0m

                       Computation: 3133 steps/s (collection: 0.496s, learning 2.119s)
               Value function loss: 99758.4543
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 9329.17
               Mean episode length: 394.82
                 Mean success rate: 74.50
                  Mean reward/step: 24.96
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20111360
                    Iteration time: 2.61s
                        Total time: 6322.43s
                               ETA: 3981.5s

################################################################################
                     [1m Learning iteration 2455/4000 [0m

                       Computation: 3107 steps/s (collection: 0.503s, learning 2.133s)
               Value function loss: 81168.4060
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 9275.68
               Mean episode length: 392.29
                 Mean success rate: 74.00
                  Mean reward/step: 25.07
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 2.64s
                        Total time: 6325.06s
                               ETA: 3978.9s

################################################################################
                     [1m Learning iteration 2456/4000 [0m

                       Computation: 3163 steps/s (collection: 0.516s, learning 2.074s)
               Value function loss: 87055.6748
                    Surrogate loss: 0.0085
             Mean action noise std: 0.91
                       Mean reward: 9207.88
               Mean episode length: 387.52
                 Mean success rate: 73.00
                  Mean reward/step: 24.59
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20127744
                    Iteration time: 2.59s
                        Total time: 6327.65s
                               ETA: 3976.4s

################################################################################
                     [1m Learning iteration 2457/4000 [0m

                       Computation: 3214 steps/s (collection: 0.489s, learning 2.060s)
               Value function loss: 57328.4414
                    Surrogate loss: 0.0115
             Mean action noise std: 0.91
                       Mean reward: 9429.15
               Mean episode length: 394.25
                 Mean success rate: 74.50
                  Mean reward/step: 24.74
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 2.55s
                        Total time: 6330.20s
                               ETA: 3973.8s

################################################################################
                     [1m Learning iteration 2458/4000 [0m

                       Computation: 3169 steps/s (collection: 0.502s, learning 2.083s)
               Value function loss: 121215.3678
                    Surrogate loss: 0.0101
             Mean action noise std: 0.91
                       Mean reward: 9602.96
               Mean episode length: 400.21
                 Mean success rate: 75.50
                  Mean reward/step: 24.97
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20144128
                    Iteration time: 2.58s
                        Total time: 6332.79s
                               ETA: 3971.2s

################################################################################
                     [1m Learning iteration 2459/4000 [0m

                       Computation: 3192 steps/s (collection: 0.490s, learning 2.076s)
               Value function loss: 86028.4683
                    Surrogate loss: 0.0102
             Mean action noise std: 0.91
                       Mean reward: 9910.42
               Mean episode length: 405.94
                 Mean success rate: 77.00
                  Mean reward/step: 25.00
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 2.57s
                        Total time: 6335.35s
                               ETA: 3968.6s

################################################################################
                     [1m Learning iteration 2460/4000 [0m

                       Computation: 3176 steps/s (collection: 0.508s, learning 2.071s)
               Value function loss: 132828.2971
                    Surrogate loss: 0.0108
             Mean action noise std: 0.91
                       Mean reward: 10155.20
               Mean episode length: 408.86
                 Mean success rate: 78.00
                  Mean reward/step: 24.75
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 20160512
                    Iteration time: 2.58s
                        Total time: 6337.93s
                               ETA: 3966.0s

################################################################################
                     [1m Learning iteration 2461/4000 [0m

                       Computation: 3189 steps/s (collection: 0.481s, learning 2.088s)
               Value function loss: 84778.3946
                    Surrogate loss: 0.0110
             Mean action noise std: 0.91
                       Mean reward: 9935.68
               Mean episode length: 404.76
                 Mean success rate: 77.00
                  Mean reward/step: 23.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 2.57s
                        Total time: 6340.50s
                               ETA: 3963.5s

################################################################################
                     [1m Learning iteration 2462/4000 [0m

                       Computation: 3219 steps/s (collection: 0.500s, learning 2.045s)
               Value function loss: 102562.4042
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 10018.19
               Mean episode length: 407.98
                 Mean success rate: 77.50
                  Mean reward/step: 23.08
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20176896
                    Iteration time: 2.54s
                        Total time: 6343.04s
                               ETA: 3960.9s

################################################################################
                     [1m Learning iteration 2463/4000 [0m

                       Computation: 3119 steps/s (collection: 0.538s, learning 2.087s)
               Value function loss: 103424.5971
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 10501.80
               Mean episode length: 425.01
                 Mean success rate: 81.50
                  Mean reward/step: 24.02
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 2.63s
                        Total time: 6345.67s
                               ETA: 3958.3s

################################################################################
                     [1m Learning iteration 2464/4000 [0m

                       Computation: 3212 steps/s (collection: 0.488s, learning 2.062s)
               Value function loss: 131514.4029
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 10418.11
               Mean episode length: 420.99
                 Mean success rate: 80.50
                  Mean reward/step: 23.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20193280
                    Iteration time: 2.55s
                        Total time: 6348.22s
                               ETA: 3955.7s

################################################################################
                     [1m Learning iteration 2465/4000 [0m

                       Computation: 3192 steps/s (collection: 0.493s, learning 2.073s)
               Value function loss: 74027.4355
                    Surrogate loss: 0.0114
             Mean action noise std: 0.91
                       Mean reward: 10110.83
               Mean episode length: 414.32
                 Mean success rate: 78.50
                  Mean reward/step: 23.53
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 2.57s
                        Total time: 6350.78s
                               ETA: 3953.1s

################################################################################
                     [1m Learning iteration 2466/4000 [0m

                       Computation: 3237 steps/s (collection: 0.472s, learning 2.059s)
               Value function loss: 127493.3606
                    Surrogate loss: 0.0104
             Mean action noise std: 0.91
                       Mean reward: 10115.84
               Mean episode length: 412.42
                 Mean success rate: 78.50
                  Mean reward/step: 23.14
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 20209664
                    Iteration time: 2.53s
                        Total time: 6353.32s
                               ETA: 3950.5s

################################################################################
                     [1m Learning iteration 2467/4000 [0m

                       Computation: 3156 steps/s (collection: 0.493s, learning 2.103s)
               Value function loss: 100088.8502
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 9929.71
               Mean episode length: 408.52
                 Mean success rate: 77.50
                  Mean reward/step: 22.93
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 2.60s
                        Total time: 6355.91s
                               ETA: 3948.0s

################################################################################
                     [1m Learning iteration 2468/4000 [0m

                       Computation: 3163 steps/s (collection: 0.499s, learning 2.091s)
               Value function loss: 77297.2936
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 9773.10
               Mean episode length: 402.92
                 Mean success rate: 77.00
                  Mean reward/step: 23.48
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20226048
                    Iteration time: 2.59s
                        Total time: 6358.50s
                               ETA: 3945.4s

################################################################################
                     [1m Learning iteration 2469/4000 [0m

                       Computation: 3201 steps/s (collection: 0.451s, learning 2.108s)
               Value function loss: 109453.9036
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 9713.92
               Mean episode length: 406.30
                 Mean success rate: 77.50
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 2.56s
                        Total time: 6361.06s
                               ETA: 3942.8s

################################################################################
                     [1m Learning iteration 2470/4000 [0m

                       Computation: 3165 steps/s (collection: 0.491s, learning 2.097s)
               Value function loss: 74439.7197
                    Surrogate loss: 0.0116
             Mean action noise std: 0.91
                       Mean reward: 9554.03
               Mean episode length: 397.50
                 Mean success rate: 76.00
                  Mean reward/step: 23.98
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 20242432
                    Iteration time: 2.59s
                        Total time: 6363.65s
                               ETA: 3940.3s

################################################################################
                     [1m Learning iteration 2471/4000 [0m

                       Computation: 3231 steps/s (collection: 0.457s, learning 2.078s)
               Value function loss: 100569.2983
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 9385.62
               Mean episode length: 388.02
                 Mean success rate: 74.50
                  Mean reward/step: 24.06
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 2.54s
                        Total time: 6366.18s
                               ETA: 3937.7s

################################################################################
                     [1m Learning iteration 2472/4000 [0m

                       Computation: 3173 steps/s (collection: 0.507s, learning 2.075s)
               Value function loss: 88035.4790
                    Surrogate loss: 0.0098
             Mean action noise std: 0.91
                       Mean reward: 9142.73
               Mean episode length: 376.95
                 Mean success rate: 72.00
                  Mean reward/step: 23.59
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20258816
                    Iteration time: 2.58s
                        Total time: 6368.76s
                               ETA: 3935.1s

################################################################################
                     [1m Learning iteration 2473/4000 [0m

                       Computation: 3222 steps/s (collection: 0.458s, learning 2.084s)
               Value function loss: 47218.3150
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 8708.85
               Mean episode length: 365.59
                 Mean success rate: 70.00
                  Mean reward/step: 24.53
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 2.54s
                        Total time: 6371.31s
                               ETA: 3932.5s

################################################################################
                     [1m Learning iteration 2474/4000 [0m

                       Computation: 3178 steps/s (collection: 0.504s, learning 2.074s)
               Value function loss: 107459.4432
                    Surrogate loss: 0.0121
             Mean action noise std: 0.91
                       Mean reward: 8732.74
               Mean episode length: 366.00
                 Mean success rate: 70.00
                  Mean reward/step: 24.87
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 20275200
                    Iteration time: 2.58s
                        Total time: 6373.88s
                               ETA: 3929.9s

################################################################################
                     [1m Learning iteration 2475/4000 [0m

                       Computation: 3202 steps/s (collection: 0.471s, learning 2.086s)
               Value function loss: 101271.8908
                    Surrogate loss: 0.0122
             Mean action noise std: 0.91
                       Mean reward: 8630.43
               Mean episode length: 365.67
                 Mean success rate: 70.00
                  Mean reward/step: 24.05
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 2.56s
                        Total time: 6376.44s
                               ETA: 3927.3s

################################################################################
                     [1m Learning iteration 2476/4000 [0m

                       Computation: 3224 steps/s (collection: 0.458s, learning 2.082s)
               Value function loss: 131637.2203
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 8721.70
               Mean episode length: 368.52
                 Mean success rate: 71.50
                  Mean reward/step: 23.20
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 20291584
                    Iteration time: 2.54s
                        Total time: 6378.98s
                               ETA: 3924.7s

################################################################################
                     [1m Learning iteration 2477/4000 [0m

                       Computation: 3128 steps/s (collection: 0.476s, learning 2.143s)
               Value function loss: 95912.7469
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 8957.78
               Mean episode length: 376.96
                 Mean success rate: 73.50
                  Mean reward/step: 22.81
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 2.62s
                        Total time: 6381.60s
                               ETA: 3922.2s

################################################################################
                     [1m Learning iteration 2478/4000 [0m

                       Computation: 3176 steps/s (collection: 0.476s, learning 2.103s)
               Value function loss: 59844.4291
                    Surrogate loss: 0.0105
             Mean action noise std: 0.91
                       Mean reward: 8790.49
               Mean episode length: 372.88
                 Mean success rate: 72.00
                  Mean reward/step: 23.20
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 20307968
                    Iteration time: 2.58s
                        Total time: 6384.18s
                               ETA: 3919.6s

################################################################################
                     [1m Learning iteration 2479/4000 [0m

                       Computation: 3177 steps/s (collection: 0.480s, learning 2.098s)
               Value function loss: 103537.3581
                    Surrogate loss: 0.0109
             Mean action noise std: 0.91
                       Mean reward: 8641.56
               Mean episode length: 369.72
                 Mean success rate: 71.50
                  Mean reward/step: 24.06
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 2.58s
                        Total time: 6386.76s
                               ETA: 3917.0s

################################################################################
                     [1m Learning iteration 2480/4000 [0m

                       Computation: 3215 steps/s (collection: 0.488s, learning 2.059s)
               Value function loss: 91678.5244
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 8680.64
               Mean episode length: 372.25
                 Mean success rate: 71.50
                  Mean reward/step: 23.67
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20324352
                    Iteration time: 2.55s
                        Total time: 6389.31s
                               ETA: 3914.4s

################################################################################
                     [1m Learning iteration 2481/4000 [0m

                       Computation: 3217 steps/s (collection: 0.456s, learning 2.090s)
               Value function loss: 115601.1400
                    Surrogate loss: 0.0103
             Mean action noise std: 0.91
                       Mean reward: 8698.79
               Mean episode length: 372.25
                 Mean success rate: 72.00
                  Mean reward/step: 23.73
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 2.55s
                        Total time: 6391.85s
                               ETA: 3911.9s

################################################################################
                     [1m Learning iteration 2482/4000 [0m

                       Computation: 3154 steps/s (collection: 0.498s, learning 2.098s)
               Value function loss: 65851.5663
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 8780.30
               Mean episode length: 375.50
                 Mean success rate: 71.50
                  Mean reward/step: 23.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20340736
                    Iteration time: 2.60s
                        Total time: 6394.45s
                               ETA: 3909.3s

################################################################################
                     [1m Learning iteration 2483/4000 [0m

                       Computation: 3148 steps/s (collection: 0.476s, learning 2.125s)
               Value function loss: 92218.8335
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 8959.84
               Mean episode length: 378.68
                 Mean success rate: 72.50
                  Mean reward/step: 23.74
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 2.60s
                        Total time: 6397.05s
                               ETA: 3906.7s

################################################################################
                     [1m Learning iteration 2484/4000 [0m

                       Computation: 3205 steps/s (collection: 0.475s, learning 2.081s)
               Value function loss: 89137.9677
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 9130.46
               Mean episode length: 380.60
                 Mean success rate: 72.50
                  Mean reward/step: 24.50
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 20357120
                    Iteration time: 2.56s
                        Total time: 6399.61s
                               ETA: 3904.1s

################################################################################
                     [1m Learning iteration 2485/4000 [0m

                       Computation: 3175 steps/s (collection: 0.502s, learning 2.078s)
               Value function loss: 109238.8808
                    Surrogate loss: 0.0111
             Mean action noise std: 0.91
                       Mean reward: 8557.11
               Mean episode length: 362.63
                 Mean success rate: 68.00
                  Mean reward/step: 24.08
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 2.58s
                        Total time: 6402.19s
                               ETA: 3901.6s

################################################################################
                     [1m Learning iteration 2486/4000 [0m

                       Computation: 3188 steps/s (collection: 0.492s, learning 2.077s)
               Value function loss: 73576.0392
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 8830.11
               Mean episode length: 368.87
                 Mean success rate: 69.00
                  Mean reward/step: 23.23
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 20373504
                    Iteration time: 2.57s
                        Total time: 6404.76s
                               ETA: 3899.0s

################################################################################
                     [1m Learning iteration 2487/4000 [0m

                       Computation: 3163 steps/s (collection: 0.534s, learning 2.055s)
               Value function loss: 89638.5228
                    Surrogate loss: 0.0107
             Mean action noise std: 0.91
                       Mean reward: 8858.99
               Mean episode length: 366.36
                 Mean success rate: 69.00
                  Mean reward/step: 23.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 2.59s
                        Total time: 6407.35s
                               ETA: 3896.4s

################################################################################
                     [1m Learning iteration 2488/4000 [0m

                       Computation: 3280 steps/s (collection: 0.464s, learning 2.034s)
               Value function loss: 92949.6596
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 9037.84
               Mean episode length: 374.47
                 Mean success rate: 70.50
                  Mean reward/step: 23.63
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20389888
                    Iteration time: 2.50s
                        Total time: 6409.84s
                               ETA: 3893.8s

################################################################################
                     [1m Learning iteration 2489/4000 [0m

                       Computation: 3186 steps/s (collection: 0.482s, learning 2.088s)
               Value function loss: 83633.8913
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 8628.41
               Mean episode length: 359.82
                 Mean success rate: 67.50
                  Mean reward/step: 23.90
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 2.57s
                        Total time: 6412.41s
                               ETA: 3891.2s

################################################################################
                     [1m Learning iteration 2490/4000 [0m

                       Computation: 3206 steps/s (collection: 0.483s, learning 2.072s)
               Value function loss: 97878.9898
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 9031.04
               Mean episode length: 371.14
                 Mean success rate: 70.00
                  Mean reward/step: 23.48
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20406272
                    Iteration time: 2.55s
                        Total time: 6414.97s
                               ETA: 3888.6s

################################################################################
                     [1m Learning iteration 2491/4000 [0m

                       Computation: 3256 steps/s (collection: 0.466s, learning 2.050s)
               Value function loss: 134372.7551
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 8948.94
               Mean episode length: 371.18
                 Mean success rate: 71.00
                  Mean reward/step: 23.42
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 2.52s
                        Total time: 6417.48s
                               ETA: 3886.0s

################################################################################
                     [1m Learning iteration 2492/4000 [0m

                       Computation: 3251 steps/s (collection: 0.467s, learning 2.052s)
               Value function loss: 77983.3814
                    Surrogate loss: 0.0103
             Mean action noise std: 0.91
                       Mean reward: 8642.28
               Mean episode length: 366.56
                 Mean success rate: 69.50
                  Mean reward/step: 22.46
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20422656
                    Iteration time: 2.52s
                        Total time: 6420.00s
                               ETA: 3883.4s

################################################################################
                     [1m Learning iteration 2493/4000 [0m

                       Computation: 3260 steps/s (collection: 0.442s, learning 2.070s)
               Value function loss: 87948.8989
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 8737.66
               Mean episode length: 369.34
                 Mean success rate: 70.50
                  Mean reward/step: 22.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 2.51s
                        Total time: 6422.51s
                               ETA: 3880.8s

################################################################################
                     [1m Learning iteration 2494/4000 [0m

                       Computation: 3273 steps/s (collection: 0.460s, learning 2.043s)
               Value function loss: 81716.1830
                    Surrogate loss: 0.0097
             Mean action noise std: 0.91
                       Mean reward: 8649.09
               Mean episode length: 367.10
                 Mean success rate: 70.00
                  Mean reward/step: 23.39
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 20439040
                    Iteration time: 2.50s
                        Total time: 6425.02s
                               ETA: 3878.2s

################################################################################
                     [1m Learning iteration 2495/4000 [0m

                       Computation: 3248 steps/s (collection: 0.449s, learning 2.072s)
               Value function loss: 120824.9867
                    Surrogate loss: 0.0088
             Mean action noise std: 0.91
                       Mean reward: 9000.76
               Mean episode length: 381.18
                 Mean success rate: 73.00
                  Mean reward/step: 23.30
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 2.52s
                        Total time: 6427.54s
                               ETA: 3875.6s

################################################################################
                     [1m Learning iteration 2496/4000 [0m

                       Computation: 3313 steps/s (collection: 0.438s, learning 2.034s)
               Value function loss: 98234.3218
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 8609.67
               Mean episode length: 368.28
                 Mean success rate: 70.00
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 20455424
                    Iteration time: 2.47s
                        Total time: 6430.01s
                               ETA: 3872.9s

################################################################################
                     [1m Learning iteration 2497/4000 [0m

                       Computation: 3221 steps/s (collection: 0.457s, learning 2.086s)
               Value function loss: 100229.1786
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 8855.24
               Mean episode length: 375.61
                 Mean success rate: 72.00
                  Mean reward/step: 22.32
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 2.54s
                        Total time: 6432.55s
                               ETA: 3870.3s

################################################################################
                     [1m Learning iteration 2498/4000 [0m

                       Computation: 3169 steps/s (collection: 0.479s, learning 2.106s)
               Value function loss: 86999.7418
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 8734.44
               Mean episode length: 372.20
                 Mean success rate: 70.50
                  Mean reward/step: 22.93
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20471808
                    Iteration time: 2.58s
                        Total time: 6435.14s
                               ETA: 3867.8s

################################################################################
                     [1m Learning iteration 2499/4000 [0m

                       Computation: 3216 steps/s (collection: 0.483s, learning 2.063s)
               Value function loss: 60522.1581
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 8815.42
               Mean episode length: 376.21
                 Mean success rate: 71.50
                  Mean reward/step: 23.89
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 2.55s
                        Total time: 6437.69s
                               ETA: 3865.2s

################################################################################
                     [1m Learning iteration 2500/4000 [0m

                       Computation: 3259 steps/s (collection: 0.448s, learning 2.066s)
               Value function loss: 84608.4464
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 8557.22
               Mean episode length: 370.56
                 Mean success rate: 70.00
                  Mean reward/step: 24.52
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 20488192
                    Iteration time: 2.51s
                        Total time: 6440.20s
                               ETA: 3862.6s

################################################################################
                     [1m Learning iteration 2501/4000 [0m

                       Computation: 3239 steps/s (collection: 0.473s, learning 2.055s)
               Value function loss: 106702.0210
                    Surrogate loss: 0.0094
             Mean action noise std: 0.90
                       Mean reward: 8867.71
               Mean episode length: 375.42
                 Mean success rate: 71.50
                  Mean reward/step: 24.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 2.53s
                        Total time: 6442.73s
                               ETA: 3860.0s

################################################################################
                     [1m Learning iteration 2502/4000 [0m

                       Computation: 3282 steps/s (collection: 0.452s, learning 2.044s)
               Value function loss: 80063.3197
                    Surrogate loss: 0.0100
             Mean action noise std: 0.90
                       Mean reward: 8826.37
               Mean episode length: 372.98
                 Mean success rate: 71.00
                  Mean reward/step: 24.52
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20504576
                    Iteration time: 2.50s
                        Total time: 6445.22s
                               ETA: 3857.3s

################################################################################
                     [1m Learning iteration 2503/4000 [0m

                       Computation: 3295 steps/s (collection: 0.442s, learning 2.044s)
               Value function loss: 84303.2191
                    Surrogate loss: 0.0129
             Mean action noise std: 0.90
                       Mean reward: 9096.12
               Mean episode length: 382.60
                 Mean success rate: 73.00
                  Mean reward/step: 25.12
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 2.49s
                        Total time: 6447.71s
                               ETA: 3854.7s

################################################################################
                     [1m Learning iteration 2504/4000 [0m

                       Computation: 3274 steps/s (collection: 0.455s, learning 2.047s)
               Value function loss: 75645.9109
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 9028.39
               Mean episode length: 382.35
                 Mean success rate: 72.50
                  Mean reward/step: 24.97
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 20520960
                    Iteration time: 2.50s
                        Total time: 6450.21s
                               ETA: 3852.1s

################################################################################
                     [1m Learning iteration 2505/4000 [0m

                       Computation: 3261 steps/s (collection: 0.440s, learning 2.072s)
               Value function loss: 83252.0026
                    Surrogate loss: 0.0094
             Mean action noise std: 0.90
                       Mean reward: 8731.08
               Mean episode length: 375.07
                 Mean success rate: 70.00
                  Mean reward/step: 25.22
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 2.51s
                        Total time: 6452.72s
                               ETA: 3849.5s

################################################################################
                     [1m Learning iteration 2506/4000 [0m

                       Computation: 3227 steps/s (collection: 0.478s, learning 2.061s)
               Value function loss: 112891.9965
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 8852.37
               Mean episode length: 381.19
                 Mean success rate: 71.00
                  Mean reward/step: 25.13
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20537344
                    Iteration time: 2.54s
                        Total time: 6455.26s
                               ETA: 3846.9s

################################################################################
                     [1m Learning iteration 2507/4000 [0m

                       Computation: 3269 steps/s (collection: 0.443s, learning 2.062s)
               Value function loss: 105022.4915
                    Surrogate loss: 0.0086
             Mean action noise std: 0.90
                       Mean reward: 8936.00
               Mean episode length: 385.02
                 Mean success rate: 71.50
                  Mean reward/step: 24.06
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 2.51s
                        Total time: 6457.77s
                               ETA: 3844.3s

################################################################################
                     [1m Learning iteration 2508/4000 [0m

                       Computation: 3291 steps/s (collection: 0.431s, learning 2.058s)
               Value function loss: 88191.2033
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 8936.07
               Mean episode length: 385.63
                 Mean success rate: 71.50
                  Mean reward/step: 24.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20553728
                    Iteration time: 2.49s
                        Total time: 6460.26s
                               ETA: 3841.7s

################################################################################
                     [1m Learning iteration 2509/4000 [0m

                       Computation: 3287 steps/s (collection: 0.477s, learning 2.015s)
               Value function loss: 75898.3802
                    Surrogate loss: 0.0085
             Mean action noise std: 0.91
                       Mean reward: 9254.76
               Mean episode length: 393.80
                 Mean success rate: 73.00
                  Mean reward/step: 25.08
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 2.49s
                        Total time: 6462.75s
                               ETA: 3839.0s

################################################################################
                     [1m Learning iteration 2510/4000 [0m

                       Computation: 3291 steps/s (collection: 0.433s, learning 2.056s)
               Value function loss: 94499.5004
                    Surrogate loss: 0.0105
             Mean action noise std: 0.90
                       Mean reward: 9401.33
               Mean episode length: 396.49
                 Mean success rate: 74.50
                  Mean reward/step: 26.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20570112
                    Iteration time: 2.49s
                        Total time: 6465.24s
                               ETA: 3836.4s

################################################################################
                     [1m Learning iteration 2511/4000 [0m

                       Computation: 3226 steps/s (collection: 0.461s, learning 2.078s)
               Value function loss: 138830.3992
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 9414.29
               Mean episode length: 394.18
                 Mean success rate: 74.00
                  Mean reward/step: 26.07
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 2.54s
                        Total time: 6467.78s
                               ETA: 3833.8s

################################################################################
                     [1m Learning iteration 2512/4000 [0m

                       Computation: 3271 steps/s (collection: 0.457s, learning 2.047s)
               Value function loss: 91786.5818
                    Surrogate loss: 0.0139
             Mean action noise std: 0.90
                       Mean reward: 9373.14
               Mean episode length: 392.43
                 Mean success rate: 73.50
                  Mean reward/step: 25.43
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 20586496
                    Iteration time: 2.50s
                        Total time: 6470.28s
                               ETA: 3831.2s

################################################################################
                     [1m Learning iteration 2513/4000 [0m

                       Computation: 3195 steps/s (collection: 0.451s, learning 2.113s)
               Value function loss: 92041.8817
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 9446.42
               Mean episode length: 395.46
                 Mean success rate: 74.50
                  Mean reward/step: 24.94
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 2.56s
                        Total time: 6472.84s
                               ETA: 3828.6s

################################################################################
                     [1m Learning iteration 2514/4000 [0m

                       Computation: 3192 steps/s (collection: 0.465s, learning 2.101s)
               Value function loss: 91872.4056
                    Surrogate loss: 0.0100
             Mean action noise std: 0.90
                       Mean reward: 10019.46
               Mean episode length: 407.22
                 Mean success rate: 78.00
                  Mean reward/step: 25.61
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 20602880
                    Iteration time: 2.57s
                        Total time: 6475.41s
                               ETA: 3826.0s

################################################################################
                     [1m Learning iteration 2515/4000 [0m

                       Computation: 3178 steps/s (collection: 0.493s, learning 2.084s)
               Value function loss: 84984.7092
                    Surrogate loss: 0.0109
             Mean action noise std: 0.91
                       Mean reward: 10168.06
               Mean episode length: 409.55
                 Mean success rate: 80.00
                  Mean reward/step: 25.87
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 2.58s
                        Total time: 6477.99s
                               ETA: 3823.5s

################################################################################
                     [1m Learning iteration 2516/4000 [0m

                       Computation: 3164 steps/s (collection: 0.494s, learning 2.095s)
               Value function loss: 122978.0492
                    Surrogate loss: 0.0131
             Mean action noise std: 0.90
                       Mean reward: 10089.55
               Mean episode length: 402.44
                 Mean success rate: 79.50
                  Mean reward/step: 25.45
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 20619264
                    Iteration time: 2.59s
                        Total time: 6480.57s
                               ETA: 3820.9s

################################################################################
                     [1m Learning iteration 2517/4000 [0m

                       Computation: 3170 steps/s (collection: 0.493s, learning 2.090s)
               Value function loss: 78488.4140
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 10243.77
               Mean episode length: 403.25
                 Mean success rate: 79.50
                  Mean reward/step: 24.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 2.58s
                        Total time: 6483.16s
                               ETA: 3818.3s

################################################################################
                     [1m Learning iteration 2518/4000 [0m

                       Computation: 3254 steps/s (collection: 0.436s, learning 2.082s)
               Value function loss: 89793.9823
                    Surrogate loss: 0.0128
             Mean action noise std: 0.90
                       Mean reward: 10258.26
               Mean episode length: 403.54
                 Mean success rate: 80.00
                  Mean reward/step: 24.39
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 20635648
                    Iteration time: 2.52s
                        Total time: 6485.68s
                               ETA: 3815.7s

################################################################################
                     [1m Learning iteration 2519/4000 [0m

                       Computation: 3153 steps/s (collection: 0.493s, learning 2.105s)
               Value function loss: 85545.8820
                    Surrogate loss: 0.0127
             Mean action noise std: 0.90
                       Mean reward: 10201.82
               Mean episode length: 402.20
                 Mean success rate: 80.00
                  Mean reward/step: 24.78
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 2.60s
                        Total time: 6488.27s
                               ETA: 3813.1s

################################################################################
                     [1m Learning iteration 2520/4000 [0m

                       Computation: 3180 steps/s (collection: 0.497s, learning 2.079s)
               Value function loss: 99467.8824
                    Surrogate loss: 0.0120
             Mean action noise std: 0.90
                       Mean reward: 9801.54
               Mean episode length: 389.00
                 Mean success rate: 77.00
                  Mean reward/step: 24.63
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 20652032
                    Iteration time: 2.58s
                        Total time: 6490.85s
                               ETA: 3810.6s

################################################################################
                     [1m Learning iteration 2521/4000 [0m

                       Computation: 3247 steps/s (collection: 0.454s, learning 2.069s)
               Value function loss: 89919.6521
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 9787.04
               Mean episode length: 387.89
                 Mean success rate: 77.00
                  Mean reward/step: 23.98
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 2.52s
                        Total time: 6493.37s
                               ETA: 3808.0s

################################################################################
                     [1m Learning iteration 2522/4000 [0m

                       Computation: 3193 steps/s (collection: 0.471s, learning 2.094s)
               Value function loss: 103304.2102
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9383.67
               Mean episode length: 376.17
                 Mean success rate: 74.00
                  Mean reward/step: 23.86
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 20668416
                    Iteration time: 2.57s
                        Total time: 6495.94s
                               ETA: 3805.4s

################################################################################
                     [1m Learning iteration 2523/4000 [0m

                       Computation: 3195 steps/s (collection: 0.489s, learning 2.075s)
               Value function loss: 112128.7047
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 9504.46
               Mean episode length: 382.36
                 Mean success rate: 75.50
                  Mean reward/step: 23.46
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 2.56s
                        Total time: 6498.50s
                               ETA: 3802.8s

################################################################################
                     [1m Learning iteration 2524/4000 [0m

                       Computation: 3178 steps/s (collection: 0.491s, learning 2.086s)
               Value function loss: 84985.4092
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 9571.75
               Mean episode length: 383.71
                 Mean success rate: 75.00
                  Mean reward/step: 23.81
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20684800
                    Iteration time: 2.58s
                        Total time: 6501.08s
                               ETA: 3800.2s

################################################################################
                     [1m Learning iteration 2525/4000 [0m

                       Computation: 3181 steps/s (collection: 0.471s, learning 2.104s)
               Value function loss: 64505.0367
                    Surrogate loss: 0.0092
             Mean action noise std: 0.90
                       Mean reward: 9580.75
               Mean episode length: 384.05
                 Mean success rate: 75.00
                  Mean reward/step: 24.93
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 2.57s
                        Total time: 6503.65s
                               ETA: 3797.7s

################################################################################
                     [1m Learning iteration 2526/4000 [0m

                       Computation: 3059 steps/s (collection: 0.513s, learning 2.164s)
               Value function loss: 132048.2846
                    Surrogate loss: 0.0096
             Mean action noise std: 0.90
                       Mean reward: 9444.01
               Mean episode length: 378.90
                 Mean success rate: 74.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 20701184
                    Iteration time: 2.68s
                        Total time: 6506.33s
                               ETA: 3795.1s

################################################################################
                     [1m Learning iteration 2527/4000 [0m

                       Computation: 3192 steps/s (collection: 0.465s, learning 2.101s)
               Value function loss: 104423.5035
                    Surrogate loss: 0.0091
             Mean action noise std: 0.90
                       Mean reward: 9711.80
               Mean episode length: 386.11
                 Mean success rate: 75.50
                  Mean reward/step: 23.87
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 2.57s
                        Total time: 6508.90s
                               ETA: 3792.6s

################################################################################
                     [1m Learning iteration 2528/4000 [0m

                       Computation: 3119 steps/s (collection: 0.478s, learning 2.148s)
               Value function loss: 103332.3913
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 9647.88
               Mean episode length: 385.54
                 Mean success rate: 74.50
                  Mean reward/step: 23.27
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 20717568
                    Iteration time: 2.63s
                        Total time: 6511.52s
                               ETA: 3790.0s

################################################################################
                     [1m Learning iteration 2529/4000 [0m

                       Computation: 3056 steps/s (collection: 0.506s, learning 2.175s)
               Value function loss: 85253.0614
                    Surrogate loss: 0.0109
             Mean action noise std: 0.91
                       Mean reward: 9772.12
               Mean episode length: 387.92
                 Mean success rate: 75.00
                  Mean reward/step: 23.66
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 2.68s
                        Total time: 6514.20s
                               ETA: 3787.5s

################################################################################
                     [1m Learning iteration 2530/4000 [0m

                       Computation: 3058 steps/s (collection: 0.530s, learning 2.149s)
               Value function loss: 66494.7495
                    Surrogate loss: 0.0088
             Mean action noise std: 0.91
                       Mean reward: 9872.24
               Mean episode length: 391.60
                 Mean success rate: 76.50
                  Mean reward/step: 24.02
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 20733952
                    Iteration time: 2.68s
                        Total time: 6516.88s
                               ETA: 3785.0s

################################################################################
                     [1m Learning iteration 2531/4000 [0m

                       Computation: 3128 steps/s (collection: 0.486s, learning 2.132s)
               Value function loss: 122449.9340
                    Surrogate loss: 0.0129
             Mean action noise std: 0.91
                       Mean reward: 10069.95
               Mean episode length: 403.06
                 Mean success rate: 79.00
                  Mean reward/step: 24.90
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 2.62s
                        Total time: 6519.50s
                               ETA: 3782.4s

################################################################################
                     [1m Learning iteration 2532/4000 [0m

                       Computation: 3147 steps/s (collection: 0.469s, learning 2.134s)
               Value function loss: 102635.6787
                    Surrogate loss: 0.0106
             Mean action noise std: 0.91
                       Mean reward: 10055.64
               Mean episode length: 400.74
                 Mean success rate: 78.50
                  Mean reward/step: 24.17
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 20750336
                    Iteration time: 2.60s
                        Total time: 6522.10s
                               ETA: 3779.9s

################################################################################
                     [1m Learning iteration 2533/4000 [0m

                       Computation: 3132 steps/s (collection: 0.480s, learning 2.135s)
               Value function loss: 67879.2965
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 9635.48
               Mean episode length: 386.78
                 Mean success rate: 75.50
                  Mean reward/step: 24.49
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 2.62s
                        Total time: 6524.72s
                               ETA: 3777.3s

################################################################################
                     [1m Learning iteration 2534/4000 [0m

                       Computation: 3144 steps/s (collection: 0.491s, learning 2.114s)
               Value function loss: 81298.0789
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 9447.14
               Mean episode length: 384.44
                 Mean success rate: 74.50
                  Mean reward/step: 24.70
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20766720
                    Iteration time: 2.61s
                        Total time: 6527.32s
                               ETA: 3774.8s

################################################################################
                     [1m Learning iteration 2535/4000 [0m

                       Computation: 3124 steps/s (collection: 0.510s, learning 2.111s)
               Value function loss: 75799.8590
                    Surrogate loss: 0.0104
             Mean action noise std: 0.91
                       Mean reward: 9160.96
               Mean episode length: 377.20
                 Mean success rate: 73.00
                  Mean reward/step: 25.26
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 2.62s
                        Total time: 6529.94s
                               ETA: 3772.2s

################################################################################
                     [1m Learning iteration 2536/4000 [0m

                       Computation: 3142 steps/s (collection: 0.492s, learning 2.115s)
               Value function loss: 120227.1674
                    Surrogate loss: 0.0103
             Mean action noise std: 0.91
                       Mean reward: 9458.39
               Mean episode length: 387.99
                 Mean success rate: 75.00
                  Mean reward/step: 25.16
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20783104
                    Iteration time: 2.61s
                        Total time: 6532.55s
                               ETA: 3769.7s

################################################################################
                     [1m Learning iteration 2537/4000 [0m

                       Computation: 3176 steps/s (collection: 0.518s, learning 2.061s)
               Value function loss: 110149.2712
                    Surrogate loss: 0.0138
             Mean action noise std: 0.90
                       Mean reward: 9694.28
               Mean episode length: 398.86
                 Mean success rate: 77.50
                  Mean reward/step: 24.95
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 2.58s
                        Total time: 6535.13s
                               ETA: 3767.1s

################################################################################
                     [1m Learning iteration 2538/4000 [0m

                       Computation: 3329 steps/s (collection: 0.433s, learning 2.027s)
               Value function loss: 140346.1286
                    Surrogate loss: 0.0118
             Mean action noise std: 0.90
                       Mean reward: 9864.42
               Mean episode length: 403.85
                 Mean success rate: 79.00
                  Mean reward/step: 23.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 20799488
                    Iteration time: 2.46s
                        Total time: 6537.59s
                               ETA: 3764.5s

################################################################################
                     [1m Learning iteration 2539/4000 [0m

                       Computation: 3306 steps/s (collection: 0.428s, learning 2.049s)
               Value function loss: 75897.0443
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 9487.08
               Mean episode length: 395.86
                 Mean success rate: 76.50
                  Mean reward/step: 23.14
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 2.48s
                        Total time: 6540.07s
                               ETA: 3761.8s

################################################################################
                     [1m Learning iteration 2540/4000 [0m

                       Computation: 3203 steps/s (collection: 0.484s, learning 2.073s)
               Value function loss: 77959.7340
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 9408.17
               Mean episode length: 392.21
                 Mean success rate: 75.00
                  Mean reward/step: 23.59
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20815872
                    Iteration time: 2.56s
                        Total time: 6542.63s
                               ETA: 3759.2s

################################################################################
                     [1m Learning iteration 2541/4000 [0m

                       Computation: 3148 steps/s (collection: 0.477s, learning 2.125s)
               Value function loss: 94883.8859
                    Surrogate loss: 0.0127
             Mean action noise std: 0.90
                       Mean reward: 9173.50
               Mean episode length: 385.48
                 Mean success rate: 73.50
                  Mean reward/step: 24.47
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 2.60s
                        Total time: 6545.23s
                               ETA: 3756.7s

################################################################################
                     [1m Learning iteration 2542/4000 [0m

                       Computation: 3198 steps/s (collection: 0.507s, learning 2.054s)
               Value function loss: 111894.0527
                    Surrogate loss: 0.0105
             Mean action noise std: 0.90
                       Mean reward: 9329.82
               Mean episode length: 393.43
                 Mean success rate: 74.50
                  Mean reward/step: 24.19
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 20832256
                    Iteration time: 2.56s
                        Total time: 6547.79s
                               ETA: 3754.1s

################################################################################
                     [1m Learning iteration 2543/4000 [0m

                       Computation: 3267 steps/s (collection: 0.431s, learning 2.076s)
               Value function loss: 107800.5121
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9628.49
               Mean episode length: 398.92
                 Mean success rate: 77.50
                  Mean reward/step: 23.27
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 2.51s
                        Total time: 6550.30s
                               ETA: 3751.5s

################################################################################
                     [1m Learning iteration 2544/4000 [0m

                       Computation: 3252 steps/s (collection: 0.442s, learning 2.077s)
               Value function loss: 71023.1497
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 9456.47
               Mean episode length: 392.19
                 Mean success rate: 75.50
                  Mean reward/step: 22.98
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20848640
                    Iteration time: 2.52s
                        Total time: 6552.81s
                               ETA: 3748.9s

################################################################################
                     [1m Learning iteration 2545/4000 [0m

                       Computation: 3220 steps/s (collection: 0.469s, learning 2.075s)
               Value function loss: 86967.7154
                    Surrogate loss: 0.0132
             Mean action noise std: 0.90
                       Mean reward: 9087.19
               Mean episode length: 377.76
                 Mean success rate: 72.50
                  Mean reward/step: 23.63
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 2.54s
                        Total time: 6555.36s
                               ETA: 3746.3s

################################################################################
                     [1m Learning iteration 2546/4000 [0m

                       Computation: 3208 steps/s (collection: 0.499s, learning 2.054s)
               Value function loss: 85209.5088
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 8276.63
               Mean episode length: 350.05
                 Mean success rate: 65.50
                  Mean reward/step: 24.43
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 20865024
                    Iteration time: 2.55s
                        Total time: 6557.91s
                               ETA: 3743.7s

################################################################################
                     [1m Learning iteration 2547/4000 [0m

                       Computation: 3269 steps/s (collection: 0.436s, learning 2.070s)
               Value function loss: 118085.9400
                    Surrogate loss: 0.0090
             Mean action noise std: 0.90
                       Mean reward: 8088.05
               Mean episode length: 344.35
                 Mean success rate: 64.00
                  Mean reward/step: 24.27
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 2.51s
                        Total time: 6560.42s
                               ETA: 3741.1s

################################################################################
                     [1m Learning iteration 2548/4000 [0m

                       Computation: 3282 steps/s (collection: 0.450s, learning 2.045s)
               Value function loss: 107899.0954
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 8333.20
               Mean episode length: 348.00
                 Mean success rate: 65.50
                  Mean reward/step: 23.20
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 20881408
                    Iteration time: 2.50s
                        Total time: 6562.91s
                               ETA: 3738.5s

################################################################################
                     [1m Learning iteration 2549/4000 [0m

                       Computation: 3045 steps/s (collection: 0.521s, learning 2.169s)
               Value function loss: 124805.0469
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 8076.95
               Mean episode length: 338.42
                 Mean success rate: 64.00
                  Mean reward/step: 23.64
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 2.69s
                        Total time: 6565.60s
                               ETA: 3736.0s

################################################################################
                     [1m Learning iteration 2550/4000 [0m

                       Computation: 3171 steps/s (collection: 0.494s, learning 2.088s)
               Value function loss: 78892.6350
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 8224.15
               Mean episode length: 340.58
                 Mean success rate: 65.00
                  Mean reward/step: 24.25
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20897792
                    Iteration time: 2.58s
                        Total time: 6568.19s
                               ETA: 3733.4s

################################################################################
                     [1m Learning iteration 2551/4000 [0m

                       Computation: 3201 steps/s (collection: 0.480s, learning 2.078s)
               Value function loss: 103500.7514
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 8130.96
               Mean episode length: 338.22
                 Mean success rate: 64.50
                  Mean reward/step: 24.57
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 2.56s
                        Total time: 6570.74s
                               ETA: 3730.8s

################################################################################
                     [1m Learning iteration 2552/4000 [0m

                       Computation: 3153 steps/s (collection: 0.476s, learning 2.122s)
               Value function loss: 92820.3584
                    Surrogate loss: 0.0126
             Mean action noise std: 0.90
                       Mean reward: 8346.56
               Mean episode length: 345.86
                 Mean success rate: 65.00
                  Mean reward/step: 24.28
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20914176
                    Iteration time: 2.60s
                        Total time: 6573.34s
                               ETA: 3728.2s

################################################################################
                     [1m Learning iteration 2553/4000 [0m

                       Computation: 3129 steps/s (collection: 0.473s, learning 2.144s)
               Value function loss: 94008.1090
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 8591.99
               Mean episode length: 354.87
                 Mean success rate: 67.50
                  Mean reward/step: 24.14
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 2.62s
                        Total time: 6575.96s
                               ETA: 3725.7s

################################################################################
                     [1m Learning iteration 2554/4000 [0m

                       Computation: 3131 steps/s (collection: 0.534s, learning 2.082s)
               Value function loss: 106669.5595
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 8970.09
               Mean episode length: 372.38
                 Mean success rate: 71.00
                  Mean reward/step: 23.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 20930560
                    Iteration time: 2.62s
                        Total time: 6578.57s
                               ETA: 3723.1s

################################################################################
                     [1m Learning iteration 2555/4000 [0m

                       Computation: 3204 steps/s (collection: 0.471s, learning 2.086s)
               Value function loss: 84847.6176
                    Surrogate loss: 0.0088
             Mean action noise std: 0.90
                       Mean reward: 9196.53
               Mean episode length: 377.44
                 Mean success rate: 73.00
                  Mean reward/step: 24.12
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 2.56s
                        Total time: 6581.13s
                               ETA: 3720.6s

################################################################################
                     [1m Learning iteration 2556/4000 [0m

                       Computation: 3216 steps/s (collection: 0.449s, learning 2.099s)
               Value function loss: 53266.5369
                    Surrogate loss: 0.0121
             Mean action noise std: 0.90
                       Mean reward: 9231.82
               Mean episode length: 380.23
                 Mean success rate: 73.50
                  Mean reward/step: 24.87
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 20946944
                    Iteration time: 2.55s
                        Total time: 6583.68s
                               ETA: 3718.0s

################################################################################
                     [1m Learning iteration 2557/4000 [0m

                       Computation: 3172 steps/s (collection: 0.471s, learning 2.111s)
               Value function loss: 82183.4713
                    Surrogate loss: 0.0140
             Mean action noise std: 0.90
                       Mean reward: 8868.76
               Mean episode length: 371.21
                 Mean success rate: 71.50
                  Mean reward/step: 25.25
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 2.58s
                        Total time: 6586.26s
                               ETA: 3715.4s

################################################################################
                     [1m Learning iteration 2558/4000 [0m

                       Computation: 3172 steps/s (collection: 0.476s, learning 2.106s)
               Value function loss: 129913.7228
                    Surrogate loss: 0.0126
             Mean action noise std: 0.90
                       Mean reward: 9327.02
               Mean episode length: 384.75
                 Mean success rate: 73.00
                  Mean reward/step: 24.41
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20963328
                    Iteration time: 2.58s
                        Total time: 6588.84s
                               ETA: 3712.8s

################################################################################
                     [1m Learning iteration 2559/4000 [0m

                       Computation: 3203 steps/s (collection: 0.471s, learning 2.086s)
               Value function loss: 100894.4683
                    Surrogate loss: 0.0143
             Mean action noise std: 0.90
                       Mean reward: 9458.84
               Mean episode length: 389.12
                 Mean success rate: 73.50
                  Mean reward/step: 23.64
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 2.56s
                        Total time: 6591.40s
                               ETA: 3710.2s

################################################################################
                     [1m Learning iteration 2560/4000 [0m

                       Computation: 3179 steps/s (collection: 0.489s, learning 2.087s)
               Value function loss: 79506.3805
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 9617.02
               Mean episode length: 394.36
                 Mean success rate: 74.50
                  Mean reward/step: 23.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 20979712
                    Iteration time: 2.58s
                        Total time: 6593.98s
                               ETA: 3707.7s

################################################################################
                     [1m Learning iteration 2561/4000 [0m

                       Computation: 3224 steps/s (collection: 0.474s, learning 2.067s)
               Value function loss: 76838.4526
                    Surrogate loss: 0.0094
             Mean action noise std: 0.90
                       Mean reward: 9312.56
               Mean episode length: 385.45
                 Mean success rate: 72.50
                  Mean reward/step: 23.58
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 2.54s
                        Total time: 6596.52s
                               ETA: 3705.1s

################################################################################
                     [1m Learning iteration 2562/4000 [0m

                       Computation: 3250 steps/s (collection: 0.466s, learning 2.054s)
               Value function loss: 109094.1850
                    Surrogate loss: 0.0125
             Mean action noise std: 0.90
                       Mean reward: 9185.06
               Mean episode length: 382.15
                 Mean success rate: 72.00
                  Mean reward/step: 23.98
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 20996096
                    Iteration time: 2.52s
                        Total time: 6599.04s
                               ETA: 3702.5s

################################################################################
                     [1m Learning iteration 2563/4000 [0m

                       Computation: 3233 steps/s (collection: 0.490s, learning 2.043s)
               Value function loss: 96894.4988
                    Surrogate loss: 0.0117
             Mean action noise std: 0.90
                       Mean reward: 8673.19
               Mean episode length: 367.19
                 Mean success rate: 68.50
                  Mean reward/step: 23.32
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 2.53s
                        Total time: 6601.57s
                               ETA: 3699.9s

################################################################################
                     [1m Learning iteration 2564/4000 [0m

                       Computation: 3154 steps/s (collection: 0.506s, learning 2.091s)
               Value function loss: 101224.3299
                    Surrogate loss: 0.0122
             Mean action noise std: 0.90
                       Mean reward: 8990.07
               Mean episode length: 379.72
                 Mean success rate: 71.00
                  Mean reward/step: 22.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21012480
                    Iteration time: 2.60s
                        Total time: 6604.17s
                               ETA: 3697.3s

################################################################################
                     [1m Learning iteration 2565/4000 [0m

                       Computation: 3198 steps/s (collection: 0.488s, learning 2.072s)
               Value function loss: 92197.1596
                    Surrogate loss: 0.0073
             Mean action noise std: 0.90
                       Mean reward: 9175.67
               Mean episode length: 386.96
                 Mean success rate: 72.50
                  Mean reward/step: 22.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 2.56s
                        Total time: 6606.73s
                               ETA: 3694.7s

################################################################################
                     [1m Learning iteration 2566/4000 [0m

                       Computation: 3223 steps/s (collection: 0.454s, learning 2.088s)
               Value function loss: 86593.8396
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 9343.13
               Mean episode length: 390.29
                 Mean success rate: 74.00
                  Mean reward/step: 22.84
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21028864
                    Iteration time: 2.54s
                        Total time: 6609.27s
                               ETA: 3692.1s

################################################################################
                     [1m Learning iteration 2567/4000 [0m

                       Computation: 3214 steps/s (collection: 0.485s, learning 2.063s)
               Value function loss: 110514.7482
                    Surrogate loss: 0.0129
             Mean action noise std: 0.90
                       Mean reward: 8948.40
               Mean episode length: 376.71
                 Mean success rate: 71.00
                  Mean reward/step: 23.00
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 2.55s
                        Total time: 6611.82s
                               ETA: 3689.5s

################################################################################
                     [1m Learning iteration 2568/4000 [0m

                       Computation: 3239 steps/s (collection: 0.467s, learning 2.062s)
               Value function loss: 85479.6701
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 8550.23
               Mean episode length: 363.50
                 Mean success rate: 68.50
                  Mean reward/step: 22.82
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 21045248
                    Iteration time: 2.53s
                        Total time: 6614.35s
                               ETA: 3686.9s

################################################################################
                     [1m Learning iteration 2569/4000 [0m

                       Computation: 3158 steps/s (collection: 0.480s, learning 2.114s)
               Value function loss: 95015.6981
                    Surrogate loss: 0.0100
             Mean action noise std: 0.90
                       Mean reward: 8796.48
               Mean episode length: 372.37
                 Mean success rate: 70.50
                  Mean reward/step: 23.14
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 2.59s
                        Total time: 6616.94s
                               ETA: 3684.4s

################################################################################
                     [1m Learning iteration 2570/4000 [0m

                       Computation: 3240 steps/s (collection: 0.496s, learning 2.032s)
               Value function loss: 92774.5867
                    Surrogate loss: 0.0105
             Mean action noise std: 0.90
                       Mean reward: 8639.60
               Mean episode length: 362.71
                 Mean success rate: 68.50
                  Mean reward/step: 22.98
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 21061632
                    Iteration time: 2.53s
                        Total time: 6619.47s
                               ETA: 3681.8s

################################################################################
                     [1m Learning iteration 2571/4000 [0m

                       Computation: 3195 steps/s (collection: 0.483s, learning 2.080s)
               Value function loss: 70007.1229
                    Surrogate loss: 0.0079
             Mean action noise std: 0.90
                       Mean reward: 8830.87
               Mean episode length: 370.96
                 Mean success rate: 70.00
                  Mean reward/step: 23.25
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 2.56s
                        Total time: 6622.03s
                               ETA: 3679.2s

################################################################################
                     [1m Learning iteration 2572/4000 [0m

                       Computation: 3203 steps/s (collection: 0.451s, learning 2.106s)
               Value function loss: 80282.5494
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 9271.66
               Mean episode length: 382.64
                 Mean success rate: 73.00
                  Mean reward/step: 24.28
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 21078016
                    Iteration time: 2.56s
                        Total time: 6624.59s
                               ETA: 3676.6s

################################################################################
                     [1m Learning iteration 2573/4000 [0m

                       Computation: 3177 steps/s (collection: 0.498s, learning 2.081s)
               Value function loss: 90344.3503
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 9291.35
               Mean episode length: 383.67
                 Mean success rate: 73.50
                  Mean reward/step: 24.53
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 2.58s
                        Total time: 6627.17s
                               ETA: 3674.0s

################################################################################
                     [1m Learning iteration 2574/4000 [0m

                       Computation: 3215 steps/s (collection: 0.445s, learning 2.102s)
               Value function loss: 74826.9062
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 8781.32
               Mean episode length: 370.44
                 Mean success rate: 70.50
                  Mean reward/step: 24.07
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 21094400
                    Iteration time: 2.55s
                        Total time: 6629.71s
                               ETA: 3671.4s

################################################################################
                     [1m Learning iteration 2575/4000 [0m

                       Computation: 3232 steps/s (collection: 0.450s, learning 2.084s)
               Value function loss: 111557.2473
                    Surrogate loss: 0.0149
             Mean action noise std: 0.90
                       Mean reward: 9051.08
               Mean episode length: 375.42
                 Mean success rate: 72.00
                  Mean reward/step: 23.43
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 2.53s
                        Total time: 6632.25s
                               ETA: 3668.8s

################################################################################
                     [1m Learning iteration 2576/4000 [0m

                       Computation: 3110 steps/s (collection: 0.484s, learning 2.150s)
               Value function loss: 92878.8774
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 9173.76
               Mean episode length: 379.82
                 Mean success rate: 73.00
                  Mean reward/step: 23.48
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 21110784
                    Iteration time: 2.63s
                        Total time: 6634.88s
                               ETA: 3666.3s

################################################################################
                     [1m Learning iteration 2577/4000 [0m

                       Computation: 3172 steps/s (collection: 0.472s, learning 2.110s)
               Value function loss: 90636.0312
                    Surrogate loss: 0.0140
             Mean action noise std: 0.90
                       Mean reward: 9276.06
               Mean episode length: 384.90
                 Mean success rate: 73.50
                  Mean reward/step: 23.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 2.58s
                        Total time: 6637.46s
                               ETA: 3663.7s

################################################################################
                     [1m Learning iteration 2578/4000 [0m

                       Computation: 3182 steps/s (collection: 0.466s, learning 2.108s)
               Value function loss: 112249.3646
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 9464.29
               Mean episode length: 393.56
                 Mean success rate: 75.50
                  Mean reward/step: 23.54
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 21127168
                    Iteration time: 2.57s
                        Total time: 6640.04s
                               ETA: 3661.2s

################################################################################
                     [1m Learning iteration 2579/4000 [0m

                       Computation: 3157 steps/s (collection: 0.465s, learning 2.129s)
               Value function loss: 111961.8044
                    Surrogate loss: 0.0132
             Mean action noise std: 0.90
                       Mean reward: 9388.47
               Mean episode length: 395.53
                 Mean success rate: 76.00
                  Mean reward/step: 22.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 2.59s
                        Total time: 6642.63s
                               ETA: 3658.6s

################################################################################
                     [1m Learning iteration 2580/4000 [0m

                       Computation: 3253 steps/s (collection: 0.475s, learning 2.043s)
               Value function loss: 107853.9486
                    Surrogate loss: 0.0122
             Mean action noise std: 0.90
                       Mean reward: 9884.83
               Mean episode length: 412.63
                 Mean success rate: 80.00
                  Mean reward/step: 22.30
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 21143552
                    Iteration time: 2.52s
                        Total time: 6645.15s
                               ETA: 3656.0s

################################################################################
                     [1m Learning iteration 2581/4000 [0m

                       Computation: 3114 steps/s (collection: 0.486s, learning 2.144s)
               Value function loss: 88152.2368
                    Surrogate loss: 0.0162
             Mean action noise std: 0.90
                       Mean reward: 9851.35
               Mean episode length: 412.88
                 Mean success rate: 80.00
                  Mean reward/step: 22.29
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 2.63s
                        Total time: 6647.78s
                               ETA: 3653.4s

################################################################################
                     [1m Learning iteration 2582/4000 [0m

                       Computation: 3196 steps/s (collection: 0.469s, learning 2.094s)
               Value function loss: 91898.6770
                    Surrogate loss: 0.0134
             Mean action noise std: 0.90
                       Mean reward: 9062.91
               Mean episode length: 388.40
                 Mean success rate: 73.00
                  Mean reward/step: 22.49
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 21159936
                    Iteration time: 2.56s
                        Total time: 6650.34s
                               ETA: 3650.9s

################################################################################
                     [1m Learning iteration 2583/4000 [0m

                       Computation: 3230 steps/s (collection: 0.449s, learning 2.087s)
               Value function loss: 113942.2686
                    Surrogate loss: 0.0123
             Mean action noise std: 0.90
                       Mean reward: 8913.69
               Mean episode length: 384.55
                 Mean success rate: 72.50
                  Mean reward/step: 22.70
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 2.54s
                        Total time: 6652.88s
                               ETA: 3648.3s

################################################################################
                     [1m Learning iteration 2584/4000 [0m

                       Computation: 3224 steps/s (collection: 0.478s, learning 2.062s)
               Value function loss: 75173.4900
                    Surrogate loss: 0.0120
             Mean action noise std: 0.90
                       Mean reward: 8844.01
               Mean episode length: 383.52
                 Mean success rate: 72.50
                  Mean reward/step: 22.93
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 21176320
                    Iteration time: 2.54s
                        Total time: 6655.42s
                               ETA: 3645.7s

################################################################################
                     [1m Learning iteration 2585/4000 [0m

                       Computation: 3158 steps/s (collection: 0.473s, learning 2.121s)
               Value function loss: 112184.6508
                    Surrogate loss: 0.0129
             Mean action noise std: 0.90
                       Mean reward: 8735.40
               Mean episode length: 380.43
                 Mean success rate: 72.50
                  Mean reward/step: 23.46
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 2.59s
                        Total time: 6658.01s
                               ETA: 3643.1s

################################################################################
                     [1m Learning iteration 2586/4000 [0m

                       Computation: 3137 steps/s (collection: 0.515s, learning 2.096s)
               Value function loss: 96019.6035
                    Surrogate loss: 0.0118
             Mean action noise std: 0.90
                       Mean reward: 8426.43
               Mean episode length: 369.75
                 Mean success rate: 71.00
                  Mean reward/step: 23.48
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 21192704
                    Iteration time: 2.61s
                        Total time: 6660.62s
                               ETA: 3640.6s

################################################################################
                     [1m Learning iteration 2587/4000 [0m

                       Computation: 3201 steps/s (collection: 0.530s, learning 2.028s)
               Value function loss: 66450.4602
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 8211.91
               Mean episode length: 360.80
                 Mean success rate: 69.50
                  Mean reward/step: 23.19
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 2.56s
                        Total time: 6663.18s
                               ETA: 3638.0s

################################################################################
                     [1m Learning iteration 2588/4000 [0m

                       Computation: 3234 steps/s (collection: 0.464s, learning 2.069s)
               Value function loss: 73358.8395
                    Surrogate loss: 0.0119
             Mean action noise std: 0.90
                       Mean reward: 8122.17
               Mean episode length: 354.72
                 Mean success rate: 68.50
                  Mean reward/step: 23.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 21209088
                    Iteration time: 2.53s
                        Total time: 6665.72s
                               ETA: 3635.4s

################################################################################
                     [1m Learning iteration 2589/4000 [0m

                       Computation: 3174 steps/s (collection: 0.461s, learning 2.119s)
               Value function loss: 96859.6062
                    Surrogate loss: 0.0084
             Mean action noise std: 0.90
                       Mean reward: 7970.48
               Mean episode length: 348.31
                 Mean success rate: 67.50
                  Mean reward/step: 24.53
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 2.58s
                        Total time: 6668.30s
                               ETA: 3632.8s

################################################################################
                     [1m Learning iteration 2590/4000 [0m

                       Computation: 3160 steps/s (collection: 0.501s, learning 2.091s)
               Value function loss: 98653.9004
                    Surrogate loss: 0.0099
             Mean action noise std: 0.90
                       Mean reward: 8119.83
               Mean episode length: 354.28
                 Mean success rate: 69.50
                  Mean reward/step: 24.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21225472
                    Iteration time: 2.59s
                        Total time: 6670.89s
                               ETA: 3630.2s

################################################################################
                     [1m Learning iteration 2591/4000 [0m

                       Computation: 3187 steps/s (collection: 0.463s, learning 2.108s)
               Value function loss: 80220.3436
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 8301.69
               Mean episode length: 360.58
                 Mean success rate: 71.50
                  Mean reward/step: 24.88
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 2.57s
                        Total time: 6673.46s
                               ETA: 3627.7s

################################################################################
                     [1m Learning iteration 2592/4000 [0m

                       Computation: 3195 steps/s (collection: 0.476s, learning 2.087s)
               Value function loss: 62593.0991
                    Surrogate loss: 0.0085
             Mean action noise std: 0.90
                       Mean reward: 8623.56
               Mean episode length: 368.34
                 Mean success rate: 73.50
                  Mean reward/step: 25.14
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 21241856
                    Iteration time: 2.56s
                        Total time: 6676.02s
                               ETA: 3625.1s

################################################################################
                     [1m Learning iteration 2593/4000 [0m

                       Computation: 3149 steps/s (collection: 0.458s, learning 2.143s)
               Value function loss: 114918.2186
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 8929.76
               Mean episode length: 379.14
                 Mean success rate: 75.50
                  Mean reward/step: 25.50
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 2.60s
                        Total time: 6678.62s
                               ETA: 3622.5s

################################################################################
                     [1m Learning iteration 2594/4000 [0m

                       Computation: 3134 steps/s (collection: 0.501s, learning 2.113s)
               Value function loss: 127297.3531
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 8970.66
               Mean episode length: 381.14
                 Mean success rate: 75.50
                  Mean reward/step: 25.06
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 21258240
                    Iteration time: 2.61s
                        Total time: 6681.24s
                               ETA: 3620.0s

################################################################################
                     [1m Learning iteration 2595/4000 [0m

                       Computation: 3141 steps/s (collection: 0.505s, learning 2.103s)
               Value function loss: 77950.7659
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 8974.83
               Mean episode length: 384.55
                 Mean success rate: 76.00
                  Mean reward/step: 24.59
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 2.61s
                        Total time: 6683.85s
                               ETA: 3617.4s

################################################################################
                     [1m Learning iteration 2596/4000 [0m

                       Computation: 3220 steps/s (collection: 0.488s, learning 2.056s)
               Value function loss: 108843.6552
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 9490.16
               Mean episode length: 398.20
                 Mean success rate: 79.00
                  Mean reward/step: 24.26
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 21274624
                    Iteration time: 2.54s
                        Total time: 6686.39s
                               ETA: 3614.8s

################################################################################
                     [1m Learning iteration 2597/4000 [0m

                       Computation: 3156 steps/s (collection: 0.499s, learning 2.097s)
               Value function loss: 68554.3527
                    Surrogate loss: 0.0079
             Mean action noise std: 0.90
                       Mean reward: 9418.37
               Mean episode length: 398.75
                 Mean success rate: 77.50
                  Mean reward/step: 24.39
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 2.60s
                        Total time: 6688.99s
                               ETA: 3612.3s

################################################################################
                     [1m Learning iteration 2598/4000 [0m

                       Computation: 3164 steps/s (collection: 0.507s, learning 2.082s)
               Value function loss: 137995.5527
                    Surrogate loss: 0.0100
             Mean action noise std: 0.90
                       Mean reward: 9565.70
               Mean episode length: 404.17
                 Mean success rate: 78.50
                  Mean reward/step: 24.20
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 21291008
                    Iteration time: 2.59s
                        Total time: 6691.57s
                               ETA: 3609.7s

################################################################################
                     [1m Learning iteration 2599/4000 [0m

                       Computation: 3214 steps/s (collection: 0.472s, learning 2.076s)
               Value function loss: 74653.0217
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 9341.57
               Mean episode length: 396.23
                 Mean success rate: 77.00
                  Mean reward/step: 23.71
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 2.55s
                        Total time: 6694.12s
                               ETA: 3607.1s

################################################################################
                     [1m Learning iteration 2600/4000 [0m

                       Computation: 3242 steps/s (collection: 0.463s, learning 2.064s)
               Value function loss: 83925.7670
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 9649.06
               Mean episode length: 401.67
                 Mean success rate: 78.00
                  Mean reward/step: 24.78
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 21307392
                    Iteration time: 2.53s
                        Total time: 6696.65s
                               ETA: 3604.5s

################################################################################
                     [1m Learning iteration 2601/4000 [0m

                       Computation: 3223 steps/s (collection: 0.453s, learning 2.088s)
               Value function loss: 130683.9732
                    Surrogate loss: 0.0118
             Mean action noise std: 0.90
                       Mean reward: 9823.06
               Mean episode length: 410.75
                 Mean success rate: 80.00
                  Mean reward/step: 24.91
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 2.54s
                        Total time: 6699.19s
                               ETA: 3601.9s

################################################################################
                     [1m Learning iteration 2602/4000 [0m

                       Computation: 3254 steps/s (collection: 0.466s, learning 2.051s)
               Value function loss: 100814.5466
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 9700.86
               Mean episode length: 404.30
                 Mean success rate: 79.00
                  Mean reward/step: 23.78
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 21323776
                    Iteration time: 2.52s
                        Total time: 6701.71s
                               ETA: 3599.3s

################################################################################
                     [1m Learning iteration 2603/4000 [0m

                       Computation: 3155 steps/s (collection: 0.498s, learning 2.098s)
               Value function loss: 57595.6583
                    Surrogate loss: 0.0087
             Mean action noise std: 0.90
                       Mean reward: 9594.52
               Mean episode length: 400.44
                 Mean success rate: 78.00
                  Mean reward/step: 23.88
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 2.60s
                        Total time: 6704.30s
                               ETA: 3596.7s

################################################################################
                     [1m Learning iteration 2604/4000 [0m

                       Computation: 3187 steps/s (collection: 0.493s, learning 2.077s)
               Value function loss: 128510.5029
                    Surrogate loss: 0.0099
             Mean action noise std: 0.90
                       Mean reward: 9854.15
               Mean episode length: 402.26
                 Mean success rate: 78.00
                  Mean reward/step: 25.07
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21340160
                    Iteration time: 2.57s
                        Total time: 6706.87s
                               ETA: 3594.2s

################################################################################
                     [1m Learning iteration 2605/4000 [0m

                       Computation: 3224 steps/s (collection: 0.455s, learning 2.085s)
               Value function loss: 58145.6776
                    Surrogate loss: 0.0083
             Mean action noise std: 0.90
                       Mean reward: 9813.89
               Mean episode length: 399.62
                 Mean success rate: 77.50
                  Mean reward/step: 25.38
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 2.54s
                        Total time: 6709.41s
                               ETA: 3591.6s

################################################################################
                     [1m Learning iteration 2606/4000 [0m

                       Computation: 3195 steps/s (collection: 0.501s, learning 2.063s)
               Value function loss: 99283.0967
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 9852.28
               Mean episode length: 401.38
                 Mean success rate: 78.50
                  Mean reward/step: 25.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21356544
                    Iteration time: 2.56s
                        Total time: 6711.98s
                               ETA: 3589.0s

################################################################################
                     [1m Learning iteration 2607/4000 [0m

                       Computation: 3220 steps/s (collection: 0.487s, learning 2.057s)
               Value function loss: 62582.2569
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 9705.27
               Mean episode length: 396.27
                 Mean success rate: 78.50
                  Mean reward/step: 25.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 2.54s
                        Total time: 6714.52s
                               ETA: 3586.4s

################################################################################
                     [1m Learning iteration 2608/4000 [0m

                       Computation: 3144 steps/s (collection: 0.510s, learning 2.095s)
               Value function loss: 76000.5744
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 9574.29
               Mean episode length: 391.69
                 Mean success rate: 76.50
                  Mean reward/step: 25.79
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 21372928
                    Iteration time: 2.61s
                        Total time: 6717.13s
                               ETA: 3583.8s

################################################################################
                     [1m Learning iteration 2609/4000 [0m

                       Computation: 3244 steps/s (collection: 0.457s, learning 2.067s)
               Value function loss: 121439.1711
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 9941.26
               Mean episode length: 401.73
                 Mean success rate: 78.50
                  Mean reward/step: 25.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 2.52s
                        Total time: 6719.65s
                               ETA: 3581.2s

################################################################################
                     [1m Learning iteration 2610/4000 [0m

                       Computation: 3196 steps/s (collection: 0.499s, learning 2.064s)
               Value function loss: 109597.4779
                    Surrogate loss: 0.0094
             Mean action noise std: 0.90
                       Mean reward: 9762.69
               Mean episode length: 397.62
                 Mean success rate: 77.00
                  Mean reward/step: 24.33
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 21389312
                    Iteration time: 2.56s
                        Total time: 6722.21s
                               ETA: 3578.7s

################################################################################
                     [1m Learning iteration 2611/4000 [0m

                       Computation: 3192 steps/s (collection: 0.494s, learning 2.072s)
               Value function loss: 80248.0855
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 9334.27
               Mean episode length: 381.74
                 Mean success rate: 73.50
                  Mean reward/step: 24.21
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 2.57s
                        Total time: 6724.78s
                               ETA: 3576.1s

################################################################################
                     [1m Learning iteration 2612/4000 [0m

                       Computation: 3218 steps/s (collection: 0.477s, learning 2.068s)
               Value function loss: 94728.9578
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 9492.50
               Mean episode length: 384.24
                 Mean success rate: 74.00
                  Mean reward/step: 24.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 21405696
                    Iteration time: 2.55s
                        Total time: 6727.33s
                               ETA: 3573.5s

################################################################################
                     [1m Learning iteration 2613/4000 [0m

                       Computation: 3221 steps/s (collection: 0.469s, learning 2.074s)
               Value function loss: 88744.2854
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9889.07
               Mean episode length: 398.35
                 Mean success rate: 77.50
                  Mean reward/step: 25.19
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 2.54s
                        Total time: 6729.87s
                               ETA: 3570.9s

################################################################################
                     [1m Learning iteration 2614/4000 [0m

                       Computation: 3239 steps/s (collection: 0.465s, learning 2.064s)
               Value function loss: 123530.1878
                    Surrogate loss: 0.0120
             Mean action noise std: 0.90
                       Mean reward: 9869.02
               Mean episode length: 397.71
                 Mean success rate: 77.50
                  Mean reward/step: 24.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 21422080
                    Iteration time: 2.53s
                        Total time: 6732.40s
                               ETA: 3568.3s

################################################################################
                     [1m Learning iteration 2615/4000 [0m

                       Computation: 3175 steps/s (collection: 0.476s, learning 2.104s)
               Value function loss: 74897.8032
                    Surrogate loss: 0.0142
             Mean action noise std: 0.90
                       Mean reward: 10091.33
               Mean episode length: 405.57
                 Mean success rate: 78.50
                  Mean reward/step: 24.10
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 2.58s
                        Total time: 6734.98s
                               ETA: 3565.7s

################################################################################
                     [1m Learning iteration 2616/4000 [0m

                       Computation: 3167 steps/s (collection: 0.501s, learning 2.085s)
               Value function loss: 88246.3228
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 9813.07
               Mean episode length: 395.31
                 Mean success rate: 76.50
                  Mean reward/step: 24.54
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21438464
                    Iteration time: 2.59s
                        Total time: 6737.56s
                               ETA: 3563.2s

################################################################################
                     [1m Learning iteration 2617/4000 [0m

                       Computation: 3252 steps/s (collection: 0.461s, learning 2.058s)
               Value function loss: 123619.7076
                    Surrogate loss: 0.0147
             Mean action noise std: 0.90
                       Mean reward: 10459.95
               Mean episode length: 414.82
                 Mean success rate: 80.00
                  Mean reward/step: 24.23
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 2.52s
                        Total time: 6740.08s
                               ETA: 3560.6s

################################################################################
                     [1m Learning iteration 2618/4000 [0m

                       Computation: 3191 steps/s (collection: 0.494s, learning 2.073s)
               Value function loss: 112860.7144
                    Surrogate loss: 0.0122
             Mean action noise std: 0.90
                       Mean reward: 10676.73
               Mean episode length: 419.12
                 Mean success rate: 81.50
                  Mean reward/step: 23.99
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21454848
                    Iteration time: 2.57s
                        Total time: 6742.65s
                               ETA: 3558.0s

################################################################################
                     [1m Learning iteration 2619/4000 [0m

                       Computation: 3248 steps/s (collection: 0.441s, learning 2.081s)
               Value function loss: 96909.6299
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 10543.88
               Mean episode length: 413.90
                 Mean success rate: 80.50
                  Mean reward/step: 24.29
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 2.52s
                        Total time: 6745.17s
                               ETA: 3555.4s

################################################################################
                     [1m Learning iteration 2620/4000 [0m

                       Computation: 3195 steps/s (collection: 0.464s, learning 2.100s)
               Value function loss: 105800.3934
                    Surrogate loss: 0.0144
             Mean action noise std: 0.90
                       Mean reward: 10146.55
               Mean episode length: 402.51
                 Mean success rate: 78.50
                  Mean reward/step: 24.34
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 21471232
                    Iteration time: 2.56s
                        Total time: 6747.73s
                               ETA: 3552.8s

################################################################################
                     [1m Learning iteration 2621/4000 [0m

                       Computation: 3202 steps/s (collection: 0.471s, learning 2.086s)
               Value function loss: 89593.0903
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 10077.92
               Mean episode length: 402.39
                 Mean success rate: 79.50
                  Mean reward/step: 23.98
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 2.56s
                        Total time: 6750.29s
                               ETA: 3550.2s

################################################################################
                     [1m Learning iteration 2622/4000 [0m

                       Computation: 3209 steps/s (collection: 0.472s, learning 2.080s)
               Value function loss: 85638.2131
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 9832.04
               Mean episode length: 398.25
                 Mean success rate: 78.00
                  Mean reward/step: 23.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 21487616
                    Iteration time: 2.55s
                        Total time: 6752.84s
                               ETA: 3547.6s

################################################################################
                     [1m Learning iteration 2623/4000 [0m

                       Computation: 3180 steps/s (collection: 0.481s, learning 2.094s)
               Value function loss: 88441.2355
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 9581.66
               Mean episode length: 389.31
                 Mean success rate: 76.50
                  Mean reward/step: 24.34
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 2.58s
                        Total time: 6755.42s
                               ETA: 3545.1s

################################################################################
                     [1m Learning iteration 2624/4000 [0m

                       Computation: 3138 steps/s (collection: 0.514s, learning 2.097s)
               Value function loss: 110502.7409
                    Surrogate loss: 0.0171
             Mean action noise std: 0.90
                       Mean reward: 9683.03
               Mean episode length: 393.74
                 Mean success rate: 77.50
                  Mean reward/step: 24.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21504000
                    Iteration time: 2.61s
                        Total time: 6758.03s
                               ETA: 3542.5s

################################################################################
                     [1m Learning iteration 2625/4000 [0m

                       Computation: 3167 steps/s (collection: 0.477s, learning 2.109s)
               Value function loss: 109304.7137
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 9582.28
               Mean episode length: 391.18
                 Mean success rate: 76.50
                  Mean reward/step: 24.40
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 2.59s
                        Total time: 6760.62s
                               ETA: 3539.9s

################################################################################
                     [1m Learning iteration 2626/4000 [0m

                       Computation: 3237 steps/s (collection: 0.455s, learning 2.076s)
               Value function loss: 99415.6529
                    Surrogate loss: 0.0091
             Mean action noise std: 0.90
                       Mean reward: 9287.36
               Mean episode length: 383.90
                 Mean success rate: 74.50
                  Mean reward/step: 24.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 21520384
                    Iteration time: 2.53s
                        Total time: 6763.15s
                               ETA: 3537.3s

################################################################################
                     [1m Learning iteration 2627/4000 [0m

                       Computation: 3138 steps/s (collection: 0.497s, learning 2.112s)
               Value function loss: 95387.5266
                    Surrogate loss: 0.0119
             Mean action noise std: 0.90
                       Mean reward: 8758.02
               Mean episode length: 368.25
                 Mean success rate: 71.50
                  Mean reward/step: 24.48
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 2.61s
                        Total time: 6765.76s
                               ETA: 3534.8s

################################################################################
                     [1m Learning iteration 2628/4000 [0m

                       Computation: 3225 steps/s (collection: 0.474s, learning 2.066s)
               Value function loss: 69532.4847
                    Surrogate loss: 0.0118
             Mean action noise std: 0.90
                       Mean reward: 8558.14
               Mean episode length: 362.90
                 Mean success rate: 70.50
                  Mean reward/step: 24.47
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21536768
                    Iteration time: 2.54s
                        Total time: 6768.30s
                               ETA: 3532.2s

################################################################################
                     [1m Learning iteration 2629/4000 [0m

                       Computation: 3217 steps/s (collection: 0.474s, learning 2.072s)
               Value function loss: 111334.0193
                    Surrogate loss: 0.0131
             Mean action noise std: 0.90
                       Mean reward: 8832.58
               Mean episode length: 372.39
                 Mean success rate: 72.50
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 2.55s
                        Total time: 6770.84s
                               ETA: 3529.6s

################################################################################
                     [1m Learning iteration 2630/4000 [0m

                       Computation: 3247 steps/s (collection: 0.445s, learning 2.078s)
               Value function loss: 106115.7590
                    Surrogate loss: 0.0123
             Mean action noise std: 0.90
                       Mean reward: 9314.30
               Mean episode length: 383.82
                 Mean success rate: 74.50
                  Mean reward/step: 24.50
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 21553152
                    Iteration time: 2.52s
                        Total time: 6773.37s
                               ETA: 3527.0s

################################################################################
                     [1m Learning iteration 2631/4000 [0m

                       Computation: 3253 steps/s (collection: 0.466s, learning 2.052s)
               Value function loss: 83270.2088
                    Surrogate loss: 0.0125
             Mean action noise std: 0.90
                       Mean reward: 9322.88
               Mean episode length: 384.21
                 Mean success rate: 74.00
                  Mean reward/step: 24.28
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 2.52s
                        Total time: 6775.88s
                               ETA: 3524.4s

################################################################################
                     [1m Learning iteration 2632/4000 [0m

                       Computation: 3233 steps/s (collection: 0.475s, learning 2.059s)
               Value function loss: 108709.1797
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 9427.58
               Mean episode length: 383.35
                 Mean success rate: 73.50
                  Mean reward/step: 24.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 21569536
                    Iteration time: 2.53s
                        Total time: 6778.42s
                               ETA: 3521.8s

################################################################################
                     [1m Learning iteration 2633/4000 [0m

                       Computation: 3257 steps/s (collection: 0.427s, learning 2.087s)
               Value function loss: 89112.3134
                    Surrogate loss: 0.0093
             Mean action noise std: 0.90
                       Mean reward: 9240.85
               Mean episode length: 379.85
                 Mean success rate: 73.00
                  Mean reward/step: 24.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 2.51s
                        Total time: 6780.93s
                               ETA: 3519.2s

################################################################################
                     [1m Learning iteration 2634/4000 [0m

                       Computation: 3243 steps/s (collection: 0.461s, learning 2.064s)
               Value function loss: 110310.7342
                    Surrogate loss: 0.0088
             Mean action noise std: 0.90
                       Mean reward: 8940.48
               Mean episode length: 374.21
                 Mean success rate: 71.50
                  Mean reward/step: 24.53
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 21585920
                    Iteration time: 2.53s
                        Total time: 6783.46s
                               ETA: 3516.6s

################################################################################
                     [1m Learning iteration 2635/4000 [0m

                       Computation: 3158 steps/s (collection: 0.471s, learning 2.123s)
               Value function loss: 95539.4982
                    Surrogate loss: 0.0120
             Mean action noise std: 0.90
                       Mean reward: 9119.46
               Mean episode length: 381.10
                 Mean success rate: 73.50
                  Mean reward/step: 24.33
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 2.59s
                        Total time: 6786.05s
                               ETA: 3514.0s

################################################################################
                     [1m Learning iteration 2636/4000 [0m

                       Computation: 3196 steps/s (collection: 0.470s, learning 2.093s)
               Value function loss: 129544.3577
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 9349.37
               Mean episode length: 385.57
                 Mean success rate: 75.00
                  Mean reward/step: 23.64
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 21602304
                    Iteration time: 2.56s
                        Total time: 6788.61s
                               ETA: 3511.4s

################################################################################
                     [1m Learning iteration 2637/4000 [0m

                       Computation: 3305 steps/s (collection: 0.423s, learning 2.056s)
               Value function loss: 105914.8763
                    Surrogate loss: 0.0129
             Mean action noise std: 0.90
                       Mean reward: 9281.77
               Mean episode length: 383.60
                 Mean success rate: 73.50
                  Mean reward/step: 23.44
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 2.48s
                        Total time: 6791.09s
                               ETA: 3508.8s

################################################################################
                     [1m Learning iteration 2638/4000 [0m

                       Computation: 3237 steps/s (collection: 0.483s, learning 2.047s)
               Value function loss: 113196.0420
                    Surrogate loss: 0.0090
             Mean action noise std: 0.90
                       Mean reward: 8679.64
               Mean episode length: 362.73
                 Mean success rate: 70.00
                  Mean reward/step: 23.39
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 21618688
                    Iteration time: 2.53s
                        Total time: 6793.62s
                               ETA: 3506.2s

################################################################################
                     [1m Learning iteration 2639/4000 [0m

                       Computation: 3181 steps/s (collection: 0.465s, learning 2.110s)
               Value function loss: 66368.9715
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 8592.34
               Mean episode length: 358.75
                 Mean success rate: 69.50
                  Mean reward/step: 24.21
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 2.58s
                        Total time: 6796.20s
                               ETA: 3503.6s

################################################################################
                     [1m Learning iteration 2640/4000 [0m

                       Computation: 3238 steps/s (collection: 0.473s, learning 2.057s)
               Value function loss: 84514.3017
                    Surrogate loss: 0.0094
             Mean action noise std: 0.90
                       Mean reward: 8482.75
               Mean episode length: 356.78
                 Mean success rate: 69.50
                  Mean reward/step: 24.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 21635072
                    Iteration time: 2.53s
                        Total time: 6798.73s
                               ETA: 3501.0s

################################################################################
                     [1m Learning iteration 2641/4000 [0m

                       Computation: 3228 steps/s (collection: 0.437s, learning 2.100s)
               Value function loss: 112247.3981
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 8644.33
               Mean episode length: 358.72
                 Mean success rate: 70.50
                  Mean reward/step: 24.78
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 2.54s
                        Total time: 6801.26s
                               ETA: 3498.5s

################################################################################
                     [1m Learning iteration 2642/4000 [0m

                       Computation: 3239 steps/s (collection: 0.466s, learning 2.064s)
               Value function loss: 103570.7625
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 8695.51
               Mean episode length: 357.12
                 Mean success rate: 69.50
                  Mean reward/step: 24.24
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 21651456
                    Iteration time: 2.53s
                        Total time: 6803.79s
                               ETA: 3495.9s

################################################################################
                     [1m Learning iteration 2643/4000 [0m

                       Computation: 3275 steps/s (collection: 0.440s, learning 2.061s)
               Value function loss: 92357.8417
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 8674.37
               Mean episode length: 354.81
                 Mean success rate: 69.00
                  Mean reward/step: 24.40
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 2.50s
                        Total time: 6806.29s
                               ETA: 3493.2s

################################################################################
                     [1m Learning iteration 2644/4000 [0m

                       Computation: 3239 steps/s (collection: 0.463s, learning 2.066s)
               Value function loss: 71924.1357
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 8622.74
               Mean episode length: 350.41
                 Mean success rate: 68.00
                  Mean reward/step: 24.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21667840
                    Iteration time: 2.53s
                        Total time: 6808.82s
                               ETA: 3490.6s

################################################################################
                     [1m Learning iteration 2645/4000 [0m

                       Computation: 3209 steps/s (collection: 0.490s, learning 2.063s)
               Value function loss: 116132.6121
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 8860.48
               Mean episode length: 361.87
                 Mean success rate: 70.00
                  Mean reward/step: 24.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 2.55s
                        Total time: 6811.38s
                               ETA: 3488.1s

################################################################################
                     [1m Learning iteration 2646/4000 [0m

                       Computation: 3258 steps/s (collection: 0.447s, learning 2.067s)
               Value function loss: 112897.0629
                    Surrogate loss: 0.0105
             Mean action noise std: 0.90
                       Mean reward: 9172.34
               Mean episode length: 367.44
                 Mean success rate: 72.00
                  Mean reward/step: 24.75
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21684224
                    Iteration time: 2.51s
                        Total time: 6813.89s
                               ETA: 3485.5s

################################################################################
                     [1m Learning iteration 2647/4000 [0m

                       Computation: 3242 steps/s (collection: 0.450s, learning 2.077s)
               Value function loss: 81297.5736
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 9069.62
               Mean episode length: 365.60
                 Mean success rate: 71.50
                  Mean reward/step: 24.88
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 2.53s
                        Total time: 6816.42s
                               ETA: 3482.9s

################################################################################
                     [1m Learning iteration 2648/4000 [0m

                       Computation: 3244 steps/s (collection: 0.456s, learning 2.069s)
               Value function loss: 116244.7354
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9519.45
               Mean episode length: 380.21
                 Mean success rate: 74.50
                  Mean reward/step: 24.43
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21700608
                    Iteration time: 2.52s
                        Total time: 6818.94s
                               ETA: 3480.3s

################################################################################
                     [1m Learning iteration 2649/4000 [0m

                       Computation: 3335 steps/s (collection: 0.439s, learning 2.017s)
               Value function loss: 84407.2006
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 9672.70
               Mean episode length: 388.56
                 Mean success rate: 76.00
                  Mean reward/step: 24.37
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 2.46s
                        Total time: 6821.40s
                               ETA: 3477.6s

################################################################################
                     [1m Learning iteration 2650/4000 [0m

                       Computation: 3254 steps/s (collection: 0.470s, learning 2.047s)
               Value function loss: 64875.8541
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 9973.56
               Mean episode length: 397.12
                 Mean success rate: 78.00
                  Mean reward/step: 24.88
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 21716992
                    Iteration time: 2.52s
                        Total time: 6823.91s
                               ETA: 3475.0s

################################################################################
                     [1m Learning iteration 2651/4000 [0m

                       Computation: 3255 steps/s (collection: 0.467s, learning 2.049s)
               Value function loss: 132554.8609
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9614.45
               Mean episode length: 389.35
                 Mean success rate: 75.50
                  Mean reward/step: 24.71
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 2.52s
                        Total time: 6826.43s
                               ETA: 3472.4s

################################################################################
                     [1m Learning iteration 2652/4000 [0m

                       Computation: 3193 steps/s (collection: 0.451s, learning 2.114s)
               Value function loss: 108453.6567
                    Surrogate loss: 0.0139
             Mean action noise std: 0.90
                       Mean reward: 10102.73
               Mean episode length: 402.83
                 Mean success rate: 78.50
                  Mean reward/step: 23.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21733376
                    Iteration time: 2.56s
                        Total time: 6829.00s
                               ETA: 3469.8s

################################################################################
                     [1m Learning iteration 2653/4000 [0m

                       Computation: 3287 steps/s (collection: 0.452s, learning 2.039s)
               Value function loss: 88531.5860
                    Surrogate loss: 0.0142
             Mean action noise std: 0.90
                       Mean reward: 9958.89
               Mean episode length: 401.56
                 Mean success rate: 78.50
                  Mean reward/step: 23.90
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 2.49s
                        Total time: 6831.49s
                               ETA: 3467.2s

################################################################################
                     [1m Learning iteration 2654/4000 [0m

                       Computation: 3168 steps/s (collection: 0.459s, learning 2.127s)
               Value function loss: 113714.2914
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 10054.51
               Mean episode length: 408.50
                 Mean success rate: 81.00
                  Mean reward/step: 23.81
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 21749760
                    Iteration time: 2.59s
                        Total time: 6834.07s
                               ETA: 3464.7s

################################################################################
                     [1m Learning iteration 2655/4000 [0m

                       Computation: 3168 steps/s (collection: 0.452s, learning 2.134s)
               Value function loss: 86195.1933
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9893.36
               Mean episode length: 404.33
                 Mean success rate: 80.00
                  Mean reward/step: 23.83
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 2.59s
                        Total time: 6836.66s
                               ETA: 3462.1s

################################################################################
                     [1m Learning iteration 2656/4000 [0m

                       Computation: 3227 steps/s (collection: 0.458s, learning 2.080s)
               Value function loss: 101031.5906
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 10059.96
               Mean episode length: 410.06
                 Mean success rate: 81.50
                  Mean reward/step: 24.37
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21766144
                    Iteration time: 2.54s
                        Total time: 6839.20s
                               ETA: 3459.5s

################################################################################
                     [1m Learning iteration 2657/4000 [0m

                       Computation: 3201 steps/s (collection: 0.463s, learning 2.096s)
               Value function loss: 81976.0675
                    Surrogate loss: 0.0125
             Mean action noise std: 0.90
                       Mean reward: 9909.82
               Mean episode length: 406.81
                 Mean success rate: 80.00
                  Mean reward/step: 24.46
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 2.56s
                        Total time: 6841.75s
                               ETA: 3456.9s

################################################################################
                     [1m Learning iteration 2658/4000 [0m

                       Computation: 3181 steps/s (collection: 0.452s, learning 2.123s)
               Value function loss: 146028.6789
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 10000.10
               Mean episode length: 408.40
                 Mean success rate: 81.00
                  Mean reward/step: 23.80
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 21782528
                    Iteration time: 2.58s
                        Total time: 6844.33s
                               ETA: 3454.3s

################################################################################
                     [1m Learning iteration 2659/4000 [0m

                       Computation: 3170 steps/s (collection: 0.456s, learning 2.128s)
               Value function loss: 83135.3074
                    Surrogate loss: 0.0131
             Mean action noise std: 0.90
                       Mean reward: 9664.11
               Mean episode length: 395.63
                 Mean success rate: 78.00
                  Mean reward/step: 23.10
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 2.58s
                        Total time: 6846.91s
                               ETA: 3451.8s

################################################################################
                     [1m Learning iteration 2660/4000 [0m

                       Computation: 3206 steps/s (collection: 0.468s, learning 2.087s)
               Value function loss: 74107.4088
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 10004.84
               Mean episode length: 408.36
                 Mean success rate: 81.00
                  Mean reward/step: 23.74
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 21798912
                    Iteration time: 2.55s
                        Total time: 6849.47s
                               ETA: 3449.2s

################################################################################
                     [1m Learning iteration 2661/4000 [0m

                       Computation: 3173 steps/s (collection: 0.478s, learning 2.103s)
               Value function loss: 103951.9045
                    Surrogate loss: 0.0117
             Mean action noise std: 0.90
                       Mean reward: 10025.41
               Mean episode length: 409.56
                 Mean success rate: 81.00
                  Mean reward/step: 24.26
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 2.58s
                        Total time: 6852.05s
                               ETA: 3446.6s

################################################################################
                     [1m Learning iteration 2662/4000 [0m

                       Computation: 3180 steps/s (collection: 0.464s, learning 2.111s)
               Value function loss: 88898.3325
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 9568.51
               Mean episode length: 397.12
                 Mean success rate: 78.00
                  Mean reward/step: 24.00
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 21815296
                    Iteration time: 2.58s
                        Total time: 6854.63s
                               ETA: 3444.0s

################################################################################
                     [1m Learning iteration 2663/4000 [0m

                       Computation: 3183 steps/s (collection: 0.482s, learning 2.091s)
               Value function loss: 83977.2505
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 9707.15
               Mean episode length: 398.60
                 Mean success rate: 79.00
                  Mean reward/step: 24.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 2.57s
                        Total time: 6857.20s
                               ETA: 3441.5s

################################################################################
                     [1m Learning iteration 2664/4000 [0m

                       Computation: 3158 steps/s (collection: 0.486s, learning 2.108s)
               Value function loss: 76512.9923
                    Surrogate loss: 0.0127
             Mean action noise std: 0.90
                       Mean reward: 9194.69
               Mean episode length: 383.88
                 Mean success rate: 74.50
                  Mean reward/step: 24.24
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 21831680
                    Iteration time: 2.59s
                        Total time: 6859.79s
                               ETA: 3438.9s

################################################################################
                     [1m Learning iteration 2665/4000 [0m

                       Computation: 3205 steps/s (collection: 0.460s, learning 2.095s)
               Value function loss: 90633.2016
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 9323.44
               Mean episode length: 386.18
                 Mean success rate: 74.50
                  Mean reward/step: 24.75
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 2.56s
                        Total time: 6862.35s
                               ETA: 3436.3s

################################################################################
                     [1m Learning iteration 2666/4000 [0m

                       Computation: 3087 steps/s (collection: 0.489s, learning 2.164s)
               Value function loss: 82993.9361
                    Surrogate loss: 0.0100
             Mean action noise std: 0.90
                       Mean reward: 9062.79
               Mean episode length: 377.92
                 Mean success rate: 72.50
                  Mean reward/step: 25.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 21848064
                    Iteration time: 2.65s
                        Total time: 6865.00s
                               ETA: 3433.8s

################################################################################
                     [1m Learning iteration 2667/4000 [0m

                       Computation: 3121 steps/s (collection: 0.508s, learning 2.117s)
               Value function loss: 152489.5549
                    Surrogate loss: 0.0120
             Mean action noise std: 0.90
                       Mean reward: 9301.51
               Mean episode length: 384.44
                 Mean success rate: 73.50
                  Mean reward/step: 25.63
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 2.62s
                        Total time: 6867.63s
                               ETA: 3431.2s

################################################################################
                     [1m Learning iteration 2668/4000 [0m

                       Computation: 3127 steps/s (collection: 0.475s, learning 2.144s)
               Value function loss: 111733.6799
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 9143.10
               Mean episode length: 382.33
                 Mean success rate: 73.50
                  Mean reward/step: 24.73
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21864448
                    Iteration time: 2.62s
                        Total time: 6870.24s
                               ETA: 3428.7s

################################################################################
                     [1m Learning iteration 2669/4000 [0m

                       Computation: 3154 steps/s (collection: 0.453s, learning 2.143s)
               Value function loss: 107251.2187
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9277.09
               Mean episode length: 383.06
                 Mean success rate: 73.50
                  Mean reward/step: 23.82
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 2.60s
                        Total time: 6872.84s
                               ETA: 3426.1s

################################################################################
                     [1m Learning iteration 2670/4000 [0m

                       Computation: 3138 steps/s (collection: 0.498s, learning 2.112s)
               Value function loss: 82061.1909
                    Surrogate loss: 0.0085
             Mean action noise std: 0.90
                       Mean reward: 9053.43
               Mean episode length: 376.79
                 Mean success rate: 72.00
                  Mean reward/step: 24.57
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21880832
                    Iteration time: 2.61s
                        Total time: 6875.45s
                               ETA: 3423.6s

################################################################################
                     [1m Learning iteration 2671/4000 [0m

                       Computation: 3062 steps/s (collection: 0.516s, learning 2.159s)
               Value function loss: 87837.3542
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 8929.05
               Mean episode length: 374.30
                 Mean success rate: 70.50
                  Mean reward/step: 25.22
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 2.68s
                        Total time: 6878.13s
                               ETA: 3421.0s

################################################################################
                     [1m Learning iteration 2672/4000 [0m

                       Computation: 3199 steps/s (collection: 0.443s, learning 2.118s)
               Value function loss: 81175.8516
                    Surrogate loss: 0.0133
             Mean action noise std: 0.90
                       Mean reward: 9269.32
               Mean episode length: 386.06
                 Mean success rate: 73.00
                  Mean reward/step: 25.91
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 21897216
                    Iteration time: 2.56s
                        Total time: 6880.69s
                               ETA: 3418.5s

################################################################################
                     [1m Learning iteration 2673/4000 [0m

                       Computation: 3106 steps/s (collection: 0.502s, learning 2.135s)
               Value function loss: 115966.0931
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 9486.01
               Mean episode length: 391.88
                 Mean success rate: 74.50
                  Mean reward/step: 25.86
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 2.64s
                        Total time: 6883.32s
                               ETA: 3415.9s

################################################################################
                     [1m Learning iteration 2674/4000 [0m

                       Computation: 3137 steps/s (collection: 0.476s, learning 2.135s)
               Value function loss: 109916.1861
                    Surrogate loss: 0.0130
             Mean action noise std: 0.90
                       Mean reward: 9856.76
               Mean episode length: 401.30
                 Mean success rate: 77.00
                  Mean reward/step: 25.25
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21913600
                    Iteration time: 2.61s
                        Total time: 6885.93s
                               ETA: 3413.4s

################################################################################
                     [1m Learning iteration 2675/4000 [0m

                       Computation: 3172 steps/s (collection: 0.464s, learning 2.118s)
               Value function loss: 76835.1906
                    Surrogate loss: 0.0119
             Mean action noise std: 0.90
                       Mean reward: 9843.24
               Mean episode length: 400.49
                 Mean success rate: 76.50
                  Mean reward/step: 24.91
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 2.58s
                        Total time: 6888.52s
                               ETA: 3410.8s

################################################################################
                     [1m Learning iteration 2676/4000 [0m

                       Computation: 3116 steps/s (collection: 0.498s, learning 2.131s)
               Value function loss: 95685.3286
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 9941.88
               Mean episode length: 401.76
                 Mean success rate: 77.00
                  Mean reward/step: 25.85
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 21929984
                    Iteration time: 2.63s
                        Total time: 6891.14s
                               ETA: 3408.2s

################################################################################
                     [1m Learning iteration 2677/4000 [0m

                       Computation: 3139 steps/s (collection: 0.512s, learning 2.097s)
               Value function loss: 113170.3753
                    Surrogate loss: 0.0130
             Mean action noise std: 0.90
                       Mean reward: 10038.16
               Mean episode length: 405.32
                 Mean success rate: 77.50
                  Mean reward/step: 26.15
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 2.61s
                        Total time: 6893.75s
                               ETA: 3405.7s

################################################################################
                     [1m Learning iteration 2678/4000 [0m

                       Computation: 3181 steps/s (collection: 0.510s, learning 2.065s)
               Value function loss: 112297.0914
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 9984.38
               Mean episode length: 403.27
                 Mean success rate: 77.00
                  Mean reward/step: 25.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 21946368
                    Iteration time: 2.57s
                        Total time: 6896.33s
                               ETA: 3403.1s

################################################################################
                     [1m Learning iteration 2679/4000 [0m

                       Computation: 3250 steps/s (collection: 0.457s, learning 2.063s)
               Value function loss: 118784.3965
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 9951.67
               Mean episode length: 400.76
                 Mean success rate: 77.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 2.52s
                        Total time: 6898.85s
                               ETA: 3400.5s

################################################################################
                     [1m Learning iteration 2680/4000 [0m

                       Computation: 3219 steps/s (collection: 0.476s, learning 2.068s)
               Value function loss: 91616.9895
                    Surrogate loss: 0.0157
             Mean action noise std: 0.90
                       Mean reward: 10173.50
               Mean episode length: 405.03
                 Mean success rate: 78.00
                  Mean reward/step: 24.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21962752
                    Iteration time: 2.54s
                        Total time: 6901.39s
                               ETA: 3397.9s

################################################################################
                     [1m Learning iteration 2681/4000 [0m

                       Computation: 3250 steps/s (collection: 0.453s, learning 2.068s)
               Value function loss: 72111.6684
                    Surrogate loss: 0.0133
             Mean action noise std: 0.90
                       Mean reward: 10277.53
               Mean episode length: 407.60
                 Mean success rate: 79.00
                  Mean reward/step: 25.23
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 2.52s
                        Total time: 6903.91s
                               ETA: 3395.3s

################################################################################
                     [1m Learning iteration 2682/4000 [0m

                       Computation: 3299 steps/s (collection: 0.419s, learning 2.064s)
               Value function loss: 119262.1873
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 10334.61
               Mean episode length: 408.23
                 Mean success rate: 79.50
                  Mean reward/step: 25.80
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 21979136
                    Iteration time: 2.48s
                        Total time: 6906.40s
                               ETA: 3392.7s

################################################################################
                     [1m Learning iteration 2683/4000 [0m

                       Computation: 3254 steps/s (collection: 0.501s, learning 2.017s)
               Value function loss: 80587.6267
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 10226.64
               Mean episode length: 404.03
                 Mean success rate: 78.00
                  Mean reward/step: 25.00
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 2.52s
                        Total time: 6908.91s
                               ETA: 3390.1s

################################################################################
                     [1m Learning iteration 2684/4000 [0m

                       Computation: 3178 steps/s (collection: 0.504s, learning 2.074s)
               Value function loss: 107837.3100
                    Surrogate loss: 0.0076
             Mean action noise std: 0.90
                       Mean reward: 9994.69
               Mean episode length: 396.06
                 Mean success rate: 76.00
                  Mean reward/step: 24.71
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 21995520
                    Iteration time: 2.58s
                        Total time: 6911.49s
                               ETA: 3387.5s

################################################################################
                     [1m Learning iteration 2685/4000 [0m

                       Computation: 3250 steps/s (collection: 0.489s, learning 2.032s)
               Value function loss: 125642.7686
                    Surrogate loss: 0.0125
             Mean action noise std: 0.90
                       Mean reward: 10205.51
               Mean episode length: 403.00
                 Mean success rate: 78.50
                  Mean reward/step: 24.12
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 2.52s
                        Total time: 6914.01s
                               ETA: 3384.9s

################################################################################
                     [1m Learning iteration 2686/4000 [0m

                       Computation: 3255 steps/s (collection: 0.480s, learning 2.036s)
               Value function loss: 88191.4816
                    Surrogate loss: 0.0096
             Mean action noise std: 0.90
                       Mean reward: 9604.57
               Mean episode length: 385.09
                 Mean success rate: 74.50
                  Mean reward/step: 23.58
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 22011904
                    Iteration time: 2.52s
                        Total time: 6916.53s
                               ETA: 3382.3s

################################################################################
                     [1m Learning iteration 2687/4000 [0m

                       Computation: 3174 steps/s (collection: 0.483s, learning 2.097s)
               Value function loss: 82522.6584
                    Surrogate loss: 0.0086
             Mean action noise std: 0.90
                       Mean reward: 9712.39
               Mean episode length: 387.58
                 Mean success rate: 75.00
                  Mean reward/step: 23.87
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 2.58s
                        Total time: 6919.11s
                               ETA: 3379.8s

################################################################################
                     [1m Learning iteration 2688/4000 [0m

                       Computation: 3231 steps/s (collection: 0.480s, learning 2.055s)
               Value function loss: 70634.9505
                    Surrogate loss: 0.0131
             Mean action noise std: 0.90
                       Mean reward: 9942.05
               Mean episode length: 393.85
                 Mean success rate: 76.50
                  Mean reward/step: 24.84
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22028288
                    Iteration time: 2.54s
                        Total time: 6921.64s
                               ETA: 3377.2s

################################################################################
                     [1m Learning iteration 2689/4000 [0m

                       Computation: 3163 steps/s (collection: 0.482s, learning 2.108s)
               Value function loss: 103348.3703
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 9887.24
               Mean episode length: 399.07
                 Mean success rate: 76.50
                  Mean reward/step: 25.77
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 2.59s
                        Total time: 6924.23s
                               ETA: 3374.6s

################################################################################
                     [1m Learning iteration 2690/4000 [0m

                       Computation: 3141 steps/s (collection: 0.480s, learning 2.127s)
               Value function loss: 79875.8429
                    Surrogate loss: 0.0105
             Mean action noise std: 0.90
                       Mean reward: 9752.14
               Mean episode length: 394.36
                 Mean success rate: 76.00
                  Mean reward/step: 25.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22044672
                    Iteration time: 2.61s
                        Total time: 6926.84s
                               ETA: 3372.0s

################################################################################
                     [1m Learning iteration 2691/4000 [0m

                       Computation: 3082 steps/s (collection: 0.494s, learning 2.163s)
               Value function loss: 82105.7421
                    Surrogate loss: 0.0105
             Mean action noise std: 0.90
                       Mean reward: 9925.14
               Mean episode length: 397.24
                 Mean success rate: 76.50
                  Mean reward/step: 25.59
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 2.66s
                        Total time: 6929.50s
                               ETA: 3369.5s

################################################################################
                     [1m Learning iteration 2692/4000 [0m

                       Computation: 3063 steps/s (collection: 0.507s, learning 2.167s)
               Value function loss: 85413.1004
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 10037.99
               Mean episode length: 401.44
                 Mean success rate: 77.00
                  Mean reward/step: 26.05
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 22061056
                    Iteration time: 2.67s
                        Total time: 6932.17s
                               ETA: 3367.0s

################################################################################
                     [1m Learning iteration 2693/4000 [0m

                       Computation: 3155 steps/s (collection: 0.501s, learning 2.095s)
               Value function loss: 108605.8070
                    Surrogate loss: 0.0121
             Mean action noise std: 0.90
                       Mean reward: 10079.44
               Mean episode length: 400.05
                 Mean success rate: 77.50
                  Mean reward/step: 26.45
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 2.60s
                        Total time: 6934.77s
                               ETA: 3364.4s

################################################################################
                     [1m Learning iteration 2694/4000 [0m

                       Computation: 3151 steps/s (collection: 0.499s, learning 2.100s)
               Value function loss: 100676.5800
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 10643.87
               Mean episode length: 417.61
                 Mean success rate: 82.00
                  Mean reward/step: 26.13
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22077440
                    Iteration time: 2.60s
                        Total time: 6937.37s
                               ETA: 3361.9s

################################################################################
                     [1m Learning iteration 2695/4000 [0m

                       Computation: 3105 steps/s (collection: 0.516s, learning 2.122s)
               Value function loss: 118757.8917
                    Surrogate loss: 0.0147
             Mean action noise std: 0.90
                       Mean reward: 10593.13
               Mean episode length: 418.36
                 Mean success rate: 82.00
                  Mean reward/step: 26.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 2.64s
                        Total time: 6940.01s
                               ETA: 3359.3s

################################################################################
                     [1m Learning iteration 2696/4000 [0m

                       Computation: 3173 steps/s (collection: 0.468s, learning 2.113s)
               Value function loss: 80521.1133
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 10586.83
               Mean episode length: 418.30
                 Mean success rate: 82.00
                  Mean reward/step: 26.32
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 22093824
                    Iteration time: 2.58s
                        Total time: 6942.59s
                               ETA: 3356.7s

################################################################################
                     [1m Learning iteration 2697/4000 [0m

                       Computation: 3125 steps/s (collection: 0.509s, learning 2.112s)
               Value function loss: 64496.8618
                    Surrogate loss: 0.0093
             Mean action noise std: 0.90
                       Mean reward: 10749.29
               Mean episode length: 424.56
                 Mean success rate: 83.50
                  Mean reward/step: 27.01
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 2.62s
                        Total time: 6945.21s
                               ETA: 3354.2s

################################################################################
                     [1m Learning iteration 2698/4000 [0m

                       Computation: 3124 steps/s (collection: 0.469s, learning 2.153s)
               Value function loss: 128926.0929
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 11007.81
               Mean episode length: 434.99
                 Mean success rate: 85.50
                  Mean reward/step: 26.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22110208
                    Iteration time: 2.62s
                        Total time: 6947.83s
                               ETA: 3351.6s

################################################################################
                     [1m Learning iteration 2699/4000 [0m

                       Computation: 3125 steps/s (collection: 0.497s, learning 2.124s)
               Value function loss: 106107.5508
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 11143.12
               Mean episode length: 438.60
                 Mean success rate: 86.50
                  Mean reward/step: 24.97
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 2.62s
                        Total time: 6950.45s
                               ETA: 3349.1s

################################################################################
                     [1m Learning iteration 2700/4000 [0m

                       Computation: 3054 steps/s (collection: 0.531s, learning 2.151s)
               Value function loss: 142054.3576
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 11240.43
               Mean episode length: 440.06
                 Mean success rate: 88.00
                  Mean reward/step: 23.86
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 22126592
                    Iteration time: 2.68s
                        Total time: 6953.13s
                               ETA: 3346.6s

################################################################################
                     [1m Learning iteration 2701/4000 [0m

                       Computation: 3138 steps/s (collection: 0.496s, learning 2.114s)
               Value function loss: 105837.3463
                    Surrogate loss: 0.0128
             Mean action noise std: 0.90
                       Mean reward: 11043.79
               Mean episode length: 438.92
                 Mean success rate: 87.50
                  Mean reward/step: 23.51
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 2.61s
                        Total time: 6955.74s
                               ETA: 3344.0s

################################################################################
                     [1m Learning iteration 2702/4000 [0m

                       Computation: 3156 steps/s (collection: 0.507s, learning 2.088s)
               Value function loss: 106468.9994
                    Surrogate loss: 0.0083
             Mean action noise std: 0.90
                       Mean reward: 10981.74
               Mean episode length: 437.12
                 Mean success rate: 86.50
                  Mean reward/step: 23.87
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22142976
                    Iteration time: 2.59s
                        Total time: 6958.34s
                               ETA: 3341.4s

################################################################################
                     [1m Learning iteration 2703/4000 [0m

                       Computation: 3091 steps/s (collection: 0.511s, learning 2.139s)
               Value function loss: 72848.6297
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 11017.92
               Mean episode length: 437.02
                 Mean success rate: 86.50
                  Mean reward/step: 24.45
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 2.65s
                        Total time: 6960.99s
                               ETA: 3338.9s

################################################################################
                     [1m Learning iteration 2704/4000 [0m

                       Computation: 3050 steps/s (collection: 0.543s, learning 2.143s)
               Value function loss: 107429.6285
                    Surrogate loss: 0.0134
             Mean action noise std: 0.90
                       Mean reward: 10799.73
               Mean episode length: 430.44
                 Mean success rate: 84.50
                  Mean reward/step: 25.44
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22159360
                    Iteration time: 2.69s
                        Total time: 6963.67s
                               ETA: 3336.4s

################################################################################
                     [1m Learning iteration 2705/4000 [0m

                       Computation: 3094 steps/s (collection: 0.508s, learning 2.139s)
               Value function loss: 95396.6970
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 10632.75
               Mean episode length: 420.68
                 Mean success rate: 83.00
                  Mean reward/step: 24.89
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 2.65s
                        Total time: 6966.32s
                               ETA: 3333.8s

################################################################################
                     [1m Learning iteration 2706/4000 [0m

                       Computation: 3105 steps/s (collection: 0.488s, learning 2.150s)
               Value function loss: 104943.1440
                    Surrogate loss: 0.0092
             Mean action noise std: 0.90
                       Mean reward: 10675.41
               Mean episode length: 422.23
                 Mean success rate: 82.50
                  Mean reward/step: 24.98
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 22175744
                    Iteration time: 2.64s
                        Total time: 6968.96s
                               ETA: 3331.3s

################################################################################
                     [1m Learning iteration 2707/4000 [0m

                       Computation: 3185 steps/s (collection: 0.468s, learning 2.103s)
               Value function loss: 61673.0011
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 10591.40
               Mean episode length: 417.33
                 Mean success rate: 81.50
                  Mean reward/step: 25.24
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 2.57s
                        Total time: 6971.53s
                               ETA: 3328.7s

################################################################################
                     [1m Learning iteration 2708/4000 [0m

                       Computation: 3081 steps/s (collection: 0.508s, learning 2.151s)
               Value function loss: 113356.4961
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 10577.87
               Mean episode length: 413.75
                 Mean success rate: 80.50
                  Mean reward/step: 25.38
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22192128
                    Iteration time: 2.66s
                        Total time: 6974.19s
                               ETA: 3326.2s

################################################################################
                     [1m Learning iteration 2709/4000 [0m

                       Computation: 3133 steps/s (collection: 0.493s, learning 2.121s)
               Value function loss: 114187.9868
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 10531.04
               Mean episode length: 412.03
                 Mean success rate: 80.50
                  Mean reward/step: 24.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 2.61s
                        Total time: 6976.80s
                               ETA: 3323.6s

################################################################################
                     [1m Learning iteration 2710/4000 [0m

                       Computation: 3116 steps/s (collection: 0.532s, learning 2.097s)
               Value function loss: 82164.9947
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 10470.23
               Mean episode length: 411.46
                 Mean success rate: 79.50
                  Mean reward/step: 24.10
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22208512
                    Iteration time: 2.63s
                        Total time: 6979.43s
                               ETA: 3321.1s

################################################################################
                     [1m Learning iteration 2711/4000 [0m

                       Computation: 3127 steps/s (collection: 0.490s, learning 2.129s)
               Value function loss: 79698.4791
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 10478.21
               Mean episode length: 412.81
                 Mean success rate: 80.00
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 2.62s
                        Total time: 6982.05s
                               ETA: 3318.5s

################################################################################
                     [1m Learning iteration 2712/4000 [0m

                       Computation: 3045 steps/s (collection: 0.566s, learning 2.124s)
               Value function loss: 76809.0184
                    Surrogate loss: 0.0108
             Mean action noise std: 0.90
                       Mean reward: 10398.04
               Mean episode length: 409.00
                 Mean success rate: 79.00
                  Mean reward/step: 24.35
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22224896
                    Iteration time: 2.69s
                        Total time: 6984.74s
                               ETA: 3316.0s

################################################################################
                     [1m Learning iteration 2713/4000 [0m

                       Computation: 3147 steps/s (collection: 0.503s, learning 2.099s)
               Value function loss: 78780.8147
                    Surrogate loss: 0.0118
             Mean action noise std: 0.90
                       Mean reward: 10317.19
               Mean episode length: 410.02
                 Mean success rate: 79.00
                  Mean reward/step: 25.38
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 2.60s
                        Total time: 6987.34s
                               ETA: 3313.5s

################################################################################
                     [1m Learning iteration 2714/4000 [0m

                       Computation: 3212 steps/s (collection: 0.452s, learning 2.098s)
               Value function loss: 113516.3959
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 10601.30
               Mean episode length: 418.71
                 Mean success rate: 81.00
                  Mean reward/step: 25.89
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22241280
                    Iteration time: 2.55s
                        Total time: 6989.89s
                               ETA: 3310.9s

################################################################################
                     [1m Learning iteration 2715/4000 [0m

                       Computation: 3207 steps/s (collection: 0.470s, learning 2.084s)
               Value function loss: 145884.0266
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 10830.78
               Mean episode length: 427.62
                 Mean success rate: 83.00
                  Mean reward/step: 25.59
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 2.55s
                        Total time: 6992.45s
                               ETA: 3308.3s

################################################################################
                     [1m Learning iteration 2716/4000 [0m

                       Computation: 3189 steps/s (collection: 0.474s, learning 2.094s)
               Value function loss: 128796.8687
                    Surrogate loss: 0.0099
             Mean action noise std: 0.90
                       Mean reward: 10986.55
               Mean episode length: 435.08
                 Mean success rate: 84.50
                  Mean reward/step: 24.40
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 22257664
                    Iteration time: 2.57s
                        Total time: 6995.02s
                               ETA: 3305.7s

################################################################################
                     [1m Learning iteration 2717/4000 [0m

                       Computation: 3091 steps/s (collection: 0.531s, learning 2.119s)
               Value function loss: 94094.0962
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 10687.01
               Mean episode length: 426.70
                 Mean success rate: 83.50
                  Mean reward/step: 24.23
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 2.65s
                        Total time: 6997.67s
                               ETA: 3303.2s

################################################################################
                     [1m Learning iteration 2718/4000 [0m

                       Computation: 3140 steps/s (collection: 0.492s, learning 2.116s)
               Value function loss: 93018.8193
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 10614.92
               Mean episode length: 424.19
                 Mean success rate: 83.00
                  Mean reward/step: 24.10
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22274048
                    Iteration time: 2.61s
                        Total time: 7000.27s
                               ETA: 3300.6s

################################################################################
                     [1m Learning iteration 2719/4000 [0m

                       Computation: 3209 steps/s (collection: 0.454s, learning 2.098s)
               Value function loss: 77149.6050
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 10561.39
               Mean episode length: 423.84
                 Mean success rate: 82.50
                  Mean reward/step: 24.93
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 2.55s
                        Total time: 7002.83s
                               ETA: 3298.0s

################################################################################
                     [1m Learning iteration 2720/4000 [0m

                       Computation: 3140 steps/s (collection: 0.474s, learning 2.134s)
               Value function loss: 108213.8254
                    Surrogate loss: 0.0133
             Mean action noise std: 0.90
                       Mean reward: 10388.36
               Mean episode length: 417.21
                 Mean success rate: 82.00
                  Mean reward/step: 25.48
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 22290432
                    Iteration time: 2.61s
                        Total time: 7005.43s
                               ETA: 3295.5s

################################################################################
                     [1m Learning iteration 2721/4000 [0m

                       Computation: 3141 steps/s (collection: 0.501s, learning 2.106s)
               Value function loss: 120733.5666
                    Surrogate loss: 0.0092
             Mean action noise std: 0.90
                       Mean reward: 10670.63
               Mean episode length: 425.06
                 Mean success rate: 83.50
                  Mean reward/step: 25.45
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 2.61s
                        Total time: 7008.04s
                               ETA: 3292.9s

################################################################################
                     [1m Learning iteration 2722/4000 [0m

                       Computation: 3153 steps/s (collection: 0.492s, learning 2.105s)
               Value function loss: 73454.3714
                    Surrogate loss: 0.0108
             Mean action noise std: 0.90
                       Mean reward: 10729.76
               Mean episode length: 430.45
                 Mean success rate: 84.50
                  Mean reward/step: 24.91
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 22306816
                    Iteration time: 2.60s
                        Total time: 7010.64s
                               ETA: 3290.3s

################################################################################
                     [1m Learning iteration 2723/4000 [0m

                       Computation: 3153 steps/s (collection: 0.511s, learning 2.086s)
               Value function loss: 79599.3956
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 10467.75
               Mean episode length: 419.81
                 Mean success rate: 82.50
                  Mean reward/step: 24.81
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 2.60s
                        Total time: 7013.24s
                               ETA: 3287.8s

################################################################################
                     [1m Learning iteration 2724/4000 [0m

                       Computation: 3222 steps/s (collection: 0.469s, learning 2.073s)
               Value function loss: 95728.9970
                    Surrogate loss: 0.0082
             Mean action noise std: 0.90
                       Mean reward: 10316.01
               Mean episode length: 415.67
                 Mean success rate: 82.00
                  Mean reward/step: 25.28
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22323200
                    Iteration time: 2.54s
                        Total time: 7015.78s
                               ETA: 3285.2s

################################################################################
                     [1m Learning iteration 2725/4000 [0m

                       Computation: 3171 steps/s (collection: 0.494s, learning 2.089s)
               Value function loss: 107072.3730
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 10239.00
               Mean episode length: 414.15
                 Mean success rate: 81.50
                  Mean reward/step: 25.47
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 2.58s
                        Total time: 7018.36s
                               ETA: 3282.6s

################################################################################
                     [1m Learning iteration 2726/4000 [0m

                       Computation: 3189 steps/s (collection: 0.497s, learning 2.071s)
               Value function loss: 109181.8737
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 10253.18
               Mean episode length: 413.69
                 Mean success rate: 81.50
                  Mean reward/step: 25.12
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22339584
                    Iteration time: 2.57s
                        Total time: 7020.93s
                               ETA: 3280.0s

################################################################################
                     [1m Learning iteration 2727/4000 [0m

                       Computation: 3160 steps/s (collection: 0.515s, learning 2.077s)
               Value function loss: 85190.9249
                    Surrogate loss: 0.0090
             Mean action noise std: 0.90
                       Mean reward: 10168.16
               Mean episode length: 409.20
                 Mean success rate: 80.50
                  Mean reward/step: 25.18
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 2.59s
                        Total time: 7023.52s
                               ETA: 3277.5s

################################################################################
                     [1m Learning iteration 2728/4000 [0m

                       Computation: 3203 steps/s (collection: 0.458s, learning 2.099s)
               Value function loss: 71615.0799
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 10341.93
               Mean episode length: 413.32
                 Mean success rate: 81.00
                  Mean reward/step: 25.45
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 22355968
                    Iteration time: 2.56s
                        Total time: 7026.08s
                               ETA: 3274.9s

################################################################################
                     [1m Learning iteration 2729/4000 [0m

                       Computation: 3155 steps/s (collection: 0.508s, learning 2.089s)
               Value function loss: 120409.0617
                    Surrogate loss: 0.0108
             Mean action noise std: 0.90
                       Mean reward: 10582.42
               Mean episode length: 424.25
                 Mean success rate: 83.50
                  Mean reward/step: 25.90
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 2.60s
                        Total time: 7028.67s
                               ETA: 3272.3s

################################################################################
                     [1m Learning iteration 2730/4000 [0m

                       Computation: 3212 steps/s (collection: 0.480s, learning 2.069s)
               Value function loss: 83087.4446
                    Surrogate loss: 0.0081
             Mean action noise std: 0.90
                       Mean reward: 10203.98
               Mean episode length: 411.02
                 Mean success rate: 80.50
                  Mean reward/step: 24.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22372352
                    Iteration time: 2.55s
                        Total time: 7031.22s
                               ETA: 3269.7s

################################################################################
                     [1m Learning iteration 2731/4000 [0m

                       Computation: 3191 steps/s (collection: 0.484s, learning 2.083s)
               Value function loss: 133409.8767
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 10676.09
               Mean episode length: 424.77
                 Mean success rate: 83.50
                  Mean reward/step: 23.63
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 2.57s
                        Total time: 7033.79s
                               ETA: 3267.2s

################################################################################
                     [1m Learning iteration 2732/4000 [0m

                       Computation: 3195 steps/s (collection: 0.474s, learning 2.089s)
               Value function loss: 107841.0183
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 10586.52
               Mean episode length: 424.56
                 Mean success rate: 84.00
                  Mean reward/step: 22.16
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 22388736
                    Iteration time: 2.56s
                        Total time: 7036.36s
                               ETA: 3264.6s

################################################################################
                     [1m Learning iteration 2733/4000 [0m

                       Computation: 3125 steps/s (collection: 0.500s, learning 2.121s)
               Value function loss: 112972.6773
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 10838.51
               Mean episode length: 429.81
                 Mean success rate: 85.50
                  Mean reward/step: 21.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 2.62s
                        Total time: 7038.98s
                               ETA: 3262.0s

################################################################################
                     [1m Learning iteration 2734/4000 [0m

                       Computation: 3174 steps/s (collection: 0.487s, learning 2.093s)
               Value function loss: 78868.1604
                    Surrogate loss: 0.0100
             Mean action noise std: 0.90
                       Mean reward: 10910.01
               Mean episode length: 429.68
                 Mean success rate: 85.50
                  Mean reward/step: 22.34
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22405120
                    Iteration time: 2.58s
                        Total time: 7041.56s
                               ETA: 3259.5s

################################################################################
                     [1m Learning iteration 2735/4000 [0m

                       Computation: 3243 steps/s (collection: 0.480s, learning 2.046s)
               Value function loss: 68686.6241
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 10542.19
               Mean episode length: 421.71
                 Mean success rate: 84.00
                  Mean reward/step: 23.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 2.53s
                        Total time: 7044.08s
                               ETA: 3256.9s

################################################################################
                     [1m Learning iteration 2736/4000 [0m

                       Computation: 3243 steps/s (collection: 0.475s, learning 2.050s)
               Value function loss: 110264.3026
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 10143.75
               Mean episode length: 411.44
                 Mean success rate: 82.00
                  Mean reward/step: 24.09
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 22421504
                    Iteration time: 2.53s
                        Total time: 7046.61s
                               ETA: 3254.3s

################################################################################
                     [1m Learning iteration 2737/4000 [0m

                       Computation: 3121 steps/s (collection: 0.527s, learning 2.097s)
               Value function loss: 115603.2226
                    Surrogate loss: 0.0133
             Mean action noise std: 0.90
                       Mean reward: 10155.74
               Mean episode length: 416.12
                 Mean success rate: 82.50
                  Mean reward/step: 22.90
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 2.62s
                        Total time: 7049.23s
                               ETA: 3251.7s

################################################################################
                     [1m Learning iteration 2738/4000 [0m

                       Computation: 3198 steps/s (collection: 0.485s, learning 2.076s)
               Value function loss: 77310.4465
                    Surrogate loss: 0.0118
             Mean action noise std: 0.90
                       Mean reward: 9911.73
               Mean episode length: 409.82
                 Mean success rate: 81.00
                  Mean reward/step: 22.86
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22437888
                    Iteration time: 2.56s
                        Total time: 7051.79s
                               ETA: 3249.1s

################################################################################
                     [1m Learning iteration 2739/4000 [0m

                       Computation: 3262 steps/s (collection: 0.438s, learning 2.074s)
               Value function loss: 64025.1858
                    Surrogate loss: 0.0096
             Mean action noise std: 0.90
                       Mean reward: 9907.29
               Mean episode length: 407.93
                 Mean success rate: 80.50
                  Mean reward/step: 24.01
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 2.51s
                        Total time: 7054.30s
                               ETA: 3246.5s

################################################################################
                     [1m Learning iteration 2740/4000 [0m

                       Computation: 3257 steps/s (collection: 0.429s, learning 2.086s)
               Value function loss: 110428.4775
                    Surrogate loss: 0.0133
             Mean action noise std: 0.90
                       Mean reward: 9896.60
               Mean episode length: 414.69
                 Mean success rate: 81.50
                  Mean reward/step: 24.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22454272
                    Iteration time: 2.51s
                        Total time: 7056.82s
                               ETA: 3243.9s

################################################################################
                     [1m Learning iteration 2741/4000 [0m

                       Computation: 3222 steps/s (collection: 0.457s, learning 2.085s)
               Value function loss: 73945.1512
                    Surrogate loss: 0.0118
             Mean action noise std: 0.90
                       Mean reward: 9889.54
               Mean episode length: 414.45
                 Mean success rate: 81.00
                  Mean reward/step: 24.30
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 2.54s
                        Total time: 7059.36s
                               ETA: 3241.3s

################################################################################
                     [1m Learning iteration 2742/4000 [0m

                       Computation: 3201 steps/s (collection: 0.465s, learning 2.094s)
               Value function loss: 96096.1867
                    Surrogate loss: 0.0121
             Mean action noise std: 0.90
                       Mean reward: 9851.88
               Mean episode length: 414.45
                 Mean success rate: 81.00
                  Mean reward/step: 24.30
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22470656
                    Iteration time: 2.56s
                        Total time: 7061.92s
                               ETA: 3238.8s

################################################################################
                     [1m Learning iteration 2743/4000 [0m

                       Computation: 3240 steps/s (collection: 0.464s, learning 2.064s)
               Value function loss: 93359.5529
                    Surrogate loss: 0.0130
             Mean action noise std: 0.90
                       Mean reward: 9597.45
               Mean episode length: 410.81
                 Mean success rate: 79.00
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 2.53s
                        Total time: 7064.45s
                               ETA: 3236.2s

################################################################################
                     [1m Learning iteration 2744/4000 [0m

                       Computation: 3149 steps/s (collection: 0.470s, learning 2.131s)
               Value function loss: 47214.9136
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 9061.20
               Mean episode length: 395.61
                 Mean success rate: 76.00
                  Mean reward/step: 24.57
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22487040
                    Iteration time: 2.60s
                        Total time: 7067.05s
                               ETA: 3233.6s

################################################################################
                     [1m Learning iteration 2745/4000 [0m

                       Computation: 3181 steps/s (collection: 0.496s, learning 2.078s)
               Value function loss: 92451.8623
                    Surrogate loss: 0.0132
             Mean action noise std: 0.90
                       Mean reward: 9273.52
               Mean episode length: 400.96
                 Mean success rate: 77.00
                  Mean reward/step: 25.61
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 2.57s
                        Total time: 7069.62s
                               ETA: 3231.0s

################################################################################
                     [1m Learning iteration 2746/4000 [0m

                       Computation: 3179 steps/s (collection: 0.493s, learning 2.083s)
               Value function loss: 112098.5869
                    Surrogate loss: 0.0125
             Mean action noise std: 0.90
                       Mean reward: 9698.62
               Mean episode length: 412.65
                 Mean success rate: 79.50
                  Mean reward/step: 25.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22503424
                    Iteration time: 2.58s
                        Total time: 7072.20s
                               ETA: 3228.4s

################################################################################
                     [1m Learning iteration 2747/4000 [0m

                       Computation: 3208 steps/s (collection: 0.510s, learning 2.044s)
               Value function loss: 125716.0581
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 9569.76
               Mean episode length: 410.49
                 Mean success rate: 79.50
                  Mean reward/step: 24.04
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 2.55s
                        Total time: 7074.75s
                               ETA: 3225.9s

################################################################################
                     [1m Learning iteration 2748/4000 [0m

                       Computation: 3261 steps/s (collection: 0.431s, learning 2.080s)
               Value function loss: 121139.4617
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 9715.52
               Mean episode length: 416.27
                 Mean success rate: 81.00
                  Mean reward/step: 23.62
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 22519808
                    Iteration time: 2.51s
                        Total time: 7077.27s
                               ETA: 3223.3s

################################################################################
                     [1m Learning iteration 2749/4000 [0m

                       Computation: 3262 steps/s (collection: 0.437s, learning 2.074s)
               Value function loss: 101846.3907
                    Surrogate loss: 0.0126
             Mean action noise std: 0.90
                       Mean reward: 9720.17
               Mean episode length: 414.00
                 Mean success rate: 80.50
                  Mean reward/step: 23.31
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 2.51s
                        Total time: 7079.78s
                               ETA: 3220.7s

################################################################################
                     [1m Learning iteration 2750/4000 [0m

                       Computation: 3243 steps/s (collection: 0.479s, learning 2.047s)
               Value function loss: 69464.8174
                    Surrogate loss: 0.0135
             Mean action noise std: 0.90
                       Mean reward: 9870.59
               Mean episode length: 412.68
                 Mean success rate: 80.50
                  Mean reward/step: 23.53
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 22536192
                    Iteration time: 2.53s
                        Total time: 7082.30s
                               ETA: 3218.1s

################################################################################
                     [1m Learning iteration 2751/4000 [0m

                       Computation: 3177 steps/s (collection: 0.474s, learning 2.104s)
               Value function loss: 100529.2415
                    Surrogate loss: 0.0140
             Mean action noise std: 0.90
                       Mean reward: 9668.94
               Mean episode length: 404.56
                 Mean success rate: 78.50
                  Mean reward/step: 23.50
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 2.58s
                        Total time: 7084.88s
                               ETA: 3215.5s

################################################################################
                     [1m Learning iteration 2752/4000 [0m

                       Computation: 3162 steps/s (collection: 0.504s, learning 2.086s)
               Value function loss: 112905.6781
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 9523.40
               Mean episode length: 396.15
                 Mean success rate: 77.50
                  Mean reward/step: 23.00
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 22552576
                    Iteration time: 2.59s
                        Total time: 7087.47s
                               ETA: 3212.9s

################################################################################
                     [1m Learning iteration 2753/4000 [0m

                       Computation: 3207 steps/s (collection: 0.467s, learning 2.087s)
               Value function loss: 107191.7685
                    Surrogate loss: 0.0127
             Mean action noise std: 0.90
                       Mean reward: 10083.10
               Mean episode length: 414.60
                 Mean success rate: 81.50
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 2.55s
                        Total time: 7090.03s
                               ETA: 3210.3s

################################################################################
                     [1m Learning iteration 2754/4000 [0m

                       Computation: 3182 steps/s (collection: 0.461s, learning 2.112s)
               Value function loss: 68341.3904
                    Surrogate loss: 0.0150
             Mean action noise std: 0.90
                       Mean reward: 9853.63
               Mean episode length: 407.85
                 Mean success rate: 80.00
                  Mean reward/step: 22.69
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22568960
                    Iteration time: 2.57s
                        Total time: 7092.60s
                               ETA: 3207.8s

################################################################################
                     [1m Learning iteration 2755/4000 [0m

                       Computation: 3252 steps/s (collection: 0.468s, learning 2.050s)
               Value function loss: 85480.0962
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 9850.65
               Mean episode length: 407.29
                 Mean success rate: 79.50
                  Mean reward/step: 23.31
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 2.52s
                        Total time: 7095.12s
                               ETA: 3205.2s

################################################################################
                     [1m Learning iteration 2756/4000 [0m

                       Computation: 3160 steps/s (collection: 0.499s, learning 2.094s)
               Value function loss: 64683.4457
                    Surrogate loss: 0.0090
             Mean action noise std: 0.90
                       Mean reward: 9908.14
               Mean episode length: 407.29
                 Mean success rate: 79.50
                  Mean reward/step: 23.33
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 22585344
                    Iteration time: 2.59s
                        Total time: 7097.71s
                               ETA: 3202.6s

################################################################################
                     [1m Learning iteration 2757/4000 [0m

                       Computation: 3229 steps/s (collection: 0.454s, learning 2.083s)
               Value function loss: 101850.9645
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 10140.27
               Mean episode length: 414.75
                 Mean success rate: 80.50
                  Mean reward/step: 23.63
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 2.54s
                        Total time: 7100.25s
                               ETA: 3200.0s

################################################################################
                     [1m Learning iteration 2758/4000 [0m

                       Computation: 3245 steps/s (collection: 0.466s, learning 2.058s)
               Value function loss: 76062.2641
                    Surrogate loss: 0.0083
             Mean action noise std: 0.90
                       Mean reward: 9984.96
               Mean episode length: 408.01
                 Mean success rate: 79.00
                  Mean reward/step: 23.11
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22601728
                    Iteration time: 2.52s
                        Total time: 7102.77s
                               ETA: 3197.4s

################################################################################
                     [1m Learning iteration 2759/4000 [0m

                       Computation: 3249 steps/s (collection: 0.436s, learning 2.085s)
               Value function loss: 78241.9281
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 10023.91
               Mean episode length: 411.32
                 Mean success rate: 79.50
                  Mean reward/step: 22.85
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 2.52s
                        Total time: 7105.29s
                               ETA: 3194.8s

################################################################################
                     [1m Learning iteration 2760/4000 [0m

                       Computation: 3175 steps/s (collection: 0.475s, learning 2.105s)
               Value function loss: 71028.4224
                    Surrogate loss: 0.0091
             Mean action noise std: 0.90
                       Mean reward: 9725.04
               Mean episode length: 403.06
                 Mean success rate: 78.00
                  Mean reward/step: 22.96
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22618112
                    Iteration time: 2.58s
                        Total time: 7107.87s
                               ETA: 3192.2s

################################################################################
                     [1m Learning iteration 2761/4000 [0m

                       Computation: 3279 steps/s (collection: 0.431s, learning 2.067s)
               Value function loss: 97626.5937
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 9755.08
               Mean episode length: 407.46
                 Mean success rate: 79.50
                  Mean reward/step: 23.14
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 2.50s
                        Total time: 7110.37s
                               ETA: 3189.6s

################################################################################
                     [1m Learning iteration 2762/4000 [0m

                       Computation: 3169 steps/s (collection: 0.462s, learning 2.122s)
               Value function loss: 119406.3527
                    Surrogate loss: 0.0129
             Mean action noise std: 0.90
                       Mean reward: 10027.31
               Mean episode length: 418.25
                 Mean success rate: 82.00
                  Mean reward/step: 22.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22634496
                    Iteration time: 2.58s
                        Total time: 7112.95s
                               ETA: 3187.1s

################################################################################
                     [1m Learning iteration 2763/4000 [0m

                       Computation: 3260 steps/s (collection: 0.447s, learning 2.066s)
               Value function loss: 108123.8755
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 9971.52
               Mean episode length: 415.36
                 Mean success rate: 80.50
                  Mean reward/step: 21.86
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 2.51s
                        Total time: 7115.47s
                               ETA: 3184.5s

################################################################################
                     [1m Learning iteration 2764/4000 [0m

                       Computation: 3190 steps/s (collection: 0.507s, learning 2.061s)
               Value function loss: 97893.2547
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 9703.03
               Mean episode length: 410.06
                 Mean success rate: 79.50
                  Mean reward/step: 21.76
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 22650880
                    Iteration time: 2.57s
                        Total time: 7118.03s
                               ETA: 3181.9s

################################################################################
                     [1m Learning iteration 2765/4000 [0m

                       Computation: 3246 steps/s (collection: 0.445s, learning 2.078s)
               Value function loss: 62425.0556
                    Surrogate loss: 0.0094
             Mean action noise std: 0.90
                       Mean reward: 9566.42
               Mean episode length: 405.54
                 Mean success rate: 78.50
                  Mean reward/step: 22.10
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 2.52s
                        Total time: 7120.56s
                               ETA: 3179.3s

################################################################################
                     [1m Learning iteration 2766/4000 [0m

                       Computation: 3206 steps/s (collection: 0.469s, learning 2.086s)
               Value function loss: 84186.3262
                    Surrogate loss: 0.0121
             Mean action noise std: 0.90
                       Mean reward: 9487.94
               Mean episode length: 402.92
                 Mean success rate: 78.00
                  Mean reward/step: 23.48
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22667264
                    Iteration time: 2.55s
                        Total time: 7123.11s
                               ETA: 3176.7s

################################################################################
                     [1m Learning iteration 2767/4000 [0m

                       Computation: 3145 steps/s (collection: 0.515s, learning 2.090s)
               Value function loss: 116860.1557
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 9424.03
               Mean episode length: 407.92
                 Mean success rate: 79.00
                  Mean reward/step: 23.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 2.60s
                        Total time: 7125.72s
                               ETA: 3174.1s

################################################################################
                     [1m Learning iteration 2768/4000 [0m

                       Computation: 3212 steps/s (collection: 0.468s, learning 2.082s)
               Value function loss: 95600.4727
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9193.71
               Mean episode length: 405.99
                 Mean success rate: 78.50
                  Mean reward/step: 22.96
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22683648
                    Iteration time: 2.55s
                        Total time: 7128.27s
                               ETA: 3171.6s

################################################################################
                     [1m Learning iteration 2769/4000 [0m

                       Computation: 3178 steps/s (collection: 0.487s, learning 2.091s)
               Value function loss: 94124.6027
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 9552.04
               Mean episode length: 417.57
                 Mean success rate: 81.00
                  Mean reward/step: 22.27
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 2.58s
                        Total time: 7130.84s
                               ETA: 3169.0s

################################################################################
                     [1m Learning iteration 2770/4000 [0m

                       Computation: 3213 steps/s (collection: 0.473s, learning 2.076s)
               Value function loss: 66901.2163
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 9562.15
               Mean episode length: 419.31
                 Mean success rate: 80.50
                  Mean reward/step: 22.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22700032
                    Iteration time: 2.55s
                        Total time: 7133.39s
                               ETA: 3166.4s

################################################################################
                     [1m Learning iteration 2771/4000 [0m

                       Computation: 3256 steps/s (collection: 0.457s, learning 2.058s)
               Value function loss: 98131.2667
                    Surrogate loss: 0.0128
             Mean action noise std: 0.90
                       Mean reward: 8979.60
               Mean episode length: 397.94
                 Mean success rate: 75.50
                  Mean reward/step: 23.55
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 2.52s
                        Total time: 7135.91s
                               ETA: 3163.8s

################################################################################
                     [1m Learning iteration 2772/4000 [0m

                       Computation: 3189 steps/s (collection: 0.474s, learning 2.094s)
               Value function loss: 69134.2877
                    Surrogate loss: 0.0126
             Mean action noise std: 0.90
                       Mean reward: 8918.51
               Mean episode length: 398.19
                 Mean success rate: 75.50
                  Mean reward/step: 23.75
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 22716416
                    Iteration time: 2.57s
                        Total time: 7138.48s
                               ETA: 3161.2s

################################################################################
                     [1m Learning iteration 2773/4000 [0m

                       Computation: 3236 steps/s (collection: 0.472s, learning 2.059s)
               Value function loss: 73333.7624
                    Surrogate loss: 0.0118
             Mean action noise std: 0.90
                       Mean reward: 8989.13
               Mean episode length: 399.28
                 Mean success rate: 76.00
                  Mean reward/step: 24.48
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 2.53s
                        Total time: 7141.01s
                               ETA: 3158.6s

################################################################################
                     [1m Learning iteration 2774/4000 [0m

                       Computation: 3216 steps/s (collection: 0.459s, learning 2.088s)
               Value function loss: 98759.4612
                    Surrogate loss: 0.0126
             Mean action noise std: 0.90
                       Mean reward: 9159.71
               Mean episode length: 404.11
                 Mean success rate: 77.00
                  Mean reward/step: 24.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22732800
                    Iteration time: 2.55s
                        Total time: 7143.56s
                               ETA: 3156.0s

################################################################################
                     [1m Learning iteration 2775/4000 [0m

                       Computation: 3141 steps/s (collection: 0.475s, learning 2.133s)
               Value function loss: 60135.6974
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 9044.80
               Mean episode length: 401.40
                 Mean success rate: 75.50
                  Mean reward/step: 23.54
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 2.61s
                        Total time: 7146.16s
                               ETA: 3153.5s

################################################################################
                     [1m Learning iteration 2776/4000 [0m

                       Computation: 3237 steps/s (collection: 0.447s, learning 2.083s)
               Value function loss: 77784.4400
                    Surrogate loss: 0.0119
             Mean action noise std: 0.90
                       Mean reward: 9225.40
               Mean episode length: 410.35
                 Mean success rate: 77.50
                  Mean reward/step: 24.04
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22749184
                    Iteration time: 2.53s
                        Total time: 7148.69s
                               ETA: 3150.9s

################################################################################
                     [1m Learning iteration 2777/4000 [0m

                       Computation: 3272 steps/s (collection: 0.437s, learning 2.066s)
               Value function loss: 88189.4014
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 9160.89
               Mean episode length: 405.40
                 Mean success rate: 76.00
                  Mean reward/step: 24.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 2.50s
                        Total time: 7151.20s
                               ETA: 3148.3s

################################################################################
                     [1m Learning iteration 2778/4000 [0m

                       Computation: 3198 steps/s (collection: 0.482s, learning 2.079s)
               Value function loss: 121338.3986
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 9403.81
               Mean episode length: 405.90
                 Mean success rate: 76.50
                  Mean reward/step: 24.30
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22765568
                    Iteration time: 2.56s
                        Total time: 7153.76s
                               ETA: 3145.7s

################################################################################
                     [1m Learning iteration 2779/4000 [0m

                       Computation: 3215 steps/s (collection: 0.505s, learning 2.043s)
               Value function loss: 114679.7424
                    Surrogate loss: 0.0088
             Mean action noise std: 0.90
                       Mean reward: 9494.02
               Mean episode length: 408.35
                 Mean success rate: 77.00
                  Mean reward/step: 23.16
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 2.55s
                        Total time: 7156.31s
                               ETA: 3143.1s

################################################################################
                     [1m Learning iteration 2780/4000 [0m

                       Computation: 3218 steps/s (collection: 0.458s, learning 2.087s)
               Value function loss: 108079.4362
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 9377.29
               Mean episode length: 403.69
                 Mean success rate: 76.00
                  Mean reward/step: 22.87
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 22781952
                    Iteration time: 2.55s
                        Total time: 7158.85s
                               ETA: 3140.5s

################################################################################
                     [1m Learning iteration 2781/4000 [0m

                       Computation: 3217 steps/s (collection: 0.473s, learning 2.073s)
               Value function loss: 75851.5425
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 9454.53
               Mean episode length: 406.75
                 Mean success rate: 77.00
                  Mean reward/step: 23.21
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 2.55s
                        Total time: 7161.40s
                               ETA: 3137.9s

################################################################################
                     [1m Learning iteration 2782/4000 [0m

                       Computation: 3239 steps/s (collection: 0.482s, learning 2.047s)
               Value function loss: 100049.7510
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 9573.23
               Mean episode length: 410.62
                 Mean success rate: 77.50
                  Mean reward/step: 23.86
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 22798336
                    Iteration time: 2.53s
                        Total time: 7163.93s
                               ETA: 3135.3s

################################################################################
                     [1m Learning iteration 2783/4000 [0m

                       Computation: 3269 steps/s (collection: 0.448s, learning 2.058s)
               Value function loss: 101689.8886
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 9342.40
               Mean episode length: 401.31
                 Mean success rate: 75.50
                  Mean reward/step: 24.04
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 2.51s
                        Total time: 7166.43s
                               ETA: 3132.7s

################################################################################
                     [1m Learning iteration 2784/4000 [0m

                       Computation: 3196 steps/s (collection: 0.529s, learning 2.033s)
               Value function loss: 115471.1854
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 9498.53
               Mean episode length: 407.57
                 Mean success rate: 77.00
                  Mean reward/step: 23.65
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 22814720
                    Iteration time: 2.56s
                        Total time: 7168.99s
                               ETA: 3130.2s

################################################################################
                     [1m Learning iteration 2785/4000 [0m

                       Computation: 3235 steps/s (collection: 0.473s, learning 2.058s)
               Value function loss: 69372.9626
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 9524.57
               Mean episode length: 407.66
                 Mean success rate: 77.00
                  Mean reward/step: 23.60
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 2.53s
                        Total time: 7171.53s
                               ETA: 3127.6s

################################################################################
                     [1m Learning iteration 2786/4000 [0m

                       Computation: 3219 steps/s (collection: 0.436s, learning 2.109s)
               Value function loss: 64687.1588
                    Surrogate loss: 0.0094
             Mean action noise std: 0.90
                       Mean reward: 9628.04
               Mean episode length: 411.24
                 Mean success rate: 78.00
                  Mean reward/step: 24.34
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 22831104
                    Iteration time: 2.54s
                        Total time: 7174.07s
                               ETA: 3125.0s

################################################################################
                     [1m Learning iteration 2787/4000 [0m

                       Computation: 3264 steps/s (collection: 0.476s, learning 2.034s)
               Value function loss: 78366.0710
                    Surrogate loss: 0.0127
             Mean action noise std: 0.90
                       Mean reward: 9537.63
               Mean episode length: 410.56
                 Mean success rate: 77.50
                  Mean reward/step: 24.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 2.51s
                        Total time: 7176.58s
                               ETA: 3122.4s

################################################################################
                     [1m Learning iteration 2788/4000 [0m

                       Computation: 3288 steps/s (collection: 0.436s, learning 2.055s)
               Value function loss: 45643.9267
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 9302.67
               Mean episode length: 401.85
                 Mean success rate: 76.00
                  Mean reward/step: 24.91
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 22847488
                    Iteration time: 2.49s
                        Total time: 7179.07s
                               ETA: 3119.8s

################################################################################
                     [1m Learning iteration 2789/4000 [0m

                       Computation: 3159 steps/s (collection: 0.499s, learning 2.094s)
               Value function loss: 72446.6643
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 8975.36
               Mean episode length: 391.10
                 Mean success rate: 74.00
                  Mean reward/step: 25.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 2.59s
                        Total time: 7181.66s
                               ETA: 3117.2s

################################################################################
                     [1m Learning iteration 2790/4000 [0m

                       Computation: 3234 steps/s (collection: 0.474s, learning 2.059s)
               Value function loss: 119767.3183
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 9333.46
               Mean episode length: 398.30
                 Mean success rate: 76.00
                  Mean reward/step: 25.03
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 22863872
                    Iteration time: 2.53s
                        Total time: 7184.20s
                               ETA: 3114.6s

################################################################################
                     [1m Learning iteration 2791/4000 [0m

                       Computation: 3250 steps/s (collection: 0.482s, learning 2.038s)
               Value function loss: 71612.3395
                    Surrogate loss: 0.0117
             Mean action noise std: 0.90
                       Mean reward: 9341.35
               Mean episode length: 397.48
                 Mean success rate: 76.50
                  Mean reward/step: 24.65
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 2.52s
                        Total time: 7186.72s
                               ETA: 3112.0s

################################################################################
                     [1m Learning iteration 2792/4000 [0m

                       Computation: 3235 steps/s (collection: 0.495s, learning 2.037s)
               Value function loss: 96390.2612
                    Surrogate loss: 0.0117
             Mean action noise std: 0.90
                       Mean reward: 9131.93
               Mean episode length: 385.67
                 Mean success rate: 74.50
                  Mean reward/step: 24.48
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 22880256
                    Iteration time: 2.53s
                        Total time: 7189.25s
                               ETA: 3109.4s

################################################################################
                     [1m Learning iteration 2793/4000 [0m

                       Computation: 3218 steps/s (collection: 0.498s, learning 2.047s)
               Value function loss: 77833.7271
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 9360.01
               Mean episode length: 390.23
                 Mean success rate: 75.50
                  Mean reward/step: 24.44
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 2.55s
                        Total time: 7191.79s
                               ETA: 3106.8s

################################################################################
                     [1m Learning iteration 2794/4000 [0m

                       Computation: 3214 steps/s (collection: 0.480s, learning 2.069s)
               Value function loss: 143743.1596
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9582.32
               Mean episode length: 394.75
                 Mean success rate: 76.50
                  Mean reward/step: 24.23
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 22896640
                    Iteration time: 2.55s
                        Total time: 7194.34s
                               ETA: 3104.2s

################################################################################
                     [1m Learning iteration 2795/4000 [0m

                       Computation: 3206 steps/s (collection: 0.467s, learning 2.088s)
               Value function loss: 128742.2797
                    Surrogate loss: 0.0091
             Mean action noise std: 0.90
                       Mean reward: 9530.14
               Mean episode length: 389.88
                 Mean success rate: 76.00
                  Mean reward/step: 23.36
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 2.56s
                        Total time: 7196.90s
                               ETA: 3101.7s

################################################################################
                     [1m Learning iteration 2796/4000 [0m

                       Computation: 3237 steps/s (collection: 0.429s, learning 2.102s)
               Value function loss: 105088.6817
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 9166.02
               Mean episode length: 374.58
                 Mean success rate: 72.50
                  Mean reward/step: 22.49
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 22913024
                    Iteration time: 2.53s
                        Total time: 7199.43s
                               ETA: 3099.1s

################################################################################
                     [1m Learning iteration 2797/4000 [0m

                       Computation: 3255 steps/s (collection: 0.448s, learning 2.069s)
               Value function loss: 92212.2346
                    Surrogate loss: 0.0092
             Mean action noise std: 0.90
                       Mean reward: 9381.90
               Mean episode length: 382.25
                 Mean success rate: 73.00
                  Mean reward/step: 23.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 2.52s
                        Total time: 7201.95s
                               ETA: 3096.5s

################################################################################
                     [1m Learning iteration 2798/4000 [0m

                       Computation: 3102 steps/s (collection: 0.495s, learning 2.145s)
               Value function loss: 99737.6930
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 9664.08
               Mean episode length: 392.43
                 Mean success rate: 75.50
                  Mean reward/step: 23.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22929408
                    Iteration time: 2.64s
                        Total time: 7204.59s
                               ETA: 3093.9s

################################################################################
                     [1m Learning iteration 2799/4000 [0m

                       Computation: 3248 steps/s (collection: 0.431s, learning 2.091s)
               Value function loss: 93510.5392
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 9659.87
               Mean episode length: 392.39
                 Mean success rate: 75.50
                  Mean reward/step: 23.14
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 2.52s
                        Total time: 7207.11s
                               ETA: 3091.3s

################################################################################
                     [1m Learning iteration 2800/4000 [0m

                       Computation: 3219 steps/s (collection: 0.475s, learning 2.070s)
               Value function loss: 115589.8112
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 9899.75
               Mean episode length: 406.43
                 Mean success rate: 77.50
                  Mean reward/step: 22.75
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 22945792
                    Iteration time: 2.54s
                        Total time: 7209.65s
                               ETA: 3088.7s

################################################################################
                     [1m Learning iteration 2801/4000 [0m

                       Computation: 3178 steps/s (collection: 0.457s, learning 2.121s)
               Value function loss: 63826.5643
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 9880.34
               Mean episode length: 407.27
                 Mean success rate: 77.00
                  Mean reward/step: 23.16
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 2.58s
                        Total time: 7212.23s
                               ETA: 3086.2s

################################################################################
                     [1m Learning iteration 2802/4000 [0m

                       Computation: 3174 steps/s (collection: 0.483s, learning 2.097s)
               Value function loss: 77807.6659
                    Surrogate loss: 0.0085
             Mean action noise std: 0.90
                       Mean reward: 9749.80
               Mean episode length: 403.72
                 Mean success rate: 76.50
                  Mean reward/step: 24.86
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22962176
                    Iteration time: 2.58s
                        Total time: 7214.81s
                               ETA: 3083.6s

################################################################################
                     [1m Learning iteration 2803/4000 [0m

                       Computation: 3352 steps/s (collection: 0.428s, learning 2.015s)
               Value function loss: 52102.5962
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 9541.65
               Mean episode length: 399.42
                 Mean success rate: 75.00
                  Mean reward/step: 25.37
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 2.44s
                        Total time: 7217.25s
                               ETA: 3081.0s

################################################################################
                     [1m Learning iteration 2804/4000 [0m

                       Computation: 3216 steps/s (collection: 0.453s, learning 2.094s)
               Value function loss: 56107.2885
                    Surrogate loss: 0.0084
             Mean action noise std: 0.90
                       Mean reward: 9369.25
               Mean episode length: 390.68
                 Mean success rate: 73.50
                  Mean reward/step: 25.54
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 22978560
                    Iteration time: 2.55s
                        Total time: 7219.80s
                               ETA: 3078.4s

################################################################################
                     [1m Learning iteration 2805/4000 [0m

                       Computation: 3136 steps/s (collection: 0.473s, learning 2.139s)
               Value function loss: 120457.7473
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 9595.37
               Mean episode length: 395.14
                 Mean success rate: 74.50
                  Mean reward/step: 25.91
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 2.61s
                        Total time: 7222.41s
                               ETA: 3075.8s

################################################################################
                     [1m Learning iteration 2806/4000 [0m

                       Computation: 3164 steps/s (collection: 0.461s, learning 2.127s)
               Value function loss: 67327.4405
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 9616.86
               Mean episode length: 396.09
                 Mean success rate: 75.00
                  Mean reward/step: 24.79
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22994944
                    Iteration time: 2.59s
                        Total time: 7225.00s
                               ETA: 3073.3s

################################################################################
                     [1m Learning iteration 2807/4000 [0m

                       Computation: 3097 steps/s (collection: 0.502s, learning 2.143s)
               Value function loss: 103640.2046
                    Surrogate loss: 0.0126
             Mean action noise std: 0.90
                       Mean reward: 9961.28
               Mean episode length: 406.89
                 Mean success rate: 78.00
                  Mean reward/step: 25.02
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 2.64s
                        Total time: 7227.65s
                               ETA: 3070.7s

################################################################################
                     [1m Learning iteration 2808/4000 [0m

                       Computation: 3133 steps/s (collection: 0.500s, learning 2.114s)
               Value function loss: 86618.8646
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 10254.91
               Mean episode length: 413.39
                 Mean success rate: 79.50
                  Mean reward/step: 25.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23011328
                    Iteration time: 2.61s
                        Total time: 7230.26s
                               ETA: 3068.2s

################################################################################
                     [1m Learning iteration 2809/4000 [0m

                       Computation: 3168 steps/s (collection: 0.479s, learning 2.106s)
               Value function loss: 108499.2379
                    Surrogate loss: 0.0125
             Mean action noise std: 0.90
                       Mean reward: 9893.57
               Mean episode length: 408.25
                 Mean success rate: 77.00
                  Mean reward/step: 25.58
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 2.59s
                        Total time: 7232.84s
                               ETA: 3065.6s

################################################################################
                     [1m Learning iteration 2810/4000 [0m

                       Computation: 3221 steps/s (collection: 0.465s, learning 2.078s)
               Value function loss: 140170.8572
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 9816.55
               Mean episode length: 404.74
                 Mean success rate: 76.00
                  Mean reward/step: 24.48
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 23027712
                    Iteration time: 2.54s
                        Total time: 7235.39s
                               ETA: 3063.0s

################################################################################
                     [1m Learning iteration 2811/4000 [0m

                       Computation: 3115 steps/s (collection: 0.506s, learning 2.123s)
               Value function loss: 133361.5621
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 10195.20
               Mean episode length: 415.82
                 Mean success rate: 79.00
                  Mean reward/step: 23.52
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 2.63s
                        Total time: 7238.02s
                               ETA: 3060.5s

################################################################################
                     [1m Learning iteration 2812/4000 [0m

                       Computation: 3011 steps/s (collection: 0.547s, learning 2.173s)
               Value function loss: 80557.1411
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 10352.78
               Mean episode length: 421.46
                 Mean success rate: 80.50
                  Mean reward/step: 23.42
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23044096
                    Iteration time: 2.72s
                        Total time: 7240.74s
                               ETA: 3057.9s

################################################################################
                     [1m Learning iteration 2813/4000 [0m

                       Computation: 3104 steps/s (collection: 0.524s, learning 2.115s)
               Value function loss: 95054.8938
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 10511.04
               Mean episode length: 426.12
                 Mean success rate: 82.00
                  Mean reward/step: 23.85
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 2.64s
                        Total time: 7243.38s
                               ETA: 3055.4s

################################################################################
                     [1m Learning iteration 2814/4000 [0m

                       Computation: 3160 steps/s (collection: 0.476s, learning 2.116s)
               Value function loss: 99325.1857
                    Surrogate loss: 0.0091
             Mean action noise std: 0.90
                       Mean reward: 10122.01
               Mean episode length: 416.75
                 Mean success rate: 79.00
                  Mean reward/step: 23.74
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 23060480
                    Iteration time: 2.59s
                        Total time: 7245.97s
                               ETA: 3052.8s

################################################################################
                     [1m Learning iteration 2815/4000 [0m

                       Computation: 3112 steps/s (collection: 0.474s, learning 2.159s)
               Value function loss: 96687.5611
                    Surrogate loss: 0.0093
             Mean action noise std: 0.90
                       Mean reward: 10170.43
               Mean episode length: 418.47
                 Mean success rate: 79.50
                  Mean reward/step: 24.13
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 2.63s
                        Total time: 7248.60s
                               ETA: 3050.3s

################################################################################
                     [1m Learning iteration 2816/4000 [0m

                       Computation: 3130 steps/s (collection: 0.497s, learning 2.120s)
               Value function loss: 81826.4580
                    Surrogate loss: 0.0087
             Mean action noise std: 0.90
                       Mean reward: 10144.94
               Mean episode length: 422.63
                 Mean success rate: 80.00
                  Mean reward/step: 24.39
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 23076864
                    Iteration time: 2.62s
                        Total time: 7251.22s
                               ETA: 3047.7s

################################################################################
                     [1m Learning iteration 2817/4000 [0m

                       Computation: 3196 steps/s (collection: 0.519s, learning 2.044s)
               Value function loss: 92775.1916
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 9892.77
               Mean episode length: 412.55
                 Mean success rate: 78.50
                  Mean reward/step: 24.81
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 2.56s
                        Total time: 7253.78s
                               ETA: 3045.1s

################################################################################
                     [1m Learning iteration 2818/4000 [0m

                       Computation: 3247 steps/s (collection: 0.440s, learning 2.082s)
               Value function loss: 68768.2453
                    Surrogate loss: 0.0121
             Mean action noise std: 0.90
                       Mean reward: 9900.99
               Mean episode length: 404.98
                 Mean success rate: 78.50
                  Mean reward/step: 24.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23093248
                    Iteration time: 2.52s
                        Total time: 7256.30s
                               ETA: 3042.6s

################################################################################
                     [1m Learning iteration 2819/4000 [0m

                       Computation: 3182 steps/s (collection: 0.511s, learning 2.064s)
               Value function loss: 80443.0215
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 9488.63
               Mean episode length: 391.25
                 Mean success rate: 76.00
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.57s
                        Total time: 7258.88s
                               ETA: 3040.0s

################################################################################
                     [1m Learning iteration 2820/4000 [0m

                       Computation: 3211 steps/s (collection: 0.478s, learning 2.073s)
               Value function loss: 66315.2528
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 9136.21
               Mean episode length: 382.20
                 Mean success rate: 73.50
                  Mean reward/step: 25.31
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 23109632
                    Iteration time: 2.55s
                        Total time: 7261.43s
                               ETA: 3037.4s

################################################################################
                     [1m Learning iteration 2821/4000 [0m

                       Computation: 3291 steps/s (collection: 0.458s, learning 2.031s)
               Value function loss: 133600.6717
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 8998.28
               Mean episode length: 372.60
                 Mean success rate: 72.00
                  Mean reward/step: 25.39
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 2.49s
                        Total time: 7263.92s
                               ETA: 3034.8s

################################################################################
                     [1m Learning iteration 2822/4000 [0m

                       Computation: 3162 steps/s (collection: 0.492s, learning 2.098s)
               Value function loss: 67035.6978
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 8917.54
               Mean episode length: 368.82
                 Mean success rate: 71.50
                  Mean reward/step: 24.85
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23126016
                    Iteration time: 2.59s
                        Total time: 7266.51s
                               ETA: 3032.2s

################################################################################
                     [1m Learning iteration 2823/4000 [0m

                       Computation: 3227 steps/s (collection: 0.485s, learning 2.054s)
               Value function loss: 105056.4629
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 8926.47
               Mean episode length: 370.05
                 Mean success rate: 71.00
                  Mean reward/step: 24.38
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 2.54s
                        Total time: 7269.05s
                               ETA: 3029.6s

################################################################################
                     [1m Learning iteration 2824/4000 [0m

                       Computation: 3158 steps/s (collection: 0.529s, learning 2.064s)
               Value function loss: 88556.0930
                    Surrogate loss: 0.0094
             Mean action noise std: 0.90
                       Mean reward: 9238.12
               Mean episode length: 377.37
                 Mean success rate: 73.00
                  Mean reward/step: 24.57
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 23142400
                    Iteration time: 2.59s
                        Total time: 7271.64s
                               ETA: 3027.1s

################################################################################
                     [1m Learning iteration 2825/4000 [0m

                       Computation: 3215 steps/s (collection: 0.486s, learning 2.062s)
               Value function loss: 143870.5613
                    Surrogate loss: 0.0128
             Mean action noise std: 0.90
                       Mean reward: 9654.78
               Mean episode length: 390.62
                 Mean success rate: 76.00
                  Mean reward/step: 24.59
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 2.55s
                        Total time: 7274.19s
                               ETA: 3024.5s

################################################################################
                     [1m Learning iteration 2826/4000 [0m

                       Computation: 3239 steps/s (collection: 0.469s, learning 2.060s)
               Value function loss: 83858.1962
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 9455.31
               Mean episode length: 383.12
                 Mean success rate: 74.00
                  Mean reward/step: 24.08
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23158784
                    Iteration time: 2.53s
                        Total time: 7276.72s
                               ETA: 3021.9s

################################################################################
                     [1m Learning iteration 2827/4000 [0m

                       Computation: 3245 steps/s (collection: 0.477s, learning 2.047s)
               Value function loss: 136304.7402
                    Surrogate loss: 0.0139
             Mean action noise std: 0.90
                       Mean reward: 9554.94
               Mean episode length: 388.78
                 Mean success rate: 74.50
                  Mean reward/step: 24.10
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 2.52s
                        Total time: 7279.24s
                               ETA: 3019.3s

################################################################################
                     [1m Learning iteration 2828/4000 [0m

                       Computation: 3204 steps/s (collection: 0.489s, learning 2.067s)
               Value function loss: 81536.7732
                    Surrogate loss: 0.0108
             Mean action noise std: 0.90
                       Mean reward: 9574.89
               Mean episode length: 391.19
                 Mean success rate: 75.00
                  Mean reward/step: 24.00
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 23175168
                    Iteration time: 2.56s
                        Total time: 7281.80s
                               ETA: 3016.7s

################################################################################
                     [1m Learning iteration 2829/4000 [0m

                       Computation: 3203 steps/s (collection: 0.482s, learning 2.074s)
               Value function loss: 117700.3552
                    Surrogate loss: 0.0105
             Mean action noise std: 0.90
                       Mean reward: 9763.77
               Mean episode length: 393.82
                 Mean success rate: 76.00
                  Mean reward/step: 23.58
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 2.56s
                        Total time: 7284.35s
                               ETA: 3014.1s

################################################################################
                     [1m Learning iteration 2830/4000 [0m

                       Computation: 3170 steps/s (collection: 0.467s, learning 2.116s)
               Value function loss: 102614.2020
                    Surrogate loss: 0.0105
             Mean action noise std: 0.90
                       Mean reward: 9896.02
               Mean episode length: 400.67
                 Mean success rate: 77.00
                  Mean reward/step: 23.24
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23191552
                    Iteration time: 2.58s
                        Total time: 7286.94s
                               ETA: 3011.6s

################################################################################
                     [1m Learning iteration 2831/4000 [0m

                       Computation: 3074 steps/s (collection: 0.495s, learning 2.169s)
               Value function loss: 106715.9440
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 9701.55
               Mean episode length: 393.43
                 Mean success rate: 75.50
                  Mean reward/step: 23.79
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 2.66s
                        Total time: 7289.60s
                               ETA: 3009.0s

################################################################################
                     [1m Learning iteration 2832/4000 [0m

                       Computation: 3039 steps/s (collection: 0.551s, learning 2.145s)
               Value function loss: 74578.7914
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 9429.14
               Mean episode length: 383.70
                 Mean success rate: 73.50
                  Mean reward/step: 23.78
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23207936
                    Iteration time: 2.70s
                        Total time: 7292.30s
                               ETA: 3006.5s

################################################################################
                     [1m Learning iteration 2833/4000 [0m

                       Computation: 3168 steps/s (collection: 0.462s, learning 2.123s)
               Value function loss: 67219.3879
                    Surrogate loss: 0.0082
             Mean action noise std: 0.90
                       Mean reward: 9162.41
               Mean episode length: 378.04
                 Mean success rate: 72.00
                  Mean reward/step: 24.30
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 2.59s
                        Total time: 7294.88s
                               ETA: 3003.9s

################################################################################
                     [1m Learning iteration 2834/4000 [0m

                       Computation: 3062 steps/s (collection: 0.508s, learning 2.167s)
               Value function loss: 80924.5721
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 8726.73
               Mean episode length: 366.81
                 Mean success rate: 69.00
                  Mean reward/step: 24.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 23224320
                    Iteration time: 2.67s
                        Total time: 7297.56s
                               ETA: 3001.4s

################################################################################
                     [1m Learning iteration 2835/4000 [0m

                       Computation: 3073 steps/s (collection: 0.541s, learning 2.124s)
               Value function loss: 93485.6071
                    Surrogate loss: 0.0086
             Mean action noise std: 0.90
                       Mean reward: 8790.59
               Mean episode length: 368.13
                 Mean success rate: 69.50
                  Mean reward/step: 24.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 2.66s
                        Total time: 7300.22s
                               ETA: 2998.9s

################################################################################
                     [1m Learning iteration 2836/4000 [0m

                       Computation: 3168 steps/s (collection: 0.476s, learning 2.110s)
               Value function loss: 96733.3723
                    Surrogate loss: 0.0100
             Mean action noise std: 0.90
                       Mean reward: 8844.41
               Mean episode length: 370.43
                 Mean success rate: 70.50
                  Mean reward/step: 24.82
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23240704
                    Iteration time: 2.59s
                        Total time: 7302.81s
                               ETA: 2996.3s

################################################################################
                     [1m Learning iteration 2837/4000 [0m

                       Computation: 3224 steps/s (collection: 0.476s, learning 2.065s)
               Value function loss: 105954.7761
                    Surrogate loss: 0.0079
             Mean action noise std: 0.90
                       Mean reward: 9009.42
               Mean episode length: 375.00
                 Mean success rate: 71.50
                  Mean reward/step: 23.57
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 2.54s
                        Total time: 7305.35s
                               ETA: 2993.7s

################################################################################
                     [1m Learning iteration 2838/4000 [0m

                       Computation: 3228 steps/s (collection: 0.442s, learning 2.095s)
               Value function loss: 84597.4692
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 8729.52
               Mean episode length: 369.96
                 Mean success rate: 69.50
                  Mean reward/step: 23.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 23257088
                    Iteration time: 2.54s
                        Total time: 7307.89s
                               ETA: 2991.1s

################################################################################
                     [1m Learning iteration 2839/4000 [0m

                       Computation: 3122 steps/s (collection: 0.476s, learning 2.147s)
               Value function loss: 82623.5815
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 8644.81
               Mean episode length: 364.87
                 Mean success rate: 68.50
                  Mean reward/step: 23.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 2.62s
                        Total time: 7310.51s
                               ETA: 2988.6s

################################################################################
                     [1m Learning iteration 2840/4000 [0m

                       Computation: 3117 steps/s (collection: 0.468s, learning 2.159s)
               Value function loss: 89503.3482
                    Surrogate loss: 0.0118
             Mean action noise std: 0.90
                       Mean reward: 8749.23
               Mean episode length: 366.27
                 Mean success rate: 69.00
                  Mean reward/step: 23.74
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 23273472
                    Iteration time: 2.63s
                        Total time: 7313.14s
                               ETA: 2986.0s

################################################################################
                     [1m Learning iteration 2841/4000 [0m

                       Computation: 3181 steps/s (collection: 0.470s, learning 2.105s)
               Value function loss: 119013.8451
                    Surrogate loss: 0.0077
             Mean action noise std: 0.90
                       Mean reward: 8861.17
               Mean episode length: 369.60
                 Mean success rate: 70.00
                  Mean reward/step: 23.02
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 2.57s
                        Total time: 7315.71s
                               ETA: 2983.4s

################################################################################
                     [1m Learning iteration 2842/4000 [0m

                       Computation: 3151 steps/s (collection: 0.499s, learning 2.100s)
               Value function loss: 81828.2037
                    Surrogate loss: 0.0084
             Mean action noise std: 0.90
                       Mean reward: 8185.74
               Mean episode length: 351.29
                 Mean success rate: 65.00
                  Mean reward/step: 22.84
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 23289856
                    Iteration time: 2.60s
                        Total time: 7318.31s
                               ETA: 2980.9s

################################################################################
                     [1m Learning iteration 2843/4000 [0m

                       Computation: 3136 steps/s (collection: 0.476s, learning 2.136s)
               Value function loss: 112780.7180
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 8512.27
               Mean episode length: 359.77
                 Mean success rate: 67.50
                  Mean reward/step: 22.85
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.61s
                        Total time: 7320.92s
                               ETA: 2978.3s

################################################################################
                     [1m Learning iteration 2844/4000 [0m

                       Computation: 3068 steps/s (collection: 0.496s, learning 2.173s)
               Value function loss: 103799.2776
                    Surrogate loss: 0.0118
             Mean action noise std: 0.90
                       Mean reward: 8608.95
               Mean episode length: 361.71
                 Mean success rate: 68.00
                  Mean reward/step: 22.80
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23306240
                    Iteration time: 2.67s
                        Total time: 7323.59s
                               ETA: 2975.8s

################################################################################
                     [1m Learning iteration 2845/4000 [0m

                       Computation: 3153 steps/s (collection: 0.485s, learning 2.113s)
               Value function loss: 117801.2441
                    Surrogate loss: 0.0099
             Mean action noise std: 0.90
                       Mean reward: 8665.76
               Mean episode length: 360.12
                 Mean success rate: 67.50
                  Mean reward/step: 22.54
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 2.60s
                        Total time: 7326.19s
                               ETA: 2973.2s

################################################################################
                     [1m Learning iteration 2846/4000 [0m

                       Computation: 3184 steps/s (collection: 0.488s, learning 2.085s)
               Value function loss: 100465.8102
                    Surrogate loss: 0.0089
             Mean action noise std: 0.90
                       Mean reward: 8712.85
               Mean episode length: 362.58
                 Mean success rate: 67.50
                  Mean reward/step: 22.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 23322624
                    Iteration time: 2.57s
                        Total time: 7328.76s
                               ETA: 2970.6s

################################################################################
                     [1m Learning iteration 2847/4000 [0m

                       Computation: 3108 steps/s (collection: 0.478s, learning 2.158s)
               Value function loss: 98036.6718
                    Surrogate loss: 0.0132
             Mean action noise std: 0.90
                       Mean reward: 8853.26
               Mean episode length: 370.17
                 Mean success rate: 69.50
                  Mean reward/step: 22.39
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 2.64s
                        Total time: 7331.40s
                               ETA: 2968.1s

################################################################################
                     [1m Learning iteration 2848/4000 [0m

                       Computation: 3169 steps/s (collection: 0.469s, learning 2.115s)
               Value function loss: 63065.3581
                    Surrogate loss: 0.0135
             Mean action noise std: 0.90
                       Mean reward: 8869.67
               Mean episode length: 372.06
                 Mean success rate: 70.00
                  Mean reward/step: 23.35
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23339008
                    Iteration time: 2.58s
                        Total time: 7333.98s
                               ETA: 2965.5s

################################################################################
                     [1m Learning iteration 2849/4000 [0m

                       Computation: 3123 steps/s (collection: 0.512s, learning 2.111s)
               Value function loss: 65444.1471
                    Surrogate loss: 0.0089
             Mean action noise std: 0.90
                       Mean reward: 8773.51
               Mean episode length: 367.60
                 Mean success rate: 68.50
                  Mean reward/step: 24.53
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 2.62s
                        Total time: 7336.60s
                               ETA: 2963.0s

################################################################################
                     [1m Learning iteration 2850/4000 [0m

                       Computation: 3195 steps/s (collection: 0.475s, learning 2.089s)
               Value function loss: 79167.3059
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 8777.00
               Mean episode length: 367.41
                 Mean success rate: 69.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23355392
                    Iteration time: 2.56s
                        Total time: 7339.17s
                               ETA: 2960.4s

################################################################################
                     [1m Learning iteration 2851/4000 [0m

                       Computation: 3222 steps/s (collection: 0.450s, learning 2.092s)
               Value function loss: 71593.0799
                    Surrogate loss: 0.0085
             Mean action noise std: 0.90
                       Mean reward: 9032.38
               Mean episode length: 377.58
                 Mean success rate: 71.50
                  Mean reward/step: 25.32
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 2.54s
                        Total time: 7341.71s
                               ETA: 2957.8s

################################################################################
                     [1m Learning iteration 2852/4000 [0m

                       Computation: 3140 steps/s (collection: 0.513s, learning 2.095s)
               Value function loss: 127229.5619
                    Surrogate loss: 0.0085
             Mean action noise std: 0.90
                       Mean reward: 9282.50
               Mean episode length: 384.87
                 Mean success rate: 74.00
                  Mean reward/step: 24.76
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 23371776
                    Iteration time: 2.61s
                        Total time: 7344.32s
                               ETA: 2955.2s

################################################################################
                     [1m Learning iteration 2853/4000 [0m

                       Computation: 3120 steps/s (collection: 0.531s, learning 2.094s)
               Value function loss: 64936.9646
                    Surrogate loss: 0.0081
             Mean action noise std: 0.90
                       Mean reward: 9161.67
               Mean episode length: 382.62
                 Mean success rate: 73.00
                  Mean reward/step: 23.87
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 2.63s
                        Total time: 7346.94s
                               ETA: 2952.7s

################################################################################
                     [1m Learning iteration 2854/4000 [0m

                       Computation: 3142 steps/s (collection: 0.488s, learning 2.119s)
               Value function loss: 119829.1377
                    Surrogate loss: 0.0089
             Mean action noise std: 0.90
                       Mean reward: 9075.81
               Mean episode length: 382.72
                 Mean success rate: 74.00
                  Mean reward/step: 24.30
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 23388160
                    Iteration time: 2.61s
                        Total time: 7349.55s
                               ETA: 2950.1s

################################################################################
                     [1m Learning iteration 2855/4000 [0m

                       Computation: 3166 steps/s (collection: 0.486s, learning 2.101s)
               Value function loss: 75973.5374
                    Surrogate loss: 0.0078
             Mean action noise std: 0.90
                       Mean reward: 9142.65
               Mean episode length: 385.13
                 Mean success rate: 74.50
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 2.59s
                        Total time: 7352.14s
                               ETA: 2947.5s

################################################################################
                     [1m Learning iteration 2856/4000 [0m

                       Computation: 3093 steps/s (collection: 0.516s, learning 2.132s)
               Value function loss: 109015.5852
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 9340.88
               Mean episode length: 391.11
                 Mean success rate: 76.00
                  Mean reward/step: 25.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23404544
                    Iteration time: 2.65s
                        Total time: 7354.79s
                               ETA: 2945.0s

################################################################################
                     [1m Learning iteration 2857/4000 [0m

                       Computation: 3175 steps/s (collection: 0.488s, learning 2.092s)
               Value function loss: 88766.3010
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 9512.30
               Mean episode length: 395.83
                 Mean success rate: 77.50
                  Mean reward/step: 25.06
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 2.58s
                        Total time: 7357.37s
                               ETA: 2942.4s

################################################################################
                     [1m Learning iteration 2858/4000 [0m

                       Computation: 3167 steps/s (collection: 0.465s, learning 2.121s)
               Value function loss: 127212.7430
                    Surrogate loss: 0.0070
             Mean action noise std: 0.90
                       Mean reward: 9706.84
               Mean episode length: 403.90
                 Mean success rate: 79.00
                  Mean reward/step: 24.97
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 23420928
                    Iteration time: 2.59s
                        Total time: 7359.95s
                               ETA: 2939.9s

################################################################################
                     [1m Learning iteration 2859/4000 [0m

                       Computation: 3151 steps/s (collection: 0.470s, learning 2.130s)
               Value function loss: 87879.1665
                    Surrogate loss: 0.0123
             Mean action noise std: 0.90
                       Mean reward: 9989.40
               Mean episode length: 412.86
                 Mean success rate: 81.00
                  Mean reward/step: 24.30
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 2.60s
                        Total time: 7362.55s
                               ETA: 2937.3s

################################################################################
                     [1m Learning iteration 2860/4000 [0m

                       Computation: 3066 steps/s (collection: 0.536s, learning 2.135s)
               Value function loss: 99239.8301
                    Surrogate loss: 0.0093
             Mean action noise std: 0.90
                       Mean reward: 9834.71
               Mean episode length: 410.83
                 Mean success rate: 80.00
                  Mean reward/step: 24.42
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 23437312
                    Iteration time: 2.67s
                        Total time: 7365.22s
                               ETA: 2934.8s

################################################################################
                     [1m Learning iteration 2861/4000 [0m

                       Computation: 3086 steps/s (collection: 0.539s, learning 2.115s)
               Value function loss: 132357.3610
                    Surrogate loss: 0.0094
             Mean action noise std: 0.90
                       Mean reward: 10114.41
               Mean episode length: 415.21
                 Mean success rate: 81.00
                  Mean reward/step: 23.79
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 2.65s
                        Total time: 7367.88s
                               ETA: 2932.2s

################################################################################
                     [1m Learning iteration 2862/4000 [0m

                       Computation: 3162 steps/s (collection: 0.457s, learning 2.133s)
               Value function loss: 89490.0382
                    Surrogate loss: 0.0076
             Mean action noise std: 0.90
                       Mean reward: 10226.22
               Mean episode length: 417.76
                 Mean success rate: 82.00
                  Mean reward/step: 23.38
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23453696
                    Iteration time: 2.59s
                        Total time: 7370.47s
                               ETA: 2929.7s

################################################################################
                     [1m Learning iteration 2863/4000 [0m

                       Computation: 3191 steps/s (collection: 0.471s, learning 2.096s)
               Value function loss: 87007.1857
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 10362.62
               Mean episode length: 423.88
                 Mean success rate: 82.00
                  Mean reward/step: 24.33
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 2.57s
                        Total time: 7373.03s
                               ETA: 2927.1s

################################################################################
                     [1m Learning iteration 2864/4000 [0m

                       Computation: 3185 steps/s (collection: 0.487s, learning 2.085s)
               Value function loss: 64776.1228
                    Surrogate loss: 0.0076
             Mean action noise std: 0.90
                       Mean reward: 10315.70
               Mean episode length: 419.55
                 Mean success rate: 81.50
                  Mean reward/step: 24.99
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 23470080
                    Iteration time: 2.57s
                        Total time: 7375.61s
                               ETA: 2924.5s

################################################################################
                     [1m Learning iteration 2865/4000 [0m

                       Computation: 3160 steps/s (collection: 0.521s, learning 2.071s)
               Value function loss: 98262.4637
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 10022.89
               Mean episode length: 411.88
                 Mean success rate: 79.50
                  Mean reward/step: 26.07
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 2.59s
                        Total time: 7378.20s
                               ETA: 2921.9s

################################################################################
                     [1m Learning iteration 2866/4000 [0m

                       Computation: 3170 steps/s (collection: 0.503s, learning 2.081s)
               Value function loss: 69975.7686
                    Surrogate loss: 0.0087
             Mean action noise std: 0.90
                       Mean reward: 9715.94
               Mean episode length: 398.86
                 Mean success rate: 76.50
                  Mean reward/step: 25.93
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23486464
                    Iteration time: 2.58s
                        Total time: 7380.78s
                               ETA: 2919.4s

################################################################################
                     [1m Learning iteration 2867/4000 [0m

                       Computation: 3208 steps/s (collection: 0.500s, learning 2.053s)
               Value function loss: 67602.7570
                    Surrogate loss: 0.0120
             Mean action noise std: 0.90
                       Mean reward: 9898.71
               Mean episode length: 405.60
                 Mean success rate: 78.00
                  Mean reward/step: 26.01
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.55s
                        Total time: 7383.33s
                               ETA: 2916.8s

################################################################################
                     [1m Learning iteration 2868/4000 [0m

                       Computation: 3125 steps/s (collection: 0.526s, learning 2.095s)
               Value function loss: 144168.6783
                    Surrogate loss: 0.0125
             Mean action noise std: 0.90
                       Mean reward: 10002.55
               Mean episode length: 405.76
                 Mean success rate: 78.00
                  Mean reward/step: 25.57
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 23502848
                    Iteration time: 2.62s
                        Total time: 7385.96s
                               ETA: 2914.2s

################################################################################
                     [1m Learning iteration 2869/4000 [0m

                       Computation: 3128 steps/s (collection: 0.513s, learning 2.106s)
               Value function loss: 67844.0353
                    Surrogate loss: 0.0088
             Mean action noise std: 0.90
                       Mean reward: 9845.13
               Mean episode length: 397.54
                 Mean success rate: 77.00
                  Mean reward/step: 25.34
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 2.62s
                        Total time: 7388.57s
                               ETA: 2911.7s

################################################################################
                     [1m Learning iteration 2870/4000 [0m

                       Computation: 3161 steps/s (collection: 0.466s, learning 2.125s)
               Value function loss: 90517.4506
                    Surrogate loss: 0.0086
             Mean action noise std: 0.90
                       Mean reward: 10113.98
               Mean episode length: 404.90
                 Mean success rate: 78.50
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23519232
                    Iteration time: 2.59s
                        Total time: 7391.17s
                               ETA: 2909.1s

################################################################################
                     [1m Learning iteration 2871/4000 [0m

                       Computation: 3179 steps/s (collection: 0.486s, learning 2.090s)
               Value function loss: 57046.6849
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 10101.14
               Mean episode length: 405.91
                 Mean success rate: 78.50
                  Mean reward/step: 25.97
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 2.58s
                        Total time: 7393.74s
                               ETA: 2906.5s

################################################################################
                     [1m Learning iteration 2872/4000 [0m

                       Computation: 3191 steps/s (collection: 0.476s, learning 2.090s)
               Value function loss: 128799.4483
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 9999.41
               Mean episode length: 404.38
                 Mean success rate: 78.50
                  Mean reward/step: 26.13
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 23535616
                    Iteration time: 2.57s
                        Total time: 7396.31s
                               ETA: 2903.9s

################################################################################
                     [1m Learning iteration 2873/4000 [0m

                       Computation: 3199 steps/s (collection: 0.465s, learning 2.095s)
               Value function loss: 91917.9400
                    Surrogate loss: 0.0078
             Mean action noise std: 0.90
                       Mean reward: 9979.20
               Mean episode length: 404.70
                 Mean success rate: 79.00
                  Mean reward/step: 25.32
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 2.56s
                        Total time: 7398.87s
                               ETA: 2901.4s

################################################################################
                     [1m Learning iteration 2874/4000 [0m

                       Computation: 3161 steps/s (collection: 0.474s, learning 2.117s)
               Value function loss: 99547.5178
                    Surrogate loss: 0.0083
             Mean action noise std: 0.90
                       Mean reward: 10118.64
               Mean episode length: 407.29
                 Mean success rate: 79.50
                  Mean reward/step: 24.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23552000
                    Iteration time: 2.59s
                        Total time: 7401.46s
                               ETA: 2898.8s

################################################################################
                     [1m Learning iteration 2875/4000 [0m

                       Computation: 3269 steps/s (collection: 0.440s, learning 2.065s)
               Value function loss: 91122.7500
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 10177.25
               Mean episode length: 407.82
                 Mean success rate: 79.50
                  Mean reward/step: 24.91
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 2.51s
                        Total time: 7403.97s
                               ETA: 2896.2s

################################################################################
                     [1m Learning iteration 2876/4000 [0m

                       Computation: 3163 steps/s (collection: 0.496s, learning 2.093s)
               Value function loss: 142587.2457
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 10801.86
               Mean episode length: 427.51
                 Mean success rate: 84.00
                  Mean reward/step: 24.50
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 23568384
                    Iteration time: 2.59s
                        Total time: 7406.56s
                               ETA: 2893.6s

################################################################################
                     [1m Learning iteration 2877/4000 [0m

                       Computation: 3173 steps/s (collection: 0.478s, learning 2.104s)
               Value function loss: 112050.8001
                    Surrogate loss: 0.0126
             Mean action noise std: 0.90
                       Mean reward: 10831.98
               Mean episode length: 430.53
                 Mean success rate: 84.50
                  Mean reward/step: 23.25
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 2.58s
                        Total time: 7409.14s
                               ETA: 2891.1s

################################################################################
                     [1m Learning iteration 2878/4000 [0m

                       Computation: 3159 steps/s (collection: 0.477s, learning 2.116s)
               Value function loss: 101201.4724
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 10703.63
               Mean episode length: 428.15
                 Mean success rate: 84.00
                  Mean reward/step: 23.34
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23584768
                    Iteration time: 2.59s
                        Total time: 7411.73s
                               ETA: 2888.5s

################################################################################
                     [1m Learning iteration 2879/4000 [0m

                       Computation: 3284 steps/s (collection: 0.451s, learning 2.043s)
               Value function loss: 64862.2173
                    Surrogate loss: 0.0122
             Mean action noise std: 0.90
                       Mean reward: 10533.75
               Mean episode length: 422.37
                 Mean success rate: 82.50
                  Mean reward/step: 23.87
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 2.49s
                        Total time: 7414.22s
                               ETA: 2885.9s

################################################################################
                     [1m Learning iteration 2880/4000 [0m

                       Computation: 3137 steps/s (collection: 0.581s, learning 2.030s)
               Value function loss: 77318.8278
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 10451.66
               Mean episode length: 418.51
                 Mean success rate: 82.00
                  Mean reward/step: 24.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 23601152
                    Iteration time: 2.61s
                        Total time: 7416.83s
                               ETA: 2883.3s

################################################################################
                     [1m Learning iteration 2881/4000 [0m

                       Computation: 3221 steps/s (collection: 0.468s, learning 2.075s)
               Value function loss: 69034.8978
                    Surrogate loss: 0.0091
             Mean action noise std: 0.90
                       Mean reward: 10347.95
               Mean episode length: 419.38
                 Mean success rate: 82.00
                  Mean reward/step: 24.76
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 2.54s
                        Total time: 7419.38s
                               ETA: 2880.7s

################################################################################
                     [1m Learning iteration 2882/4000 [0m

                       Computation: 3230 steps/s (collection: 0.479s, learning 2.057s)
               Value function loss: 68498.7195
                    Surrogate loss: 0.0099
             Mean action noise std: 0.90
                       Mean reward: 10442.14
               Mean episode length: 422.62
                 Mean success rate: 82.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23617536
                    Iteration time: 2.54s
                        Total time: 7421.91s
                               ETA: 2878.1s

################################################################################
                     [1m Learning iteration 2883/4000 [0m

                       Computation: 3250 steps/s (collection: 0.448s, learning 2.073s)
               Value function loss: 95355.7690
                    Surrogate loss: 0.0090
             Mean action noise std: 0.90
                       Mean reward: 10362.33
               Mean episode length: 418.18
                 Mean success rate: 80.50
                  Mean reward/step: 26.10
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 2.52s
                        Total time: 7424.43s
                               ETA: 2875.6s

################################################################################
                     [1m Learning iteration 2884/4000 [0m

                       Computation: 3149 steps/s (collection: 0.543s, learning 2.058s)
               Value function loss: 100730.5760
                    Surrogate loss: 0.0100
             Mean action noise std: 0.90
                       Mean reward: 10643.21
               Mean episode length: 425.05
                 Mean success rate: 82.00
                  Mean reward/step: 25.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 23633920
                    Iteration time: 2.60s
                        Total time: 7427.03s
                               ETA: 2873.0s

################################################################################
                     [1m Learning iteration 2885/4000 [0m

                       Computation: 3176 steps/s (collection: 0.506s, learning 2.073s)
               Value function loss: 113293.6430
                    Surrogate loss: 0.0094
             Mean action noise std: 0.90
                       Mean reward: 10674.78
               Mean episode length: 424.55
                 Mean success rate: 82.00
                  Mean reward/step: 25.13
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 2.58s
                        Total time: 7429.61s
                               ETA: 2870.4s

################################################################################
                     [1m Learning iteration 2886/4000 [0m

                       Computation: 3224 steps/s (collection: 0.476s, learning 2.065s)
               Value function loss: 91741.4352
                    Surrogate loss: 0.0078
             Mean action noise std: 0.90
                       Mean reward: 10428.84
               Mean episode length: 417.81
                 Mean success rate: 80.50
                  Mean reward/step: 24.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23650304
                    Iteration time: 2.54s
                        Total time: 7432.15s
                               ETA: 2867.8s

################################################################################
                     [1m Learning iteration 2887/4000 [0m

                       Computation: 3164 steps/s (collection: 0.507s, learning 2.082s)
               Value function loss: 77115.7554
                    Surrogate loss: 0.0082
             Mean action noise std: 0.90
                       Mean reward: 10383.92
               Mean episode length: 417.12
                 Mean success rate: 80.00
                  Mean reward/step: 24.85
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 2.59s
                        Total time: 7434.74s
                               ETA: 2865.3s

################################################################################
                     [1m Learning iteration 2888/4000 [0m

                       Computation: 3198 steps/s (collection: 0.479s, learning 2.082s)
               Value function loss: 109764.6086
                    Surrogate loss: 0.0092
             Mean action noise std: 0.90
                       Mean reward: 10256.63
               Mean episode length: 416.58
                 Mean success rate: 80.00
                  Mean reward/step: 24.77
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23666688
                    Iteration time: 2.56s
                        Total time: 7437.30s
                               ETA: 2862.7s

################################################################################
                     [1m Learning iteration 2889/4000 [0m

                       Computation: 3177 steps/s (collection: 0.514s, learning 2.064s)
               Value function loss: 116107.5701
                    Surrogate loss: 0.0138
             Mean action noise std: 0.90
                       Mean reward: 10250.60
               Mean episode length: 417.26
                 Mean success rate: 79.50
                  Mean reward/step: 24.63
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 2.58s
                        Total time: 7439.88s
                               ETA: 2860.1s

################################################################################
                     [1m Learning iteration 2890/4000 [0m

                       Computation: 3165 steps/s (collection: 0.501s, learning 2.088s)
               Value function loss: 76385.4873
                    Surrogate loss: 0.0117
             Mean action noise std: 0.90
                       Mean reward: 10367.30
               Mean episode length: 422.90
                 Mean success rate: 81.00
                  Mean reward/step: 24.25
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23683072
                    Iteration time: 2.59s
                        Total time: 7442.47s
                               ETA: 2857.5s

################################################################################
                     [1m Learning iteration 2891/4000 [0m

                       Computation: 3217 steps/s (collection: 0.481s, learning 2.066s)
               Value function loss: 133574.0703
                    Surrogate loss: 0.0118
             Mean action noise std: 0.90
                       Mean reward: 10471.10
               Mean episode length: 421.06
                 Mean success rate: 80.50
                  Mean reward/step: 24.69
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.55s
                        Total time: 7445.02s
                               ETA: 2855.0s

################################################################################
                     [1m Learning iteration 2892/4000 [0m

                       Computation: 3141 steps/s (collection: 0.530s, learning 2.078s)
               Value function loss: 131136.6895
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 10585.41
               Mean episode length: 425.04
                 Mean success rate: 82.00
                  Mean reward/step: 23.42
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 23699456
                    Iteration time: 2.61s
                        Total time: 7447.62s
                               ETA: 2852.4s

################################################################################
                     [1m Learning iteration 2893/4000 [0m

                       Computation: 3209 steps/s (collection: 0.493s, learning 2.060s)
               Value function loss: 89600.7448
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 9958.07
               Mean episode length: 404.60
                 Mean success rate: 77.00
                  Mean reward/step: 23.07
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 2.55s
                        Total time: 7450.18s
                               ETA: 2849.8s

################################################################################
                     [1m Learning iteration 2894/4000 [0m

                       Computation: 3182 steps/s (collection: 0.491s, learning 2.083s)
               Value function loss: 81283.4425
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 9574.92
               Mean episode length: 396.33
                 Mean success rate: 75.50
                  Mean reward/step: 23.62
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23715840
                    Iteration time: 2.57s
                        Total time: 7452.75s
                               ETA: 2847.2s

################################################################################
                     [1m Learning iteration 2895/4000 [0m

                       Computation: 3170 steps/s (collection: 0.494s, learning 2.091s)
               Value function loss: 95543.9365
                    Surrogate loss: 0.0088
             Mean action noise std: 0.90
                       Mean reward: 9511.02
               Mean episode length: 393.79
                 Mean success rate: 75.00
                  Mean reward/step: 24.75
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 2.58s
                        Total time: 7455.34s
                               ETA: 2844.7s

################################################################################
                     [1m Learning iteration 2896/4000 [0m

                       Computation: 3225 steps/s (collection: 0.455s, learning 2.085s)
               Value function loss: 77484.3105
                    Surrogate loss: 0.0092
             Mean action noise std: 0.90
                       Mean reward: 9453.37
               Mean episode length: 389.48
                 Mean success rate: 74.50
                  Mean reward/step: 25.13
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23732224
                    Iteration time: 2.54s
                        Total time: 7457.88s
                               ETA: 2842.1s

################################################################################
                     [1m Learning iteration 2897/4000 [0m

                       Computation: 3123 steps/s (collection: 0.517s, learning 2.106s)
               Value function loss: 81538.8319
                    Surrogate loss: 0.0134
             Mean action noise std: 0.90
                       Mean reward: 9596.45
               Mean episode length: 390.06
                 Mean success rate: 75.00
                  Mean reward/step: 25.60
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 2.62s
                        Total time: 7460.50s
                               ETA: 2839.5s

################################################################################
                     [1m Learning iteration 2898/4000 [0m

                       Computation: 3197 steps/s (collection: 0.522s, learning 2.040s)
               Value function loss: 80033.7904
                    Surrogate loss: 0.0071
             Mean action noise std: 0.90
                       Mean reward: 9444.50
               Mean episode length: 386.02
                 Mean success rate: 74.50
                  Mean reward/step: 25.83
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23748608
                    Iteration time: 2.56s
                        Total time: 7463.06s
                               ETA: 2836.9s

################################################################################
                     [1m Learning iteration 2899/4000 [0m

                       Computation: 3118 steps/s (collection: 0.486s, learning 2.141s)
               Value function loss: 121643.7766
                    Surrogate loss: 0.0086
             Mean action noise std: 0.90
                       Mean reward: 9657.20
               Mean episode length: 392.68
                 Mean success rate: 76.00
                  Mean reward/step: 26.30
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 2.63s
                        Total time: 7465.69s
                               ETA: 2834.4s

################################################################################
                     [1m Learning iteration 2900/4000 [0m

                       Computation: 3249 steps/s (collection: 0.467s, learning 2.053s)
               Value function loss: 61232.2077
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 9909.14
               Mean episode length: 399.70
                 Mean success rate: 77.50
                  Mean reward/step: 25.43
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 23764992
                    Iteration time: 2.52s
                        Total time: 7468.21s
                               ETA: 2831.8s

################################################################################
                     [1m Learning iteration 2901/4000 [0m

                       Computation: 3173 steps/s (collection: 0.454s, learning 2.127s)
               Value function loss: 111103.8323
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 9917.51
               Mean episode length: 402.41
                 Mean success rate: 78.00
                  Mean reward/step: 25.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 2.58s
                        Total time: 7470.79s
                               ETA: 2829.2s

################################################################################
                     [1m Learning iteration 2902/4000 [0m

                       Computation: 3226 steps/s (collection: 0.478s, learning 2.061s)
               Value function loss: 65765.5185
                    Surrogate loss: 0.0091
             Mean action noise std: 0.90
                       Mean reward: 9528.18
               Mean episode length: 389.50
                 Mean success rate: 75.50
                  Mean reward/step: 25.49
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23781376
                    Iteration time: 2.54s
                        Total time: 7473.33s
                               ETA: 2826.6s

################################################################################
                     [1m Learning iteration 2903/4000 [0m

                       Computation: 3275 steps/s (collection: 0.455s, learning 2.046s)
               Value function loss: 116448.6520
                    Surrogate loss: 0.0108
             Mean action noise std: 0.90
                       Mean reward: 9663.17
               Mean episode length: 394.38
                 Mean success rate: 77.00
                  Mean reward/step: 26.07
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 2.50s
                        Total time: 7475.83s
                               ETA: 2824.0s

################################################################################
                     [1m Learning iteration 2904/4000 [0m

                       Computation: 3174 steps/s (collection: 0.490s, learning 2.090s)
               Value function loss: 103143.5492
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 9681.64
               Mean episode length: 393.61
                 Mean success rate: 76.50
                  Mean reward/step: 25.39
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 23797760
                    Iteration time: 2.58s
                        Total time: 7478.41s
                               ETA: 2821.5s

################################################################################
                     [1m Learning iteration 2905/4000 [0m

                       Computation: 3241 steps/s (collection: 0.471s, learning 2.056s)
               Value function loss: 103996.8779
                    Surrogate loss: 0.0079
             Mean action noise std: 0.90
                       Mean reward: 9947.40
               Mean episode length: 403.46
                 Mean success rate: 78.00
                  Mean reward/step: 24.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 2.53s
                        Total time: 7480.94s
                               ETA: 2818.9s

################################################################################
                     [1m Learning iteration 2906/4000 [0m

                       Computation: 3298 steps/s (collection: 0.442s, learning 2.042s)
               Value function loss: 98640.9029
                    Surrogate loss: 0.0087
             Mean action noise std: 0.90
                       Mean reward: 9981.19
               Mean episode length: 404.54
                 Mean success rate: 78.50
                  Mean reward/step: 24.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23814144
                    Iteration time: 2.48s
                        Total time: 7483.42s
                               ETA: 2816.3s

################################################################################
                     [1m Learning iteration 2907/4000 [0m

                       Computation: 3320 steps/s (collection: 0.452s, learning 2.015s)
               Value function loss: 149475.2879
                    Surrogate loss: 0.0128
             Mean action noise std: 0.90
                       Mean reward: 10123.72
               Mean episode length: 408.44
                 Mean success rate: 79.00
                  Mean reward/step: 24.70
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 2.47s
                        Total time: 7485.89s
                               ETA: 2813.6s

################################################################################
                     [1m Learning iteration 2908/4000 [0m

                       Computation: 3213 steps/s (collection: 0.473s, learning 2.076s)
               Value function loss: 110493.8870
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 10602.72
               Mean episode length: 419.85
                 Mean success rate: 81.50
                  Mean reward/step: 24.02
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23830528
                    Iteration time: 2.55s
                        Total time: 7488.44s
                               ETA: 2811.1s

################################################################################
                     [1m Learning iteration 2909/4000 [0m

                       Computation: 3217 steps/s (collection: 0.498s, learning 2.048s)
               Value function loss: 96852.2976
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 10450.66
               Mean episode length: 415.60
                 Mean success rate: 80.50
                  Mean reward/step: 23.83
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 2.55s
                        Total time: 7490.98s
                               ETA: 2808.5s

################################################################################
                     [1m Learning iteration 2910/4000 [0m

                       Computation: 3212 steps/s (collection: 0.489s, learning 2.061s)
               Value function loss: 95335.4816
                    Surrogate loss: 0.0130
             Mean action noise std: 0.90
                       Mean reward: 10125.29
               Mean episode length: 405.86
                 Mean success rate: 78.50
                  Mean reward/step: 24.54
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 23846912
                    Iteration time: 2.55s
                        Total time: 7493.53s
                               ETA: 2805.9s

################################################################################
                     [1m Learning iteration 2911/4000 [0m

                       Computation: 3279 steps/s (collection: 0.447s, learning 2.051s)
               Value function loss: 82325.3455
                    Surrogate loss: 0.0123
             Mean action noise std: 0.90
                       Mean reward: 10263.16
               Mean episode length: 411.72
                 Mean success rate: 79.50
                  Mean reward/step: 24.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 2.50s
                        Total time: 7496.03s
                               ETA: 2803.3s

################################################################################
                     [1m Learning iteration 2912/4000 [0m

                       Computation: 3237 steps/s (collection: 0.472s, learning 2.058s)
               Value function loss: 87433.8003
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 10747.04
               Mean episode length: 425.38
                 Mean success rate: 82.00
                  Mean reward/step: 25.53
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23863296
                    Iteration time: 2.53s
                        Total time: 7498.56s
                               ETA: 2800.7s

################################################################################
                     [1m Learning iteration 2913/4000 [0m

                       Computation: 3254 steps/s (collection: 0.465s, learning 2.053s)
               Value function loss: 73352.2565
                    Surrogate loss: 0.0083
             Mean action noise std: 0.90
                       Mean reward: 10705.99
               Mean episode length: 423.37
                 Mean success rate: 81.50
                  Mean reward/step: 26.31
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 2.52s
                        Total time: 7501.08s
                               ETA: 2798.1s

################################################################################
                     [1m Learning iteration 2914/4000 [0m

                       Computation: 3211 steps/s (collection: 0.481s, learning 2.070s)
               Value function loss: 97192.0956
                    Surrogate loss: 0.0075
             Mean action noise std: 0.90
                       Mean reward: 10823.67
               Mean episode length: 426.16
                 Mean success rate: 82.00
                  Mean reward/step: 27.00
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23879680
                    Iteration time: 2.55s
                        Total time: 7503.63s
                               ETA: 2795.5s

################################################################################
                     [1m Learning iteration 2915/4000 [0m

                       Computation: 3189 steps/s (collection: 0.499s, learning 2.070s)
               Value function loss: 94085.2378
                    Surrogate loss: 0.0108
             Mean action noise std: 0.90
                       Mean reward: 10815.13
               Mean episode length: 427.10
                 Mean success rate: 82.50
                  Mean reward/step: 26.39
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.57s
                        Total time: 7506.20s
                               ETA: 2792.9s

################################################################################
                     [1m Learning iteration 2916/4000 [0m

                       Computation: 3160 steps/s (collection: 0.500s, learning 2.092s)
               Value function loss: 64675.1021
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 10959.70
               Mean episode length: 431.02
                 Mean success rate: 83.50
                  Mean reward/step: 26.38
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 23896064
                    Iteration time: 2.59s
                        Total time: 7508.79s
                               ETA: 2790.4s

################################################################################
                     [1m Learning iteration 2917/4000 [0m

                       Computation: 3259 steps/s (collection: 0.448s, learning 2.066s)
               Value function loss: 84474.3465
                    Surrogate loss: 0.0076
             Mean action noise std: 0.90
                       Mean reward: 11048.27
               Mean episode length: 431.02
                 Mean success rate: 83.50
                  Mean reward/step: 26.28
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 2.51s
                        Total time: 7511.30s
                               ETA: 2787.8s

################################################################################
                     [1m Learning iteration 2918/4000 [0m

                       Computation: 3236 steps/s (collection: 0.454s, learning 2.077s)
               Value function loss: 65967.2891
                    Surrogate loss: 0.0094
             Mean action noise std: 0.90
                       Mean reward: 11013.56
               Mean episode length: 431.02
                 Mean success rate: 83.50
                  Mean reward/step: 26.75
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 23912448
                    Iteration time: 2.53s
                        Total time: 7513.83s
                               ETA: 2785.2s

################################################################################
                     [1m Learning iteration 2919/4000 [0m

                       Computation: 3167 steps/s (collection: 0.510s, learning 2.077s)
               Value function loss: 142873.9010
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 10361.96
               Mean episode length: 412.21
                 Mean success rate: 79.00
                  Mean reward/step: 25.67
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 2.59s
                        Total time: 7516.42s
                               ETA: 2782.6s

################################################################################
                     [1m Learning iteration 2920/4000 [0m

                       Computation: 3225 steps/s (collection: 0.472s, learning 2.068s)
               Value function loss: 95567.8029
                    Surrogate loss: 0.0140
             Mean action noise std: 0.90
                       Mean reward: 10447.89
               Mean episode length: 416.46
                 Mean success rate: 80.00
                  Mean reward/step: 24.50
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23928832
                    Iteration time: 2.54s
                        Total time: 7518.96s
                               ETA: 2780.0s

################################################################################
                     [1m Learning iteration 2921/4000 [0m

                       Computation: 3118 steps/s (collection: 0.500s, learning 2.127s)
               Value function loss: 114688.1131
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 10859.56
               Mean episode length: 428.55
                 Mean success rate: 82.50
                  Mean reward/step: 24.05
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 2.63s
                        Total time: 7521.59s
                               ETA: 2777.5s

################################################################################
                     [1m Learning iteration 2922/4000 [0m

                       Computation: 3251 steps/s (collection: 0.466s, learning 2.054s)
               Value function loss: 100747.2466
                    Surrogate loss: 0.0073
             Mean action noise std: 0.90
                       Mean reward: 10678.02
               Mean episode length: 420.55
                 Mean success rate: 81.00
                  Mean reward/step: 23.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23945216
                    Iteration time: 2.52s
                        Total time: 7524.11s
                               ETA: 2774.9s

################################################################################
                     [1m Learning iteration 2923/4000 [0m

                       Computation: 3220 steps/s (collection: 0.466s, learning 2.078s)
               Value function loss: 168741.0072
                    Surrogate loss: 0.0074
             Mean action noise std: 0.90
                       Mean reward: 10720.53
               Mean episode length: 421.31
                 Mean success rate: 81.50
                  Mean reward/step: 22.97
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 2.54s
                        Total time: 7526.65s
                               ETA: 2772.3s

################################################################################
                     [1m Learning iteration 2924/4000 [0m

                       Computation: 3227 steps/s (collection: 0.446s, learning 2.092s)
               Value function loss: 94414.4297
                    Surrogate loss: 0.0062
             Mean action noise std: 0.90
                       Mean reward: 10802.76
               Mean episode length: 425.62
                 Mean success rate: 82.50
                  Mean reward/step: 22.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23961600
                    Iteration time: 2.54s
                        Total time: 7529.19s
                               ETA: 2769.7s

################################################################################
                     [1m Learning iteration 2925/4000 [0m

                       Computation: 3142 steps/s (collection: 0.488s, learning 2.120s)
               Value function loss: 95953.5858
                    Surrogate loss: 0.0085
             Mean action noise std: 0.90
                       Mean reward: 11008.47
               Mean episode length: 428.44
                 Mean success rate: 83.00
                  Mean reward/step: 22.91
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 2.61s
                        Total time: 7531.80s
                               ETA: 2767.2s

################################################################################
                     [1m Learning iteration 2926/4000 [0m

                       Computation: 3222 steps/s (collection: 0.490s, learning 2.052s)
               Value function loss: 93414.3112
                    Surrogate loss: 0.0086
             Mean action noise std: 0.90
                       Mean reward: 10994.83
               Mean episode length: 430.90
                 Mean success rate: 83.50
                  Mean reward/step: 23.98
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23977984
                    Iteration time: 2.54s
                        Total time: 7534.34s
                               ETA: 2764.6s

################################################################################
                     [1m Learning iteration 2927/4000 [0m

                       Computation: 3182 steps/s (collection: 0.485s, learning 2.089s)
               Value function loss: 82771.0030
                    Surrogate loss: 0.0119
             Mean action noise std: 0.90
                       Mean reward: 10688.26
               Mean episode length: 422.57
                 Mean success rate: 81.50
                  Mean reward/step: 24.34
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.57s
                        Total time: 7536.91s
                               ETA: 2762.0s

################################################################################
                     [1m Learning iteration 2928/4000 [0m

                       Computation: 3219 steps/s (collection: 0.462s, learning 2.082s)
               Value function loss: 67000.9818
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 10526.79
               Mean episode length: 414.22
                 Mean success rate: 80.00
                  Mean reward/step: 24.32
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23994368
                    Iteration time: 2.54s
                        Total time: 7539.46s
                               ETA: 2759.4s

################################################################################
                     [1m Learning iteration 2929/4000 [0m

                       Computation: 3197 steps/s (collection: 0.494s, learning 2.068s)
               Value function loss: 67630.9533
                    Surrogate loss: 0.0082
             Mean action noise std: 0.90
                       Mean reward: 10573.26
               Mean episode length: 418.05
                 Mean success rate: 80.50
                  Mean reward/step: 24.83
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 2.56s
                        Total time: 7542.02s
                               ETA: 2756.8s

################################################################################
                     [1m Learning iteration 2930/4000 [0m

                       Computation: 3235 steps/s (collection: 0.471s, learning 2.061s)
               Value function loss: 110731.8840
                    Surrogate loss: 0.0096
             Mean action noise std: 0.90
                       Mean reward: 10511.51
               Mean episode length: 417.81
                 Mean success rate: 80.50
                  Mean reward/step: 25.23
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 24010752
                    Iteration time: 2.53s
                        Total time: 7544.55s
                               ETA: 2754.2s

################################################################################
                     [1m Learning iteration 2931/4000 [0m

                       Computation: 3335 steps/s (collection: 0.439s, learning 2.017s)
               Value function loss: 57836.5778
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 10287.97
               Mean episode length: 411.84
                 Mean success rate: 79.50
                  Mean reward/step: 24.96
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 2.46s
                        Total time: 7547.01s
                               ETA: 2751.6s

################################################################################
                     [1m Learning iteration 2932/4000 [0m

                       Computation: 3224 steps/s (collection: 0.452s, learning 2.089s)
               Value function loss: 90430.1150
                    Surrogate loss: 0.0087
             Mean action noise std: 0.90
                       Mean reward: 10131.41
               Mean episode length: 412.43
                 Mean success rate: 79.50
                  Mean reward/step: 25.86
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 24027136
                    Iteration time: 2.54s
                        Total time: 7549.55s
                               ETA: 2749.0s

################################################################################
                     [1m Learning iteration 2933/4000 [0m

                       Computation: 3307 steps/s (collection: 0.443s, learning 2.033s)
               Value function loss: 56771.2286
                    Surrogate loss: 0.0089
             Mean action noise std: 0.90
                       Mean reward: 9870.73
               Mean episode length: 403.36
                 Mean success rate: 77.50
                  Mean reward/step: 26.02
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 2.48s
                        Total time: 7552.02s
                               ETA: 2746.4s

################################################################################
                     [1m Learning iteration 2934/4000 [0m

                       Computation: 3205 steps/s (collection: 0.462s, learning 2.093s)
               Value function loss: 101651.5391
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9555.00
               Mean episode length: 396.19
                 Mean success rate: 75.50
                  Mean reward/step: 26.38
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24043520
                    Iteration time: 2.56s
                        Total time: 7554.58s
                               ETA: 2743.8s

################################################################################
                     [1m Learning iteration 2935/4000 [0m

                       Computation: 3189 steps/s (collection: 0.489s, learning 2.079s)
               Value function loss: 121427.8285
                    Surrogate loss: 0.0087
             Mean action noise std: 0.90
                       Mean reward: 9484.08
               Mean episode length: 396.77
                 Mean success rate: 75.50
                  Mean reward/step: 25.28
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 2.57s
                        Total time: 7557.15s
                               ETA: 2741.3s

################################################################################
                     [1m Learning iteration 2936/4000 [0m

                       Computation: 3128 steps/s (collection: 0.498s, learning 2.121s)
               Value function loss: 108161.9334
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 9227.76
               Mean episode length: 389.92
                 Mean success rate: 73.50
                  Mean reward/step: 24.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 24059904
                    Iteration time: 2.62s
                        Total time: 7559.77s
                               ETA: 2738.7s

################################################################################
                     [1m Learning iteration 2937/4000 [0m

                       Computation: 3140 steps/s (collection: 0.508s, learning 2.100s)
               Value function loss: 89427.9631
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 9387.21
               Mean episode length: 393.85
                 Mean success rate: 74.50
                  Mean reward/step: 24.15
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 2.61s
                        Total time: 7562.38s
                               ETA: 2736.1s

################################################################################
                     [1m Learning iteration 2938/4000 [0m

                       Computation: 3187 steps/s (collection: 0.502s, learning 2.068s)
               Value function loss: 116967.6378
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 9664.34
               Mean episode length: 401.38
                 Mean success rate: 76.50
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 24076288
                    Iteration time: 2.57s
                        Total time: 7564.95s
                               ETA: 2733.6s

################################################################################
                     [1m Learning iteration 2939/4000 [0m

                       Computation: 3085 steps/s (collection: 0.543s, learning 2.112s)
               Value function loss: 144564.8025
                    Surrogate loss: 0.0067
             Mean action noise std: 0.90
                       Mean reward: 9976.56
               Mean episode length: 408.47
                 Mean success rate: 78.50
                  Mean reward/step: 23.45
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.65s
                        Total time: 7567.60s
                               ETA: 2731.0s

################################################################################
                     [1m Learning iteration 2940/4000 [0m

                       Computation: 3229 steps/s (collection: 0.475s, learning 2.061s)
               Value function loss: 76341.8921
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 9959.84
               Mean episode length: 405.92
                 Mean success rate: 78.00
                  Mean reward/step: 23.04
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 24092672
                    Iteration time: 2.54s
                        Total time: 7570.14s
                               ETA: 2728.4s

################################################################################
                     [1m Learning iteration 2941/4000 [0m

                       Computation: 3183 steps/s (collection: 0.470s, learning 2.103s)
               Value function loss: 94124.9444
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 9977.69
               Mean episode length: 404.14
                 Mean success rate: 77.00
                  Mean reward/step: 24.31
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 2.57s
                        Total time: 7572.71s
                               ETA: 2725.9s

################################################################################
                     [1m Learning iteration 2942/4000 [0m

                       Computation: 3211 steps/s (collection: 0.455s, learning 2.096s)
               Value function loss: 101918.6746
                    Surrogate loss: 0.0090
             Mean action noise std: 0.90
                       Mean reward: 10261.80
               Mean episode length: 412.40
                 Mean success rate: 79.00
                  Mean reward/step: 24.36
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 24109056
                    Iteration time: 2.55s
                        Total time: 7575.26s
                               ETA: 2723.3s

################################################################################
                     [1m Learning iteration 2943/4000 [0m

                       Computation: 3189 steps/s (collection: 0.501s, learning 2.067s)
               Value function loss: 91146.4084
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 10317.29
               Mean episode length: 414.12
                 Mean success rate: 80.00
                  Mean reward/step: 24.81
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 2.57s
                        Total time: 7577.83s
                               ETA: 2720.7s

################################################################################
                     [1m Learning iteration 2944/4000 [0m

                       Computation: 3184 steps/s (collection: 0.491s, learning 2.082s)
               Value function loss: 100073.7030
                    Surrogate loss: 0.0084
             Mean action noise std: 0.90
                       Mean reward: 9651.45
               Mean episode length: 390.38
                 Mean success rate: 74.50
                  Mean reward/step: 24.57
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 24125440
                    Iteration time: 2.57s
                        Total time: 7580.40s
                               ETA: 2718.1s

################################################################################
                     [1m Learning iteration 2945/4000 [0m

                       Computation: 3189 steps/s (collection: 0.470s, learning 2.098s)
               Value function loss: 95587.6885
                    Surrogate loss: 0.0105
             Mean action noise std: 0.90
                       Mean reward: 9559.09
               Mean episode length: 386.33
                 Mean success rate: 74.00
                  Mean reward/step: 24.21
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 2.57s
                        Total time: 7582.97s
                               ETA: 2715.6s

################################################################################
                     [1m Learning iteration 2946/4000 [0m

                       Computation: 3165 steps/s (collection: 0.475s, learning 2.113s)
               Value function loss: 110740.1955
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 9427.38
               Mean episode length: 380.91
                 Mean success rate: 72.50
                  Mean reward/step: 23.81
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 24141824
                    Iteration time: 2.59s
                        Total time: 7585.56s
                               ETA: 2713.0s

################################################################################
                     [1m Learning iteration 2947/4000 [0m

                       Computation: 3184 steps/s (collection: 0.443s, learning 2.129s)
               Value function loss: 43433.5095
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 9053.11
               Mean episode length: 368.81
                 Mean success rate: 70.00
                  Mean reward/step: 24.03
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 2.57s
                        Total time: 7588.13s
                               ETA: 2710.4s

################################################################################
                     [1m Learning iteration 2948/4000 [0m

                       Computation: 3134 steps/s (collection: 0.460s, learning 2.153s)
               Value function loss: 100816.2910
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 8803.92
               Mean episode length: 359.99
                 Mean success rate: 68.50
                  Mean reward/step: 24.65
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24158208
                    Iteration time: 2.61s
                        Total time: 7590.74s
                               ETA: 2707.9s

################################################################################
                     [1m Learning iteration 2949/4000 [0m

                       Computation: 3094 steps/s (collection: 0.484s, learning 2.163s)
               Value function loss: 68021.8291
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 8517.39
               Mean episode length: 352.93
                 Mean success rate: 66.50
                  Mean reward/step: 25.14
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 2.65s
                        Total time: 7593.39s
                               ETA: 2705.3s

################################################################################
                     [1m Learning iteration 2950/4000 [0m

                       Computation: 3134 steps/s (collection: 0.484s, learning 2.129s)
               Value function loss: 153343.9956
                    Surrogate loss: 0.0085
             Mean action noise std: 0.90
                       Mean reward: 8586.66
               Mean episode length: 353.18
                 Mean success rate: 66.50
                  Mean reward/step: 24.82
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 24174592
                    Iteration time: 2.61s
                        Total time: 7596.00s
                               ETA: 2702.7s

################################################################################
                     [1m Learning iteration 2951/4000 [0m

                       Computation: 3094 steps/s (collection: 0.517s, learning 2.130s)
               Value function loss: 94594.5862
                    Surrogate loss: 0.0070
             Mean action noise std: 0.90
                       Mean reward: 8297.44
               Mean episode length: 348.34
                 Mean success rate: 64.50
                  Mean reward/step: 23.68
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 2.65s
                        Total time: 7598.65s
                               ETA: 2700.2s

################################################################################
                     [1m Learning iteration 2952/4000 [0m

                       Computation: 3235 steps/s (collection: 0.449s, learning 2.083s)
               Value function loss: 96818.3451
                    Surrogate loss: 0.0117
             Mean action noise std: 0.90
                       Mean reward: 8536.31
               Mean episode length: 351.88
                 Mean success rate: 65.00
                  Mean reward/step: 23.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 24190976
                    Iteration time: 2.53s
                        Total time: 7601.18s
                               ETA: 2697.6s

################################################################################
                     [1m Learning iteration 2953/4000 [0m

                       Computation: 3182 steps/s (collection: 0.473s, learning 2.101s)
               Value function loss: 101554.1780
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 8885.40
               Mean episode length: 363.10
                 Mean success rate: 67.50
                  Mean reward/step: 24.21
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 2.57s
                        Total time: 7603.76s
                               ETA: 2695.0s

################################################################################
                     [1m Learning iteration 2954/4000 [0m

                       Computation: 3049 steps/s (collection: 0.539s, learning 2.148s)
               Value function loss: 151414.1463
                    Surrogate loss: 0.0093
             Mean action noise std: 0.90
                       Mean reward: 9219.33
               Mean episode length: 372.69
                 Mean success rate: 69.50
                  Mean reward/step: 24.36
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 24207360
                    Iteration time: 2.69s
                        Total time: 7606.44s
                               ETA: 2692.5s

################################################################################
                     [1m Learning iteration 2955/4000 [0m

                       Computation: 3131 steps/s (collection: 0.490s, learning 2.126s)
               Value function loss: 95135.9198
                    Surrogate loss: 0.0120
             Mean action noise std: 0.90
                       Mean reward: 9174.15
               Mean episode length: 373.31
                 Mean success rate: 69.50
                  Mean reward/step: 22.94
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 2.62s
                        Total time: 7609.06s
                               ETA: 2689.9s

################################################################################
                     [1m Learning iteration 2956/4000 [0m

                       Computation: 3171 steps/s (collection: 0.467s, learning 2.116s)
               Value function loss: 74509.1514
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 9342.02
               Mean episode length: 378.81
                 Mean success rate: 70.50
                  Mean reward/step: 23.99
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 24223744
                    Iteration time: 2.58s
                        Total time: 7611.64s
                               ETA: 2687.4s

################################################################################
                     [1m Learning iteration 2957/4000 [0m

                       Computation: 3291 steps/s (collection: 0.462s, learning 2.027s)
               Value function loss: 87334.3032
                    Surrogate loss: 0.0082
             Mean action noise std: 0.90
                       Mean reward: 9349.52
               Mean episode length: 382.86
                 Mean success rate: 71.00
                  Mean reward/step: 24.87
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 2.49s
                        Total time: 7614.13s
                               ETA: 2684.8s

################################################################################
                     [1m Learning iteration 2958/4000 [0m

                       Computation: 3252 steps/s (collection: 0.480s, learning 2.038s)
               Value function loss: 74817.7810
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 9353.44
               Mean episode length: 380.17
                 Mean success rate: 70.50
                  Mean reward/step: 25.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 24240128
                    Iteration time: 2.52s
                        Total time: 7616.65s
                               ETA: 2682.2s

################################################################################
                     [1m Learning iteration 2959/4000 [0m

                       Computation: 3231 steps/s (collection: 0.489s, learning 2.046s)
               Value function loss: 81774.6249
                    Surrogate loss: 0.0088
             Mean action noise std: 0.90
                       Mean reward: 9285.05
               Mean episode length: 380.46
                 Mean success rate: 70.50
                  Mean reward/step: 25.51
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 2.53s
                        Total time: 7619.19s
                               ETA: 2679.6s

################################################################################
                     [1m Learning iteration 2960/4000 [0m

                       Computation: 3296 steps/s (collection: 0.445s, learning 2.039s)
               Value function loss: 111658.8403
                    Surrogate loss: 0.0083
             Mean action noise std: 0.90
                       Mean reward: 9691.66
               Mean episode length: 393.15
                 Mean success rate: 74.00
                  Mean reward/step: 25.41
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 24256512
                    Iteration time: 2.48s
                        Total time: 7621.67s
                               ETA: 2677.0s

################################################################################
                     [1m Learning iteration 2961/4000 [0m

                       Computation: 3256 steps/s (collection: 0.453s, learning 2.062s)
               Value function loss: 119053.2393
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 9599.47
               Mean episode length: 389.41
                 Mean success rate: 73.00
                  Mean reward/step: 25.40
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 2.52s
                        Total time: 7624.19s
                               ETA: 2674.4s

################################################################################
                     [1m Learning iteration 2962/4000 [0m

                       Computation: 3222 steps/s (collection: 0.480s, learning 2.063s)
               Value function loss: 93772.2929
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 9724.43
               Mean episode length: 394.45
                 Mean success rate: 74.00
                  Mean reward/step: 25.12
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24272896
                    Iteration time: 2.54s
                        Total time: 7626.73s
                               ETA: 2671.8s

################################################################################
                     [1m Learning iteration 2963/4000 [0m

                       Computation: 3261 steps/s (collection: 0.471s, learning 2.041s)
               Value function loss: 68673.9524
                    Surrogate loss: 0.0096
             Mean action noise std: 0.90
                       Mean reward: 9763.68
               Mean episode length: 394.90
                 Mean success rate: 74.00
                  Mean reward/step: 26.08
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 2.51s
                        Total time: 7629.24s
                               ETA: 2669.2s

################################################################################
                     [1m Learning iteration 2964/4000 [0m

                       Computation: 3214 steps/s (collection: 0.497s, learning 2.051s)
               Value function loss: 72148.2416
                    Surrogate loss: 0.0073
             Mean action noise std: 0.90
                       Mean reward: 9533.73
               Mean episode length: 390.45
                 Mean success rate: 72.50
                  Mean reward/step: 26.69
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 24289280
                    Iteration time: 2.55s
                        Total time: 7631.79s
                               ETA: 2666.6s

################################################################################
                     [1m Learning iteration 2965/4000 [0m

                       Computation: 3170 steps/s (collection: 0.484s, learning 2.100s)
               Value function loss: 105394.0617
                    Surrogate loss: 0.0123
             Mean action noise std: 0.90
                       Mean reward: 9731.79
               Mean episode length: 397.88
                 Mean success rate: 74.00
                  Mean reward/step: 26.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 2.58s
                        Total time: 7634.37s
                               ETA: 2664.1s

################################################################################
                     [1m Learning iteration 2966/4000 [0m

                       Computation: 3174 steps/s (collection: 0.481s, learning 2.099s)
               Value function loss: 149069.4695
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 10079.29
               Mean episode length: 410.37
                 Mean success rate: 76.50
                  Mean reward/step: 25.71
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 24305664
                    Iteration time: 2.58s
                        Total time: 7636.95s
                               ETA: 2661.5s

################################################################################
                     [1m Learning iteration 2967/4000 [0m

                       Computation: 3231 steps/s (collection: 0.469s, learning 2.066s)
               Value function loss: 75056.6736
                    Surrogate loss: 0.0073
             Mean action noise std: 0.90
                       Mean reward: 10252.31
               Mean episode length: 415.88
                 Mean success rate: 78.00
                  Mean reward/step: 25.35
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 2.53s
                        Total time: 7639.49s
                               ETA: 2658.9s

################################################################################
                     [1m Learning iteration 2968/4000 [0m

                       Computation: 3210 steps/s (collection: 0.497s, learning 2.054s)
               Value function loss: 91173.2817
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 10364.45
               Mean episode length: 417.51
                 Mean success rate: 78.00
                  Mean reward/step: 25.40
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 24322048
                    Iteration time: 2.55s
                        Total time: 7642.04s
                               ETA: 2656.3s

################################################################################
                     [1m Learning iteration 2969/4000 [0m

                       Computation: 3167 steps/s (collection: 0.495s, learning 2.092s)
               Value function loss: 110594.5170
                    Surrogate loss: 0.0080
             Mean action noise std: 0.90
                       Mean reward: 10783.97
               Mean episode length: 428.90
                 Mean success rate: 81.00
                  Mean reward/step: 25.44
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 2.59s
                        Total time: 7644.63s
                               ETA: 2653.7s

################################################################################
                     [1m Learning iteration 2970/4000 [0m

                       Computation: 3093 steps/s (collection: 0.522s, learning 2.126s)
               Value function loss: 162027.5182
                    Surrogate loss: 0.0076
             Mean action noise std: 0.90
                       Mean reward: 11288.64
               Mean episode length: 444.17
                 Mean success rate: 84.50
                  Mean reward/step: 24.72
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 24338432
                    Iteration time: 2.65s
                        Total time: 7647.27s
                               ETA: 2651.2s

################################################################################
                     [1m Learning iteration 2971/4000 [0m

                       Computation: 3197 steps/s (collection: 0.467s, learning 2.095s)
               Value function loss: 92994.7047
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 11244.10
               Mean episode length: 442.04
                 Mean success rate: 84.50
                  Mean reward/step: 24.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 2.56s
                        Total time: 7649.84s
                               ETA: 2648.6s

################################################################################
                     [1m Learning iteration 2972/4000 [0m

                       Computation: 3183 steps/s (collection: 0.472s, learning 2.101s)
               Value function loss: 65271.8712
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 10979.40
               Mean episode length: 437.18
                 Mean success rate: 83.00
                  Mean reward/step: 25.16
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 24354816
                    Iteration time: 2.57s
                        Total time: 7652.41s
                               ETA: 2646.0s

################################################################################
                     [1m Learning iteration 2973/4000 [0m

                       Computation: 3124 steps/s (collection: 0.496s, learning 2.126s)
               Value function loss: 90760.5372
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 11310.15
               Mean episode length: 445.82
                 Mean success rate: 85.50
                  Mean reward/step: 26.38
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 2.62s
                        Total time: 7655.03s
                               ETA: 2643.5s

################################################################################
                     [1m Learning iteration 2974/4000 [0m

                       Computation: 3044 steps/s (collection: 0.563s, learning 2.128s)
               Value function loss: 75185.7434
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 11111.67
               Mean episode length: 440.03
                 Mean success rate: 84.50
                  Mean reward/step: 26.16
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24371200
                    Iteration time: 2.69s
                        Total time: 7657.72s
                               ETA: 2640.9s

################################################################################
                     [1m Learning iteration 2975/4000 [0m

                       Computation: 3112 steps/s (collection: 0.517s, learning 2.115s)
               Value function loss: 145065.2178
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 11072.42
               Mean episode length: 433.90
                 Mean success rate: 84.50
                  Mean reward/step: 26.06
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 2.63s
                        Total time: 7660.35s
                               ETA: 2638.4s

################################################################################
                     [1m Learning iteration 2976/4000 [0m

                       Computation: 3167 steps/s (collection: 0.479s, learning 2.108s)
               Value function loss: 82859.5598
                    Surrogate loss: 0.0078
             Mean action noise std: 0.90
                       Mean reward: 11142.26
               Mean episode length: 434.58
                 Mean success rate: 85.00
                  Mean reward/step: 25.60
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 24387584
                    Iteration time: 2.59s
                        Total time: 7662.94s
                               ETA: 2635.8s

################################################################################
                     [1m Learning iteration 2977/4000 [0m

                       Computation: 3172 steps/s (collection: 0.461s, learning 2.121s)
               Value function loss: 101330.4187
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 10999.80
               Mean episode length: 430.24
                 Mean success rate: 84.00
                  Mean reward/step: 25.39
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 2.58s
                        Total time: 7665.52s
                               ETA: 2633.3s

################################################################################
                     [1m Learning iteration 2978/4000 [0m

                       Computation: 3061 steps/s (collection: 0.519s, learning 2.157s)
               Value function loss: 68409.6738
                    Surrogate loss: 0.0120
             Mean action noise std: 0.90
                       Mean reward: 10966.31
               Mean episode length: 428.92
                 Mean success rate: 83.50
                  Mean reward/step: 25.39
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 24403968
                    Iteration time: 2.68s
                        Total time: 7668.20s
                               ETA: 2630.7s

################################################################################
                     [1m Learning iteration 2979/4000 [0m

                       Computation: 3121 steps/s (collection: 0.521s, learning 2.104s)
               Value function loss: 100306.7507
                    Surrogate loss: 0.0084
             Mean action noise std: 0.90
                       Mean reward: 11003.59
               Mean episode length: 434.14
                 Mean success rate: 84.00
                  Mean reward/step: 26.08
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 2.62s
                        Total time: 7670.82s
                               ETA: 2628.2s

################################################################################
                     [1m Learning iteration 2980/4000 [0m

                       Computation: 3220 steps/s (collection: 0.477s, learning 2.067s)
               Value function loss: 67764.6432
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 10940.34
               Mean episode length: 432.31
                 Mean success rate: 84.00
                  Mean reward/step: 26.97
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 24420352
                    Iteration time: 2.54s
                        Total time: 7673.37s
                               ETA: 2625.6s

################################################################################
                     [1m Learning iteration 2981/4000 [0m

                       Computation: 3157 steps/s (collection: 0.492s, learning 2.102s)
               Value function loss: 129066.7021
                    Surrogate loss: 0.0108
             Mean action noise std: 0.90
                       Mean reward: 10775.85
               Mean episode length: 424.05
                 Mean success rate: 82.50
                  Mean reward/step: 26.66
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 2.59s
                        Total time: 7675.96s
                               ETA: 2623.0s

################################################################################
                     [1m Learning iteration 2982/4000 [0m

                       Computation: 3117 steps/s (collection: 0.542s, learning 2.086s)
               Value function loss: 97897.7381
                    Surrogate loss: 0.0073
             Mean action noise std: 0.90
                       Mean reward: 10762.28
               Mean episode length: 422.60
                 Mean success rate: 82.50
                  Mean reward/step: 25.34
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 24436736
                    Iteration time: 2.63s
                        Total time: 7678.59s
                               ETA: 2620.5s

################################################################################
                     [1m Learning iteration 2983/4000 [0m

                       Computation: 3142 steps/s (collection: 0.468s, learning 2.139s)
               Value function loss: 92688.8245
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 11045.19
               Mean episode length: 427.46
                 Mean success rate: 84.00
                  Mean reward/step: 25.10
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 2.61s
                        Total time: 7681.20s
                               ETA: 2617.9s

################################################################################
                     [1m Learning iteration 2984/4000 [0m

                       Computation: 3136 steps/s (collection: 0.505s, learning 2.107s)
               Value function loss: 109748.3428
                    Surrogate loss: 0.0140
             Mean action noise std: 0.90
                       Mean reward: 10996.40
               Mean episode length: 424.94
                 Mean success rate: 83.50
                  Mean reward/step: 25.53
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 24453120
                    Iteration time: 2.61s
                        Total time: 7683.81s
                               ETA: 2615.3s

################################################################################
                     [1m Learning iteration 2985/4000 [0m

                       Computation: 3158 steps/s (collection: 0.484s, learning 2.110s)
               Value function loss: 97617.1279
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 11509.10
               Mean episode length: 440.29
                 Mean success rate: 87.00
                  Mean reward/step: 25.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 2.59s
                        Total time: 7686.40s
                               ETA: 2612.8s

################################################################################
                     [1m Learning iteration 2986/4000 [0m

                       Computation: 3108 steps/s (collection: 0.530s, learning 2.105s)
               Value function loss: 126253.5881
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 11406.32
               Mean episode length: 437.29
                 Mean success rate: 86.00
                  Mean reward/step: 24.22
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 24469504
                    Iteration time: 2.64s
                        Total time: 7689.04s
                               ETA: 2610.2s

################################################################################
                     [1m Learning iteration 2987/4000 [0m

                       Computation: 3069 steps/s (collection: 0.556s, learning 2.113s)
               Value function loss: 72563.5515
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 11348.03
               Mean episode length: 437.13
                 Mean success rate: 86.00
                  Mean reward/step: 24.31
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 2.67s
                        Total time: 7691.71s
                               ETA: 2607.7s

################################################################################
                     [1m Learning iteration 2988/4000 [0m

                       Computation: 3225 steps/s (collection: 0.450s, learning 2.090s)
               Value function loss: 90457.4911
                    Surrogate loss: 0.0088
             Mean action noise std: 0.90
                       Mean reward: 11185.21
               Mean episode length: 430.85
                 Mean success rate: 85.50
                  Mean reward/step: 25.59
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24485888
                    Iteration time: 2.54s
                        Total time: 7694.25s
                               ETA: 2605.1s

################################################################################
                     [1m Learning iteration 2989/4000 [0m

                       Computation: 3192 steps/s (collection: 0.466s, learning 2.100s)
               Value function loss: 119716.9898
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 11173.56
               Mean episode length: 429.48
                 Mean success rate: 85.00
                  Mean reward/step: 25.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 2.57s
                        Total time: 7696.81s
                               ETA: 2602.5s

################################################################################
                     [1m Learning iteration 2990/4000 [0m

                       Computation: 3186 steps/s (collection: 0.466s, learning 2.105s)
               Value function loss: 94158.3369
                    Surrogate loss: 0.0074
             Mean action noise std: 0.90
                       Mean reward: 11295.27
               Mean episode length: 433.81
                 Mean success rate: 85.50
                  Mean reward/step: 25.41
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 24502272
                    Iteration time: 2.57s
                        Total time: 7699.38s
                               ETA: 2599.9s

################################################################################
                     [1m Learning iteration 2991/4000 [0m

                       Computation: 3054 steps/s (collection: 0.553s, learning 2.129s)
               Value function loss: 114712.5866
                    Surrogate loss: 0.0126
             Mean action noise std: 0.90
                       Mean reward: 11253.39
               Mean episode length: 435.42
                 Mean success rate: 85.50
                  Mean reward/step: 25.16
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 2.68s
                        Total time: 7702.07s
                               ETA: 2597.4s

################################################################################
                     [1m Learning iteration 2992/4000 [0m

                       Computation: 3162 steps/s (collection: 0.495s, learning 2.096s)
               Value function loss: 104718.2498
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 11169.29
               Mean episode length: 432.25
                 Mean success rate: 84.50
                  Mean reward/step: 25.22
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24518656
                    Iteration time: 2.59s
                        Total time: 7704.66s
                               ETA: 2594.8s

################################################################################
                     [1m Learning iteration 2993/4000 [0m

                       Computation: 3211 steps/s (collection: 0.456s, learning 2.095s)
               Value function loss: 120597.1027
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 11112.17
               Mean episode length: 432.37
                 Mean success rate: 84.00
                  Mean reward/step: 24.70
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 2.55s
                        Total time: 7707.21s
                               ETA: 2592.2s

################################################################################
                     [1m Learning iteration 2994/4000 [0m

                       Computation: 3203 steps/s (collection: 0.465s, learning 2.092s)
               Value function loss: 47327.2076
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 11002.08
               Mean episode length: 430.54
                 Mean success rate: 83.50
                  Mean reward/step: 25.44
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 24535040
                    Iteration time: 2.56s
                        Total time: 7709.76s
                               ETA: 2589.7s

################################################################################
                     [1m Learning iteration 2995/4000 [0m

                       Computation: 3086 steps/s (collection: 0.507s, learning 2.147s)
               Value function loss: 77748.8634
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 10442.71
               Mean episode length: 417.98
                 Mean success rate: 81.00
                  Mean reward/step: 26.48
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 2.65s
                        Total time: 7712.42s
                               ETA: 2587.1s

################################################################################
                     [1m Learning iteration 2996/4000 [0m

                       Computation: 3183 steps/s (collection: 0.489s, learning 2.084s)
               Value function loss: 67863.2019
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 10468.02
               Mean episode length: 418.31
                 Mean success rate: 81.50
                  Mean reward/step: 26.89
       Mean episode length/episode: 31.15
--------------------------------------------------------------------------------
                   Total timesteps: 24551424
                    Iteration time: 2.57s
                        Total time: 7714.99s
                               ETA: 2584.5s

################################################################################
                     [1m Learning iteration 2997/4000 [0m

                       Computation: 3141 steps/s (collection: 0.490s, learning 2.117s)
               Value function loss: 131088.6846
                    Surrogate loss: 0.0120
             Mean action noise std: 0.90
                       Mean reward: 10652.28
               Mean episode length: 425.83
                 Mean success rate: 82.50
                  Mean reward/step: 26.50
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 2.61s
                        Total time: 7717.60s
                               ETA: 2582.0s

################################################################################
                     [1m Learning iteration 2998/4000 [0m

                       Computation: 3163 steps/s (collection: 0.477s, learning 2.112s)
               Value function loss: 98207.5941
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 10752.37
               Mean episode length: 427.36
                 Mean success rate: 82.50
                  Mean reward/step: 25.53
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24567808
                    Iteration time: 2.59s
                        Total time: 7720.19s
                               ETA: 2579.4s

################################################################################
                     [1m Learning iteration 2999/4000 [0m

                       Computation: 3225 steps/s (collection: 0.462s, learning 2.077s)
               Value function loss: 80429.2673
                    Surrogate loss: 0.0105
             Mean action noise std: 0.90
                       Mean reward: 11026.77
               Mean episode length: 435.88
                 Mean success rate: 84.00
                  Mean reward/step: 25.81
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 2.54s
                        Total time: 7722.73s
                               ETA: 2576.8s

################################################################################
                     [1m Learning iteration 3000/4000 [0m

                       Computation: 3135 steps/s (collection: 0.489s, learning 2.124s)
               Value function loss: 117075.7349
                    Surrogate loss: 0.0083
             Mean action noise std: 0.90
                       Mean reward: 10751.53
               Mean episode length: 423.86
                 Mean success rate: 82.00
                  Mean reward/step: 25.83
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 24584192
                    Iteration time: 2.61s
                        Total time: 7725.34s
                               ETA: 2574.3s

################################################################################
                     [1m Learning iteration 3001/4000 [0m

                       Computation: 3176 steps/s (collection: 0.474s, learning 2.105s)
               Value function loss: 151364.7152
                    Surrogate loss: 0.0088
             Mean action noise std: 0.90
                       Mean reward: 10982.18
               Mean episode length: 430.51
                 Mean success rate: 84.00
                  Mean reward/step: 25.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 2.58s
                        Total time: 7727.92s
                               ETA: 2571.7s

################################################################################
                     [1m Learning iteration 3002/4000 [0m

                       Computation: 3236 steps/s (collection: 0.445s, learning 2.087s)
               Value function loss: 89521.9986
                    Surrogate loss: 0.0117
             Mean action noise std: 0.90
                       Mean reward: 10806.74
               Mean episode length: 425.26
                 Mean success rate: 83.00
                  Mean reward/step: 23.93
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24600576
                    Iteration time: 2.53s
                        Total time: 7730.45s
                               ETA: 2569.1s

################################################################################
                     [1m Learning iteration 3003/4000 [0m

                       Computation: 3105 steps/s (collection: 0.529s, learning 2.109s)
               Value function loss: 67966.1877
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 10924.35
               Mean episode length: 427.58
                 Mean success rate: 83.50
                  Mean reward/step: 24.35
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 2.64s
                        Total time: 7733.09s
                               ETA: 2566.5s

################################################################################
                     [1m Learning iteration 3004/4000 [0m

                       Computation: 3233 steps/s (collection: 0.495s, learning 2.039s)
               Value function loss: 113341.5145
                    Surrogate loss: 0.0117
             Mean action noise std: 0.90
                       Mean reward: 10980.76
               Mean episode length: 429.05
                 Mean success rate: 84.00
                  Mean reward/step: 25.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24616960
                    Iteration time: 2.53s
                        Total time: 7735.62s
                               ETA: 2564.0s

################################################################################
                     [1m Learning iteration 3005/4000 [0m

                       Computation: 3171 steps/s (collection: 0.471s, learning 2.112s)
               Value function loss: 97219.3721
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 11274.06
               Mean episode length: 437.62
                 Mean success rate: 85.50
                  Mean reward/step: 25.77
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 2.58s
                        Total time: 7738.20s
                               ETA: 2561.4s

################################################################################
                     [1m Learning iteration 3006/4000 [0m

                       Computation: 3179 steps/s (collection: 0.474s, learning 2.102s)
               Value function loss: 128784.4357
                    Surrogate loss: 0.0105
             Mean action noise std: 0.90
                       Mean reward: 11386.85
               Mean episode length: 440.00
                 Mean success rate: 86.00
                  Mean reward/step: 25.59
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24633344
                    Iteration time: 2.58s
                        Total time: 7740.78s
                               ETA: 2558.8s

################################################################################
                     [1m Learning iteration 3007/4000 [0m

                       Computation: 3214 steps/s (collection: 0.485s, learning 2.063s)
               Value function loss: 71893.8909
                    Surrogate loss: 0.0117
             Mean action noise std: 0.90
                       Mean reward: 11092.86
               Mean episode length: 433.00
                 Mean success rate: 85.00
                  Mean reward/step: 25.06
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 2.55s
                        Total time: 7743.33s
                               ETA: 2556.2s

################################################################################
                     [1m Learning iteration 3008/4000 [0m

                       Computation: 3164 steps/s (collection: 0.515s, learning 2.074s)
               Value function loss: 114286.3727
                    Surrogate loss: 0.0147
             Mean action noise std: 0.90
                       Mean reward: 11119.32
               Mean episode length: 433.95
                 Mean success rate: 85.00
                  Mean reward/step: 25.37
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 24649728
                    Iteration time: 2.59s
                        Total time: 7745.92s
                               ETA: 2553.7s

################################################################################
                     [1m Learning iteration 3009/4000 [0m

                       Computation: 3220 steps/s (collection: 0.450s, learning 2.094s)
               Value function loss: 90260.4433
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 10880.54
               Mean episode length: 430.44
                 Mean success rate: 84.00
                  Mean reward/step: 24.93
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 2.54s
                        Total time: 7748.46s
                               ETA: 2551.1s

################################################################################
                     [1m Learning iteration 3010/4000 [0m

                       Computation: 3205 steps/s (collection: 0.465s, learning 2.091s)
               Value function loss: 84378.8476
                    Surrogate loss: 0.0125
             Mean action noise std: 0.90
                       Mean reward: 10936.07
               Mean episode length: 434.29
                 Mean success rate: 84.50
                  Mean reward/step: 25.48
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 24666112
                    Iteration time: 2.56s
                        Total time: 7751.02s
                               ETA: 2548.5s

################################################################################
                     [1m Learning iteration 3011/4000 [0m

                       Computation: 3109 steps/s (collection: 0.521s, learning 2.113s)
               Value function loss: 65761.8909
                    Surrogate loss: 0.0131
             Mean action noise std: 0.90
                       Mean reward: 10995.98
               Mean episode length: 437.07
                 Mean success rate: 85.00
                  Mean reward/step: 25.22
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 2.63s
                        Total time: 7753.65s
                               ETA: 2545.9s

################################################################################
                     [1m Learning iteration 3012/4000 [0m

                       Computation: 3182 steps/s (collection: 0.461s, learning 2.113s)
               Value function loss: 95997.4775
                    Surrogate loss: 0.0099
             Mean action noise std: 0.90
                       Mean reward: 10790.55
               Mean episode length: 427.96
                 Mean success rate: 83.00
                  Mean reward/step: 26.00
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24682496
                    Iteration time: 2.57s
                        Total time: 7756.23s
                               ETA: 2543.4s

################################################################################
                     [1m Learning iteration 3013/4000 [0m

                       Computation: 3242 steps/s (collection: 0.453s, learning 2.073s)
               Value function loss: 130454.1227
                    Surrogate loss: 0.0126
             Mean action noise std: 0.90
                       Mean reward: 10870.64
               Mean episode length: 430.21
                 Mean success rate: 83.00
                  Mean reward/step: 25.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 2.53s
                        Total time: 7758.75s
                               ETA: 2540.8s

################################################################################
                     [1m Learning iteration 3014/4000 [0m

                       Computation: 3143 steps/s (collection: 0.515s, learning 2.092s)
               Value function loss: 71255.6145
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 10143.06
               Mean episode length: 406.69
                 Mean success rate: 77.50
                  Mean reward/step: 24.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 24698880
                    Iteration time: 2.61s
                        Total time: 7761.36s
                               ETA: 2538.2s

################################################################################
                     [1m Learning iteration 3015/4000 [0m

                       Computation: 3131 steps/s (collection: 0.545s, learning 2.071s)
               Value function loss: 90649.0355
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 9924.06
               Mean episode length: 396.90
                 Mean success rate: 75.50
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 2.62s
                        Total time: 7763.97s
                               ETA: 2535.6s

################################################################################
                     [1m Learning iteration 3016/4000 [0m

                       Computation: 3133 steps/s (collection: 0.524s, learning 2.090s)
               Value function loss: 118384.2916
                    Surrogate loss: 0.0143
             Mean action noise std: 0.90
                       Mean reward: 10083.22
               Mean episode length: 401.36
                 Mean success rate: 76.50
                  Mean reward/step: 25.15
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24715264
                    Iteration time: 2.61s
                        Total time: 7766.59s
                               ETA: 2533.1s

################################################################################
                     [1m Learning iteration 3017/4000 [0m

                       Computation: 3219 steps/s (collection: 0.493s, learning 2.051s)
               Value function loss: 131924.3233
                    Surrogate loss: 0.0123
             Mean action noise std: 0.90
                       Mean reward: 9894.58
               Mean episode length: 396.21
                 Mean success rate: 75.00
                  Mean reward/step: 24.82
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 2.54s
                        Total time: 7769.13s
                               ETA: 2530.5s

################################################################################
                     [1m Learning iteration 3018/4000 [0m

                       Computation: 3142 steps/s (collection: 0.507s, learning 2.100s)
               Value function loss: 96253.3073
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 9918.11
               Mean episode length: 395.46
                 Mean success rate: 74.50
                  Mean reward/step: 24.87
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24731648
                    Iteration time: 2.61s
                        Total time: 7771.74s
                               ETA: 2527.9s

################################################################################
                     [1m Learning iteration 3019/4000 [0m

                       Computation: 3193 steps/s (collection: 0.507s, learning 2.058s)
               Value function loss: 65584.6515
                    Surrogate loss: 0.0122
             Mean action noise std: 0.90
                       Mean reward: 9642.06
               Mean episode length: 385.29
                 Mean success rate: 72.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 2.57s
                        Total time: 7774.31s
                               ETA: 2525.4s

################################################################################
                     [1m Learning iteration 3020/4000 [0m

                       Computation: 3151 steps/s (collection: 0.498s, learning 2.101s)
               Value function loss: 107862.9697
                    Surrogate loss: 0.0122
             Mean action noise std: 0.90
                       Mean reward: 8927.94
               Mean episode length: 364.23
                 Mean success rate: 67.50
                  Mean reward/step: 24.79
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 24748032
                    Iteration time: 2.60s
                        Total time: 7776.90s
                               ETA: 2522.8s

################################################################################
                     [1m Learning iteration 3021/4000 [0m

                       Computation: 3194 steps/s (collection: 0.485s, learning 2.079s)
               Value function loss: 92547.1190
                    Surrogate loss: 0.0119
             Mean action noise std: 0.90
                       Mean reward: 8773.85
               Mean episode length: 357.42
                 Mean success rate: 66.50
                  Mean reward/step: 23.89
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 2.56s
                        Total time: 7779.47s
                               ETA: 2520.2s

################################################################################
                     [1m Learning iteration 3022/4000 [0m

                       Computation: 3200 steps/s (collection: 0.475s, learning 2.085s)
               Value function loss: 119871.3157
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9235.91
               Mean episode length: 371.57
                 Mean success rate: 70.00
                  Mean reward/step: 23.68
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 24764416
                    Iteration time: 2.56s
                        Total time: 7782.03s
                               ETA: 2517.6s

################################################################################
                     [1m Learning iteration 3023/4000 [0m

                       Computation: 3201 steps/s (collection: 0.499s, learning 2.059s)
               Value function loss: 79023.6979
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 9748.44
               Mean episode length: 387.54
                 Mean success rate: 74.00
                  Mean reward/step: 24.86
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.56s
                        Total time: 7784.59s
                               ETA: 2515.1s

################################################################################
                     [1m Learning iteration 3024/4000 [0m

                       Computation: 3205 steps/s (collection: 0.504s, learning 2.052s)
               Value function loss: 120132.0823
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9690.07
               Mean episode length: 388.38
                 Mean success rate: 74.00
                  Mean reward/step: 24.85
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 24780800
                    Iteration time: 2.56s
                        Total time: 7787.14s
                               ETA: 2512.5s

################################################################################
                     [1m Learning iteration 3025/4000 [0m

                       Computation: 3157 steps/s (collection: 0.503s, learning 2.092s)
               Value function loss: 77409.2173
                    Surrogate loss: 0.0099
             Mean action noise std: 0.90
                       Mean reward: 9454.00
               Mean episode length: 381.18
                 Mean success rate: 72.00
                  Mean reward/step: 25.16
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 2.59s
                        Total time: 7789.74s
                               ETA: 2509.9s

################################################################################
                     [1m Learning iteration 3026/4000 [0m

                       Computation: 3183 steps/s (collection: 0.478s, learning 2.095s)
               Value function loss: 70402.3916
                    Surrogate loss: 0.0123
             Mean action noise std: 0.90
                       Mean reward: 9435.38
               Mean episode length: 381.06
                 Mean success rate: 71.50
                  Mean reward/step: 25.73
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 24797184
                    Iteration time: 2.57s
                        Total time: 7792.31s
                               ETA: 2507.3s

################################################################################
                     [1m Learning iteration 3027/4000 [0m

                       Computation: 3211 steps/s (collection: 0.494s, learning 2.057s)
               Value function loss: 55115.6420
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9539.60
               Mean episode length: 382.81
                 Mean success rate: 72.50
                  Mean reward/step: 26.25
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 2.55s
                        Total time: 7794.86s
                               ETA: 2504.8s

################################################################################
                     [1m Learning iteration 3028/4000 [0m

                       Computation: 3307 steps/s (collection: 0.419s, learning 2.057s)
               Value function loss: 101667.1704
                    Surrogate loss: 0.0108
             Mean action noise std: 0.90
                       Mean reward: 9625.22
               Mean episode length: 384.03
                 Mean success rate: 72.50
                  Mean reward/step: 26.63
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24813568
                    Iteration time: 2.48s
                        Total time: 7797.34s
                               ETA: 2502.2s

################################################################################
                     [1m Learning iteration 3029/4000 [0m

                       Computation: 3230 steps/s (collection: 0.465s, learning 2.071s)
               Value function loss: 99493.8670
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 10061.80
               Mean episode length: 395.44
                 Mean success rate: 75.50
                  Mean reward/step: 26.43
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 2.54s
                        Total time: 7799.87s
                               ETA: 2499.6s

################################################################################
                     [1m Learning iteration 3030/4000 [0m

                       Computation: 3322 steps/s (collection: 0.428s, learning 2.038s)
               Value function loss: 123982.1663
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 10607.48
               Mean episode length: 411.25
                 Mean success rate: 79.50
                  Mean reward/step: 25.48
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 24829952
                    Iteration time: 2.47s
                        Total time: 7802.34s
                               ETA: 2497.0s

################################################################################
                     [1m Learning iteration 3031/4000 [0m

                       Computation: 3242 steps/s (collection: 0.480s, learning 2.046s)
               Value function loss: 124060.5287
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 10846.60
               Mean episode length: 420.00
                 Mean success rate: 81.50
                  Mean reward/step: 24.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 2.53s
                        Total time: 7804.87s
                               ETA: 2494.4s

################################################################################
                     [1m Learning iteration 3032/4000 [0m

                       Computation: 3196 steps/s (collection: 0.514s, learning 2.049s)
               Value function loss: 82275.1904
                    Surrogate loss: 0.0108
             Mean action noise std: 0.90
                       Mean reward: 10546.36
               Mean episode length: 412.61
                 Mean success rate: 79.50
                  Mean reward/step: 24.96
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24846336
                    Iteration time: 2.56s
                        Total time: 7807.43s
                               ETA: 2491.8s

################################################################################
                     [1m Learning iteration 3033/4000 [0m

                       Computation: 3329 steps/s (collection: 0.453s, learning 2.007s)
               Value function loss: 86184.0444
                    Surrogate loss: 0.0099
             Mean action noise std: 0.90
                       Mean reward: 10305.88
               Mean episode length: 408.02
                 Mean success rate: 78.50
                  Mean reward/step: 25.18
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 2.46s
                        Total time: 7809.89s
                               ETA: 2489.2s

################################################################################
                     [1m Learning iteration 3034/4000 [0m

                       Computation: 3298 steps/s (collection: 0.440s, learning 2.043s)
               Value function loss: 84059.6130
                    Surrogate loss: 0.0139
             Mean action noise std: 0.90
                       Mean reward: 10182.90
               Mean episode length: 408.20
                 Mean success rate: 78.00
                  Mean reward/step: 25.66
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 24862720
                    Iteration time: 2.48s
                        Total time: 7812.37s
                               ETA: 2486.6s

################################################################################
                     [1m Learning iteration 3035/4000 [0m

                       Computation: 3321 steps/s (collection: 0.446s, learning 2.020s)
               Value function loss: 118712.2514
                    Surrogate loss: 0.0105
             Mean action noise std: 0.90
                       Mean reward: 10478.64
               Mean episode length: 416.37
                 Mean success rate: 80.50
                  Mean reward/step: 26.24
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 2.47s
                        Total time: 7814.84s
                               ETA: 2484.0s

################################################################################
                     [1m Learning iteration 3036/4000 [0m

                       Computation: 3306 steps/s (collection: 0.463s, learning 2.015s)
               Value function loss: 143936.4277
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 10702.56
               Mean episode length: 420.63
                 Mean success rate: 81.50
                  Mean reward/step: 25.25
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 24879104
                    Iteration time: 2.48s
                        Total time: 7817.32s
                               ETA: 2481.4s

################################################################################
                     [1m Learning iteration 3037/4000 [0m

                       Computation: 3311 steps/s (collection: 0.446s, learning 2.028s)
               Value function loss: 124074.0641
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 10783.84
               Mean episode length: 423.70
                 Mean success rate: 82.00
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 2.47s
                        Total time: 7819.79s
                               ETA: 2478.8s

################################################################################
                     [1m Learning iteration 3038/4000 [0m

                       Computation: 3211 steps/s (collection: 0.443s, learning 2.108s)
               Value function loss: 79239.4777
                    Surrogate loss: 0.0105
             Mean action noise std: 0.90
                       Mean reward: 10551.56
               Mean episode length: 418.14
                 Mean success rate: 80.50
                  Mean reward/step: 23.82
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24895488
                    Iteration time: 2.55s
                        Total time: 7822.34s
                               ETA: 2476.2s

################################################################################
                     [1m Learning iteration 3039/4000 [0m

                       Computation: 3279 steps/s (collection: 0.437s, learning 2.061s)
               Value function loss: 133399.3547
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 10808.96
               Mean episode length: 425.75
                 Mean success rate: 82.50
                  Mean reward/step: 24.46
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 2.50s
                        Total time: 7824.84s
                               ETA: 2473.6s

################################################################################
                     [1m Learning iteration 3040/4000 [0m

                       Computation: 3170 steps/s (collection: 0.487s, learning 2.097s)
               Value function loss: 117931.8545
                    Surrogate loss: 0.0121
             Mean action noise std: 0.90
                       Mean reward: 10651.77
               Mean episode length: 421.50
                 Mean success rate: 81.50
                  Mean reward/step: 23.97
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24911872
                    Iteration time: 2.58s
                        Total time: 7827.42s
                               ETA: 2471.0s

################################################################################
                     [1m Learning iteration 3041/4000 [0m

                       Computation: 3195 steps/s (collection: 0.492s, learning 2.071s)
               Value function loss: 44464.2029
                    Surrogate loss: 0.0119
             Mean action noise std: 0.90
                       Mean reward: 10529.53
               Mean episode length: 417.41
                 Mean success rate: 80.00
                  Mean reward/step: 24.27
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 2.56s
                        Total time: 7829.99s
                               ETA: 2468.4s

################################################################################
                     [1m Learning iteration 3042/4000 [0m

                       Computation: 3167 steps/s (collection: 0.485s, learning 2.101s)
               Value function loss: 47099.9697
                    Surrogate loss: 0.0087
             Mean action noise std: 0.90
                       Mean reward: 10677.79
               Mean episode length: 422.54
                 Mean success rate: 81.50
                  Mean reward/step: 25.54
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 24928256
                    Iteration time: 2.59s
                        Total time: 7832.57s
                               ETA: 2465.9s

################################################################################
                     [1m Learning iteration 3043/4000 [0m

                       Computation: 3198 steps/s (collection: 0.496s, learning 2.065s)
               Value function loss: 80163.8284
                    Surrogate loss: 0.0125
             Mean action noise std: 0.90
                       Mean reward: 10477.48
               Mean episode length: 416.66
                 Mean success rate: 80.00
                  Mean reward/step: 26.43
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 2.56s
                        Total time: 7835.13s
                               ETA: 2463.3s

################################################################################
                     [1m Learning iteration 3044/4000 [0m

                       Computation: 3137 steps/s (collection: 0.506s, learning 2.105s)
               Value function loss: 119550.0521
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 10739.78
               Mean episode length: 419.89
                 Mean success rate: 81.00
                  Mean reward/step: 25.61
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24944640
                    Iteration time: 2.61s
                        Total time: 7837.75s
                               ETA: 2460.7s

################################################################################
                     [1m Learning iteration 3045/4000 [0m

                       Computation: 3172 steps/s (collection: 0.482s, learning 2.100s)
               Value function loss: 112502.8238
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 10764.78
               Mean episode length: 419.39
                 Mean success rate: 80.50
                  Mean reward/step: 24.94
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 2.58s
                        Total time: 7840.33s
                               ETA: 2458.1s

################################################################################
                     [1m Learning iteration 3046/4000 [0m

                       Computation: 3297 steps/s (collection: 0.433s, learning 2.052s)
               Value function loss: 85843.3519
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 10753.56
               Mean episode length: 419.39
                 Mean success rate: 80.50
                  Mean reward/step: 24.35
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 24961024
                    Iteration time: 2.48s
                        Total time: 7842.81s
                               ETA: 2455.5s

################################################################################
                     [1m Learning iteration 3047/4000 [0m

                       Computation: 3208 steps/s (collection: 0.477s, learning 2.076s)
               Value function loss: 118348.7937
                    Surrogate loss: 0.0134
             Mean action noise std: 0.90
                       Mean reward: 10753.65
               Mean episode length: 422.19
                 Mean success rate: 81.50
                  Mean reward/step: 24.25
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 2.55s
                        Total time: 7845.37s
                               ETA: 2453.0s

################################################################################
                     [1m Learning iteration 3048/4000 [0m

                       Computation: 3168 steps/s (collection: 0.503s, learning 2.082s)
               Value function loss: 93725.8186
                    Surrogate loss: 0.0133
             Mean action noise std: 0.90
                       Mean reward: 10630.20
               Mean episode length: 418.70
                 Mean success rate: 80.50
                  Mean reward/step: 24.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 24977408
                    Iteration time: 2.59s
                        Total time: 7847.95s
                               ETA: 2450.4s

################################################################################
                     [1m Learning iteration 3049/4000 [0m

                       Computation: 3201 steps/s (collection: 0.453s, learning 2.106s)
               Value function loss: 91008.5434
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 10692.23
               Mean episode length: 421.88
                 Mean success rate: 81.50
                  Mean reward/step: 24.65
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 2.56s
                        Total time: 7850.51s
                               ETA: 2447.8s

################################################################################
                     [1m Learning iteration 3050/4000 [0m

                       Computation: 3227 steps/s (collection: 0.497s, learning 2.041s)
               Value function loss: 68647.2882
                    Surrogate loss: 0.0152
             Mean action noise std: 0.90
                       Mean reward: 10520.62
               Mean episode length: 417.96
                 Mean success rate: 80.00
                  Mean reward/step: 25.11
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 24993792
                    Iteration time: 2.54s
                        Total time: 7853.05s
                               ETA: 2445.2s

################################################################################
                     [1m Learning iteration 3051/4000 [0m

                       Computation: 3245 steps/s (collection: 0.486s, learning 2.038s)
               Value function loss: 122685.0968
                    Surrogate loss: 0.0130
             Mean action noise std: 0.90
                       Mean reward: 10531.86
               Mean episode length: 417.64
                 Mean success rate: 80.00
                  Mean reward/step: 25.34
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 2.52s
                        Total time: 7855.57s
                               ETA: 2442.6s

################################################################################
                     [1m Learning iteration 3052/4000 [0m

                       Computation: 3191 steps/s (collection: 0.463s, learning 2.104s)
               Value function loss: 126457.0143
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 10589.83
               Mean episode length: 421.57
                 Mean success rate: 81.50
                  Mean reward/step: 24.74
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 25010176
                    Iteration time: 2.57s
                        Total time: 7858.14s
                               ETA: 2440.1s

################################################################################
                     [1m Learning iteration 3053/4000 [0m

                       Computation: 3219 steps/s (collection: 0.486s, learning 2.059s)
               Value function loss: 156969.5709
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 10440.32
               Mean episode length: 422.21
                 Mean success rate: 81.50
                  Mean reward/step: 23.76
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 2.54s
                        Total time: 7860.68s
                               ETA: 2437.5s

################################################################################
                     [1m Learning iteration 3054/4000 [0m

                       Computation: 3291 steps/s (collection: 0.444s, learning 2.045s)
               Value function loss: 59801.0606
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 10318.79
               Mean episode length: 418.00
                 Mean success rate: 80.50
                  Mean reward/step: 23.71
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 25026560
                    Iteration time: 2.49s
                        Total time: 7863.17s
                               ETA: 2434.9s

################################################################################
                     [1m Learning iteration 3055/4000 [0m

                       Computation: 3237 steps/s (collection: 0.470s, learning 2.061s)
               Value function loss: 125933.7824
                    Surrogate loss: 0.0108
             Mean action noise std: 0.90
                       Mean reward: 9886.85
               Mean episode length: 405.50
                 Mean success rate: 78.00
                  Mean reward/step: 24.61
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 2.53s
                        Total time: 7865.70s
                               ETA: 2432.3s

################################################################################
                     [1m Learning iteration 3056/4000 [0m

                       Computation: 3228 steps/s (collection: 0.453s, learning 2.084s)
               Value function loss: 100934.1152
                    Surrogate loss: 0.0138
             Mean action noise std: 0.90
                       Mean reward: 9932.00
               Mean episode length: 407.75
                 Mean success rate: 78.00
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 25042944
                    Iteration time: 2.54s
                        Total time: 7868.24s
                               ETA: 2429.7s

################################################################################
                     [1m Learning iteration 3057/4000 [0m

                       Computation: 3251 steps/s (collection: 0.430s, learning 2.090s)
               Value function loss: 42158.1309
                    Surrogate loss: 0.0134
             Mean action noise std: 0.90
                       Mean reward: 9787.13
               Mean episode length: 403.06
                 Mean success rate: 77.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 2.52s
                        Total time: 7870.76s
                               ETA: 2427.1s

################################################################################
                     [1m Learning iteration 3058/4000 [0m

                       Computation: 3191 steps/s (collection: 0.485s, learning 2.082s)
               Value function loss: 45473.7696
                    Surrogate loss: 0.0091
             Mean action noise std: 0.90
                       Mean reward: 9580.75
               Mean episode length: 394.55
                 Mean success rate: 75.50
                  Mean reward/step: 25.33
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 25059328
                    Iteration time: 2.57s
                        Total time: 7873.33s
                               ETA: 2424.5s

################################################################################
                     [1m Learning iteration 3059/4000 [0m

                       Computation: 3265 steps/s (collection: 0.457s, learning 2.051s)
               Value function loss: 107743.3566
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 9641.77
               Mean episode length: 394.48
                 Mean success rate: 76.00
                  Mean reward/step: 26.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 2.51s
                        Total time: 7875.83s
                               ETA: 2421.9s

################################################################################
                     [1m Learning iteration 3060/4000 [0m

                       Computation: 3311 steps/s (collection: 0.458s, learning 2.016s)
               Value function loss: 106827.0004
                    Surrogate loss: 0.0132
             Mean action noise std: 0.90
                       Mean reward: 9292.66
               Mean episode length: 383.48
                 Mean success rate: 74.00
                  Mean reward/step: 25.72
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 25075712
                    Iteration time: 2.47s
                        Total time: 7878.31s
                               ETA: 2419.3s

################################################################################
                     [1m Learning iteration 3061/4000 [0m

                       Computation: 3310 steps/s (collection: 0.440s, learning 2.034s)
               Value function loss: 88948.8156
                    Surrogate loss: 0.0084
             Mean action noise std: 0.90
                       Mean reward: 9178.22
               Mean episode length: 380.81
                 Mean success rate: 73.50
                  Mean reward/step: 25.18
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 2.47s
                        Total time: 7880.78s
                               ETA: 2416.7s

################################################################################
                     [1m Learning iteration 3062/4000 [0m

                       Computation: 3334 steps/s (collection: 0.434s, learning 2.023s)
               Value function loss: 76424.1126
                    Surrogate loss: 0.0087
             Mean action noise std: 0.90
                       Mean reward: 9443.41
               Mean episode length: 388.94
                 Mean success rate: 75.00
                  Mean reward/step: 25.54
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 25092096
                    Iteration time: 2.46s
                        Total time: 7883.24s
                               ETA: 2414.1s

################################################################################
                     [1m Learning iteration 3063/4000 [0m

                       Computation: 3285 steps/s (collection: 0.462s, learning 2.031s)
               Value function loss: 145751.2936
                    Surrogate loss: 0.0073
             Mean action noise std: 0.90
                       Mean reward: 9439.98
               Mean episode length: 382.55
                 Mean success rate: 74.50
                  Mean reward/step: 25.10
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 2.49s
                        Total time: 7885.73s
                               ETA: 2411.5s

################################################################################
                     [1m Learning iteration 3064/4000 [0m

                       Computation: 3316 steps/s (collection: 0.448s, learning 2.022s)
               Value function loss: 95844.3533
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9265.23
               Mean episode length: 381.75
                 Mean success rate: 75.00
                  Mean reward/step: 24.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 25108480
                    Iteration time: 2.47s
                        Total time: 7888.20s
                               ETA: 2408.9s

################################################################################
                     [1m Learning iteration 3065/4000 [0m

                       Computation: 3288 steps/s (collection: 0.459s, learning 2.032s)
               Value function loss: 91200.7901
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9431.51
               Mean episode length: 382.98
                 Mean success rate: 75.50
                  Mean reward/step: 24.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 2.49s
                        Total time: 7890.69s
                               ETA: 2406.3s

################################################################################
                     [1m Learning iteration 3066/4000 [0m

                       Computation: 3228 steps/s (collection: 0.510s, learning 2.028s)
               Value function loss: 74361.6729
                    Surrogate loss: 0.0093
             Mean action noise std: 0.90
                       Mean reward: 9382.59
               Mean episode length: 379.32
                 Mean success rate: 74.50
                  Mean reward/step: 24.97
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25124864
                    Iteration time: 2.54s
                        Total time: 7893.23s
                               ETA: 2403.7s

################################################################################
                     [1m Learning iteration 3067/4000 [0m

                       Computation: 3256 steps/s (collection: 0.478s, learning 2.037s)
               Value function loss: 116579.0025
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 9666.02
               Mean episode length: 386.88
                 Mean success rate: 77.00
                  Mean reward/step: 25.21
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 2.52s
                        Total time: 7895.75s
                               ETA: 2401.2s

################################################################################
                     [1m Learning iteration 3068/4000 [0m

                       Computation: 3321 steps/s (collection: 0.445s, learning 2.021s)
               Value function loss: 119951.2010
                    Surrogate loss: 0.0119
             Mean action noise std: 0.90
                       Mean reward: 9878.14
               Mean episode length: 395.44
                 Mean success rate: 79.00
                  Mean reward/step: 24.55
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 25141248
                    Iteration time: 2.47s
                        Total time: 7898.21s
                               ETA: 2398.5s

################################################################################
                     [1m Learning iteration 3069/4000 [0m

                       Computation: 3314 steps/s (collection: 0.464s, learning 2.008s)
               Value function loss: 114798.1405
                    Surrogate loss: 0.0127
             Mean action noise std: 0.90
                       Mean reward: 10233.89
               Mean episode length: 407.05
                 Mean success rate: 81.50
                  Mean reward/step: 23.71
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 2.47s
                        Total time: 7900.68s
                               ETA: 2395.9s

################################################################################
                     [1m Learning iteration 3070/4000 [0m

                       Computation: 3251 steps/s (collection: 0.488s, learning 2.031s)
               Value function loss: 77886.9783
                    Surrogate loss: 0.0096
             Mean action noise std: 0.90
                       Mean reward: 10334.52
               Mean episode length: 410.50
                 Mean success rate: 82.00
                  Mean reward/step: 24.03
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25157632
                    Iteration time: 2.52s
                        Total time: 7903.20s
                               ETA: 2393.4s

################################################################################
                     [1m Learning iteration 3071/4000 [0m

                       Computation: 3265 steps/s (collection: 0.489s, learning 2.020s)
               Value function loss: 122470.8533
                    Surrogate loss: 0.0125
             Mean action noise std: 0.90
                       Mean reward: 10144.32
               Mean episode length: 401.17
                 Mean success rate: 80.50
                  Mean reward/step: 24.05
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.51s
                        Total time: 7905.71s
                               ETA: 2390.8s

################################################################################
                     [1m Learning iteration 3072/4000 [0m

                       Computation: 3317 steps/s (collection: 0.434s, learning 2.035s)
               Value function loss: 61640.4669
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 10299.08
               Mean episode length: 406.57
                 Mean success rate: 81.00
                  Mean reward/step: 24.22
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25174016
                    Iteration time: 2.47s
                        Total time: 7908.18s
                               ETA: 2388.2s

################################################################################
                     [1m Learning iteration 3073/4000 [0m

                       Computation: 3363 steps/s (collection: 0.409s, learning 2.027s)
               Value function loss: 67293.5257
                    Surrogate loss: 0.0092
             Mean action noise std: 0.90
                       Mean reward: 10205.85
               Mean episode length: 406.79
                 Mean success rate: 81.00
                  Mean reward/step: 25.36
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 2.44s
                        Total time: 7910.62s
                               ETA: 2385.5s

################################################################################
                     [1m Learning iteration 3074/4000 [0m

                       Computation: 3250 steps/s (collection: 0.476s, learning 2.044s)
               Value function loss: 73443.2760
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 10295.32
               Mean episode length: 407.92
                 Mean success rate: 80.50
                  Mean reward/step: 26.65
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 25190400
                    Iteration time: 2.52s
                        Total time: 7913.14s
                               ETA: 2382.9s

################################################################################
                     [1m Learning iteration 3075/4000 [0m

                       Computation: 3259 steps/s (collection: 0.494s, learning 2.019s)
               Value function loss: 95221.7091
                    Surrogate loss: 0.0084
             Mean action noise std: 0.90
                       Mean reward: 10373.33
               Mean episode length: 410.62
                 Mean success rate: 81.00
                  Mean reward/step: 26.27
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 2.51s
                        Total time: 7915.65s
                               ETA: 2380.4s

################################################################################
                     [1m Learning iteration 3076/4000 [0m

                       Computation: 3315 steps/s (collection: 0.455s, learning 2.016s)
               Value function loss: 96918.7710
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 9966.52
               Mean episode length: 403.38
                 Mean success rate: 79.00
                  Mean reward/step: 25.82
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 25206784
                    Iteration time: 2.47s
                        Total time: 7918.12s
                               ETA: 2377.8s

################################################################################
                     [1m Learning iteration 3077/4000 [0m

                       Computation: 3272 steps/s (collection: 0.480s, learning 2.024s)
               Value function loss: 80261.3210
                    Surrogate loss: 0.0092
             Mean action noise std: 0.90
                       Mean reward: 10153.60
               Mean episode length: 409.02
                 Mean success rate: 80.00
                  Mean reward/step: 25.21
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 2.50s
                        Total time: 7920.62s
                               ETA: 2375.2s

################################################################################
                     [1m Learning iteration 3078/4000 [0m

                       Computation: 3287 steps/s (collection: 0.474s, learning 2.018s)
               Value function loss: 100410.8580
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 9966.17
               Mean episode length: 406.31
                 Mean success rate: 78.50
                  Mean reward/step: 25.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25223168
                    Iteration time: 2.49s
                        Total time: 7923.12s
                               ETA: 2372.6s

################################################################################
                     [1m Learning iteration 3079/4000 [0m

                       Computation: 3334 steps/s (collection: 0.436s, learning 2.021s)
               Value function loss: 132227.1273
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 10041.92
               Mean episode length: 408.81
                 Mean success rate: 79.00
                  Mean reward/step: 25.54
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 2.46s
                        Total time: 7925.57s
                               ETA: 2370.0s

################################################################################
                     [1m Learning iteration 3080/4000 [0m

                       Computation: 3337 steps/s (collection: 0.449s, learning 2.005s)
               Value function loss: 102827.9739
                    Surrogate loss: 0.0089
             Mean action noise std: 0.90
                       Mean reward: 9800.24
               Mean episode length: 399.71
                 Mean success rate: 76.50
                  Mean reward/step: 24.60
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 25239552
                    Iteration time: 2.45s
                        Total time: 7928.03s
                               ETA: 2367.3s

################################################################################
                     [1m Learning iteration 3081/4000 [0m

                       Computation: 3377 steps/s (collection: 0.423s, learning 2.002s)
               Value function loss: 91601.6226
                    Surrogate loss: 0.0091
             Mean action noise std: 0.90
                       Mean reward: 9708.40
               Mean episode length: 396.48
                 Mean success rate: 76.00
                  Mean reward/step: 24.93
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 2.43s
                        Total time: 7930.45s
                               ETA: 2364.7s

################################################################################
                     [1m Learning iteration 3082/4000 [0m

                       Computation: 3353 steps/s (collection: 0.424s, learning 2.018s)
               Value function loss: 89772.7971
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 9644.60
               Mean episode length: 396.80
                 Mean success rate: 75.50
                  Mean reward/step: 25.48
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25255936
                    Iteration time: 2.44s
                        Total time: 7932.90s
                               ETA: 2362.1s

################################################################################
                     [1m Learning iteration 3083/4000 [0m

                       Computation: 3365 steps/s (collection: 0.422s, learning 2.012s)
               Value function loss: 120155.1177
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 10163.76
               Mean episode length: 410.69
                 Mean success rate: 79.00
                  Mean reward/step: 25.20
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.43s
                        Total time: 7935.33s
                               ETA: 2359.5s

################################################################################
                     [1m Learning iteration 3084/4000 [0m

                       Computation: 3313 steps/s (collection: 0.442s, learning 2.030s)
               Value function loss: 131923.8210
                    Surrogate loss: 0.0094
             Mean action noise std: 0.90
                       Mean reward: 9992.55
               Mean episode length: 405.79
                 Mean success rate: 78.00
                  Mean reward/step: 24.41
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 25272320
                    Iteration time: 2.47s
                        Total time: 7937.80s
                               ETA: 2356.9s

################################################################################
                     [1m Learning iteration 3085/4000 [0m

                       Computation: 3333 steps/s (collection: 0.432s, learning 2.025s)
               Value function loss: 80075.4061
                    Surrogate loss: 0.0082
             Mean action noise std: 0.90
                       Mean reward: 10266.73
               Mean episode length: 412.69
                 Mean success rate: 80.00
                  Mean reward/step: 23.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 2.46s
                        Total time: 7940.26s
                               ETA: 2354.3s

################################################################################
                     [1m Learning iteration 3086/4000 [0m

                       Computation: 3210 steps/s (collection: 0.474s, learning 2.077s)
               Value function loss: 109930.5857
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 10269.26
               Mean episode length: 406.70
                 Mean success rate: 79.00
                  Mean reward/step: 24.79
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 25288704
                    Iteration time: 2.55s
                        Total time: 7942.81s
                               ETA: 2351.7s

################################################################################
                     [1m Learning iteration 3087/4000 [0m

                       Computation: 3175 steps/s (collection: 0.478s, learning 2.102s)
               Value function loss: 106641.1626
                    Surrogate loss: 0.0117
             Mean action noise std: 0.90
                       Mean reward: 10460.42
               Mean episode length: 408.68
                 Mean success rate: 79.50
                  Mean reward/step: 24.90
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 2.58s
                        Total time: 7945.39s
                               ETA: 2349.1s

################################################################################
                     [1m Learning iteration 3088/4000 [0m

                       Computation: 3114 steps/s (collection: 0.442s, learning 2.188s)
               Value function loss: 82878.2546
                    Surrogate loss: 0.0123
             Mean action noise std: 0.90
                       Mean reward: 9618.50
               Mean episode length: 382.15
                 Mean success rate: 74.00
                  Mean reward/step: 24.93
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 25305088
                    Iteration time: 2.63s
                        Total time: 7948.02s
                               ETA: 2346.6s

################################################################################
                     [1m Learning iteration 3089/4000 [0m

                       Computation: 3171 steps/s (collection: 0.467s, learning 2.116s)
               Value function loss: 64089.6836
                    Surrogate loss: 0.0085
             Mean action noise std: 0.90
                       Mean reward: 9701.43
               Mean episode length: 385.24
                 Mean success rate: 75.00
                  Mean reward/step: 25.61
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 2.58s
                        Total time: 7950.61s
                               ETA: 2344.0s

################################################################################
                     [1m Learning iteration 3090/4000 [0m

                       Computation: 3137 steps/s (collection: 0.482s, learning 2.129s)
               Value function loss: 85173.3899
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 9874.76
               Mean episode length: 389.38
                 Mean success rate: 75.50
                  Mean reward/step: 26.33
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25321472
                    Iteration time: 2.61s
                        Total time: 7953.22s
                               ETA: 2341.5s

################################################################################
                     [1m Learning iteration 3091/4000 [0m

                       Computation: 3149 steps/s (collection: 0.456s, learning 2.145s)
               Value function loss: 114464.6646
                    Surrogate loss: 0.0091
             Mean action noise std: 0.90
                       Mean reward: 9962.87
               Mean episode length: 391.77
                 Mean success rate: 76.00
                  Mean reward/step: 26.35
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 2.60s
                        Total time: 7955.82s
                               ETA: 2338.9s

################################################################################
                     [1m Learning iteration 3092/4000 [0m

                       Computation: 3197 steps/s (collection: 0.448s, learning 2.114s)
               Value function loss: 102856.3292
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 10020.47
               Mean episode length: 394.08
                 Mean success rate: 76.50
                  Mean reward/step: 25.56
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25337856
                    Iteration time: 2.56s
                        Total time: 7958.38s
                               ETA: 2336.3s

################################################################################
                     [1m Learning iteration 3093/4000 [0m

                       Computation: 3124 steps/s (collection: 0.472s, learning 2.149s)
               Value function loss: 74836.5766
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 10148.64
               Mean episode length: 397.00
                 Mean success rate: 77.00
                  Mean reward/step: 25.58
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 2.62s
                        Total time: 7961.00s
                               ETA: 2333.8s

################################################################################
                     [1m Learning iteration 3094/4000 [0m

                       Computation: 3060 steps/s (collection: 0.543s, learning 2.134s)
               Value function loss: 127319.6671
                    Surrogate loss: 0.0096
             Mean action noise std: 0.90
                       Mean reward: 10457.54
               Mean episode length: 405.50
                 Mean success rate: 79.00
                  Mean reward/step: 25.94
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 25354240
                    Iteration time: 2.68s
                        Total time: 7963.68s
                               ETA: 2331.2s

################################################################################
                     [1m Learning iteration 3095/4000 [0m

                       Computation: 3151 steps/s (collection: 0.465s, learning 2.135s)
               Value function loss: 124457.6918
                    Surrogate loss: 0.0092
             Mean action noise std: 0.90
                       Mean reward: 10365.52
               Mean episode length: 403.85
                 Mean success rate: 79.00
                  Mean reward/step: 25.33
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 2.60s
                        Total time: 7966.28s
                               ETA: 2328.6s

################################################################################
                     [1m Learning iteration 3096/4000 [0m

                       Computation: 3169 steps/s (collection: 0.479s, learning 2.106s)
               Value function loss: 100990.7178
                    Surrogate loss: 0.0090
             Mean action noise std: 0.90
                       Mean reward: 10694.84
               Mean episode length: 416.44
                 Mean success rate: 81.50
                  Mean reward/step: 24.42
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25370624
                    Iteration time: 2.58s
                        Total time: 7968.86s
                               ETA: 2326.1s

################################################################################
                     [1m Learning iteration 3097/4000 [0m

                       Computation: 3126 steps/s (collection: 0.470s, learning 2.151s)
               Value function loss: 92893.3607
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 10530.48
               Mean episode length: 416.70
                 Mean success rate: 81.00
                  Mean reward/step: 24.86
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 2.62s
                        Total time: 7971.48s
                               ETA: 2323.5s

################################################################################
                     [1m Learning iteration 3098/4000 [0m

                       Computation: 3193 steps/s (collection: 0.519s, learning 2.046s)
               Value function loss: 100247.6842
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 10461.31
               Mean episode length: 416.88
                 Mean success rate: 81.00
                  Mean reward/step: 25.26
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25387008
                    Iteration time: 2.57s
                        Total time: 7974.05s
                               ETA: 2320.9s

################################################################################
                     [1m Learning iteration 3099/4000 [0m

                       Computation: 3166 steps/s (collection: 0.527s, learning 2.060s)
               Value function loss: 110756.1441
                    Surrogate loss: 0.0078
             Mean action noise std: 0.90
                       Mean reward: 10948.45
               Mean episode length: 431.72
                 Mean success rate: 84.00
                  Mean reward/step: 25.04
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 2.59s
                        Total time: 7976.63s
                               ETA: 2318.4s

################################################################################
                     [1m Learning iteration 3100/4000 [0m

                       Computation: 3229 steps/s (collection: 0.476s, learning 2.061s)
               Value function loss: 153402.8141
                    Surrogate loss: 0.0117
             Mean action noise std: 0.90
                       Mean reward: 11191.20
               Mean episode length: 440.20
                 Mean success rate: 86.00
                  Mean reward/step: 24.00
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 25403392
                    Iteration time: 2.54s
                        Total time: 7979.17s
                               ETA: 2315.8s

################################################################################
                     [1m Learning iteration 3101/4000 [0m

                       Computation: 3275 steps/s (collection: 0.476s, learning 2.025s)
               Value function loss: 85785.7797
                    Surrogate loss: 0.0096
             Mean action noise std: 0.90
                       Mean reward: 11241.57
               Mean episode length: 442.12
                 Mean success rate: 86.50
                  Mean reward/step: 24.33
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 2.50s
                        Total time: 7981.67s
                               ETA: 2313.2s

################################################################################
                     [1m Learning iteration 3102/4000 [0m

                       Computation: 3187 steps/s (collection: 0.522s, learning 2.048s)
               Value function loss: 85311.1879
                    Surrogate loss: 0.0133
             Mean action noise std: 0.90
                       Mean reward: 10963.24
               Mean episode length: 433.39
                 Mean success rate: 84.50
                  Mean reward/step: 24.88
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 25419776
                    Iteration time: 2.57s
                        Total time: 7984.24s
                               ETA: 2310.6s

################################################################################
                     [1m Learning iteration 3103/4000 [0m

                       Computation: 3260 steps/s (collection: 0.466s, learning 2.046s)
               Value function loss: 102821.3789
                    Surrogate loss: 0.0100
             Mean action noise std: 0.90
                       Mean reward: 11014.54
               Mean episode length: 433.94
                 Mean success rate: 85.50
                  Mean reward/step: 24.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 2.51s
                        Total time: 7986.76s
                               ETA: 2308.0s

################################################################################
                     [1m Learning iteration 3104/4000 [0m

                       Computation: 3252 steps/s (collection: 0.462s, learning 2.057s)
               Value function loss: 71651.2886
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 10419.15
               Mean episode length: 416.70
                 Mean success rate: 81.00
                  Mean reward/step: 24.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 25436160
                    Iteration time: 2.52s
                        Total time: 7989.27s
                               ETA: 2305.4s

################################################################################
                     [1m Learning iteration 3105/4000 [0m

                       Computation: 3180 steps/s (collection: 0.502s, learning 2.074s)
               Value function loss: 59101.6505
                    Surrogate loss: 0.0084
             Mean action noise std: 0.90
                       Mean reward: 9865.87
               Mean episode length: 400.35
                 Mean success rate: 77.00
                  Mean reward/step: 25.52
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 2.58s
                        Total time: 7991.85s
                               ETA: 2302.9s

################################################################################
                     [1m Learning iteration 3106/4000 [0m

                       Computation: 3204 steps/s (collection: 0.493s, learning 2.064s)
               Value function loss: 93106.5113
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 10007.16
               Mean episode length: 405.60
                 Mean success rate: 78.00
                  Mean reward/step: 26.66
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 25452544
                    Iteration time: 2.56s
                        Total time: 7994.41s
                               ETA: 2300.3s

################################################################################
                     [1m Learning iteration 3107/4000 [0m

                       Computation: 3221 steps/s (collection: 0.462s, learning 2.081s)
               Value function loss: 124697.1566
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 10341.67
               Mean episode length: 413.81
                 Mean success rate: 80.50
                  Mean reward/step: 25.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 2.54s
                        Total time: 7996.95s
                               ETA: 2297.7s

################################################################################
                     [1m Learning iteration 3108/4000 [0m

                       Computation: 3262 steps/s (collection: 0.452s, learning 2.058s)
               Value function loss: 85213.2094
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 10231.28
               Mean episode length: 409.52
                 Mean success rate: 79.50
                  Mean reward/step: 24.95
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25468928
                    Iteration time: 2.51s
                        Total time: 7999.46s
                               ETA: 2295.1s

################################################################################
                     [1m Learning iteration 3109/4000 [0m

                       Computation: 3217 steps/s (collection: 0.454s, learning 2.092s)
               Value function loss: 65538.7336
                    Surrogate loss: 0.0091
             Mean action noise std: 0.90
                       Mean reward: 10254.21
               Mean episode length: 410.69
                 Mean success rate: 80.50
                  Mean reward/step: 25.19
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 2.55s
                        Total time: 8002.01s
                               ETA: 2292.5s

################################################################################
                     [1m Learning iteration 3110/4000 [0m

                       Computation: 3087 steps/s (collection: 0.527s, learning 2.126s)
               Value function loss: 125712.2078
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 9836.26
               Mean episode length: 395.73
                 Mean success rate: 77.50
                  Mean reward/step: 25.32
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 25485312
                    Iteration time: 2.65s
                        Total time: 8004.66s
                               ETA: 2290.0s

################################################################################
                     [1m Learning iteration 3111/4000 [0m

                       Computation: 3192 steps/s (collection: 0.480s, learning 2.086s)
               Value function loss: 129356.4066
                    Surrogate loss: 0.0090
             Mean action noise std: 0.90
                       Mean reward: 9967.58
               Mean episode length: 399.30
                 Mean success rate: 78.00
                  Mean reward/step: 24.45
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 2.57s
                        Total time: 8007.22s
                               ETA: 2287.4s

################################################################################
                     [1m Learning iteration 3112/4000 [0m

                       Computation: 3140 steps/s (collection: 0.501s, learning 2.107s)
               Value function loss: 68463.8028
                    Surrogate loss: 0.0089
             Mean action noise std: 0.90
                       Mean reward: 9915.58
               Mean episode length: 395.75
                 Mean success rate: 77.00
                  Mean reward/step: 24.49
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25501696
                    Iteration time: 2.61s
                        Total time: 8009.83s
                               ETA: 2284.8s

################################################################################
                     [1m Learning iteration 3113/4000 [0m

                       Computation: 3130 steps/s (collection: 0.508s, learning 2.109s)
               Value function loss: 82543.4436
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9772.53
               Mean episode length: 391.19
                 Mean success rate: 76.00
                  Mean reward/step: 25.53
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 2.62s
                        Total time: 8012.45s
                               ETA: 2282.3s

################################################################################
                     [1m Learning iteration 3114/4000 [0m

                       Computation: 3084 steps/s (collection: 0.522s, learning 2.133s)
               Value function loss: 115792.6452
                    Surrogate loss: 0.0086
             Mean action noise std: 0.90
                       Mean reward: 10213.71
               Mean episode length: 403.13
                 Mean success rate: 78.50
                  Mean reward/step: 25.40
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 25518080
                    Iteration time: 2.66s
                        Total time: 8015.10s
                               ETA: 2279.7s

################################################################################
                     [1m Learning iteration 3115/4000 [0m

                       Computation: 3112 steps/s (collection: 0.520s, learning 2.112s)
               Value function loss: 144076.5744
                    Surrogate loss: 0.0083
             Mean action noise std: 0.90
                       Mean reward: 10875.92
               Mean episode length: 424.50
                 Mean success rate: 83.50
                  Mean reward/step: 24.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 2.63s
                        Total time: 8017.74s
                               ETA: 2277.2s

################################################################################
                     [1m Learning iteration 3116/4000 [0m

                       Computation: 3270 steps/s (collection: 0.490s, learning 2.015s)
               Value function loss: 75414.7193
                    Surrogate loss: 0.0107
             Mean action noise std: 0.90
                       Mean reward: 10674.43
               Mean episode length: 420.29
                 Mean success rate: 82.50
                  Mean reward/step: 23.61
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25534464
                    Iteration time: 2.50s
                        Total time: 8020.24s
                               ETA: 2274.6s

################################################################################
                     [1m Learning iteration 3117/4000 [0m

                       Computation: 3366 steps/s (collection: 0.453s, learning 1.980s)
               Value function loss: 87664.0269
                    Surrogate loss: 0.0076
             Mean action noise std: 0.90
                       Mean reward: 10459.16
               Mean episode length: 412.61
                 Mean success rate: 81.00
                  Mean reward/step: 24.05
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 2.43s
                        Total time: 8022.67s
                               ETA: 2272.0s

################################################################################
                     [1m Learning iteration 3118/4000 [0m

                       Computation: 3139 steps/s (collection: 0.525s, learning 2.084s)
               Value function loss: 106124.5529
                    Surrogate loss: 0.0100
             Mean action noise std: 0.90
                       Mean reward: 10196.05
               Mean episode length: 404.49
                 Mean success rate: 78.50
                  Mean reward/step: 24.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 25550848
                    Iteration time: 2.61s
                        Total time: 8025.28s
                               ETA: 2269.4s

################################################################################
                     [1m Learning iteration 3119/4000 [0m

                       Computation: 3202 steps/s (collection: 0.489s, learning 2.069s)
               Value function loss: 89172.7135
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 10025.66
               Mean episode length: 402.83
                 Mean success rate: 78.00
                  Mean reward/step: 24.91
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 2.56s
                        Total time: 8027.84s
                               ETA: 2266.8s

################################################################################
                     [1m Learning iteration 3120/4000 [0m

                       Computation: 3255 steps/s (collection: 0.483s, learning 2.034s)
               Value function loss: 84214.1795
                    Surrogate loss: 0.0078
             Mean action noise std: 0.90
                       Mean reward: 9802.15
               Mean episode length: 397.81
                 Mean success rate: 76.00
                  Mean reward/step: 24.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 25567232
                    Iteration time: 2.52s
                        Total time: 8030.36s
                               ETA: 2264.2s

################################################################################
                     [1m Learning iteration 3121/4000 [0m

                       Computation: 3249 steps/s (collection: 0.474s, learning 2.047s)
               Value function loss: 90229.1518
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 9631.17
               Mean episode length: 394.66
                 Mean success rate: 75.00
                  Mean reward/step: 25.32
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 2.52s
                        Total time: 8032.88s
                               ETA: 2261.7s

################################################################################
                     [1m Learning iteration 3122/4000 [0m

                       Computation: 3219 steps/s (collection: 0.501s, learning 2.044s)
               Value function loss: 79586.2974
                    Surrogate loss: 0.0054
             Mean action noise std: 0.90
                       Mean reward: 9502.41
               Mean episode length: 389.37
                 Mean success rate: 74.50
                  Mean reward/step: 26.23
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25583616
                    Iteration time: 2.54s
                        Total time: 8035.42s
                               ETA: 2259.1s

################################################################################
                     [1m Learning iteration 3123/4000 [0m

                       Computation: 3199 steps/s (collection: 0.511s, learning 2.049s)
               Value function loss: 105245.4531
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 9792.13
               Mean episode length: 395.94
                 Mean success rate: 76.00
                  Mean reward/step: 26.87
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 2.56s
                        Total time: 8037.98s
                               ETA: 2256.5s

################################################################################
                     [1m Learning iteration 3124/4000 [0m

                       Computation: 3246 steps/s (collection: 0.491s, learning 2.032s)
               Value function loss: 72190.5308
                    Surrogate loss: 0.0118
             Mean action noise std: 0.90
                       Mean reward: 9886.59
               Mean episode length: 397.44
                 Mean success rate: 76.50
                  Mean reward/step: 26.76
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 25600000
                    Iteration time: 2.52s
                        Total time: 8040.51s
                               ETA: 2253.9s

################################################################################
                     [1m Learning iteration 3125/4000 [0m

                       Computation: 3298 steps/s (collection: 0.447s, learning 2.037s)
               Value function loss: 108653.2691
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 9918.05
               Mean episode length: 400.90
                 Mean success rate: 77.00
                  Mean reward/step: 26.48
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 2.48s
                        Total time: 8042.99s
                               ETA: 2251.3s

################################################################################
                     [1m Learning iteration 3126/4000 [0m

                       Computation: 3194 steps/s (collection: 0.517s, learning 2.047s)
               Value function loss: 130069.4945
                    Surrogate loss: 0.0073
             Mean action noise std: 0.90
                       Mean reward: 10049.64
               Mean episode length: 401.86
                 Mean success rate: 77.00
                  Mean reward/step: 25.21
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 25616384
                    Iteration time: 2.56s
                        Total time: 8045.56s
                               ETA: 2248.7s

################################################################################
                     [1m Learning iteration 3127/4000 [0m

                       Computation: 3215 steps/s (collection: 0.494s, learning 2.054s)
               Value function loss: 85742.3359
                    Surrogate loss: 0.0079
             Mean action noise std: 0.90
                       Mean reward: 10127.19
               Mean episode length: 401.75
                 Mean success rate: 77.00
                  Mean reward/step: 24.83
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 2.55s
                        Total time: 8048.10s
                               ETA: 2246.2s

################################################################################
                     [1m Learning iteration 3128/4000 [0m

                       Computation: 3252 steps/s (collection: 0.485s, learning 2.033s)
               Value function loss: 69014.6658
                    Surrogate loss: 0.0079
             Mean action noise std: 0.90
                       Mean reward: 10179.95
               Mean episode length: 403.62
                 Mean success rate: 77.50
                  Mean reward/step: 25.56
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 25632768
                    Iteration time: 2.52s
                        Total time: 8050.62s
                               ETA: 2243.6s

################################################################################
                     [1m Learning iteration 3129/4000 [0m

                       Computation: 3264 steps/s (collection: 0.462s, learning 2.047s)
               Value function loss: 77401.3229
                    Surrogate loss: 0.0100
             Mean action noise std: 0.90
                       Mean reward: 10296.28
               Mean episode length: 406.34
                 Mean success rate: 78.00
                  Mean reward/step: 26.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 2.51s
                        Total time: 8053.13s
                               ETA: 2241.0s

################################################################################
                     [1m Learning iteration 3130/4000 [0m

                       Computation: 3209 steps/s (collection: 0.540s, learning 2.012s)
               Value function loss: 111847.2895
                    Surrogate loss: 0.0082
             Mean action noise std: 0.90
                       Mean reward: 10473.89
               Mean episode length: 413.82
                 Mean success rate: 79.50
                  Mean reward/step: 26.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 25649152
                    Iteration time: 2.55s
                        Total time: 8055.68s
                               ETA: 2238.4s

################################################################################
                     [1m Learning iteration 3131/4000 [0m

                       Computation: 3273 steps/s (collection: 0.467s, learning 2.035s)
               Value function loss: 138499.4563
                    Surrogate loss: 0.0076
             Mean action noise std: 0.90
                       Mean reward: 10665.48
               Mean episode length: 418.21
                 Mean success rate: 81.50
                  Mean reward/step: 25.11
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.50s
                        Total time: 8058.19s
                               ETA: 2235.8s

################################################################################
                     [1m Learning iteration 3132/4000 [0m

                       Computation: 3278 steps/s (collection: 0.456s, learning 2.043s)
               Value function loss: 94371.1309
                    Surrogate loss: 0.0050
             Mean action noise std: 0.90
                       Mean reward: 10984.79
               Mean episode length: 427.54
                 Mean success rate: 83.00
                  Mean reward/step: 24.40
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 25665536
                    Iteration time: 2.50s
                        Total time: 8060.69s
                               ETA: 2233.2s

################################################################################
                     [1m Learning iteration 3133/4000 [0m

                       Computation: 3212 steps/s (collection: 0.489s, learning 2.061s)
               Value function loss: 89812.3142
                    Surrogate loss: 0.0092
             Mean action noise std: 0.90
                       Mean reward: 10635.23
               Mean episode length: 419.69
                 Mean success rate: 81.00
                  Mean reward/step: 25.00
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 2.55s
                        Total time: 8063.24s
                               ETA: 2230.6s

################################################################################
                     [1m Learning iteration 3134/4000 [0m

                       Computation: 3168 steps/s (collection: 0.520s, learning 2.065s)
               Value function loss: 108760.0669
                    Surrogate loss: 0.0088
             Mean action noise std: 0.90
                       Mean reward: 10431.46
               Mean episode length: 411.31
                 Mean success rate: 79.50
                  Mean reward/step: 25.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 25681920
                    Iteration time: 2.59s
                        Total time: 8065.82s
                               ETA: 2228.1s

################################################################################
                     [1m Learning iteration 3135/4000 [0m

                       Computation: 3227 steps/s (collection: 0.513s, learning 2.025s)
               Value function loss: 128537.6803
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 10505.60
               Mean episode length: 412.17
                 Mean success rate: 79.50
                  Mean reward/step: 24.84
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 2.54s
                        Total time: 8068.36s
                               ETA: 2225.5s

################################################################################
                     [1m Learning iteration 3136/4000 [0m

                       Computation: 3219 steps/s (collection: 0.488s, learning 2.057s)
               Value function loss: 87161.1426
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 10304.31
               Mean episode length: 405.75
                 Mean success rate: 78.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25698304
                    Iteration time: 2.54s
                        Total time: 8070.90s
                               ETA: 2222.9s

################################################################################
                     [1m Learning iteration 3137/4000 [0m

                       Computation: 3070 steps/s (collection: 0.562s, learning 2.106s)
               Value function loss: 93667.2776
                    Surrogate loss: 0.0131
             Mean action noise std: 0.90
                       Mean reward: 10488.86
               Mean episode length: 413.00
                 Mean success rate: 80.00
                  Mean reward/step: 24.40
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 2.67s
                        Total time: 8073.57s
                               ETA: 2220.4s

################################################################################
                     [1m Learning iteration 3138/4000 [0m

                       Computation: 3191 steps/s (collection: 0.495s, learning 2.072s)
               Value function loss: 80588.5341
                    Surrogate loss: 0.0083
             Mean action noise std: 0.90
                       Mean reward: 10475.82
               Mean episode length: 410.47
                 Mean success rate: 79.50
                  Mean reward/step: 25.26
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 25714688
                    Iteration time: 2.57s
                        Total time: 8076.14s
                               ETA: 2217.8s

################################################################################
                     [1m Learning iteration 3139/4000 [0m

                       Computation: 3258 steps/s (collection: 0.470s, learning 2.044s)
               Value function loss: 104783.2597
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 10648.48
               Mean episode length: 418.49
                 Mean success rate: 81.00
                  Mean reward/step: 25.36
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 2.51s
                        Total time: 8078.65s
                               ETA: 2215.2s

################################################################################
                     [1m Learning iteration 3140/4000 [0m

                       Computation: 3225 steps/s (collection: 0.497s, learning 2.043s)
               Value function loss: 51122.8936
                    Surrogate loss: 0.0086
             Mean action noise std: 0.90
                       Mean reward: 10427.48
               Mean episode length: 409.98
                 Mean success rate: 79.00
                  Mean reward/step: 25.30
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25731072
                    Iteration time: 2.54s
                        Total time: 8081.19s
                               ETA: 2212.6s

################################################################################
                     [1m Learning iteration 3141/4000 [0m

                       Computation: 3209 steps/s (collection: 0.490s, learning 2.062s)
               Value function loss: 145593.3579
                    Surrogate loss: 0.0073
             Mean action noise std: 0.90
                       Mean reward: 10603.57
               Mean episode length: 412.88
                 Mean success rate: 79.50
                  Mean reward/step: 25.58
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 2.55s
                        Total time: 8083.74s
                               ETA: 2210.0s

################################################################################
                     [1m Learning iteration 3142/4000 [0m

                       Computation: 3258 steps/s (collection: 0.481s, learning 2.033s)
               Value function loss: 99005.2578
                    Surrogate loss: 0.0086
             Mean action noise std: 0.90
                       Mean reward: 10503.77
               Mean episode length: 409.01
                 Mean success rate: 79.00
                  Mean reward/step: 24.27
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 25747456
                    Iteration time: 2.51s
                        Total time: 8086.26s
                               ETA: 2207.4s

################################################################################
                     [1m Learning iteration 3143/4000 [0m

                       Computation: 3218 steps/s (collection: 0.501s, learning 2.044s)
               Value function loss: 81843.0294
                    Surrogate loss: 0.0089
             Mean action noise std: 0.90
                       Mean reward: 10562.28
               Mean episode length: 408.15
                 Mean success rate: 79.00
                  Mean reward/step: 23.88
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.55s
                        Total time: 8088.80s
                               ETA: 2204.9s

################################################################################
                     [1m Learning iteration 3144/4000 [0m

                       Computation: 3193 steps/s (collection: 0.502s, learning 2.062s)
               Value function loss: 65227.0093
                    Surrogate loss: 0.0093
             Mean action noise std: 0.90
                       Mean reward: 10657.82
               Mean episode length: 414.33
                 Mean success rate: 80.00
                  Mean reward/step: 24.86
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 25763840
                    Iteration time: 2.56s
                        Total time: 8091.37s
                               ETA: 2202.3s

################################################################################
                     [1m Learning iteration 3145/4000 [0m

                       Computation: 3222 steps/s (collection: 0.482s, learning 2.060s)
               Value function loss: 111505.2549
                    Surrogate loss: 0.0117
             Mean action noise std: 0.90
                       Mean reward: 10256.72
               Mean episode length: 405.17
                 Mean success rate: 77.50
                  Mean reward/step: 25.69
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 2.54s
                        Total time: 8093.91s
                               ETA: 2199.7s

################################################################################
                     [1m Learning iteration 3146/4000 [0m

                       Computation: 3117 steps/s (collection: 0.522s, learning 2.106s)
               Value function loss: 143879.9189
                    Surrogate loss: 0.0100
             Mean action noise std: 0.90
                       Mean reward: 10348.28
               Mean episode length: 408.95
                 Mean success rate: 78.00
                  Mean reward/step: 25.03
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 25780224
                    Iteration time: 2.63s
                        Total time: 8096.54s
                               ETA: 2197.2s

################################################################################
                     [1m Learning iteration 3147/4000 [0m

                       Computation: 3208 steps/s (collection: 0.505s, learning 2.048s)
               Value function loss: 118883.1547
                    Surrogate loss: 0.0094
             Mean action noise std: 0.90
                       Mean reward: 10093.47
               Mean episode length: 403.01
                 Mean success rate: 76.00
                  Mean reward/step: 23.84
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 2.55s
                        Total time: 8099.09s
                               ETA: 2194.6s

################################################################################
                     [1m Learning iteration 3148/4000 [0m

                       Computation: 3260 steps/s (collection: 0.470s, learning 2.043s)
               Value function loss: 90044.2151
                    Surrogate loss: 0.0105
             Mean action noise std: 0.90
                       Mean reward: 10121.23
               Mean episode length: 402.62
                 Mean success rate: 76.00
                  Mean reward/step: 24.26
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 25796608
                    Iteration time: 2.51s
                        Total time: 8101.60s
                               ETA: 2192.0s

################################################################################
                     [1m Learning iteration 3149/4000 [0m

                       Computation: 3214 steps/s (collection: 0.494s, learning 2.054s)
               Value function loss: 84922.0262
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 10145.03
               Mean episode length: 401.76
                 Mean success rate: 76.00
                  Mean reward/step: 25.80
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 2.55s
                        Total time: 8104.15s
                               ETA: 2189.4s

################################################################################
                     [1m Learning iteration 3150/4000 [0m

                       Computation: 3181 steps/s (collection: 0.499s, learning 2.076s)
               Value function loss: 119763.0514
                    Surrogate loss: 0.0084
             Mean action noise std: 0.90
                       Mean reward: 10286.86
               Mean episode length: 408.69
                 Mean success rate: 78.00
                  Mean reward/step: 26.12
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 25812992
                    Iteration time: 2.58s
                        Total time: 8106.73s
                               ETA: 2186.8s

################################################################################
                     [1m Learning iteration 3151/4000 [0m

                       Computation: 3172 steps/s (collection: 0.510s, learning 2.072s)
               Value function loss: 126968.5287
                    Surrogate loss: 0.0088
             Mean action noise std: 0.90
                       Mean reward: 10314.89
               Mean episode length: 412.78
                 Mean success rate: 79.00
                  Mean reward/step: 25.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 2.58s
                        Total time: 8109.31s
                               ETA: 2184.3s

################################################################################
                     [1m Learning iteration 3152/4000 [0m

                       Computation: 3121 steps/s (collection: 0.512s, learning 2.112s)
               Value function loss: 96403.1902
                    Surrogate loss: 0.0090
             Mean action noise std: 0.90
                       Mean reward: 10532.76
               Mean episode length: 420.40
                 Mean success rate: 80.50
                  Mean reward/step: 25.08
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25829376
                    Iteration time: 2.62s
                        Total time: 8111.93s
                               ETA: 2181.7s

################################################################################
                     [1m Learning iteration 3153/4000 [0m

                       Computation: 3202 steps/s (collection: 0.520s, learning 2.038s)
               Value function loss: 54358.6081
                    Surrogate loss: 0.0076
             Mean action noise std: 0.90
                       Mean reward: 10216.41
               Mean episode length: 410.93
                 Mean success rate: 79.00
                  Mean reward/step: 25.64
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 2.56s
                        Total time: 8114.49s
                               ETA: 2179.1s

################################################################################
                     [1m Learning iteration 3154/4000 [0m

                       Computation: 3236 steps/s (collection: 0.467s, learning 2.064s)
               Value function loss: 87765.7465
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 10177.25
               Mean episode length: 407.19
                 Mean success rate: 78.00
                  Mean reward/step: 26.21
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25845760
                    Iteration time: 2.53s
                        Total time: 8117.02s
                               ETA: 2176.5s

################################################################################
                     [1m Learning iteration 3155/4000 [0m

                       Computation: 3200 steps/s (collection: 0.504s, learning 2.056s)
               Value function loss: 97225.9487
                    Surrogate loss: 0.0105
             Mean action noise std: 0.90
                       Mean reward: 10491.07
               Mean episode length: 419.38
                 Mean success rate: 81.00
                  Mean reward/step: 25.61
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.56s
                        Total time: 8119.58s
                               ETA: 2174.0s

################################################################################
                     [1m Learning iteration 3156/4000 [0m

                       Computation: 3125 steps/s (collection: 0.542s, learning 2.079s)
               Value function loss: 78580.8344
                    Surrogate loss: 0.0068
             Mean action noise std: 0.90
                       Mean reward: 10309.16
               Mean episode length: 415.08
                 Mean success rate: 80.50
                  Mean reward/step: 25.38
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25862144
                    Iteration time: 2.62s
                        Total time: 8122.20s
                               ETA: 2171.4s

################################################################################
                     [1m Learning iteration 3157/4000 [0m

                       Computation: 3126 steps/s (collection: 0.563s, learning 2.057s)
               Value function loss: 125086.5625
                    Surrogate loss: 0.0070
             Mean action noise std: 0.90
                       Mean reward: 10229.88
               Mean episode length: 410.76
                 Mean success rate: 80.00
                  Mean reward/step: 24.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 2.62s
                        Total time: 8124.82s
                               ETA: 2168.8s

################################################################################
                     [1m Learning iteration 3158/4000 [0m

                       Computation: 3248 steps/s (collection: 0.485s, learning 2.037s)
               Value function loss: 97253.8680
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 10158.28
               Mean episode length: 407.85
                 Mean success rate: 79.50
                  Mean reward/step: 23.99
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 25878528
                    Iteration time: 2.52s
                        Total time: 8127.35s
                               ETA: 2166.3s

################################################################################
                     [1m Learning iteration 3159/4000 [0m

                       Computation: 3247 steps/s (collection: 0.491s, learning 2.032s)
               Value function loss: 71422.6592
                    Surrogate loss: 0.0083
             Mean action noise std: 0.90
                       Mean reward: 10312.16
               Mean episode length: 415.00
                 Mean success rate: 81.00
                  Mean reward/step: 24.82
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 2.52s
                        Total time: 8129.87s
                               ETA: 2163.7s

################################################################################
                     [1m Learning iteration 3160/4000 [0m

                       Computation: 3181 steps/s (collection: 0.504s, learning 2.071s)
               Value function loss: 63304.7775
                    Surrogate loss: 0.0093
             Mean action noise std: 0.90
                       Mean reward: 10342.96
               Mean episode length: 412.07
                 Mean success rate: 80.50
                  Mean reward/step: 25.74
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25894912
                    Iteration time: 2.57s
                        Total time: 8132.44s
                               ETA: 2161.1s

################################################################################
                     [1m Learning iteration 3161/4000 [0m

                       Computation: 3160 steps/s (collection: 0.535s, learning 2.057s)
               Value function loss: 102096.6489
                    Surrogate loss: 0.0067
             Mean action noise std: 0.90
                       Mean reward: 10274.49
               Mean episode length: 406.56
                 Mean success rate: 79.00
                  Mean reward/step: 25.91
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 2.59s
                        Total time: 8135.03s
                               ETA: 2158.5s

################################################################################
                     [1m Learning iteration 3162/4000 [0m

                       Computation: 3186 steps/s (collection: 0.502s, learning 2.069s)
               Value function loss: 147754.3740
                    Surrogate loss: 0.0092
             Mean action noise std: 0.90
                       Mean reward: 10054.67
               Mean episode length: 401.23
                 Mean success rate: 77.50
                  Mean reward/step: 25.20
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 25911296
                    Iteration time: 2.57s
                        Total time: 8137.61s
                               ETA: 2156.0s

################################################################################
                     [1m Learning iteration 3163/4000 [0m

                       Computation: 3229 steps/s (collection: 0.492s, learning 2.045s)
               Value function loss: 87482.1867
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 10337.85
               Mean episode length: 410.52
                 Mean success rate: 79.00
                  Mean reward/step: 23.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 2.54s
                        Total time: 8140.14s
                               ETA: 2153.4s

################################################################################
                     [1m Learning iteration 3164/4000 [0m

                       Computation: 3196 steps/s (collection: 0.518s, learning 2.045s)
               Value function loss: 100539.2298
                    Surrogate loss: 0.0086
             Mean action noise std: 0.90
                       Mean reward: 9984.99
               Mean episode length: 396.18
                 Mean success rate: 76.00
                  Mean reward/step: 23.83
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 25927680
                    Iteration time: 2.56s
                        Total time: 8142.71s
                               ETA: 2150.8s

################################################################################
                     [1m Learning iteration 3165/4000 [0m

                       Computation: 3222 steps/s (collection: 0.492s, learning 2.051s)
               Value function loss: 125716.4193
                    Surrogate loss: 0.0090
             Mean action noise std: 0.90
                       Mean reward: 10122.16
               Mean episode length: 401.58
                 Mean success rate: 77.00
                  Mean reward/step: 24.85
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 2.54s
                        Total time: 8145.25s
                               ETA: 2148.2s

################################################################################
                     [1m Learning iteration 3166/4000 [0m

                       Computation: 3223 steps/s (collection: 0.476s, learning 2.065s)
               Value function loss: 122510.4477
                    Surrogate loss: 0.0082
             Mean action noise std: 0.90
                       Mean reward: 10161.41
               Mean episode length: 401.06
                 Mean success rate: 77.00
                  Mean reward/step: 24.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25944064
                    Iteration time: 2.54s
                        Total time: 8147.79s
                               ETA: 2145.6s
Traceback (most recent call last):
  File "tools/train_ppo.py", line 51, in <module>
    train()
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "tools/train_ppo.py", line 47, in train
    ppo.run(num_learning_iterations=max_iterations, log_interval=cfg.train.learn.save_interval)
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/mvp/ppo/ppo.py", line 232, in run
    next_obs, rews, dones, infos = self.vec_env.step(actions)
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/pixmc/tasks/base/vec_task.py", line 129, in step
    self.task.step(actions_tensor)
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/pixmc/tasks/base/base_task.py", line 142, in step
    self.gym.simulate(self.sim)
KeyboardInterrupt
